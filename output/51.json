[
  {
    "title": "Two case studies of open source software development",
    "doi": "https://doi.org/10.1145/567793.567795",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Audris Mockus; Roy T. Fielding; James D. Herbsleb",
    "corresponding_authors": "",
    "abstract": "According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.",
    "cited_by_count": 1694,
    "openalex_id": "https://openalex.org/W2107294940",
    "type": "article"
  },
  {
    "title": "Developing multiagent systems",
    "doi": "https://doi.org/10.1145/958961.958963",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Franco Zambonelli; Nicholas R. Jennings; Michael Wooldridge",
    "corresponding_authors": "",
    "abstract": "Systems composed of interacting autonomous agents offer a promising software engineering approach for developing applications in complex domains. However, this multiagent system paradigm introduces a number of new abstractions and design/development issues when compared with more traditional approaches to software development. Accordingly, new analysis and design methodologies, as well as new tools, are needed to effectively engineer such systems. Against this background, the contribution of this article is twofold. First, we synthesize and clarify the key abstractions of agent-based computing as they pertain to agent-oriented software engineering. In particular, we argue that a multiagent system can naturally be viewed and architected as a computational organization , and we identify the appropriate organizational abstractions that are central to the analysis and design of such systems. Second, we detail and extend the Gaia methodology for the analysis and design of multiagent systems. Gaia exploits the aforementioned organizational abstractions to provide clear guidelines for the analysis and design of complex and open software systems. Two representative case studies are introduced to exemplify Gaia's concepts and to show its use and effectiveness in different types of multiagent system.",
    "cited_by_count": 1295,
    "openalex_id": "https://openalex.org/W2129899354",
    "type": "article"
  },
  {
    "title": "A formal basis for architectural connection",
    "doi": "https://doi.org/10.1145/258077.258078",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Robert J. Allen; David Garlan",
    "corresponding_authors": "",
    "abstract": "As software systems become more complex, the overall system structure—or software architecture—becomes a central design problem. An important step toward an engineering discipline of software is a formal basis for describing and analyzing these designs. In the article we present a formal approach to one aspect of architectural design: the interactions among components. The key idea is to define architectural connectors as explicit semantic entities. These are specified as a collection of protocols that characterize each of the participant roles in an interaction and how these roles interact. We illustrate how this scheme can be used to define a variety of common architectural connectors. We further provide a formal semantics and show how this leads to a system in which architectural compatibility can be checked in a way analogous to type-checking in programming languages.",
    "cited_by_count": 1269,
    "openalex_id": "https://openalex.org/W2013658284",
    "type": "article"
  },
  {
    "title": "Alloy",
    "doi": "https://doi.org/10.1145/505145.505149",
    "publication_date": "2002-04-01",
    "publication_year": 2002,
    "authors": "Daniel Jackson",
    "corresponding_authors": "Daniel Jackson",
    "abstract": "Alloy is a little language for describing structural properties. It offers a declaration syntax compatible with graphical object models, and a set-based formula syntax powerful enough to express complex constraints and yet amenable to a fully automatic semantic analysis. Its meaning is given by translation to an even smaller (formally defined) kernel. This paper presents the language in its entirety, and explains its motivation, contributions and deficiencies.",
    "cited_by_count": 1158,
    "openalex_id": "https://openalex.org/W2060440626",
    "type": "article"
  },
  {
    "title": "The STATEMATE semantics of statecharts",
    "doi": "https://doi.org/10.1145/235321.235322",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "David Harel; A. Naamad",
    "corresponding_authors": "",
    "abstract": "We describe the semantics of statecharts as implemented in the STATEMATE system. This was the first executable semantics defined for the language and has been in use for almost a decade. In terms of the controversy around whether changes made in a given step should take effect in the current step or in the next one, this semantics adopts the latter approach.",
    "cited_by_count": 1059,
    "openalex_id": "https://openalex.org/W2129395888",
    "type": "article"
  },
  {
    "title": "Discovering models of software processes from event-based data",
    "doi": "https://doi.org/10.1145/287000.287001",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Jonathan E. Cook; Alexander L. Wolf",
    "corresponding_authors": "",
    "abstract": "Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies, which would be lowered by automated assistance in creating formal models. To this end, we have developed a data analysis technique that we term process discovery. Under this technique, data describing process events are first captured from an on-going process and then used to generate a formal model of the behavior of that process. In this article we describe a Markov method that we developed specifically for process discovery, as well as describe two additional methods that we adopted from other domains and augmented for our purposes. The three methods range from the purely algorithmic to the purely statistical. We compare the methods and discuss their application in an industrial case study.",
    "cited_by_count": 886,
    "openalex_id": "https://openalex.org/W2004865374",
    "type": "article"
  },
  {
    "title": "Four dark corners of requirements engineering",
    "doi": "https://doi.org/10.1145/237432.237434",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Pamela Zave; Michael Jackson",
    "corresponding_authors": "",
    "abstract": "Research in requirements engineering has produced an extensive body of knowledge, but there are four areas in which the foundation of the discipline seems weak or obscure. This article shines some light in the “four dark corners,” exposing problems and proposing solutions. We show that all descriptions involved in requirements engineering should be descriptions of the environment. We show that certain control information is necessary for sound requirements engineering, and we explain the close association between domain knowledge and refinement of requirements. Together these conclusions explain the precise nature of requirements, specifications, and domain knowledge, as well as the precise nature of the relationships among them. They establish minimum standards for what information should be represented in a requirements language. They also make it possible to determine exactly what it means for requirements and engineering to be successfully completed.",
    "cited_by_count": 739,
    "openalex_id": "https://openalex.org/W2113435553",
    "type": "article"
  },
  {
    "title": "A safe, efficient regression test selection technique",
    "doi": "https://doi.org/10.1145/248233.248262",
    "publication_date": "1997-04-01",
    "publication_year": 1997,
    "authors": "Gregg Rothermel; Mary Jean Harrold",
    "corresponding_authors": "",
    "abstract": "Regression testing is an expensive but necessary maintenance activity performed on modified software to provide confidence that changes are correct and do not adversely affect other portions of the softwore. A regression test selection technique choses, from an existing test set, thests that are deemed necessary to validate modified software. We present a new technique for regression test selection. Our algorithms construct control flow graphs for a precedure or program and its modified version and use these graphs to select tests that execute changed code from the original test suite. We prove that, under certain conditions, the set of tests our technique selects includes every test from the original test suite that con expose faults in the modified procedfdure or program. Under these conditions our algorithms are safe . Moreover, although our algorithms may select some tests that cannot expose faults, they are at lease as precise as other safe regression test selection algorithms. Unlike many other regression test selection algorithms, our algorithms handle all language constructs and all types of program modifications. We have implemented our algorithms; initial empirical studies indicate that our technique can significantly reduce the cost of regression testing modified software.",
    "cited_by_count": 711,
    "openalex_id": "https://openalex.org/W2020538887",
    "type": "article"
  },
  {
    "title": "A methodology for controlling the size of a test suite",
    "doi": "https://doi.org/10.1145/152388.152391",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Mary Jean Harrold; Rajiv Gupta; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "This paper presents a technique to select a representative set of test cases from a test suite that provides the same coverage as the entire test suite. This selection is performed by identifying, and then eliminating, the redundant and obsolete test cases in the test suite. The representative set replaces the original test suite and thus, potentially produces a smaller test suite. The representative set can also be used to identify those test cases that should be rerun to test the program after it has been changed. Our technique is independent of the testing methodology and only requires an association between a testing requirement and the test cases that satisfy the requirement. We illustrate the technique using the data flow testing methodology. The reduction that is possible with our technique is illustrated by experimental results.",
    "cited_by_count": 679,
    "openalex_id": "https://openalex.org/W1998393968",
    "type": "article"
  },
  {
    "title": "An experimental determination of sufficient mutant operators",
    "doi": "https://doi.org/10.1145/227607.227610",
    "publication_date": "1996-04-01",
    "publication_year": 1996,
    "authors": "A. Jefferson Offutt; Ammei Lee; Gregg Rothermel; Roland H. Untch; Christian Zapf",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on An experimental determination of sufficient mutant operators Authors: A. Jefferson Offutt George Mason University George Mason UniversityView Profile , Ammei Lee George Mason University George Mason UniversityView Profile , Gregg Rothermel Clemson University Clemson UniversityView Profile , Roland H. Untch Middle Tennessee State University Middle Tennessee State UniversityView Profile , Christian Zapf Siemens Corporation Siemens CorporationView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 5Issue 2April 1996 pp 99–118https://doi.org/10.1145/227607.227610Online:01 April 1996Publication History 447citation1,917DownloadsMetricsTotal Citations447Total Downloads1,917Last 12 Months159Last 6 weeks22 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 651,
    "openalex_id": "https://openalex.org/W2121084350",
    "type": "article"
  },
  {
    "title": "Automated consistency checking of requirements specifications",
    "doi": "https://doi.org/10.1145/234426.234431",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Constance Heitmeyer; Ralph D. Jeffords; B. Labaw",
    "corresponding_authors": "",
    "abstract": "This article describes a formal analysis technique, called consistency checking , for automatic detection of errors, such as type errors, nondeterminism, missing cases, and circular definitions, in requirements specifications. The technique is designed to analyze requirements specifications expressed in the SCR (Software Cost Reduction) tabular notation. As background, the SCR approach to specifying requirements is reviewed. To provide a formal semantics for the SCR notation and a foundation for consistency checking, a formal requirements model is introduced; the model represents a software system as a finite-state automation which produces externally visible outputs in response to changes in monitored environmental quantities. Results of two experiments are presented which evaluated the utility and scalability of our technique for consistency checking in real-world avionics application. The role of consistency checking during the requirements phase of software development is discussed.",
    "cited_by_count": 609,
    "openalex_id": "https://openalex.org/W2125708466",
    "type": "article"
  },
  {
    "title": "The design and implementation of hierarchical software systems with reusable components",
    "doi": "https://doi.org/10.1145/136586.136587",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Don Batory; Sean O’Malley",
    "corresponding_authors": "",
    "abstract": "We present a domain-independent model of hierarchical software system design and construction that is based on interchangeable software components and large-scale reuse. The model unifies the conceptualizations of two independent projects, Genesis and Avoca, that are successful examples of software component/building-block technologies and domain modeling. Building-block technologies exploit large-scale reuse, rely on open architecture software, and elevate the granularity of programming to the subsystem level. Domain modeling formalizes the similarities and differences among systems of a domain. We believe our model is a blueprint for achieving software component technologies in many domains.",
    "cited_by_count": 589,
    "openalex_id": "https://openalex.org/W2023546887",
    "type": "article"
  },
  {
    "title": "Protecting privacy using the decentralized label model",
    "doi": "https://doi.org/10.1145/363516.363526",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Andrew C. Myers; Barbara Liskov",
    "corresponding_authors": "",
    "abstract": "Stronger protection is needed for the confidentiality and integrity of data, because programs containing untrusted code are the rule rather than the exception. Information flow control allows the enforcement of end-to-end security policies, but has been difficult to put into practice. This article describes the decentralized label model, a new label model for control of information flow in systems with mutual distrust and decentralized authority. The model improves on existing multilevel security models by allowing users to declassify information in a decentralized way, and by improving support for fine-grained data sharing. It supports static program analysis of information flow, so that programs can be certified to permit only acceptable information flows, while largely avoiding the overhead of run-time checking. The article introduces the language Jif, an extension to Java that provides static checking of information flow using the decentralized label model.",
    "cited_by_count": 572,
    "openalex_id": "https://openalex.org/W2154564703",
    "type": "article"
  },
  {
    "title": "Runtime Verification for LTL and TLTL",
    "doi": "https://doi.org/10.1145/2000799.2000800",
    "publication_date": "2011-09-01",
    "publication_year": 2011,
    "authors": "Andreas Bauer; Martin Leucker; Christian Schallhart",
    "corresponding_authors": "",
    "abstract": "This article studies runtime verification of properties expressed either in lineartime temporal logic (LTL) or timed lineartime temporal logic (TLTL). It classifies runtime verification in identifying its distinguishing features to model checking and testing, respectively. It introduces a three-valued semantics (with truth values true, false, inconclusive ) as an adequate interpretation as to whether a partial observation of a running system meets an LTL or TLTL property. For LTL, a conceptually simple monitor generation procedure is given, which is optimal in two respects: First, the size of the generated deterministic monitor is minimal , and, second, the monitor identifies a continuously monitored trace as either satisfying or falsifying a property as early as possible . The feasibility of the developed methodology is demontrated using a collection of real-world temporal logic specifications. Moreover, the presented approach is related to the properties monitorable in general and is compared to existing concepts in the literature. It is shown that the set of monitorable properties does not only encompass the safety and cosafety properties but is strictly larger. For TLTL, the same road map is followed by first defining a three-valued semantics. The corresponding construction of a timed monitor is more involved, yet, as shown, possible.",
    "cited_by_count": 533,
    "openalex_id": "https://openalex.org/W2027769154",
    "type": "article"
  },
  {
    "title": "Specification matching of software components",
    "doi": "https://doi.org/10.1145/261640.261641",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "Amy Moormann Zaremski; Jeannette M. Wing",
    "corresponding_authors": "",
    "abstract": "Specification matching is a way to compare two software components, based on descriptions of the component's behaviors. In the context of software reuse and library retrieval, it can help determine whether one component can be substituted for another or how one can be modified to fit the requirements of the other. In the context of object-oriented programming, it can help determine when one type is a behavioral subtype of another. We use formal specifications to describe the behavior of software components and, hence, to determine whether two components match. We give precise definitions of not just exact match, but, more relevantly, various flavors of relaxed match. These definitions capture the notions of generalization, specialization, and substitutability of software components. Since our formal specifications are pre- and postconditions written as predicates in first-order logic, we rely on theorem proving to determine match and mismatch. We give examples from our implementation of specification matching using the Larch Prover.",
    "cited_by_count": 522,
    "openalex_id": "https://openalex.org/W1975255815",
    "type": "article"
  },
  {
    "title": "A model for spectra-based software diagnosis",
    "doi": "https://doi.org/10.1145/2000791.2000795",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Lee Naish; Hua Jie Lee; Kotagiri Ramamohanarao",
    "corresponding_authors": "",
    "abstract": "This article presents an improved approach to assist diagnosis of failures in software (fault localisation) by ranking program statements or blocks in accordance with to how likely they are to be buggy. We present a very simple single-bug program to model the problem. By examining different possible execution paths through this model program over a number of test cases, the effectiveness of different proposed spectral ranking methods can be evaluated in idealised conditions. The results are remarkably consistent to those arrived at empirically using the Siemens test suite and Space benchmarks. The model also helps identify groups of metrics that are equivalent for ranking. Due to the simplicity of the model, an optimal ranking method can be devised. This new method out-performs previously proposed methods for the model program, the Siemens test suite and Space. It also helps provide insight into other ranking methods.",
    "cited_by_count": 488,
    "openalex_id": "https://openalex.org/W2010833880",
    "type": "article"
  },
  {
    "title": "Model driven security",
    "doi": "https://doi.org/10.1145/1125808.1125810",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "David Basin; Jürgen Doser; Torsten Lodderstedt",
    "corresponding_authors": "",
    "abstract": "We present a new approach to building secure systems. In our approach, which we call Model Driven Security, designers specify system models along with their security requirements and use tools to automatically generate system architectures from the models, including complete, configured access control infrastructures. Rather than fixing one particular modeling language for this process, we propose a general schema for constructing such languages that combines languages for modeling systems with languages for modeling security. We present several instances of this schema that combine (both syntactically and semantically) different UML modeling languages with a security modeling language for formalizing access control requirements. From models in the combined languages, we automatically generate access control infrastructures for server-based applications, built from declarative and programmatic access control mechanisms. The modeling languages and generation process are semantically well-founded and are based on an extension of Role-Based Access Control. We have implemented this approach in a UML-based CASE-tool and report on experiments.",
    "cited_by_count": 483,
    "openalex_id": "https://openalex.org/W2010173096",
    "type": "article"
  },
  {
    "title": "Parameterized object sensitivity for points-to analysis for Java",
    "doi": "https://doi.org/10.1145/1044834.1044835",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Ana Milanova; Atanas Rountev; Barbara G. Ryder",
    "corresponding_authors": "",
    "abstract": "The goal of points-to analysis for Java is to determine the set of objects pointed to by a reference variable or a reference object field. We present object sensitivity , a new form of context sensitivity for flow-insensitive points-to analysis for Java. The key idea of our approach is to analyze a method separately for each of the object names that represent run-time objects on which this method may be invoked. To ensure flexibility and practicality, we propose a parameterization framework that allows analysis designers to control the tradeoffs between cost and precision in the object-sensitive analysis. Side-effect analysis determines the memory locations that may be modified by the execution of a program statement. Def-use analysis identifies pairs of statements that set the value of a memory location and subsequently use that value. The information computed by such analyses has a wide variety of uses in compilers and software tools. This work proposes new versions of these analyses that are based on object-sensitive points-to analysis.We have implemented two instantiations of our parameterized object-sensitive points-to analysis. On a set of 23 Java programs, our experiments show that these analyses have comparable cost to a context-insensitive points-to analysis for Java which is based on Andersen's analysis for C. Our results also show that object sensitivity significantly improves the precision of side-effect analysis and call graph construction, compared to (1) context-insensitive analysis, and (2) context-sensitive points-to analysis that models context using the invoking call site. These experiments demonstrate that object-sensitive analyses can achieve substantial precision improvement, while at the same time remaining efficient and practical.",
    "cited_by_count": 433,
    "openalex_id": "https://openalex.org/W2162544703",
    "type": "article"
  },
  {
    "title": "Investigations of the software testing coupling effect",
    "doi": "https://doi.org/10.1145/125489.125473",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "A. Jefferson Offutt",
    "corresponding_authors": "A. Jefferson Offutt",
    "abstract": "Fault-based testing strategies test software by focusing on specific, common types of faults. The coupling effect hypothesizes that test data sets that detect simple types of faults are sensitive enough to detect more complex types of faults. This paper describes empirical investigations into the coupling effect over a specific class of software faults. All of the results from this investigation support the validity of the coupling effect. The major conclusion from this investigation is the fact that by explicitly testing for simple faults, we are also implicitly testing for more complicated faults, giving us confidence that fault-based testing is an effective way to test software.",
    "cited_by_count": 382,
    "openalex_id": "https://openalex.org/W1996596150",
    "type": "article"
  },
  {
    "title": "Recovering traceability links in software artifact management systems using information retrieval methods",
    "doi": "https://doi.org/10.1145/1276933.1276934",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Andrea De Lucia; Fausto Fasano; Rocco Oliveto; Genoveffa Tortora",
    "corresponding_authors": "",
    "abstract": "The main drawback of existing software artifact management systems is the lack of automatic or semi-automatic traceability link generation and maintenance. We have improved an artifact management system with a traceability recovery tool based on Latent Semantic Indexing (LSI), an information retrieval technique. We have assessed LSI to identify strengths and limitations of using information retrieval techniques for traceability recovery and devised the need for an incremental approach. The method and the tool have been evaluated during the development of seventeen software projects involving about 150 students. We observed that although tools based on information retrieval provide a useful support for the identification of traceability links during software development, they are still far to support a complete semi-automatic recovery of all links. The results of our experience have also shown that such tools can help to identify quality problems in the textual description of traced artifacts.",
    "cited_by_count": 354,
    "openalex_id": "https://openalex.org/W2138378644",
    "type": "article"
  },
  {
    "title": "A theoretical analysis of the risk evaluation formulas for spectrum-based fault localization",
    "doi": "https://doi.org/10.1145/2522920.2522924",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Xiaoyuan Xie; Tsong Yueh Chen; Fei‐Ching Kuo; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "An important research area of Spectrum-Based Fault Localization (SBFL) is the effectiveness of risk evaluation formulas. Most previous studies have adopted an empirical approach, which can hardly be considered as sufficiently comprehensive because of the huge number of combinations of various factors in SBFL. Though some studies aimed at overcoming the limitations of the empirical approach, none of them has provided a completely satisfactory solution. Therefore, we provide a theoretical investigation on the effectiveness of risk evaluation formulas. We define two types of relations between formulas, namely, equivalent and better. To identify the relations between formulas, we develop an innovative framework for the theoretical investigation. Our framework is based on the concept that the determinant for the effectiveness of a formula is the number of statements with risk values higher than the risk value of the faulty statement. We group all program statements into three disjoint sets with risk values higher than, equal to, and lower than the risk value of the faulty statement, respectively. For different formulas, the sizes of their sets are compared using the notion of subset. We use this framework to identify the maximal formulas which should be the only formulas to be used in SBFL.",
    "cited_by_count": 342,
    "openalex_id": "https://openalex.org/W2070249305",
    "type": "article"
  },
  {
    "title": "An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3340544",
    "publication_date": "2019-09-02",
    "publication_year": 2019,
    "authors": "Michele Tufano; Cody Watson; Gabriele Bavota; Massimiliano Di Penta; Martin White; Denys Poshyvanyk",
    "corresponding_authors": "",
    "abstract": "Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9--50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.",
    "cited_by_count": 302,
    "openalex_id": "https://openalex.org/W2972082064",
    "type": "article"
  },
  {
    "title": "Reducing the effort of bug report triage",
    "doi": "https://doi.org/10.1145/2000791.2000794",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "John Anvik; Gail C. Murphy",
    "corresponding_authors": "",
    "abstract": "A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process. To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70% and 98% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15% precision of hand-tuned developer recommenders.",
    "cited_by_count": 278,
    "openalex_id": "https://openalex.org/W2113351233",
    "type": "article"
  },
  {
    "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3695988",
    "publication_date": "2024-09-20",
    "publication_year": 2024,
    "authors": "Xinyi Hou; Yanjie Zhao; Yue Liu; Zhou Yang; Kailong Wang; Li Li; Xiapu Luo; David Lo; John Grundy; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE_SLR .",
    "cited_by_count": 261,
    "openalex_id": "https://openalex.org/W4402665833",
    "type": "article"
  },
  {
    "title": "Many-Objective Software Remodularization Using NSGA-III",
    "doi": "https://doi.org/10.1145/2729974",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Mohamed Wiem Mkaouer; Marouane Kessentini; Adnan Shaout; Patrice Koligheu; Slim Bechikh; Kalyanmoy Deb; Ali Ouni",
    "corresponding_authors": "",
    "abstract": "Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this article, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and reuse the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers.",
    "cited_by_count": 250,
    "openalex_id": "https://openalex.org/W2250322698",
    "type": "article"
  },
  {
    "title": "The ABC of Software Engineering Research",
    "doi": "https://doi.org/10.1145/3241743",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "Klaas-Jan Stol; Brian Fitzgerald",
    "corresponding_authors": "",
    "abstract": "A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.",
    "cited_by_count": 234,
    "openalex_id": "https://openalex.org/W2890801208",
    "type": "article"
  },
  {
    "title": "Software Engineering for AI-Based Systems: A Survey",
    "doi": "https://doi.org/10.1145/3487043",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Silverio Martínez‐Fernández; Justus Bogner; Xavier Franch; Marc Oriol; Julien Siebert; Adam Trendowicz; Anna Maria Vollmer; Stefan Wagner",
    "corresponding_authors": "",
    "abstract": "AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image- and speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state of the art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.",
    "cited_by_count": 227,
    "openalex_id": "https://openalex.org/W4220891756",
    "type": "article"
  },
  {
    "title": "DeepWukong",
    "doi": "https://doi.org/10.1145/3436877",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Xiao Cheng; Haoyu Wang; Jiayi Hua; Guoai Xu; Yulei Sui",
    "corresponding_authors": "",
    "abstract": "Static bug detection has shown its effectiveness in detecting well-defined memory errors, e.g., memory leaks, buffer overflows, and null dereference. However, modern software systems have a wide variety of vulnerabilities. These vulnerabilities are extremely complicated with sophisticated programming logic, and these bugs are often caused by different bad programming practices, challenging existing bug detection solutions. It is hard and labor-intensive to develop precise and efficient static analysis solutions for different types of vulnerabilities, particularly for those that may not have a clear specification as the traditional well-defined vulnerabilities. This article presents D eep W ukong , a new deep-learning-based embedding approach to static detection of software vulnerabilities for C/C++ programs. Our approach makes a new attempt by leveraging advanced recent graph neural networks to embed code fragments in a compact and low-dimensional representation, producing a new code representation that preserves high-level programming logic (in the form of control- and data-flows) together with the natural language information of a program. Our evaluation studies the top 10 most common C/C++ vulnerabilities during the past 3 years. We have conducted our experiments using 105,428 real-world programs by comparing our approach with four well-known traditional static vulnerability detectors and three state-of-the-art deep-learning-based approaches. The experimental results demonstrate the effectiveness of our research and have shed light on the promising direction of combining program analysis with deep learning techniques to address the general static code analysis challenges.",
    "cited_by_count": 215,
    "openalex_id": "https://openalex.org/W3161071537",
    "type": "article"
  },
  {
    "title": "A Large-Scale Evaluation of Automated Unit Test Generation Using EvoSuite",
    "doi": "https://doi.org/10.1145/2685612",
    "publication_date": "2014-12-23",
    "publication_year": 2014,
    "authors": "Gordon Fraser; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "Research on software testing produces many innovative automated techniques, but because software testing is by necessity incomplete and approximate, any new technique faces the challenge of an empirical assessment. In the past, we have demonstrated scientific advance in automated unit test generation with the E VO S UITE tool by evaluating it on manually selected open-source projects or examples that represent a particular problem addressed by the underlying technique. However, demonstrating scientific advance is not necessarily the same as demonstrating practical value; even if VO S UITE worked well on the software projects we selected for evaluation, it might not scale up to the complexity of real systems. Ideally, one would use large “real-world” software systems to minimize the threats to external validity when evaluating research tools. However, neither choosing such software systems nor applying research prototypes to them are trivial tasks. In this article we present the results of a large experiment in unit test generation using the VO S UITE tool on 100 randomly chosen open-source projects, the 10 most popular open-source projects according to the SourceForge Web site, seven industrial projects, and 11 automatically generated software projects. The study confirms that VO S UITE can achieve good levels of branch coverage (on average, 71% per class) in practice. However, the study also exemplifies how the choice of software systems for an empirical study can influence the results of the experiments, which can serve to inform researchers to make more conscious choices in the selection of software system subjects. Furthermore, our experiments demonstrate how practical limitations interfere with scientific advances, branch coverage on an unbiased sample is affected by predominant environmental dependencies. The surprisingly large effect of such practical engineering problems in unit testing will hopefully lead to a larger appreciation of work in this area, thus supporting transfer of knowledge from software testing research to practice.",
    "cited_by_count": 208,
    "openalex_id": "https://openalex.org/W2147002252",
    "type": "article"
  },
  {
    "title": "RESTful API Automated Test Case Generation with EvoMaster",
    "doi": "https://doi.org/10.1145/3293455",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Andrea Arcuri",
    "corresponding_authors": "Andrea Arcuri",
    "abstract": "RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called E vo M aster , which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W2997401484",
    "type": "article"
  },
  {
    "title": "A Tale of Two Cities: Software Developers Working from Home during the COVID-19 Pandemic",
    "doi": "https://doi.org/10.1145/3487567",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Denae Ford; Margaret‐Anne Storey; Thomas Zimmermann; Christian Bird; Sonia Jaffe; Chandra Maddila; Jenna Butler; Brian Houck; Nachiappan Nagappan",
    "corresponding_authors": "",
    "abstract": "The COVID-19 pandemic has shaken the world to its core and has provoked an overnight exodus of developers who normally worked in an office setting to working from home. The magnitude of this shift and the factors that have accompanied this new unplanned work setting go beyond what the software engineering community has previously understood to be remote work. To find out how developers and their productivity were affected, we distributed two surveys (with a combined total of 3,634 responses that answered all required questions) weeks apart to understand the presence and prevalence of the benefits, challenges, and opportunities to improve this special circumstance of remote work. From our thematic qualitative analysis and statistical quantitative analysis, we find that there is a dichotomy of developer experiences influenced by many different factors (that for some are a benefit, while for others a challenge). For example, a benefit for some was being close to family members but for others having family members share their working space and interrupting their focus, was a challenge. Our surveys led to powerful narratives from respondents and revealed the scale at which these experiences exist to provide insights as to how the future of (pandemic) remote work can evolve.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W3081249378",
    "type": "article"
  },
  {
    "title": "How Far We Have Progressed in the Journey? An Examination of Cross-Project Defect Prediction",
    "doi": "https://doi.org/10.1145/3183339",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Yuming Zhou; Yibiao Yang; Hongmin Lu; Lin Chen; Yanhui Li; Yangyang Zhao; Junyan Qian; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "Background. Recent years have seen an increasing interest in cross-project defect prediction (CPDP), which aims to apply defect prediction models built on source projects to a target project. Currently, a variety of (complex) CPDP models have been proposed with a promising prediction performance. Problem. Most, if not all, of the existing CPDP models are not compared against those simple module size models that are easy to implement and have shown a good performance in defect prediction in the literature. Objective. We aim to investigate how far we have really progressed in the journey by comparing the performance in defect prediction between the existing CPDP models and simple module size models. Method. We first use module size in the target project to build two simple defect prediction models, ManualDown and ManualUp, which do not require any training data from source projects. ManualDown considers a larger module as more defect-prone, while ManualUp considers a smaller module as more defect-prone. Then, we take the following measures to ensure a fair comparison on the performance in defect prediction between the existing CPDP models and the simple module size models: using the same publicly available data sets, using the same performance indicators, and using the prediction performance reported in the original cross-project defect prediction studies. Result. The simple module size models have a prediction performance comparable or even superior to most of the existing CPDP models in the literature, including many newly proposed models. Conclusion. The results caution us that, if the prediction performance is the goal, the real progress in CPDP is not being achieved as it might have been envisaged. We hence recommend that future studies should include ManualDown/ManualUp as the baseline models for comparison when developing new CPDP models to predict defects in a complete target project.",
    "cited_by_count": 176,
    "openalex_id": "https://openalex.org/W2802138742",
    "type": "article"
  },
  {
    "title": "An Empirical Study of the Impact of Hyperparameter Tuning and Model Optimization on the Performance Properties of Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3506695",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Lizhi Liao; Heng Li; Weiyi Shang; Lei Ma",
    "corresponding_authors": "",
    "abstract": "Deep neural network (DNN) models typically have many hyperparameters that can be configured to achieve optimal performance on a particular dataset. Practitioners usually tune the hyperparameters of their DNN models by training a number of trial models with different configurations of the hyperparameters, to find the optimal hyperparameter configuration that maximizes the training accuracy or minimizes the training loss. As such hyperparameter tuning usually focuses on the model accuracy or the loss function, it is not clear and remains under-explored how the process impacts other performance properties of DNN models, such as inference latency and model size. On the other hand, standard DNN models are often large in size and computing-intensive, prohibiting them from being directly deployed in resource-bounded environments such as mobile devices and Internet of Things (IoT) devices. To tackle this problem, various model optimization techniques (e.g., pruning or quantization) are proposed to make DNN models smaller and less computing-intensive so that they are better suited for resource-bounded environments. However, it is neither clear how the model optimization techniques impact other performance properties of DNN models such as inference latency and battery consumption, nor how the model optimization techniques impact the effect of hyperparameter tuning (i.e., the compounding effect). Therefore, in this paper, we perform a comprehensive study on four representative and widely-adopted DNN models, i.e., CNN image classification , Resnet-50 , CNN text classification , and LSTM sentiment classification , to investigate how different DNN model hyperparameters affect the standard DNN models, as well as how the hyperparameter tuning combined with model optimization affect the optimized DNN models, in terms of various performance properties (e.g., inference latency or battery consumption). Our empirical results indicate that tuning specific hyperparameters has heterogeneous impact on the performance of DNN models across different models and different performance properties. In particular, although the top tuned DNN models usually have very similar accuracy, they may have significantly different performance in terms of other aspects (e.g., inference latency). We also observe that model optimization has a confounding effect on the impact of hyperparameters on DNN model performance. For example, two sets of hyperparameters may result in standard models with similar performance but their performance may become significantly different after they are optimized and deployed on the mobile device. Our findings highlight that practitioners can benefit from paying attention to a variety of performance properties and the confounding effect of model optimization when tuning and optimizing their DNN models.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W4210647952",
    "type": "article"
  },
  {
    "title": "In-IDE Code Generation from Natural Language: Promise and Challenges",
    "doi": "https://doi.org/10.1145/3487569",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Frank F. Xu; Bogdan Vasilescu; Graham Neubig",
    "corresponding_authors": "",
    "abstract": "A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code , especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries , but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, “At the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?” To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W3123221944",
    "type": "article"
  },
  {
    "title": "A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research",
    "doi": "https://doi.org/10.1145/3485275",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Cody Watson; Nathan Cooper; David Nader Palacio; Kevin Moran; Denys Poshyvanyk",
    "corresponding_authors": "",
    "abstract": "An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE &amp; DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23 unique SE tasks. We center our analysis around the components of learning , a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W3084812981",
    "type": "article"
  },
  {
    "title": "Defining a Knowledge Graph Development Process Through a Systematic Review",
    "doi": "https://doi.org/10.1145/3522586",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Gytė Tamašauskaitė; Paul Groth",
    "corresponding_authors": "",
    "abstract": "Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W4225123286",
    "type": "review"
  },
  {
    "title": "Navigating the Complexity of Generative AI Adoption in Software Engineering",
    "doi": "https://doi.org/10.1145/3652154",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Daniel Russo",
    "corresponding_authors": "Daniel Russo",
    "abstract": "This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares-Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W4393278146",
    "type": "article"
  },
  {
    "title": "Rise of the Planet of Serverless Computing: A Systematic Review",
    "doi": "https://doi.org/10.1145/3579643",
    "publication_date": "2023-01-09",
    "publication_year": 2023,
    "authors": "Jinfeng Wen; Zhenpeng Chen; Xin Jin; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W4313889498",
    "type": "review"
  },
  {
    "title": "A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers",
    "doi": "https://doi.org/10.1145/3583561",
    "publication_date": "2023-02-09",
    "publication_year": 2023,
    "authors": "Zhenpeng Chen; Jie M. Zhang; Federica Sarro; Mark Harman",
    "corresponding_authors": "",
    "abstract": "Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical study of 17 representative bias mitigation methods for Machine Learning (ML) classifiers, evaluated with 11 ML performance metrics (e.g., accuracy), 4 fairness metrics, and 20 types of fairness-performance tradeoff assessment, applied to 8 widely-adopted software decision tasks. The empirical coverage is much more comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance tradeoff measures compared to previous work on this important software property. We find that (1) the bias mitigation methods significantly decrease ML performance in 53% of the studied scenarios (ranging between 42%∼66% according to different ML performance metrics); (2) the bias mitigation methods significantly improve fairness measured by the 4 used metrics in 46% of all the scenarios (ranging between 24%∼59% according to different fairness metrics); (3) the bias mitigation methods even lead to decrease in both fairness and ML performance in 25% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, the choice of protected attributes, and the set of metrics used to assess fairness and ML performance; (5) there is no bias mitigation method that can achieve the best tradeoff in all the scenarios. The best method that we find outperforms other methods in 30% of the scenarios. Researchers and practitioners need to choose the bias mitigation method best suited to their intended application scenario(s).",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W4319736227",
    "type": "article"
  },
  {
    "title": "A Survey on Automated Driving System Testing: Landscapes and Trends",
    "doi": "https://doi.org/10.1145/3579642",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Shuncheng Tang; Zhenya Zhang; Yi Zhang; Jixiang Zhou; Yan Guo; Shuang Liu; Shengjian Guo; Yan‐Fu Li; Lei Ma; Yinxing Xue; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Automated Driving Systems ( ADS ) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W4319964781",
    "type": "article"
  },
  {
    "title": "Self-collaboration Code Generation via ChatGPT",
    "doi": "https://doi.org/10.1145/3672459",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Yihong Dong; Xue Jiang; Zhi Jin; Ge Li",
    "corresponding_authors": "",
    "abstract": "Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct “experts,” each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9–47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W4399557965",
    "type": "article"
  },
  {
    "title": "An Empirical Study of the Non-determinism of ChatGPT in Code Generation",
    "doi": "https://doi.org/10.1145/3697010",
    "publication_date": "2024-09-26",
    "publication_year": 2024,
    "authors": "Shuyin Ouyang; Jie M. Zhang; Mark Harman; Meng Wang",
    "corresponding_authors": "",
    "abstract": "There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different code for the same prompt. Such non-determinism affects the correctness and consistency of the generated code, undermines developers’ trust in LLMs, and yields low reproducibility in LLM-based papers. Nevertheless, there is no work investigating how serious this non-determinism threat is. To fill this gap, this paper conducts an empirical study on the non-determinism of ChatGPT in code generation. We chose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems across three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval) with three aspects of code similarities: semantic similarity, syntactic similarity, and structural similarity. Our results reveal that ChatGPT exhibits a high degree of non-determinism under the default setting: the ratio of coding tasks with zero equal test output across different requests is 75.76%, 51.00%, and 47.56% for three different code generation datasets (i.e., CodeContests, APPS, and HumanEval), respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration ( temperature =1). In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W4402860127",
    "type": "article"
  },
  {
    "title": "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues",
    "doi": "https://doi.org/10.1145/3643674",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Yue Liu; Thanh Le-Cong; Ratnadira Widyasari; Chakkrit Tantithamthavorn; Li Li; Xuan-Bach D. Le; David Lo",
    "corresponding_authors": "",
    "abstract": "Since its introduction in November 2022, ChatGPT has rapidly gained popularity due to its remarkable ability in language understanding and human-like responses. ChatGPT, based on GPT-3.5 architecture, has shown great promise for revolutionizing various research fields, including code generation. However, the reliability and quality of code generated by ChatGPT remain unexplored, raising concerns about potential risks associated with the widespread use of ChatGPT-driven code generation. In this article, we systematically study the quality of 4,066 ChatGPT-generated programs of code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is threefold. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,756 programs are deemed correct, 1,082 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,930 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT’s self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of artificial intelligence models such as ChatGPT.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W4391282616",
    "type": "article"
  },
  {
    "title": "Structured Chain-of-Thought Prompting for Code Generation",
    "doi": "https://doi.org/10.1145/3690635",
    "publication_date": "2024-08-29",
    "publication_year": 2024,
    "authors": "Jia Li; Ge Li; Yongmin Li; Zhi Jin",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have shown impressive abilities in code generation. Chain-of-Thought (CoT) prompting is the state-of-the-art approach to utilizing LLMs. CoT prompting asks LLMs first to generate CoTs ( i.e., intermediate natural language reasoning steps) and then output the code. However, the accuracy of CoT prompting still can not satisfy practical applications. For example, gpt-3.5-turbo with CoT prompting only achieves 53.29% Pass@1 in HumanEval. In this paper, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation named SCoT prompting. Our motivation is that human developers follow structured programming. Developers use three programming structures ( i.e., sequential, branch, and loop) to design and implement structured programs. Thus, we ask LLMs to use three programming structures to generate SCoTs (structured reasoning steps) before outputting the final code. Compared to CoT prompting, SCoT prompting explicitly introduces programming structures and unlocks the structured programming thinking of LLMs. We apply SCoT prompting to two LLMs ( i.e., gpt-4-turbo, gpt-3.5-turbo, and DeepSeek Coder-Instruct-{1.3B, 6.7B, 33B}) and evaluate it on three benchmarks ( i.e., HumanEval, MBPP, and MBCPP). SCoT prompting outperforms CoT prompting by up to 13.79% in Pass@1. SCoT prompting is robust to examples and achieves substantial improvements. The human evaluation also shows human developers prefer programs from SCoT prompting.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4401996408",
    "type": "article"
  },
  {
    "title": "LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead",
    "doi": "https://doi.org/10.1145/3712003",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Junda He; Christoph Treude; David Lo",
    "corresponding_authors": "",
    "abstract": "Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This paper explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this paper, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4406325768",
    "type": "article"
  },
  {
    "title": "Quantum Software Engineering: Roadmap and Challenges Ahead",
    "doi": "https://doi.org/10.1145/3712002",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Juan M. Murillo; José Garcí­a-Alonso; Enrique Moguel; Johanna Barzen; Frank Leymann; Shaukat Ali; Tao Yue; Paolo Arcaini; Ricardo Pérez‐Castillo; Ignacio García‐Rodríguez de Guzmán; Mario Piattini; Antonio Ruiz–Cortés; Antonio Brogi; Jianjun Zhao; Andriy Miranskyy; Manuel Wimmer",
    "corresponding_authors": "",
    "abstract": "As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective —key qualities of any industry-grade software— mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyse in depth the current state of quantum software engineering research. From this analysis, the key areas of quantum software engineering are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing Quantum Software Engineering.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4406957844",
    "type": "article"
  },
  {
    "title": "Research Directions in Software Supply Chain Security",
    "doi": "https://doi.org/10.1145/3714464",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Laurie Williams; Giacomo Benedetti; Sivana Hamer; Ranindya Paramitha; Ishtiaq Rahman; Mahzabin Tamanna; Greg Tystahl; Nusrat Zahan; Patrick Morrison; Yasemin Acar; Michel Cukier; Christian Kästner; Alexandros Kapravelos; Dominik Wermke; William Enck",
    "corresponding_authors": "",
    "abstract": "Reusable software libraries, frameworks, and components, such as those provided by open-source ecosystems and third-party suppliers, accelerate digital innovation. However, recent years have shown almost exponential growth in attackers leveraging these software artifacts to launch software supply chain attacks. Past well-known software supply chain attacks include the SolarWinds, log4j, and xz utils incidents. Supply chain attacks are considered to have three major attack vectors: through vulnerabilities and malware accidentally or intentionally injected into open-source and third-party dependencies/components/containers ; by infiltrating the build infrastructure during the build and deployment processes; and through targeted techniques aimed at the humans involved in software development, such as through social engineering. Plummeting trust in the software supply chain could decelerate digital innovation if the software industry reduces its use of open-source and third-party artifacts to reduce risks. This paper contains perspectives and knowledge obtained from intentional outreach with practitioners to understand their practical challenges and from extensive research efforts. We then provide an overview of current research efforts to secure the software supply chain. Finally, we propose a future research agenda to close software supply chain attack vectors and support the software industry.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4406866500",
    "type": "article"
  },
  {
    "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models",
    "doi": "https://doi.org/10.1145/3714461",
    "publication_date": "2025-01-22",
    "publication_year": 2025,
    "authors": "Martin Weyssow; Xin Zhou; Kisub Kim; David Lo; Houari Sahraoui",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner, i.e. , without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4406688098",
    "type": "article"
  },
  {
    "title": "<i>ContrastRepair</i> : Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs",
    "doi": "https://doi.org/10.1145/3719345",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Jiaolong Kong; Xiaofei Xie; Mingfei Cheng; Shangqing Liu; Xiaoning Du; Qi Guo",
    "corresponding_authors": "",
    "abstract": "Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this paper, we propose ContrastRepair , a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM. Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing such informative feedback, ContrastRepair enables the LLM to produce effective bug fixes. The implementation of ContrastRepair is based on the state-of-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated. We evaluate ContrastRepair on multiple benchmark datasets, including Defects4J, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair. For instance, among Defects4J 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4408124887",
    "type": "article"
  },
  {
    "title": "Modeling software architectures in the Unified Modeling Language",
    "doi": "https://doi.org/10.1145/504087.504088",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Nenad Medvidović; David S. Rosenblum; David Redmiles; Jason E. Robbins",
    "corresponding_authors": "",
    "abstract": "The Unified Modeling Language (UML) is a family of design notations that is rapidly becoming a de facto standard software design language. UML provides a variety of useful capabilities to the software designer, including multiple, interrelated design views, a semiformal semantics expressed as a UML meta model, and an associated language for expressing formal logic constraints on design elements. The primary goal of this work is an assessment of UML's expressive power for modeling software architectures in the manner in which a number of existing software architecture description languages (ADLs) model architectures. This paper presents two strategies for supporting architectural concerns within UML. One strategy involves using UML \"as is,\" while the other incorporates useful features of existing ADLs as UML extensions. We discuss the applicability, strengths, and weaknesses of the two strategies. The strategies are applied on three ADLs that, as a whole, represent a broad cross-section of present-day ADL capabilities. One conclusion of our work is that UML currently lacks support for capturing and exploiting certain architectural concerns whose importance has been demonstrated through the research and practice of software architectures. In particular, UML lacks direct support for modeling and exploiting architectural styles, explicit software connectors, and local and global architectural constraints.",
    "cited_by_count": 374,
    "openalex_id": "https://openalex.org/W2092431575",
    "type": "article"
  },
  {
    "title": "The chaining approach for software test data generation",
    "doi": "https://doi.org/10.1145/226155.226158",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Roger Ferguson; Bogdan Korel",
    "corresponding_authors": "",
    "abstract": "Software testing is very labor intensive and expensive and accounts for a significant portion of software system development cost. If the testing process could be automated, the cost of developing software could be significantly reduced. Test data generation in program testing is the process of identifying a set of test data that satisfies a selected testing criterion, such as statement coverage and branch coverage. In this article we present a chaining approach for automated software test data generation which builds on the current theory of execution-oriented test data generation. In the chaining approach, test data are derived based on the actual execution of the program under test. For many programs, the execution of the selected statement may require prior execution of some other statements. The existing methods of test data generation may not efficiently generate test data for these types of programs because they only use control flow information of a program during the search process. The chaining approach uses data dependence analysis to guide the search process, i.e., data dependence analysis automatically identifies statements that affect the execution of the selected statement. The chaining approach uses these statements to form a sequence of statements that is to be executed prior to the execution of the selected statement. The experiments have shown that the chaining approach may significantly improve the chances of finding test data as compared to the existing methods of automated test data generation.",
    "cited_by_count": 367,
    "openalex_id": "https://openalex.org/W2059942119",
    "type": "article"
  },
  {
    "title": "An empirical study of regression test selection techniques",
    "doi": "https://doi.org/10.1145/367008.367020",
    "publication_date": "2001-04-01",
    "publication_year": 2001,
    "authors": "Todd Graves; Mary Jean Harrold; Jungmin Kim; Adam Porter; Gregg Rothermel",
    "corresponding_authors": "",
    "abstract": "Regression testing is the process of validating modified software to detect whether new errors have been introduced into previously tested code and to provide confidence that modifications are correct. Since regression testing is an expensive process, researchers have proposed regression test selection techniques as a way to reduce some of this expense. These techniques attempt to reduce costs by selecting and running only a subset of the test cases in a program's existing test suite. Although there have been some analytical and empirical evaluations of individual techniques, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been reported in the literature. We conducted an experiment to examine the relative costs and benefits of several regression test selection techniques. The experiment examined five techniques for reusing test cases, focusing on their relative ablilities to reduce regression testing effort and uncover faults in modified programs. Our results highlight several differences between the techiques, and expose essential trade-offs that should be considered when choosing a technique for practical application.",
    "cited_by_count": 361,
    "openalex_id": "https://openalex.org/W2118542736",
    "type": "article"
  },
  {
    "title": "The ASTOOT approach to testing object-oriented programs",
    "doi": "https://doi.org/10.1145/192218.192221",
    "publication_date": "1994-04-01",
    "publication_year": 1994,
    "authors": "Roong-Ko Doong; Phyllis G. Frankl",
    "corresponding_authors": "",
    "abstract": "This article describes a new approach to the unit testing of object-oriented programs, a set of tools based on this approach, and two case studies. In this approach, each test case consists of a tuple of sequences of messages, along with tags indicating whether these sequences should put objects of the class under test into equivalent states and/or return objects that are in equivalent states. Tests are executed by sending the sequences to objects of the class under test, then invoking a user-supplied equivalence-checking mechanism. This approach allows for substantial automation of many aspects of testing, including test case generation, test driver generation, test execution, and test checking. Experimental prototypes of tools for test generation and test execution are described. The test generation tool requires the availability of an algebraic specification of the abstract data type being tested, but the test execution tool can be used when no formal specification is available. Using the test execution tools, case studies involving execution of tens of thousands of test cases, with various sequence lengths, parameters, and combinations of operations were performed. The relationships among likelihood of detecting an error and sequence length, range of parameters, and relative frequency of various operations were investigated for priority queue and sorted-list implementations having subtle errors. In each case, long sequences tended to be more likely to detect the error, provided that the range of parameters was sufficiently large and likelihood of detecting an error tended to increase up to a threshold value as the parameter range increased.",
    "cited_by_count": 333,
    "openalex_id": "https://openalex.org/W2170108788",
    "type": "article"
  },
  {
    "title": "Law-governed interaction",
    "doi": "https://doi.org/10.1145/352591.352592",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Naftaly H. Minsky; Victoria Ungureanu",
    "corresponding_authors": "",
    "abstract": "Software technology is undergoing a transition form monolithic systems, constructed according to a single overall design, into conglomerates of semiautonomous, heterogeneous, and independently designed subsystems, constructed and managed by different organizations, with little, if any, knowledge of each other. Among the problems inherent in such conglomerates, none is more serious than the difficulty to control the activities of the disparate agents operating in it, and the difficulty for such agents to coordinate their activities with each other. We argue that the nature of coordination and control required for such systems calls for the following principles to be satisfied: (1) coordination policies need to be enforced: (2) the enforcement needs to be decentralized; and (3) coordination policies need to be formulated explicitly—rather than being implicit in the code of the agents involved—and they should be enforced by means of a generic, broad spectrum mechanism; and (4) it should be possible to deploy and enforce a policy incrementally, without exacting any cost from agents and activities not subject to it. We describe a mechansim called law-governed interaction (LGI), currently implemented by the Moses toolkit, which has been designed to satisfy these principles. We show that LGI is at least as general as a conventional centralized coordination mechanism (CCM), and that it is more scalable, and generally more efficient, then CCM.",
    "cited_by_count": 325,
    "openalex_id": "https://openalex.org/W2068310911",
    "type": "article"
  },
  {
    "title": "Signature matching",
    "doi": "https://doi.org/10.1145/210134.210179",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Amy Moormann Zaremski; Jeannette M. Wing",
    "corresponding_authors": "",
    "abstract": "Signature matching is a method for organizing, navigating through, and retrieving from software libraries. We consider two kinds of software library components—functions and modules—and hence two kinds of matching—function matching and module matching. The signature of a function is simply its type; the signature of a module is a multiset of user-defined types and a multiset of function signatures. For both functions and modules, we consider not just exact match but also various flavors of relaxed match. We describe various applications of signature matching as a tool for using software libraries, inspired by the use of our implementation of a function signature matcher written in Standard ML.",
    "cited_by_count": 301,
    "openalex_id": "https://openalex.org/W2165871256",
    "type": "article"
  },
  {
    "title": "A meta-environment for generating programming environments",
    "doi": "https://doi.org/10.1145/151257.151260",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Paul Klint",
    "corresponding_authors": "Paul Klint",
    "abstract": "Over the last decade, considerable progress has been made in solving the problems of automatic generation of programming/development environments, given a formal definition of some programming or specification language. In most cases, research has focused on the functionality and efficiency of the generated environments, and, of course, these aspects will ultimately determine the acceptance of environment generators. However, only marginal attention has been paid to the development process of formal language definitions itself. Assuming that the quality of automatically generated environments will be satisfactory within a few years, the development costs of formal language definitions will then become the next limiting factor determining ultimate success and acceptance of environment generators. In this paper we describe the design and implementation of a meta-environment (a development environment for formal language definitions) based on the formalism ASF + SDF. This meta-environment is currently being implemented as part of the Centaur system and is, at least partly, obtained by applying environment generation techniques to the language definition formalism itself. A central problem is providing fully interactive editing of modular language definitions such that modifications made to the language definition during editing can be translated immediately to modifications in the programming environment generated from the original language definition. Therefore, some of the issues addressed are the treatment of formalisms with user-definable syntax and incremental program generation techniques.",
    "cited_by_count": 299,
    "openalex_id": "https://openalex.org/W2154940799",
    "type": "article"
  },
  {
    "title": "UML-B",
    "doi": "https://doi.org/10.1145/1125808.1125811",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Colin Snook; Michael Butler",
    "corresponding_authors": "",
    "abstract": "The emergence of the UML as a de facto standard for object-oriented modeling has been mirrored by the success of the B method as a practically useful formal modeling technique. The two notations have much to offer each other. The UML provides an accessible visualization of models facilitating communication of ideas but lacks formal precise semantics. B, on the other hand, has the precision to support animation and rigorous verification but requires significant effort in training to overcome the mathematical barrier that many practitioners perceive. We utilize a derivation of the B notation as an action and constraint language for the UML and define the semantics of UML entities via a translation into B. Through the UML-B profile we provide specializations of UML entities to support model refinement. The result is a formally precise variant of UML that can be used for refinement based, object-oriented behavioral modeling. The design of UML-B has been guided by industrial applications.",
    "cited_by_count": 296,
    "openalex_id": "https://openalex.org/W1985804037",
    "type": "article"
  },
  {
    "title": "Mixin layers",
    "doi": "https://doi.org/10.1145/505145.505148",
    "publication_date": "2002-04-01",
    "publication_year": 2002,
    "authors": "Yannis Smaragdakis; Don Batory",
    "corresponding_authors": "",
    "abstract": "A \"refinement\" is a functionality addition to a software project that can affect multiple dispersed implementation entities (functions, classes, etc.). In this paper, we examine large-scale refinements in terms of a fundamental object-oriented technique called collaboration-based design. We explain how collaborations can be expressed in existing programming languages or can be supported with new language constructs (which we have implemented as extensions to the Java language). We present a specific expression of large-scale refinements called mixin layers , and demonstrate how it overcomes the scalability difficulties that plagued prior work. We also show how we used mixin layers as the primary implementation technique for building an extensible Java compiler, JTS.",
    "cited_by_count": 296,
    "openalex_id": "https://openalex.org/W2140249393",
    "type": "article"
  },
  {
    "title": "Toward an engineering discipline for grammarware",
    "doi": "https://doi.org/10.1145/1072997.1073000",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Paul Klint; Ralf Lämmel; C. Verhoef",
    "corresponding_authors": "",
    "abstract": "Grammarware comprises grammars and all grammar-dependent software. The term grammar is meant here in the sense of all established grammar formalisms and grammar notations including context-free grammars, class dictionaries, and XML schemas as well as some forms of tree and graph grammars. The term grammar-dependent software refers to all software that involves grammar knowledge in an essential manner. Archetypal examples of grammar-dependent software are parsers, program converters, and XML document processors. Despite the pervasive role of grammars in software systems, the engineering aspects of grammarware are insufficiently understood. We lay out an agenda that is meant to promote research on increasing the productivity of grammarware development and on improving the quality of grammarware. To this end, we identify the problems with the current grammarware practices, the barriers that currently hamper research, and the promises of an engineering discipline for grammarware, its principles, and the research challenges that have to be addressed.",
    "cited_by_count": 270,
    "openalex_id": "https://openalex.org/W2014459598",
    "type": "article"
  },
  {
    "title": "Software process validation",
    "doi": "https://doi.org/10.1145/304399.304401",
    "publication_date": "1999-04-01",
    "publication_year": 1999,
    "authors": "Jonathan E. Cook; Alexander L. Wolf",
    "corresponding_authors": "",
    "abstract": "To a great extent, the usefulness of a formal model of a software process lies in its ability to accurately predict the behavior of the executing process. Similarly, the usefulness of an executing process lies largely in its ability to fulfill the requirements embodied in a formal model of the process. When process models and process executions diverge, something significant is happening. We have developed techniques for uncovering and measuring the discrepancies between models and executions, which we call process validation . Process validation takes a process execution and a process model, and measures the level of correspondence between the two. Our metrics are tailorable and give process engineers control over determining the severity of different types of discrepancies. The techniques provide detailed information once a high-level measurement indicates the presence of a problem. We have applied our processes validation methods in an industrial case study, of which a portion is described in this article.",
    "cited_by_count": 268,
    "openalex_id": "https://openalex.org/W2030668207",
    "type": "article"
  },
  {
    "title": "Designing data marts for data warehouses",
    "doi": "https://doi.org/10.1145/384189.384190",
    "publication_date": "2001-10-01",
    "publication_year": 2001,
    "authors": "Angela Bonifati; Fabiano Cattaneo; Stefano Ceri; Alfonso Fuggetta; Stefano Paraboschi",
    "corresponding_authors": "",
    "abstract": "Data warehouses are databases devoted to analytical processing. They are used to support decision-making activities in most modern business settings, when complex data sets have to be studied and analyzed. The technology for analytical processing assumes that data are presented in the form of simple data marts, consisting of a well-identified collection of facts and data analysis dimensions (star schema). Despite the wide diffusion of data warehouse technology and concepts, we still miss methods that help and guide the designer in identifying and extracting such data marts out of an enterprisewide information system, covering the upstream, requirement-driven stages of the design process. Many existing methods and tools support the activities related to the efficient implementation of data marts on top of specialized technology (such as the ROLAP or MOLAP data servers). This paper presents a method to support the identification and design of data marts. The method is based on three basic steps. A first top-down step makes it possible to elicit and consolidate user requirements and expectations. This is accomplished by exploiting a goal-oriented process based on the Goal/Question/Metric paradigm developed at the University of Maryland. Ideal data marts are derived from user requirements. The second bottom-up step extracts candidate data marts",
    "cited_by_count": 267,
    "openalex_id": "https://openalex.org/W1971323001",
    "type": "article"
  },
  {
    "title": "Formalizing style to understand descriptions of software architecture",
    "doi": "https://doi.org/10.1145/226241.226244",
    "publication_date": "1995-10-01",
    "publication_year": 1995,
    "authors": "Gregory D. Abowd; Robert J. Allen; David Garlan",
    "corresponding_authors": "",
    "abstract": "The software architecture of most systems is usually described informally and diagrammatically by means of boxes and lines. In order for these descriptions to be meaningful, the diagrams are understood by interpreting the boxes and lines in specific, conventionalized ways. The informal, imprecise nature of these interpretations has a number of limitations. In this article we consider these conventionalized interpretations as architectural styles and provide a formal framework for their uniform definition. In addition to providing a template for precisely defining new architectural styles, this framework allows for analysis within and between different architectural styles.",
    "cited_by_count": 266,
    "openalex_id": "https://openalex.org/W1989623274",
    "type": "article"
  },
  {
    "title": "LIME",
    "doi": "https://doi.org/10.1145/1151695.1151698",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Amy L. Murphy; Gian Pietro Picco; Gruia-Catalin Roman",
    "corresponding_authors": "",
    "abstract": "LIME (Linda in a mobile environment) is a model and middleware supporting the development of applications that exhibit the physical mobility of hosts, logical mobility of agents, or both. LIME adopts a coordination perspective inspired by work on the Linda model. The context for computation, represented in Linda by a globally accessible persistent tuple space, is refined in LIME to transient sharing of the identically named tuple spaces carried by individual mobile units. Tuple spaces are also extended with a notion of location and programs are given the ability to react to specified states. The resulting model provides a minimalist set of abstractions that facilitates the rapid and dependable development of mobile applications. In this article we illustrate the model underlying LIME, provide a formal semantic characterization for the operations it makes available to the application developer, present its current design and implementation, and discuss lessons learned in developing applications that involve physical mobility.",
    "cited_by_count": 254,
    "openalex_id": "https://openalex.org/W2069946274",
    "type": "article"
  },
  {
    "title": "Representing concerns in source code",
    "doi": "https://doi.org/10.1145/1189748.1189751",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Martin P. Robillard; Gail C. Murphy",
    "corresponding_authors": "",
    "abstract": "A software modification task often addresses several concerns . A concern is anything a stakeholder may want to consider as a conceptual unit, including features, nonfunctional requirements, and design idioms. In many cases, the source code implementing a concern is not encapsulated in a single programming language module, and is instead scattered and tangled throughout a system. Inadequate separation of concerns increases the difficulty of evolving software in a correct and cost-effective manner. To make it easier to modify concerns that are not well modularized, we propose an approach in which the implementation of concerns is documented in artifacts, called concern graphs. Concern graphs are abstract models that describe which parts of the source code are relevant to different concerns. We present a formal model for concern graphs and the tool support we developed to enable software developers to create and use concern graphs during software evolution tasks. We report on five empirical studies, providing evidence that concern graphs support views and operations that facilitate the task of modifying the code implementing scattered concerns, are cost-effective to create and use, and robust enough to be used with different versions of a software system.",
    "cited_by_count": 221,
    "openalex_id": "https://openalex.org/W2153546999",
    "type": "article"
  },
  {
    "title": "From business process models to process-oriented software systems",
    "doi": "https://doi.org/10.1145/1555392.1555395",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Chun Ouyang; Marlon Dumas; Wil M. P. van der Aalst; Arthur H. M. ter Hofstede; Jan Mendling",
    "corresponding_authors": "",
    "abstract": "Several methods for enterprise systems analysis rely on flow-oriented representations of business operations, otherwise known as business process models. The Business Process Modeling Notation (BPMN) is a standard for capturing such models. BPMN models facilitate communication between domain experts and analysts and provide input to software development projects. Meanwhile, there is an emergence of methods for enterprise software development that rely on detailed process definitions that are executed by process engines. These process definitions refine their counterpart BPMN models by introducing data manipulation, application binding, and other implementation details. The de facto standard for defining executable processes is the Business Process Execution Language (BPEL). Accordingly, a standards-based method for developing process-oriented systems is to start with BPMN models and to translate these models into BPEL definitions for subsequent refinement. However, instrumenting this method is challenging because BPMN models and BPEL definitions are structurally very different. Existing techniques for translating BPMN to BPEL only work for limited classes of BPMN models. This article proposes a translation technique that does not impose structural restrictions on the source BPMN model. At the same time, the technique emphasizes the generation of readable (block-structured) BPEL code. An empirical evaluation conducted over a large collection of process models shows that the resulting BPEL definitions are largely block-structured. Beyond its direct relevance in the context of BPMN and BPEL, the technique presented in this article addresses issues that arise when translating from graph-oriented to block-structure flow definition languages.",
    "cited_by_count": 219,
    "openalex_id": "https://openalex.org/W1971478729",
    "type": "article"
  },
  {
    "title": "Conjunction as composition",
    "doi": "https://doi.org/10.1145/158431.158438",
    "publication_date": "1993-10-01",
    "publication_year": 1993,
    "authors": "Pamela Zave; Michael Jackson",
    "corresponding_authors": "",
    "abstract": "Partial specifications written in many different specification languages can be composed if they are all given semantics in the same domain, or alternatively, all translated into a common style of predicate logic. The common semantic domain must be very general, the particular semantics assigned to each specification language must be conducive to composition, and there must be some means of communication that enables specifications to build on one another. The criteria for success are that a wide variety of specification languages should be accommodated, there should be no restrictions on where boundaries between languages can be placed, and intuitive expectations of the specifier should be met.",
    "cited_by_count": 211,
    "openalex_id": "https://openalex.org/W1979290877",
    "type": "article"
  },
  {
    "title": "Programming pervasive and mobile computing applications",
    "doi": "https://doi.org/10.1145/1538942.1538945",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "Marco Mamei; Franco Zambonelli",
    "corresponding_authors": "",
    "abstract": "Pervasive and mobile computing call for suitable middleware and programming models to support the activities of complex software systems in dynamic network environments. In this article we present TOTA (“Tuples On The Air”), a novel middleware and programming approach for supporting adaptive context-aware activities in pervasive and mobile computing scenarios. The key idea in TOTA is to rely on spatially distributed tuples, adaptively propagated across a network on the basis of application-specific rules, for both representing contextual information and supporting uncoupled interactions between application components. TOTA promotes a simple way of programming that facilitates access to distributed information, navigation in complex environments, and the achievement of complex coordination tasks in a fully distributed and adaptive way, mostly freeing programmers and system managers from the need to take care of low-level issues related to network dynamics. This article includes both application examples to clarify concepts and performance figures to show the feasibility of the approach",
    "cited_by_count": 211,
    "openalex_id": "https://openalex.org/W2033226581",
    "type": "article"
  },
  {
    "title": "Symbolic model checking of UML activity diagrams",
    "doi": "https://doi.org/10.1145/1125808.1125809",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Rik Eshuis",
    "corresponding_authors": "Rik Eshuis",
    "abstract": "Two translations from activity diagrams to the input language of NuSMV, a symbolic model verifier, are presented. Both translations map an activity diagram into a finite state machine and are inspired by existing statechart semantics. The requirements-level translation defines state machines that can be efficiently verified, but are a bit unrealistic since they assume the perfect synchrony hypothesis. The implementation-level translation defines state machines that cannot be verified so efficiently, but that are more realistic since they do not use the perfect synchrony hypothesis. To justify the use of the requirements-level translation, we show that for a large class of activity diagrams and certain properties, both translations are equivalent: regardless of which translation is used, the outcome of model checking is the same. Moreover, for linear stuttering-closed properties, the implementation-level translation is equivalent to a slightly modified version of the requirements-level translation. We use the two translations to model check data integrity constraints for an activity diagram and a set of class diagrams that specify the data manipulated in the activities. Both translations have been implemented in two tools. We discuss our experiences in applying both translations to model check some large example activity diagrams.",
    "cited_by_count": 204,
    "openalex_id": "https://openalex.org/W2120450154",
    "type": "article"
  },
  {
    "title": "Business Process Model Merging",
    "doi": "https://doi.org/10.1145/2430545.2430547",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Marcello La Rosa; Marlon Dumas; Reina Uba; Remco Dijkman",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of constructing consolidated business process models out of collections of process models that share common fragments. The article considers the construction of unions of multiple models (called merged models ) as well as intersections (called digests ). Merged models are intended for analysts who wish to create a model that subsumes a collection of process models -- typically representing variants of the same underlying process -- with the aim of replacing the variants with the merged model. Digests, on the other hand, are intended for analysts who wish to identify the most recurring fragments across a collection of process models, so that they can focus their efforts on optimizing these fragments. The article presents an algorithm for computing merged models and an algorithm for extracting digests from a merged model. The merging and digest extraction algorithms have been implemented and tested against collections of process models taken from multiple application domains. The tests show that the merging algorithm produces compact models and scales up to process models containing hundreds of nodes. Furthermore, a case study conducted in a large insurance company has demonstrated the usefulness of the merging and digest extraction operators in a practical setting.",
    "cited_by_count": 199,
    "openalex_id": "https://openalex.org/W2116677259",
    "type": "article"
  },
  {
    "title": "Power laws in software",
    "doi": "https://doi.org/10.1145/1391984.1391986",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Πάνος Λουρίδας; Diomidis Spinellis; Vasileios Vlachos",
    "corresponding_authors": "",
    "abstract": "A single statistical framework, comprising power law distributions and scale-free networks, seems to fit a wide variety of phenomena. There is evidence that power laws appear in software at the class and function level. We show that distributions with long, fat tails in software are much more pervasive than previously established, appearing at various levels of abstraction, in diverse systems and languages. The implications of this phenomenon cover various aspects of software engineering research and practice.",
    "cited_by_count": 198,
    "openalex_id": "https://openalex.org/W2124418175",
    "type": "article"
  },
  {
    "title": "Automated assistance for program restructuring",
    "doi": "https://doi.org/10.1145/152388.152389",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "William G. Griswold; David Notkin",
    "corresponding_authors": "",
    "abstract": "Maintenance tends to degrade the structure of software, ultimately making maintenance more costly. At times, then, it is worthwhile to manipulate the structure of a system to make changes easier. However, manual restructuring is an error-prone and expensive activity. By separating structural manipulations from other maintenance activities, the semantics of a system can be held constant by a tool, assuring that no errors are introduced by restructuring. To allow the maintenance team to focus on the aspects of restructuring and maintenance requiring human judgment, a transformation-based tool can be provided—based on a model that exploits preserving data flow dependence and control flow dependence—to automate the repetitive, error-prone, and computationally demanding aspects of restructuring. A set of automatable transformations is introduced; their impact on structure is described, and their usefulness is demonstrated in examples. A model to aid building meaning-preserving restructuring transformations is described, and its realization in a functioning prototype tool for restructuring Scheme programs is discussed.",
    "cited_by_count": 192,
    "openalex_id": "https://openalex.org/W1972064644",
    "type": "article"
  },
  {
    "title": "Reasoning about inconsistencies in natural language requirements",
    "doi": "https://doi.org/10.1145/1072997.1072999",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Vincenzo Gervasi; Didar Zowghi",
    "corresponding_authors": "",
    "abstract": "The use of logic in identifying and analyzing inconsistency in requirements from multiple stakeholders has been found to be effective in a number of studies. Nonmonotonic logic is a theoretically well-founded formalism that is especially suited for supporting the evolution of requirements. However, direct use of logic for expressing requirements and discussing them with stakeholders poses serious usability problems, since in most cases stakeholders cannot be expected to be fluent with formal logic. In this article, we explore the integration of natural language parsing techniques with default reasoning to overcome these difficulties. We also propose a method for automatically discovering inconsistencies in the requirements from multiple stakeholders, using both theorem-proving and model-checking techniques, and show how to deal with them in a formal manner. These techniques were implemented and tested in a prototype tool called CARL . The effectiveness of the techniques and of the tool are illustrated by a classic example involving conflicting requirements from multiple stakeholders.",
    "cited_by_count": 190,
    "openalex_id": "https://openalex.org/W2072082216",
    "type": "article"
  },
  {
    "title": "Achieving scalable model-based testing through test case diversity",
    "doi": "https://doi.org/10.1145/2430536.2430540",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Hadi Hemmati; Andrea Arcuri; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "The increase in size and complexity of modern software systems requires scalable, systematic, and automated testing approaches. Model-based testing (MBT), as a systematic and automated test case generation technique, is being successfully applied to verify industrial-scale systems and is supported by commercial tools. However, scalability is still an open issue for large systems, as in practice there are limits to the amount of testing that can be performed in industrial contexts. Even with standard coverage criteria, the resulting test suites generated by MBT techniques can be very large and expensive to execute, especially for system level testing on real deployment platforms and network facilities. Therefore, a scalable MBT technique should be flexible regarding the size of the generated test suites and should be easily accommodated to fit resource and time constraints. Our approach is to select a subset of the generated test suite in such a way that it can be realistically executed and analyzed within the time and resource constraints, while preserving the fault revealing power of the original test suite to a maximum extent. In this article, to address this problem, we introduce a family of similarity-based test case selection techniques for test suites generated from state machines. We evaluate 320 different similarity-based selection techniques and then compare the effectiveness of the best similarity-based selection technique with other common selection techniques in the literature. The results based on two industrial case studies, in the domain of embedded systems, show significant benefits and a large improvement in performance when using a similarity-based approach. We complement these analyses with further studies on the scalability of the technique and the effects of failure rate on its effectiveness. We also propose a method to identify optimal tradeoffs between the number of test cases to run and fault detection.",
    "cited_by_count": 190,
    "openalex_id": "https://openalex.org/W2073125568",
    "type": "article"
  },
  {
    "title": "Managing inconsistent specifications",
    "doi": "https://doi.org/10.1145/292182.292187",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "Anthony Hunter; Bashar Nuseibeh",
    "corresponding_authors": "",
    "abstract": "In previous work, we advocated continued development of specifications in the presence of inconsistency. To support this, we used classical logic to represent partial specifications and to identify inconsistencies between them. We now present an adaptation of classical logic, which we term quasi-classical (QC) logic, that allows continued reasoning in the presence of inconsistency. The adaptation is a weakening of classical logic that prohibits all trivial derivations, but still allows all resolvants of the assumptions to be derived. Furthermore, the connectives behave in a classical manner. We then present a development called labeled QC logic that records and tracks assumptions used in reasoning. This facilitates a logical analysis of inconsistent information. We discuss that application of labeled QC logic in the analysis of multiperspective specifications. Such specifications are developed by multiple particpants who hold overlapping, often inconsistent, views of the systems they are developing.",
    "cited_by_count": 189,
    "openalex_id": "https://openalex.org/W1968128450",
    "type": "article"
  },
  {
    "title": "Computing similarity in a reuse library system",
    "doi": "https://doi.org/10.1145/131736.131739",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Eduardo Jenkins Ostertag; James Hendler; Rubén Prieto Díaz; Christine L. Braun",
    "corresponding_authors": "",
    "abstract": "This paper presents an AI based library system for software reuse, called AIRS, that allows a developer to browse a software library in search of components that best meet some stated requirement. A component is described by a set of ( feature, term ) pairs. A feature represents a classification criterion, and is defined by a set of related terms. The system allows to represent packages (logical units that group a set of components) which are also described in terms of features. Candidate reuse components and packages are selected from the library based on the degree of similarity between their descriptions and a given target description. Similarity is quantified by a nonnegative magnitude ( distance ) proportional to the effort required to obtain the target given a candidate. Distances are computed by comparator functions based on the subsumption, closeness, and package relations. We present a formalization of the concepts on which the AIRS system is based. The functionality of a prototype implementation of the AIRS system is illustrated by application to two different software libraries: a set of Ada packages for data structure manipulation, and a set of C components for use in Command, Control, and Information Systems. Finally, we discuss some of the ideas we are currently exploring to automate the construction of AIRS classification libraries.",
    "cited_by_count": 188,
    "openalex_id": "https://openalex.org/W2091007968",
    "type": "article"
  },
  {
    "title": "A comprehensive approach for the development of modular software architecture description languages",
    "doi": "https://doi.org/10.1145/1061254.1061258",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "Eric M. Dashofy; André van der Hoek; Richard N. Taylor",
    "corresponding_authors": "",
    "abstract": "Research over the past decade has revealed that modeling software architecture at the level of components and connectors is useful in a growing variety of contexts. This has led to the development of a plethora of notations for representing software architectures, each focusing on different aspects of the systems being modeled. In general, these notations have been developed without regard to reuse or extension. This makes the effort in adapting an existing notation to a new purpose commensurate with developing a new notation from scratch. To address this problem, we have developed an approach that allows for the rapid construction of new architecture description languages (ADLs). Our approach is unique because it encapsulates ADL features in modules that are composed to form ADLs. We achieve this by leveraging the extension mechanisms provided by XML and XML schemas. We have defined a set of generic, reusable ADL modules called xADL 2.0, useful as an ADL by itself, but also extensible to support new applications and domains. To support this extensibility, we have developed a set of reflective syntax-based tools that adapt to language changes automatically, as well as several semantically-aware tools that provide support for advanced features of xADL 2.0. We demonstrate the effectiveness, scalability, and flexibility of our approach through a diverse set of experiences. First, our approach has been applied in industrial contexts, modeling software architectures for aircraft software and spacecraft systems. Second, we show how xADL 2.0 can be extended to support the modeling features found in two different representations for modeling product-line architectures. Finally, we show how our infrastructure has been used to support its own development. The technical contribution of our infrastructure is augmented by several research contributions: the first decomposition of an architecture description language into modules, insights about how to develop new language modules and a process for integrating them, and insights about the roles of different kinds of tools in a modular ADL-based infrastructure.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W2143831726",
    "type": "article"
  },
  {
    "title": "Type checking annotation-based product lines",
    "doi": "https://doi.org/10.1145/2211616.2211617",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Christian Kästner; Sven Apel; Thomas Thüm; Gunter Saake",
    "corresponding_authors": "",
    "abstract": "Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire product line. We present a product-line-aware type system that can type check an entire software product line without generating each variant in isolation. Specifically, we extend the Featherweight Java calculus with feature annotations for product-line development and prove formally that all program variants generated from a well typed product line are well typed. Furthermore, we present a solution to the problem of typing mutually exclusive features. We discuss how results from our formalization helped implement our own product-line tool CIDE for full Java and report of our experience with detecting type errors in four existing software product line implementations.",
    "cited_by_count": 183,
    "openalex_id": "https://openalex.org/W2070929035",
    "type": "article"
  },
  {
    "title": "Markov analysis of software specifications",
    "doi": "https://doi.org/10.1145/151299.151326",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "James A. Whittaker; Jesse H. Poore",
    "corresponding_authors": "",
    "abstract": "A procedure for modeling software usage with the finite state, discrete parameter Markov chain is described. It involves rigorous analysis of the specification before design and coding begin. Many benefits emerge from this process, including the ability to synthesize a macro level usage distribution from a micro level understanding of how the software will be used. This usage distribution becomes the basis for a statistical test of the software, which is fundamental to the Cleanroom development process. Some analytical results known for Markov chains that have meaningful implications and interpretations for the software development process are described.",
    "cited_by_count": 182,
    "openalex_id": "https://openalex.org/W2001355653",
    "type": "article"
  },
  {
    "title": "Designing and comparing automated test oracles for GUI-based software applications",
    "doi": "https://doi.org/10.1145/1189748.1189752",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Qing Xie; Atif M. Memon",
    "corresponding_authors": "",
    "abstract": "Test designers widely believe that the overall effectiveness and cost of software testing depends largely on the type and number of test cases executed on the software. This article shows that the test oracle , a mechanism that determines whether a software is executed correctly for a test case, also significantly impacts the fault detection effectiveness and cost of a test case. Graphical user interfaces (GUIs), which have become ubiquitous for interacting with today's software, have created new challenges for test oracle development. Test designers manually “assert” the expected values of specific properties of certain GUI widgets in each test case; during test execution, these assertions are used as test oracles to determine whether the GUI executed correctly. Since a test case for a GUI is a sequence of events, a test designer must decide: (1) what to assert; and (2) how frequently to check an assertion, for example, after each event in the test case or after the entire test case has completed execution. Variations of these two factors significantly impact the fault-detection ability and cost of a GUI test case. A technique to declaratively specify different types of automated GUI test oracles is described. Six instances of test oracles are developed and compared in an experiment on four software systems. The results show that test oracles do affect the fault detection ability of test cases in different and interesting ways: (1) Test cases significantly lose their fault detection ability when using “weak” test oracles; (2) in many cases, invoking a “thorough” oracle at the end of test case execution yields the best cost-benefit ratio; (3) certain test cases detect faults only if the oracle is invoked during a small “window of opportunity” during test execution; and (4) using thorough and frequently-executing test oracles can compensate for not having long test cases.",
    "cited_by_count": 181,
    "openalex_id": "https://openalex.org/W1996574389",
    "type": "article"
  },
  {
    "title": "Incremental elaboration of scenario-based specifications and behavior models using implied scenarios",
    "doi": "https://doi.org/10.1145/1005561.1005563",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Sebastián Uchitel; Jeff Kramer; Jeff Magee",
    "corresponding_authors": "",
    "abstract": "Behavior modeling has proved to be successful in helping uncover design flaws of concurrent and distributed systems. Nevertheless, it has not had a widespread impact on practitioners because model construction remains a difficult task and because the benefits of behavior analysis appear at the end of the model construction effort. In contrast, scenario-based specifications have a wide acceptance in industry and are well suited for developing first approximations of intended behavior; however, they are still maturing with respect to rigorous semantics and analysis tools.This article proposes a process for elaborating system behavior that exploits the potential benefits of behavior modeling and scenario-based specifications yet ameliorates their shortcomings. The concept that drives the elaboration process is that of implied scenarios . Implied scenarios identify gaps in scenario-based specifications that arise from specifying the global behavior of a system that will be implemented component-wise. They are the result of a mismatch between the behavioral and architectural aspects of scenario-based specifications. Due to the partial nature of scenario-based specifications, implied scenarios need to be validated as desired or undesired behavior. The scenario specifications are then updated accordingly with new positive or negative scenarios. By iteratively detecting and validating implied scenarios, it is possible to incrementally elaborate the behavior described both in the scenario-based specification and models. The proposed elaboration process starts with a message sequence chart (MSC) specification that includes basic, high-level and negative MSCs. Implied scenario detection is performed by synthesis and automated analysis of behavior models. The final outcome consists of four artifacts: (1) an MSC specification that has been evolved from its original form to cover important aspects of the concurrent nature of the system that were under-specified or absent in the original specification, (2) a behavior model that captures the component structure of the system that, combined with (3) a constraint model and (4) a property model that provides the basis for modeling and reasoning about system design.",
    "cited_by_count": 180,
    "openalex_id": "https://openalex.org/W2159263584",
    "type": "article"
  },
  {
    "title": "An empirical study of industrial requirements engineering process assessment and improvement",
    "doi": "https://doi.org/10.1145/1044834.1044837",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Ian Sommerville; Jane Ransom",
    "corresponding_authors": "",
    "abstract": "This article describes an empirical study in industry of requirements engineering process maturity assessment and improvement. Our aims were to evaluate a requirements engineering process maturity model and to assess if improvements in requirements engineering process maturity lead to business improvements. We first briefly describe the process maturity model that we used and modifications to this model to accommodate process improvement. We present initial maturity assessment results for nine companies, describe how process improvements were selected and present data on how RE process maturity changed after these improvements were introduced. We discuss how business benefits were assessed and the difficulties of relating process maturity improvements to these business benefits. All companies reported business benefits and satisfaction with their participation in the study. Our conclusions are that the RE process maturity model is useful in supporting maturity assessment and in identifying process improvements and there is some evidence to suggest that process improvement leads to business benefits. However, whether these business benefits were a consequence of the changes to the RE process or whether these benefits resulted from side-effects of the study such as greater self-awareness of business processes remains an open question.",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W2048976958",
    "type": "article"
  },
  {
    "title": "Some Code Smells Have a Significant but Small Effect on Faults",
    "doi": "https://doi.org/10.1145/2629648",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Tracy Hall; Min Zhang; David Bowes; Yi Sun",
    "corresponding_authors": "",
    "abstract": "We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness.",
    "cited_by_count": 168,
    "openalex_id": "https://openalex.org/W2060561050",
    "type": "article"
  },
  {
    "title": "Lightweight, Obfuscation-Resilient Detection and Family Identification of Android Malware",
    "doi": "https://doi.org/10.1145/3162625",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Joshua Garcia; Mahmoud Hammad; Sam Malek",
    "corresponding_authors": "",
    "abstract": "The number of malicious Android apps is increasing rapidly. Android malware can damage or alter other files or settings, install additional applications, and so on. To determine such behaviors, a security analyst can significantly benefit from identifying the family to which an Android malware belongs rather than only detecting if an app is malicious. Techniques for detecting Android malware, and determining their families, lack the ability to handle certain obfuscations that aim to thwart detection. Moreover, some prior techniques face scalability issues, preventing them from detecting malware in a timely manner. To address these challenges, we present a novel machine-learning-based Android malware detection and family identification approach, RevealDroid, that operates without the need to perform complex program analyses or to extract large sets of features. Specifically, our selected features leverage categorized Android API usage, reflection-based features, and features from native binaries of apps. We assess RevealDroid for accuracy, efficiency, and obfuscation resilience using a large dataset consisting of more than 54,000 malicious and benign apps. Our experiments show that RevealDroid achieves an accuracy of 98% in detection of malware and an accuracy of 95% in determination of their families. We further demonstrate RevealDroid’s superiority against state-of-the-art approaches.",
    "cited_by_count": 167,
    "openalex_id": "https://openalex.org/W2783327762",
    "type": "article"
  },
  {
    "title": "On the Comprehension of Program Comprehension",
    "doi": "https://doi.org/10.1145/2622669",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Walid Maalej; Rebecca Tiarks; Tobias Roehm; Rainer Koschke",
    "corresponding_authors": "",
    "abstract": "Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension. We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge. We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments. Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support.",
    "cited_by_count": 166,
    "openalex_id": "https://openalex.org/W2144651866",
    "type": "article"
  },
  {
    "title": "Effective typestate verification in the presence of aliasing",
    "doi": "https://doi.org/10.1145/1348250.1348255",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Stephen J. Fink; Eran Yahav; Nurit Dor; G. Ramalingam; Emmanuel Geay",
    "corresponding_authors": "",
    "abstract": "This article addresses the challenge of sound typestate verification, with acceptable precision, for real-world Java programs. We present a novel framework for verification of typestate properties, including several new techniques to precisely treat aliases without undue performance costs. In particular, we present a flow-sensitive, context-sensitive, integrated verifier that utilizes a parametric abstract domain combining typestate and aliasing information. To scale to real programs without compromising precision, we present a staged verification system in which faster verifiers run as early stages which reduce the workload for later, more precise, stages. We have evaluated our framework on a number of real Java programs, checking correct API usage for various Java standard libraries. The results show that our approach scales to hundreds of thousands of lines of code, and verifies correctness for 93% of the potential points of failure.",
    "cited_by_count": 155,
    "openalex_id": "https://openalex.org/W1971327145",
    "type": "article"
  },
  {
    "title": "Types and modularity for implicit invocation with implicit announcement",
    "doi": "https://doi.org/10.1145/1767751.1767752",
    "publication_date": "2010-06-01",
    "publication_year": 2010,
    "authors": "Friedrich Steimann; Thomas Pawlitzki; Sven Apel; Christian Kästner",
    "corresponding_authors": "",
    "abstract": "Through implicit invocation, procedures are called without explicitly referencing them. Implicit announcement adds to this implicitness by not only keeping implicit which procedures are called, but also where or when—under implicit invocation with implicit announcement, the call site contains no signs of that, or what it calls. Recently, aspect-oriented programming has popularized implicit invocation with implicit announcement as a possibility to separate concerns that lead to interwoven code if conventional programming techniques are used. However, as has been noted elsewhere, as currently implemented it establishes strong implicit dependencies between components, hampering independent software development and evolution. To address this problem, we present a type-based modularization of implicit invocation with implicit announcement that is inspired by how interfaces and exceptions are realized in Java. By extending an existing compiler and by rewriting several programs to make use of our proposed language constructs, we found that the imposed declaration clutter tends to be moderate; in particular, we found that, for general applications of implicit invocation with implicit announcement, fears that programs utilizing our form of modularization become unreasonably verbose are unjustified.",
    "cited_by_count": 153,
    "openalex_id": "https://openalex.org/W2092658016",
    "type": "article"
  },
  {
    "title": "Automatically repairing event sequence-based GUI test suites for regression testing",
    "doi": "https://doi.org/10.1145/1416563.1416564",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Atif M. Memon",
    "corresponding_authors": "Atif M. Memon",
    "abstract": "Although graphical user interfaces (GUIs) constitute a large part of the software being developed today and are typically created using rapid prototyping, there are no effective regression testing techniques for GUIs. The needs of GUI regression testing differ from those of traditional software. When the structure of a GUI is modified, test cases from the original GUI's suite are either reusable or unusable on the modified GUI. Because GUI test case generation is expensive, our goal is to make the unusable test cases usable, thereby helping to retain the suite's event coverage. The idea of reusing these unusable ( obsolete ) test cases has not been explored before. This article shows that a large number of test cases become unusable for GUIs. It presents a new GUI regression testing technique that first automatically determines the usable and unusable test cases from a test suite after a GUI modification, then determines the unusable test cases that can be repaired so that they can execute on the modified GUI, and finally uses repairing transformations to repair the test cases. This regression testing technique along with four repairing transformations has been implemented. An empirical study for four open-source applications demonstrates that (1) this approach is effective in that many of the test cases can be repaired, and is practical in terms of its time performance, (2) certain types of test cases are more prone to becoming unusable, and (3) certain types of “dominator” events, when modified, make a large number of test cases unusable.",
    "cited_by_count": 145,
    "openalex_id": "https://openalex.org/W2043709926",
    "type": "article"
  },
  {
    "title": "Multi-Criteria Code Refactoring Using Search-Based Software Engineering",
    "doi": "https://doi.org/10.1145/2932631",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "Ali Ouni; Marouane Kessentini; Houari Sahraoui; Katsuro Inoue; Kalyanmoy Deb",
    "corresponding_authors": "",
    "abstract": "One of the most widely used techniques to improve the quality of existing software systems is refactoring—the process of improving the design of existing code by changing its internal structure without altering its external behavior. While it is important to suggest refactorings that improve the quality and structure of the system, many other criteria are also important to consider, such as reducing the number of code changes, preserving the semantics of the software design and not only its behavior, and maintaining consistency with the previously applied refactorings. In this article, we propose a multi-objective search-based approach for automating the recommendation of refactorings. The process aims at finding the optimal sequence of refactorings that (i) improves the quality by minimizing the number of design defects, (ii) minimizes code changes required to fix those defects, (iii) preserves design semantics, and (iv) maximizes the consistency with the previously code changes. We evaluated the efficiency of our approach using a benchmark of six open-source systems, 11 different types of refactorings (move method, move field, pull up method, pull up field, push down method, push down field, inline class, move class, extract class, extract method, and extract interface) and six commonly occurring design defect types (blob, spaghetti code, functional decomposition, data class, shotgun surgery, and feature envy) through an empirical study conducted with experts. In addition, we performed an industrial validation of our technique, with 10 software engineers, on a large project provided by our industrial partner. We found that the proposed refactorings succeed in preserving the design coherence of the code, with an acceptable level of code change score while reusing knowledge from recorded refactorings applied in the past to similar contexts.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2475137645",
    "type": "article"
  },
  {
    "title": "The Choice Calculus",
    "doi": "https://doi.org/10.1145/2063239.2063245",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Martin Erwig; Eric Walkingshaw",
    "corresponding_authors": "",
    "abstract": "Many areas of computer science are concerned with some form of variation in software---from managing changes to software over time to supporting families of related artifacts. We present the choice calculus, a fundamental representation for software variation that can serve as a common language of discourse for variation research, filling a role similar to the lambda calculus in programming language research. We also develop an associated theory of software variation, including sound transformations of variation artifacts, the definition of strategic normal forms, and a design theory for variation structures, which will support the development of better algorithms and tools.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2016954568",
    "type": "article"
  },
  {
    "title": "Method and developer characteristics for effective agile method tailoring",
    "doi": "https://doi.org/10.1145/1767751.1767753",
    "publication_date": "2010-06-01",
    "publication_year": 2010,
    "authors": "Kieran Conboy; Brian Fitzgerald",
    "corresponding_authors": "",
    "abstract": "It has long been acknowledged that software methods should be tailored if they are to achieve optimum effect. However comparatively little research has been carried out to date on this topic in general, and more notably, on agile methods in particular. This dearth of evidence in the case of agile methods is especially significant in that it is reasonable to expect that such methods would particularly lend themselves to tailoring. In this research, we present a framework based on interviews with 20 senior software development researchers and a review of the extant literature. The framework is comprised of two sets of factors—characteristics of the method, and developer practices—that can improve method tailoring effectiveness. Drawing on the framework, we then interviewed 16 expert XP practitioners to examine the current state and effectiveness of XP tailoring efforts, and to shed light on issues the framework identified as being important. The article concludes with a set of recommendations for research and practice that would advance our understanding of the method tailoring area.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2056795153",
    "type": "article"
  },
  {
    "title": "When and How to Use Multilevel Modelling",
    "doi": "https://doi.org/10.1145/2685615",
    "publication_date": "2014-12-23",
    "publication_year": 2014,
    "authors": "Juan de Lara; Esther Guerra; Jesús Sánchez Cuadrado",
    "corresponding_authors": "",
    "abstract": "Model-Driven Engineering (MDE) promotes models as the primary artefacts in the software development process, from which code for the final application is derived. Standard approaches to MDE (like those based on MOF or EMF) advocate a two-level metamodelling setting where Domain-Specific Modelling Languages (DSMLs) are defined through a metamodel that is instantiated to build models at the metalevel below. Multilevel modelling (also called deep metamodelling ) extends the standard approach to metamodelling by enabling modelling at an arbitrary number of metalevels, not necessarily two. Proposers of multilevel modelling claim this leads to simpler model descriptions in some situations, although its applicability has been scarcely evaluated. Thus, practitioners may find it difficult to discern when to use it and how to implement multilevel solutions in practice. In this article, we discuss those situations where the use of multilevel modelling is beneficial, and identify recurring patterns and idioms. Moreover, in order to assess how often the identified patterns arise in practice, we have analysed a wide range of existing two-level DSMLs from different sources and domains, to detect when their elements could be rearranged in more than two metalevels. The results show this scenario is not uncommon, while in some application domains (like software architecture and enterprise/process modelling) pervasive, with a high average number of pattern occurrences per metamodel.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2087486361",
    "type": "article"
  },
  {
    "title": "Solving the Search for Source Code",
    "doi": "https://doi.org/10.1145/2581377",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Kathryn T. Stolee; Sebastian Elbaum; Daniel Dobos",
    "corresponding_authors": "",
    "abstract": "Programmers frequently search for source code to reuse using keyword searches. The search effectiveness in facilitating reuse, however, depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches are either not flexible enough to find approximate matches or require the programmer to define complex specifications as queries. We propose a novel approach to semantic code search that addresses several of these limitations and is designed for queries that can be described using a concrete input/output example. In this approach, programmers write lightweight specifications as inputs and expected output examples. Unlike existing approaches to semantic search, we use an SMT solver to identify programs or program fragments in a repository, which have been automatically transformed into constraints using symbolic analysis, that match the programmer-provided specification. We instantiated and evaluated this approach in subsets of three languages, the Java String library, Yahoo! Pipes mashup language, and SQL select statements, exploring its generality, utility, and trade-offs. The results indicate that this approach is effective at finding relevant code, can be used on its own or to filter results from keyword searches to increase search precision, and is adaptable to find approximate matches and then guide modifications to match the user specifications when exact matches do not already exist. These gains in precision and flexibility come at the cost of performance, for which underlying factors and mitigation strategies are identified.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2060055745",
    "type": "article"
  },
  {
    "title": "Neural Network-based Detection of Self-Admitted Technical Debt",
    "doi": "https://doi.org/10.1145/3324916",
    "publication_date": "2019-07-29",
    "publication_year": 2019,
    "authors": "Xiaoxue Ren; Zhenchang Xing; Xin Xia; David Lo; Xinyu Wang; John Grundy",
    "corresponding_authors": "",
    "abstract": "Technical debt is a metaphor to reflect the tradeoff software engineers make between short-term benefits and long-term stability. Self-admitted technical debt (SATD), a variant of technical debt, has been proposed to identify debt that is intentionally introduced during software development, e.g., temporary fixes and workarounds. Previous studies have leveraged human-summarized patterns (which represent n-gram phrases that can be used to identify SATD) or text-mining techniques to detect SATD in source code comments. However, several characteristics of SATD features in code comments, such as vocabulary diversity, project uniqueness, length, and semantic variations, pose a big challenge to the accuracy of pattern or traditional text-mining-based SATD detection, especially for cross-project deployment. Furthermore, although traditional text-mining-based method outperforms pattern-based method in prediction accuracy, the text features it uses are less intuitive than human-summarized patterns, which makes the prediction results hard to explain. To improve the accuracy of SATD prediction, especially for cross-project prediction, we propose a Convolutional Neural Network-- (CNN) based approach for classifying code comments as SATD or non-SATD. To improve the explainability of our model’s prediction results, we exploit the computational structure of CNNs to identify key phrases and patterns in code comments that are most relevant to SATD. We have conducted an extensive set of experiments with 62,566 code comments from 10 open-source projects and a user study with 150 comments of another three projects. Our evaluation confirms the effectiveness of different aspects of our approach and its superior performance, generalizability, adaptability, and explainability over current state-of-the-art traditional text-mining-based methods for SATD classification.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2966158888",
    "type": "article"
  },
  {
    "title": "Improving software modularization via automated analysis of latent topics and dependencies",
    "doi": "https://doi.org/10.1145/2559935",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Gabriele Bavota; Malcom Gethers; Rocco Oliveto; Denys Poshyvanyk; Andrea De Lucia",
    "corresponding_authors": "",
    "abstract": "Oftentimes, during software maintenance the original program modularization decays, thus reducing its quality. One of the main reasons for such architectural erosion is suboptimal placement of source-code classes in software packages. To alleviate this issue, we propose an automated approach to help developers improve the quality of software modularization. Our approach analyzes underlying latent topics in source code as well as structural dependencies to recommend (and explain) refactoring operations aiming at moving a class to a more suitable package. The topics are acquired via Relational Topic Models (RTM), a probabilistic topic modeling technique. The resulting tool, coined as R 3 (Rational Refactoring via RTM), has been evaluated in two empirical studies. The results of the first study conducted on nine software systems indicate that R 3 provides a coupling reduction from 10% to 30% among the software modules. The second study with 62 developers confirms that R 3 is able to provide meaningful recommendations (and explanations) for move class refactoring. Specifically, more than 70% of the recommendations were considered meaningful from a functional point of view.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W1975790660",
    "type": "article"
  },
  {
    "title": "Facilitating the transition from use case models to analysis models",
    "doi": "https://doi.org/10.1145/2430536.2430539",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Tao Yue; Lionel Briand; Yvan Labiche",
    "corresponding_authors": "",
    "abstract": "Use case modeling, including use case diagrams and use case specifications (UCSs), is commonly applied to structure and document requirements. UCSs are usually structured but unrestricted textual documents complying with a certain use case template. However, because Use Case Models (UCMods) remain essentially textual, ambiguity is inevitably introduced. In this article, we propose a use case modeling approach, called Restricted Use Case Modeling (RUCM), which is composed of a set of well-defined restriction rules and a modified use case template. The goal is two-fold: (1) restrict the way users can document UCSs in order to reduce ambiguity and (2) facilitate the manual derivation of initial analysis models which, when using the Unified Modeling Language (UML), are typically composed of class diagrams, sequence diagrams, and possibly other types of diagrams. Though the proposed restriction rules and template are based on a clear rationale, two main questions need to be investigated. First, do users find them too restrictive or impractical in certain situations? In other words, can users express the same requirements with RUCM as with unrestricted use cases? Second, do the rules and template have a positive, significant impact on the quality of the constructed analysis models? To investigate these questions, we performed and report on two controlled experiments, which evaluate the restriction rules and use case template in terms of (1) whether they are easy to apply while developing UCMods and facilitate the understanding of UCSs, and (2) whether they help users manually derive higher quality analysis models than what can be generated when they are not used, in terms of correctness, completeness, and redundancy. This article reports on the first controlled experiments that evaluate the applicability of restriction rules on use case modeling and their impact on the quality of analysis models. The measures we have defined to characterize restriction rules and the quality of analysis class and sequence diagrams can be reused to perform similar experiments in the future, either with RUCM or other approaches. Results show that the restriction rules are overall easy to apply and that RUCM results into significant improvements over traditional approaches (i.e., with standard templates, without restrictions) in terms of class correctness and class diagram completeness, message correctness and sequence diagram completeness, and understandability of UCSs.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2090908516",
    "type": "article"
  },
  {
    "title": "Peer Review on Open-Source Software Projects",
    "doi": "https://doi.org/10.1145/2594458",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Peter C. Rigby; Daniel M. Germán; Laura Cowen; Margaret‐Anne Storey",
    "corresponding_authors": "",
    "abstract": "Peer review is seen as an important quality-assurance mechanism in both industrial development and the open-source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, software peer reviews are not as well understood. To develop an empirical understanding of OSS peer review, we examine the review policies of 25 OSS projects and study the archival records of six large, mature, successful OSS projects. We extract a series of measures based on those used in traditional inspection experiments. We measure the frequency of review, the size of the contribution under review, the level of participation during review, the experience and expertise of the individuals involved in the review, the review interval, and the number of issues discussed during review. We create statistical models of the review efficiency, review interval, and effectiveness, the issues discussed during review, to determine which measures have the largest impact on review efficacy. We find that OSS peer reviews are conducted asynchronously by empowered experts who focus on changes that are in their area of expertise. Reviewers provide timely, regular feedback on small changes. The descriptive statistics clearly show that OSS review is drastically different from traditional inspection.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W1982335236",
    "type": "article"
  },
  {
    "title": "Assessing and Improving Malware Detection Sustainability through App Evolution Studies",
    "doi": "https://doi.org/10.1145/3371924",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Haipeng Cai",
    "corresponding_authors": "Haipeng Cai",
    "abstract": "Machine learning–based classification dominates current malware detection approaches for Android. However, due to the evolution of both the Android platform and its user apps, existing such techniques are widely limited by their reliance on new malware samples, which may not be timely available, and constant retraining, which is often very costly. As a result, new and emerging malware slips through, as seen from the continued surging of malware in the wild. Thus, a more practical detector needs not only to be accurate on particular datasets but, more critically, to be able to sustain its capabilities over time without frequent retraining. In this article, we propose and study the sustainability problem for learning-based app classifiers. We define sustainability metrics and compare them among five state-of-the-art malware detectors for Android. We further developed DroidSpan , a novel classification system based on a new behavioral profile for Android apps that captures sensitive access distribution from lightweight profiling. We evaluated the sustainability of DroidSpan versus the five detectors as baselines on longitudinal datasets across the past eight years, which include 13,627 benign apps and 12,755 malware. Through our extensive experiments, we showed that DroidSpan significantly outperformed all the baselines in substainability at reasonable costs, by 6%–32% for same-period detection and 21%–37% for over-time detection. The main takeaway , which also explains the superiority of DroidSpan , is that the use of features consistently differentiating malware from benign apps over time is essential for sustainable learning-based malware detection, and that these features can be learned from studies on app evolution.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W3016369654",
    "type": "article"
  },
  {
    "title": "A Unified Test Case Prioritization Approach",
    "doi": "https://doi.org/10.1145/2685614",
    "publication_date": "2014-12-23",
    "publication_year": 2014,
    "authors": "Dan Hao; Lingming Zhang; Lu Zhang; Gregg Rothermel; Hong Mei",
    "corresponding_authors": "",
    "abstract": "Test case prioritization techniques attempt to reorder test cases in a manner that increases the rate at which faults are detected during regression testing. Coverage-based test case prioritization techniques typically use one of two overall strategies: a total strategy or an additional strategy . These strategies prioritize test cases based on the total number of code (or code-related) elements covered per test case and the number of additional (not yet covered) code (or code-related) elements covered per test case, respectively. In this article, we present a unified test case prioritization approach that encompasses both the total and additional strategies. Our unified test case prioritization approach includes two models ( basic and extended ) by which a spectrum of test case prioritization techniques ranging from a purely total to a purely additional technique can be defined by specifying the value of a parameter referred to as the f p value. To evaluate our approach, we performed an empirical study on 28 Java objects and 40 C objects, considering the impact of three internal factors (model type, choice of f p value, and coverage type) and three external factors (coverage granularity, test case granularity, and programming/testing paradigm), all of which can be manipulated by our approach. Our results demonstrate that a wide range of techniques derived from our basic and extended models with uniform f p values can outperform purely total techniques and are competitive with purely additional techniques. Considering the influence of each internal and external factor studied, the results demonstrate that various values of each factor have nontrivial influence on test case prioritization techniques.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2165834023",
    "type": "article"
  },
  {
    "title": "Status Quo in Requirements Engineering",
    "doi": "https://doi.org/10.1145/3306607",
    "publication_date": "2019-02-26",
    "publication_year": 2019,
    "authors": "Stefan Wagner; Daniel Méndéz; Michael Felderer; Antonio Vetrò; Marcos Kalinowski; Roel Wieringa; Dietmar Pfahl; Tayana Conte; Marie-Therese Christiansson; Des Greer; Casper Lassenius; Tomi Männistö; Maleknaz Nayebi; Markku Oivo; Birgit Penzenstadler; Rafael Prikladnicki; Guenther Ruhe; André Schekelmann; Sagar Sen; Rodrigo Spínola; Ahmed Tuzcu; José Luis de la Vara; Dietmar Winkler",
    "corresponding_authors": "",
    "abstract": "Requirements Engineering (RE) has established itself as a software engineering discipline during the past decades. While researchers have been investigating the RE discipline with a plethora of empirical studies, attempts to systematically derive an empirically-based theory in context of the RE discipline have just recently been started. However, such a theory is needed if we are to define and motivate guidance in performing high quality RE research and practice. We aim at providing an empirical and valid foundation for a theory of RE, which helps software engineers establish effective and efficient RE processes. We designed a survey instrument and theory that has now been replicated in 10 countries world-wide. We evaluate the propositions of the theory with bootstrapped confidence intervals and derive potential explanations for the propositions. We report on the underlying theory and the full results obtained from the replication studies with participants from 228 organisations. Our results represent a substantial step forward towards developing an empirically-based theory of RE giving insights into current practices with RE processes. The results reveal, for example, that there are no strong differences between organisations in different countries and regions, that interviews, facilitated meetings and prototyping are the most used elicitation techniques, that requirements are often documented textually, that traces between requirements and code or design documents is common, requirements specifications themselves are rarely changed and that requirements engineering (process) improvement endeavours are mostly intrinsically motivated. Our study establishes a theory that can be used as starting point for many further studies for more detailed investigations. Practitioners can use the results as theory-supported guidance on selecting suitable RE methods and techniques.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W4288363128",
    "type": "article"
  },
  {
    "title": "A Baseline Model for Software Effort Estimation",
    "doi": "https://doi.org/10.1145/2738037",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Peter A. Whigham; Caitlin A. Owen; Stephen G. MacDonell",
    "corresponding_authors": "",
    "abstract": "Software effort estimation (SEE) is a core activity in all software processes and development lifecycles. A range of increasingly complex methods has been considered in the past 30 years for the prediction of effort, often with mixed and contradictory results. The comparative assessment of effort prediction methods has therefore become a common approach when considering how best to predict effort over a range of project types. Unfortunately, these assessments use a variety of sampling methods and error measurements, making comparison with other work difficult. This article proposes an automatically transformed linear model (ATLM) as a suitable baseline model for comparison against SEE methods. ATLM is simple yet performs well over a range of different project types. In addition, ATLM may be used with mixed numeric and categorical data and requires no parameter tuning. It is also deterministic, meaning that results obtained are amenable to replication. These and other arguments for using ATLM as a baseline model are presented, and a reference implementation described and made available. We suggest that ATLM should be used as a baseline of effort prediction quality for all future model comparisons in SEE.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2203366176",
    "type": "article"
  },
  {
    "title": "SIP",
    "doi": "https://doi.org/10.1145/2897760",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Robert M. Hierons; Miqing Li; Xiaohui Liu; Sergio Segura; Wei Zheng",
    "corresponding_authors": "",
    "abstract": "A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classical many-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2286629231",
    "type": "article"
  },
  {
    "title": "Test Selection for Deep Learning Systems",
    "doi": "https://doi.org/10.1145/3417330",
    "publication_date": "2021-01-03",
    "publication_year": 2021,
    "authors": "W. F. Mader; Mike Papadakis; Anestis Tsakmalis; Maxime Cordy; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Testing of deep learning models is challenging due to the excessive number and complexity of the computations involved. As a result, test data selection is performed manually and in an ad hoc way. This raises the question of how we can automatically select candidate data to test deep learning models. Recent research has focused on defining metrics to measure the thoroughness of a test suite and to rely on such metrics to guide the generation of new tests. However, the problem of selecting/prioritising test inputs (e.g., to be labelled manually by humans) remains open. In this article, we perform an in-depth empirical comparison of a set of test selection metrics based on the notion of model uncertainty (model confidence on specific inputs). Intuitively, the more uncertain we are about a candidate sample, the more likely it is that this sample triggers a misclassification. Similarly, we hypothesise that the samples for which we are the most uncertain are the most informative and should be used in priority to improve the model by retraining. We evaluate these metrics on five models and three widely used image classification problems involving real and artificial (adversarial) data produced by five generation algorithms. We show that uncertainty-based metrics have a strong ability to identify misclassified inputs, being three times stronger than surprise adequacy and outperforming coverage-related metrics. We also show that these metrics lead to faster improvement in classification accuracy during retraining: up to two times faster than random selection and other state-of-the-art metrics on all models we considered.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W3120991880",
    "type": "article"
  },
  {
    "title": "Practical Accuracy Estimation for Efficient Deep Neural Network Testing",
    "doi": "https://doi.org/10.1145/3394112",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Junjie Chen; Zhuo Wu; Zan Wang; Hanmo You; Lingming Zhang; Ming Yan",
    "corresponding_authors": "",
    "abstract": "Deep neural network (DNN) has become increasingly popular and DNN testing is very critical to guarantee the correctness of DNN, i.e., the accuracy of DNN in this work. However, DNN testing suffers from a serious efficiency problem, i.e., it is costly to label each test input to know the DNN accuracy for the testing set, since labeling each test input involves multiple persons (even with domain-specific knowledge) in a manual way and the testing set is large-scale. To relieve this problem, we propose a novel and practical approach, called PACE (which is short for P ractical AC curacy E stimation), which selects a small set of test inputs that can precisely estimate the accuracy of the whole testing set. In this way, the labeling costs can be largely reduced by just labeling this small set of selected test inputs. Besides achieving a precise accuracy estimation, to make PACE more practical it is also required that it is interpretable, deterministic, and as efficient as possible. Therefore, PACE first incorporates clustering to interpretably divide test inputs with different testing capabilities (i.e., testing different functionalities of a DNN model) into different groups. Then, PACE utilizes the MMD-critic algorithm, a state-of-the-art example-based explanation algorithm, to select prototypes (i.e., the most representative test inputs) from each group, according to the group sizes, which can reduce the impact of noise due to clustering. Meanwhile, PACE also borrows the idea of adaptive random testing to select test inputs from the minority space (i.e., the test inputs that are not clustered into any group) to achieve great diversity under the required number of test inputs. The two parallel selection processes (i.e., selection from both groups and the minority space) compose the final small set of selected test inputs. We conducted an extensive study to evaluate the performance of PACE based on a comprehensive benchmark (i.e., 24 pairs of DNN models and testing sets) by considering different types of models (i.e., classification and regression models, high-accuracy and low-accuracy models, and CNN and RNN models) and different types of test inputs (i.e., original, mutated, and automatically generated test inputs). The results demonstrate that PACE is able to precisely estimate the accuracy of the whole testing set with only 1.181%∼2.302% deviations, on average, significantly outperforming the state-of-the-art approaches.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W3041012898",
    "type": "article"
  },
  {
    "title": "Code Structure–Guided Transformer for Source Code Summarization",
    "doi": "https://doi.org/10.1145/3522674",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Shuzheng Gao; Cuiyun Gao; Yulan He; Jichuan Zeng; Lunyiu Nie; Xin Xia; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W3159616622",
    "type": "article"
  },
  {
    "title": "Wireframe-based UI Design Search through Image Autoencoder",
    "doi": "https://doi.org/10.1145/3391613",
    "publication_date": "2020-06-16",
    "publication_year": 2020,
    "authors": "Jieshan Chen; Chunyang Chen; Zhenchang Xing; Xin Xia; Liming Zhu; John Grundy; Jinshui Wang",
    "corresponding_authors": "",
    "abstract": "UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for Android UI design search, and conduct extensive experiments with artificially created relevant UI designs and human evaluation of UI design search results. Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of our search engine in real-world UI design tasks.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W3036505300",
    "type": "article"
  },
  {
    "title": "A Survey of Flaky Tests",
    "doi": "https://doi.org/10.1145/3476105",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Owain Parry; Gregory M. Kapfhammer; Michael Hilton; Phil McMinn",
    "corresponding_authors": "",
    "abstract": "Tests that fail inconsistently, without changes to the code under test, are described as flaky . Flaky tests do not give a clear indication of the presence of software bugs and thus limit the reliability of the test suites that contain them. A recent survey of software developers found that 59% claimed to deal with flaky tests on a monthly, weekly, or daily basis. As well as being detrimental to developers, flaky tests have also been shown to limit the applicability of useful techniques in software testing research. In general, one can think of flaky tests as being a threat to the validity of any methodology that assumes the outcome of a test only depends on the source code it covers. In this article, we systematically survey the body of literature relevant to flaky test research, amounting to 76 papers. We split our analysis into four parts: addressing the causes of flaky tests, their costs and consequences, detection strategies, and approaches for their mitigation and repair. Our findings and their implications have consequences for how the software-testing community deals with test flakiness, pertinent to practitioners and of interest to those wanting to familiarize themselves with the research area.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W3210114756",
    "type": "article"
  },
  {
    "title": "Developing Cost-Effective Blockchain-Powered Applications",
    "doi": "https://doi.org/10.1145/3431726",
    "publication_date": "2021-03-09",
    "publication_year": 2021,
    "authors": "Abdullah Ahmad Zarir; Gustavo A. Oliva; Zhen Ming Jiang; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "Ethereum is a blockchain platform that hosts and executes smart contracts. Executing a function of a smart contract burns a certain amount of gas units (a.k.a., gas usage). The total gas usage depends on how much computing power is necessary to carry out the execution of the function. Ethereum follows a free-market policy for deciding the transaction fee for executing a transaction. More specifically, transaction issuers choose how much they are willing to pay for each unit of gas (a.k.a., gas price). The final transaction fee corresponds to the gas price times the gas usage. Miners process transactions to gain mining rewards, which come directly from these transaction fees. The flexibility and the inherent complexity of the gas system pose challenges to the development of blockchain-powered applications. Developers of blockchain-powered applications need to translate requests received in the frontend of their application into one or more smart contract transactions. Yet, it is unclear how developers should set the gas parameters of these transactions given that (i) miners are free to prioritize transactions whichever way they wish and (ii) the gas usage of a contract transaction is only known after the transaction is processed and included in a new block. In this article, we analyze the gas usage of Ethereum transactions that were processed between Oct. 2017 and Feb. 2019 (the Byzantium era). We discover that (i) most miners prioritize transactions based on their gas price only, (ii) 25% of the functions that received at least 10 transactions have an unstable gas usage (coefficient of variation = 19%), and (iii) a simple prediction model that operates on the recent gas usage of a function achieves an R-Squared of 0.76 and a median absolute percentage error of 3.3%. We conclude that (i) blockchain-powered application developers should be aware that transaction prioritization in Ethereum is frequently done based solely on the gas price of transactions (e.g., a higher transaction fee does not necessarily imply a higher transaction priority) and act accordingly and (ii) blockchain-powered application developers can leverage gas usage prediction models similar to ours to make more informed decisions to set the gas price of their transactions. Lastly, based on our findings, we list and discuss promising avenues for future research.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W3134742627",
    "type": "article"
  },
  {
    "title": "Women’s Participation in Open Source Software: A Survey of the Literature",
    "doi": "https://doi.org/10.1145/3510460",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Bianca Trinkenreich; Igor Wiese; Anita Sarma; Marco Aurélio Gerosa; Igor Steinmacher",
    "corresponding_authors": "",
    "abstract": "Women are underrepresented in Open Source Software (OSS) projects, as a result of which, not only do women lose career and skill development opportunities, but the projects themselves suffer from a lack of diversity of perspectives. Practitioners and researchers need to understand more about the phenomenon; however, studies about women in open source are spread across multiple fields, including information systems, software engineering, and social science. This article systematically maps, aggregates, and synthesizes the state-of-the-art on women’s participation in OSS. It focuses on women contributors’ representation and demographics, how they contribute, their motivations and challenges, and strategies employed by communities to attract and retain women. We identified 51 articles (published between 2000 and 2021) that investigated women’s participation in OSS. We found evidence in these papers about who are the women who contribute, what motivates them to contribute, what types of contributions they make, challenges they face, and strategies proposed to support their participation. According to these studies, only about 5% of projects were reported to have women as core developers, and women authored less than 5% of pull-requests, but had similar or even higher rates of pull-request acceptances than men. Women make both code and non-code contributions, and their motivations to contribute include learning new skills, altruism, reciprocity, and kinship. Challenges that women face in OSS are mainly social, including lack of peer parity and non-inclusive communication from a toxic culture. We found 10 strategies reported in the literature, which we mapped to the reported challenges. Based on these results, we provide guidelines for future research and practice.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W3162948197",
    "type": "article"
  },
  {
    "title": "Deep Reinforcement Learning for Black-box Testing of Android Apps",
    "doi": "https://doi.org/10.1145/3502868",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Andrea Romdhana; Alessio Merlo; Mariano Ceccato; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "The state space of Android apps is huge and its thorough exploration during testing remains a major challenge. In fact, the best exploration strategy is highly dependent on the features of the app under test. Reinforcement Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and error, guided by positive or negative reward, rather than by explicit supervision. Deep RL is a recent extension of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL suitable for complex exploration spaces such as the one of Android apps. However, state of the art, publicly available tools only support basic, tabular RL. We have developed ARES, a Deep RL approach for black-box testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than the baselines, which include state of the art RL based tools, such as TimeMachine and Q-Testing. We also investigated qualitatively the reasons behind such performance and we have identified the key features of Android apps that make Deep RL particularly effective on them to be the presence of chained and blocking activities.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W3120672518",
    "type": "article"
  },
  {
    "title": "Accessibility in Software Practice: A Practitioner’s Perspective",
    "doi": "https://doi.org/10.1145/3503508",
    "publication_date": "2022-05-19",
    "publication_year": 2022,
    "authors": "Tingting Bi; Xin Xia; David Lo; John G. Grundy; Thomas Zimmermann; Denae Ford",
    "corresponding_authors": "",
    "abstract": "Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitioners’ viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W3136428280",
    "type": "article"
  },
  {
    "title": "Opinion Mining for Software Development: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3490388",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Bin Lin; Nathan Cassee; Alexander Serebrenik; Gabriele Bavota; Nicole Novielli; Michele Lanza",
    "corresponding_authors": "",
    "abstract": "Opinion mining, sometimes referred to as sentiment analysis, has gained increasing attention in software engineering (SE) studies. SE researchers have applied opinion mining techniques in various contexts, such as identifying developers’ emotions expressed in code comments and extracting users’ critics toward mobile apps. Given the large amount of relevant studies available, it can take considerable time for researchers and developers to figure out which approaches they can adopt in their own studies and what perils these approaches entail. We conducted a systematic literature review involving 185 papers. More specifically, we present (1) well-defined categories of opinion mining-related software development activities, (2) available opinion mining approaches, whether they are evaluated when adopted in other studies, and how their performance is compared, (3) available datasets for performance evaluation and tool customization, and (4) concerns or limitations SE researchers might need to take into account when applying/customizing these opinion mining techniques. The results of our study serve as references to choose suitable opinion mining tools for software development activities and provide critical insights for the further development of opinion mining techniques in the SE domain.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W3207231909",
    "type": "article"
  },
  {
    "title": "Context-Aware Code Change Embedding for Better Patch Correctness Assessment",
    "doi": "https://doi.org/10.1145/3505247",
    "publication_date": "2022-05-18",
    "publication_year": 2022,
    "authors": "Bo Lin; Shangwen Wang; Ming Wen; Xiaoguang Mao",
    "corresponding_authors": "",
    "abstract": "Despite the capability in successfully fixing more and more real-world bugs, existing Automated Program Repair (APR) techniques are still challenged by the long-standing overfitting problem (i.e., a generated patch that passes all tests is actually incorrect). Plenty of approaches have been proposed for automated patch correctness assessment (APCA ). Nonetheless, dynamic ones (i.e., those that needed to execute tests) are time-consuming while static ones (i.e., those built on top of static code features) are less precise. Therefore, embedding techniques have been proposed recently, which assess patch correctness via embedding token sequences extracted from the changed code of a generated patch. However, existing techniques rarely considered the context information and program structures of a generated patch, which are crucial for patch correctness assessment as revealed by existing studies. In this study, we explore the idea of context-aware code change embedding considering program structures for patch correctness assessment. Specifically, given a patch, we not only focus on the changed code but also take the correlated unchanged part into consideration, through which the context information can be extracted and leveraged. We then utilize the AST path technique for representation where the structure information from AST node can be captured. Finally, based on several pre-defined heuristics, we build a deep learning based classifier to predict the correctness of the patch. We implemented this idea as Cache and performed extensive experiments to assess its effectiveness. Our results demonstrate that Cache can (1) perform better than previous representation learning based techniques (e.g., Cache relatively outperforms existing techniques by \\( \\approx \\) 6%, \\( \\approx \\) 3%, and \\( \\approx \\) 16%, respectively under three diverse experiment settings), and (2) achieve overall higher performance than existing APCA techniques while even being more precise than certain dynamic ones including PATCH-SIM (92.9% vs. 83.0%). Further results reveal that the context information and program structures leveraged by Cache contributed significantly to its outstanding performance.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W4280528532",
    "type": "article"
  },
  {
    "title": "A Survey of Learning-based Automated Program Repair",
    "doi": "https://doi.org/10.1145/3631974",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Quanjun Zhang; Chunrong Fang; Yuxiang Ma; Weisong Sun; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance. In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: https://github.com/iSEngLab/AwesomeLearningAPR .",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W4388422146",
    "type": "article"
  },
  {
    "title": "A Theory of Scrum Team Effectiveness",
    "doi": "https://doi.org/10.1145/3571849",
    "publication_date": "2022-11-22",
    "publication_year": 2022,
    "authors": "Christiaan Verwijs; Daniel Russo",
    "corresponding_authors": "",
    "abstract": "Scrum teams are at the heart of the Scrum framework. Nevertheless, an integrated and systemic theory that can explain what makes some Scrum teams more effective than others is still missing. To address this gap, we performed a 7-year-long mixed-methods investigation composed of two main phases. First, we induced a theoretical model from 13 exploratory field studies. Our model proposes that the effectiveness of Scrum teams depends on five high-level factors (responsiveness, stakeholder concern, continuous improvement, team autonomy, and management support) and 13 lower-level factors. In the second phase of our study, we validated our model with a covariance-based structural equation modeling analysis using data from about 5,000 developers and 2,000 Scrum teams that we gathered with a custom-built survey. Results suggest a very good fit of the empirical data in our theoretical model (CFI = 0.959, RMSEA = 0.038, SRMR = 0.035). Accordingly, this research allowed us to (1) propose and validate a generalizable theory for effective Scrum teams and (2) formulate clear recommendations for how organizations can better support Scrum teams.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W3164595741",
    "type": "article"
  },
  {
    "title": "Testing RESTful APIs: A Survey",
    "doi": "https://doi.org/10.1145/3617175",
    "publication_date": "2023-08-21",
    "publication_year": 2023,
    "authors": "Amid Golmohammadi; Man Zhang; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "In industry, RESTful APIs are widely used to build modern Cloud Applications. Testing them is challenging, because not only do they rely on network communications, but also they deal with external services like databases. Therefore, there has been a large amount of research sprout in recent years on how to automatically verify this kind of web services. In this article, we present a comprehensive review of the current state-of-the-art in testing RESTful APIs based on the analysis of 92 scientific articles. These articles were gathered by utilizing search queries formulated around the concept of RESTful API testing on seven popular databases. We eliminated irrelevant articles based on our predefined criteria and conducted a snowballing phase to minimize the possibility of missing any relevant paper. This survey categorizes and summarizes the existing scientific work on testing RESTful APIs and discusses the current challenges in the verification of RESTful APIs. This survey clearly shows an increasing interest among researchers in this field, from 2017 onward. However, there are still a lot of open research challenges to overcome.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W4386027284",
    "type": "article"
  },
  {
    "title": "NPC: <u>N</u> euron <u>P</u> ath <u>C</u> overage via Characterizing Decision Logic of Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3490489",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Xiaofei Xie; Tianlin Li; Jian Wang; Lei Ma; Qing Guo; Felix Juefei-Xu; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be. In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W4210455774",
    "type": "article"
  },
  {
    "title": "Arachne: Search-Based Repair of Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3563210",
    "publication_date": "2022-09-13",
    "publication_year": 2022,
    "authors": "Jeongju Sohn; Sungmin Kang; Shin Yoo",
    "corresponding_authors": "",
    "abstract": "The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This paper introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses Differential Evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond Convolutional Neural Networks.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W4296592009",
    "type": "article"
  },
  {
    "title": "<i>FaaSLight</i> : General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing",
    "doi": "https://doi.org/10.1145/3585007",
    "publication_date": "2023-02-22",
    "publication_year": 2023,
    "authors": "Xuanzhe Liu; Jinfeng Wen; Zhenpeng Chen; Ding Li; Junkai Chen; Yi Liu; Haoyu Wang; Xin Jin",
    "corresponding_authors": "",
    "abstract": "Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency. In this article, we propose an approach, namely, FaaSLight , to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform - and language-agnostic . In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W4321495163",
    "type": "article"
  },
  {
    "title": "Toward Understanding Deep Learning Framework Bugs",
    "doi": "https://doi.org/10.1145/3587155",
    "publication_date": "2023-03-16",
    "publication_year": 2023,
    "authors": "Junjie Chen; Yihua Liang; Qingchao Shen; Jiajun Jiang; Shuochuan Li",
    "corresponding_authors": "",
    "abstract": "DL frameworks are the basis of constructing all DL programs and models, and thus their bugs could lead to the unexpected behaviors of any DL program or model relying on them. Such a wide effect demonstrates the necessity and importance of guaranteeing DL frameworks’ quality. Understanding the characteristics of DL framework bugs is a fundamental step for this quality assurance task, facilitating designing effective bug detection and debugging approaches. Hence, in this work, we conduct the most large-scale study on 1,000 bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch, MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework bugs associated with five components decomposed from DL frameworks, as well as measuring test coverage achieved by three state-of-the-art testing techniques, we obtain 12 major findings for the comprehensive understanding of DL framework bugs and the current status of existing DL framework testing practice, and then provide a series of actionable guidelines for better DL framework bug detection and debugging. Finally, based on the guidelines, we design and implement a prototype DL-framework testing tool, called TenFuzz , which is evaluated to be effective and finds three unknown bugs on the latest TensorFlow framework in a preliminary study, indicating the significance of our guidelines.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W4327594616",
    "type": "article"
  },
  {
    "title": "Security Misconfigurations in Open Source Kubernetes Manifests: An Empirical Study",
    "doi": "https://doi.org/10.1145/3579639",
    "publication_date": "2023-01-10",
    "publication_year": 2023,
    "authors": "Akond Rahman; Shazibul Islam Shamim; Dibyendu Brinto Bose; Rahul Pandita",
    "corresponding_authors": "",
    "abstract": "Context: Kubernetes has emerged as the de-facto tool for automated container orchestration. Business and government organizations are increasingly adopting Kubernetes for automated software deployments. Kubernetes is being used to provision applications in a wide range of domains, such as time series forecasting, edge computing, and high-performance computing. Due to such a pervasive presence, Kubernetes-related security misconfigurations can cause large-scale security breaches. Thus, a systematic analysis of security misconfigurations in Kubernetes manifests, i.e., configuration files used for Kubernetes, can help practitioners secure their Kubernetes clusters. Objective: The goal of this paper is to help practitioners secure their Kubernetes clusters by identifying security misconfigurations that occur in Kubernetes manifests . Methodology: We conduct an empirical study with 2,039 Kubernetes manifests mined from 92 open-source software repositories to systematically characterize security misconfigurations in Kubernetes manifests. We also construct a static analysis tool called Security Linter for Kubernetes Manifests ( SLI-KUBE ) to quantify the frequency of the identified security misconfigurations. Results: In all, we identify 11 categories of security misconfigurations, such as absent resource limit, absent securityContext , and activation of hostIPC . Specifically, we identify 1,051 security misconfigurations in 2,039 manifests. We also observe the identified security misconfigurations affect entities that perform mesh-related load balancing, as well as provision pods and stateful applications. Furthermore, practitioners agreed to fix 60% of 10 misconfigurations reported by us. Conclusion: Our empirical study shows Kubernetes manifests to include security misconfigurations, which necessitates security-focused code reviews and application of static analysis when Kubernetes manifests are developed.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4315480688",
    "type": "article"
  },
  {
    "title": "Ethics in the Age of AI: An Analysis of AI Practitioners’ Awareness and Challenges",
    "doi": "https://doi.org/10.1145/3635715",
    "publication_date": "2023-12-04",
    "publication_year": 2023,
    "authors": "Aastha Pant; Rashina Hoda; Simone V. Spiegler; Chakkrit Tantithamthavorn; Burak Turhan",
    "corresponding_authors": "",
    "abstract": "Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI—AI practitioners—have to say about their understanding of AI ethics and the challenges associated with incorporating it into the AI-based systems they develop? Understanding AI practitioners’ views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners’ awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners’ responses, our findings indicate that the majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies . Privacy protection and security was the ethical principle that the majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges, and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W4389307142",
    "type": "article"
  },
  {
    "title": "NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing",
    "doi": "https://doi.org/10.1145/3580598",
    "publication_date": "2023-03-31",
    "publication_year": 2023,
    "authors": "Shisong Qin; Fan Hu; Zheyu Ma; Bodong Zhao; Tingting Yin; Chao Zhang",
    "corresponding_authors": "",
    "abstract": "As an essential component responsible for communication, network services are security critical, thus, it is vital to find their vulnerabilities. Fuzzing is currently one of the most popular software vulnerability discovery techniques, widely adopted due to its high efficiency and low false positives. However, existing coverage-guided fuzzers mainly aim at stateless local applications, leaving stateful network services underexplored. Recently, some fuzzers targeting network services have been proposed but have certain limitations, for example, insufficient or inaccurate state representation and low testing efficiency. In this article, we propose a new fuzzing solution NSFuzz for stateful network services. We studied typical implementations of network service programs to determine how they represent states and interact with clients. Accordingly, we propose (1) a program variable–based state representation scheme and (2) an efficient interaction synchronization mechanism to improve fuzzing efficiency. We implemented a prototype of NSFuzz, which uses static analysis and annotation application programming interfaces (APIs) to identify synchronization points and state variables within the services. It then achieves fast I/O synchronization and accurate service state tracing to carry out efficient state-aware fuzzing via lightweight compile-time instrumentation. The evaluation results show that compared with other network service fuzzers, including AFL net and S tate AFL, our solution NSFuzz could infer a more accurate state model during fuzzing and improve fuzzing throughput by up to 200×. In addition, NSFuzz could improve code coverage by up to 25% and trigger more crashes in less time. We also performed a fuzzing campaign to find new bugs in the latest version of the target services; 8 zero-day vulnerabilities have been found by NSFuzz.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4362466275",
    "type": "article"
  },
  {
    "title": "Open Problems in Fuzzing RESTful APIs: A Comparison of Tools",
    "doi": "https://doi.org/10.1145/3597205",
    "publication_date": "2023-05-13",
    "publication_year": 2023,
    "authors": "Man Zhang; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "RESTful APIs are a type of web service that are widely used in industry. In the past few years, a lot of effort in the research community has been spent in designing novel techniques to automatically fuzz those APIs to find faults in them. Many real faults were automatically found in a large variety of RESTful APIs. However, usually the analyzed fuzzers treat the APIs as black-box, and no analysis of what is actually covered in these systems is done. Therefore, although these fuzzers are clearly useful for practitioners, we do not know their current limitations and actual effectiveness. Solving this is a necessary step to be able to design better, more efficient, and effective techniques. To address this issue, in this article we compare seven state-of-the-art fuzzers on 18 open source—1 industrial and 1 artificial—RESTful APIs. We then analyze the source code for which parts of these APIs the fuzzers fail to generate tests. This analysis points to clear limitations of these current fuzzers, listing concrete follow-up challenges for the research community.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4376504003",
    "type": "article"
  },
  {
    "title": "Asteria-Pro: Enhancing Deep Learning-based Binary Code Similarity Detection by Incorporating Domain Knowledge",
    "doi": "https://doi.org/10.1145/3604611",
    "publication_date": "2023-06-17",
    "publication_year": 2023,
    "authors": "Shouguo Yang; Chaopeng Dong; Yang Xiao; Yiran Cheng; Zhiqiang Shi; Zhi Li; Limin Sun",
    "corresponding_authors": "",
    "abstract": "Widespread code reuse allows vulnerabilities to proliferate among a vast variety of firmware. There is an urgent need to detect these vulnerable codes effectively and efficiently. By measuring code similarities, AI-based binary code similarity detection is applied to detecting vulnerable code at scale. Existing studies have proposed various function features to capture the commonality for similarity detection. Nevertheless, the significant code syntactic variability induced by the diversity of IoT hardware architectures diminishes the accuracy of binary code similarity detection. In our earlier study and the tool Asteria , we adopted a Tree-LSTM network to summarize function semantics as function commonality, and the evaluation result indicates an advanced performance. However, it still has utility concerns due to excessive time costs and inadequate precision while searching for large-scale firmware bugs. To this end, we propose a novel deep learning-enhancement architecture by incorporating domain knowledge-based pre-filtration and re-ranking modules, and we develop a prototype named Asteria-Pro based on Asteria . The pre-filtration module eliminates dissimilar functions, thus reducing the subsequent deep learning-model calculations. The re-ranking module boosts the rankings of vulnerable functions among candidates generated by the deep learning model. Our evaluation indicates that the pre-filtration module cuts the calculation time by 96.9%, and the re-ranking module improves MRR and Recall by 23.71% and 36.4%, respectively. By incorporating these modules, Asteria-Pro outperforms existing state-of-the-art approaches in the bug search task by a significant margin. Furthermore, our evaluation shows that embedding baseline methods with pre-filtration and re-ranking modules significantly improves their precision. We conduct a large-scale real-world firmware bug search, and Asteria-Pro manages to detect 1,482 vulnerable functions with a high precision 91.65%.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4381050433",
    "type": "article"
  },
  {
    "title": "Self-planning Code Generation with Large Language Models",
    "doi": "https://doi.org/10.1145/3672456",
    "publication_date": "2024-06-13",
    "publication_year": 2024,
    "authors": "Xue Jiang; Yihong Dong; L. F. Wang; Fang Zheng; Qiwei Shang; Ge Li; Zhi Jin; Wenpin Jiao",
    "corresponding_authors": "",
    "abstract": "Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4% in Pass@1 compared to direct code generation, and up to 11.9% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4399601909",
    "type": "article"
  },
  {
    "title": "Fairness Testing: A Comprehensive Survey and Analysis of Trends",
    "doi": "https://doi.org/10.1145/3652155",
    "publication_date": "2024-03-11",
    "publication_year": 2024,
    "authors": "Zhenpeng Chen; Jie M. Zhang; Max Hort; Mark Harman; Federica Sarro",
    "corresponding_authors": "",
    "abstract": "Unfair behaviors of Machine Learning (ML) software have garnered increasing attention and concern among software engineers. To tackle this issue, extensive research has been dedicated to conducting fairness testing of ML software, and this article offers a comprehensive survey of existing studies in this field. We collect 100 papers and organize them based on the testing workflow (i.e., how to test) and testing components (i.e., what to test). Furthermore, we analyze the research focus, trends, and promising directions in the realm of fairness testing. We also identify widely adopted datasets and open-source tools for fairness testing.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4392645489",
    "type": "article"
  },
  {
    "title": "Large Language Model for Vulnerability Detection and Repair: Literature Review and the Road Ahead",
    "doi": "https://doi.org/10.1145/3708522",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Xin Zhou; Sicong Cao; Xiaobing Sun; David Lo",
    "corresponding_authors": "",
    "abstract": "The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, encompassing 43 papers published across 25 distinct venues, along with 15 high-quality preprint papers, bringing the total to 58 papers. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of limitations of existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4405543707",
    "type": "article"
  },
  {
    "title": "Towards Enhancing Privacy-Preservation of a Federated Learning CNN Intrusion Detection System in IoT: Method and Empirical Study",
    "doi": "https://doi.org/10.1145/3695998",
    "publication_date": "2024-09-12",
    "publication_year": 2024,
    "authors": "Damiano Torre; Anitha Chennamaneni; J.K. Jo; Gitika Vyas; Brandon Sabrsula",
    "corresponding_authors": "",
    "abstract": "Enormous risks and hidden dangers of information security exist in the applications of Internet of Things (IoT) technologies. To secure IoT software systems, software engineers have to deploy advanced security software such as Intrusion Detection Systems (IDS) that are able to keep track of how the IoT devices behave within the network and detect any malicious activity that may be occurring. Considering that IoT devices generate large amounts of data, Artificial intelligence (AI) is often regarded as the best method for implementing IDS thanks to AI's high capability in processing large amounts of IoT data. To tackle these security concerns, specifically the ones tied to the privacy of data used in IoT systems, the software implementation of a Federated Learning (FL) method is often used to improve both privacy preservation (PP) and scalability in IoT networks. In this paper, we present a FL IDS that leverages a 1 Dimensional Convolutional Neural Network (CNN) for efficient and accurate intrusion detection in IoT networks. To address the critical issue of PP in FL, we incorporate three techniques: Differential Privacy, Diffie–Hellman Key Exchange, and Homomorphic Encryption. To evaluate the effectiveness of our solution, we conduct experiments on seven publicly available IoT datasets: TON IoT, IoT-23, Bot-IoT, CIC IoT 2023, CIC IoMT 2024, RT-IoT 2022, and EdgeIIoT. Our CNN-based approach achieves outstanding performance with an average accuracy, precision, recall, and F1-score of 97.31%, 95.59%, 92.43%, and 92.69%, respectively, across these datasets. These results demonstrate the effectiveness of our approach in accurately identifying and detecting intrusions in IoT networks. Furthermore, our experiments reveal that implementing all three PP techniques only incurs a minimal increase in computation time, with a 10% overhead compared to our solution without any PP mechanisms. This finding highlights the feasibility and efficiency of our solution in maintaining privacy while achieving high performance. Finally, we show the effectiveness of our solution through a comparison study with other recent IDS trained and tested on the same datasets we use.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4402483954",
    "type": "article"
  },
  {
    "title": "Communicating Study Design Trade-offs in Software Engineering",
    "doi": "https://doi.org/10.1145/3649598",
    "publication_date": "2024-03-02",
    "publication_year": 2024,
    "authors": "Martin P. Robillard; Deeksha M. Arya; Neil Ernst; Jin Guo; Maxime Lamothe; Mathieu Nassif; Nicole Novielli; Alexander Serebrenik; Igor Steinmacher; Klaas-Jan Stol",
    "corresponding_authors": "",
    "abstract": "Reflecting on the limitations of a study is a crucial part of the research process. In software engineering studies, this reflection is typically conveyed through discussions of study limitations or threats to validity. In current practice, such discussions seldom provide sufficient insight to understand the rationale for decisions taken before and during the study, and their implications. We revisit the practice of discussing study limitations and threats to validity and identify its weaknesses. We propose to refocus this practice of self-reflection to a discussion centered on the notion of trade-offs . We argue that documenting trade-offs allows researchers to clarify how the benefits of their study design decisions outweigh the costs of possible alternatives. We present guidelines for reporting trade-offs in a way that promotes a fair and dispassionate assessment of researchers’ work.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4392347631",
    "type": "article"
  },
  {
    "title": "Navigating the Complexity of Generative AI Adoption in Software Engineering - RCR Report",
    "doi": "https://doi.org/10.1145/3680471",
    "publication_date": "2024-07-23",
    "publication_year": 2024,
    "authors": "Daniel Russo",
    "corresponding_authors": "Daniel Russo",
    "abstract": "This Replicated Computational Results (RCR) report complements the study “Navigating the Complexity of Generative AI Adoption in Software Engineering,” which examines the factors influencing the integration of AI tools in software engineering practices. Employing a mixed-methods approach grounded in the Technology Acceptance Model, Diffusion of Innovation Theory, and Social Cognitive Theory, the study introduces the Human-AI Collaboration and Adaptation Framework (HACAF), validated through PLS-SEM analysis. The replication package detailed herein includes survey instruments, raw data, and analysis scripts essential for reproducing the study’s findings. By providing these artifacts, the RCR report aims to support transparency, enable replication, and encourage further research on effective AI tool adoption strategies in software engineering.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4400914462",
    "type": "article"
  },
  {
    "title": "On the Effectiveness of Large Language Models in Domain-Specific Code Generation",
    "doi": "https://doi.org/10.1145/3697012",
    "publication_date": "2024-10-01",
    "publication_year": 2024,
    "authors": "Xiaodong Gu; Meng Chen; Yalan Lin; Y. S. Hu; Hongyu Zhang; Chengcheng Wan; Zhao Wei; Yong Xu; Juhong Wang",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite significant achievements, they rely on enormous training data to acquire a broad spectrum of open-domain knowledge. Besides, their evaluation revolves around open-domain benchmarks like HumanEval, which primarily consist of programming contests. Therefore, it is hard to fully characterize the intricacies and challenges associated with particular domains (e.g., web, game, and math). In this paper, we conduct an in-depth study of the LLMs in domain-specific code generation. Our results demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. We further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, we further investigate how to effectively incorporate API knowledge into the code generation process. We experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these strategies as a new code generation approach called DomCoder . Experimental results show that all strategies of DomCoder improve the effectiveness of domain-specific code generation under certain settings.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4403007658",
    "type": "article"
  },
  {
    "title": "Towards Effective Detection of Ponzi schemes on Ethereum with Contract Runtime Behavior Graph",
    "doi": "https://doi.org/10.1145/3707458",
    "publication_date": "2024-12-10",
    "publication_year": 2024,
    "authors": "Ruichao Liang; Jing Chen; Cong Wu; Kun He; Yueming Wu; Weisong Sun; Ruiying Du; Qingchuan Zhao; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Ponzi schemes, a form of scam, have been discovered in Ethereum smart contracts in recent years, causing massive financial losses. Existing detection methods primarily focus on rule-based approaches and machine learning techniques that utilize static information as features. However, these methods have significant limitations. Rule-based approaches rely on pre-defined rules with limited capabilities and domain knowledge dependency. Using static information like opcodes for machine learning fails to effectively characterize Ponzi contracts, resulting in poor reliability and interpretability. Our research shows no significant difference between Ponzi and non-Ponzi contracts at the opcode level. Moreover, relying on static information like transactions for machine learning requires a certain number of transactions to achieve detection, which limits the scalability of detection and hinders the identification of 0-day Ponzi schemes. In this paper, we propose PonziGuard , an efficient Ponzi scheme detection approach based on contract runtime behavior. Inspired by the observation that a contract’s runtime behavior is more effective in disguising Ponzi contracts from the innocent contracts, PonziGuard establishes a comprehensive graph representation called contract runtime behavior graph (CRBG), to accurately depict the behavior of Ponzi contracts. Furthermore, it formulates the detection process as a graph classification task on CRBG, enhancing its overall effectiveness. The experiment results show that PonziGuard surpasses the current state-of-the-art approaches in the ground-truth dataset, achieving a precision of 96.9%, recall of 98.2%, and F1-score of 97.5%. It also exhibits the highest level of interpretability among the current tools. We applied PonziGuard to Ethereum Mainnet and demonstrated its effectiveness in real-world scenarios. Using PonziGuard , we identified 805 Ponzi contracts on Ethereum Mainnet, which have resulted in an estimated economic loss of 281,700 Ether or approximately $500 million USD. We also found 0-day Ponzi schemes in the recently deployed 10,000 smart contracts.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4405239636",
    "type": "article"
  },
  {
    "title": "LLM-Powered Static Binary Taint Analysis",
    "doi": "https://doi.org/10.1145/3711816",
    "publication_date": "2025-01-08",
    "publication_year": 2025,
    "authors": "Puzhuo Liu; C. P. Sun; Yaowen Zheng; Xuan Feng; Chuan Qin; Y. F. Wang; Zhenyang Xu; Zhi Li; Peng Di; Yu Jiang; Limin Sun",
    "corresponding_authors": "",
    "abstract": "This paper proposes LATTE, the first static binary taint analysis that is powered by a large language model (LLM). LATTE is superior to the state of the art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully automated while prior static binary taint analyzers need rely on human expertise to manually customize taint propagation rules and vulnerability inspection rules. Second, LATTE is significantly effective in vulnerability detection, demonstrated by our comprehensive evaluations. For example, LATTE has found 37 new bugs in real-world firmware, which the baselines failed to find. Moreover, 10 of them have been assigned CVE numbers. Lastly, LATTE incurs remarkably low engineering cost, making it a cost-efficient and scalable solution for security researchers and practitioners. We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4406153408",
    "type": "article"
  },
  {
    "title": "AI for DevSecOps: A Landscape and Future Opportunities",
    "doi": "https://doi.org/10.1145/3712190",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Michael C. Fu; Jirat Pasuksmit; Chakkrit Tantithamthavorn",
    "corresponding_authors": "",
    "abstract": "DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow. However, integrating security into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security. AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. They have the potential to reduce manual efforts and can be incorporated into DevOps practices to support consistent delivery speed while aligning with the principles of the DevSecOps paradigm. This paper seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 security tasks associated with the DevSecOps process and reviewed existing AI-driven security approaches, the problems they addressed, and the 65 benchmarks used to evaluate those approaches. Drawing insights from our findings, in RQ2, we discussed state-of-the-art AI-driven security approaches, highlighted 15 challenges in existing research, and proposed 15 corresponding avenues for future opportunities.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4406466792",
    "type": "article"
  },
  {
    "title": "The Current Challenges of Software Engineering in the Era of Large Language Models",
    "doi": "https://doi.org/10.1145/3712005",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Cuiyun Gao; Xing Hu; Shan Gao; Xin Xia; Zhi Jin",
    "corresponding_authors": "",
    "abstract": "With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities. The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement &amp; design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4406324010",
    "type": "article"
  },
  {
    "title": "Failure Diagnosis in Microservice Systems: A Comprehensive Survey and Analysis",
    "doi": "https://doi.org/10.1145/3715005",
    "publication_date": "2025-01-23",
    "publication_year": 2025,
    "authors": "Shenglin Zhang; Sibo Xia; Wenzhao Fan; Binpeng Shi; Xiao Xiong; Zhenyu Zhong; Minghua Ma; Yongqian Sun; Dan Pei",
    "corresponding_authors": "",
    "abstract": "Widely adopted for their scalability and flexibility, modern microservice systems present unique failure diagnosis challenges due to their independent deployment and dynamic interactions. This complexity can lead to cascading failures that negatively impact operational efficiency and user experience. Recognizing the critical role of fault diagnosis in improving the stability and reliability of microservice systems, researchers have conducted extensive studies and achieved a number of significant results. This survey provides an exhaustive review of 98 scientific papers from 2003 to the present, including a thorough examination and elucidation of the fundamental concepts, system architecture, and problem statement. It also includes a qualitative analysis of the dimensions, providing an in-depth discussion of current best practices and future directions, aiming to further its development and application. In addition, this survey compiles publicly available datasets, toolkits, and evaluation metrics to facilitate the selection and validation of techniques for practitioners.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4406738151",
    "type": "article"
  },
  {
    "title": "The Future of AI-Driven Software Engineering",
    "doi": "https://doi.org/10.1145/3715003",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Valerio Terragni; Annie Vella; Partha S. Roop; Kelly Blincoe",
    "corresponding_authors": "",
    "abstract": "A paradigm shift is underway in Software Engineering, with AI systems such as LLMs playing an increasingly important role in boosting software development productivity. This trend is anticipated to persist. In the next years, we expect a growing symbiotic partnership between human software developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this paper, we present our vision of the future of software development in an AI-driven world and explore the key challenges that our research community should address to realize this vision.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4406798194",
    "type": "article"
  },
  {
    "title": "Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation",
    "doi": "https://doi.org/10.1145/3714462",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Chong Wang; Jian Zhang; Yebo Feng; Tianlin Li; Weisong Sun; Yang Liu; Xin Peng",
    "corresponding_authors": "",
    "abstract": "Recent code large language models (LLMs) have shown promising performance in generating standalone functions. However, they face limitations in repository-level code generation due to their lack of awareness of repository-level dependencies ( e.g., user-defined attributes), resulting in dependency errors such as undefined-variable and no-member errors. In this work, we introduce ToolGen , an approach that integrates autocompletion tools into the code LLM generation process to address these dependencies. ToolGen comprises two main phases: Trigger Insertion and Model Fine-tuning (Offline), and Tool-integrated Code Generation (Online). During the offline phase, ToolGen augments functions within a given code corpus with a special mark token, indicating positions to trigger autocompletion tools. These augmented functions, along with their corresponding descriptions, are then used to fine-tune a selected code LLM. In the online phase, ToolGen iteratively generates functions by predicting tokens step-by-step using the fine-tuned LLM. Whenever a mark token is encountered, ToolGen invokes the autocompletion tool to suggest code completions and selects the most appropriate one through constrained greedy search. We conduct comprehensive experiments to evaluate ToolGen ’s effectiveness in repository-level code generation across three distinct code LLMs: CodeGPT, CodeT5, and CodeLlama. To facilitate this evaluation, we create a benchmark comprising 671 real-world code repositories and introduce two new dependency-based metrics: Dependency Coverage and Static Validity Rate . The results demonstrate that ToolGen significantly improves Dependency Coverage by 31.4% to 39.1% and Static Validity Rate by 44.9% to 57.7% across the three LLMs, while maintaining competitive or improved performance in widely recognized similarity metrics such as BLEU-4, CodeBLEU, Edit Similarity, and Exact Match. On the CoderEval dataset, ToolGen achieves improvements of 40.0% and 25.0% in test pass rate (Pass@1) for CodeT5 and CodeLlama, respectively, while maintaining the same pass rate for CodeGPT. ToolGen also demonstrates high efficiency in repository-level code generation, with latency ranging from 0.63 to 2.34 seconds for generating each function. Furthermore, our generalizability evaluation confirms ToolGen ’s consistent performance when applied to diverse code LLMs, encompassing various model architectures and scales.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4406866655",
    "type": "article"
  },
  {
    "title": "Software Engineering by and for Humans in an AI Era",
    "doi": "https://doi.org/10.1145/3715111",
    "publication_date": "2025-02-03",
    "publication_year": 2025,
    "authors": "Silvia Abrahão; John Grundy; Mauro Pezzè; Margaret‐Anne Storey; Damian A. Tamburri",
    "corresponding_authors": "",
    "abstract": "The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, artificial intelligence (AI), and autonomous systems. This roadmap paper explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle —spanning professional engineers, end users, and end-user developers —we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This paper provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4407084737",
    "type": "article"
  },
  {
    "title": "Bias Testing and Mitigation in LLM-based Code Generation",
    "doi": "https://doi.org/10.1145/3724117",
    "publication_date": "2025-03-18",
    "publication_year": 2025,
    "authors": "Dong Huang; Jie M. Zhang; Qingwen Bu; Xiaofei Xie; Junjie Chen; Heming Cui",
    "corresponding_authors": "",
    "abstract": "As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47% to 49.10% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88% to 4.79% for GPT-4) 1 .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4408562727",
    "type": "article"
  },
  {
    "title": "Research on WebAssembly Runtimes: A Survey",
    "doi": "https://doi.org/10.1145/3714465",
    "publication_date": "2025-01-23",
    "publication_year": 2025,
    "authors": "Yixuan Zhang; Mugeng Liu; Haoyu Wang; Yun Ma; Gang Huang; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "WebAssembly (abbreviated as Wasm) was initially introduced for the Web and quickly extended its reach into various domains beyond the Web. To create Wasm applications, developers can compile high-level programming languages into Wasm binaries or manually write the textual format of Wasm and translate it into Wasm binaries by the toolchain. Regardless of whether it is utilized within or outside the Web, the execution of Wasm binaries is supported by the Wasm runtime. Such a runtime provides a secure, memory-efficient, and sandboxed execution environment to execute Wasm binaries. This paper provides a comprehensive survey of research on Wasm runtimes with 103 collected research papers related to Wasm runtimes following the traditional systematic literature review process. It characterizes existing studies from two different angles, including the internal research of Wasm runtimes (Wasm runtime design, testing, and analysis) and the external research (applying Wasm runtimes to various domains). This paper also proposes future research directions about Wasm runtimes.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4406738193",
    "type": "article"
  },
  {
    "title": "Testing and Debugging Quantum Programs: The Road to 2030",
    "doi": "https://doi.org/10.1145/3715106",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Neilson Carlos Leite Ramalho; Higor Amario de Souza; Marcos Lordello Chaim",
    "corresponding_authors": "",
    "abstract": "Quantum computing has existed in the theoretical realm for several decades. Recently, quantum computing has re-emerged as a promising technology to solve problems that a classical computer could take hundreds of years to solve. However, there are challenges and opportunities for academics and practitioners regarding software engineering practices for testing and debugging quantum programs. This paper presents a roadmap for addressing these challenges, pointing out the existing gaps in the literature and suggesting research directions. We discuss the limitations caused by noise, the no-cloning theorem, the lack of a standard architecture for quantum computers, among others. Regarding testing, we highlight gaps and opportunities related to transpilation, mutation analysis, input states with hybrid interfaces, program analysis, and coverage. For debugging, we present the current strategies, including classical techniques applied to quantum programs, quantum-specific assertions, and quantum-related bug patterns. We introduce a conceptual model to illustrate concepts regarding the testing and debugging of quantum programs and the relationship between them. Those concepts are used to identify and discuss research challenges to cope with quantum programs through 2030, focusing on the interfaces between classical and quantum computing and on creating testing and debugging techniques that take advantage of the unique quantum computing characteristics.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4406798224",
    "type": "article"
  },
  {
    "title": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An Empirical Study",
    "doi": "https://doi.org/10.1145/3716848",
    "publication_date": "2025-02-12",
    "publication_year": 2025,
    "authors": "Yujia Fu; Peng Liang; Amjed Tahir; Zengyang Li; Mojtaba Shahin; Jixing Yu; Jinfu Chen",
    "corresponding_authors": "",
    "abstract": "Modern code generation tools utilizing AI models like Large Language Models (LLMs) have gained increased popularity due to their ability to produce functional code. However, their usage presents security challenges, often resulting in insecure code merging into the code base. Thus, evaluating the quality of generated code, especially its security, is crucial. While prior research explored various aspects of code generation, the focus on security has been limited, mostly examining code produced in controlled environments rather than open source development scenarios. To address this gap, we conducted an empirical study, analyzing code snippets generated by GitHub Copilot and two other AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub projects. Our analysis identified 733 snippets, revealing a high likelihood of security weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets affected. These issues span 43 Common Weakness Enumeration (CWE) categories, including significant ones like CWE-330: Use of Insufficiently Random Values , CWE-94: Improper Control of Generation of Code , and CWE-79: Cross-site Scripting . Notably, eight of those CWEs are among the 2023 CWE Top-25, highlighting their severity. We further examined using Copilot Chat to fix security issues in Copilot-generated code by providing Copilot Chat with warning messages from the static analysis tools, and up to 55.5% of the security issues can be fixed. We finally provide the suggestions for mitigating security issues in generated code.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4407392582",
    "type": "article"
  },
  {
    "title": "How Configurable is the Linux Kernel? Analyzing Two Decades of Feature-Model History",
    "doi": "https://doi.org/10.1145/3729423",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Elias Kuiter; Chico Sundermann; Thomas Thüm; Tobias Heß; Sebastian Krieter; Gunter Saake",
    "corresponding_authors": "",
    "abstract": "Today, the operating system Linux is widely used in diverse environments, as its kernel can be configured flexibly. In many configurable systems, managing such variability can be facilitated in all development phases with product-line analyses. These analyses often require knowledge about the system's features and their dependencies, which are documented in a feature model. Despite their potential, product-line analyses are rarely applied to the Linux kernel in practice, as its feature model still challenges scalability and accuracy of analyses. Unfortunately, these challenges also severely limit our knowledge about two fundamental metrics of the kernel's configurability, namely its number of features and configurations. We identify four key limitations in the literature related to the scalability, accuracy, and influence factors of these metrics, and, by extension, other product-line analyses: (1) Analysis results for the Linux kernel are not comparable, because relevant information is not reported; (2) there is no consensus on how to define features in Linux, which leads to flawed analysis results; (3) only few versions of the Linux kernel have ever been analyzed, none of which are recent; and (4) the kernel is perceived as complex, although we lack empirical evidence that supports this claim. In this paper, we address these limitations with a comprehensive, empirical study of the Linux kernel's configurability, which spans its feature model's entire history from 2002 to 2024. We address the above limitations as follows: (1) We characterize parameters that are relevant when reporting analysis results; (2) we propose and evaluate a novel definition of features in Linux as a standardization effort; (3) we contribute torte , a tool that analyzes arbitrary versions of the Linux kernel's feature model; and (4) we investigate the current and possible future configurability of the kernel on more than 3,000 feature-model versions. Based on our results, we highlight eleven major insights into the Linux kernel's configurability and make seven actionable recommendations for researchers and practitioners.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4409475865",
    "type": "article"
  },
  {
    "title": "Designing Adaptive User Interfaces for mHealth Applications Targeting Chronic Disease: A User-Centered Approach",
    "doi": "https://doi.org/10.1145/3731750",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Wei Wang; John Grundy; Hourieh Khalajzadeh; Anuradha Madugalla; Humphrey O. Obie",
    "corresponding_authors": "",
    "abstract": "Mobile Health (mHealth) applications have demonstrated considerable potential in supporting chronic disease self-management; however, they remain underutilized due to low engagement, limited accessibility, and poor long-term adherence. These issues are particularly prominent among users with chronic disease, whose needs and capabilities vary widely. To address this, Adaptive User Interfaces (AUIs) offer a dynamic solution by tailoring interface features to users’ preferences, health status, and contexts. This paper presents a two-stage study to develop and validate actionable AUI design guidelines for mHealth applications. In stage one , an AUI prototype was evaluated through focus groups, interviews, and a standalone survey, revealing key user challenges and preferences. These insights informed the creation of an initial set of guidelines. In stage two , the guidelines were refined based on feedback from 20 end users and evaluated by 43 software practitioners through two surveys. This process resulted in nine finalized guidelines. To assess real-world relevance, a case study of four mHealth applications was conducted, with findings supported by user reviews highlighting the utility of the guidelines in identifying critical adaptation issues. This study offers actionable, evidence-based guidelines that help software practitioners design AUI in mHealth to better support individuals managing chronic diseases.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4409797601",
    "type": "article"
  },
  {
    "title": "Multi-valued symbolic model-checking",
    "doi": "https://doi.org/10.1145/990010.990011",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Marsha Chećhik; Benet Devereux; Steve Easterbrook; Arie Gurfinkel",
    "corresponding_authors": "",
    "abstract": "This article introduces the concept of multi-valued model-checking and describes a multi-valued symbolic model-checker, ΧChek. Multi-valued model-checking is a generalization of classical model-checking, useful for analyzing models that contain uncertainty (lack of essential information) or inconsistency (contradictory information, often occurring when information is gathered from multiple sources). Multi-valued logics support the explicit modeling of uncertainty and disagreement by providing additional truth values in the logic.This article provides a theoretical basis for multi-valued model-checking and discusses some of its applications. A companion article [Chechik et al. 2002b] describes implementation issues in detail. The model-checker works for any member of a large class of multi-valued logics. Our modeling language is based on a generalization of Kripke structures, where both atomic propositions and transitions between states may take any of the truth values of a given multi-valued logic. Properties are expressed in ΧCTL, our multi-valued extension of the temporal logic CTL.We define the class of logics, present the theory of multi-valued sets and multi-valued relations used in our model-checking algorithm, and define the multi-valued extensions of CTL and Kripke structures. We explore the relationship between ΧCTL and CTL, and provide a symbolic model-checking algorithm for ΧCTL. We also address the use of fairness in multi-valued model-checking. Finally, we discuss some applications of the multi-valued model-checking approach.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W2034102118",
    "type": "article"
  },
  {
    "title": "In black and white",
    "doi": "https://doi.org/10.1145/287000.287004",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Huo Yan Chen; T.H. Tse; F. T. Chan; Tsong Yueh Chen",
    "corresponding_authors": "",
    "abstract": "Because of the growing importance of object-oriented programming, a number of testing strategies have been proposed. They are based either on pure black-box or white-box techniques. We propose in this article a methodology to integrate the black- and white-box techniques. The black-box technique is used to select test cases. The white-box technique is mainly applied to determine whether two objects resulting from the program execution of a test care are observationally equivalent. It is also used to select test cases in some situations. We define the concept of a fundamental pair as a pair of equivalent terms that are formed by replacing all the variables on both sides of an axiom by normal forms. We prove that an implementation is consistent with respect to all equivalent terms if and only if it is consistent with respect to all fundamental pairs. In other words, the testing coverage of fundamental pairs is as good as that of all possible term rewritings, and hence we need only concentrate on the testing of fundamental pairs. Our strategy is based on mathematical theorems. According to the strategy, we propose an algorithm for selecting a finite set of fundamental pairs as test cases. Given a pair of equivalent terms as a test case, we should then determine whether the objects that result from executing the implemented program are observationally equivalent. We prove, however, that the observational equivalence of objects cannot be determined using a finite set of observable contexts (which are operation sequences ending with an observer function) derived from any black-box technique. Hence we supplement our approach with a “relevant observable context” technique, which is a heuristic white-box technique to select a relevant finite subset of the set of observable contexts for determining the observational equivalence. The relevant observable contezxts are constructed from a data member relevance graph (DRG), which is an abstraction of the given implementation for a given specificatin. A semiautomatic tool hass been developed to support this technique.",
    "cited_by_count": 181,
    "openalex_id": "https://openalex.org/W2083411793",
    "type": "article"
  },
  {
    "title": "Static analysis to support the evolution of exception structure in object-oriented systems",
    "doi": "https://doi.org/10.1145/941566.941569",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Martin P. Robillard; Gail C. Murphy",
    "corresponding_authors": "",
    "abstract": "Exception-handling mechanisms in modern programming languages provide a means to help software developers build robust applications by separating the normal control flow of a program from the control flow of the program under exceptional situations. Separating the exceptional structure from the code associated with normal operations bears some consequences. One consequence is that developers wishing to improve the robustness of a program must figure out which exceptions, if any, can flow to a point in the program. Unfortunately, in large programs, this exceptional control flow can be difficult, if not impossible, to determine.In this article, we present a model that encapsulates the minimal concepts necessary for a developer to determine exception flow for object-oriented languages that define exceptions as objects. Using these concepts, we describe why exception-flow information is needed to build and evolve robust programs. We then describe Jex, a static analysis tool we have developed to provide exception-flow information for Java systems based on this model. The Jex tool provides a view of the actual exception types that might arise at different program points and of the handlers that are present. Use of this tool on a collection of Java library and application source code demonstrates that the approach can be helpful to support both local and global improvements to the exception-handling structure of a system.",
    "cited_by_count": 173,
    "openalex_id": "https://openalex.org/W2165688098",
    "type": "article"
  },
  {
    "title": "Reconciling environment integration and software evolution",
    "doi": "https://doi.org/10.1145/131736.131744",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Kevin J. Sullivan; David Notkin",
    "corresponding_authors": "",
    "abstract": "Common software design approaches complicate both tool integration and software evolution when applied in the development of integrated environments. We illustrate this by tracing the evolution of three different designs for a simple integrated environment as representative changes are made to the requirements. We present an approach that eases integration and evolution by preserving tool independence in the face of integration. We design tool integration relationships as separate components called mediators , and we design tools to implicitly invoke mediators that integrate them. Mediators separate tools from each other, while implicit invocation allows tools to remain independent of mediators. To enable the use of our approach on a range of platforms, we provide a formalized model and requirements for implicit invocation mechanisms. We apply this model both to analyze existing mechanisms and in the design of a mechanism for C++.",
    "cited_by_count": 165,
    "openalex_id": "https://openalex.org/W2149244483",
    "type": "article"
  },
  {
    "title": "Flexible consistency checking",
    "doi": "https://doi.org/10.1145/839268.839271",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Christian Nentwich; Wolfgang Emmerich; Anthony Finkelsteiin; Ernst Ellmer",
    "corresponding_authors": "",
    "abstract": "The problem of managing the consistency of heterogeneous, distributed software engineering documents is central to the development of large and complex systems. We show how this problem can be addressed using xlinkit, a lightweight framework for consistency checking that leverages standard Internet technologies. xlinkit provides flexibility, strong diagnostics, and support for distribution and document heterogeneity. We use xlinkit in a comprehensive case study that demonstrates how design, implementation and deployment information of an Enterprise JavaBeans system can be checked for consistency, and rechecked incrementally when changes are made.",
    "cited_by_count": 164,
    "openalex_id": "https://openalex.org/W1983421263",
    "type": "article"
  },
  {
    "title": "Fault classes and error detection capability of specification-based testing",
    "doi": "https://doi.org/10.1145/322993.322996",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "D. Richard Kuhn",
    "corresponding_authors": "D. Richard Kuhn",
    "abstract": "Some varieties of specification-based testing rely upon methods for generating test cases from predicates in a software specification. These methods derive various test conditions from logic expressions, with the aim of detecting different types of faults. Some authors have presented empirical results on the ability of specification-based test generation methods to detect failures. This article describes a method for cokmputing the conditions that must be covered by a test set for the test set to guarantee detection of the particular fault class. It is shown that there is a coverage hierarchy to fault classes that is consistent with, and may therefore explain, experimental results on fault-based testing. The method is also shown to be effective for computing MCDC-adequate tests.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W2073018571",
    "type": "article"
  },
  {
    "title": "Impact of software engineering research on the practice of software configuration management",
    "doi": "https://doi.org/10.1145/1101815.1101817",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "Jacky Estublier; David B. Leblang; André van der Hoek; Reidar Conradi; Geoffrey Clemm; Walter F. Tichy; Darcy Wiborg-Weber",
    "corresponding_authors": "",
    "abstract": "Software Configuration Management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger, more long lasting, and more mission and life critical. This article discusses the evolution of SCM technology from the early days of software development to the present, with a particular emphasis on the impact that university and industrial research has had along the way. Based on an analysis of the publication history and evolution in functionality of the available SCM systems, we trace the critical ideas in the field from their early inception to their eventual maturation in commercially and freely available SCM systems. In doing so, this article creates a detailed record of the critical value of SCM research and illustrates how research results have shaped the functionality of today's SCM systems.",
    "cited_by_count": 156,
    "openalex_id": "https://openalex.org/W2128939574",
    "type": "article"
  },
  {
    "title": "A methodology for testing spreadsheets",
    "doi": "https://doi.org/10.1145/366378.366385",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Gregg Rothermel; Margaret Burnett; Lixin Li; Christopher Dupuis; A. Sheretov",
    "corresponding_authors": "",
    "abstract": "Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults; thus, we would like to provide at least some of the benefits of formal testing methodologies to the creators of spreadsheets. This article presents a testing methodology that adapts data flow adequacy criteria and coverage monitoring to the task of testing spreadsheets. To accommodate the evaluation model used with spreadsheets, and the interactive process by which they are created, our methodology is incremental. To accommodate the users of spreadsheet languages, we provide an interface to our methodology that does not require an understanding of testing theory. We have implemented our testing methodology in the context of the Forms/3 visual spreadsheet language. We report on the methodology, its time and space costs, and the mapping from the testing strategy to the user interface. In an empirical study, we found that test suites created according to our methodology detected, on average, 81% of the faults in a set of faulty spreadsheets, significantly outperforming randomly generated test suites.",
    "cited_by_count": 152,
    "openalex_id": "https://openalex.org/W1999504357",
    "type": "article"
  },
  {
    "title": "Process modeling in Web applications",
    "doi": "https://doi.org/10.1145/1178625.1178627",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "Marco Brambilla; Stefano Ceri; Piero Fraternali; Ioana Manolescu",
    "corresponding_authors": "",
    "abstract": "While Web applications evolve towards ubiquitous, enterprise-wide or multienterprise information systems, they face new requirements, such as the capability of managing complex processes spanning multiple users and organizations, by interconnecting software provided by different organizations. Significant efforts are currently being invested in application integration, to support the composition of business processes of different companies, so as to create complex, multiparty business scenarios. In this setting, Web applications, which were originally conceived to allow the user-to-system dialogue, are extended with Web services, which enable system-to-system interaction, and with process control primitives, which permit the implementation of the required business constraints. This article presents new Web engineering methods for the high-level specification of applications featuring business processes and remote services invocation. Process- and service-enabled Web applications benefit from the high-level modeling and automatic code generation techniques that have been fruitfully applied to conventional Web applications, broadening the class of Web applications that take advantage of these powerful software engineering techniques. All the concepts presented in this article are fully implemented within a CASE tool.",
    "cited_by_count": 151,
    "openalex_id": "https://openalex.org/W1998638247",
    "type": "article"
  },
  {
    "title": "Delta algorithms",
    "doi": "https://doi.org/10.1145/279310.279321",
    "publication_date": "1998-04-01",
    "publication_year": 1998,
    "authors": "James J. Hunt; Kiem-Phong Vo; Walter F. Tichy",
    "corresponding_authors": "",
    "abstract": "Delta algorithms compress data by encoding one file in terms of another. This type of compression is useful in a number of situations: strong multiple versions of data, displaying differences, merging changes, distributing updates, storing backups, transmitting video sequences, and others. This article studies the performance parameters of several delta algorithms, using a benchmark of over 1,300 pairs of files taken from two successive releases of GNU software. Results indicate that modern delta compression algorithms based on Ziv-Lempel techniques significantly outperform diff , a popular but older delta compressor, in terms of compression ratio. The modern compressors also correlate better with the actual difference between files without sacrificing performance.",
    "cited_by_count": 151,
    "openalex_id": "https://openalex.org/W2050140640",
    "type": "article"
  },
  {
    "title": "An empirical study of static call graph extractors",
    "doi": "https://doi.org/10.1145/279310.279314",
    "publication_date": "1998-04-01",
    "publication_year": 1998,
    "authors": "Gail C. Murphy; David Notkin; William G. Griswold; Erica Lan",
    "corresponding_authors": "",
    "abstract": "Informally, a call graph represents calls between entities in a given program. The call graphs that compilers compute to determine the applicability of an optimization must typically be conservative: a call may be omitted only if it can never occur in any execution of the program. Numerous software engineering tools also extract call graphs with the expectation that they will help software engineers increase their understanding of a program. The requirements placed on software engineering tools that compute call graphs are typically more relaxed than for compilers. For example, some false negatives—calls that can in fact take place in some execution of the program, but which are omitted from the call graph—may be acceptable, depending on the understanding task at hand. In this article, we empirically show a consequence of this spectrum of requirements by comparing the C call graphs extracted from three software systems (mapmaker, mosaic, and gcc) by nine tools (cflow, cawk, CIA, Field, GCT, Imagix, LSME, Mawk, and Rigiparse). A quantitative analysis of the call graphs extracted for each system shows considerable variation, a result that is counterintuitive to many experienced software engineers. A qualitative analysis of these results reveals a number of reasons for this variation: differing treatments of macros, function pointers, input formats, etc. The fundamental problem is not that variances among the graphs extracted by different tools exist, but that software engineers have little sense of the dimensions of approximation in any particular call graph. In this article, we describe and discuss the study, sketch a design space for static call graph extractors, and discuss the impact of our study on practitioners, tool developers, and researchers. Although this article considers only one kind of information, call graphs, many of the observations also apply to static extractors of other kinds of information, such as inheritance structures, file dependences, and references to global variables.",
    "cited_by_count": 151,
    "openalex_id": "https://openalex.org/W2109427294",
    "type": "article"
  },
  {
    "title": "Parallel changes in large-scale software development",
    "doi": "https://doi.org/10.1145/383876.383878",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Dewayne E. Perry; Harvey Siy; Lawrence G. Votta",
    "corresponding_authors": "",
    "abstract": "An essential characteristic of large-scale software development is parallel development by teams of developers. How this parallel development is structured and supported has a profound effect on both the quality and timeliness of the product. We conduct an observational case study in which we collect and analyze the change and configuration management history of a legacy system to delineate the boundaries of, and to understand the nature of, the problems encountered in parallel development. The results of our studies are (1) that the degree of parallelism is very highhigher than considered by tool builders; (2) there are multiple levels of parallelism, and the data for some important aspects are uniform and consistent for all levels; (3) the tails of the distributions are long, indicating the tail, rather than the mean, must receive serious attention in providing solutions for these problems; and (4) there is a significant correlation between the degree of parallel work on a given component and the number of quality problems it has. Thus, the results of this study are important both for tool builders and for process and project engineers.",
    "cited_by_count": 150,
    "openalex_id": "https://openalex.org/W2154007683",
    "type": "article"
  },
  {
    "title": "Understanding the sources of variation in software inspections",
    "doi": "https://doi.org/10.1145/268411.268421",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Adam Porter; Harvey Siy; Audris Mockus; Lawrence G. Votta",
    "corresponding_authors": "",
    "abstract": "In a previous experiment, we determined how various changes in three structural elements of the software inspection process (team size and the number and sequencing of sessions) altered effectiveness and interval. Our results showed that such changes did not significantly influence the defect detection rate, but that certain combinations of changes dramatically increased the inspection interval. We also observed a large amount of unexplained variance in the data, indicating that other factors must be affecting inspection performance. The nature and extent of these other factors now have to be determined to ensure that they had not biased our earlier results. Also, identifying these other factors might suggest additional ways to improve the efficiency of inspections. Acting on the hypothesis that the “inputs” into the inspection process (reviewers, authors, and code units) were significant sources of variation, we modeled their effects on inspection performance. We found that they were responsible for much more variation in detect detection than was process structure. This leads us to conclude that better defect detection techniques, not better process structures, are the key to improving inspection effectiveness. The combined effects of process inputs and process structure on the inspection interval accounted for only a small percentage of the variance in inspection interval. Therefore, there must be other factors which need to be identified.",
    "cited_by_count": 149,
    "openalex_id": "https://openalex.org/W2169232226",
    "type": "article"
  },
  {
    "title": "SNIAFL",
    "doi": "https://doi.org/10.1145/1131421.1131424",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "Wei Zhao; Lu Zhang; Yin Liu; Jiasu Sun; Fuqing Yang",
    "corresponding_authors": "",
    "abstract": "To facilitate software maintenance and evolution, a helpful step is to locate features concerned in a particular maintenance task. In the literature, both dynamic and interactive approaches have been proposed for feature location. In this article, we present a static and noninteractive method for achieving this objective. The main idea of our approach is to use information retrieval (IR) technology to reveal the basic connections between features and computational units in the source code. Due to the imprecision of retrieved connections, we use a static representation of the source code named BRCG (branch-reserving call graph) to further recover both relevant and specific computational units for each feature. A premise of our approach is that programmers should use meaningful names as identifiers. We also performed an experimental study based on two real-world software systems to evaluate our approach. According to experimental results, our approach is quite effective in acquiring the relevant and specific computational units for most features.",
    "cited_by_count": 148,
    "openalex_id": "https://openalex.org/W2041853131",
    "type": "article"
  },
  {
    "title": "A formal model of services",
    "doi": "https://doi.org/10.1145/1189748.1189753",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Manfred Broy; Ingolf H. Krüger; Michael Meisinger",
    "corresponding_authors": "",
    "abstract": "Service-oriented software systems rapidly gain importance across application domains: They emphasize functionality (services), rather structural entities (components), as the basic building block for system composition. More specifically, services coordinate the interplay of components to accomplish specific tasks. In this article, we establish a foundation of service orientation: Based on the Focus theory of distributed systems (see Broy and Stølen [2001]), we introduce a theory and formal model of services. In Focus, systems are composed of interacting components. A component is a total behavior. We introduce a formal model of services where, in contrast, a service is a partial behavior. For services and components, we work out foundational specification techniques and outline methodological development steps. We show how services can be structured and how software architectures can be composed of services and components. Although our emphasis is on a theoretical foundation of the notion of services, we demonstrate utility of the concepts we introduce by means of a running example from the automotive domain.",
    "cited_by_count": 144,
    "openalex_id": "https://openalex.org/W2025850194",
    "type": "article"
  },
  {
    "title": "Verifying security protocols with Brutus",
    "doi": "https://doi.org/10.1145/363516.363528",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "E. M. Clarke; Somesh Jha; W. Marrero",
    "corresponding_authors": "",
    "abstract": "Due to the rapid growth of the “Internet” and the “World Wide Web” security has become a very important concern in the design and implementation of software systems. Since security has become an important issue, the number of protocols in this domain has become very large. These protocols are very diverse in nature. If a software architect wants to deploy some of these protocols in a system, they have to be sure that the protocol has the right properties as dictated by the requirements of the system. In this article we present BRUTUS, a tool for verifying properties of security protocols. This tool can be viewed as a special-purpose model checker for security protocols. We also present reduction techniques that make the tool efficient. Experimental results are provided to demonstrate the efficiency of BRUTUS.",
    "cited_by_count": 143,
    "openalex_id": "https://openalex.org/W2032577199",
    "type": "article"
  },
  {
    "title": "Efficient path conditions in dependence graphs for software safety analysis",
    "doi": "https://doi.org/10.1145/1178625.1178628",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "Gregor Snelting; Torsten Robschink; Jens Krinke",
    "corresponding_authors": "",
    "abstract": "A new method for software safety analysis is presented which uses program slicing and constraint solving to construct and analyze path conditions , conditions defined on a program's input variables which must hold for information flow between two points in a program. Path conditions are constructed from subgraphs of a program's dependence graph, specifically, slices and chops. The article describes how constraint solvers can be used to determine if a path condition is satisfiable and, if so, to construct a witness for a safety violation, such as an information flow from a program point at one security level to another program point at a different security level. Such a witness can prove useful in legal matters.The article reviews previous research on path conditions in program dependence graphs; presents new extensions of path conditions for arrays, pointers, abstract data types, and multithreaded programs; presents new decomposition formulae for path conditions; demonstrates how interval analysis and BDDs (binary decision diagrams) can be used to reduce the scalability problem for path conditions; and presents case studies illustrating the use of path conditions in safety analysis. Applying interval analysis and BDDs is shown to overcome the combinatorial explosion that can occur in constructing path conditions. Case studies and empirical data demonstrate the usefulness of path conditions for analyzing practical programs, in particular, how illegal influences on safety-critical programs can be discovered and analyzed.",
    "cited_by_count": 141,
    "openalex_id": "https://openalex.org/W2054383157",
    "type": "article"
  },
  {
    "title": "Reengineering of configurations based on mathematical concept analysis",
    "doi": "https://doi.org/10.1145/227607.227613",
    "publication_date": "1996-04-01",
    "publication_year": 1996,
    "authors": "Gregor Snelting",
    "corresponding_authors": "Gregor Snelting",
    "abstract": "article Free Access Share on Reengineering of configurations based on mathematical concept analysis Author: Gregor Snelting Technische Universitat Braunschweig Technische Universitat BraunschweigView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 5Issue 2April 1996 pp 146–189https://doi.org/10.1145/227607.227613Published:01 April 1996Publication History 95citation920DownloadsMetricsTotal Citations95Total Downloads920Last 12 Months24Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 140,
    "openalex_id": "https://openalex.org/W2129200997",
    "type": "article"
  },
  {
    "title": "Achieving extensibility through product-lines and domain-specific languages",
    "doi": "https://doi.org/10.1145/505145.505147",
    "publication_date": "2002-04-01",
    "publication_year": 2002,
    "authors": "Don Batory; Clay Johnson; Bob Macdonald; Dale von Heeder",
    "corresponding_authors": "",
    "abstract": "This is a case study in the use of product-line architectures (PLAs) and domain-specific languages (DSLs) to design an extensible command-and-control simulator for Army fire support. The reusable components of our PLA are layers or \"aspects\" whose addition or removal simultaneously impacts the source code of multiple objects in multiple, distributed programs. The complexity of our component specifications is substantially reduced by using a DSL for defining and refining state machines, abstractions that are fundamental to simulators. We present preliminary results that show how our PLA and DSL synergistically produce a more flexible way of implementing state-machine-based simulators than is possible with a pure Java implementation.",
    "cited_by_count": 139,
    "openalex_id": "https://openalex.org/W2010035000",
    "type": "article"
  },
  {
    "title": "Lightweight lexical source model extraction",
    "doi": "https://doi.org/10.1145/234426.234441",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Gail C. Murphy; David Notkin",
    "corresponding_authors": "",
    "abstract": "Software engineers maintaining an existing software system often depend on the mechanized extraction of information from system artifacts. Some useful kinds of information—source models—are well known: call graphs, file dependences, etc. Predicting every kind of source model that a software engineer may need is impossible. We have developed a lightweight approach for generating flexible and tolerant source model extractors from lexical specifications. The approach is lightweight in that the specifications are relatively small and easy to write. It is flexible in that there are few constraints on the kinds of artifacts from which source models are extracted (e.g., we can extract from source code, structured data files, documentation, etc.). It is tolerant in that there are few constraints on the condition of the artifacts. For example, we can extract from source that cannot necessarily be compiled. Our approach extended the kinds of source models that can be easily produced from lexical information while avoiding the constraints and brittleness of most parser-based approaches. We have developed tools to support this approach and applied the tools to the extraction of a number of different source models (file dependences, event interactions, call graphs) from a variety of system artifacts (C, C++, CLOS, Eiffel. TCL, structured data). We discuss our approach and describe its application to extract source models not available using existing systems; for example, we compute the implicitly-invokes relation over Field tools. We compare and contrast our approach to the conventional lexical and syntactic approaches of generating source models.",
    "cited_by_count": 136,
    "openalex_id": "https://openalex.org/W2112486798",
    "type": "article"
  },
  {
    "title": "Assessing process-centered software engineering environments",
    "doi": "https://doi.org/10.1145/258077.258080",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Vincenzo Ambriola; Reidar Conradi; Alfonso Fuggetta",
    "corresponding_authors": "",
    "abstract": "Process-centered software engineering environments (PSEEs) are the most recent generation of environments supporting software development activities. They exploit an representation of the process (called the process model that specifies how to carry out software development activities, the roles and tasks of software developers, and how to use and control software development tools. A process model is therefore a vehicle to better understand and communicate the process. If it is expressed in a formal notation, it can be used to support a variety of activities such as process analysis, process simulation, and process enactment. PSEEs provide automatic support for these activities. They exploit languages based on different paradigms, such as Petri nets and rule-based systems. They include facilities to edit and analyze process models. By enacting the process model, a PSEE provides a variety of services, such as assistance for software developers, automation of routine tasks, invocation and control of software development tools, and enforcement of mandatory rules and practices. Several PSEEs have been developed, both as research projects and as commercial products. The initial deployment and exploitation of this technology have made it possible to produce a significant amount of experiences, comments, evaluations, and feedback. We still lack, however, consistent and comprehensive assessment methods that can be used to collect and organize this information. This article aims at contributing to the definition of such methods, by providing a systematic comparison grid and by accomplishing an initial evaluation of the state of the art in the field. This evaluation takes into account the systems that have been developed by the authors in the past five years, as well as the main characteristics of other well-known environments",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2037751825",
    "type": "article"
  },
  {
    "title": "Experimental results from an automatic test case generator",
    "doi": "https://doi.org/10.1145/151257.151258",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Richard A. DeMillo; A. Jefferson Offutt",
    "corresponding_authors": "",
    "abstract": "Constraint-based testing is a novel way of generating test data to detect specific types of common programming faults. The conditions under which faults will be detected are encoded as mathematical systems of constraints in terms of program symbols. A set of tools, collectively called Godzilla, has been implemented that automatically generates constraint systems and solves them to create test cases for use by the Mothra testing system. Experimental results from using Godzilla show that the technique can produce test data that is very close in terms of mutation adequacy to test data that is produced manually, and at substantially reduced cost. Additionally, these experiments have suggested a new procedure for unit testing, where test cases are viewed as throw-away items rather than scarce resources.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2059204751",
    "type": "article"
  },
  {
    "title": "TACCLE",
    "doi": "https://doi.org/10.1145/366378.366380",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Huo Yan Chen; T.H. Tse; Tsong Yueh Chen",
    "corresponding_authors": "",
    "abstract": "Object-oriented programming consists of several different levels of abstraction, namely, the algorithmic level, class level, cluster level, and system level. The testing of object-oriented software at the algorithmic and system levels is similar to conventional program testing. Testing at the class and cluster levels poses new challenges. Since methods and objects may interact with one another with unforeseen combinations and invocations, they are much more complex to simulate and test than the hierarchy of functional calls in conventional programs. In this paper, we propose a methodology for object-oriented software testing at the class and cluster levels. In class-level testing, it is essential to determine whether objects produced from the execution of implemented systems would preserve the properties defined by the specification, such as behavioral equivalence and nonequivalence. Our class-level testing methodology addresses both of these aspects. For the testing of behavioral equivalence, we propose to select fundamental pairs of equivalent ground terms as test cases using a black-box technique based on algebraic specifications, and then determine by means of a white-box technique whether the objects resulting from executing such test cases are observationally equivalent. To address the testing of behavioral nonequivalence, we have identified and analyzed several nontrivial problems in the current literature. We propose to classify term equivalence into four types, thereby setting up new concepts and deriving important properties. Based on these results, we propose an approach to deal with the problems in the generation of nonequivalent ground terms as test cases. Relatively little research has contributed to cluster-level testing. In this paper, we also discuss black-box testing at the cluster level. We illustrate the feasibility of using contract, a formal specification language for the behavioral dependencies and interactions among cooperating objects of different classes in a given cluster. We propose an approach to test the interactions among different classes using every individual message-passing rule in the given Contract specification. We also present an approach to examine the interactions among composite message-passing sequences. We have developed four testing tools to support our methodology.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2171207016",
    "type": "article"
  },
  {
    "title": "An experimental study of fault detection in user requirements documents",
    "doi": "https://doi.org/10.1145/128894.128897",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "G. Schneider; Johnny Martin; Wei‐Tek Tsai",
    "corresponding_authors": "",
    "abstract": "This paper describes a software engineering experiment designed to confirm results from an earlier project which measured fault detection rates in user requirements documents (URD). The experiment described in this paper involves the creation of a standardized URD with a known number of injected faults of specific type. Nine independent inspection teams were given this URD with instructions to locate as many faults as possible using the N-fold requirements inspection technique developed by the authors. Results obtained from this experiment confirm earlier conclusions about the low rate of fault detection in requirements documents using formal inspections and the advantages to be gained using the N-fold inspection method. The experiment also provides new results concerning variability in inspection team performance and the relative difficulty of locating different classes of URD faults.",
    "cited_by_count": 133,
    "openalex_id": "https://openalex.org/W2046876961",
    "type": "article"
  },
  {
    "title": "Program integration for languages with procedure calls",
    "doi": "https://doi.org/10.1145/201055.201056",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "David Binkley; Susan Horwitz; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "Given a program Base and two variants, A and B, each created by modifying separate copies of Base, the goal of program integration is to determine whether the modifications interfere, and if they do not, to create an integrated program that incorporates both sets of changes as well as the portions of Base preserved in both variants. Text-based integration techniques, such as the one used by the Unix diff3 utility, are obviously unsatisfactory because one has no guarantees about how the execution behavior of the integrated program relates to the behaviors of Base, A, and B. The first program integration algorithm to provide such guarantees was developed by Horwitz, Prins, and Reps. However, a limitation of that algorithm is that it only applied to programs written in a restricted language—in particular, the algorithm does not handle programs with procedures. This article describes a generalization of the Horwitz-Prins-Reps algorithm that handles programs that consist of multiple (and possibly mutually recursive) procedures. We show that two straightforward generalizations of the Horwitz-Prins-Reps algorithm yield unsatisfactory results. The key issue in developing a satisfactory algorithm is how to take into account different calling contexts when determining what has changed in the variants A and B. Our solution to this problem involves identifying two different kinds of affected components of A and B: those affected regardless of how the procedure is called, and those affected by a changed or new calling context. The algorithm makes use of interprocedural program slicing to identify these components, as well as components in Base, A, and B with the same behavior.",
    "cited_by_count": 129,
    "openalex_id": "https://openalex.org/W2094944943",
    "type": "article"
  },
  {
    "title": "Metamodel-based model conformance and multiview consistency checking",
    "doi": "https://doi.org/10.1145/1243987.1243989",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "Richard F. Paige; Phillip J. Brooke; Jonathan S. Ostroff",
    "corresponding_authors": "",
    "abstract": "Model-driven development, using languages such as UML and BON, often makes use of multiple diagrams (e.g., class and sequence diagrams) when modeling systems. These diagrams, presenting different views of a system of interest, may be inconsistent. A metamodel provides a unifying framework in which to ensure and check consistency, while at the same time providing the means to distinguish between valid and invalid models, that is, conformance. Two formal specifications of the metamodel for an object-oriented modeling language are presented, and it is shown how to use these specifications for model conformance and multiview consistency checking. Comparisons are made in terms of completeness and the level of automation each provide for checking multiview consistency and model conformance. The lessons learned from applying formal techniques to the problems of metamodeling, model conformance, and multiview consistency checking are summarized.",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W2004901564",
    "type": "article"
  },
  {
    "title": "Amoeba",
    "doi": "https://doi.org/10.1145/1571629.1571632",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Nirmit Desai; Amit K. Chopra; Munindar P. Singh",
    "corresponding_authors": "",
    "abstract": "Business service engagements involve processes that extend across two or more autonomous organizations. Because of regulatory and competitive reasons, requirements for cross-organizational business processes often evolve in subtle ways. The changes may concern the business transactions supported by a process, the organizational structure of the parties participating in the process, or the contextual policies that apply to the process. Current business process modeling approaches handle such changes in an ad hoc manner, and lack a principled means for determining what needs to be changed and where. Cross-organizational settings exacerbate the shortcomings of traditional approaches because changes in one organization can potentially affect the workings of another. This article describes Amoeba, a methodology for business processes that is based on business protocols . Protocols capture the business meaning of interactions among autonomous parties via commitments. Amoeba includes guidelines for (1) specifying cross-organizational processes using business protocols, and (2) handling the evolution of requirements via a novel application of protocol composition. This article evaluates Amoeba using enhancements of a real-life business scenario of auto-insurance claim processing, and an aerospace case study.",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W2159351281",
    "type": "article"
  },
  {
    "title": "DSD-Crasher",
    "doi": "https://doi.org/10.1145/1348250.1348254",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Christoph Csallner; Yannis Smaragdakis; Tao Xie",
    "corresponding_authors": "",
    "abstract": "DSD-Crasher is a bug finding tool that follows a three-step approach to program analysis: D. Capture the program's intended execution behavior with dynamic invariant detection. The derived invariants exclude many unwanted values from the program's input domain. S. Statically analyze the program within the restricted input domain to explore many paths. D. Automatically generate test cases that focus on reproducing the predictions of the static analysis. Thereby confirmed results are feasible. This three-step approach yields benefits compared to past two-step combinations in the literature. In our evaluation with third-party applications, we demonstrate higher precision over tools that lack a dynamic step and higher efficiency over tools that lack a static step.",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W2024442685",
    "type": "article"
  },
  {
    "title": "Retrieving reusable software by sampling behavior",
    "doi": "https://doi.org/10.1145/152388.152392",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Andy Podgurski; Lynn Pierce",
    "corresponding_authors": "",
    "abstract": "A new method, called behavior sampling , is proposed for automated retrieval of reusable components from software libraries. Behavior sampling exploits the property of software that distinguished it from other forms of test: executability. Basic behavior sampling identifies relevant routines by executing candidates on a searcher-supplied sample of operational inputs and by comparing their output to output provided by the searcher. The probabilistic basis for behavior sampling is described, and experimental results are reported that suggest that basic behavior sampling exhibits high precision when used with small samples. Extensions to basic behavior sampling are proposed to improve its recall and to make it applicable to the retrieval of abstract data types and object classes.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2420370654",
    "type": "article"
  },
  {
    "title": "A reference architecture for the component factory",
    "doi": "https://doi.org/10.1145/125489.122823",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Victor R. Basili; Gianluigi Caldiera; Giovanni Cantone",
    "corresponding_authors": "",
    "abstract": "Software reuse can be achieved through an organization that focuses on utilization of life cycle products from previous developments. The component factory is both an example of the more general concepts of experience and domain factory and an organizational unit worth being considered independently. The critical features of such an organization are flexibility and continuous improvement. In order to achieve these features we can represent the architecture of the factory at different levels of abstraction and define a reference architecture from which specific architectures can be derived by instantiation. A reference architecture is an implementation and organization independent representation of the component factory and its environment. The paper outlines this reference architecture, discusses the instantiation process, and presents some examples of specific architectures by comparing them in the framework of the reference model.",
    "cited_by_count": 123,
    "openalex_id": "https://openalex.org/W2114091595",
    "type": "article"
  },
  {
    "title": "An extended fault class hierarchy for specification-based testing",
    "doi": "https://doi.org/10.1145/1072997.1072998",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Man Fai Lau; Yuen Tak Yu",
    "corresponding_authors": "",
    "abstract": "Kuhn, followed by Tsuchiya and Kikuno, have developed a hierarchy of relationships among several common types of faults (such as variable and expression faults) for specification-based testing by studying the corresponding fault detection conditions. Their analytical results can help explain the relative effectiveness of various fault-based testing techniques previously proposed in the literature. This article extends and complements their studies by analyzing the relationships between variable and literal faults, and among literal, operator, term, and expression faults. Our analysis is more comprehensive and produces a richer set of findings that interpret previous empirical results, can be applied to the design and evaluation of test methods, and inform the way that test cases should be prioritized for earlier detection of faults. Although this work originated from the detection of faults related to specifications, our results are equally applicable to program-based predicate testing that involves logic expressions.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2042788642",
    "type": "article"
  },
  {
    "title": "Evaluating the benefits of context-sensitive points-to analysis using a BDD-based implementation",
    "doi": "https://doi.org/10.1145/1391984.1391987",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Ondřej Lhoták; Laurie Hendren",
    "corresponding_authors": "",
    "abstract": "We present Paddle, a framework of BDD-based context-sensitive points-to and call graph analyses for Java, as well as client analyses that use their results. Paddle supports several variations of context-sensitive analyses, including call site strings and object sensitivity, and context-sensitively specializes both pointer variables and the heap abstraction. We empirically evaluate the precision of these context-sensitive analyses on significant Java programs. We find that that object-sensitive analyses are more precise than comparable variations of the other approaches, and that specializing the heap abstraction improves precision more than extending the length of context strings.",
    "cited_by_count": 115,
    "openalex_id": "https://openalex.org/W1987035533",
    "type": "article"
  },
  {
    "title": "Topology analysis of software dependencies",
    "doi": "https://doi.org/10.1145/13487689.13487691",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Martin P. Robillard",
    "corresponding_authors": "Martin P. Robillard",
    "abstract": "Before performing a modification task, a developer usually has to investigate the source code of a system to understand how to carry out the task. Discovering the code relevant to a change task is costly because it is a human activity whose success depends on a large number of unpredictable factors, such as intuition and luck. Although studies have shown that effective developers tend to explore a program by following structural dependencies, no methodology is available to guide their navigation through the thousands of dependency paths found in a nontrivial program. We describe a technique to automatically propose and rank program elements that are potentially interesting to a developer investigating source code. Our technique is based on an analysis of the topology of structural dependencies in a program. It takes as input a set of program elements of interest to a developer and produces a fuzzy set describing other elements of potential interest. Empirical evaluation of our technique indicates that it can help developers quickly select program elements worthy of investigation while avoiding less interesting ones.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2116357421",
    "type": "article"
  },
  {
    "title": "A Precise Method-Method Interaction-Based Cohesion Metric for Object-Oriented Classes",
    "doi": "https://doi.org/10.1145/2089116.2089118",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Jehad Al Dallal; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "The building of highly cohesive classes is an important objective in object-oriented design. Class cohesion refers to the relatedness of the class members, and it indicates one important aspect of the class design quality. A meaningful class cohesion metric helps object-oriented software developers detect class design weaknesses and refactor classes accordingly. Several class cohesion metrics have been proposed in the literature. Most of these metrics are applicable based on low-level design information such as attribute references in methods. Some of these metrics capture class cohesion by counting the number of method pairs that share common attributes. A few metrics measure cohesion more precisely by considering the degree of interaction, through attribute references, between each pair of methods. However, the formulas applied by these metrics to measure the degree of interaction cause the metrics to violate important mathematical properties, thus undermining their construct validity and leading to misleading cohesion measurement. In this paper, we propose a formula that precisely measures the degree of interaction between each pair of methods, and we use it as a basis to introduce a low-level design class cohesion metric (LSCC). We verify that the proposed formula does not cause the metric to violate important mathematical properties. In addition, we provide a mechanism to use this metric as a useful indicator for refactoring weakly cohesive classes, thus showing its usefulness in improving class cohesion. Finally, we empirically validate LSCC. Using four open source software systems and eleven cohesion metrics, we investigate the relationship between LSCC, other cohesion metrics, and fault occurrences in classes. Our results show that LSCC is one of three metrics that explains more accurately the presence of faults in classes. LSCC is the only one among the three metrics to comply with important mathematical properties, and statistical analysis shows it captures a measurement dimension of its own. This suggests that LSCC is a better alternative, when taking into account both theoretical and empirical results, as a measure to guide the refactoring of classes. From a more general standpoint, the results suggest that class quality, as measured in terms of fault occurrences, can be more accurately explained by cohesion metrics that account for the degree of interaction between each pair of methods.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2014412501",
    "type": "article"
  },
  {
    "title": "How Well Do Search Engines Support Code Retrieval on the Web?",
    "doi": "https://doi.org/10.1145/2063239.2063243",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Susan Elliott Sim; Medha Umarji; Sukanya Ratanotayanon; Cristina Videira Lopes",
    "corresponding_authors": "",
    "abstract": "Software developers search the Web for various kinds of source code for diverse reasons. In a previous study, we found that searches varied along two dimensions: the size of the search target (e.g., block, subsystem, or system) and the motivation for the search (e.g., reference example or as-is reuse). Would each of these kinds of searches require different search technologies? To answer this question, we conducted an experiment with 36 participants to evaluate three diverse approaches (general purpose information retrieval, source code search, and component reuse), as represented by five Web sites (Google, Koders, Krugle, Google Code Search, and SourceForge). The independent variables were search engine, size of search target, and motivation for search. The dependent variable was the participants judgement of the relevance of the first ten hits. We found that it was easier to find reference examples than components for as-is reuse and that participants obtained the best results using a general-purpose information retrieval site. However, we also found an interaction effect: code-specific search engines worked better in searches for subsystems, but Google worked better on searches for blocks. These results can be used to guide the creation of new tools for retrieving source code from the Web.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W1993786160",
    "type": "article"
  },
  {
    "title": "J-Orchestra",
    "doi": "https://doi.org/10.1145/1555392.1555394",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Eli Tilevich; Yannis Smaragdakis",
    "corresponding_authors": "",
    "abstract": "J-Orchestra is a system that enhances centralized Java programs with distribution capabilities. Operating at the bytecode level, J-Orchestra transforms a centralized Java program (i.e., running on a single Java Virtual Machine (JVM)) into a distributed one (i.e., running across multiple JVMs). This transformation effectively separates distribution concerns from the core functionality of a program. J-Orchestra follows a semiautomatic transformation process. Through a GUI, the user selects program elements (at class granularity) and assigns them to network locations. Based on the user's input, the J-Orchestra backend automatically partitions the program through compiler-level techniques, without changes to the JVM or to the Java Runtime Environment (JRE) classes. By means of bytecode engineering and code generation, J-Orchestra substitutes method calls with remote method calls, direct object references with proxy references, etc. It also translates Java language features (e.g., static methods and fields, inheritance, inner classes, new object construction, etc.) for efficient distributed execution. We detail the main technical issues that J-Orchestra addresses, including its mechanism for program transformation in the presence of unmodifiable code (e.g., in JRE classes) and the translation of concurrency and synchronization constructs to work correctly over the network. We further discuss a case study of transforming a large, commercial, third-party application for efficient execution in a client server environment and outline the architectural characteristics of centralized programs that are amenable to automated distribution with J-Orchestra.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W1997642340",
    "type": "article"
  },
  {
    "title": "The small-world effect",
    "doi": "https://doi.org/10.1145/1824760.1824763",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Param Vir Singh",
    "corresponding_authors": "Param Vir Singh",
    "abstract": "In this study we investigate the impact of community-level networks—relationships that exist among developers in an OSS community—on the productivity of member developers. Specifically, we argue that OSS community networks characterized by small-world properties would positively influence the productivity of the member developers by providing them with speedy and reliable access to more quantity and variety of information and knowledge resources. Specific hypotheses are developed and tested using longitudinal data on a large panel of 4,279 projects from 15 different OSS communities hosted at Sourceforge. Our results suggest that significant variation exists in small-world properties of OSS communities at Sourceforge. After accounting for project, foundry, and time-specific observed and unobserved effects, we found a statistically significant relationship between small-world properties of a community and the technical and commercial success of the software produced by its members. In contrast to the findings of prior research, we also found the lack of a significant relationship between closeness and betweenness centralities of the project teams and their success. These results were robust to a number of controls and model specifications.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W1995252666",
    "type": "article"
  },
  {
    "title": "Functional size measurement revisited",
    "doi": "https://doi.org/10.1145/1363102.1363106",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Çiğdem Gencel; Onur Demirörs",
    "corresponding_authors": "",
    "abstract": "There are various approaches to software size measurement. Among these, the metrics and methods based on measuring the functionality attribute have become widely used since the original method was introduced in 1979. Although functional size measurement methods have gone a long way, they still provide challenges for software managers. This article identifies improvement opportunities based on empirical studies we performed on ongoing projects. We also compare our findings with the extended dataset provided by the International Software Benchmarking Standards Group (ISBSG).",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2044388715",
    "type": "article"
  },
  {
    "title": "Personalized Reliability Prediction of Web Services",
    "doi": "https://doi.org/10.1145/2430545.2430548",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Zibin Zheng; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Service Oriented Architecture (SOA) is a business-centric IT architectural approach for building distributed systems. Reliability of service-oriented systems heavily depends on the remote Web services as well as the unpredictable Internet connections. Designing efficient and effective reliability prediction approaches of Web services has become an important research issue. In this article, we propose two personalized reliability prediction approaches of Web services, that is, neighborhood-based approach and model-based approach. The neighborhood-based approach employs past failure data of similar neighbors (either service users or Web services) to predict the Web service reliability. On the other hand, the model-based approach fits a factor model based on the available Web service failure data and use this factor model to make further reliability prediction. Extensive experiments are conducted with our real-world Web service datasets, which include about 23 millions invocation results on more than 3,000 real-world Web services. The experimental results show that our proposed reliability prediction approaches obtain better reliability prediction accuracy than other competing approaches.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W1965310012",
    "type": "article"
  },
  {
    "title": "Breaking up is hard to do",
    "doi": "https://doi.org/10.1145/1348250.1348253",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Jamieson M. Cobleigh; George S. Avrunin; Lori A. Clarke",
    "corresponding_authors": "",
    "abstract": "Finite-state verification techniques are often hampered by the state-explosion problem. One proposed approach for addressing this problem is assume-guarantee reasoning, where a system under analysis is partitioned into subsystems and these subsystems are analyzed individually. By composing the results of these analyses, it can be determined whether or not the system satisfies a property. Because each subsystem is smaller than the whole system, analyzing each subsystem individually may reduce the overall cost of verification. Often the behavior of a subsystem is dependent on the subsystems with which it interacts, and thus it is usually necessary to provide assumptions about the environment in which a subsystem executes. Because developing assumptions has been a difficult manual task, the evaluation of assume-guarantee reasoning has been limited. Using recent advances for automatically generating assumptions, we undertook a study to determine if assume-guarantee reasoning provides an advantage over monolithic verification. In this study, we considered all two-way decompositions for a set of systems and properties, using two different verifiers, FLAVERS and LTSA. By increasing the number of repeated tasks in these systems, we evaluated the decompositions as they were scaled. We found that in only a few cases can assume-guarantee reasoning verify properties on larger systems than monolithic verification can, and in these cases the systems that can be analyzed are only a few sizes larger. Although these results are discouraging, they provide insight about research directions that should be pursued and highlight the importance of experimental evaluation in this area.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2064880497",
    "type": "article"
  },
  {
    "title": "Fault localization prioritization",
    "doi": "https://doi.org/10.1145/2491509.2491513",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Shin Yoo; Mark Harman; David Clark",
    "corresponding_authors": "",
    "abstract": "Test case prioritization techniques seek to maximize early fault detection. Fault localization seeks to use test cases already executed to help find the fault location. There is a natural interplay between the two techniques; once a fault is detected, we often switch focus to fault fixing, for which localization may be a first step. In this article we introduce the Fault Localization Prioritization (FLP) problem, which combines prioritization and localization. We evaluate three techniques: a novel FLP technique based on information theory, FLINT (Fault Localization using INformation Theory), that we introduce in this article, a standard Test Case Prioritization (TCP) technique, and a “test similarity technique” used in previous work. Our evaluation uses five different releases of four software systems. The results indicate that FLP and TCP can statistically significantly reduce fault localization costs for 73% and 76% of cases, respectively, and that FLINT significantly outperforms similarity-based localization techniques in 52% of the cases considered in the study.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W2050238751",
    "type": "article"
  },
  {
    "title": "Recommending Adaptive Changes for Framework Evolution",
    "doi": "https://doi.org/10.1145/2000799.2000805",
    "publication_date": "2011-09-01",
    "publication_year": 2011,
    "authors": "Barthélémy Dagenais; Martin P. Robillard",
    "corresponding_authors": "",
    "abstract": "In the course of a framework’s evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework’s evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework was adapted to its own changes. In a study of the evolution of one open source framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision. In a second study of the evolution of two frameworks, we found that related change detection approaches were better at discovering systematic changes and that SemDiff was complementary to these approaches by detecting non-trivial changes such as when a functionality is imported from an external library.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2133363731",
    "type": "article"
  },
  {
    "title": "Concept location using formal concept analysis and information retrieval",
    "doi": "https://doi.org/10.1145/2377656.2377660",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Denys Poshyvanyk; Malcom Gethers; Andrian Marcus",
    "corresponding_authors": "",
    "abstract": "The article addresses the problem of concept location in source code by proposing an approach that combines Formal Concept Analysis and Information Retrieval. In the proposed approach, Latent Semantic Indexing, an advanced Information Retrieval approach, is used to map textual descriptions of software features or bug reports to relevant parts of the source code, presented as a ranked list of source code elements. Given the ranked list, the approach selects the most relevant attributes from the best ranked documents, clusters the results, and presents them as a concept lattice, generated using Formal Concept Analysis. The approach is evaluated through a large case study on concept location in the source code on six open-source systems, using several hundred features and bugs. The empirical study focuses on the analysis of various configurations of the generated concept lattices and the results indicate that our approach is effective in organizing different concepts and their relationships present in the subset of the search results. In consequence, the proposed concept location method has been shown to outperform a standalone Information Retrieval based concept location technique by reducing the number of irrelevant search results across all the systems and lattice configurations evaluated, potentially reducing the programmers' effort during software maintenance tasks involving concept location.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W1996901220",
    "type": "article"
  },
  {
    "title": "Does Automated Unit Test Generation Really Help Software Testers? A Controlled Empirical Study",
    "doi": "https://doi.org/10.1145/2699688",
    "publication_date": "2015-09-02",
    "publication_year": 2015,
    "authors": "Gordon Fraser; Matt Staats; Phil McMinn; Andrea Arcuri; Frank Padberg",
    "corresponding_authors": "",
    "abstract": "Work on automated test generation has produced several tools capable of generating test data which achieves high structural coverage over a program. In the absence of a specification, developers are expected to manually construct or verify the test oracle for each test input. Nevertheless, it is assumed that these generated tests ease the task of testing for the developer, as testing is reduced to checking the results of tests. While this assumption has persisted for decades, there has been no conclusive evidence to date confirming it. However, the limited adoption in industry indicates this assumption may not be correct, and calls into question the practical value of test generation tools. To investigate this issue, we performed two controlled experiments comparing a total of 97 subjects split between writing tests manually and writing tests with the aid of an automated unit test generation tool, E vo S uite . We found that, on one hand, tool support leads to clear improvements in commonly applied quality metrics such as code coverage (up to 300% increase). However, on the other hand, there was no measurable improvement in the number of bugs actually found by developers. Our results not only cast some doubt on how the research community evaluates test generation tools, but also point to improvements and future work necessary before automated test generation tools will be widely adopted by practitioners.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2056514427",
    "type": "article"
  },
  {
    "title": "aToucan",
    "doi": "https://doi.org/10.1145/2699697",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Tao Yue; Lionel Briand; Yvan Labiche",
    "corresponding_authors": "",
    "abstract": "The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model comprising class, sequence and activity diagrams from a use case model and to automatically establish traceability links between model elements of the use case model and the generated analysis model. Note that our goal is to save effort through automated support, not to replace human abstraction and decision making. Seven (six) case studies were performed to compare class (sequence) diagrams generated by aToucan to the ones created by experts, Masters students, and trained, fourth-year undergraduate students. Results show that aToucan performs well regarding consistency (e.g., 88% class diagram consistency) and completeness (e.g., 80% class completeness) when comparing generated class diagrams with reference class diagrams created by experts and Masters students. Similarly, sequence diagrams automatically generated by aToucan are highly consistent with the ones devised by experts and are also rather complete, for instance, 91% and 97% message consistency and completeness, respectively. Further, statistical tests show that aToucan significantly outperforms fourth-year engineering students in this respect, thus demonstrating the value of automation. We also conducted two industrial case studies demonstrating the applicability of aToucan in two different industrial domains. Results showed that the vast majority of model elements generated by aToucan are correct and that therefore, in practice, such models would be good initial models to refine and augment so as to converge towards to correct and complete analysis models. A performance analysis shows that the execution time of aToucan (when generating class and sequence diagrams) is dependent on the number of simple sentences contained in the use case model and remains within a range of a few minutes. Five different software system descriptions (18 use cases altogether) were performed to evaluate the generation of activity diagrams. Results show that aToucan can generate 100% complete and correct control flow information of activity diagrams and on average 85% data flAow information completeness. Moreover, we show that aToucan outperforms three commercial tools in terms of activity diagram generation.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W1794343297",
    "type": "article"
  },
  {
    "title": "Systematizing pragmatic software reuse",
    "doi": "https://doi.org/10.1145/2377656.2377657",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Reid Holmes; Robert J. Walker",
    "corresponding_authors": "",
    "abstract": "Many software reuse tasks involve reusing source code that was not designed in a manner conducive to those tasks, requiring that ad hoc modifications be applied. Such pragmatic reuse tasks are a reality in disciplined industrial practice; they arise for a variety of organizational and technical reasons. To investigate a pragmatic reuse task, a developer must navigate through, and reason about, source code dependencies in order to identify program elements that are relevant to the task and to decide how those elements should be reused. The developer must then convert his mental model of the task into a set of actions that he can perform. These steps are poorly supported by modern development tools and practices. We provide a model for the process involved in performing a pragmatic reuse task, including the need to capture (mentally or otherwise) the developer's decisions about how each program element should be treated: this is a pragmatic-reuse plan . We provide partial support for this model via a tool suite, called Gilligan; other parts of the model are supported via standard IDE tools. Using a pragmatic-reuse plan, Gilligan can semiautomatically transform the selected source code from its originating system and integrate it into the developer's system. We have evaluated Gilligan through a series of case studies and experiments (each involving industrial developers) using a variety of source systems and tasks; we report in particular on a previously unpublished, formal experiment. The results show that pragmatic-reuse plans are a robust metaphor for capturing pragmatic reuse intent and that, relative to standard IDE tools, Gilligan can (1) significantly decrease the time that developers require to perform pragmatic reuse tasks, (2) increase the likelihood that developers will successfully complete pragmatic reuse tasks, (3) decrease the time required by developers to identify infeasible reuse tasks, and (4) improve developers' sense of their ability to manage the risk in such tasks.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2021538299",
    "type": "article"
  },
  {
    "title": "Prevalence of coincidental correctness and mitigation of its impact on fault localization",
    "doi": "https://doi.org/10.1145/2559932",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Wes Masri; Rawad Abou Assi",
    "corresponding_authors": "",
    "abstract": "Researchers have argued that for failure to be observed the following three conditions must be met: C R = the defect was reached; C I = the program has transitioned into an infectious state; and C P = the infection has propagated to the output. Coincidental Correctness (CC) arises when the program produces the correct output while condition C R is met but not C P . We recognize two forms of coincidental correctness, weak and strong. In weak CC , C R is met, whereas C I might or might not be met, whereas in strong CC , both C R and C I are met. In this work we first show that CC is prevalent in both of its forms and demonstrate that it is a safety reducing factor for Coverage-Based Fault Localization (CBFL). We then propose two techniques for cleansing test suites from coincidental correctness to enhance CBFL, given that the test cases have already been classified as failing or passing. We evaluated the effectiveness of our techniques by empirically quantifying their accuracy in identifying weak CC tests. The results were promising, for example, the better performing technique, using 105 test suites and statement coverage, exhibited 9% false negatives, 30% false positives, and no false negatives nor false positives in 14.3% of the test suites. Also using 73 test suites and more complex coverage, the numbers were 12%, 19%, and 15%, respectively.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2065201438",
    "type": "article"
  },
  {
    "title": "Code-Smell Detection as a Bilevel Problem",
    "doi": "https://doi.org/10.1145/2675067",
    "publication_date": "2014-10-14",
    "publication_year": 2014,
    "authors": "Dilan Sahin; Marouane Kessentini; Slim Bechikh; Kalyanmoy Deb",
    "corresponding_authors": "",
    "abstract": "Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code-smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open-source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86% in terms of precision and recall. The results confirm the outperformance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2008593255",
    "type": "article"
  },
  {
    "title": "Boa",
    "doi": "https://doi.org/10.1145/2803171",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Robert Dyer; Hoan Anh Nguyen; Hridesh Rajan; Tien N. Nguyen",
    "corresponding_authors": "",
    "abstract": "In today's software-centric world, ultra-large-scale software repositories, such as SourceForge, GitHub, and Google Code, are the new library of Alexandria. They contain an enormous corpus of software and related information. Scientists and engineers alike are interested in analyzing this wealth of information. However, systematic extraction and analysis of relevant data from these repositories for testing hypotheses is hard, and best left for mining software repository (MSR) experts! Specifically, mining source code yields significant insights into software development artifacts and processes. Unfortunately, mining source code at a large scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse grained, or sacrifice studying the history of the code. In this article we address mining source code: (a) at a very large scale; (b) at a fine-grained level of detail; and (c) with full history information. To address these challenges, we present domain-specific language features for source-code mining in our language and infrastructure called Boa . The goal of Boa is to ease testing MSR-related hypotheses. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also show drastic improvements in scalability.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W821933395",
    "type": "article"
  },
  {
    "title": "Using Cohesion and Coupling for Software Remodularization",
    "doi": "https://doi.org/10.1145/2928268",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "Ivan Candela; Gabriele Bavota; Barbara Russo; Rocco Oliveto",
    "corresponding_authors": "",
    "abstract": "Refactoring and, in particular, remodularization operations can be performed to repair the design of a software system and remove the erosion caused by software evolution. Various approaches have been proposed to support developers during the remodularization of a software system. Most of these approaches are based on the underlying assumption that developers pursue an optimal balance between cohesion and coupling when modularizing the classes of their systems. Thus, a remodularization recommender proposes a solution that implicitly provides a (near) optimal balance between such quality attributes. However, there is still no empirical evidence that such a balance is the desideratum by developers. This article aims at analyzing both objectively and subjectively the aforementioned phenomenon. Specifically, we present the results of (1) a large study analyzing the modularization quality, in terms of package cohesion and coupling, of 100 open-source systems, and (2) a survey conducted with 29 developers aimed at understanding the driving factors they consider when performing modularization tasks. The results achieved have been used to distill a set of lessons learned that might be considered to design more effective remodularization recommenders.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2465318792",
    "type": "article"
  },
  {
    "title": "Software effort estimation as a multiobjective learning problem",
    "doi": "https://doi.org/10.1145/2522920.2522928",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Leandro L. Minku; Xin Yao",
    "corresponding_authors": "",
    "abstract": "Ensembles of learning machines are promising for software effort estimation (SEE), but need to be tailored for this task to have their potential exploited. A key issue when creating ensembles is to produce diverse and accurate base models. Depending on how differently different performance measures behave for SEE, they could be used as a natural way of creating SEE ensembles. We propose to view SEE model creation as a multiobjective learning problem. A multiobjective evolutionary algorithm (MOEA) is used to better understand the tradeoff among different performance measures by creating SEE models through the simultaneous optimisation of these measures. We show that the performance measures behave very differently, presenting sometimes even opposite trends. They are then used as a source of diversity for creating SEE ensembles. A good tradeoff among different measures can be obtained by using an ensemble of MOEA solutions. This ensemble performs similarly or better than a model that does not consider these measures explicitly. Besides, MOEA is also flexible, allowing emphasis of a particular measure if desired. In conclusion, MOEA can be used to better understand the relationship among performance measures and has shown to be very effective in creating SEE models.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2001851943",
    "type": "article"
  },
  {
    "title": "Portfolio",
    "doi": "https://doi.org/10.1145/2522920.2522930",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Collin McMillan; Denys Poshyvanyk; Mark Grechanik; Qing Xie; Chen Fu",
    "corresponding_authors": "",
    "abstract": "Different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or ordinary code fragments. Therefore, developers require support in finding relevant functions and determining how these functions are used. Unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse. We provide this support to programmers in a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We have built Portfolio using a combination of models that address surfing behavior of programmers and sharing related concepts among functions. We conducted two experiments: first, an experiment with 49 C/C++ programmers to compare Portfolio to Google Code Search and Koders using a standard methodology for evaluating information-retrieval-based engines; and second, an experiment with 19 Java programmers to compare Portfolio to Koders. The results show with strong statistical significance that users find more relevant functions with higher precision with Portfolio than with Google Code Search and Koders. We also show that by using PageRank, Portfolio is able to rank returned relevant functions more efficiently.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2026608875",
    "type": "article"
  },
  {
    "title": "Validating software metrics",
    "doi": "https://doi.org/10.1145/2377656.2377661",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Andrew Meneely; Ben Smith; Laurie Williams",
    "corresponding_authors": "",
    "abstract": "Context . Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a “valid” metric. Aim . The debate over what constitutes a valid metric centers on software metrics validation criteria. The objective of this article is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature. Method . We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis to explore the relationships amongst the criteria. Results . Our 47 validation criteria represent a diverse view of what constitutes a valid metric. We present an analysis of the criteria's categorization, conflicts, common themes, and philosophical motivations behind the validation criteria. Conclusions . Although the 47 validation criteria are not conflict-free, the diversity of motivations and philosophies behind the validation criteria indicates that metrics validation is complex. Researchers proposing new metrics should consider the applicability of the validation criteria in terms of our categorization and analysis. Rather than arbitrarily choosing validation criteria for each metric, researchers should choose criteria that can confirm that the metric is appropriate for its intended use. We conclude that metrics validation criteria provide answers to questions that researchers have about the merits and limitations of a metric.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2041628322",
    "type": "article"
  },
  {
    "title": "Intelligent Code Completion with Bayesian Networks",
    "doi": "https://doi.org/10.1145/2744200",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Sebastian Proksch; Johannes Lerch; Mira Mezini",
    "corresponding_authors": "",
    "abstract": "Code completion is an integral part of modern Integrated Development Environments (IDEs). Developers often use it to explore Application Programming Interfaces (APIs). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items. This work extends one of these existing approaches, the Best Matching Neighbor (BMN) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, Pattern-based Bayesian Networks (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed. Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that PBN can obtain comparable prediction quality to BMN, while model size and inference speed scale better with large input sizes.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2266912522",
    "type": "article"
  },
  {
    "title": "Synthesizing nonanomalous event-based controllers for liveness goals",
    "doi": "https://doi.org/10.1145/2430536.2430543",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Nicolás D’Ippolito; Vı́ctor Braberman; Nir Piterman; Sebastián Uchitel",
    "corresponding_authors": "",
    "abstract": "We present SGR(1), a novel synthesis technique and methodological guidelines for automatically constructing event-based behavior models. Our approach works for an expressive subset of liveness properties, distinguishes between controlled and monitored actions, and differentiates system goals from environment assumptions. We show that assumptions must be modeled carefully in order to avoid synthesizing anomalous behavior models. We characterize nonanomalous models and propose assumption compatibility, a sufficient condition, as a methodological guideline.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2084477406",
    "type": "article"
  },
  {
    "title": "Configuring Software Product Lines by Combining Many-Objective Optimization and SAT Solvers",
    "doi": "https://doi.org/10.1145/3176644",
    "publication_date": "2017-10-31",
    "publication_year": 2017,
    "authors": "Yi Xiang; Yuren Zhou; Zibin Zheng; Miqing Li",
    "corresponding_authors": "",
    "abstract": "A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search--based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2789351846",
    "type": "article"
  },
  {
    "title": "Assessing the Effect of Screen Mockups on the Comprehension of Functional Requirements",
    "doi": "https://doi.org/10.1145/2629457",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Filippo Ricca; Giuseppe Scanniello; Marco Torchiano; Gianna Reggio; Egidio Astesiano",
    "corresponding_authors": "",
    "abstract": "Over the last few years, the software engineering community has proposed a number of modeling methods to represent functional requirements. Among them, use cases are recognized as an easy to use and intuitive way to capture and define such requirements. Screen mockups (also called user-interface sketches or user interface-mockups) have been proposed as a complement to use cases for improving the comprehension of functional requirements. In this article, we aim at quantifying the benefits achievable by augmenting use cases with screen mockups in the comprehension of functional requirements with respect to effectiveness, effort, and efficiency. For this purpose, we conducted a family of four controlled experiments, involving 139 participants having different profiles. The experiments involved comprehension tasks performed on the requirements documents of two desktop applications. Independently from the participants' profile, we found a statistically significant large effect of the presence of screen mockups on both comprehension effectiveness and comprehension task efficiency. No significant effect was observed on the effort to complete tasks. The main pragmatic lesson is that the screen mockups addition to use cases is able to almost double the efficiency of comprehension tasks.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2037922987",
    "type": "article"
  },
  {
    "title": "FEMOSAA",
    "doi": "https://doi.org/10.1145/3204459",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Tao Chen; Ke Li; Rami Bahsoon; Xin Yao",
    "corresponding_authors": "",
    "abstract": "Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2798336324",
    "type": "article"
  },
  {
    "title": "Impact-Driven Process Model Repair",
    "doi": "https://doi.org/10.1145/2980764",
    "publication_date": "2016-10-12",
    "publication_year": 2016,
    "authors": "Artem Polyvyanyy; Wil M. P. van der Aalst; Arthur H. M. ter Hofstede; Moe Thandar Wynn",
    "corresponding_authors": "",
    "abstract": "The abundance of event data in today’s information systems makes it possible to “confront” process models with the actual observed behavior. Process mining techniques use event logs to discover process models that describe the observed behavior, and to check conformance of process models by diagnosing deviations between models and reality. In many situations, it is desirable to mediate between a preexisting model and observed behavior. Hence, we would like to repair the model while improving the correspondence between model and log as much as possible. The approach presented in this article assigns predefined costs to repair actions (allowing inserting or skipping of activities). Given a maximum degree of change, we search for models that are optimal in terms of fitness—that is, the fraction of behavior in the log not possible according to the model is minimized. To compute fitness, we need to align the model and log, which can be time consuming. Hence, finding an optimal repair may be intractable. We propose different alternative approaches to speed up repair. The number of alignment computations can be reduced dramatically while still returning near-optimal repairs. The different approaches have been implemented using the process mining framework ProM and evaluated using real-life logs.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2508985460",
    "type": "article"
  },
  {
    "title": "A taxonomy for requirements engineering and software test alignment",
    "doi": "https://doi.org/10.1145/2523088",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Michael Unterkalmsteiner; Robert Feldt; Tony Gorschek",
    "corresponding_authors": "",
    "abstract": "Requirements Engineering and Software Testing are mature areas and have seen a lot of research. Nevertheless, their interactions have been sparsely explored beyond the concept of traceability. To fill this gap, we propose a definition of requirements engineering and software test (REST) alignment, a taxonomy that characterizes the methods linking the respective areas, and a process to assess alignment. The taxonomy can support researchers to identify new opportunities for investigation, as well as practitioners to compare alignment methods and evaluate alignment, or lack thereof. We constructed the REST taxonomy by analyzing alignment methods published in literature, iteratively validating the emerging dimensions. The resulting concept of an information dyad characterizes the exchange of information required for any alignment to take place. We demonstrate use of the taxonomy by applying it on five in-depth cases and illustrate angles of analysis on a set of thirteen alignment methods. In addition, we developed an assessment framework (REST-bench), applied it in an industrial assessment, and showed that it, with a low effort, can identify opportunities to improve REST alignment. Although we expect that the taxonomy can be further refined, we believe that the information dyad is a valid and useful construct to understand alignment.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2088927456",
    "type": "article"
  },
  {
    "title": "Human Competitiveness of Genetic Programming in Spectrum-Based Fault Localisation",
    "doi": "https://doi.org/10.1145/3078840",
    "publication_date": "2017-01-31",
    "publication_year": 2017,
    "authors": "Shin Yoo; Xiaoyuan Xie; Fei‐Ching Kuo; Tsong Yueh Chen; Mark Harman",
    "corresponding_authors": "",
    "abstract": "We report on the application of Genetic Programming to Software Fault Localisation, a problem in the area of Search-Based Software Engineering (SBSE). We give both empirical and theoretical evidence for the human competitiveness of the evolved fault localisation formulæ under the single fault scenario, compared to those generated by human ingenuity and reported in many papers, published over more than a decade. Though there have been previous human competitive results claimed for SBSE problems, this is the first time that evolved solutions have been formally proved to be human competitive. We further prove that no future human investigation could outperform the evolved solutions. We complement these proofs with an empirical analysis of both human and evolved solutions, which indicates that the evolved solutions are not only theoretically human competitive, but also convey similar practical benefits to human-evolved counterparts.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2733797843",
    "type": "article"
  },
  {
    "title": "Inflow and Retention in OSS Communities with Commercial Involvement",
    "doi": "https://doi.org/10.1145/2876443",
    "publication_date": "2016-04-27",
    "publication_year": 2016,
    "authors": "Minghui Zhou; Audris Mockus; Xiujuan Ma; Lu Zhang; Hong Mei",
    "corresponding_authors": "",
    "abstract": "Motivation: Open-source projects are often supported by companies, but such involvement often affects the robust contributor inflow needed to sustain the project and sometimes prompts key contributors to leave. To capture user innovation and to maintain quality of software and productivity of teams, these projects need to attract and retain contributors. Aim: We want to understand and quantify how inflow and retention are shaped by policies and actions of companies in three application server projects. Method: We identified three hybrid projects implementing the same JavaEE specification and used published literature, online materials, and interviews to quantify actions and policies companies used to get involved. We collected project repository data, analyzed affiliation history of project participants, and used generalized linear models and survival analysis to measure contributor inflow and retention. Results: We identified coherent groups of policies and actions undertaken by sponsoring companies as three models of community involvement and quantified tradeoffs between the inflow and retention each model provides. We found that full control mechanisms and high intensity of commercial involvement were associated with a decrease of external inflow and with improved retention. However, a shared control mechanism was associated with increased external inflow contemporaneously with the increase of commercial involvement. Implications: Inspired by a natural experiment, our methods enabled us to quantify aspects of the balance between community and private interests in open- source software projects and provide clear implications for the structure of future open-source communities.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2344784027",
    "type": "article"
  },
  {
    "title": "Variability Bugs in Highly Configurable Systems",
    "doi": "https://doi.org/10.1145/3149119",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Iago Abal; Jean Melo; Ștefan Stănciulescu; Claus Brabrand; Márcio Ribeiro; Andrzej Wąsowski",
    "corresponding_authors": "",
    "abstract": "Variability-sensitive verification pursues effective analysis of the exponentially many variants of a program family. Several variability-aware techniques have been proposed, but researchers still lack examples of concrete bugs induced by variability, occurring in real large-scale systems. A collection of real world bugs is needed to evaluate tool implementations of variability-sensitive analyses by testing them on real bugs. We present a qualitative study of 98 diverse variability bugs (i.e., bugs that occur in some variants and not in others) collected from bug-fixing commits in the Linux, Apache, BusyBox, and Marlin repositories. We analyze each of the bugs, and record the results in a database. For each bug, we create a self-contained simplified version and a simplified patch, in order to help researchers who are not experts on these subject studies to understand them, so that they can use these bugs for evaluation of their tools. In addition, we provide single-function versions of the bugs, which are useful for evaluating intra-procedural analyses. A web-based user interface for the database allows to conveniently browse and visualize the collection of bugs. Our study provides insights into the nature and occurrence of variability bugs in four highly-configurable systems implemented in C/C++, and shows in what ways variability hinders comprehension and the uncovering of software bugs.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2783199130",
    "type": "article"
  },
  {
    "title": "Predicting Node Failures in an Ultra-Large-Scale Cloud Computing Platform",
    "doi": "https://doi.org/10.1145/3385187",
    "publication_date": "2020-04-29",
    "publication_year": 2020,
    "authors": "Yangguang Li; Zhen Ming Jiang; Heng Li; Ahmed E. Hassan; Cheng He; Ruirui Huang; Zhengda Zeng; Mian Wang; Pinan Chen",
    "corresponding_authors": "",
    "abstract": "Many software services today are hosted on cloud computing platforms, such as Amazon EC2, due to many benefits like reduced operational costs. However, node failures in these platforms can impact the availability of their hosted services and potentially lead to large financial losses. Predicting node failures before they actually occur is crucial, as it enables DevOps engineers to minimize their impact by performing preventative actions. However, such predictions are hard due to many challenges like the enormous size of the monitoring data and the complexity of the failure symptoms. AIOps ( A rtificial I ntelligence for IT Op eration s ), a recently introduced approach in DevOps, leverages data analytics and machine learning to improve the quality of computing platforms in a cost-effective manner. However, the successful adoption of such AIOps solutions requires much more than a top-performing machine learning model. Instead, AIOps solutions must be trustable, interpretable, maintainable, scalable, and evaluated in context. To cope with these challenges, in this article we report our process of building an AIOps solution for predicting node failures for an ultra-large-scale cloud computing platform at Alibaba. We expect our experiences to be of value to researchers and practitioners, who are interested in building and maintaining AIOps solutions for large-scale cloud computing platforms.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W3040857534",
    "type": "article"
  },
  {
    "title": "Understanding Software-2.0",
    "doi": "https://doi.org/10.1145/3453478",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Malinda Dilhara; Ameya Ketkar; Danny Dig",
    "corresponding_authors": "",
    "abstract": "Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models , i.e., Software-2.0 , has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries. We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings. Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries , (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W3119696503",
    "type": "article"
  },
  {
    "title": "When and How to Make Breaking Changes",
    "doi": "https://doi.org/10.1145/3447245",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Christopher Bogart; Christian Kästner; James D. Herbsleb; Ferdian Thung",
    "corresponding_authors": "",
    "abstract": "Open source software projects often rely on package management systems that help projects discover, incorporate, and maintain dependencies on other packages, maintained by other people. Such systems save a great deal of effort over ad hoc ways of advertising, packaging, and transmitting useful libraries, but coordination among project teams is still needed when one package makes a breaking change affecting other packages. Ecosystems differ in their approaches to breaking changes, and there is no general theory to explain the relationships between features, behavioral norms, ecosystem outcomes, and motivating values. We address this through two empirical studies. In an interview case study, we contrast Eclipse, NPM, and CRAN, demonstrating that these different norms for coordination of breaking changes shift the costs of using and maintaining the software among stakeholders, appropriate to each ecosystem’s mission. In a second study, we combine a survey, repository mining, and document analysis to broaden and systematize these observations across 18 ecosystems. We find that all ecosystems share values such as stability and compatibility, but differ in other values. Ecosystems’ practices often support their espoused values, but in surprisingly diverse ways. The data provides counterevidence against easy generalizations about why ecosystem communities do what they do.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W3184420437",
    "type": "article"
  },
  {
    "title": "Generating Question Titles for Stack Overflow from Mined Code Snippets",
    "doi": "https://doi.org/10.1145/3401026",
    "publication_date": "2020-09-26",
    "publication_year": 2020,
    "authors": "Zhipeng Gao; Xin Xia; John Grundy; David Lo; Yuan-Fang Li",
    "corresponding_authors": "",
    "abstract": "Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet. The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help. Previous studies have shown that} a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow. These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process. Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach. Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem. We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation. We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow-up work.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W3090867931",
    "type": "article"
  },
  {
    "title": "Security Smells in Ansible and Chef Scripts",
    "doi": "https://doi.org/10.1145/3408897",
    "publication_date": "2021-01-20",
    "publication_year": 2021,
    "authors": "Akond Rahman; Md Rayhanur Rahman; Chris Parnin; Laurie Williams",
    "corresponding_authors": "",
    "abstract": "Context: Security smells are recurring coding patterns that are indicative of security weakness and require further inspection. As infrastructure as code (IaC) scripts, such as Ansible and Chef scripts, are used to provision cloud-based servers and systems at scale, security smells in IaC scripts could be used to enable malicious users to exploit vulnerabilities in the provisioned systems. Goal: The goal of this article is to help practitioners avoid insecure coding practices while developing infrastructure as code scripts through an empirical study of security smells in Ansible and Chef scripts. Methodology: We conduct a replication study where we apply qualitative analysis with 1,956 IaC scripts to identify security smells for IaC scripts written in two languages: Ansible and Chef. We construct a static analysis tool called Security Linter for Ansible and Chef scripts (SLAC) to automatically identify security smells in 50,323 scripts collected from 813 open source software repositories. We also submit bug reports for 1,000 randomly selected smell occurrences. Results: We identify two security smells not reported in prior work: missing default in case statement and no integrity check. By applying SLAC we identify 46,600 occurrences of security smells that include 7,849 hard-coded passwords. We observe agreement for 65 of the responded 94 bug reports, which suggests the relevance of security smells for Ansible and Chef scripts amongst practitioners. Conclusion: We observe security smells to be prevalent in Ansible and Chef scripts, similarly to that of the Puppet scripts. We recommend practitioners to rigorously inspect the presence of the identified security smells in Ansible and Chef scripts using (i) code review, and (ii) static analysis tools.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W3123074563",
    "type": "article"
  },
  {
    "title": "Smart Contract Repair",
    "doi": "https://doi.org/10.1145/3402450",
    "publication_date": "2020-09-26",
    "publication_year": 2020,
    "authors": "Xiao Liang Yu; Omar I. Al-Bataineh; David Lo; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Smart contracts are automated or self-enforcing contracts that can be used to exchange assets without having to place trust in third parties. Many commercial transactions use smart contracts due to their potential benefits in terms of secure peer-to-peer transactions independent of external parties. Experience shows that many commonly used smart contracts are vulnerable to serious malicious attacks, which may enable attackers to steal valuable assets of involving parties. There is, therefore, a need to apply analysis and automated repair techniques to detect and repair bugs in smart contracts before being deployed. In this work, we present the first general-purpose automated smart contract repair approach that is also gas-aware. Our repair method is search-based and searches among mutations of the buggy contract. Our method also considers the gas usage of the candidate patches by leveraging our novel notion of gas dominance relationship . We have made our smart contract repair tool SCRepair available open-source, for investigation by the wider community.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W3091512861",
    "type": "article"
  },
  {
    "title": "Why an Android App Is Classified as Malware",
    "doi": "https://doi.org/10.1145/3423096",
    "publication_date": "2021-03-10",
    "publication_year": 2021,
    "authors": "Bozhi Wu; Sen Chen; Cuiyun Gao; Lingling Fan; Yang Liu; Weiping Wen; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Machine learning–(ML) based approach is considered as one of the most promising techniques for Android malware detection and has achieved high accuracy by leveraging commonly used features. In practice, most of the ML classifications only provide a binary label to mobile users and app security analysts. However, stakeholders are more interested in the reason why apps are classified as malicious in both academia and industry. This belongs to the research area of interpretable ML but in a specific research domain (i.e., mobile malware detection). Although several interpretable ML methods have been exhibited to explain the final classification results in many cutting-edge Artificial Intelligent–based research fields, until now, there is no study interpreting why an app is classified as malware or unveiling the domain-specific challenges. In this article, to fill this gap, we propose a novel and interpretable ML-based approach (named XMal ) to classify malware with high accuracy and explain the classification result meanwhile. (1) The first classification phase of XMal hinges multi-layer perceptron and attention mechanism and also pinpoints the key features most related to the classification result. (2) The second interpreting phase aims at automatically producing neural language descriptions to interpret the core malicious behaviors within apps. We evaluate the behavior description results by leveraging a human study and an in-depth quantitative analysis. Moreover, we further compare XMal with the existing interpretable ML-based methods (i.e., Drebin and LIME) to demonstrate the effectiveness of XMal . We find that XMal is able to reveal the malicious behaviors more accurately. Additionally, our experiments show that XMal can also interpret the reason why some samples are misclassified by ML classifiers. Our study peeks into the interpretable ML through the research of Android malware detection and analysis.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W3133963303",
    "type": "article"
  },
  {
    "title": "On the Impact of Sample Duplication in Machine-Learning-Based Android Malware Detection",
    "doi": "https://doi.org/10.1145/3446905",
    "publication_date": "2021-05-08",
    "publication_year": 2021,
    "authors": "Yanjie Zhao; Li Li; Haoyu Wang; Haipeng Cai; Tegawendé F. Bissyandé; Jacques Klein; John G. Grundy",
    "corresponding_authors": "",
    "abstract": "Malware detection at scale in the Android realm is often carried out using machine learning techniques. State-of-the-art approaches such as DREBIN and MaMaDroid are reported to yield high detection rates when assessed against well-known datasets. Unfortunately, such datasets may include a large portion of duplicated samples, which may bias recorded experimental results and insights. In this article, we perform extensive experiments to measure the performance gap that occurs when datasets are de-duplicated. Our experimental results reveal that duplication in published datasets has a limited impact on supervised malware classification models. This observation contrasts with the finding of Allamanis on the general case of machine learning bias for big code. Our experiments, however, show that sample duplication more substantially affects unsupervised learning models (e.g., malware family clustering). Nevertheless, we argue that our fellow researchers and practitioners should always take sample duplication into consideration when performing machine-learning-based (via either supervised or unsupervised learning) Android malware detections, no matter how significant the impact might be.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W3161331107",
    "type": "article"
  },
  {
    "title": "SPI: Automated Identification of Security Patches via Commits",
    "doi": "https://doi.org/10.1145/3468854",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Yaqin Zhou; Jing Kai Siow; Chenyu Wang; Shangqing Liu; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research. To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related. We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%. We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3202579690",
    "type": "article"
  },
  {
    "title": "Towards Robustness of Deep Program Processing Models—Detection, Estimation, and Enhancement",
    "doi": "https://doi.org/10.1145/3511887",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Huangzhao Zhang; Zhiyi Fu; Ge Li; Lei Ma; Zhehao Zhao; Hua’an Yang; Yizhe Sun; Yang Liu; Zhi Jin",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy). However, the robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs, still lacks systematic investigation as an important quality indicator. This article initiates an early step and proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models for source code processing. We first propose an optimization-based attack technique CARROT A to generate valid adversarial source code examples effectively and efficiently. Based on this, we define the robustness metrics and propose robustness measurement toolkit CARROT M , which employs the worst-case performance approximation under the allowable perturbations. We further propose to improve the robustness of the DL models by adversarial training (CARROT T ) with our proposed attack techniques. Our in-depth evaluations on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM, ASTNN, LSCNN, TBCNN, CodeBERT, and CDLH, demonstrate the usefulness of our techniques for ❶ effective and efficient adversarial example detection, ❷ tight robustness estimation, and ❸ effective robustness enhancement.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W4210772589",
    "type": "article"
  },
  {
    "title": "Stateful Serverless Computing with <scp>Crucial</scp>",
    "doi": "https://doi.org/10.1145/3490386",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Daniel Barcelona-Pons; Pierre Sutra; Marc Sánchez‐Artigas; Gerard Parı́s; Pedro Garcı́a-López",
    "corresponding_authors": "",
    "abstract": "Serverless computing greatly simplifies the use of cloud resources. In particular, Function-as-a-Service (FaaS) platforms enable programmers to develop applications as individual functions that can run and scale independently. Unfortunately, applications that require fine-grained support for mutable state and synchronization, such as machine learning (ML) and scientific computing, are notoriously hard to build with this new paradigm. In this work, we aim at bridging this gap. We present Crucial , a system to program highly-parallel stateful serverless applications. Crucial retains the simplicity of serverless computing. It is built upon the key insight that FaaS resembles to concurrent programming at the scale of a datacenter. Accordingly, a distributed shared memory layer is the natural answer to the needs for fine-grained state management and synchronization. Crucial allows to port effortlessly a multi-threaded code base to serverless, where it can benefit from the scalability and pay-per-use model of FaaS platforms. We validate Crucial with the help of micro-benchmarks and by considering various stateful applications. Beyond classical parallel tasks (e.g., a Monte Carlo simulation), these applications include representative ML algorithms such as k -means and logistic regression. Our evaluation shows that Crucial obtains superior or comparable performance to Apache Spark at similar cost (18%–40% faster). We also use Crucial to port (part of) a state-of-the-art multi-threaded ML library to serverless. The ported application is up to 30% faster than with a dedicated high-end server. Finally, we attest that Crucial can rival in performance with a single-machine, multi-threaded implementation of a complex coordination problem. Overall, Crucial delivers all these benefits with less than 6% of changes in the code bases of the evaluated applications.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W4220810575",
    "type": "article"
  },
  {
    "title": "Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments",
    "doi": "https://doi.org/10.1145/3533818",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Christian Birchler; Sajad Khatiri; Pouria Derakhshanfar; Sebastiano Panichella; Annibale Panichella",
    "corresponding_authors": "",
    "abstract": "Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints. In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer , prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer , use single-objective and multi-objective genetic algorithms ( GA ), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier. Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly ( P - value &lt;=0.1 e -10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs. MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45% of the test execution cost) is negligible.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W4281385259",
    "type": "article"
  },
  {
    "title": "Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review",
    "doi": "https://doi.org/10.1145/3542945",
    "publication_date": "2022-06-16",
    "publication_year": 2022,
    "authors": "Hugo Araujo; Mohammad Reza Mousavi; Mahsa Varshosaz",
    "corresponding_authors": "",
    "abstract": "We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving or evaluating testing techniques, process, or tools that address the system-level qualities of RAS. Our survey is performed based on a rigorous methodology structured in three phases. First, we made use of a set of 26 seed papers (selected by domain experts) and the SERP-TEST taxonomy to design our search query and (domain-specific) taxonomy. Second, we conducted a search in three academic search engines and applied our inclusion and exclusion criteria to the results. Respectively, we made use of related work and domain specialists (50 academics and 15 industry experts) to validate and refine the search query. As a result, we encountered 10,735 studies, out of which, 195 were included, reviewed and coded. Our objective is to answer four research questions, pertaining to (1) the type of models, (2) measures for system performance and testing adequacy, (3) tools and their availability, and (4) evidence of applicability, particularly in industrial contexts. We analyse the results of our coding to identify strengths and gaps in the domain and present recommendations to researchers and practitioners. Our findings show that variants of temporal logics are most widely used for modelling requirements and properties, while variants of state-machines and transition systems are used widely for modelling system behaviour. Other common models concern epistemic logics for specifying requirements and belief-desire-intention models for specifying system behaviour. Apart from time and epistemics, other aspects captured in models concern probabilities (e.g., for modelling uncertainty) and continuous trajectories (e.g., for modelling vehicle dynamics and kinematics). Many papers lack any rigorous measure of efficiency, effectiveness, or adequacy for their proposed techniques, processes, or tools. Among those that provide a measure of efficiency, effectiveness, or adequacy the majority use domain-agnostic generic measures such as number of failures, size of state-space or verification time were most used. There is a trend in addressing the research gap in this respect by developing domain-specific notions of performance and adequacy. Defining widely-accepted rigorous measures of performance and adequacy for each domain is an identified research gap. In terms of tools, the most widely used tools are well-established model-checkers such as Prism and Uppaal, as well as simulation tools such as Gazebo; Matlab/Simulink is another widely used toolset in this domain. Overall there is very limited evidence of industrial applicability in the papers published in this domain. There is even a gap considering consolidated benchmarks for various types of autonomous systems.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W4282981328",
    "type": "review"
  },
  {
    "title": "Adversarial Robustness of Deep Code Comment Generation",
    "doi": "https://doi.org/10.1145/3501256",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Yu Zhou; Xiaoqing Zhang; Juanjuan Shen; Tingting Han; Taolue Chen; Harald C. Gall",
    "corresponding_authors": "",
    "abstract": "Deep neural networks (DNNs) have shown remarkable performance in a variety of domains such as computer vision, speech recognition, and natural language processing. Recently they also have been applied to various software engineering tasks, typically involving processing source code. DNNs are well-known to be vulnerable to adversarial examples, i.e., fabricated inputs that could lead to various misbehaviors of the DNN model while being perceived as benign by humans. In this paper, we focus on the code comment generation task in software engineering and study the robustness issue of the DNNs when they are applied to this task. We propose ACCENT (Adversarial Code Comment gENeraTor) , an identifier substitution approach to craft adversarial code snippets, which are syntactically correct and semantically close to the original code snippet, but may mislead the DNNs to produce completely irrelevant code comments. In order to improve the robustness, ACCENT also incorporates a novel training method, which can be applied to existing code comment generation models. We conduct comprehensive experiments to evaluate our approach by attacking the mainstream encoder-decoder architectures on two large-scale publicly available datasets. The results show that ACCENT efficiently produces stable attacks with functionality-preserving adversarial examples, and the generated examples have better transferability compared with the baselines. We also confirm, via experiments, the effectiveness in improving model robustness with our training method.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W4210499321",
    "type": "article"
  },
  {
    "title": "The Weights Can Be Harmful: Pareto Search versus Weighted Search in Multi-objective Search-based Software Engineering",
    "doi": "https://doi.org/10.1145/3514233",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Tao Chen; Miqing Li",
    "corresponding_authors": "",
    "abstract": "In presence of multiple objectives to be optimized in Search-Based Software Engineering (SBSE), Pareto search has been commonly adopted. It searches for a good approximation of the problem’s Pareto-optimal solutions, from which the stakeholders choose the most preferred solution according to their preferences. However, when clear preferences of the stakeholders (e.g., a set of weights that reflect relative importance between objectives) are available prior to the search, weighted search is believed to be the first choice, since it simplifies the search via converting the original multi-objective problem into a single-objective one and enables the search to focus on what only the stakeholders are interested in. This article questions such a “ weighted search first ” belief. We show that the weights can, in fact, be harmful to the search process even in the presence of clear preferences. Specifically, we conduct a large-scale empirical study that consists of 38 systems/projects from three representative SBSE problems, together with two types of search budget and nine sets of weights, leading to 604 cases of comparisons. Our key finding is that weighted search reaches a certain level of solution quality by consuming relatively less resources at the early stage of the search; however, Pareto search is significantly better than its weighted counterpart the majority of the time (up to 77% of the cases), as long as we allow a sufficient, but not unrealistic search budget. This is a beneficial result, as it discovers a potentially new “rule-of-thumb” for the SBSE community: Even when clear preferences are available, it is recommended to always consider Pareto search by default for multi-objective SBSE problems, provided that solution quality is more important. Weighted search, in contrast, should only be preferred when the resource/search budget is limited, especially for expensive SBSE problems. This, together with other findings and actionable suggestions in the article, allows us to codify pragmatic and comprehensive guidance on choosing weighted and Pareto search for SBSE under the circumstance that clear preferences are available. All code and data can be accessed at https://github.com/ideas-labo/pareto-vs-weight-for-sbse .",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4221016682",
    "type": "article"
  },
  {
    "title": "Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems",
    "doi": "https://doi.org/10.1145/3544792",
    "publication_date": "2022-06-29",
    "publication_year": 2022,
    "authors": "Tahereh Zohdinasab; Vincenzo Riccio; Alessio Gambi; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the system’s (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systems’ feature space at large. In this article, we propose DeepHyperion-CS , a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion . DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200% more feature map cells than the original training set.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W4299559948",
    "type": "article"
  },
  {
    "title": "Predictive Models in Software Engineering: Challenges and Opportunities",
    "doi": "https://doi.org/10.1145/3503509",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Yanming Yang; Xin Xia; David Lo; Tingting Bi; John Grundy; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3047958222",
    "type": "article"
  },
  {
    "title": "An Empirical Study on Data Distribution-Aware Test Selection for Deep Learning Enhancement",
    "doi": "https://doi.org/10.1145/3511598",
    "publication_date": "2022-04-19",
    "publication_year": 2022,
    "authors": "Qiang Hu; Yuejun Guo; Maxime Cordy; Xiaofei Xie; Lei Ma; Mike Papadakis; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Similar to traditional software that is constantly under evolution, deep neural networks need to evolve upon the rapid growth of test data for continuous enhancement (e.g., adapting to distribution shift in a new environment for deployment). However, it is labor intensive to manually label all of the collected test data. Test selection solves this problem by strategically choosing a small set to label. Via retraining with the selected set, deep neural networks will achieve competitive accuracy. Unfortunately, existing selection metrics involve three main limitations: (1) using different retraining processes, (2) ignoring data distribution shifts, and (3) being insufficiently evaluated. To fill this gap, we first conduct a systemically empirical study to reveal the impact of the retraining process and data distribution on model enhancement. Then based on our findings, we propose DAT, a novel distribution-aware test selection metric. Experimental results reveal that retraining using both the training and selected data outperforms using only the selected data. None of the selection metrics perform the best under various data distributions. By contrast, DAT effectively alleviates the impact of distribution shifts and outperforms the compared metrics by up to five times and 30.09% accuracy improvement for model enhancement on simulated and in-the-wild distribution shift scenarios, respectively.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W4224214784",
    "type": "article"
  },
  {
    "title": "<scp>deGraphCS</scp> : Embedding Variable-based Flow Graph for Neural Code Search",
    "doi": "https://doi.org/10.1145/3546066",
    "publication_date": "2022-07-20",
    "publication_year": 2022,
    "authors": "Chen Zeng; Yue Yu; Shanshan Li; Xin Xia; Zhiming Wang; Mingyang Geng; Linxiao Bai; Wei Dong; Xiangke Liao",
    "corresponding_authors": "",
    "abstract": "With the rapid increase of public code repositories, developers maintain a great desire to retrieve precise code snippets by using natural language. Despite existing deep learning-based approaches that provide end-to-end solutions (i.e., accept natural language as queries and show related code fragments), the performance of code search in the large-scale repositories is still low in accuracy because of the code representation (e.g., AST) and modeling (e.g., directly fusing features in the attention stage). In this paper, we propose a novel learnable de ep G raph for C ode S earch (called deGraphCS ) to transfer source code into variable-based flow graphs based on an intermediate representation technique, which can model code semantics more precisely than directly processing the code as text or using the syntax tree representation. Furthermore, we propose a graph optimization mechanism to refine the code representation and apply an improved gated graph neural network to model variable-based flow graphs. To evaluate the effectiveness of deGraphCS , we collect a large-scale dataset from GitHub containing 41,152 code snippets written in the C language and reproduce several typical deep code search methods for comparison. The experimental results show that deGraphCS can achieve state-of-the-art performance and accurately retrieve code snippets satisfying the needs of the users.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3138081324",
    "type": "article"
  },
  {
    "title": "The Influence of Human Aspects on Requirements Engineering-related Activities: Software Practitioners’ Perspective",
    "doi": "https://doi.org/10.1145/3546943",
    "publication_date": "2022-07-06",
    "publication_year": 2022,
    "authors": "Dulaji Hidellaarachchi; John Grundy; Rashina Hoda; Ingo Mueller",
    "corresponding_authors": "",
    "abstract": "Requirements Engineering (RE)-related activities require high collaboration between various roles in software engineering (SE), such as requirements engineers, stakeholders, developers, and so on. Their demographics, views, understanding of technologies, working styles, communication and collaboration capabilities make RE highly human-dependent. Identifying how “human aspects” —such as motivation, domain knowledge, communication skills, personality, emotions, culture, and so on—might impact RE-related activities would help us improve RE and SE in general. This study aims at better understanding current industry perspectives on the influence of human aspects on RE-related activities, specifically focusing on motivation and personality, by targeting software practitioners involved in RE-related activities. Our findings indicate that software practitioners consider motivation, domain knowledge, attitude, communication skills and personality as highly important human aspects when involved in RE-related activities. A set of factors were identified as software practitioners’ key motivational factors when involved in RE-related activities, along with important personality characteristics to have when involved in RE. We also identified factors that made individuals less effective when involved in RE-related activities and obtained some feedback on measuring individuals’ performance when involved in RE. The findings from our study suggest various areas needing more investigation, and we summarise a set of key recommendations for further research.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4283830566",
    "type": "article"
  },
  {
    "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems",
    "doi": "https://doi.org/10.1145/3582571",
    "publication_date": "2023-02-02",
    "publication_year": 2023,
    "authors": "Qinghua Xu; Shaukat Ali; Tao Yue",
    "corresponding_authors": "",
    "abstract": "Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4319037055",
    "type": "article"
  },
  {
    "title": "Dissecting American Fuzzy Lop: A FuzzBench Evaluation",
    "doi": "https://doi.org/10.1145/3580596",
    "publication_date": "2023-01-20",
    "publication_year": 2023,
    "authors": "Andrea Fioraldi; Alessandro Mantovani; Dominik Maier; Davide Balzarotti",
    "corresponding_authors": "",
    "abstract": "AFL is one of the most used and extended fuzzers, adopted by industry and academic researchers alike. Although the community agrees on AFL’s effectiveness at discovering new vulnerabilities and its outstanding usability, many of its internal design choices remain untested to date. Security practitioners often clone the project “as-is” and use it as a starting point to develop new techniques, usually taking everything under the hood for granted. Instead, we believe that a careful analysis of the different parameters could help modern fuzzers improve their performance and explain how each choice can affect the outcome of security testing, either negatively or positively. The goal of this work is to provide a comprehensive understanding of the internal mechanisms of AFL by performing experiments and by comparing different metrics used to evaluate fuzzers. This can help to show the effectiveness of some techniques and to clarify which aspects are instead outdated. To perform our study, we performed nine unique experiments that we carried out on the popular Fuzzbench platform. Each test focuses on a different aspect of AFL, ranging from its mutation approach to the feedback encoding scheme and its scheduling methodologies. Our findings show that each design choice affects different factors of AFL. Some of these are positively correlated with the number of detected bugs or the coverage of the target application, whereas other features are related to usability and reliability. Most important, we believe that the outcome of our experiments indicates which parts of AFL we should preserve in the design of modern fuzzers.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4317536056",
    "type": "article"
  },
  {
    "title": "Automated Identification of Toxic Code Reviews Using ToxiCR",
    "doi": "https://doi.org/10.1145/3583562",
    "publication_date": "2023-02-09",
    "publication_year": 2023,
    "authors": "Jaydeb Sarker; Asif Kamal Turzo; Ming Dong; Amiangshu Bosu",
    "corresponding_authors": "",
    "abstract": "Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR , a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at https://github.com/WSU-SEAL/ToxiCR .",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4319737124",
    "type": "article"
  },
  {
    "title": "White-Box Fuzzing RPC-Based APIs with EvoMaster: An Industrial Case Study",
    "doi": "https://doi.org/10.1145/3585009",
    "publication_date": "2023-02-23",
    "publication_year": 2023,
    "authors": "Man Zhang; Andrea Arcuri; Yonggang Li; Yang Liu; Kaiming Xue",
    "corresponding_authors": "",
    "abstract": "Remote Procedure Call (RPC) is a communication protocol to support client-server interactions among services over a network. RPC is widely applied in industry for building large-scale distributed systems, such as Microservices. Modern RPC frameworks include, for example, Thrift, gRPC, SOFARPC, and Dubbo. Testing such systems using RPC communications is very challenging, due to the complexity of distributed systems and various RPC frameworks the system could employ. To the best of our knowledge, there does not exist any tool or solution that could enable automated testing of modern RPC-based services. To fill this gap, in this article we propose the first approach in the literature, together with an open source tool, for fuzzing modern RPC-based APIs. The approach is in the context of white-box testing with search-based techniques. To tackle schema extraction of various RPC frameworks, we formulate a RPC schema specification along with a parser that allows the extraction from source code of any JVM RPC-based APIs. Then, with the extracted schema we employ a search to produce tests by maximizing white-box heuristics and newly defined heuristics specific to the RPC domain. We built our approach as an extension to an open source fuzzer (i.e., EvoMaster ), and the approach has been integrated into a real industrial pipeline that could be applied to a real industrial development process for fuzzing RPC-based APIs. To assess our novel approach, we conducted an empirical study with two artificial and four industrial web services selected by our industrial partner. In addition, to further demonstrate its effectiveness and application in industrial settings, we report results of employing our tool for fuzzing another 50 industrial APIs autonomously conducted by our industrial partner in their testing processes. Results show that our novel approach is capable of enabling automated test case generation for industrial RPC-based APIs (i.e., 2 artificial and 54 industrial). We also compared with a simple gray-box technique and existing manually written tests. Our white-box solution achieves significant improvements on code coverage. Regarding fault detection, by conducting a careful review with our industrial partner of the tests generated by our novel approach in the selected four industrial APIs, a total of 41 real faults were identified, which have now been fixed. Another 8,377 detected faults are currently under investigation.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4321482783",
    "type": "article"
  },
  {
    "title": "An Extractive-and-Abstractive Framework for Source Code Summarization",
    "doi": "https://doi.org/10.1145/3632742",
    "publication_date": "2023-11-13",
    "publication_year": 2023,
    "authors": "Weisong Sun; Chunrong Fang; Yuchen Chen; Quanjun Zhang; Guanhong Tao; Yudu You; Tingxu Han; Yifei Ge; Yuling Hu; Bin Luo; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "(Source) Code summarization aims to automatically generate summaries/comments for given code snippets in the form of natural language. Such summaries play a key role in helping developers understand and maintain source code. Existing code summarization techniques can be categorized into extractive methods and abstractive methods . The extractive methods extract a subset of important statements and keywords from the code snippet using retrieval techniques and generate a summary that preserves factual details in important statements and keywords. However, such a subset may miss identifier or entity naming, and consequently, the naturalness of the generated summary is usually poor. The abstractive methods can generate human-written-like summaries leveraging encoder-decoder models. However, the generated summaries often miss important factual details. To generate human-written-like summaries with preserved factual details, we propose a novel extractive-and-abstractive framework. The extractive module in the framework performs the task of extractive code summarization, which takes in the code snippet and predicts important statements containing key factual details. The abstractive module in the framework performs the task of abstractive code summarization, which takes in the code snippet and important statements in parallel and generates a succinct and human-written-like natural language summary. We evaluate the effectiveness of our technique, called EACS, by conducting extensive experiments on three datasets involving six programming languages. Experimental results show that EACS significantly outperforms state-of-the-art techniques for all three widely used metrics, including BLEU, METEOR, and ROUGH-L. In addition, the human evaluation demonstrates that the summaries generated by EACS have higher naturalness and informativeness and are more relevant to given code snippets.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4388631682",
    "type": "article"
  },
  {
    "title": "Early Validation and Verification of System Behaviour in Model-based Systems Engineering: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3631976",
    "publication_date": "2023-11-07",
    "publication_year": 2023,
    "authors": "Johan Cederbladh; Antonio Cicchetti; Jagadish Suryadevara",
    "corresponding_authors": "",
    "abstract": "In the Systems Engineering (SE) domain there has been a paradigm shift from document-based to model-based system development artefacts; in fact, new methodologies are emerging to meet the increasing complexity of current systems and the corresponding growing need of digital workflows. In this regard, Model-Based Systems Engineering (MBSE) is considered as a key enabler by many central players of the SE community. MBSE has reached an adequate level of maturity, and there exist documented success stories in its adoption in industry. In particular, one significant benefit of utilising MBSE when compared to the traditional manual and document-centric workflows is that models are available from early phases of systems development; these enable a multitude of analyses prior any implementation effort together with other relevant capabilities, like the automation of development tasks. Nonetheless, it is noticeable there is a lack of a common understanding for how formal analyses for the verification and validation (V&amp;V) of systems behaviour, specifically in the early phases of development, could be placed in an MBSE setting. In this article, we report on the planning, execution, and results of a systematic literature review regarding the early V&amp;V of systems behaviour in the context of model-based systems engineering. The review aims to provide a structured representation of the state of the art with respect to motivations, proposed solutions, and limitations. From an initial set of potentially relevant 701 peer-reviewed publications we selected 149 primary studies, which we analysed according to a rigorous data extraction, analysis, and synthesis process. Based on our results, early V&amp;V has usually the goal of checking the quality of a system design to avoid discovering flaws when parts are being concretely realised; SysML is a de facto standard for describing the system under study, while the solutions for the analyses tend to be varied; also V&amp;V analyses tend to target varied properties with a slight predominance of functional concerns, and following the variation mentioned so far the proposed solutions are largely context specific; the proposed approaches are usually presented without explicit limitations, while when limitations are discussed, readiness of the solutions, handling of analyses simplifications/assumptions, and languages/tools integration are among the most frequently mentioned issues. Based on the survey results and the standard SE practices, we discuss how the current state-of-the-art MBSE supports early V&amp;V of systems behaviour with a special focus on industrial adoption and identify relevant challenges to be researched further.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4388455124",
    "type": "article"
  },
  {
    "title": "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains",
    "doi": "https://doi.org/10.1145/3638247",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Yu Cheng; Jieshan Chen; Qing Huang; Zhenchang Xing; Xiwei Xu; Qinghua Lu",
    "corresponding_authors": "",
    "abstract": "The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper , which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4390051886",
    "type": "article"
  },
  {
    "title": "On the Reliability and Explainability of Language Models for Program Generation",
    "doi": "https://doi.org/10.1145/3641540",
    "publication_date": "2024-01-18",
    "publication_year": 2024,
    "authors": "Yue Liu; Chakkrit Tantithamthavorn; Yonghui Liu; Li Li",
    "corresponding_authors": "",
    "abstract": "Recent studies have adopted pre-trained language models, such as CodeT5 and CodeGPT, for automated program generation tasks like code generation, repair, and translation. Numerous language model-based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance. However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences. This raises the question: are these techniques sufficiently trustworthy for automated program generation? Consequently, Further research is needed to understand model logic and assess reliability and explainability. To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation. We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication, causing over-optimistic results. Our explainability analysis reveals that, in various experimental scenarios, language models can recognize code grammar and structural information, but they exhibit limited robustness to changes in input sequences. Overall, more rigorous evaluation approaches and benchmarks are critical to enhance the reliability and explainability of automated program generation moving forward. Our findings provide important guidelines for this goal.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4390976306",
    "type": "article"
  },
  {
    "title": "DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3644388",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Zohreh Aghababaeyan; Manel Abdellatif; Mahboubeh Dadkhah; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. In particular, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this article, we propose DeepGD , a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault-revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by selecting diverse mispredicted inputs. The experimental results conducted on four widely used datasets and five DNN models show that in terms of fault-revealing ability, (1) white-box, coverage-based approaches fare poorly, (2) DeepGD outperforms existing black-box test selection approaches in terms of fault detection, and (3) DeepGD also leads to better guidance for DNN model retraining when using selected inputs to augment the training set.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4391614764",
    "type": "article"
  },
  {
    "title": "sGuard+: Machine Learning Guided Rule-Based Automated Vulnerability Repair on Smart Contracts",
    "doi": "https://doi.org/10.1145/3641846",
    "publication_date": "2024-02-08",
    "publication_year": 2024,
    "authors": "Cuifeng Gao; Wenzhang Yang; Jiaming Ye; Yinxing Xue; Jun Sun",
    "corresponding_authors": "",
    "abstract": "Smart contracts are becoming appealing targets for hackers because of the vast amount of cryptocurrencies under their control. Asset loss due to the exploitation of smart contract codes has increased significantly in recent years. To guarantee that smart contracts are vulnerability-free, there are many works to detect the vulnerabilities of smart contracts, but only a few vulnerability repair works have been proposed. Repairing smart contract vulnerabilities at the source code level is attractive as it is transparent to users, whereas existing repair tools, such as SCRepair and sGuard , suffer from many limitations: (1) ignoring the code of vulnerability prevention; (2) possibly applying the repair to the wrong statements and changing the original business logic of smart contracts; and (3) showing poor performance in terms of time and gas overhead. In this work, we propose machine learning guided rule-based automated vulnerability repair on smart contracts to improve the effectiveness and efficiency of sGuard . To address the limitations mentioned above, we design the features that characterize both the symptoms of vulnerabilities and the methods of vulnerability prevention to learn various vulnerability patterns and reduce false positives. Additionally, a fine-grained localization algorithm is designed by traversing the nodes of the abstract syntax tree, and we refine and extend the repair rules of sGuard to preserve the original business logic of smart contracts and support new vulnerability types. Our tool, named sGuard+ , reduces time overhead based on machine learning models, and reduces gas overhead by fewer code changes and precise patching. In our experiment, we collect a publicly available vulnerability dataset from CVE, SWC, and SmartBugs Curated as a ground truth for evaluations. Overall, sGuard+ repairs more vulnerabilities with less time and gas overhead than state-of-the-art tools. Furthermore, we reproduce about 9,000 historical transactions for regression testing. It is shown that sGuard+ has no impact on the original business logic of smart contracts.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4391653691",
    "type": "article"
  },
  {
    "title": "Testing Multi-Subroutine Quantum Programs: From Unit Testing to Integration Testing",
    "doi": "https://doi.org/10.1145/3656339",
    "publication_date": "2024-04-05",
    "publication_year": 2024,
    "authors": "Peixun Long; Jianjun Zhao",
    "corresponding_authors": "",
    "abstract": "Quantum computing has emerged as a promising field with the potential to revolutionize various domains by harnessing the principles of quantum mechanics. As quantum hardware and algorithms continue to advance, developing high-quality quantum software has become crucial. However, testing quantum programs poses unique challenges due to the distinctive characteristics of quantum systems and the complexity of multi-subroutine programs. This article addresses the specific testing requirements of multi-subroutine quantum programs. We begin by investigating critical properties by surveying existing quantum libraries and providing insights into the challenges of testing these programs. Building upon this understanding, we focus on testing criteria and techniques based on the whole testing process perspective, spanning from unit testing to integration testing. We delve into various aspects, including IO analysis, quantum relation checking, structural testing, behavior testing, integration of subroutine pairs, and test case generation. We also introduce novel testing principles and criteria to guide the testing process. We conduct comprehensive testing on typical quantum subroutines, including diverse mutants and randomized inputs, to evaluate our proposed approach. The analysis of failures provides valuable insights into the effectiveness of our testing methodology. Additionally, we present case studies on representative multi-subroutine quantum programs, demonstrating the practical application and effectiveness of our proposed testing principles and criteria.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4393992776",
    "type": "article"
  },
  {
    "title": "Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities",
    "doi": "https://doi.org/10.1145/3664606",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Wei Ma; Shangqing Liu; Mengjie Zhao; Xiaofei Xie; Wenhan Wang; Qiang Hu; Junyin Zhang; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Code models have made significant advancements in code intelligence by encoding knowledge about programming languages. While previous studies have explored the capabilities of these models in learning code syntax, there has been limited investigation on their ability to understand code semantics. Additionally, existing analyses assume that the number of edges between nodes at the abstract syntax tree (AST) is related to syntax distance, and also often require transforming the high-dimensional space of deep learning models to a low-dimensional one, which may introduce inaccuracies. To study how code models represent code syntax and semantics, we conduct a comprehensive analysis of seven code models, including four representative code pre-trained models (CodeBERT, GraphCodeBERT, CodeT5, and UnixCoder) and three large language models (LLMs) (StarCoder, CodeLlama and CodeT5+). We design four probing tasks to assess the models’ capacities in learning both code syntax and semantics. These probing tasks reconstruct code syntax and semantics structures (AST, control dependence graph (CDG), data dependence graph (DDG), and control flow graph (CFG)) in the representation space. These structures are core concepts for code understanding. We also investigate the syntax token role in each token representation and the long dependency between the code tokens. Additionally, we analyze the distribution of attention weights related to code semantic structures. Through extensive analysis, our findings highlight the strengths and limitations of different code models in learning code syntax and semantics. The results demonstrate that these models excel in learning code syntax, successfully capturing the syntax relationships between tokens and the syntax roles of individual tokens. However, their performance in encoding code semantics varies. CodeT5 and CodeBERT demonstrate proficiency in capturing control and data dependencies, whereas UnixCoder shows weaker performance in this aspect. We do not observe LLMs generally performing much better than pre-trained models. The shallow layers of LLMs perform better than their deep layers. The investigation of attention weights reveals that different attention heads play distinct roles in encoding code semantics. Our research findings emphasize the need for further enhancements in code models to better learn code semantics. This study contributes to the understanding of code models’ abilities in syntax and semantics analysis. Our findings provide guidance for future improvements in code models, facilitating their effective application in various code-related tasks.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4396773582",
    "type": "article"
  },
  {
    "title": "When ChatGPT Meets Smart Contract Vulnerability Detection: How Far Are We?",
    "doi": "https://doi.org/10.1145/3702973",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Chong Chen; Jianzhong Su; Jiachi Chen; Yanlin Wang; Tingting Bi; Jianxing Yu; Y. F. Wang; Xingwei Lin; Ting Chen; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "With the development of blockchain technology, smart contracts have become an important component of blockchain applications. Despite their crucial role, the development of smart contracts may introduce vulnerabilities and potentially lead to severe consequences, such as financial losses. Meanwhile, large language models, represented by ChatGPT, have gained great attentions, showcasing great capabilities in code analysis tasks. In this paper, we presented an empirical study to investigate the performance of ChatGPT in identifying smart contract vulnerabilities. Initially, we evaluated ChatGPT's effectiveness using a publicly available smart contract dataset. Our findings discover that while ChatGPT achieves a high recall rate, its precision in pinpointing smart contract vulnerabilities is limited. Furthermore, ChatGPT's performance varies when detecting different vulnerability types. We delved into the root causes for the false positives generated by ChatGPT, and categorized them into four groups. Second, by comparing ChatGPT with other state-of-the-art smart contract vulnerability detection tools, we found that ChatGPT's F-score is lower than others for 3 out of the 7 vulnerabilities. In the case of the remaining 4 vulnerabilities, ChatGPT exhibits a slight advantage over these tools. Finally, we analyzed the limitation of ChatGPT in smart contract vulnerability detection, revealing that the robustness of ChatGPT in this field needs to be improved from two aspects: its uncertainty in answering questions; and the limited length of the detected code. In general, our research provides insights into the strengths and weaknesses of employing large language models, specifically ChatGPT, for the detection of smart contract vulnerabilities.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4404060141",
    "type": "article"
  },
  {
    "title": "Engineering Digital Systems for Humanity: a Research Roadmap",
    "doi": "https://doi.org/10.1145/3712006",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Marco Autili; Martina De Sanctis; Paola Inverardi; Patrizio Pelliccione",
    "corresponding_authors": "",
    "abstract": "As testified by new regulations like the European AI Act, worries about the human and societal impact of (autonomous) software technologies are becoming of public concern. Human, societal, and environmental values, alongside traditional software quality, are increasingly recognized as essential for sustainability and long-term well-being. Traditionally, systems are engineered taking into account business goals and technology drivers. Considering the growing awareness in the community, in this paper, we argue that engineering of systems should also consider human, societal, and environmental drivers. Then, we identify the macro and technological challenges by focusing on humans and their role while co-existing with digital systems. The first challenge considers humans in a proactive role when interacting with digital systems, i.e., taking initiative in making things happen instead of reacting to events. The second concerns humans having a reactive role in interacting with digital systems, i.e., humans interacting with digital systems as a reaction to events. The third challenge focuses on humans with a passive role, i.e., they experience, enjoy or even suffer the decisions and/or actions of digital systems. The fourth challenge concerns the duality of trust and trustworthiness, with humans playing any role. Building on the new human, societal, and environmental drivers and the macro and technological challenges, we identify a research roadmap of digital systems for humanity. The research roadmap is concretized in a number of research directions organized into four groups: development process, requirements engineering, software architecture and design, and verification and validation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406324613",
    "type": "article"
  },
  {
    "title": "Mitigating Regression Faults Induced by Feature Evolution in Deep Learning Systems",
    "doi": "https://doi.org/10.1145/3712199",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Hanmo You; Zan Wang; Xuyang Chen; Junjie Chen; Jun Sun; Shuang Liu; Zishuo Dong",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) systems have been widely utilized across various domains. However, the evolution of DL systems can result in regression faults. In addition to the evolution of DL systems through the incorporation of new data, feature evolution, such as the addition of new features, is also common and can introduce regression faults. In this work, we first investigate the underlying factors that are correlated with regression faults in feature evolution scenarios, i.e., redundancy and contribution shift. Based on our investigation, we propose a novel mitigation approach called FeaProtect, which aims to minimize the impact of these two factors. To evaluate the performance of FeaProtect, we conducted an extensive study comparing it with state-of-the-art approaches. The results show that FeaProtect outperforms the in-processing baseline approaches, with an average improvement of 50.6% \\(\\sim\\) 56.4% in terms of regression fault mitigation. We also show that FeaProtect can further enhance the effectiveness of mitigating regression faults by integrating with state-of-the-art post-processing approaches.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406386001",
    "type": "article"
  },
  {
    "title": "A Roadmap for Simulation-Based Testing of Autonomous Cyber-Physical Systems: Challenges and Future Direction",
    "doi": "https://doi.org/10.1145/3711906",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Christian Birchler; Sajad Khatiri; Pooja Rani; Timo Kehrer; Sebastiano Panichella",
    "corresponding_authors": "",
    "abstract": "As the era of autonomous cyber-physical systems (ACPSs), such as unmanned aerial vehicles and self-driving cars, unfolds, the demand for robust testing methodologies is key to realizing the adoption of such systems in real-world scenarios. However, traditional software testing paradigms face unprecedented challenges in ensuring the safety and reliability of these systems. In response, this paper pioneers a strategic roadmap for simulation-based system-level testing of ACPSs, specifically focusing on autonomous systems. Our paper discusses the relevant challenges and obstacles of ACPSs, focusing on test automation and quality assurance, hence advocating for tailored solutions to address the unique demands of autonomous systems. While providing concrete definitions of test cases within simulation environments, we also accentuate the need to create new benchmark assets and the development of automated tools tailored explicitly for autonomous systems in the software engineering community. This paper not only highlights the relevant, pressing issues the software engineering community should focus on (in terms of practices, expected automation, and paradigms), but it also outlines ways to tackle them. By outlining the various domains and challenges of simulation-based testing/development for ACPSs, we provide directions for future research efforts.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406466506",
    "type": "article"
  },
  {
    "title": "An Empirical Study on Challenges for LLM Application Developers",
    "doi": "https://doi.org/10.1145/3715007",
    "publication_date": "2025-01-23",
    "publication_year": 2025,
    "authors": "Xiang Chen; Chaoyang Gao; Chunyang Chen; Guangbei Zhang; Yong Liu",
    "corresponding_authors": "",
    "abstract": "In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI’s ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development. Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406738283",
    "type": "article"
  },
  {
    "title": "Hybrid Automated Program Repair by Combining Large Language Models and Program Analysis",
    "doi": "https://doi.org/10.1145/3715004",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Fengjie Li; Jiajun Jiang; Jiajun Sun; Hongyu Zhang",
    "corresponding_authors": "",
    "abstract": "Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this paper introduces an innovative APR approach called GiantRepair . Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, GiantRepair first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that GiantRepair not only effectively repairs more bugs (an average of 27.78% on Defects4J v1.2 and 23.40% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406798911",
    "type": "article"
  },
  {
    "title": "Identifying Affected Third-Party Java Libraries from Textual Descriptions of Vulnerabilities and Libraries",
    "doi": "https://doi.org/10.1145/3717060",
    "publication_date": "2025-02-13",
    "publication_year": 2025,
    "authors": "Tianyu Chen; Lin Li; Bingjie Shan; Guangtai Liang; Ding Li; Qianxiang Wang; Tao Xie",
    "corresponding_authors": "",
    "abstract": "To address security vulnerabilities arising from third-party libraries, security researchers maintain databases monitoring and curating vulnerability reports. Application developers can identify libraries affected by vulnerability reports (in short, affected libraries) by directly querying the databases with their used libraries. However, the querying results of affected libraries are not reliable due to the incompleteness of vulnerability reports. Thus, current approaches model the task of identifying affected libraries as a named-entity-recognition (NER) task or an extreme multi-label learning (XML) task. These approaches suffer from highly inaccurate results in identifying affected libraries with complex and similar names, e.g., Java libraries. To address these limitations, in this article, we propose VulLibMiner, the first to identify affected libraries from textual descriptions of both vulnerabilities and libraries, together with VulLib, a Java vulnerability dataset with their affected libraries. VulLibMiner consists of a TF-IDF matcher to efficiently screen out a small set of candidate libraries and a BERT-FNN model to effectively identify affected libraries from these candidates. We evaluate VulLibMiner using four state-of-the-art/practice approaches of identifying affected libraries on both their dataset named VeraJava and our VulLib dataset. Our evaluation results show that VulLibMiner can effectively identify affected libraries with an average F1 score of 0.669 while the state-of-the-art/practice approaches achieve only 0.547.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4407506186",
    "type": "article"
  },
  {
    "title": "Foundation Model Engineering: Engineering Foundation Models Just as Engineering Software",
    "doi": "https://doi.org/10.1145/3719005",
    "publication_date": "2025-02-21",
    "publication_year": 2025,
    "authors": "Dezhi Ran; M.S. Wu; Wei Yang; Tao Xie",
    "corresponding_authors": "",
    "abstract": "By treating data and models as source code, Foundation Models (FMs) become a new type of software. Mirroring the concept of software crisis, the increasing complexity of FMs makes FM crisis a tangible concern in the coming decade, appealing for new theories and methodologies from the field of software engineering. In this article, we outline our vision of introducing FM engineering, a strategic response to the anticipated FM crisis with principled engineering methodologies. FM engineering aims to mitigate potential issues in FM development and application through the introduction of declarative, automated, and unified programming interfaces for both data and model management, reducing the complexities involved in working with FMs by providing a more structured and intuitive process for developers. Through the establishment of FM engineering, we aim to provide a robust, automated, and extensible framework that addresses the imminent challenges, and discover new research opportunities for the software engineering field.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4407844292",
    "type": "article"
  },
  {
    "title": "Prompt-based Code Completion via Multi-Retrieval Augmented Generation",
    "doi": "https://doi.org/10.1145/3725812",
    "publication_date": "2025-03-26",
    "publication_year": 2025,
    "authors": "Hanzhuo Tan; Qi Luo; Ling Jiang; Zizheng Zhan; Jing Li; Haotian Zhang; Yuqun Zhang",
    "corresponding_authors": "",
    "abstract": "Automated code completion, aiming at generating subsequent tokens from unfinished code, has significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms a widely-studied code completion technique RepoCoder by 7.92% on the public benchmark CCEval, 3.19% in HumanEval-Infilling, 2.80% on our collected open-source benchmark suite, and 4.48% on the private-domain benchmark suite collected from Kuaishou Technology in terms of Exact Match . ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding an averaged 6.5% improvement over the fine-tuned model.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4408868317",
    "type": "article"
  },
  {
    "title": "Ecosystem of Large Language Models for Code",
    "doi": "https://doi.org/10.1145/3731753",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Zhou Yang; Jieke Shi; Prémkumar Dévanbu; David Lo",
    "corresponding_authors": "",
    "abstract": "The extensive availability of publicly accessible source code and the advances in language models, coupled with increasing computational resources, have led to a remarkable rise of large language models for code (LLM4Code). These models do not exist in isolation but rather depend on and interact with each other, forming a complex ecosystem that is worth studying. It motivates us to introduce a pioneering analysis of the LLM4Code ecosystem . Utilizing Hugging Face —the premier hub for transformer-based models—as our primary source, we manually curate a list of datasets and models focused on software engineering tasks. We first identify key datasets, models, and users in the ecosystem and quantify their contributions and importance. We then examine each model's documentation to trace its base model and understand the process for deriving new models. We categorize LLM4Code model reuse into nine categories, with the top three being fine-tuning , architecture sharing , and quantization . Additionally, we examine documentation and licensing practices, revealing that LLM4Code documentation is less detailed than that of general AI repositories on GitHub. The license usage pattern is also different from other software repositories, and we further analyze potential license incompatibility issues. To analyze the rapidly growing LLM4Code, we explore the potential of using LLMs to assist in constructing and analyzing the ecosystem. Advanced LLMs from OpenAI identify LLM4Code with 98% accuracy, infer base models with 87% accuracy, and predict reuse types with 89% accuracy. We employ LLMs to expand the ecosystem and find that conclusions from the manually curated dataset align with those from the automatically created one. Based on our findings, we discuss the implications and suggestions to facilitate the healthy growth of LLM4Code.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4409797602",
    "type": "article"
  },
  {
    "title": "LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models",
    "doi": "https://doi.org/10.1145/3736406",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Zhijie Liu; Yutian Tang; Meiyun Li; Xin Jin; Yunfei Long; Liang Feng Zhang; Xiapu Luo",
    "corresponding_authors": "",
    "abstract": "XML configurations are integral to the Android development framework, particularly in the realm of UI display. However, these configurations can introduce compatibility issues (bugs), resulting in divergent visual outcomes and system crashes across various Android API versions (levels). In this study, we systematically investigate LLM-based approaches for detecting and repairing configuration compatibility bugs. Our findings highlight certain limitations of LLMs in effectively identifying and resolving these bugs, while also revealing their potential in addressing complex, hard-to-repair issues that traditional tools struggle with. Leveraging these insights, we introduce the LLM-CompDroid framework, which combines the strengths of LLMs and traditional tools for bug resolution. Our experimental results demonstrate a significant enhancement in bug resolution performance by LLM-CompDroid, with LLM-CompDroid-GPT-3.5 and LLM-CompDroid-GPT-4 surpassing the state-of-the-art tool, ConfFix, by at least 9.8% and 10.4% in both Correct and Correct@k metrics, respectively. In addition, our real-world evaluation shows that LLM-CompDroid successfully repairs 21 configuration compatibility bugs with a 100% success rate, demonstrating its practical utility. This innovative approach holds promise for advancing the reliability and robustness of Android applications, making a valuable contribution to the field of software development.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4410540918",
    "type": "article"
  },
  {
    "title": "SAFKASI",
    "doi": "https://doi.org/10.1145/363516.363520",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Dan S. Wallach; Andrew W. Appel; Edward W. Felten",
    "corresponding_authors": "",
    "abstract": "In order to run untrusted code in the same process as trusted code, there must be a mechanism to allow dangerous calls to determine if their caller is authorized to exercise the privilege of using the dangerous routine. Java systems have adopted a technique called stack inspection to address this concern. But its original definition, in terms of searching stack frames, had an unclear relationship to the actual achievement of security, overconstrained the implementation of a Java system, limited many desirable optimizations such as method inlining and tail recursion, and generally interfered with interprocedural optimization. We present a new semantics for stack inspection based on a belief logic and its implementation using the calculus of security-passing style which addresses the concerns of traditional stack inspection. With security-passing style, we can efficiently represent the security context for any method activation, and we can build a new implementation strictly by rewriting the Java bytecodes before they are loaded by the system. No changes to the JVM or bytecode semantics are necessary. With a combination of static analysis and runtime optimizations, our prototype implementation showes reasonable performance (although traditional stack inspection is still faster), and is easier to consider for languages beyond Java. We call our system SAFKASI (the Security Architecture Formerly Known as Stack Inspection).",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2016696108",
    "type": "article"
  },
  {
    "title": "APPL/A",
    "doi": "https://doi.org/10.1145/214013.214017",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Stanley M. Sutton; Dennis Heimbigner; Leon J. Osterweil",
    "corresponding_authors": "",
    "abstract": "Software process programming is the coding of software processes in executable programming languages. Process programming offers many potential benefits, but their realization has been hampered by a lack of experience in the design and use of process programming languages. APPL/A is a prototype software process programming language developed to help gain this experience. It is intended for the coding of programs to represent and support software processes including process, product, and project management. APPL/A is defined as an extension to Ada, to which it adds persistent programmable relations, concurrent triggers on relation operations (for reactive control), optionally and dynamically enforceable predicates on relations (which may serve as constraints), and composite statements that provide alternative combinations of serializability, atomicity, and consistency enforcement (for programming high-level transactions). APPL/A has been used to code engineering-oriented applications, like requirements specification and design, as well as management-related activities, such as personnel assignment, task scheduling, and project monitoring. APPL/A has also enabled us to experiment with process program design techniques and architectures, including process state reification, intermittent (or persistent) processes, reflexive and metaprocesses, and multiple-process systems. Our ability to address a wide range of software processes and process characteristics indicates that the APPL/A constructs represent important and general capabilities for software process programming.— Authors' Abstract",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W2065853841",
    "type": "article"
  },
  {
    "title": "A framework for event-based software integration",
    "doi": "https://doi.org/10.1145/235321.235324",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "Daniel Barrett; Lori A. Clarke; Peri Tarr; Alexander Wise",
    "corresponding_authors": "",
    "abstract": "Although event-based software integration is one of the most prevalent approaches to loose integration, no consistent model for describing it exists. As a result, there is no uniform way to discuss event-based integration, compare approaches and implementations, specify new event-based approaches, or match user requirements with the capabilities of event-based integration systems. We attempt to address these shortcomings by specifying a generic framework for event-based integration , the EBI framework, that provides a flexible, object-oriented model for discussing and comparing event-based integration approaches. The EBI framework can model dynamic and static specification, composition, and decomposition and can be instantiated to describe the features of most common event-based integration approaches. We demonstrate how to use the framework as a reference model by comparing and contrasting three well-known integration systems: FIELD, Polylith, and CORBA.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W1987540836",
    "type": "article"
  },
  {
    "title": "An evolutionary approach to constructing effective software reuse repositories",
    "doi": "https://doi.org/10.1145/248233.248242",
    "publication_date": "1997-04-01",
    "publication_year": 1997,
    "authors": "Scott Henninger",
    "corresponding_authors": "Scott Henninger",
    "abstract": "Repositories for software reuse are faced with two interrelated problems: (1) acquiring the knowledge to initially construct the repository and (2) modifying the repository to meet the evolving and dynamic needs of software development organizations. Current software repository methods rely heavily on classification, which exacerbates acquistition and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively, This article outlines an approach that avoids these problems by choosing a retrieval method that utilizes minimal repository structure to effectively support the process of finding software conponents. The approach is demonstrated through a pair of proof-of-concept prototypes: PEEL, a tool to semiautomatically identify reusable components, and CodeFinder, a retrieval system that compensates for the lack of explicit knowledge structures through a spreading activation retrieval process. CodeFinder also allows component representations to be modified while users are searching for information. This mechanism adapts to the changing nature of the information in the repository and incrementally improves the repository while people use it. The combination of these techniques holds potential for designing software repositories that minimize up-front costs, effectively support the search process, and evolve with an organization's changing needs.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2001021688",
    "type": "article"
  },
  {
    "title": "Architecting families of software systems with process algebras",
    "doi": "https://doi.org/10.1145/606612.606614",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Marco Bernardo; Paolo Ciancarini; Lorenzo Donatiello",
    "corresponding_authors": "",
    "abstract": "Software components can give rise to several kinds of architectural mismatches when assembled together in order to form a software system. A formal description of the architecture of the resulting component-based software system may help to detect such architectural mismatches and to single out the components that cause the mismatches. In this article, we concentrate on deadlock-related architectural mismatches arising from three different causes that we identify: incompatibility between two components due to a single interaction, incompatibility between two components due to the combination of several interactions, and lack of interoperability among a set of components forming a cyclic topology. We develop a process algebra-based architectural description language called PADL, which deals with all three causes through an architectural compatibility check and an architectural interoperability check relying on standard observational equivalences. The adequacy of the architectural compatibility check is assessed on a compressing proxy system, while the adequacy of the architectural interoperability check is assessed on a cruise control system. We then address the issue of scaling the architectural compatibility and interoperability checks to architectural styles through an extension of PADL. The formalization of an architectural style is complicated by the presence of two degrees of freedom within the set of instances of the style: variability of the internal behavior of the components and variability of the topology formed by the components. As a first step towards the solution of the problem, we propose an intermediate abstraction called architectural type, whose instances differ only for the internal behavior of their components. We define an efficient architectural conformity check based on a standard observational equivalence to verify whether an architecture is an instance of an architectural type. We show that all the architectures conforming to the same architectural type possess the same compatibility and interoperability properties.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2073514926",
    "type": "article"
  },
  {
    "title": "On test suite composition and cost-effective regression testing",
    "doi": "https://doi.org/10.1145/1027092.1027093",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Gregg Rothermel; Sebastian Elbaum; Alexey Malishevsky; Praveen Kallakuri; Xuemei Qiu",
    "corresponding_authors": "",
    "abstract": "Regression testing is an expensive testing process used to revalidate software as it evolves. Various methodologies for improving regression testing processes have been explored, but the cost-effectiveness of these methodologies has been shown to vary with characteristics of regression test suites. One such characteristic involves the way in which test inputs are composed into test cases within a test suite. This article reports the results of controlled experiments examining the effects of two factors in test suite composition---test suite granularity and test input grouping---on the costs and benefits of several regression-testing-related methodologies: retest-all, regression test selection, test suite reduction, and test case prioritization. These experiments consider the application of several specific techniques, from each of these methodologies, across ten releases each of two substantial software systems, using seven levels of test suite granularity and two types of test input grouping. The effects of granularity, technique, and grouping on the cost and fault-detection effectiveness of regression testing under the given methodologies are analyzed. This analysis shows that test suite granularity significantly affects several cost-benefit factors for the methodologies considered, while test input grouping has limited effects. Further, the results expose essential tradeoffs affecting the relationship between test suite design and regression testing cost-effectiveness, with several implications for practice.",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2039571931",
    "type": "article"
  },
  {
    "title": "A comparative study of coarse- and fine-grained safe regression test-selection techniques",
    "doi": "https://doi.org/10.1145/367008.367015",
    "publication_date": "2001-04-01",
    "publication_year": 2001,
    "authors": "John Bible; Gregg Rothermel; David S. Rosenblum",
    "corresponding_authors": "",
    "abstract": "Regression test-selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite to use in retesting a modified program. Over the past two decades, numerous regression test-selection techniques have been described in the literature. Initial empirical studies of some of these techniques have suggested that they can indeed benefit testers, but so far, few studies have empirically compared different techniques. In this paper, we present the results of a comparative empirical study of two safe regression test-selection techniques. The techniques we studied have been implemented as the tools DejaVu and TestTube; we compared these tools in terms of a cost model incorporating precision (ability to eliminate unnecessary test cases), analysis cost , and test execution cost . Our results indicate, that in many instances, despite its relative lack of precision, TestTube can reduce the time required for regression testing as much as the more precise DejaVu. In other instances, particularly where the time required to execute test cases is long, DejaVu's superior precision gives it a clear advantage over TestTube. Such variations in relative performance can complicate a tester's choice of which tool to use. Our experimental results suggest that a hybrid regression test-selection tool that combines features of TestTube and DejaVu may be an answer to these complications; we present an initial case study that demonstrates the potential benefit of such a tool.",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2155235694",
    "type": "article"
  },
  {
    "title": "Context constraints for compositional reachability analysis",
    "doi": "https://doi.org/10.1145/235321.235323",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "Shing-Chi Cheung; Jeff Kramer",
    "corresponding_authors": "",
    "abstract": "Behavior analysis of complex distributed systems has led to the search for enhanced reachability analysis techniques which support modularity and which control the state explosion problem. While modularity has been achieved, state explosion in still a problem. Indeed, this problem may even be exacerbated, as a locally minimized subsystem may contain many states and transitions forbidden by its environment or context. Context constraints, specified as interface processes, are restrictions imposed by the environment on subsystem behavior. Recent research has suggested that the state explosion problem can be effectively controlled if context constraints are incorporated in compositional reachability analysis (CRA). Although theoretically very promising, the approach has rarely been used in practice because it generally requires a more complex computational model and does not contain a mechanism to derive context constraints automatically. This article presents a technique to automate the approach while using a similar computational model to that of CRA. Context constraints are derived automatically, based on a set of sufficient conditions for these constraints to be transparently included when building reachability graphs. As a result, the global reachability graph generated using the derived constraints is shown to be observationally equivalent to that generated by CRA without the inclusion of context constraints. Constraints can also be specified explicitly by users, based on their application knowledge. Erroneous constraints which contravene transparency can be identified together with an indication of the error sources. User-specified constraints can be combined with those generated automatically. The technique is illustrated using a clients/server system and other examples.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2120521925",
    "type": "article"
  },
  {
    "title": "Mobile UNITY",
    "doi": "https://doi.org/10.1145/258077.258079",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Gruia-Catalin Roman; Peter J. McCann; Jerome Y. Plun",
    "corresponding_authors": "",
    "abstract": "Mobile computing represents a major point of departure from the traditional distributed-computing paradigm. The potentially very large number of independent computing units, a decoupled computing style, frequent disconnections, continuous position changes, and the location-dependent nature of the behavior and communication patterns present designers with unprecedented challenges in the areas of modularity and dependability. So far, the literature on mobile computing is dominated by concerns having to de with the development of protocols and services. This article complements this perspective by considering the nature of the underlying formal models that will enable us to specify and reason about such computations. The basic research goal is to characterize fundamental issues facing mobile computing. We want to achieve this in a manner analogous to the way concepts such as shared variables and message passing help us understand distributed computing. The pragmatic objective is to develop techniques that facilitate the verification and design of dependable mobile systems. Toward this goal we employ the methods of UNITY. To focus on what is essential, we center our study on ad hoc networks , whose singular nature is bound to reveal the ultimate impact of movement on the way one computes and communicates in a mobile environment. To understand interactions we start with the UNITY concepts of union and superposition and consider direct generalizations to transient interactions. The motivation behind the transient nature of the interactions comes from the fact that components can communicate with each other only when they are within a a certain range. The notation we employ is a highly modular extension of the UNITY programming notation. Reasoning about mobile computations relies on extensions to the UNITY proof logic.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W1963513516",
    "type": "article"
  },
  {
    "title": "Behavior-consistent specialization of object life cycles",
    "doi": "https://doi.org/10.1145/504087.504091",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Michael Schrefl; Markus Stumptner",
    "corresponding_authors": "",
    "abstract": "Object-oriented design methodologies represent the behavior of instances of an object type not merely by a set of operations, but also by providing an overall description on how instances evolve over time. Such a description is often referred to as \"object life cycle.\"Object-oriented systems organize object types in hierarchies in which subtypes inherit and specialize the structure and behavior of their supertypes. Past experience has shown that unrestricted use of inheritance mechanisms leads to system architectures that are hard to understand and to maintain, since arbitrary differences between supertype and subtype are possible. Evidently, this is not a desirable state of affairs and the behavior of a subtype should specialize the behavior of its supertype according to some clearly defined consistency criteria. Such criteria have been formulated in terms of type systems for semantic data models and object-oriented programming languages. But corresponding criteria for the specialization of object life cycles have so far not been thoroughly investigated.This paper defines such criteria in the realm of Object Behavior Diagrams, which have been originally developed for the design of object-oriented databases. Its main contributions are necessary and sufficient rules for checking behavior consistency between object life cycles of object types in specialization hierarchies with multiple inheritance.",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W2077479316",
    "type": "article"
  },
  {
    "title": "The interpretation and utility of three cohesion metrics for object-oriented design",
    "doi": "https://doi.org/10.1145/1131421.1131422",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "Steve Counsell; Stephen Swift; Jason Crampton",
    "corresponding_authors": "",
    "abstract": "The concept of cohesion in a class has been the subject of various recent empirical studies and has been measured using many different metrics. In the structured programming paradigm, the software engineering community has adopted an informal yet meaningful and understandable definition of cohesion based on the work of Yourdon and Constantine. The object-oriented (OO) paradigm has formalised various cohesion measures, but the argument over the most meaningful of those metrics continues to be debated. Yet achieving highly cohesive software is fundamental to its comprehension and thus its maintainability. In this article we subject two object-oriented cohesion metrics, CAMC and NHD, to a rigorous mathematical analysis in order to better understand and interpret them. This analysis enables us to offer substantial arguments for preferring the NHD metric to CAMC as a measure of cohesion. Furthermore, we provide a complete understanding of the behaviour of these metrics, enabling us to attach a meaning to the values calculated by the CAMC and NHD metrics. In addition, we introduce a variant of the NHD metric and demonstrate that it has several advantages over CAMC and NHD. While it may be true that a generally accepted formal and informal definition of cohesion continues to elude the OO software engineering community, there seems considerable value in being able to compare, contrast, and interpret metrics which attempt to measure the same features of software.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2000819496",
    "type": "article"
  },
  {
    "title": "Generation of formatters for context-free languages",
    "doi": "https://doi.org/10.1145/226155.226156",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Mark van den Brand; Eelco Visser",
    "corresponding_authors": "",
    "abstract": "Good documentation is important for the production of reusable and maintainable software. For the production of accurate documentation it is necessary that the original program text is not copied manually to obtain a typeset version. Apart from being tedious, this will invariably introduce errors. The production of tools that support the production of legible and accurate documentation is a software engineering challenge in itself. We present an algebraic approach to the generation of tools that produce typographically effective presentations of computer programs. A specification of a formatter is generated from the context-free grammar of a (programming) language. These generated formatters translate abstract syntax trees of programs into box expressions. Box expressions are translated by language-independent interpreters of the box language into ASCII or T E X. The formatting rules that are generated can easily be tuned in order to get the desired formatting of programs. We demonstrate this by means of real-life applications. Furthermore, we give a practical solution for the problem of formatting comments, which occur in the original text. The formatter generation approach proposed in this article can be used to generate formatting programs for arbitrary programming environments. Our formatter generation approach can be used to automatically generate formatters that have to be programmed explicitly in other systems.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2100895292",
    "type": "article"
  },
  {
    "title": "A graphical interval logic for specifying concurrent systems",
    "doi": "https://doi.org/10.1145/192218.192226",
    "publication_date": "1994-04-01",
    "publication_year": 1994,
    "authors": "Laura K. Dillon; G. Kutty; L.E. Moser; P. M. Melliar‐Smith; Y. S. Ramakrishna",
    "corresponding_authors": "",
    "abstract": "This article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems. Experience has shown that most software engineers find standard temporal logics difficult to understand and use. The objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use. To illustrate the use of the graphical logic, the article provides some specifications for an elevator system and proves several properties of the specifications. The article also describes the tool set and the implementation.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2018118431",
    "type": "article"
  },
  {
    "title": "Checking safety properties using compositional reachability analysis",
    "doi": "https://doi.org/10.1145/295558.295570",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Shing-Chi Cheung; Jeff Kramer",
    "corresponding_authors": "",
    "abstract": "The software architecture of a distributed program can be represented by a hierarchical composition of subsystems, with interacting processes at the leaves of the hierarchy. Compositional reachability analysis (CRA) is a promising state reduction technique which can be automated and used in stages to derive the overall behavior of a distributed program based on its architecture. CRA is particularly suitable for the analysis of programs that are subject to evolutionary change. When a program evolves, only the behaviors of those subsystems affected by the change need be reevaluated. The technique however has a limitation. The properties available for analysis are constrained by the set of actions that remain globally observable. Properties involving actions encapsulated by subsystems may therefore not be analyzed. In this article, we enhance the CRA technique to check safety properties which may contain actions that are not globally observable. To achieve this, the state machine model is augmented with a special trap state labeled as π. We propose a scheme to transform, in stages, a property that involves hidden actions to one that involves only globally observable actions. The enhanced technique also includes a mechanism aiming at reducing the debugging effort. The technique is illustrated using a gas station system example.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2024613742",
    "type": "article"
  },
  {
    "title": "An upper bound on software testing effectiveness",
    "doi": "https://doi.org/10.1145/1363102.1363107",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Tsong Yueh Chen; Robert Merkel",
    "corresponding_authors": "",
    "abstract": "Failure patterns describe typical ways in which inputs revealing program failure are distributed across the input domain—in many cases, clustered together in contiguous regions. Based on these observations several debug testing methods have been developed. We examine the upper bound of debug testing effectiveness improvements possible through making assumptions about the shape, size and orientation of failure patterns. We consider the bounds for testing strategies with respect to minimizing the F-measure, maximizing the P-measure, and maximizing the E-measure. Surprisingly, we find that the empirically measured effectiveness of some existing methods that are not based on these assumptions is close to the theoretical upper bound of these strategies. The assumptions made to obtain the upper bound, and its further implications, are also examined.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2034112596",
    "type": "article"
  },
  {
    "title": "Identifying Crosscutting Concerns Using Fan-In Analysis",
    "doi": "https://doi.org/10.1145/1314493.1314496",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Marius Marin; Arie van Deursen; Leon Moonen",
    "corresponding_authors": "",
    "abstract": "Aspect mining is a reverse engineering process that aims at finding crosscutting concerns in existing systems. This article proposes an aspect mining approach based on determining methods that are called from many different places, and hence have a high fan-in , which can be seen as a symptom of crosscutting functionality. The approach is semiautomatic, and consists of three steps: metric calculation, method filtering, and call site analysis. Carrying out these steps is an interactive process supported by an Eclipse plug-in called FINT. Fan-in analysis has been applied to three open source Java systems, totaling around 200,000 lines of code. The most interesting concerns identified are discussed in detail, which includes several concerns not previously discussed in the aspect-oriented literature. The results show that a significant number of crosscutting concerns can be recognized using fan-in analysis, and each of the three steps can be supported by tools.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2171008953",
    "type": "article"
  },
  {
    "title": "Using a pilot study to derive a GUI model for automated testing",
    "doi": "https://doi.org/10.1145/1416563.1416567",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Qing Xie; Atif M. Memon",
    "corresponding_authors": "",
    "abstract": "Graphical user interfaces (GUIs) are one of the most commonly used parts of today's software. Despite their ubiquity, testing GUIs for functional correctness remains an understudied area. A typical GUI gives many degrees of freedom to an end-user, leading to an enormous input event interaction space that needs to be tested. GUI test designers generate and execute test cases (modeled as sequences of user events ) to traverse its parts; targeting a subspace in order to maximize fault detection is a nontrivial task. In this vein, in previous work, we used informal GUI code examination and personal intuition to develop an event-interaction graph (EIG). In this article we empirically derive the EIG model via a pilot study, and the resulting EIG validates our intuition used in previous work; the empirical derivation process also allows for model evolution as our understanding of GUI faults improves. Results of the pilot study show that events interact in complex ways; a GUI's response to an event may vary depending on the context established by preceding events and their execution order. The EIG model helps testers to understand the nature of interactions between GUI events when executed in test cases and why certain events detect faults, so that they can better traverse the event space. New test adequacy criteria are defined for the EIG; new algorithms use these criteria and EIG to systematically generate test cases that are shown to be effective on four fielded open-source applications.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W1993897010",
    "type": "article"
  },
  {
    "title": "Unit-level test adequacy criteria for visual dataflow languages and a testing methodology",
    "doi": "https://doi.org/10.1145/1391984.1391985",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Marcel Karam; Trevor J. Smedley; Sergiu M. Dascalu",
    "corresponding_authors": "",
    "abstract": "Visual dataflow languages (VDFLs), which include commercial and research systems, have had a substantial impact on end-user programming. Like any other programming languages, whether visual or textual, VDFLs often contain faults. A desire to provide programmers of these languages with some of the benefits of traditional testing methodologies has been the driving force behind our effort in this work. In this article we introduce, in the context of prograph, a testing methodology for VDFLs based on structural test adequacy criteria and coverage. This article also reports on the results of two empirical studies. The first study was conducted to obtain meaningful information about, in particular, the effectiveness of our all-Dus criteria in detecting a reasonable percentage of faults in VDFLs. The second study was conducted to evaluate, under the same criterion, the effectiveness of our methodology in assisting users to visually localize faults by reducing their search space. Both studies were conducted using a testing system that we have implemented in Prograph's IDE.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2056488032",
    "type": "article"
  },
  {
    "title": "An empirical study of static program slice size",
    "doi": "https://doi.org/10.1145/1217295.1217297",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "David Binkley; Nicolas Gold; Mark Harman",
    "corresponding_authors": "",
    "abstract": "This article presents results from a study of all slices from 43 programs, ranging up to 136,000 lines of code in size. The study investigates the effect of five aspects that affect slice size. Three slicing algorithms are used to study two algorithmic aspects: calling-context treatment and slice granularity. The remaining three aspects affect the upstream dependencies considered by the slicer. These include collapsing structure fields, removal of dead code, and the influence of points-to analysis. The results show that for the most precise slicer, the average slice contains just under one-third of the program. Furthermore, ignoring calling context causes a 50% increase in slice size, and while (coarse-grained) function-level slices are 33% larger than corresponding statement-level slices, they may be useful predictors of the (finer-grained) statement-level slice size. Finally, upstream analyses have an order of magnitude less influence on slice size.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2116682334",
    "type": "article"
  },
  {
    "title": "Static checking of dynamically generated queries in database applications",
    "doi": "https://doi.org/10.1145/1276933.1276935",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Gary Wassermann; Carl Gould; Zhendong Su; Prémkumar Dévanbu",
    "corresponding_authors": "",
    "abstract": "Many data-intensive applications dynamically construct queries in response to client requests and execute them. Java servlets, for example, can create strings that represent SQL queries and then send the queries, using JDBC, to a database server for execution. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. Thus, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this article, we present a sound , static program analysis technique to verify that dynamically generated query strings do not contain type errors. We describe our analysis technique and provide soundness results for our static analysis algorithm. We also describe the details of a prototype tool based on the algorithm and present several illustrative defects found in senior software-engineering student-team projects, online tutorial examples, and a real-world purchase order system written by one of the authors.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2003751975",
    "type": "article"
  },
  {
    "title": "An empirical study of slice-based cohesion and coupling metrics",
    "doi": "https://doi.org/10.1145/1314493.1314495",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Timothy M. Meyers; David Binkley",
    "corresponding_authors": "",
    "abstract": "Software reengineering is a costly endeavor, due in part to the ambiguity of where to focus reengineering effort. Coupling and Cohesion metrics, particularly quantitative cohesion metrics, have the potential to aid in this identification and to measure progress. The most extensive work on such metrics is with slice-based cohesion metrics. While their use of semantic dependence information should make them an excellent choice for cohesion measurement, their wide spread use has been impeded in part by a lack of empirical study. Recent advances in software tools make, for the first time, a large-scale empirical study of slice-based cohesion and coupling metrics possible. Four results from such a study are presented. First, “head-to-head” qualitative and quantitative comparisons of the metrics identify which metrics provide similar views of a program and which provide unique views of a program. This study includes statistical analysis showing that slice-based metrics are not proxies for simple size-based metrics such as lines of code. Second, two longitudinal studies show that slice-based metrics quantify the deterioration of a program as it ages. This serves to validate the metrics: the metrics quantify the degradation that exists during development; turning this around, the metrics can be used to measure the progress of a reengineering effort. Third, baseline values for slice-based metrics are provided. These values act as targets for reengineering efforts with modules having values outside the expected range being the most in need of attention. Finally, slice-based coupling is correlated and compared with slice-based cohesion.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W1993126296",
    "type": "article"
  },
  {
    "title": "Combining symbolic execution with model checking to verify parallel numerical programs",
    "doi": "https://doi.org/10.1145/1348250.1348256",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Stephen F. Siegel; Anastasia Mironova; George S. Avrunin; Lori A. Clarke",
    "corresponding_authors": "",
    "abstract": "We present a method to verify the correctness of parallel programs that perform complex numerical computations, including computations involving floating-point arithmetic. This method requires that a sequential version of the program be provided, to serve as the specification for the parallel one. The key idea is to use model checking, together with symbolic execution, to establish the equivalence of the two programs. In this approach the path condition from symbolic execution of the sequential program is used to constrain the search through the parallel program. To handle floating-point operations, three different types of equivalence are supported. Several examples are presented, demonstrating the approach and actual errors that were found. Limitations and directions for future research are also described.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2138856138",
    "type": "article"
  },
  {
    "title": "A systematic review of theory use in studies investigating the motivations of software engineers",
    "doi": "https://doi.org/10.1145/1525880.1525883",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Tracy Hall; Nathan Baddoo; Sarah Beecham; Hugh Robinson; Helen Sharp",
    "corresponding_authors": "",
    "abstract": "Motivated software engineers make a critical contribution to delivering successful software systems. Understanding the motivations of software engineers and the impact of motivation on software engineering outcomes could significantly affect the industry's ability to deliver good quality software systems. Understanding the motivations of people generally in relation to their work is underpinned by eight classic motivation theories from the social sciences. We would expect these classic motivation theories to play an important role in developing a rigorous understanding of the specific motivations of software engineers. In this article we investigate how this theoretical basis has been exploited in previous studies of software engineering. We analyzed 92 studies of motivation in software engineering that were published in the literature between 1980 and 2006. Our main findings are that many studies of software engineers' motivations are not explicitly underpinned by reference to the classic motivation theories. Furthermore, the findings presented in these studies are often not explicitly interpreted in terms of those theories, despite the fact that in many cases there is a relationship between those findings and the theories. Our conclusion is that although there has been a great deal of previous work looking at motivation in software engineering, the lack of reference to classic theories of motivation means that the current body of work in the area is weakened and our understanding of motivation in software engineering is not as rigorous as it may at first appear. This weakness in the current state of knowledge highlights important areas for future researchers to contribute towards developing a rigorous and usable body of knowledge in motivating software engineers.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2052438299",
    "type": "review"
  },
  {
    "title": "Verification and Validation of UML Conceptual Schemas with OCL Constraints",
    "doi": "https://doi.org/10.1145/2089116.2089123",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Anna Queralt; Ernest Teniente",
    "corresponding_authors": "",
    "abstract": "To ensure the quality of an information system, it is essential that the conceptual schema that represents the knowledge about its domain is semantically correct. The semantic correctness of a conceptual schema can be seen from two different perspectives. On the one hand, from the point of view of its definition, a conceptual schema must be right . This is ensured by means of verification techniques that check whether the schema satisfies several correctness properties. On the other hand, from the point of view of the requirements that the information system should satisfy, a schema must also be the right one . This is ensured by means of validation techniques, which help the designer understand the exact meaning of a schema and to see whether it corresponds to the requirements. In this article we propose an approach to verify and validate UML conceptual schemas, with arbitrary constraints formalized in OCL. We have also implemented our approach to show its feasibility.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2009691504",
    "type": "article"
  },
  {
    "title": "An Information Foraging Theory Perspective on Tools for Debugging, Refactoring, and Reuse Tasks",
    "doi": "https://doi.org/10.1145/2430545.2430551",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Scott Fleming; Chris Scaffidi; David Piorkowski; Margaret Burnett; Rachel Bellamy; Joseph Lawrance; Irwin Kwan",
    "corresponding_authors": "",
    "abstract": "Theories of human behavior are an important but largely untapped resource for software engineering research. They facilitate understanding of human developers’ needs and activities, and thus can serve as a valuable resource to researchers designing software engineering tools. Furthermore, theories abstract beyond specific methods and tools to fundamental principles that can be applied to new situations. Toward filling this gap, we investigate the applicability and utility of Information Foraging Theory (IFT) for understanding information-intensive software engineering tasks, drawing upon literature in three areas: debugging, refactoring, and reuse. In particular, we focus on software engineering tools that aim to support information-intensive activities, that is, activities in which developers spend time seeking information. Regarding applicability, we consider whether and how the mathematical equations within IFT can be used to explain why certain existing tools have proven empirically successful at helping software engineers. Regarding utility, we applied an IFT perspective to identify recurring design patterns in these successful tools, and consider what opportunities for future research are revealed by our IFT perspective.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2044049174",
    "type": "article"
  },
  {
    "title": "PrIMe",
    "doi": "https://doi.org/10.1145/2000791.2000792",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Simon Miles; Paul Groth; Steve Munroe; Luc Moreau",
    "corresponding_authors": "",
    "abstract": "Provenance refers to the past processes that brought about a given (version of an) object, item or entity. By knowing the provenance of data, users can often better understand, trust, reproduce, and validate it. A provenance-aware application has the functionality to answer questions regarding the provenance of the data it produces, by using documentation of past processes. PrIMe is a software engineering technique for adapting application designs to enable them to interact with a provenance middleware layer, thereby making them provenance-aware. In this article, we specify the steps involved in applying PrIMe, analyze its effectiveness, and illustrate its use with two case studies, in bioinformatics and medicine.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2058400746",
    "type": "article"
  },
  {
    "title": "Clone region descriptors",
    "doi": "https://doi.org/10.1145/1767751.1767754",
    "publication_date": "2010-06-01",
    "publication_year": 2010,
    "authors": "Ekwa Duala-Ekoko; Martin P. Robillard",
    "corresponding_authors": "",
    "abstract": "Source code duplication, commonly known as code cloning , is considered an obstacle to software maintenance because changes to a cloned region often require consistent changes to other regions of the source code. Research has provided evidence that the elimination of clones may not always be practical, feasible, or cost-effective. We present a clone management approach that describes clone regions in a robust way that is independent from the exact text of clone regions or their location in a file, and that provides support for tracking clones in evolving software. Our technique relies on the concept of abstract clone region descriptors (CRDs), which describe clone regions using a combination of their syntactic, structural, and lexical information. We present our definition of CRDs, and describe a clone tracking system capable of producing CRDs from the output of different clone detection tools, notifying developers of modifications to clone regions, and supporting updates to the documented clone relationships. We evaluated the performance and usefulness of our approach across three clone detection tools and five subject systems, and the results indicate that CRDs are a practical and robust representation for tracking code clones in evolving software.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2039822794",
    "type": "article"
  },
  {
    "title": "HAMPI",
    "doi": "https://doi.org/10.1145/2377656.2377662",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Adam Kieżun; Vijay Ganesh; Shay Artzi; Philip J. Guo; Pieter Hooimeijer; Michael D. Ernst",
    "corresponding_authors": "",
    "abstract": "Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable software reliability tools. The increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis of string-manipulating programs, so researchers end up implementing their own ad-hoc solvers. To fulfill this need, we designed and implemented Hampi, a solver for string constraints over bounded string variables. Users of Hampi specify constraints using regular expressions, context-free grammars, equality between string terms, and typical string operations such as concatenation and substring extraction. Hampi then finds a string that satisfies all the constraints or reports that the constraints are unsatisfiable. We demonstrate Hampi's expressiveness and efficiency by applying it to program analysis and automated testing. We used Hampi in static and dynamic analyses for finding SQL injection vulnerabilities in Web applications with hundreds of thousands of lines of code. We also used Hampi in the context of automated bug finding in C programs using dynamic systematic testing (also known as concolic testing). We then compared Hampi with another string solver, CFGAnalyzer, and show that Hampi is several times faster. Hampi's source code, documentation, and experimental data are available at http://people.csail.mit.edu/akiezun/hampi 1",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2044901139",
    "type": "article"
  },
  {
    "title": "Partial constraint checking for context consistency in pervasive computing",
    "doi": "https://doi.org/10.1145/1656250.1656253",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Chang Xu; Shing-Chi Cheung; W. K. Chan; Chunyang Ye",
    "corresponding_authors": "",
    "abstract": "Pervasive computing environments typically change frequently in terms of available resources and their properties. Applications in pervasive computing use contexts to capture these changes and adapt their behaviors accordingly. However, contexts available to these applications may be abnormal or imprecise due to environmental noises. This may result in context inconsistencies, which imply that contexts conflict with each other. The inconsistencies may set such an application into a wrong state or lead the application to misadjust its behavior. It is thus desirable to detect and resolve the context inconsistencies in a timely way. One popular approach is to detect context inconsistencies when contexts breach certain consistency constraints. Existing constraint checking techniques recheck the entire expression of each affected consistency constraint upon context changes. When a changed context affects only a constraint's subexpression, rechecking the entire expression can adversely delay the detection of other context inconsistencies. This article proposes a rigorous approach to identifying the parts of previous checking results that are reusable without entire rechecking. We evaluated our work on the Cabot middleware through both simulation experiments and a case study. The experimental results reported that our approach achieved over a fifteenfold performance improvement on context inconsistency detection than conventional approaches.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2064831234",
    "type": "article"
  },
  {
    "title": "Modeling and verifying hierarchical real-time systems using stateful timed CSP",
    "doi": "https://doi.org/10.1145/2430536.2430537",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Jun Sun; Yang Liu; Jin Song Dong; Y. Liu; Ling Shi; Étienne André",
    "corresponding_authors": "",
    "abstract": "Modeling and verifying complex real-time systems are challenging research problems. The de facto approach is based on Timed Automata, which are finite state automata equipped with clock variables. Timed Automata are deficient in modeling hierarchical complex systems. In this work, we propose a language called Stateful Timed CSP and an automated approach for verifying Stateful Timed CSP models. Stateful Timed CSP is based on Timed CSP and is capable of specifying hierarchical real-time systems. Through dynamic zone abstraction, finite-state zone graphs can be generated automatically from Stateful Timed CSP models, which are subject to model checking. Like Timed Automata, Stateful Timed CSP models suffer from Zeno runs, that is, system runs that take infinitely many steps within finite time. Unlike Timed Automata, model checking with non-Zenoness in Stateful Timed CSP can be achieved based on the zone graphs. We extend the PAT model checker to support system modeling and verification using Stateful Timed CSP and show its usability/scalability via verification of real-world systems.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2167694722",
    "type": "article"
  },
  {
    "title": "Temporal dependency-based checkpoint selection for dynamic verification of temporal constraints in scientific workflow systems",
    "doi": "https://doi.org/10.1145/2000791.2000793",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Jinjun Chen; Yun Yang",
    "corresponding_authors": "",
    "abstract": "In a scientific workflow system, a checkpoint selection strategy is used to select checkpoints along scientific workflow execution for verifying temporal constraints so that we can identify any temporal violations and handle them in time in order to ensure overall temporal correctness of the execution that is often essential for the usefulness of execution results. The problem of existing representative strategies is that they do not differentiate temporal constraints as, once a checkpoint is selected, they verify all temporal constraints. However, such a checkpoint does not need to be taken for those constraints whose consistency can be deduced from others. The corresponding verification of such constraints is consequently unnecessary and can severely impact overall temporal verification efficiency while the efficiency determines whether temporal violations can be identified quickly for handling in time. To address the problem, in this article, we develop a new temporal-dependency based checkpoint selection strategy which can select checkpoints in accordance with different temporal constraints. With our strategy, the corresponding unnecessary verification can be avoided. The comparison and experimental simulation further demonstrate that our new strategy can improve the efficiency of overall temporal verification significantly over the existing representative strategies.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2022905422",
    "type": "article"
  },
  {
    "title": "An in-depth study of the potentially confounding effect of class size in fault prediction",
    "doi": "https://doi.org/10.1145/2556777",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Yuming Zhou; Baowen Xu; Hareton Leung; Lin Chen",
    "corresponding_authors": "",
    "abstract": "Background . The extent of the potentially confounding effect of class size in the fault prediction context is not clear, nor is the method to remove the potentially confounding effect, or the influence of this removal on the performance of fault-proneness prediction models. Objective . We aim to provide an in-depth understanding of the effect of class size on the true associations between object-oriented metrics and fault-proneness. Method . We first employ statistical methods to examine the extent of the potentially confounding effect of class size in the fault prediction context. After that, we propose a linear regression-based method to remove the potentially confounding effect. Finally, we empirically investigate whether this removal could improve the prediction performance of fault-proneness prediction models. Results . Based on open-source software systems, we found: (a) the confounding effect of class size on the associations between object-oriented metrics and fault-proneness in general exists; (b) the proposed linear regression-based method can effectively remove the confounding effect; and (c) after removing the confounding effect, the prediction performance of fault prediction models with respect to both ranking and classification can in general be significantly improved. Conclusion . We should remove the confounding effect of class size when building fault prediction models.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W1985797722",
    "type": "article"
  },
  {
    "title": "Key factors for adopting inner source",
    "doi": "https://doi.org/10.1145/2533685",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Klaas-Jan Stol; Paris Avgeriou; Muhammad Ali Babar; Yan Lucas; Brian Fitzgerald",
    "corresponding_authors": "",
    "abstract": "A number of organizations have adopted Open Source Software (OSS) development practices to support or augment their software development processes, a phenomenon frequently referred to as Inner Source . However the adoption of Inner Source is not a straightforward issue. Many organizations are struggling with the question of whether Inner Source is an appropriate approach to software development for them in the first place. This article presents a framework derived from the literature on Inner Source, which identifies nine important factors that need to be considered when implementing Inner Source. The framework can be used as a probing instrument to assess an organization on these nine factors so as to gain an understanding of whether or not Inner Source is suitable. We applied the framework in three case studies at Philips Healthcare, Neopost Technologies, and Rolls-Royce, which are all large organizations that have either adopted Inner Source or were planning to do so. Based on the results presented in this article, we outline directions for future research.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2028264492",
    "type": "article"
  },
  {
    "title": "Editorial—looking back",
    "doi": "https://doi.org/10.1145/2430536.2431201",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2013854827",
    "type": "article"
  },
  {
    "title": "Do we need to handle every temporal violation in scientific workflow systems?",
    "doi": "https://doi.org/10.1145/2559938",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Xiao Liu; Yun Yang; Dong Yuan; Jinjun Chen",
    "corresponding_authors": "",
    "abstract": "Scientific processes are usually time constrained with overall deadlines and local milestones. In scientific workflow systems, due to the dynamic nature of the underlying computing infrastructures such as grid and cloud, execution delays often take place and result in a large number of temporal violations. Since temporal violation handling is expensive in terms of both monetary costs and time overheads, an essential question aroused is “do we need to handle every temporal violation in scientific workflow systems?” The answer would be “true” according to existing works on workflow temporal management which adopt the philosophy similar to the handling of functional exceptions, that is, every temporal violation should be handled whenever it is detected. However, based on our observation, the phenomenon of self-recovery where execution delays can be automatically compensated for by the saved execution time of subsequent workflow activities has been entirely overlooked. Therefore, considering the nonfunctional nature of temporal violations, our answer is “not necessarily true.” To take advantage of self-recovery, this article proposes a novel adaptive temporal violation handling point selection strategy where this phenomenon is effectively utilised to avoid unnecessary temporal violation handling. Based on simulations of both real-world scientific workflows and randomly generated test cases, the experimental results demonstrate that our strategy can significantly reduce the cost on temporal violation handling by over 96% while maintaining extreme low violation rate under normal circumstances.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2170835573",
    "type": "article"
  },
  {
    "title": "Degree-of-knowledge",
    "doi": "https://doi.org/10.1145/2512207",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Thomas Fritz; Gail C. Murphy; Emerson Murphy-Hill; Jingwen Ou; Emily Hill",
    "corresponding_authors": "",
    "abstract": "As a software system evolves, the system's codebase constantly changes, making it difficult for developers to answer such questions as who is knowledgeable about particular parts of the code or who needs to know about changes made. In this article, we show that an externalized model of a developer's individual knowledge of code can make it easier for developers to answer such questions. We introduce a degree-of-knowledge model that computes automatically, for each source-code element in a codebase, a real value that represents a developer's knowledge of that element based on a developer's authorship and interaction data. We present evidence that shows that both authorship and interaction data of the code are important in characterizing a developer's knowledge of code. We report on the usage of our model in case studies on expert finding, knowledge transfer, and identifying changes of interest. We show that our model improves upon an existing expertise-finding approach and can accurately identify changes for which a developer should likely be aware. We discuss how our model may provide a starting point for knowledge transfer but that more refinement is needed. Finally, we discuss the robustness of the model across multiple development sites.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W1975318342",
    "type": "article"
  },
  {
    "title": "Guidelines for Coverage-Based Comparisons of Non-Adequate Test Suites",
    "doi": "https://doi.org/10.1145/2660767",
    "publication_date": "2015-09-02",
    "publication_year": 2015,
    "authors": "Milos Gligoric; Alex Groce; Chaoqiang Zhang; Rohan Sharma; Mohammad Amin Alipour; Darko Marinov",
    "corresponding_authors": "",
    "abstract": "A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques that produce those test suites. Researchers frequently compare test suites by measuring their coverage . A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the feasible requirements is called C-adequate . Previous rigorous evaluations of coverage criteria mostly focused on such adequate test suites: given two criteria C and C ′, are C -adequate suites on average more effective than C ′-adequate suites? However, in many realistic cases, producing adequate suites is impractical or even impossible. This article presents the first extensive study that evaluates coverage criteria for the common case of non-adequate test suites: given two criteria C and C ′, which one is better to use to compare test suites? Namely, if suites T 1 , T 2 ,…, T n have coverage values c 1 , c 2 ,…, c n for C and c 1 ′, c 2 ′,…, c n ′ for C ′, is it better to compare suites based on c 1 , c 2 ,…, c n or based on c 1 ′, c 2 ′,…, c n ′ ? We evaluate a large set of plausible criteria, including basic criteria such as statement and branch coverage, as well as stronger criteria used in recent studies, including criteria based on program paths, equivalence classes of covered statements, and predicate states. The criteria are evaluated on a set of Java and C programs with both manually written and automatically generated test suites. The evaluation uses three correlation measures. Based on these experiments, two criteria perform best: branch coverage and an intraprocedural acyclic path coverage. We provide guidelines for testing researchers aiming to evaluate test suites using coverage criteria as well as for other researchers evaluating coverage criteria for research use.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W1984481956",
    "type": "article"
  },
  {
    "title": "Formal Verification of Software Countermeasures against Side-Channel Attacks",
    "doi": "https://doi.org/10.1145/2685616",
    "publication_date": "2014-12-23",
    "publication_year": 2014,
    "authors": "Hassan Eldib; Chao Wang; Patrick Schaumont",
    "corresponding_authors": "",
    "abstract": "A common strategy for designing countermeasures against power-analysis-based side-channel attacks is using random masking techniques to remove the statistical dependency between sensitive data and side-channel emissions. However, this process is both labor intensive and error prone and, currently, there is a lack of automated tools to formally assess how secure a countermeasure really is. We propose the first SMT-solver-based method for formally verifying the security of a masking countermeasure against such attacks. In addition to checking whether the sensitive data are masked by random variables, we also check whether they are perfectly masked , that is, whether the intermediate computation results in the implementation of a cryptographic algorithm are independent of the secret key. We encode this verification problem using a series of quantifier-free first-order logic formulas, whose satisfiability can be decided by an off-the-shelf SMT solver. We have implemented the proposed method in a software verification tool based on the LLVM compiler frontend and the Yices SMT solver. Our experiments on a set of recently proposed masking countermeasures for cryptographic algorithms such as AES and MAC-Keccak show the method is both effective in detecting power side-channel leaks and scalable for practical use.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2072550684",
    "type": "article"
  },
  {
    "title": "Understanding Integer Overflow in C/C++",
    "doi": "https://doi.org/10.1145/2743019",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Will Dietz; Peng Li; John Regehr; Vikram Adve",
    "corresponding_authors": "",
    "abstract": "Integer overflow bugs in C and C++ programs are difficult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for finding these bugs exist, the situation is complicated because not all overflows are bugs. Better tools need to be constructed, but a thorough understanding of the issues behind these errors does not yet exist. We developed IOC, a dynamic checking tool for integer overflows, and used it to conduct the first detailed empirical study of the prevalence and patterns of occurrence of integer overflows in C and C++ code. Our results show that intentional uses of wraparound behaviors are more common than is widely believed; for example, there are over 200 distinct locations in the SPEC CINT2000 benchmarks where overflow occurs. Although many overflows are intentional, a large number of accidental overflows also occur. Orthogonal to programmers' intent, overflows are found in both well-defined and undefined flavors. Applications executing undefined operations can be, and have been, broken by improvements in compiler optimizations. Looking beyond SPEC, we found and reported undefined integer overflows in SQLite, PostgreSQL, SafeInt, GNU MPC and GMP, Firefox, LLVM, Python, BIND, and OpenSSL; many of these have since been fixed.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2621026180",
    "type": "article"
  },
  {
    "title": "Mining Privacy Goals from Privacy Policies Using Hybridized Task Recomposition",
    "doi": "https://doi.org/10.1145/2907942",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Jaspreet Bhatia; Travis D. Breaux; Florian Schaub",
    "corresponding_authors": "",
    "abstract": "Privacy policies describe high-level goals for corporate data practices; regulators require industries to make available conspicuous, accurate privacy policies to their customers. Consequently, software requirements must conform to those privacy policies. To help stakeholders extract privacy goals from policies, we introduce a semiautomated framework that combines crowdworker annotations, natural language typed dependency parses, and a reusable lexicon to improve goal-extraction coverage, precision, and recall. The framework evaluation consists of a five-policy corpus governing web and mobile information systems, yielding an average precision of 0.73 and recall of 0.83. The results show that no single framework element alone is sufficient to extract goals; however, the overall framework compensates for elemental limitations. Human annotators are highly adaptive at discovering annotations in new texts, but those annotations can be inconsistent and incomplete; dependency parsers lack sophisticated, tacit knowledge, but they can perform exhaustive text search for prospective requirements indicators; and while the lexicon may never completely saturate, the lexicon terms can be reliably used to improve recall. Lexical reuse reduces false negatives by 41%, increasing the average recall to 0.85. Last, crowd workers were able to identify and remove false positives by around 80%, which improves average precision to 0.93.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2394985082",
    "type": "article"
  },
  {
    "title": "An Empirical Study of Meta- and Hyper-Heuristic Search for Multi-Objective Release Planning",
    "doi": "https://doi.org/10.1145/3196831",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Yuanyuan Zhang; Mark Harman; Gabriela Ochoa; Guenther Ruhe; Sjaak Brinkkemper",
    "corresponding_authors": "",
    "abstract": "A variety of meta-heuristic search algorithms have been introduced for optimising software release planning. However, there has been no comprehensive empirical study of different search algorithms across multiple different real-world datasets. In this article, we present an empirical study of global, local, and hybrid meta- and hyper-heuristic search-based algorithms on 10 real-world datasets. We find that the hyper-heuristics are particularly effective. For example, the hyper-heuristic genetic algorithm significantly outperformed the other six approaches (and with high effect size) for solution quality 85% of the time, and was also faster than all others 70% of the time. Furthermore, correlation analysis reveals that it scales well as the number of requirements increases.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2805382256",
    "type": "article"
  },
  {
    "title": "Toward Better Evolutionary Program Repair",
    "doi": "https://doi.org/10.1145/3360004",
    "publication_date": "2020-01-30",
    "publication_year": 2020,
    "authors": "Yuan Yuan; Wolfgang Banzhaf",
    "corresponding_authors": "",
    "abstract": "Bug repair is a major component of software maintenance, which requires a huge amount of manpower. Evolutionary computation, particularly genetic programming (GP), is a class of promising techniques for automating this time-consuming and expensive process. Although recent research in evolutionary program repair has made significant progress, major challenges still remain. In this article, we propose ARJA-e, a new evolutionary repair system for Java code that aims to address challenges for the search space, search algorithm, and patch overfitting. To determine a search space that is more likely to contain correct patches, ARJA-e combines two sources of fix ingredients (i.e., the statement-level redundancy assumption and repair templates) with contextual analysis-based search space reduction, thereby leveraging their complementary strengths. To encode patches in GP more properly, ARJA-e unifies the edits at different granularities into statement-level edits and then uses a lower-granularity patch representation that is characterized by the decoupling of statements for replacement and statements for insertion. ARJA-e also uses a finer-grained fitness function that can make full use of semantic information contained in the test suite, which is expected to better guide the search of GP. To alleviate patch overfitting, ARJA-e further includes a postprocessing tool that can serve the purposes of overfit detection and patch ranking. We evaluate ARJA-e on 224 real Java bugs from Defects4J and compare it with the state-of-the-art repair techniques. The evaluation results show that ARJA-e can correctly fix 39 bugs in terms of the patches ranked first, achieving substantial performance improvements over the state of the art. In addition, we analyze the effect of the components of ARJA-e qualitatively and quantitatively to demonstrate their effectiveness and advantages.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W3012626099",
    "type": "article"
  },
  {
    "title": "Emoji-powered Sentiment and Emotion Detection from Software Developers’ Communication Data",
    "doi": "https://doi.org/10.1145/3424308",
    "publication_date": "2021-01-27",
    "publication_year": 2021,
    "authors": "Zhenpeng Chen; Yanbin Cao; Huihan Yao; Xuan Lü; Xin Peng; Hong Mei; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "Sentiment and emotion detection from textual communication records of developers have various application scenarios in software engineering (SE). However, commonly used off-the-shelf sentiment/emotion detection tools cannot obtain reliable results in SE tasks and misunderstanding of technical knowledge is demonstrated to be the main reason. Then researchers start to create labeled SE-related datasets manually and customize SE-specific methods. However, the scarce labeled data can cover only very limited lexicon and expressions. In this article, we employ emojis as an instrument to address this problem. Different from manual labels that are provided by annotators, emojis are self-reported labels provided by the authors themselves to intentionally convey affective states and thus are suitable indications of sentiment and emotion in texts. Since emojis have been widely adopted in online communication, a large amount of emoji-labeled texts can be easily accessed to help tackle the scarcity of the manually labeled data. Specifically, we leverage Tweets and GitHub posts containing emojis to learn representations of SE-related texts through emoji prediction. By predicting emojis containing in each text, texts that tend to surround the same emoji are represented with similar vectors, which transfers the sentiment knowledge contained in emoji usage to the representations of texts. Then we leverage the sentiment-aware representations as well as manually labeled data to learn the final sentiment/emotion classifier via transfer learning. Compared to existing approaches, our approach can achieve significant improvement on representative benchmark datasets, with an average increase of 0.036 and 0.049 in macro-F1 in sentiment and emotion detection, respectively. Further investigations reveal that the large-scale Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource but try to transform knowledge from the open domain through ubiquitous signals such as emojis. Finally, we present the open challenges of sentiment and emotion detection in SE through a qualitative analysis of texts misclassified by our approach.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3122215046",
    "type": "article"
  },
  {
    "title": "Interpreting Deep Learning-based Vulnerability Detector Predictions Based on Heuristic Searching",
    "doi": "https://doi.org/10.1145/3429444",
    "publication_date": "2021-03-10",
    "publication_year": 2021,
    "authors": "Deqing Zou; Yawei Zhu; Shouhuai Xu; Zhen Li; Hai Jin; Hengkai Ye",
    "corresponding_authors": "",
    "abstract": "Detecting software vulnerabilities is an important problem and a recent development in tackling the problem is the use of deep learning models to detect software vulnerabilities. While effective, it is hard to explain why a deep learning model predicts a piece of code as vulnerable or not because of the black-box nature of deep learning models. Indeed, the interpretability of deep learning models is a daunting open problem. In this article, we make a significant step toward tackling the interpretability of deep learning model in vulnerability detection. Specifically, we introduce a high-fidelity explanation framework, which aims to identify a small number of tokens that make significant contributions to a detector’s prediction with respect to an example. Systematic experiments show that the framework indeed has a higher fidelity than existing methods, especially when features are not independent of each other (which often occurs in the real world). In particular, the framework can produce some vulnerability rules that can be understood by domain experts for accepting a detector’s outputs (i.e., true positives) or rejecting a detector’s outputs (i.e., false-positives and false-negatives). We also discuss limitations of the present study, which indicate interesting open problems for future research.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3134763859",
    "type": "article"
  },
  {
    "title": "Context-aware Retrieval-based Deep Commit Message Generation",
    "doi": "https://doi.org/10.1145/3464689",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Haoye Wang; Xin Xia; David Lo; Qiang He; Xinyu Wang; John Grundy",
    "corresponding_authors": "",
    "abstract": "Commit messages recorded in version control systems contain valuable information for software development, maintenance, and comprehension. Unfortunately, developers often commit code with empty or poor quality commit messages. To address this issue, several studies have proposed approaches to generate commit messages from commit diffs . Recent studies make use of neural machine translation algorithms to try and translate git diffs into commit messages and have achieved some promising results. However, these learning-based methods tend to generate high-frequency words but ignore low-frequency ones. In addition, they suffer from exposure bias issues, which leads to a gap between training phase and testing phase. In this article, we propose CoRec to address the above two limitations. Specifically, we first train a context-aware encoder-decoder model that randomly selects the previous output of the decoder or the embedding vector of a ground truth word as context to make the model gradually aware of previous alignment choices. Given a diff for testing, the trained model is reused to retrieve the most similar diff from the training set. Finally, we use the retrieval diff to guide the probability distribution for the final generated vocabulary. Our method combines the advantages of both information retrieval and neural machine translation. We evaluate CoRec on a dataset from Liu et al. and a large-scale dataset crawled from 10K popular Java repositories in Github. Our experimental results show that CoRec significantly outperforms the state-of-the-art method NNGen by 19% on average in terms of BLEU.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3185176031",
    "type": "article"
  },
  {
    "title": "Modular Tree Network for Source Code Representation Learning",
    "doi": "https://doi.org/10.1145/3409331",
    "publication_date": "2020-09-26",
    "publication_year": 2020,
    "authors": "Wenhan Wang; Ge Li; Sijie Shen; Xin Xia; Zhi Jin",
    "corresponding_authors": "",
    "abstract": "Learning representation for source code is a foundation of many program analysis tasks. In recent years, neural networks have already shown success in this area, but most existing models did not make full use of the unique structural information of programs. Although abstract syntax tree (AST)-based neural models can handle the tree structure in the source code, they cannot capture the richness of different types of substructure in programs. In this article, we propose a modular tree network that dynamically composes different neural network units into tree structures based on the input AST. Different from previous tree-structural neural network models, a modular tree network can capture the semantic differences between types of AST substructures. We evaluate our model on two tasks: program classification and code clone detection. Our model achieves the best performance compared with state-of-the-art approaches in both tasks, showing the advantage of leveraging more elaborate structure information of the source code.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W3091995628",
    "type": "article"
  },
  {
    "title": "How Should I Improve the UI of My App?",
    "doi": "https://doi.org/10.1145/3447808",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Qiuyuan Chen; Chunyang Chen; Safwat Hassan; Zhengchang Xing; Xin Xia; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "UI (User Interface) is an essential factor influencing users’ perception of an app. However, it is hard for even professional designers to determine if the UI is good or not for end-users. Users’ feedback (e.g., user reviews in the Google Play) provides a way for app owners to understand how the users perceive the UI. In this article, we conduct an in-depth empirical study to analyze the UI issues of mobile apps. In particular, we analyze more than 3M UI-related reviews from 22,199 top free-to-download apps and 9,380 top non-free apps in the Google Play Store. By comparing the rating of UI-related reviews and other reviews of an app, we observe that UI-related reviews have lower ratings than other reviews. By manually analyzing a random sample of 1,447 UI-related reviews with a 95% confidence level and a 5% interval, we identify 17 UI-related issues types that belong to four categories (i.e., “Appearance,” “Interaction,” “Experience,” and “Others” ). In these issue types, we find “Generic Review” is the most occurring one. “Comparative Review” and “Advertisement” are the most negative two UI issue types. Faced with these UI issues, we explore the patterns of interaction between app owners and users. We identify eight patterns of how app owners dialogue with users about UI issues by the review-response mechanism. We find “Apology or Appreciation” and “Information Request” are the most two frequent patterns. We find updating UI timely according to feedback is essential to satisfy users. Besides, app owners could also fix UI issues without updating UI, especially for issue types belonging to “Interaction” category. Our findings show that there exists a positive impact if app owners could actively interact with users to improve UI quality and boost users’ satisfactoriness about the UIs.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W3162893202",
    "type": "article"
  },
  {
    "title": "Beyond Tests",
    "doi": "https://doi.org/10.1145/3418461",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "Xiang Gao; Bo Wang; Gregory J. Duck; Ruyi Ji; Yingfei Xiong; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Automated program repair is an emerging technology that seeks to automatically rectify program errors and vulnerabilities. Repair techniques are driven by a correctness criterion that is often in the form of a test suite. Such test-based repair may produce overfitting patches, where the patches produced fail on tests outside the test suite driving the repair. In this work, we present a repair method that fixes program vulnerabilities without the need for a voluminous test suite. Given a vulnerability as evidenced by an exploit, the technique extracts a constraint representing the vulnerability with the help of sanitizers. The extracted constraint serves as a proof obligation that our synthesized patch should satisfy. The proof obligation is met by propagating the extracted constraint to locations that are deemed to be “suitable” fix locations. An implementation of our approach (E xtract F ix ) on top of the KLEE symbolic execution engine shows its efficacy in fixing a wide range of vulnerabilities taken from the ManyBugs benchmark, real-world CVEs and Google’s OSS-Fuzz framework. We believe that our work presents a way forward for the overfitting problem in program repair by generalizing observable hazards/vulnerabilities (as constraint) from a single failing test or exploit.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W3129269689",
    "type": "article"
  },
  {
    "title": "The Agile Success Model",
    "doi": "https://doi.org/10.1145/3464938",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Daniel Russo",
    "corresponding_authors": "Daniel Russo",
    "abstract": "Organizations are increasingly adopting Agile frameworks for their internal software development. Cost reduction, rapid deployment, requirements and mental model alignment are typical reasons for an Agile transformation. This article presents an in-depth field study of a large-scale Agile transformation in a mission-critical environment, where stakeholders' commitment was a critical success factor. The goal of such a transformation was to implement mission-oriented features, reducing costs and time to operate in critical scenarios. The project lasted several years and involved over 40 professionals. We report how a hierarchical and plan-driven organization exploited Agile methods to develop a Command & Control (C2) system. Accordingly, we first abstract our experience, inducing a success model of general use for other comparable organizations by performing a post-mortem study. The goal of the inductive research process was to identify critical success factors and their relations. Finally, we validated and generalized our model through Partial Least Squares - Structural Equation Modelling, surveying 200 software engineers involved in similar projects. We conclude the article with data-driven recommendations concerning the management of Agile projects.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W3184225832",
    "type": "article"
  },
  {
    "title": "Why My Code Summarization Model Does Not Work",
    "doi": "https://doi.org/10.1145/3434280",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "Qiuyuan Chen; Xin Xia; Han Hu; David Lo; Shanping Li",
    "corresponding_authors": "",
    "abstract": "Code summarization aims at generating a code comment given a block of source code and it is normally performed by training machine learning algorithms on existing code block-comment pairs. Code comments in practice have different intentions. For example, some code comments might explain how the methods work, while others explain why some methods are written. Previous works have shown that a relationship exists between a code block and the category of a comment associated with it. In this article, we aim to investigate to which extent we can exploit this relationship to improve code summarization performance. We first classify comments into six intention categories and manually label 20,000 code-comment pairs. These categories include “what,” “why,” “how-to-use,” “how-it-is-done,” “property,” and “others.” Based on this dataset, we conduct an experiment to investigate the performance of different state-of-the-art code summarization approaches on the categories. We find that the performance of different code summarization approaches varies substantially across the categories. Moreover, the category for which a code summarization model performs the best is different for the different models. In particular, no models perform the best for “why” and “property” comments among the six categories. We design a composite approach to demonstrate that comment category prediction can boost code summarization to reach better results. The approach leverages classified code-category labeled data to train a classifier to infer categories. Then it selects the most suitable models for inferred categories and outputs the composite results. Our composite approach outperforms other approaches that do not consider comment categories and obtains a relative improvement of 8.57% and 16.34% in terms of ROUGE-L and BLEU-4 score, respectively.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W3131328028",
    "type": "article"
  },
  {
    "title": "An Empirical Study of the Impact of Data Splitting Decisions on the Performance of AIOps Solutions",
    "doi": "https://doi.org/10.1145/3447876",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Yingzhe Lyu; Heng Li; Mohammed Sayagh; Zhen Ming Jiang; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "AIOps (Artificial Intelligence for IT Operations) leverages machine learning models to help practitioners handle the massive data produced during the operations of large-scale systems. However, due to the nature of the operation data, AIOps modeling faces several data splitting-related challenges, such as imbalanced data, data leakage, and concept drift. In this work, we study the data leakage and concept drift challenges in the context of AIOps and evaluate the impact of different modeling decisions on such challenges. Specifically, we perform a case study on two commonly studied AIOps applications: (1) predicting job failures based on trace data from a large-scale cluster environment and (2) predicting disk failures based on disk monitoring data from a large-scale cloud storage environment. First, we observe that the data leakage issue exists in AIOps solutions. Using a time-based splitting of training and validation datasets can significantly reduce such data leakage, making it more appropriate than using a random splitting in the AIOps context. Second, we show that AIOps solutions suffer from concept drift. Periodically updating AIOps models can help mitigate the impact of such concept drift, while the performance benefit and the modeling cost of increasing the update frequency depend largely on the application data and the used models. Our findings encourage future studies and practices on developing AIOps solutions to pay attention to their data-splitting decisions to handle the data leakage and concept drift challenges.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3184231327",
    "type": "article"
  },
  {
    "title": "Memory-Safety Challenge Considered Solved? An In-Depth Study with All Rust CVEs",
    "doi": "https://doi.org/10.1145/3466642",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Hui Xu; Zhuangbin Chen; Mingshen Sun; Yangfan Zhou; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Rust is an emerging programming language that aims at preventing memory-safety bugs without sacrificing much efficiency. The claimed property is very attractive to developers, and many projects start using the language. However, can Rust achieve the memory-safety promise? This article studies the question by surveying 186 real-world bug reports collected from several origins, which contain all existing Rust common vulnerability and exposures (CVEs) of memory-safety issues by 2020-12-31. We manually analyze each bug and extract their culprit patterns. Our analysis result shows that Rust can keep its promise that all memory-safety bugs require unsafe code, and many memory-safety bugs in our dataset are mild soundness issues that only leave a possibility to write memory-safety bugs without unsafe code. Furthermore, we summarize three typical categories of memory-safety bugs, including automatic memory reclaim, unsound function, and unsound generic or trait. While automatic memory claim bugs are related to the side effect of Rust newly-adopted ownership-based resource management scheme, unsound function reveals the essential challenge of Rust development for avoiding unsound code, and unsound generic or trait intensifies the risk of introducing unsoundness. Based on these findings, we propose two promising directions toward improving the security of Rust development, including several best practices of using specific APIs and methods to detect particular bugs involving unsafe code. Our work intends to raise more discussions regarding the memory-safety issues of Rust and facilitate the maturity of the language.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3202751128",
    "type": "article"
  },
  {
    "title": "Verification Witnesses",
    "doi": "https://doi.org/10.1145/3477579",
    "publication_date": "2022-05-27",
    "publication_year": 2022,
    "authors": "Dirk Beyer; Matthias Dangl; Daniel Dietsch; Matthias Heizmann; Thomas R. Lemberger; Michael Tautschnig",
    "corresponding_authors": "",
    "abstract": "Over the last years, witness-based validation of verification results has become an established practice in software verification: An independent validator re-establishes verification results of a software verifier using verification witnesses, which are stored in a standardized exchange format. In addition to validation, such exchangable information about proofs and alarms found by a verifier can be shared across verification tools, and users can apply independent third-party tools to visualize and explore witnesses to help them comprehend the causes of bugs or the reasons why a given program is correct. To achieve the goal of making verification results more accessible to engineers, it is necessary to consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the important principle of separation of concerns. We present the conceptual principles of verification witnesses, give a description of how to use them, provide a technical specification of the exchange format for witnesses, and perform an extensive experimental study on the application of witness-based result validation, using the validators CPAchecker , UAutomizer , CPA-witness2test , and FShell-witness2test .",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W4281723389",
    "type": "article"
  },
  {
    "title": "Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms",
    "doi": "https://doi.org/10.1145/3564821",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Andrea Di Sorbo; Fiorella Zampetti; Corrado Aaron Visaggio; Massimiliano Di Penta; Sebastiano Panichella",
    "corresponding_authors": "",
    "abstract": "Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W4297294773",
    "type": "article"
  },
  {
    "title": "ActivFORMS: A Formally Founded Model-based Approach to Engineer Self-adaptive Systems",
    "doi": "https://doi.org/10.1145/3522585",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Danny Weyns; Usman M. Iftikhar",
    "corresponding_authors": "",
    "abstract": "Self-adaptation equips a computing system with a feedback loop that enables it to deal with change caused by uncertainties during operation, such as changing availability of resources and fluctuating workloads. To ensure that the system complies with the adaptation goals, recent research suggests the use of formal techniques at runtime. Yet, existing approaches have three limitations that affect their practical applicability: (i) they ignore correctness of the behavior of the feedback loop, (ii) they rely on exhaustive verification at runtime to select adaptation options to realize the adaptation goals, which is time- and resource-demanding, and (iii) they provide limited or no support for changing adaptation goals at runtime. To tackle these shortcomings, we present ActivFORMS (Active FORmal Models for Self-adaptation). ActivFORMS contributes an end-to-end approach for engineering self-adaptive systems, spanning four main stages of the life cycle of a feedback loop: design, deployment, runtime adaptation, and evolution. We also present ActivFORMS-ta, a tool-supported instance of ActivFORMS that leverages timed automata models and statistical model checking at runtime. We validate the research results using an IoT application for building security monitoring that is deployed in Leuven. The experimental results demonstrate that ActivFORMS supports correctness of the behavior of the feedback loop, achieves the adaptation goals in an efficient way, and supports changing adaptation goals at runtime.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4225775897",
    "type": "article"
  },
  {
    "title": "On the Significance of Category Prediction for Code-Comment Synchronization",
    "doi": "https://doi.org/10.1145/3534117",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Zhen Yang; Jacky Keung; Xiao Yu; Yan Xiao; Zhi Jin; Jingyu Zhang",
    "corresponding_authors": "",
    "abstract": "Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing ) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED) , Relative Edit Distance (RED) , BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4281384435",
    "type": "article"
  },
  {
    "title": "Detecting and Augmenting Missing Key Aspects in Vulnerability Descriptions",
    "doi": "https://doi.org/10.1145/3498537",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Hao Guo; Sen Chen; Zhenchang Xing; Xiaohong Li; Yude Bai; Jiamou Sun",
    "corresponding_authors": "",
    "abstract": "Security vulnerabilities have been continually disclosed and documented. For the effective understanding, management, and mitigation of the fast-growing number of vulnerabilities, an important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type, and attack vector. In this article, we first investigate 133,639 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) database over the past 20 years. We find that 56%, 85%, 38%, and 28% of CVEs miss vulnerability type, root cause, attack vector, and attacker type, respectively. By comparing the differences of the latest updated CVE reports across different databases, we observe that 1,476 missing key aspects in 1,320 CVE descriptions were augmented manually in the National Vulnerability Database (NVD) , which indicates that the vulnerability database maintainers try to complete the vulnerability descriptions in practice to mitigate such a problem. To help complete the missing information of key vulnerability aspects and reduce human efforts, we propose a neural-network-based approach called PMA to predict the missing key aspects of a vulnerability based on its known aspects. We systematically explore the design space of the neural network models and empirically identify the most effective model design in the scenario. Our ablation study reveals the prominent correlations among vulnerability aspects when predicting. Trained with historical CVEs, our model achieves 88%, 71%, 61%, and 81% in F1 for predicting the missing vulnerability type, root cause, attacker type, and attack vector of 8,623 “future” CVEs across 3 years, respectively. Furthermore, we validate the predicting performance of key aspect augmentation of CVEs based on the manually augmented CVE data collected from NVD, which confirms the practicality of our approach. We finally highlight that PMA has the ability to reduce human efforts by recommending and augmenting missing key aspects for vulnerability databases, and to facilitate other research works such as severity level prediction of CVEs based on the vulnerability descriptions.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4210556785",
    "type": "article"
  },
  {
    "title": "Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features",
    "doi": "https://doi.org/10.1145/3571847",
    "publication_date": "2022-11-24",
    "publication_year": 2022,
    "authors": "Zibin Zheng; Weili Chen; Zhijie Zhong; Zhiguang Chen; Yutong Lu",
    "corresponding_authors": "",
    "abstract": "Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software system–decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Mul ti-view Cas cade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4309857254",
    "type": "article"
  },
  {
    "title": "SemMT: A Semantic-Based Testing Approach for Machine Translation Systems",
    "doi": "https://doi.org/10.1145/3490488",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Jialun Cao; Meiziniu Li; Yeting Li; Ming Wen; Shing-Chi Cheung; Haiming Chen",
    "corresponding_authors": "",
    "abstract": "Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2% and 15.4% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3109744225",
    "type": "article"
  },
  {
    "title": "SafeDrop: Detecting Memory Deallocation Bugs of Rust Programs via Static Data-flow Analysis",
    "doi": "https://doi.org/10.1145/3542948",
    "publication_date": "2022-06-21",
    "publication_year": 2022,
    "authors": "Mohan Cui; Chengjun Chen; Hui Xu; Yangfan Zhou",
    "corresponding_authors": "",
    "abstract": "Rust is an emerging programming language that aims to prevent memory-safety bugs. However, the current design of Rust also brings side effects, which may increase the risk of memory-safety issues. In particular, it employs ownership-based resource management and enforces automatic deallocation of unused resources without using the garbage collector. It may therefore falsely deallocate reclaimed memory and lead to use-after-free or double-free issues. In this article, we study the problem of invalid memory deallocation and propose SafeDrop , a static path-sensitive data-flow analysis approach to detect such bugs. Our approach analyzes each function of a Rust crate iteratively in a flow-sensitive and field-sensitive way. It leverages a modified Tarjan algorithm to achieve scalable path-sensitive analysis and a cache-based strategy for efficient inter-procedural analysis. We have implemented our approach and integrated it into the Rust compiler. Experiment results show that the approach can successfully detect all such bugs in our experiments with a limited number of false positives and incurs a very small overhead compared to the original compilation time.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3151648723",
    "type": "article"
  },
  {
    "title": "Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform",
    "doi": "https://doi.org/10.1145/3549542",
    "publication_date": "2022-07-19",
    "publication_year": 2022,
    "authors": "Michael Pacheco; Gustavo A. Oliva; Gopi Krishnan Rajbahadur; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as ÐApps. When engineering ÐApps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective ÐApps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., ÐApp developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that Etherscan’s Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStation’s Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, ÐApp developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4285794979",
    "type": "article"
  },
  {
    "title": "Verifix: Verified Repair of Programming Assignments",
    "doi": "https://doi.org/10.1145/3510418",
    "publication_date": "2022-03-28",
    "publication_year": 2022,
    "authors": "Umair Z. Ahmed; Zhiyu Fan; Jooyong Yi; Omar I. Al-Bataineh; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Automated feedback generation for introductory programming assignments is useful for programming education. Most works try to generate feedback to correct a student program by comparing its behavior with an instructor’s reference program on selected tests. In this work, our aim is to generate verifiably correct program repairs as student feedback. A student-submitted program is aligned and composed with a reference solution in terms of control flow, and the variables of the two programs are automatically aligned via predicates describing the relationship between the variables. When verification attempt for the obtained aligned program fails, we turn a verification problem into a MaxSMT problem whose solution leads to a minimal repair. We have conducted experiments on student assignments curated from a widely deployed intelligent tutoring system. Our results show that generating verified repair without sacrificing the overall repair rate is possible. In fact, our implementation, Verifix, is shown to outperform Clara, a state-of-the-art tool, in terms of repair rate. This shows the promise of using verified repair to generate high confidence feedback in programming pedagogy settings.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4220847985",
    "type": "article"
  },
  {
    "title": "Modern Code Reviews—Survey of Literature and Practice",
    "doi": "https://doi.org/10.1145/3585004",
    "publication_date": "2023-02-24",
    "publication_year": 2023,
    "authors": "Deepika Badampudi; Michael Unterkalmsteiner; Ricardo Britto",
    "corresponding_authors": "",
    "abstract": "Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is unknown whether the research community has targeted themes that practitioners consider important. Objectives: The objectives are to provide an overview of MCR research, analyze the practitioners' opinions on the importance of MCR research, investigate the alignment between research and practice, and propose future MCR research avenues. Method: We conducted a systematic mapping study to survey state of the art until and including 2021, employed the Q-Methodology to analyze the practitioners' perception of the relevance of MCR research, and analyzed the primary studies' research impact. Results: We analyzed 244 primary studies, resulting in five themes. As a result of the 1,300 survey data points, we found that the respondents are positive about research investigating the impact of MCR on product quality and MCR process properties. In contrast, they are negative about human factor- and support systems-related research. Conclusion: These results indicate a misalignment between the state of the art and the themes deemed important by most survey respondents. Researchers should focus on solutions that can improve the state of MCR practice. We provide an MCR research agenda that can potentially increase the impact of MCR research.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4321785158",
    "type": "article"
  },
  {
    "title": "Fair Enough: Searching for Sufficient Measures of Fairness",
    "doi": "https://doi.org/10.1145/3585006",
    "publication_date": "2023-03-16",
    "publication_year": 2023,
    "authors": "Suvodeep Majumder; Joymallya Chakraborty; Gina R. Bai; Kathryn T. Stolee; Tim Menzies",
    "corresponding_authors": "",
    "abstract": "Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a) 26 classification metrics can be clustered into seven groups and (b) four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1) determine what type of fairness is desirable (and we offer a handful of such types), then (2) lookup those types in our clusters, and then (3) just test for one item per cluster. For the purpose of reproducibility, our scripts and data are available at https://github.com/Repoanon ymous/Fairness_Metrics.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3208528803",
    "type": "article"
  },
  {
    "title": "<scp>CodeEditor</scp> : Learning to Edit Source Code with Pre-trained Models",
    "doi": "https://doi.org/10.1145/3597207",
    "publication_date": "2023-05-22",
    "publication_year": 2023,
    "authors": "Jia Li; Ge Li; Zhuo Li; Zhi Jin; Xing Hu; Kechi Zhang; Zhiyi Fu",
    "corresponding_authors": "",
    "abstract": "Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing. In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named CodeEditor . Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained CodeEditor in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained CodeEditor with four datasets and evaluate it on the test data. CodeEditor outperforms the SOTA baselines by 15%, 25.5%, 9.4%, and 26.6% on four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor with limited data and evaluate it on the test data. CodeEditor substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CodeEditor on the test data without training. CodeEditor correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained CodeEditor is more effective in automatic code editing.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4377238789",
    "type": "article"
  },
  {
    "title": "Representation Learning for Stack Overflow Posts: How Far Are We?",
    "doi": "https://doi.org/10.1145/3635711",
    "publication_date": "2023-12-07",
    "publication_year": 2023,
    "authors": "Junda He; Xin Zhou; Bowen Xu; Ting Zhang; Kisub Kim; Zhou Yang; Ferdian Thung; Ivana Clairine Irsan; David Lo",
    "corresponding_authors": "",
    "abstract": "The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers’ interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks (i.e., tag recommendation, relatedness prediction, and API recommendation). The results show that Post2Vec cannot further improve the SOTA techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, and GPT2) and (2) language models built with software engineering related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the “No Silver Bullet” concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the SOTA performance significantly for all the downstream tasks.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4389438812",
    "type": "article"
  },
  {
    "title": "Acrobats and Safety Nets: Problematizing Large-Scale Agile Software Development",
    "doi": "https://doi.org/10.1145/3617169",
    "publication_date": "2023-08-23",
    "publication_year": 2023,
    "authors": "Knut H. Rolland; Brian Fitzgerald; Torgeir Dingsøyr; Klaas-Jan Stol",
    "corresponding_authors": "",
    "abstract": "Agile development methods have become a standard in the software industry, including in large-scale projects. These methods share a set of underlying assumptions that distinguish them from more traditional plan-driven approaches. In this article, we adopt Alvesson and Sandberg's problematization approach to challenge three key assumptions that are prevalent in the large-scale agile literature: (1) agile and plan-driven methods are mutually exclusive; (2) self-managing and hierarchically organized teams are mutually exclusive; and (3) agile methods can scale through simple linear composition. Using a longitudinal case study of large-scale agile development, we describe a series of trigger events and episodes whereby the agile approach was tailored to address the needs of the large-scale development context, which was very much at odds with these fundamental assumptions. We develop a set of new underlying assumptions which suggest that agile and plan-driven practices are mutually enabling and necessary for coordination and scaling in large-scale agile projects. We develop nine propositions for large-scale agile projects based on these new alternative underlying assumptions. Finally, we summarize our theoretical contribution in a generic process model of continuously adjusting agile and plan-driven practices in order to accommodate process challenges in large-scale agile projects.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4386099829",
    "type": "article"
  },
  {
    "title": "Compiler Autotuning through Multiple-phase Learning",
    "doi": "https://doi.org/10.1145/3640330",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Mingxuan Zhu; Dan Hao; Junjie Chen",
    "corresponding_authors": "",
    "abstract": "Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4390740757",
    "type": "article"
  },
  {
    "title": "Method-level Bug Prediction: Problems and Promises",
    "doi": "https://doi.org/10.1145/3640331",
    "publication_date": "2024-01-13",
    "publication_year": 2024,
    "authors": "Shaiful Chowdhury; Gias Uddin; Hadi Hemmati; Reid Holmes",
    "corresponding_authors": "",
    "abstract": "Fixing software bugs can be colossally expensive, especially if they are discovered in the later phases of the software development life cycle. As such, bug prediction has been a classic problem for the research community. As of now, the Google Scholar site generates ∼113,000 hits if searched with the “bug prediction” phrase. Despite this staggering effort by the research community, bug prediction research is criticized for not being decisively adopted in practice. A significant problem of the existing research is the granularity level (i.e., class/file level) at which bug prediction is historically studied. Practitioners find it difficult and time-consuming to locate bugs at the class/file level granularity. Consequently, method-level bug prediction has become popular in the past decade. We ask, are these method-level bug prediction models ready for industry use? Unfortunately, the answer is no . The reported high accuracies of these models dwindle significantly if we evaluate them in different realistic time-sensitive contexts. It may seem hopeless at first, but, encouragingly, we show that future method-level bug prediction can be improved significantly. In general, we show how to reliably evaluate future method-level bug prediction models and how to improve them by focusing on four different improvement avenues: building noise-free bug data, addressing concept drift, selecting similar training projects, and developing a mixture of models. Our findings are based on three publicly available method-level bug datasets and a newly built bug dataset of 774,051 Java methods originating from 49 open-source software projects.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4390838289",
    "type": "article"
  },
  {
    "title": "Advanced White-Box Heuristics for Search-Based Fuzzing of REST APIs",
    "doi": "https://doi.org/10.1145/3652157",
    "publication_date": "2024-03-11",
    "publication_year": 2024,
    "authors": "Andrea Arcuri; Man Zhang; Juan Pablo Galeotti",
    "corresponding_authors": "",
    "abstract": "Due to its importance and widespread use in industry, automated testing of REST APIs has attracted major interest from the research community in the last few years. However, most of the work in the literature has been focused on black-box fuzzing. Although existing fuzzers have been used to automatically find many faults in existing APIs, there are still several open research challenges that hinder the achievement of better results (e.g., in terms of code coverage and fault finding). For example, under-specified schemas are a major issue for black-box fuzzers. Currently, EvoMaster is the only existing tool that supports white-box fuzzing of REST APIs. In this paper, we provide a series of novel white-box heuristics, including for example how to deal with under-specified constrains in API schemas, as well as under-specified schemas in SQL databases. Our novel techniques are implemented as an extension to our open-source, search-based fuzzer EvoMaster . An empirical study on 14 APIs from the EMB corpus, plus one industrial API, shows clear improvements of the results in some of these APIs.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4392652182",
    "type": "article"
  },
  {
    "title": "CodeScore: Evaluating Code Generation by Learning Code Execution",
    "doi": "https://doi.org/10.1145/3695991",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Yihong Dong; Jiazheng Ding; Xue Jiang; Ge Li; Zhuo Li; Zhi Jin",
    "corresponding_authors": "",
    "abstract": "A proper code evaluation metric (CEM) profoundly impacts the evolution of code generation, which is an important research field in NLP and software engineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU) suffer from two significant drawbacks. 1. They primarily measure the surface differences between codes without considering their functional equivalence. However, functional equivalence is pivotal in evaluating the effectiveness of code generation, as different codes can perform identical operations. 2. They are predominantly designed for the Ref-only input format. However, code evaluation necessitates versatility in input formats. Aside from Ref-only, there are NL-only and Ref&amp;NL formats, which existing match-based CEMs cannot effectively accommodate. In this paper, we propose CodeScore, a large language model (LLM)-based CEM, which estimates the functional correctness of generated code on three input types. To acquire CodeScore, we present UniCE, a unified code generation learning framework, for LLMs to learn code execution (i.e., learning PassRatio and Executability of generated code) with unified input. Extensive experimental results on multiple code evaluation datasets demonstrate that CodeScore absolutely improves up to 58.87% correlation with functional correctness compared to other CEMs, achieves state-of-the-art performance, and effectively handles three input formats.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4402516191",
    "type": "article"
  },
  {
    "title": "Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges facing DNN-based Software Vulnerability Detection",
    "doi": "https://doi.org/10.1145/3640333",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Changjie Shao; Gaolei Li; Jun Wu; Xi Zheng",
    "corresponding_authors": "",
    "abstract": "To detect software vulnerabilities with better performance, deep neural networks (DNNs) have received extensive attention recently. However, these vulnerability detection DNN models trained with code representations are vulnerable to specific perturbations on code representations. This motivates us to rethink the bane of software vulnerability detection and find function-agnostic features during code representation which we name as semantic redundant features. This paper first identifies a tight correlation between function-agnostic triggers and semantic redundant feature space (where the redundant features reside) in these DNN models. For correlation identification, we propose a novel Backdoor-based Semantic Redundancy Exploration (BSemRE) framework. In BSemRE, the sensitivity of the trained models to function-agnostic triggers is observed to verify the existence of semantic redundancy in various code representations. Specifically, acting as the typical manifestations of semantic redundancy, naming conventions, ternary operators and identically-true conditions are exploited to generate function-agnostic triggers. Extensive comparative experiments on 1,613,823 samples of eight representative vulnerability datasets and state-of-the-art code representation techniques and vulnerability detection models demonstrate that the existence of semantic redundancy determines the upper trustworthiness limit of DNN-based software vulnerability detection. To the best of our knowledge, this is the first work exploring the bane of software vulnerability detection using backdoor triggers.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4391174792",
    "type": "article"
  },
  {
    "title": "Exploring the Capabilities of LLMs for Code Change Related Tasks",
    "doi": "https://doi.org/10.1145/3709358",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Linlin Fan; Jiakun Liu; Zhongxin Liu; David Lo; Xin Xia; Shanping Li",
    "corresponding_authors": "",
    "abstract": "Developers deal with code-change-related tasks daily, e.g., reviewing code. Pre-trained code and code-change-oriented models have been adapted to help developers with such tasks. Recently, large language models (LLMs) have shown their effectiveness in code-related tasks. However, existing LLMs for code focus on general code syntax and semantics rather than the differences between two code versions. Thus, it is an open question how LLMs perform on code-change-related tasks. To answer this question, we conduct an empirical study using &gt;1B parameters LLMs on three code-change-related tasks, i.e., code review generation, commit message generation, and just-in-time comment update, with in-context learning (ICL) and parameter-efficient fine-tuning (PEFT, including LoRA and prefix-tuning). We observe that the performance of LLMs is poor without examples and generally improves with examples, but more examples do not always lead to better performance. LLMs tuned with LoRA have comparable performance to the state-of-the-art small pre-trained models. Larger models are not always better, but Llama 2 and Code Llama families are always the best. The best LLMs outperform small pre-trained models on the code changes that only modify comments and perform comparably on other code changes. We suggest future work should focus more on guiding LLMs to learn the knowledge specific to the changes related to code rather than comments for code-change-related tasks.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4405703871",
    "type": "article"
  },
  {
    "title": "Automation in Model-Driven Engineering: A look back, and ahead",
    "doi": "https://doi.org/10.1145/3712008",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Loli Burgueño; Davide Di Ruscio; Houari Sahraoui; Manuel Wimmer",
    "corresponding_authors": "",
    "abstract": "Model-Driven Engineering (MDE) provides a huge body of knowledge of automation for many different engineering tasks, especially those involving transitioning from design to implementation. With the huge progress made in Artificial Intelligence (AI), questions arise about the future of MDE, such as how existing MDE techniques and technologies can be improved or how other activities that currently lack dedicated support can also be automated. However, at the same time, it has to be revisited where and how models should be used to keep the engineers in the loop for creating, operating, and maintaining complex systems. To trigger dedicated research on these open points, we discuss the history of automation in MDE and present perspectives on how automation in MDE can be further improved and which obstacles have to be overcome in both the medium and long-term.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406494387",
    "type": "article"
  },
  {
    "title": "DDASR: Deep Diverse API Sequence Recommendation",
    "doi": "https://doi.org/10.1145/3712188",
    "publication_date": "2025-01-21",
    "publication_year": 2025,
    "authors": "Siyu Nan; Jian Wang; Neng Zhang; Duantengchuan Li; Bing Li",
    "corresponding_authors": "",
    "abstract": "Recommending API sequences is crucial in software development, saving developers time and effort. While previous studies primarily focus on accuracy, often recommending popular APIs, they tend to overlook less frequent, or ‘tail,’ APIs. This oversight, often a result of limited historical data, consequently diminishes the diversity of recommender systems. In this paper, we propose DDASR, a framework for recommending API sequences containing both popular and tail APIs. To accurately capture developer intent, we utilize recent Large Language Models for learning query representations. To gain a better understanding of tail APIs, DDASR clusters tail APIs with similar functionality and replaces them with cluster centers to produce a pseudo ground truth. Moreover, a loss function is defined based on learning-to-rank to achieve an equilibrium in accuracy and diversity due to the inherent trade-off between them. To evaluate DDASR, we conduct extensive experiments on Java and Python open-source datasets. Results demonstrate that DDASR significantly achieves the best diversity without sacrificing accuracy. Compared to seven state-of-the-art approaches, DDASR improves accuracy metrics BLEU, ROUGE, MAP, and NDCG and diversity metric coverage by 108.28%, 67.30%, 88.59%, and 45.83%, respectively on the Java dataset, as well as 9.83%, 2.45%, 8.06%, and 8.03%, respectively on the Python dataset.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406674608",
    "type": "article"
  },
  {
    "title": "Faster and Better Quantum Software Testing through Specification Reduction and Projective Measurements",
    "doi": "https://doi.org/10.1145/3714468",
    "publication_date": "2025-01-22",
    "publication_year": 2025,
    "authors": "Noah H. Oldfield; Christoph Laaber; Tao Yue; Shaukat Ali",
    "corresponding_authors": "",
    "abstract": "Quantum computing (QC) promises polynomial and exponential speedups in many domains, such as unstructured search and prime number factoring. However, quantum programs yield probabilistic outputs from exponentially growing distributions and are vulnerable to quantum-specific faults. Existing quantum software testing (QST) approaches treat quantum superpositions as classical distributions. This leads to two major limitations when applied to quantum programs: (1) an exponentially growing sample space distribution and (2) failing to detect quantum-specific faults such as phase flips. To overcome these limitations, we introduce a QST approach, which applies a reduction algorithm to a quantum program specification. The reduced specification alleviates the limitations (1) by enabling faster sampling through quantum parallelism and (2) by performing projective measurements in the mixed Hadamard basis. Our evaluation of 143 quantum programs across four categories demonstrates significant improvements in test runtimes and fault detection with our reduction approach. Average test runtimes improved from 169.9s to 11.8s, with notable enhancements in programs with large circuit depths (383.1s to 33.4s) and large program specifications (464.8s to 7.7s). Furthermore, our approach increases mutation scores from \\(54.5\\%\\) to \\(74.7\\%\\) , effectively detecting phase flip faults that non-reduced specifications miss. These results underline our approach’s importance to improve QST efficiency and effectiveness",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406698716",
    "type": "article"
  },
  {
    "title": "Assessing and Analyzing the Correctness of GitHub Copilot’s Code Suggestions",
    "doi": "https://doi.org/10.1145/3715108",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Ran Mo; D. Wang; Wenjing Zhan; Yingjie Jiang; Yepeng Wang; Yuqi Zhao; Zengyang Li; Yutao Ma",
    "corresponding_authors": "",
    "abstract": "AI programming has become a popular topic in recent years. Code suggestion, with code suggestion being a key capability of AI programming. Copilot, an “AI programmer” that provides code suggestions from natural language descriptions, has been launched by GitHub and OpenAI. By far, Copilot has been widely used by millions of developers. However, little work has systematically evaluated the correctness of Copilot's suggestions. We conducted an empirical study on all 2,033 LeetCode problems to assess Copilot's code generation across four mainstream languages: C, Java, JavaScript, and Python. We have found that: 1) 70.0% of problems received at least one correct suggestion, with language-specific rates of 29.7% (C), 57.7% (Java), 54.1% (JavaScript), and 41.0% (Python); 2) Correctness decreases as problem difficulty increases, with acceptance rates of 89.3% (Easy), 72.1% (Medium), and 43.4% (Hard); 3) Acceptance rates vary across problem domains from 49.5% to 90.1%, while Graph problems challenge C and Python most, and Prefix Sum and Heap challenge Java and JavaScript most; 4) For the incorrect suggestions, we further summarize 17 types of error reasons accounting for their incorrectness and analyzed possible causes for why these errors occur. We believe our study can provide valuable insights into Copilot's capabilities and limitations.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406866173",
    "type": "article"
  },
  {
    "title": "Scalable Similarity-Aware Test Suite Minimization with Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3715008",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Sijia Gu; Ali Mesbah",
    "corresponding_authors": "",
    "abstract": "The Multi-Criteria Test Suite Minimization (MCTSM) problem aims to remove redundant test cases, guided by adequacy criteria such as code coverage or fault detection capability. However, current techniques either exhibit a high loss of fault detection ability or face scalability challenges due to the NP-hard nature of the problem, which limits their practical utility. We propose TripRL, a novel technique that integrates traditional criteria such as statement coverage and fault detection ability with test coverage similarity into an Integer Linear Program (ILP), to produce a diverse reduced test suite with high test effectiveness. TripRL leverages bipartite graph representation and its embedding for concise ILP formulation and combines ILP with effective reinforcement learning (RL) training. This combination renders large-scale test suite minimization more scalable and enhances test effectiveness. Our empirical evaluations demonstrate that TripRL’s runtime scales linearly with the magnitude of the MCTSM problem. Notably, for large test suites from the Defects4j dataset where existing approaches fail to provide solutions within a reasonable time frame, our technique consistently delivers solutions in less than 47 minutes. The reduced test suites produced by TripRL also maintain the original statement coverage and fault detection ability while having a higher potential to detect unknown faults.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406866174",
    "type": "article"
  },
  {
    "title": "Automated Recommendation of Extracting Local Variable Refactorings",
    "doi": "https://doi.org/10.1145/3715110",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Yanjie Jiang; Xiaye Chi; Yuxia Zhang; Weixing Ji; Guangjie Li; Weixiao Wang; Yunni Xia; Lu Zhang; Hui Liu",
    "corresponding_authors": "",
    "abstract": "Extracting local variable refactoring is frequently employed to replace one or more occurrences of a complex expression with simple accesses to a newly introduced variable. To facilitate the refactoring, most IDEs can automate the extract local variable refactorings when the to-be-extracted expressions are selected by developers. However, refactoring tools usually replace all expressions that are lexically identical to the selected one without a comprehensive analysis of the safety of the refactoring. The automatically conducted refactorings may lead to serious software defects. Besides that, existing refactoring tools rely heavily on software developers to spot to-be-extracted expressions although it is often challenging for inexperienced developers and maintainers to make the selection. To this end, in this paper, we propose an automated approach, called ValExtractor+ , to recommending extract local variable refactoring opportunities and to automatically and safely conduct the refactorings. ValExtractor+ is composed of two parts, i.e., solutionAdvisor and opportunityAdvisor. Given a to-be-extracted expression, solutionAdvisor leverages lightweight static source code analysis to validate potential side effects of the expression, and to identify expressions that could be extracted together with the selected expression as a single variable without changing the semantics of the program or introducing any new exceptions. The static code analysis significantly improves the safety in automated extraction of local variables. To free programmers from manually selecting to-be-extracted expressions, opportunityAdvisor leverages solutionAdvisor to automatically retrieve all expressions that could be extracted safely as well as their refactoring solutions. It then leverages a learning-based classifier to predict which of the retrieved expressions should be extracted. Evaluations on open-source applications suggest that solutionAdvisor successfully avoided all defects (more than two hundred) caused by extracting local variable refactorings conducted by Eclipse (243 defects) or IntelliJ IDEA (263 defects). Additionally, opportunityAdvisor was able to effectively recommend expressions for extraction, achieving 307 true positives (TP) and 21,121 true negatives (TN). Four pull requests from our work (PR IDs: 66 , 333 , 439 , and 360 ) were successfully merged into the Eclipse community repository, showcasing the practical impact and robustness of our approach as recognized by the wider developer community.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406866396",
    "type": "article"
  },
  {
    "title": "Test Oracle Automation in the Era of LLMs",
    "doi": "https://doi.org/10.1145/3715107",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Facundo Molina; Alessandra Gorla; Marcelo d’Amorim",
    "corresponding_authors": "",
    "abstract": "The effectiveness of a test suite in detecting faults highly depends on the quality of its test oracles. Large Language Models (LLMs) have demonstrated remarkable proficiency in tackling diverse software testing tasks. This paper aims to present a roadmap for future research on the use of LLMs for test oracle automation. We discuss the progress made in the field of test oracle automation before the introduction of LLMs, identifying the main limitations and weaknesses of existing techniques. Additionally, we discuss recent studies on the use of LLMs for this task, highlighting the main challenges that arise from their use, e.g., how to assess quality and usefulness of the generated oracles. We conclude with a discussion about the directions and opportunities for future research on LLM-based oracle automation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406867742",
    "type": "article"
  },
  {
    "title": "Distinguishing LLM-generated from Human-written Code by Contrastive Learning",
    "doi": "https://doi.org/10.1145/3705300",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Xiaodan Xu; Chao Ni; Xinrong Guo; S. B. Liu; Xiaoya Wang; Kui Liu; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs), such as ChatGPT released by OpenAI, have attracted significant attention from both industry and academia due to their demonstrated ability to generate high-quality content for various tasks. Despite the impressive capabilities of LLMs, there are growing concerns regarding their potential risks in various fields, such as news, education, and software engineering. Recently, several commercial and open-source LLM-generated content detectors have been proposed, which, however, are primarily designed for detecting natural language content without considering the specific characteristics of program code. This paper aims to fill this gap by proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a contrastive learning framework and a semantic encoder built with UniXcoder. To assess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated code from human-written code, we first curate a large-scale Human and Machine comparison Corpus (HMCorp), which includes 550K pairs of human-written and ChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs). Based on the HMCorp dataset, our qualitative and quantitative analysis of the characteristics of ChatGPT-generated code reveals the challenge and opportunity of distinguishing ChatGPT-generated code from human-written code with their representative features. Our experimental results indicate that CodeGPTSensor can effectively identify ChatGPT-generated code, outperforming all selected baselines.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406867827",
    "type": "article"
  },
  {
    "title": "Weighted Suspiciousness and Balanced Aggregation to Boost Spectrum-based Fault Localization of Deep Learning Models",
    "doi": "https://doi.org/10.1145/3716849",
    "publication_date": "2025-02-12",
    "publication_year": 2025,
    "authors": "Wenjie Xu; Yanhui Li; Mingliang Ma; Lin Chen; Yuming Zhou",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) models have proven to be highly successful and are now essential to our everyday routines. However, DL models, like traditional software, inevitably contain bugs that affect their performance in real-world scenarios. Effective software engineering techniques are necessary to ensure their dependability. In recent years, fault localization methods for DL models have gained significant attention as a valuable tool for improving the reliability of DL models. Owing to the data-driven programming paradigm, traditional fault localization techniques are challenging to apply directly to DL programs. Previous studies have shown that neuron errors within models can lead to abnormal behavior, and they fix the DL model errors from the perspective of neurons. Nonetheless, there remains a significant gap between the DL program statement and model errors. To tackle this problem, this paper proposes a novel fault localization method for DL models, named wei G hted s U sp I ciousness an D balanc E d agg R egation ( \\(\\mathsf{GUIDER}\\) ) that revisits the idea and challenge of spectrum-based fault localization in the context of DL models. For pre-trained DL models, \\(\\mathsf{GUIDER}\\) utilizes neuron coverage information and test case confidence to compute weighted neuron suspiciousness values and employs balanced aggregation methods to elevate these values from the neuron level to the layer level, which establishes a bridge between the DL model and the DL program, facilitating the developers’ debugging process. We evaluate \\(\\mathsf{GUIDER}\\) using 161 real model bugs collected from StackOverflow and five state-of-the-art fault localization methods for DL models as baselines. The results indicate that (a) our method successfully localizes 67% of the model bugs by ranking the buggy layer to the first place (i.e., top- \\(1\\) ), significantly outperforming all five baselines, and (b) our method maintains an acceptable time overhead compared with all baseline methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4407392629",
    "type": "article"
  },
  {
    "title": "An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities",
    "doi": "https://doi.org/10.1145/3717061",
    "publication_date": "2025-02-14",
    "publication_year": 2025,
    "authors": "Zezhou Yang; Sirong Chen; Cuiyun Gao; Zhenhao Li; Xing Hu; Kui Liu; Xin Xia",
    "corresponding_authors": "",
    "abstract": "Code generation aims to automatically generate code snippets of specific programming language according to natural language descriptions. The continuous advancements in deep learning, particularly pre-trained models, have empowered the code generation task to achieve remarkable performance. One main challenge of pre-trained models for code generation is the semantic gap between developers’ natural language requirements and source code. To address the issue, prior studies typically adopt a retrieval-augmented framework for the task, where the similar code snippets collected by a retrieval process can be leveraged to help understand the requirements and provide guidance for the generation process. In a retrieval-augmented framework, similar data can be retrieved from the database using a retrieval algorithm, and original input data can be fused with retrieved data by different fusion strategies. However, there is a lack of systematic study on the application of this framework for code generation, including the impact of the final generated results and the specific usage of the framework. In this paper, we choose three popular pre-trained code models, namely CodeGen, UniXcoder, and CodeT5, to assess the impact of the quality and utilization of retrieved code on the retrieval-augmented framework. Our analysis shows that the retrieval-augmented framework is beneficial for improving the performance of the existing pre-trained models. We also provide suggestions on the utilization of the retrieval-augmented code generation framework: BM25 and Sequential Integration Fusion are recommended due to their convenience and superior performance. Sketch Filling Fusion, which extracts a sketch of relevant code, could help the model improve its performance further. Additionally, we conduct experiments to investigate the influence of the retrieval-augmented framework on large language models for code generation, showing the effectiveness of the framework, and we discuss the trade-off between performance improvement and computational costs in each phase within the framework.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4407580437",
    "type": "article"
  },
  {
    "title": "Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained Language Models",
    "doi": "https://doi.org/10.1145/3721128",
    "publication_date": "2025-02-28",
    "publication_year": 2025,
    "authors": "Quanjun Zhang; Chunrong Fang; Yi Zheng; Yaxin Zhang; Yuan Zhao; Rubing Huang; Jianyi Zhou; Yun Yang; Tao Zheng; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Unit testing validates the correctness of the units of the software system under test and serves as the cornerstone in improving software quality and reliability. To reduce manual efforts in writing unit tests, some techniques have been proposed to generate test assertions automatically, including deep learning (DL)-based, retrieval-based, and integration-based ones. Among them, recent integration-based approaches inherit from both DL-based and retrieval-based approaches and are considered state-of-the-art. Despite being promising, such integration-based approaches suffer from inherent limitations, such as retrieving assertions with lexical matching while ignoring meaningful code semantics, and generating assertions with a limited training corpus. In this paper, we propose a novel Retri eval-Augmented Deep Assertion Gen eration approach, namely RetriGen, based on a hybrid assertion retriever and a pre-trained language model (PLM)-based assertion generator. Given a focal-test, RetriGen first builds a hybrid assertion retriever to search for the most relevant test-assert pair from external codebases. The retrieval process takes both lexical similarity and semantical similarity into account via a token-based and an embedding-based retriever, respectively. RetriGen then treats assertion generation as a sequence-to-sequence task and designs a PLM-based assertion generator to predict a correct assertion with historical test-assert pairs and the retrieved external assertion. Although our concept is general and can be adapted to various off-the-shelf encoder-decoder PLMs, we implement RetriGen to facilitate assertion generation based on the recent CodeT5 model. We conduct extensive experiments to evaluate RetriGen against six state-of-the-art approaches across two large-scale datasets and two metrics. The experimental results demonstrate that RetriGen achieves 57.66% and 73.24% in terms of accuracy and CodeBLEU, outperforming all baselines with an average improvement of 50.66% and 14.14%, respectively. Furthermore, RetriGen generates 1598 and 1818 unique correct assertions that all baselines fail to produce, 3.71X and 4.58X more than the most recent approach EditAS . We also demonstrate that adopting other PLMs can provide substantial advancement, e.g., four additionally-utilized PLMs outperform EditAS by 7.91% \\(\\sim\\) 12.70% accuracy improvement, indicating the generalizability of RetriGen. Overall, our study highlights the promising future of fine-tuning off-the-shelf PLMs to generate accurate assertions by incorporating external knowledge sources.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408028067",
    "type": "article"
  },
  {
    "title": "<scp>Reeq</scp> : Testing and Mitigating Ethically Inconsistent Suggestions of Large Language Models with Reflective Equilibrium",
    "doi": "https://doi.org/10.1145/3722554",
    "publication_date": "2025-03-11",
    "publication_year": 2025,
    "authors": "Pingchuan Ma; Zhaoyu Wang; Zongjie Li; Zhenlan Ji; Ao Sun; Juergen Rahmel; Shuai Wang",
    "corresponding_authors": "",
    "abstract": "LLMs increasingly serve as general-purpose AI assistants in daily life, and their subtly unethical suggestions become a serious and real concern. It is demanding to test and mitigate such unethical suggestions from LLMs. Despite existing efforts to detect violations of “testable” facets of ethics (e.g., fairness testing), it is challenging to encode the full scope of ethics (e.g., justice, deontology) into a test oracle without human annotations or intervention. In this paper, we take inspiration from reflective equilibrium, a modern moral reasoning method in moral and political philosophy, to guide our approach. Instead of seeking unethical suggestions in LLMs, we aim to identify behavioral inconsistency in LLMs’ ethics-related suggestions. These inconsistencies are anticipated to serve as a useful proxy and hint at unethical suggestions. We formulate reflective equilibrium in the form of fixed-point iteration, instantiate it as a novel test oracle, and also employ it to form a mitigation scheme for LLMs’ behavioral inconsistency on ethics-related inputs. To facilitate testing, we also create a comprehensive test suite, EthicsSuite , with 20K moral situations. In our study, we evaluate eight widely used LLMs. Our experiments reveal that LLMs are prone to ethical inconsistencies, with 81.22% of our test cases prompting ethically inconsistent suggestions on average. Our human evaluation suggests that the majority of these inconsistencies indeed manifest unethical biases. Our mitigation scheme effectively refines a significant number (80.1%) of these suggestions for commercial LLMs such as GPT-4 and Claude.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408303513",
    "type": "article"
  },
  {
    "title": "<scp>JailGuard</scp> : A Universal Detection Framework for Prompt-based Attacks on LLM Systems",
    "doi": "https://doi.org/10.1145/3724393",
    "publication_date": "2025-03-19",
    "publication_year": 2025,
    "authors": "Xiaoyu Zhang; Cen Zhang; Tianlin Li; Yihao Huang; Xiaojun Jia; Ming Hu; Jie Zhang; Yang Liu; Shiqing Ma; Chao Shen",
    "corresponding_authors": "",
    "abstract": "The systems and software powered by Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs) have played a critical role in numerous scenarios. However, current LLM systems are vulnerable to prompt-based attacks, with jailbreaking attacks enabling the LLM system to generate harmful content, while hijacking attacks manipulate the LLM system to perform attacker-desired tasks, underscoring the necessity for detection tools. Unfortunately, existing detecting approaches are usually tailored to specific attacks, resulting in poor generalization in detecting various attacks across different modalities. To address it, we propose JailGuard , a universal detection framework deployed on top of LLM systems for prompt-based attacks across text and image modalities. JailGuard operates on the principle that attacks are inherently less robust than benign ones. Specifically, JailGuard mutates untrusted inputs to generate variants and leverages the discrepancy of the variants’ responses on the target model to distinguish attack samples from benign samples. We implement 18 mutators for text and image inputs and design a mutator combination policy to further improve detection generalization. The evaluation on the dataset containing 15 known attack types suggests that JailGuard achieves the best detection accuracy of 86.14%/82.90% on text and image inputs, outperforming state-of-the-art methods by 11.81%-25.73% and 12.20%-21.40%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408612358",
    "type": "article"
  },
  {
    "title": "Investigating the Role of Cultural Values in Adopting Large Language Models for Software Engineering",
    "doi": "https://doi.org/10.1145/3725529",
    "publication_date": "2025-03-21",
    "publication_year": 2025,
    "authors": "Stefano Lambiase; Gemma Catolino; Fabio Palomba; Filomena Ferrucci; Daniel Russo",
    "corresponding_authors": "",
    "abstract": "As a socio-technical activity, software development involves the close interconnection of people and technology. The integration of Large Language Models (LLMs) into this process exemplifies the socio-technical nature of software development. Although LLMs influence the development process, software development remains fundamentally human-centric, necessitating an investigation of the human factors in this adoption. Thus, with this study we explore the factors influencing the adoption of LLMs in software development, focusing on the role of professionals’ cultural values. Guided by the Unified Theory of Acceptance and Use of Technology (UTAUT2) and Hofstede’s cultural dimensions, we hypothesized that cultural values moderate the relationships within the UTAUT2 framework. Using Partial Least Squares-Structural Equation Modelling and data from 188 software engineers, we found that habit and performance expectancy are the primary drivers of LLM adoption, while cultural values do not significantly moderate this process. These findings suggest that, by highlighting how LLMs can boost performance and efficiency, organizations can encourage their use, no matter the cultural differences. Practical steps include offering training programs to demonstrate LLM benefits, creating a supportive environment for regular use, and continuously tracking and sharing performance improvements from using LLMs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408692857",
    "type": "article"
  },
  {
    "title": "Characterizing Installation- and Run-Time Compatibility Issues in Android Benign Apps and Malware",
    "doi": "https://doi.org/10.1145/3725810",
    "publication_date": "2025-03-25",
    "publication_year": 2025,
    "authors": "Junhua Guo; Xiaoqin Fu; Li Li; Tao Zhang; Mattia Fazzini; Haipeng Cai",
    "corresponding_authors": "",
    "abstract": "The Android ecosystem has experienced rapid growth, resulting in a diverse range of platforms and devices. This expansion has also brought about compatibility issues that negatively impact user experiences and hinder app development productivity. Existing relevant studies are focused on and limited to the “static” sense of those issues (in terms of potentialities and proneness), while only addressing compatibility issues that possibly occur during app executions. In this paper, we present an extensive and longitudinal study on app compatibility issues that are disparate from yet complementary to prior studies, characterizing the incompatibilities based on actual , exercised observations and evidence at both installation and run time. With a dataset of 74,545 benign apps and 56,919 malicious apps over a span of 12 years (2010 through 2021) and ten Android versions, we extensively examine the prevalence and symptoms/effects and causes of, as well as the contributing factors to, installation-time and run-time compatibility issues. Our study reveals 12 major novel findings regarding Android app incompatibilities. Firstly ( Findings 1,2 ), installation-time incompatibilities persisted significantly over the 12 years, even more so in malware than benign apps. Secondly ( Findings 7,8 ), run-time compatibility issues were also seen persistently over time but only on specific Android platforms (such as API 26,27,etc.) and much less by malware than benign apps. Thirdly ( Findings 5,6,11,12 ), there is a significant (moderate/stronger) correlation between an app’s specified minSdkVersion and its incompatibilities (over all symptoms and/or with respect to one of its dominating symptom), with stronger correlations seen in malware than in benign apps, for both installation- and run-time incompatibilities. Similar observations hold (although with much stronger correlation in absolute terms) when considering, instead of the minSdkVersion itself, the gap between the app’s minSdkVersion and the SDK API level of the platform the app is installed to or runs on. Lastly ( Findings 3,4,9,10 ), installation-time incompatibilities are primarily caused by the utilization of architecture-incompatible native libraries within apps, while run-time incompatibilities are mainly attributed to API changes during the evolution of the Android SDK; the symptoms of run-time failures seen by malware are much more diverse than by benign apps. In addition to these insights, we provide practical recommendations for both app developers and end users on how to effectively address compatibility issues in Android apps, as well as how to devise effective defenses against malware from the compatibility perspectives.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408813186",
    "type": "article"
  },
  {
    "title": "Artificial Intelligence for Software Engineering: The Journey so far and the Road ahead",
    "doi": "https://doi.org/10.1145/3719006",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Iftekhar Ahmed; Aldeida Aleti; Haipeng Cai; Alexander Chatzigeorgiou; Pinjia He; Xing Hu; Mauro Pezzè; Denys Poshyvanyk; Xin Xia",
    "corresponding_authors": "",
    "abstract": "Artificial intelligence and recent advances in deep learning architectures, including transformer networks and large language models, change the way people think and act to solve problems. Software engineering, as an increasingly complex process to design, develop, test, deploy, and maintain large-scale software systems for solving real-world challenges, is profoundly affected by many revolutionary artificial intelligence tools in general, and machine learning in particular. In this roadmap for artificial intelligence in software engineering, we highlight the recent deep impact of artificial intelligence on software engineering by discussing successful stories of applications of artificial intelligence to classic and new software development challenges. We identify the new challenges that the software engineering community has to address in the coming years to successfully apply artificial intelligence in software engineering, and we share our research roadmap towards the effective use of artificial intelligence in the software engineering profession, while still protecting fundamental human values. We spotlight three main areas that challenge the research in software engineering: the use of generative artificial intelligence and large language models for engineering large software systems, the need of large and unbiased datasets and benchmarks for training and evaluating deep learning and large language models for software engineering, and the need of a new code of digital ethics to apply artificial intelligence in software engineering.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409584646",
    "type": "article"
  },
  {
    "title": "Programming of Automation Configuration in Smart Home Systems: Challenges and Opportunities",
    "doi": "https://doi.org/10.1145/3731450",
    "publication_date": "2025-04-22",
    "publication_year": 2025,
    "authors": "Sheik Murad Hassan Anik; Xinghua Gao; Hao Zhong; Xiaoyin Wang; Na Meng",
    "corresponding_authors": "",
    "abstract": "As the innovation of smart devices and internet-of-things (IoT), smart homes have become prevalent. People tend to transform residences into smart homes by customizing off-the-shelf smart home platforms, instead of creating IoT systems from scratch. Among the alternatives, Home Assistant (HA) is one of the most popular platforms. It allows programmers (i.e., home residents or smart-home creators) to smartify homes by (S1) integrating selected devices into the system, and (S2) programming YAML-based software to control those devices. Unfortunately, due to the diversity of devices and complexity of automatic configurations, many programmers have difficulty correctly creating YAML files. Consequently, their smart homes may not work as expected, causing frustration and concern in people. This paper presents a novel study on issues of YAML-based automation configuration in smart homes (issues related to S2). We mined the online forum Home Assistant Community for discussion threads related to programming of automation configuration. By manually inspecting 190 threads, we revealed 3 categories of concerns: implementation, optimization, and debugging. Under each category, we classified discussions based on the issue locations and technical concepts involved. Among debugging discussions, we further classified discussions based on users’ resolution strategies; we also applied existing analysis tools to buggy YAML files, to assess the tool effectiveness. Our study reveals the common challenges faced by programmers and frequently applied resolution strategies. There are 129 (68%) examined issues concerning debugging, but existing tools can detect at most 14 of the issues and fix none. It implies that existing tools provide limited assistance in automation configuration. Our research sheds light on future directions in smart home programming.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409663801",
    "type": "article"
  },
  {
    "title": "Abundant Modalities Offer More Nutrients: Multi-Modal-Based Function-level Vulnerability Detection",
    "doi": "https://doi.org/10.1145/3731557",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "Chao Ni; Xin Yin; Xinrui Li; Xiao-Ping Xu; Zhi Yu",
    "corresponding_authors": "",
    "abstract": "Software vulnerabilities are weaknesses in software systems that can lead to significant cybersecurity risks. Recently, several deep learning (DL)-based approaches have been proposed to detect vulnerabilities at the function level. These approaches typically utilize one or a few different modalities (e.g., text representation and graph-based representation) of the function, and have shown promising performance. However, existing studies have not fully leveraged diverse modalities, particularly those that use images to represent functions for vulnerability detection. These approaches often fail to make sufficient use of the important graph structure underlying the images. In this paper, we propose MVulD+, a multi-modal-based function-level vulnerability detection approach, which fuses multi-modal features of the function (i.e., text representation, graph representation, and image representation) to detect vulnerabilities. Specifically, MVulD+ leverages a pre-trained model (i.e, UniXcoder) to capture the semantic information of the textual source code, uses a graph neural network to extract graph representations, and employs computer vision techniques to obtain image representations while preserving the graph structure of the function. To investigate the effectiveness of MVulD+, we conduct a large-scale experiment by comparing our approach with nine state-of-the-art baselines. Experimental results demonstrate that MVulD+ improves the DL-based baselines by 24.3%-125.7%, 5.2%-31.4%, 40.6%-192.2%, and 22.3%-186.9% in terms of F1-score, Accuracy, Precision, and PR-AUC, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409730509",
    "type": "article"
  },
  {
    "title": "Automated Abstract Transformer Synthesis for Reduced Product Domains",
    "doi": "https://doi.org/10.1145/3733716",
    "publication_date": "2025-05-02",
    "publication_year": 2025,
    "authors": "Pankaj Kumar Kalita; Thomas Reps; Subhajit Roy",
    "corresponding_authors": "",
    "abstract": "Designing abstract transformers for program-analysis tools is a challenging task. In the past, bugs have been discovered in such transformers, showing the difficulty of designing such transformers manually, and providing motivation for automated techniques. Recently, Kalita et al. showed how to apply program-synthesis techniques to create abstract transformers in a user-provided domain-specific language (DSL) \\({\\mathcal{L}}\\) (i.e., “ \\({\\mathcal{L}}\\) -transformers”). Their technique creates provably sound and maximally precise \\({\\mathcal{L}}\\) -transformers for an abstract domain \\(A\\) —i.e., given specifications of a concrete operation op , DSL \\({\\mathcal{L}}\\) , and abstract domain \\(A\\) , it finds a best abstract \\({\\mathcal{L}}\\) -transformer for op in \\(A\\) . However, we found that the algorithm of Kalita et al. does not succeed when applied to reduced-product domains: the need to synthesize transformers for all of the domains simultaneously blows up the search space. Because reduced-product domains are an important device for improving the precision of abstract interpretation, in this paper, we propose an algorithm to synthesize reduced \\({\\mathcal{L}}\\) -transformers \\(\\langle{f}^{\\sharp\\textsf{R}}_{1},{f}^{\\sharp\\textsf{R}}_{2},\\dots,{f}^{\\sharp \\textsf{R}}_{n}\\rangle\\) for a product domain \\(A_{1}\\times A_{2}\\times\\dots\\times A_{n}\\) , using multiple DSLs: \\({\\mathcal{L}}\\) \\(=\\langle{\\mathcal{L}}_{1},{\\mathcal{L}}_{2},\\ldots,{\\mathcal{L}}_{n}\\rangle\\) . Synthesis of reduced-product transformers is quite challenging: first, the synthesis task has to tackle an increased “feature set” because each component transformer now has access to the abstract inputs from all component domains in the product. Second, to ensure that the product transformer is maximally precise, the synthesis task needs to arrange for the component transformers to cooperate with each other. We implemented our algorithm in a tool, Amurth2 , and used it to synthesize abstract transformers for two product domains—SAFE and JSAI—available within the SAFE str framework for JavaScript program analysis. For four of the six operations supported by SAFE str , Amurth2 synthesizes more precise abstract transformers than the manually written ones available in SAFE str .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4410028412",
    "type": "article"
  },
  {
    "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
    "doi": "https://doi.org/10.1145/3736407",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Martin Weyssow; Aton Kamanda; Xin Zhou; Houari Sahraoui",
    "corresponding_authors": "",
    "abstract": "Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavor that requires a deep assessment of LLMs’ outputs. Existing methods and benchmarks rely primarily on automated metrics and static analysis tools, which often fail to capture the nuances of user instructions and LLM outputs. To address this gap, we introduce the LLM-as-a-Judge evaluation framework and present CodeUltraFeedback, a comprehensive dataset for assessing and improving LLM alignment with coding preferences. CodeUltraFeedback consists of 10,000 coding instructions, each annotated with four responses generated from a diverse pool of 14 LLMs. These responses are annotated using GPT-3.5 as a judge, with both ranking-based scores and detailed textual feedback across five distinct coding preferences. Our analysis reveals that responses from GPT-3.5 and GPT-4 are consistently rated higher than those from open-weight models, underscoring substantial alignment gaps between closed- and open-weight LLMs. In turn, we explore the usage of CodeUltraFeedback as feedback data to fine-tune and align CodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement learning from AI feedback (RLAIF) with direct preference optimization (DPO). The resulting aligned model achieves an average alignment improvement of 22.7% and 29.7% when evaluated with GPT-3.5 and GPT-4 judges, respectively. Notably, our aligned CodeLlama-7B-Instruct surpasses much larger models, such as CodeLlama-13B and 34B, in alignment with coding preferences. Despite not being explicitly trained for functional correctness, it also achieves a 10.5% and 26.6% relative improvement in Pass@ \\(1\\) and Pass@ \\(10\\) on the HumanEval+ benchmark. Our contributions demonstrate the practical value of preference tuning in code generation and set the stage for further progress in model alignment and RLAIF for automated software engineering.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4410537502",
    "type": "article"
  },
  {
    "title": "Cost and Benefit of Tracing Features with Embedded Annotations",
    "doi": "https://doi.org/10.1145/3746060",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Thorsten Berger; Wardah Mahmood; Ramzi Abu Zahra; Igor Vassilevski; Andreas Burger; Wenbin Ji; Michał Antkiewicz; Krzysztof Czarnecki",
    "corresponding_authors": "",
    "abstract": "Features are commonly used to describe the functional and non-functional characteristics of software. Especially agile development methods, such as SCRUM, FDD or XP, use features to plan and manage software development. Features are often the main units of software reuse, communication, and configuration, abstracting over code details. Especially in the age of generative AI, where feature requirements are specified as prompts and substantial code is cloned, codebases are becoming increasingly complex and redundant. This requires raising the level of abstraction at which we manage and evolve software systems. However, effectively using features requires knowing their precise locations within codebases, which is especially challenging when they are scattered across the codebase. Once implemented, the knowledge about a feature’s location quickly deteriorates when the software evolves or development teams change, requiring expensive recovery of features. This decades-old problem is known as the feature-location or concept assignment problem in software engineering, which researchers have— unsuccessfully over decades—tried to address with automated feature-location recovery techniques. The problem lies in the common belief that recording and maintaining feature locations during development is laborious and error-prone. In this study, we argue to the contrary. We hypothesize that such information can be effectively embedded into codebases, and that the arising costs will be amortized by the benefits of this information. We validated this hypothesis in a simulation study with three subjects systems: a smaller open-source system, a large commercial firmware system, and an open-source mobile app. We designed a lightweight code annotation technique and simulated its use as if annotations had been added, maintained, and exploited during the original development. We identified evolution patterns and measured the cost and benefit of these annotations. Our results show that not only the cost of adding annotations, but also that of maintaining them is negligible compared to the development and maintenance costs of the actual code. Embedding the annotations into the codebase significantly reduced their maintenance effort, because they naturally co-evolved with the code. The annotations provided a benefit for feature-related maintenance tasks, such as feature cloning or merging the clones into an integrated codebase, that exceeded the costs of using them.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4413592694",
    "type": "article"
  },
  {
    "title": "<scp>CITYWALK</scp> : Enhancing LLM-Based C++ Unit Test Generation via Project-Dependency Awareness and Language-Specific Knowledge",
    "doi": "https://doi.org/10.1145/3763791",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Yuwei Zhang; Qingyuan Lu; Kai Liu; Wensheng Dou; Jiaxin Zhu; Qian Li; Chunzi Zhang; Zheng Lin; Jun Wei",
    "corresponding_authors": "",
    "abstract": "Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality. However, writing high-quality unit tests remains a time-consuming task for developers in practice. More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results. Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored. The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests. To tackle the aforementioned problems, this paper introduces CITYWALK , a novel LLM-based framework for C++ unit test generation. CITYWALK enhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis. Furthermore, CITYWALK incorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests. We implement CITYWALK by employing the widely popular LLM GPT-4o. The experimental results show that CITYWALK outperforms current state-of-the-art approaches on a collection of ten popular C++ projects. Our findings demonstrate the effectiveness of CITYWALK in generating high-quality C++ unit tests.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4413635683",
    "type": "article"
  },
  {
    "title": "Structuring Z specifications with views",
    "doi": "https://doi.org/10.1145/226241.226249",
    "publication_date": "1995-10-01",
    "publication_year": 1995,
    "authors": "Daniel Jackson",
    "corresponding_authors": "Daniel Jackson",
    "abstract": "A view is a partial specification of a program, consisting of a state space and a set of operations. A full specification is obtained by composing several views, linking them through their states (by asserting invariants across views) and through their operations (by defining external operations as combinations of operations from different views). By encouraging multiple representations of the program's state, view structuring lends clarity and terseness to the specification of operations. And by separating different aspects of functionality, it brings modularity at the grossest level of organization, so that specifications can accommodate change more gracefully. View structuring in Z is demonstrated with a few small examples. Both the features of Z that lend themselves to view structuring and those that are a hindrance are discussed.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2073806869",
    "type": "article"
  },
  {
    "title": "A framework for formalizing inconsistencies and deviations in human-centered systems",
    "doi": "https://doi.org/10.1145/234426.234427",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Gianpaolo Cugola; Elisabetta Di Nitto; Alfonso Fuggetta; Carlo Ghezzi",
    "corresponding_authors": "",
    "abstract": "Most modern business activities are carried out by a combination of computerized tools and human agents. Typical examples are engineering design activities, office procedures, and banking systems. All these human-centered systems are characterized by the interaction among people, and between people and computerized tools. This interaction defines a process, whose effectiveness is essential to ensure the quality of the delivered products and/or services. To support these systems, process-centered environments and workflow management systems have been recently developed. They can be collectively identified with the term process technology . This technology is based on the explicit definition of the process to be followed (the process model ). The model specifies the kind of support that has to be provided to human agents. An essential property that process technology mut exhibit is the ability of tolerating, controlling, and supporting deviations and inconsistencies of the real-world behaviors with respect to the proocess model. This is necessary to provide consistent and effective support to the human-centered system, still maintaining a high degree of flexibility and adaptability to the evolving needs, preferences, an expertise of the the human agents. This article presents a formal framework to characterize the interaction between a human-centered system and its automated support. It does not aim at introducing a new language or system to describe processes. Rather, it aims at identifying the basic properties and features that make it possible to formally define the concepts of inconsistency and deviation. This formal framework can then be used to compare existing solutions and guide future research work.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W2081689954",
    "type": "article"
  },
  {
    "title": "Flow analysis for verifying properties of concurrent software systems",
    "doi": "https://doi.org/10.1145/1040291.1040292",
    "publication_date": "2004-10-01",
    "publication_year": 2004,
    "authors": "Matthew B. Dwyer; Lori A. Clarke; Jamieson M. Cobleigh; Gleb Naumovich",
    "corresponding_authors": "",
    "abstract": "This article describes FLAVERS, a finite-state verification approach that analyzes whether concurrent systems satisfy user-defined, behavioral properties. FLAVERS automatically creates a compact, event-based model of the system that supports efficient dataflow analysis. FLAVERS achieves this efficiency at the cost of precision. Analysts, however, can improve the precision of analysis results by selectively and judiciously incorporating additional semantic information into an analysis.We report on an empirical study of the performance of the FLAVERS/Ada toolset applied to a collection of multitasking Ada systems. This study indicates that sufficient precision for proving system properties can usually be achieved and that the cost for such analysis typically grows as a low-order polynomial in the size of the system.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2130851407",
    "type": "article"
  },
  {
    "title": "Interprocedural control dependence",
    "doi": "https://doi.org/10.1145/367008.367022",
    "publication_date": "2001-04-01",
    "publication_year": 2001,
    "authors": "Saurabh Sinha; Mary Jean Harrold; Gregg Rothermel",
    "corresponding_authors": "",
    "abstract": "Program-dependence information is useful for a variety of applications, such as software testing and maintenance tasks, and code optimization. Properly defined, control and data dependences can be used to identify semantic dependences. To function effectively on whole programs, tools that utilize dependence information require information about interprocedural dependences: dependences that are identified by analyzing the interactions among procedures. Many techniques for computing interprocedural data dependences exist; however, virtually no attention has been paid to interprocedural control dependence. Analysis techniques that fail to account for interprocedural control dependences can suffer unnecessary imprecision and loss of safety. This article presents a definition of interprocedural control dependence that supports the relationship of control and data dependence to semantic dependence. The article presents two approaches for computing interprocedural control dependences, and empirical results pertaining to teh use of those approaches.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2060389212",
    "type": "article"
  },
  {
    "title": "Building integrated software development environments. Part I",
    "doi": "https://doi.org/10.1145/128894.128895",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Gregor Engels; Claus Lewerentz; Manfred Nagl; W. Schäfer; Andreas Schürr",
    "corresponding_authors": "",
    "abstract": "The conceptual modeling approach of the IPSEN (Integrated Project Support Environment) project for building highly integrated environments is based on using attributed graphs to model and implement arbitrary object structures, in particular all kinds of software documents and their relationships. A language based on graph grammars, called PROGRESS (PROgrammed Graph REwriting SyStems), and a suitable method for the application of this language, called graph grammar engineering, have been developed over the last ten years. This language and method are being extensively used for specifying the complex graph structures of internal document representations as well as for specifying the functionality of all tools (editors, browsers, analyzers, debuggers) working on these internal rpresentations. This paper explains the language and the method for applying the language based on a pragmatic nontrivial example of a software production process and its corresponding documents. In particular, it is shown why and how a graph grammar-based strongly typed language is perfectly suitable to formally specify highly integrated software tools. In addition, it is shown that the implementation of these tools (i.e., an environment composed of these tools) is systematically being derived from the formal specifications.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W1996596200",
    "type": "article"
  },
  {
    "title": "Static checking of system behaviors using derived component assumptions",
    "doi": "https://doi.org/10.1145/352591.352593",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Paola Inverardi; Alexander L. Wolf; Daniel Yankelevich",
    "corresponding_authors": "",
    "abstract": "A critical challenge faced by the developer of a software system is to understand whether the system's components correctly integrate. While type theory has provided substantial help in detecting and preventing errors in mismatched static properties, much work remains in the area of dynamics. In particular, components make assumptions about their behavioral interaction with other components, but currently we have only limited ways in which to state those assumptions and to analyze those assumptions for correctness. We have formulated a method that begins to address this problem. The method operates at the architectural level so that behavioral integration errors, such as deadlock, can be revealed early and at a high level. For each component, a specification is given of its interaction behavior. Form this specification, assumptions that the component makes about the corresponding interaction behavior of the external context are automatically derived. We have defined an algorithm that performs compatibility checks between finite representations of a component's context assumptions and the actual interaction behaviors of the components with which it is intended to interact. A configuration of a system is possible if and only if a successful way of matching actual behaviors with assumptions can be found. The state-space complexity of this algorithm is significantly less than that of comparable approaches, and in the worst case, the time complexity is comparable to the worst case of standard rachability analysis.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W1974986814",
    "type": "article"
  },
  {
    "title": "Mae---a system model and environment for managing architectural evolution",
    "doi": "https://doi.org/10.1145/1018210.1018213",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "Roshanak Roshandel; André van der Hoek; Marija Mikic-Rakic; Nenad Medvidović",
    "corresponding_authors": "",
    "abstract": "As with any other artifact produced as part of the software life cycle, software architectures evolve and this evolution must be managed. One approach to doing so would be to apply any of a host of existing configuration management systems, which have long been used successfully at the level of source code. Unfortunately, such an approach leads to many problems that prevent effective management of architectural evolution. To overcome these problems, we have developed an alternative approach centered on the use of an integrated architectural and configuration management system model. Because the system model combines architectural and configuration management concepts in a single representation, it has the distinct benefit that all architectural changes can be precisely captured and clearly related to each other---both at the fine-grained level of individual architectural elements and at the coarse-grained level of architectural configurations. To support the use of the system model, we have developed Mae, an architectural evolution environment through which users can specify architectures in a traditional manner, manage the evolution of the architectures using a check-out/check-in mechanism that tracks all changes, select a specific architectural configuration, and analyze the consistency of a selected configuration. We demonstrate the benefits of our approach by showing how the system model and its accompanying environment were used in the context of several representative projects.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W1986671085",
    "type": "article"
  },
  {
    "title": "Aspect",
    "doi": "https://doi.org/10.1145/210134.210135",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Daniel Jackson",
    "corresponding_authors": "Daniel Jackson",
    "abstract": "Aspect is a static analysis technique for detecting bugs in imperative programs, consisting of an annotation language and a checking tool. Like a type declaration, an Aspect annotation of a procedure is a kind of declarative, partial specification that can be checked efficiently in a modular fashion. But instead of constraining the types of arguments and results, Aspect specifications assert dependences that should hold between inputs and outputs. The checker uses a simple dependence analysis to check code against annotations and can find bugs automatically that are not detectable by other static means, especially errors of omission, which are common, but resistant to type checking. This article explains the basic scheme and shows how it is elaborated to handle data abstraction and aliasing.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2073241739",
    "type": "article"
  },
  {
    "title": "Semantic parameterization",
    "doi": "https://doi.org/10.1145/1416563.1416565",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Travis D. Breaux; Annie I. Antón; Jon Doyle",
    "corresponding_authors": "",
    "abstract": "Software engineers must systematically account for the broad scope of environmental behavior, including nonfunctional requirements, intended to coordinate the actions of stakeholders and software systems. The Inquiry Cycle Model (ICM) provides engineers with a strategy to acquire and refine these requirements by having domain experts answer six questions: who, what, where, when, how, and why. Goal-based requirements engineering has led to the formalization of requirements to answer the ICM questions about when , how , and why goals are achieved, maintained, or avoided. In this article, we present a systematic process called Semantic Parameterization for expressing natural language domain descriptions of goals as specifications in description logic. The formalization of goals in description logic allows engineers to automate inquiries using who , what , and where questions, completing the formalization of the ICM questions. The contributions of this approach include new theory to conceptually compare and disambiguate goal specifications that enables querying goals and organizing goals into specialization hierarchies. The artifacts in the process include a dictionary that aligns the domain lexicon with unique concepts, distinguishing between synonyms and polysemes, and several natural language patterns that aid engineers in mapping common domain descriptions to formal specifications. Semantic Parameterization has been empirically validated in three case studies on policy and regulatory descriptions that govern information systems in the finance and health-care domains.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1985903918",
    "type": "article"
  },
  {
    "title": "An empirical investigation of software reuse benefits in a large telecom product",
    "doi": "https://doi.org/10.1145/1363102.1363104",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Parastoo Mohagheghi; Reidar Conradi",
    "corresponding_authors": "",
    "abstract": "Background . This article describes a case study on the benefits of software reuse in a large telecom product. The reused components were developed in-house and shared in a product-family approach. Methods . Quantitative data mined from company repositories are combined with other quantitative data and qualitative observations. Results . We observed significantly lower fault density and less modified code between successive releases of the reused components. Reuse and standardization of software architecture and processes allowed easier transfer of development when organizational changes happened. Conclusions . The study adds to the evidence of quality benefits of large-scale reuse programs and explores organizational motivations and outcomes.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2147469854",
    "type": "article"
  },
  {
    "title": "The Minimal Failure-Causing Schema of Combinatorial Testing",
    "doi": "https://doi.org/10.1145/2000799.2000801",
    "publication_date": "2011-09-01",
    "publication_year": 2011,
    "authors": "Changhai Nie; Hareton Leung",
    "corresponding_authors": "",
    "abstract": "Combinatorial Testing (CT) involves the design of a small test suite to cover the parameter value combinations so as to detect failures triggered by the interactions among these parameters. To make full use of CT and to extend its advantages, this article first gives a model of CT and then presents a theory of the Minimal Failure-causing Schema (MFS), including the concept of the MFS, proof of its existence, some of its properties, and a method of finding the MFS. Then we propose a methodology for CT based on this MFS theory and the existing research. Our MFS-based methodology emphasizes that CT should work on accurate testing requirements, and has the following advantages: 1) Detect failure to the greatest degree with the least cost. 2) Effectiveness is improved by emphasizing mining of the information in software and making full use of the information gained from test design and execution. 3) Determine the root causes of failures and reveal related faults near the exposed ones. 4) Provide a foundation and model for regression testing and software quality evaluation of CT. A case study is presented to illustrate the MFS-based CT methodology, and an empirical study on a real software developed by us shows that the MFS really exists and the methodology based on MFS can considerably improve CT.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2039938470",
    "type": "article"
  },
  {
    "title": "Path- and index-sensitive string analysis based on monadic second-order logic",
    "doi": "https://doi.org/10.1145/2522920.2522926",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Takaaki Tateishi; Marco Pistoia; Omer Tripp",
    "corresponding_authors": "",
    "abstract": "We propose a novel technique for statically verifying the strings generated by a program. The verification is conducted by encoding the program in Monadic Second-order Logic (M2L). We use M2L to describe constraints among program variables and to abstract built-in string operations. Once we encode a program in M2L, a theorem prover for M2L, such as MONA, can automatically check if a string generated by the program satisfies a given specification, and if not, exhibit a counterexample. With this approach, we can naturally encode relationships among strings, accounting also for cases in which a program manipulates strings using indices. In addition, our string analysis is path sensitive in that it accounts for the effects of string and Boolean comparisons, as well as regular-expression matches. We have implemented our string analysis algorithm, and used it to augment an industrial security analysis for Web applications by automatically detecting and verifying sanitizers —methods that eliminate malicious patterns from untrusted strings, making these strings safe to use in security-sensitive operations. On the 8 benchmarks we analyzed, our string analyzer discovered 128 previously unknown sanitizers, compared to 71 sanitizers detected by a previously presented string analysis.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W1973650063",
    "type": "article"
  },
  {
    "title": "Validation of requirements for hybrid systems",
    "doi": "https://doi.org/10.1145/2377656.2377659",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Alessandro Cimatti; Marco Roveri; Angelo Susi; Stefano Tonetta",
    "corresponding_authors": "",
    "abstract": "Flaws in requirements may have unacceptable consequences in the development of safety-critical applications. Formal approaches may help with a deep analysis that takes care of the precise semantics of the requirements. However, the proposed solutions often disregard the problem of integrating the formalization with the analysis, and the underlying logical framework lacks either expressive power, or automation. We propose a new, comprehensive approach for the validation of functional requirements of hybrid systems, where discrete components and continuous components are tightly intertwined. The proposed solution allows to tackle problems of conversion from informal to formal, traceability, automation, user acceptance, and scalability. We build on a new language, othello which is expressive enough to represent various domains of interest, yet allowing efficient procedures for checking the satisfiability. Around this, we propose a structured methodology where: informal requirements are fragmented and categorized according to their role; each fragment is formalized based on its category; specialized formal analysis techniques, optimized for requirements analysis, are finally applied. The approach was the basis of an industrial project aiming at the validation of the European Train Control System (ETCS) requirements specification. During the project a realistic subset of the ETCS specification was formalized and analyzed. The approach was positively assessed by domain experts.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2090669830",
    "type": "article"
  },
  {
    "title": "Variability-Aware Static Analysis at Scale",
    "doi": "https://doi.org/10.1145/3280986",
    "publication_date": "2018-10-31",
    "publication_year": 2018,
    "authors": "Alexander von Rhein; Jörg Liebig; Andreas Janker; Christian Kästner; Sven Apel",
    "corresponding_authors": "",
    "abstract": "The advent of variability management and generator technology enables users to derive individual system variants from a configurable code base by selecting desired configuration options. This approach gives rise to the generation of possibly billions of variants, which, however, cannot be efficiently analyzed for bugs and other properties with classic analysis techniques. To address this issue, researchers and practitioners have developed sampling heuristics and, recently, variability-aware analysis techniques. While sampling reduces the analysis effort significantly, the information obtained is necessarily incomplete, and it is unknown whether state-of-the-art sampling techniques scale to billions of variants. Variability-aware analysis techniques process the configurable code base directly, exploiting similarities among individual variants with the goal of reducing analysis effort. However, while being promising, so far, variability-aware analysis techniques have been applied mostly only to small academic examples. To learn about the mutual strengths and weaknesses of variability-aware and sample-based static-analysis techniques, we compared the two by means of seven concrete control-flow and data-flow analyses, applied to five real-world subject systems: B usybox , O pen SSL, SQL ite , the x86 L inux kernel, and u C libc . In particular, we compare the efficiency (analysis execution time) of the static analyses and their effectiveness (potential bugs found). Overall, we found that variability-aware analysis outperforms most sample-based static-analysis techniques with respect to efficiency and effectiveness. For example, checking all variants of O pen SSL with a variability-aware static analysis is faster than checking even only two variants with an analysis that does not exploit similarities among variants.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2901842721",
    "type": "article"
  },
  {
    "title": "Traceability and SysML design slices to support safety inspections",
    "doi": "https://doi.org/10.1145/2559978",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Lionel Briand; Davide Falessi; Shiva Nejati; Mehrdad Sabetzadeh; Tao Yue",
    "corresponding_authors": "",
    "abstract": "Certifying safety-critical software and ensuring its safety requires checking the conformance between safety requirements and design. Increasingly, the development of safety-critical software relies on modeling, and the System Modeling Language (SysML) is now commonly used in many industry sectors. Inspecting safety conformance by comparing design models against safety requirements requires safety inspectors to browse through large models and is consequently time consuming and error-prone. To address this, we have devised a mechanism to establish traceability between (functional) safety requirements and SysML design models to extract design slices (model fragments) that filter out irrelevant details but keep enough context information for the slices to be easy to inspect and understand. In this article, we report on a controlled experiment assessing the impact of the traceability and slicing mechanism on inspectors' conformance decisions and effort. Results show a significant decrease in effort and an increase in decisions' correctness and level of certainty.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2050672323",
    "type": "article"
  },
  {
    "title": "DIG",
    "doi": "https://doi.org/10.1145/2556782",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "ThanhVu Nguyen; Deepak Kapur; Westley Weimer; Stephanie Forrest",
    "corresponding_authors": "",
    "abstract": "This article describes and evaluates DIG, a dynamic invariant generator that infers invariants from observed program traces, focusing on numerical and array variables. For numerical invariants, DIG supports both nonlinear equalities and inequalities of arbitrary degree defined over numerical program variables. For array invariants, DIG generates nested relations among multidimensional array variables. These properties are nontrivial and challenging for current static and dynamic invariant analysis methods. The key difference between DIG and existing dynamic methods is its generative technique, which infers invariants directly from traces, instead of using traces to filter out predefined templates. To generate accurate invariants, DIG employs ideas and tools from the mathematical and formal methods domains, including equation solving, polyhedra construction, and theorem proving; for example, DIG represents and reasons about polynomial invariants using geometric shapes. Experimental results on 27 mathematical algorithms and an implementation of AES encryption provide evidence that DIG is effective at generating invariants for these programs.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2062071335",
    "type": "article"
  },
  {
    "title": "Spectrum-Based Fault Localization in Model Transformations",
    "doi": "https://doi.org/10.1145/3241744",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "Javier Troya; Sergio Segura; José Antonio Parejo; Antonio Ruiz–Cortés",
    "corresponding_authors": "",
    "abstract": "Model transformations play a cornerstone role in Model-Driven Engineering (MDE), as they provide the essential mechanisms for manipulating and transforming models. The correctness of software built using MDE techniques greatly relies on the correctness of model transformations. However, it is challenging and error prone to debug them, and the situation gets more critical as the size and complexity of model transformations grow, where manual debugging is no longer possible. Spectrum-Based Fault Localization (SBFL) uses the results of test cases and their corresponding code coverage information to estimate the likelihood of each program component (e.g., statements) of being faulty. In this article we present an approach to apply SBFL for locating the faulty rules in model transformations. We evaluate the feasibility and accuracy of the approach by comparing the effectiveness of 18 different state-of-the-art SBFL techniques at locating faults in model transformations. Evaluation results revealed that the best techniques, namely Kulcynski2 , Mountford , Ochiai , and Zoltar , lead the debugger to inspect a maximum of three rules to locate the bug in around 74% of the cases. Furthermore, we compare our approach with a static approach for fault localization in model transformations, observing a clear superiority of the proposed SBFL-based method.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2892114579",
    "type": "article"
  },
  {
    "title": "STADS",
    "doi": "https://doi.org/10.1145/3210309",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Marcel Böhme",
    "corresponding_authors": "Marcel Böhme",
    "abstract": "A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (1) to estimate the total number of feasible program branches, given that only a fraction has been covered so far; (2) to estimate the additional time required to cover 10% more branches (or to estimate the coverage achieved in one more day, respectively); or (3) to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability does not mean that none exists—even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees . In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimations (1) of the total number of species, (2) of the additional sampling effort required to discover 10% more species, or (3) of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias—AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2963147982",
    "type": "article"
  },
  {
    "title": "On the impact of UML analysis models on source-code comprehensibility and modifiability",
    "doi": "https://doi.org/10.1145/2491912",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Giuseppe Scanniello; Carmine Gravino; Marcela Genero; José A. Cruz-Lemus; Genoveffa Tortora",
    "corresponding_authors": "",
    "abstract": "We carried out a family of experiments to investigate whether the use of UML models produced in the requirements analysis process helps in the comprehensibility and modifiability of source code. The family consists of a controlled experiment and 3 external replications carried out with students and professionals from Italy and Spain. 86 participants with different abilities and levels of experience with UML took part. The results of the experiments were integrated through the use of meta-analysis. The results of both the individual experiments and meta-analysis indicate that UML models produced in the requirements analysis process influence neither the comprehensibility of source code nor its modifiability.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2077924088",
    "type": "article"
  },
  {
    "title": "Linear Programming as a Baseline for Software Effort Estimation",
    "doi": "https://doi.org/10.1145/3234940",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "Federica Sarro; Alessio Petrozziello",
    "corresponding_authors": "",
    "abstract": "Software effort estimation studies still suffer from discordant empirical results (i.e., conclusion instability) mainly due to the lack of rigorous benchmarking methods. So far only one baseline model, namely, Automatically Transformed Linear Model (ATLM), has been proposed yet it has not been extensively assessed. In this article, we propose a novel method based on Linear Programming (dubbed as Linear Programming for Effort Estimation, LP4EE) and carry out a thorough empirical study to evaluate the effectiveness of both LP4EE and ATLM for benchmarking widely used effort estimation techniques. The results of our study confirm the need to benchmark every other proposal against accurate and robust baselines. They also reveal that LP4EE is more accurate than ATLM for 17% of the experiments and more robust than ATLM against different data splits and cross-validation methods for 44% of the cases. These results suggest that using LP4EE as a baseline can help reduce conclusion instability. We make publicly available an open-source implementation of LP4EE in order to facilitate its adoption in future studies.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2889763677",
    "type": "article"
  },
  {
    "title": "Editorial Journal-First Publication for the Software Engineering Community",
    "doi": "https://doi.org/10.1145/2837717",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Matthew B. Dwyer; David S. Rosenblum",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2257166867",
    "type": "article"
  },
  {
    "title": "Do Automatically Generated Test Cases Make Debugging Easier? An Experimental Assessment of Debugging Effectiveness and Efficiency",
    "doi": "https://doi.org/10.1145/2768829",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Mariano Ceccato; Alessandro Marchetto; Leonardo Mariani; Cu Nguyen; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "Several techniques and tools have been proposed for the automatic generation of test cases. Usually, these tools are evaluated in terms of fault-revealing or coverage capability, but their impact on the manual debugging activity is not considered. The question is whether automatically generated test cases are equally effective in supporting debugging as manually written tests. We conducted a family of three experiments (five replications) with humans (in total, 55 subjects) to assess whether the features of automatically generated test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on the effectiveness and efficiency of debugging. The first two experiments compare different test case generation tools (Randoop vs. EvoSuite). The third experiment investigates the role of code identifiers in test cases (obfuscated vs. original identifiers), since a major difference between manual and automatically generated test cases is that the latter contain meaningless (obfuscated) identifiers. We show that automatically generated test cases are as useful for debugging as manual test cases. Furthermore, we find that, for less experienced developers, automatic tests are more useful on average due to their lower static and dynamic complexity.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2267952752",
    "type": "article"
  },
  {
    "title": "Join point interfaces for safe and flexible decoupling of aspects",
    "doi": "https://doi.org/10.1145/2559933",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Eric Bodden; Éric Tanter; Milton Inostroza",
    "corresponding_authors": "",
    "abstract": "In current aspect-oriented systems, aspects usually carry, through their pointcuts, explicit references to the base code. Those references are fragile and hinder important software engineering properties such as modular reasoning and independent evolution of aspects and base code. In this work, we introduce a novel abstraction called Join Point Interface, which, by design, aids modular reasoning and independent evolution by decoupling aspects from base code and by providing a modular type-checking algorithm. Join point interfaces can be used both with implicit announcement through pointcuts, and with explicit announcement, using closure join points. Join point interfaces further offer polymorphic dispatch on join points, with an advice-dispatch semantics akin to multimethods. To support flexible join point matching, we incorporate into our language an earlier proposal for generic advice, and introduce a mechanism for controlled global quantification. We motivate each language feature in detail, showing that it is necessary to obtain a language design that is both type safe and flexible enough to support typical aspect-oriented programming idioms. We have implemented join point interfaces as an open-source extension to AspectJ. A case study on existing aspect-oriented programs supports our design, and in particular shows the necessity of both generic interfaces and some mechanism for global quantification.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2140213166",
    "type": "article"
  },
  {
    "title": "Understanding and Analyzing Java Reflection",
    "doi": "https://doi.org/10.1145/3295739",
    "publication_date": "2019-02-26",
    "publication_year": 2019,
    "authors": "Yue Li; Tian Tan; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Java reflection has been widely used in a variety of applications and frameworks. It allows a software system to inspect and change the behaviour of its classes, interfaces, methods, and fields at runtime, enabling the software to adapt to dynamically changing runtime environments. However, this dynamic language feature imposes significant challenges to static analysis, because the behaviour of reflection-rich software is logically complex and statically hard to predict. As a result, existing static analysis tools either ignore reflection or handle it partially, resulting in missed, important behaviours, i.e., unsound results. Therefore, improving or even achieving soundness in static reflection analysis-an analysis that infers statically the behaviour of reflective code-will provide significant benefits to many analysis clients, such as bug detectors, security analyzers, and program verifiers. In this article, we provide a comprehensive understanding of Java reflection through examining its underlying concept, API, and real-world usage, and, building on this, we introduce a new static approach to resolving Java reflection effectively in practice. We have implemented our reflection analysis in an open-source tool, called Solar, and evaluated its effectiveness extensively with large Java programs and libraries. Our experimental results demonstrate that Solar is able to (1) resolve reflection more soundly than the state-of-the-art reflection analyses; (2) automatically and accurately identify the parts of the program where reflection is resolved unsoundly or imprecisely; and (3) guide users to iteratively refine the analysis results by using lightweight annotations until their specific requirements are satisfied.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2962823786",
    "type": "article"
  },
  {
    "title": "Precise Learn-to-Rank Fault Localization Using Dynamic and Static Features of Target Programs",
    "doi": "https://doi.org/10.1145/3345628",
    "publication_date": "2019-10-09",
    "publication_year": 2019,
    "authors": "Yunho Kim; Seokhyeon Mun; Shin Yoo; Moonzoo Kim",
    "corresponding_authors": "",
    "abstract": "Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information and structural complexity of program entities. All such information is used by GP to train a ranking model for fault localization. The empirical evaluation on 65 real-world faults from CoREBench, 84 artificial faults from SIR, and 310 real-world faults from Defects4J shows that PRINCE outperforms the state-of-the-art SBFL, MBFL, and learn-to-rank techniques significantly. PRINCE localizes a fault after reviewing 2.4% of the executed statements on average (4.2 and 3.0 times more precise than the best of the compared SBFL and MBFL techniques, respectively). Also, PRINCE ranks 52.9% of the target faults within the top ten suspicious statements.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2979367728",
    "type": "article"
  },
  {
    "title": "Recommending New Features from Mobile App Descriptions",
    "doi": "https://doi.org/10.1145/3344158",
    "publication_date": "2019-10-09",
    "publication_year": 2019,
    "authors": "He Jiang; Jingxuan Zhang; Xiaochen Li; Zhilei Ren; David Lo; Xindong Wu; Zhongxuan Luo",
    "corresponding_authors": "",
    "abstract": "The rapidly evolving mobile applications (apps) have brought great demand for developers to identify new features by inspecting the descriptions of similar apps and acquire missing features for their apps. Unfortunately, due to the huge number of apps, this manual process is time-consuming and unscalable. To help developers identify new features, we propose a new approach named SAFER. In this study, we first develop a tool to automatically extract features from app descriptions. Then, given an app, we leverage the topic model to identify its similar apps based on the extracted features and API names of apps. Finally, we design a feature recommendation algorithm to aggregate and recommend the features of identified similar apps to the specified app. Evaluated over a collection of 533 annotated features from 100 apps, SAFER achieves a Hit@15 score of up to 78.68% and outperforms the baseline approach KNN+ by 17.23% on average. In addition, we also compare SAFER against a typical technique of recommending features from user reviews, i.e., CLAP. Experimental results reveal that SAFER is superior to CLAP by 23.54% in terms of Hit@15.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2979307736",
    "type": "article"
  },
  {
    "title": "Test-Equivalence Analysis for Automatic Patch Generation",
    "doi": "https://doi.org/10.1145/3241980",
    "publication_date": "2018-10-22",
    "publication_year": 2018,
    "authors": "Sergey Mechtaev; Xiang Gao; Shin Hwei Tan; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading student assignments, and patching security vulnerabilities. A common challenge faced by existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider. The correctness criteria for program repair is often given as a suite of tests. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. In this work, we address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are “test-equivalent” for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies, respectively, and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes. Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2898024328",
    "type": "article"
  },
  {
    "title": "Why Developers Refactor Source Code",
    "doi": "https://doi.org/10.1145/3408302",
    "publication_date": "2020-09-26",
    "publication_year": 2020,
    "authors": "Jevgenija Pantiuchina; Fiorella Zampetti; Simone Scalabrino; Valentina Piantadosi; Rocco Oliveto; Gabriele Bavota; Massimiliano Di Penta",
    "corresponding_authors": "",
    "abstract": "Refactoring aims at improving code non-functional attributes without modifying its external behavior. Previous studies investigated the motivations behind refactoring by surveying developers. With the aim of generalizing and complementing their findings, we present a large-scale study quantitatively and qualitatively investigating why developers perform refactoring in open source projects. First, we mine 287,813 refactoring operations performed in the history of 150 systems. Using this dataset, we investigate the interplay between refactoring operations and process (e.g., previous changes/fixes) and product (e.g., quality metrics) metrics. Then, we manually analyze 551 merged pull requests implementing refactoring operations and classify the motivations behind the implemented refactorings (e.g., removal of code duplication). Our results led to (i) quantitative evidence of the relationship existing between certain process/product metrics and refactoring operations and (ii) a detailed taxonomy, generalizing and complementing the ones existing in the literature, of motivations pushing developers to refactor source code.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W4288076432",
    "type": "article"
  },
  {
    "title": "Visualizing Distributed System Executions",
    "doi": "https://doi.org/10.1145/3375633",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Ivan Beschastnikh; Perry Liu; Albert Xing; Patty Wang; Yuriy Brun; Michael D. Ernst",
    "corresponding_authors": "",
    "abstract": "Distributed systems pose unique challenges for software developers. Understanding the system’s communication topology and reasoning about concurrent activities of system hosts can be difficult. The standard approach, analyzing system logs, can be a tedious and complex process that involves reconstructing a system log from multiple hosts’ logs, reconciling timestamps among hosts with non-synchronized clocks, and understanding what took place during the execution encoded by the log. This article presents a novel approach for tackling three tasks frequently performed during analysis of distributed system executions: (1) understanding the relative ordering of events, (2) searching for specific patterns of interaction between hosts, and (3) identifying structural similarities and differences between pairs of executions. Our approach consists of XVector , which instruments distributed systems to capture partial ordering information that encodes the happens-before relation between events, and ShiViz , which processes the resulting logs and presents distributed system executions as interactive time-space diagrams. Two user studies with a total of 109 students and a case study with 2 developers showed that our method was effective, helping participants answer statistically significantly more system-comprehension questions correctly, with a very large effect size.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3010215199",
    "type": "article"
  },
  {
    "title": "Handling SQL Databases in Automated System Test Generation",
    "doi": "https://doi.org/10.1145/3391533",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Andrea Arcuri; Juan Pablo Galeotti",
    "corresponding_authors": "",
    "abstract": "Automated system test generation for web/enterprise systems requires either a sequence of actions on a GUI (e.g., clicking on HTML links and form buttons) or direct HTTP calls when dealing with web services (e.g., REST and SOAP). When doing white-box testing of such systems, their code can be analyzed, and the same type of heuristics (e.g., the branch distance ) used in search-based unit testing can be employed to improve performance. However, web/enterprise systems do often interact with a database. To obtain higher coverage and find new faults, the state of the databases needs to be taken into account when generating white-box tests. In this work, we present a novel heuristic to enhance search-based software testing of web/enterprise systems, which takes into account the state of the accessed databases. Furthermore, we enable the generation of SQL data directly from the test cases. This is useful when it is too difficult or time consuming to generate the right sequence of events to put the database in the right state. Also, it is useful when dealing with databases that are “read-only” for the system under test, and the actual data are generated by other services. We implemented our technique as an extension of E VO M ASTER , where system tests are generated in the JUnit format. Experiments on six RESTful APIs (five open-source and one industrial) show that our novel techniques improve coverage significantly (up to +16.5%), finding seven new faults in those systems.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3038801694",
    "type": "article"
  },
  {
    "title": "IntDroid",
    "doi": "https://doi.org/10.1145/3442588",
    "publication_date": "2021-05-08",
    "publication_year": 2021,
    "authors": "Deqing Zou; Yueming Wu; Siru Yang; Anki Chauhan; Wei Yang; Jiangying Zhong; Shihan Dou; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Android, the most popular mobile operating system, has attracted millions of users around the world. Meanwhile, the number of new Android malware instances has grown exponentially in recent years. On the one hand, existing Android malware detection systems have shown that distilling the program semantics into a graph representation and detecting malicious programs by conducting graph matching are able to achieve high accuracy on detecting Android malware. However, these traditional graph-based approaches always perform expensive program analysis and suffer from low scalability on malware detection. On the other hand, because of the high scalability of social network analysis, it has been applied to complete large-scale malware detection. However, the social-network-analysis-based method only considers simple semantic information (i.e., centrality) for achieving market-wide mobile malware scanning, which may limit the detection effectiveness when benign apps show some similar behaviors as malware. In this article, we aim to combine the high accuracy of traditional graph-based method with the high scalability of social-network-analysis--based method for Android malware detection. Instead of using traditional heavyweight static analysis, we treat function call graphs of apps as complex social networks and apply social-network--based centrality analysis to unearth the central nodes within call graphs. After obtaining the central nodes, the average intimacies between sensitive API calls and central nodes are computed to represent the semantic features of the graphs. We implement our approach in a tool called IntDroid and evaluate it on a dataset of 3,988 benign samples and 4,265 malicious samples. Experimental results show that IntDroid is capable of detecting Android malware with an F-measure of 97.1% while maintaining a True-positive Rate of 99.1%. Although the scalability is not as fast as a social-network-analysis--based method (i.e., MalScan ), compared to a traditional graph-based method, IntDroid is more than six times faster than MaMaDroid . Moreover, in a corpus of apps collected from GooglePlay market, IntDroid is able to identify 28 zero-day malware that can evade detection of existing tools, one of which has been downloaded and installed by more than ten million users. This app has also been flagged as malware by six anti-virus scanners in VirusTotal, one of which is Symantec Mobile Insight .",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3160345471",
    "type": "article"
  },
  {
    "title": "Are Multi-Language Design Smells Fault-Prone? An Empirical Study",
    "doi": "https://doi.org/10.1145/3432690",
    "publication_date": "2021-02-11",
    "publication_year": 2021,
    "authors": "Mouna Abidi; Md Saidur Rahman; Moses Openja; Foutse Khomh",
    "corresponding_authors": "",
    "abstract": "Nowadays, modern applications are developed using components written in different programming languages and technologies. The cost benefits of reuse and the advantages of each programming language are two main incentives behind the proliferation of such systems. However, as the number of languages increases, so do the challenges related to the development and maintenance of these systems. In such situations, developers may introduce design smells (i.e., anti-patterns and code smells) which are symptoms of poor design and implementation choices. Design smells are defined as poor design and coding choices that can negatively impact the quality of a software program despite satisfying functional requirements. Studies on mono-language systems suggest that the presence of design smells may indicate a higher risk of future bugs and affects code comprehension, thus making systems harder to maintain. However, the impact of multi-language design smells on software quality such as fault-proneness is yet to be investigated. In this article, we present an approach to detect multi-language design smells in the context of JNI systems. We then investigate the prevalence of those design smells and their impacts on fault-proneness. Specifically, we detect 15 design smells in 98 releases of 9 open-source JNI projects. Our results show that the design smells are prevalent in the selected projects and persist throughout the releases of the systems. We observe that, in the analyzed systems, 33.95% of the files involving communications between Java and C/C++ contain occurrences of multi-language design smells. Some kinds of smells are more prevalent than others, e.g., Unused Parameters , Too Much Scattering , and Unused Method Declaration . Our results suggest that files with multi-language design smells can often be more associated with bugs than files without these smells, and that specific smells are more correlated to fault-proneness than others. From analyzing fault-inducing commit messages, we also extracted activities that are more likely to introduce bugs in smelly files. We believe that our findings are important for practitioners as it can help them prioritize design smells during the maintenance of multi-language systems.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3131012613",
    "type": "article"
  },
  {
    "title": "On the Reproducibility and Replicability of Deep Learning in Software Engineering",
    "doi": "https://doi.org/10.1145/3477535",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Chao Liu; Cuiyun Gao; Xin Xia; David Lo; John Grundy; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Context: Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge. Objective: Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) reproducibility —whether the reported experimental results can be obtained by other researchers using authors’ artifacts (i.e., source code and datasets) with the same experimental setup; and (2) replicability —whether the reported experimental result can be obtained by other researchers using their re-implemented artifacts with a different experimental setup. We observed that DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process, unlike classical supervised machine learning (ML) methods (e.g., random forest). This study aims to investigate the urgency and importance of reproducibility and replicability for DL studies on SE tasks. Method: In this study, we conducted a literature review on 147 DL studies recently published in 20 SE venues and 20 AI (Artificial Intelligence) venues to investigate these issues. We also re-ran four representative DL models in SE to investigate important factors that may strongly affect the reproducibility and replicability of a study. Results: Our statistics show the urgency of investigating these two factors in SE, where only 10.2% of the studies investigate any research question to show that their models can address at least one issue of replicability and/or reproducibility. More than 62.6% of the studies do not even share high-quality source code or complete data to support the reproducibility of their complex models. Meanwhile, our experimental results show the importance of reproducibility and replicability, where the reported performance of a DL model could not be reproduced for an unstable optimization process. Replicability could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data. Conclusion: It is urgent for the SE community to provide a long-lasting link to a high-quality reproduction package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3211225893",
    "type": "article"
  },
  {
    "title": "How Far Have We Progressed in Identifying Self-admitted Technical Debts? A Comprehensive Empirical Study",
    "doi": "https://doi.org/10.1145/3447247",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Zhaoqiang Guo; Shiran Liu; Jinping Liu; Yanhui Li; Lin Chen; Hongmin Lu; Yuming Zhou",
    "corresponding_authors": "",
    "abstract": "Background. Self-admitted technical debt (SATD) is a special kind of technical debt that is intentionally introduced and remarked by code comments. Those technical debts reduce the quality of software and increase the cost of subsequent software maintenance. Therefore, it is necessary to find out and resolve these debts in time. Recently, many automatic approaches have been proposed to identify SATD. Problem. Popular IDEs support a number of predefined task annotation tags for indicating SATD in comments, which have been used in many projects. However, such clear prior knowledge is neglected by existing SATD identification approaches when identifying SATD. Objective. We aim to investigate how far we have really progressed in the field of SATD identification by comparing existing approaches with a simple approach that leverages the predefined task tags to identify SATD. Method. We first propose a simple heuristic approach that fuzzily Matches task Annotation Tags ( MAT ) in comments to identify SATD. In nature, MAT is an unsupervised approach, which does not need any data to train a prediction model and has a good understandability. Then, we examine the real progress in SATD identification by comparing MAT against existing approaches. Result. The experimental results reveal that: (1) MAT has a similar or even superior performance for SATD identification compared with existing approaches, regardless of whether non-effort-aware or effort-aware evaluation indicators are considered; (2) the SATDs (or non-SATDs) correctly identified by existing approaches are highly overlapped with those identified by MAT ; and (3) supervised approaches misclassify many SATDs marked with task tags as non-SATDs, which can be easily corrected by their combinations with MAT . Conclusion. It appears that the problem of SATD identification has been (unintentionally) complicated by our community, i.e., the real progress in SATD comments identification is not being achieved as it might have been envisaged. We hence suggest that, when many task tags are used in the comments of a target project, future SATD identification studies should use MAT as an easy-to-implement baseline to demonstrate the usefulness of any newly proposed approach.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3185691166",
    "type": "article"
  },
  {
    "title": "Industry–Academia Research Collaboration and Knowledge Co-creation: Patterns and Anti-patterns",
    "doi": "https://doi.org/10.1145/3494519",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Dusica Marijan; Sagar Sen",
    "corresponding_authors": "",
    "abstract": "Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry-academia collaborations. This paper reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research-practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry-academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4220892194",
    "type": "article"
  },
  {
    "title": "Context- and Fairness-Aware In-Process Crowdworker Recommendation",
    "doi": "https://doi.org/10.1145/3487571",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Junjie Wang; Ye Yang; Song Wang; Jun Hu; Qing Wang",
    "corresponding_authors": "",
    "abstract": "Identifying and optimizing open participation is essential to the success of open software development. Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to improve bug detection efficiency, i.e., detect more bugs with fewer workers. However, there are a couple of limitations in existing work. First, these studies mainly focus on one-time recommendations based on expertise matching at the beginning of a new task. Second, the recommendation results suffer from severe popularity bias, i.e., highly experienced workers are recommended in almost all the tasks, while less experienced workers rarely get recommended. This article argues the need for context- and fairness-aware in-process crowdworker recommendation in order to address these limitations. We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, i.e., no new bugs are revealed in consecutive test reports during the process of a crowdtesting task. This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened. Besides, motivated by the popularity bias in existing crowdworker recommendation approach, this study also aims at alleviating the unfairness in recommendations. Driven by these observations, this article proposes a context- and fairness-aware in-process crowdworker recommendation approach, iRec2.0, to detect more bugs earlier, shorten the non-yielding windows, and alleviate the unfairness in recommendations. It consists of three main components: (1) the modeling of dynamic testing context, (2) the learning-based ranking component, and (3) the multi-objective optimization-based re-ranking component. The evaluation is conducted on 636 crowdtesting tasks from one of the largest crowdtesting platforms, and results show the potential of iRec2.0 in improving the cost-effectiveness of crowdtesting by saving the cost, shortening the testing process, and alleviating the unfairness among workers. In detail, iRec2.0 could shorten the non-yielding window by a median of 50%–66% in different application scenarios, and consequently have potential of saving testing cost by a median of 8%–12%. Meanwhile, the recommendation frequency of the crowdworker drop from 34%–60% to 5%–26% under different scenarios, indicating its potential in alleviating the unfairness among crowdworkers.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4220958110",
    "type": "article"
  },
  {
    "title": "ReCDroid+: Automated End-to-End Crash Reproduction from Bug Reports for Android Apps",
    "doi": "https://doi.org/10.1145/3488244",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Yu Zhao; Ting Su; Yang Liu; Wei Zheng; Xiaoxue Wu; Ramakanth Kavuluru; William G. J. Halfond; Tingting Yu",
    "corresponding_authors": "",
    "abstract": "The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers heavily rely on bug reports in issue tracking systems to reproduce failures (e.g., crashes). However, the process of crash reproduction is often manually done by developers, making the resolution of bugs inefficient, especially given that bug reports are often written in natural language. To improve the productivity of developers in resolving bug reports, in this paper, we introduce a novel approach, called ReCDroid+, that can automatically reproduce crashes from bug reports for Android apps. ReCDroid+ uses a combination of natural language processing (NLP) , deep learning, and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash. We have evaluated ReCDroid+ on 66 original bug reports from 37 Android apps. The results show that ReCDroid+ successfully reproduced 42 crashes (63.6% success rate) directly from the textual description of the manually reproduced bug reports. A user study involving 12 participants demonstrates that ReCDroid+ can improve the productivity of developers when resolving crash bug reports.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4221024422",
    "type": "article"
  },
  {
    "title": "Similarity-based Web Element Localization for Robust Test Automation",
    "doi": "https://doi.org/10.1145/3571855",
    "publication_date": "2022-11-22",
    "publication_year": 2022,
    "authors": "Michel Nass; Emil Alégroth; Robert Feldt; Maurizio Leotta; Filippo Ricca",
    "corresponding_authors": "",
    "abstract": "Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11%) compared to 214 failed cases (i.e., 27%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4309694800",
    "type": "article"
  },
  {
    "title": "Duplicate Bug Report Detection: How Far Are We?",
    "doi": "https://doi.org/10.1145/3576042",
    "publication_date": "2022-12-12",
    "publication_year": 2022,
    "authors": "Ting Zhang; DongGyun Han; Venkatesh Vinayakarao; Ivana Clairine Irsan; Bowen Xu; Ferdian Thung; David Lo; Lingxiao Jiang",
    "corresponding_authors": "",
    "abstract": "Many Duplicate Bug Report Detection (DBRD) techniques have been proposed in the research literature. The industry uses some other techniques. Unfortunately, there is insufficient comparison among them, and it is unclear how far we have been. This work fills this gap by comparing the aforementioned techniques. To compare them, we first need a benchmark that can estimate how a tool would perform if applied in a realistic setting today. Thus, we first investigated potential biases that affect the fair comparison of the accuracy of DBRD techniques. Our experiments suggest that data age and issue tracking system (ITS) choice cause a significant difference. Based on these findings, we prepared a new benchmark. We then used it to evaluate DBRD techniques to estimate better how far we have been. Surprisingly, a simpler technique outperforms recently proposed sophisticated techniques on most projects in our benchmark. In addition, we compared the DBRD techniques proposed in research with those used in Mozilla and VSCode . Surprisingly, we observe that a simple technique already adopted in practice can achieve comparable results as a recently proposed research tool. Our study gives reflections on the current state of DBRD, and we share our insights to benefit future DBRD research.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4311412320",
    "type": "article"
  },
  {
    "title": "On the Faults Found in REST APIs by Automated Test Generation",
    "doi": "https://doi.org/10.1145/3491038",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Bogdan Mărculescu; Man Zhang; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "RESTful web services are often used for building a wide variety of enterprise applications. The diversity and increased number of applications using RESTful APIs means that increasing amounts of resources are spent developing and testing these systems. Automation in test data generation provides a useful way of generating test data in a fast and efficient manner. However, automated test generation often results in large test suites that are hard to evaluate and investigate manually. This article proposes a taxonomy of the faults we have found using search-based software testing techniques applied on RESTful APIs. The taxonomy is a first step in understanding, analyzing, and ultimately fixing software faults in web services and enterprise applications. We propose to apply a density-based clustering algorithm to the test cases evolved during the search to allow a better separation between different groups of faults. This is needed to enable engineers to highlight and focus on the most serious faults. Tests were automatically generated for a set of eight case studies, seven open-source and one industrial. The test cases generated during the search are clustered based on the reported last executed line and based on the error messages returned, when such error messages were available. The tests were manually evaluated to determine their root causes and to obtain additional information. The article presents a taxonomy of the faults found based on the manual analysis of 415 faults in the eight case studies and proposes a method to support the classification using clustering of the resulting test cases.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4220791610",
    "type": "article"
  },
  {
    "title": "A Study on Blockchain Architecture Design Decisions and Their Security Attacks and Threats",
    "doi": "https://doi.org/10.1145/3502740",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Sabreen Ahmadjee; Carlos Mera‐Gómez; Rami Bahsoon; Rick Kazman",
    "corresponding_authors": "",
    "abstract": "Blockchain is a disruptive technology intended to implement secure decentralised distributed systems, in which transactional data can be shared, stored, and verified by participants of the system without needing a central authentication/verification authority. Blockchain-based systems have several architectural components and variants, which architects can leverage to build secure software systems. However, there is a lack of studies to assist architects in making architecture design and configuration decisions for blockchain-based systems. This knowledge gap may increase the chance of making unsuitable design decisions and producing configurations prone to potential security risks. To address this limitation, we report our comprehensive systematic literature review to derive a taxonomy of commonly used architecture design decisions in blockchain-based systems. We map each of these decisions to potential security attacks and their posed threats. MITRE’s attack tactic categories and Microsoft STRIDE threat modeling are used to systematically classify threats and their associated attacks to identify potential attacks and threats in blockchain-based systems. Our mapping approach aims to guide architects to make justifiable design decisions that will result in more secure implementations.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4220847066",
    "type": "article"
  },
  {
    "title": "I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts",
    "doi": "https://doi.org/10.1145/3550150",
    "publication_date": "2022-07-21",
    "publication_year": 2022,
    "authors": "Zhipeng Gao; Xin Xia; David Lo; John Grundy; Xindong Zhang; Zhenchang Xing",
    "corresponding_authors": "",
    "abstract": "Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code , that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4286494280",
    "type": "article"
  },
  {
    "title": "Exploring Better Black-Box Test Case Prioritization via Log Analysis",
    "doi": "https://doi.org/10.1145/3569932",
    "publication_date": "2022-10-28",
    "publication_year": 2022,
    "authors": "Zhichao Chen; Junjie Chen; Weijing Wang; Jianyi Zhou; Meng Wang; Xiang Chen; Shan Zhou; Jianmin Wang",
    "corresponding_authors": "",
    "abstract": "Test case prioritization (TCP) has been widely studied in regression testing, which aims to optimize the execution order of test cases so as to detect more faults earlier. TCP has been divided into white-box test case prioritization (WTCP) and black-box test case prioritization (BTCP) . WTCP can achieve better prioritization effectiveness by utilizing source code information, but is not applicable in many practical scenarios (where source code is unavailable, e.g., outsourced testing). BTCP has the benefit of not relying on source code information, but tends to be less effective than WTCP. That is, both WTCP and BTCP suffer from limitations in the practical use. To improve the practicability of TCP, we aim to explore better BTCP, significantly bridging the effectiveness gap between BTCP and WTCP. In this work, instead of statically analyzing test cases themselves in existing BTCP techniques, we conduct the first study to explore whether this goal can be achieved via log analysis. Specifically, we propose to mine test logs produced during test execution to more sufficiently reflect test behaviors, and design a new BTCP framework (called LogTCP), including log pre-processing, log representation, and test case prioritization components. Based on the LogTCP framework, we instantiate seven log-based BTCP techniques by combining different log representation strategies with different prioritization strategies. We conduct an empirical study to explore the effectiveness of LogTCP. Based on 10 diverse open-source Java projects from GitHub, we compared LogTCP with three representative BTCP techniques and four representative WTCP techniques. Our results show that all of our LogTCP techniques largely perform better than all the BTCP techniques in average fault detection, to the extent that they become competitive to the WTCP techniques. That demonstrates the great potential of logs in practical TCP.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4307510928",
    "type": "article"
  },
  {
    "title": "An Accurate Identifier Renaming Prediction and Suggestion Approach",
    "doi": "https://doi.org/10.1145/3603109",
    "publication_date": "2023-05-29",
    "publication_year": 2023,
    "authors": "Jingxuan Zhang; Junpeng Luo; Jiahui Liang; Lina Gong; Zhiqiu Huang",
    "corresponding_authors": "",
    "abstract": "Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4378651134",
    "type": "article"
  },
  {
    "title": "How Important Are Good Method Names in Neural Code Generation? A Model Robustness Perspective",
    "doi": "https://doi.org/10.1145/3630010",
    "publication_date": "2023-10-23",
    "publication_year": 2023,
    "authors": "Guang Yang; Yu Zhou; Wenhua Yang; Tao Yue; Xiang Chen; Taolue Chen",
    "corresponding_authors": "",
    "abstract": "Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neu RA l co D e gener A tor R obustifier (RADAR). RADAR consists of two components: RADAR -Attack and RADAR -Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR -Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR -Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR -Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4387869031",
    "type": "article"
  },
  {
    "title": "Poison Attack and Poison Detection on Deep Source Code Processing Models",
    "doi": "https://doi.org/10.1145/3630008",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Jia Li; Zhuo Li; Huangzhao Zhang; Ge Li; Zhi Jin; Xing Hu; Xin Xia",
    "corresponding_authors": "",
    "abstract": "In the software engineering (SE) community, deep learning (DL) has recently been applied to many source code processing tasks, achieving state-of-the-art results. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat to DL models, namely, poison attacks . The attackers aim to inject insidious backdoors into DL models by poisoning the training data with poison samples. The backdoors mean that poisoned models work normally with clean inputs but produce targeted erroneous results with inputs embedded with specific triggers. By using triggers to activate backdoors, attackers can manipulate poisoned models in security-related scenarios (e.g., defect detection) and lead to severe consequences. To verify the vulnerability of deep source code processing models to poison attacks, we present a poison attack approach for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable and functionality-preserving poison samples and effectively attack deep source code processing models by poisoning the training data with poison samples. To defend against poison attacks, we further propose an effective poison detection approach named CodeDetector . CodeDetector can automatically identify poison samples in the training data. We apply CodePoisoner and CodeDetector to six deep source code processing models, including defect detection, clone detection, and code repair models. The results show that ❶ CodePoisoner conducts successful poison attacks with a high attack success rate (average: 98.3%, maximum: 100%). It validates that existing deep source code processing models have a strong vulnerability to poison attacks. ❷ CodeDetector effectively defends against multiple poison attack approaches by detecting (maximum: 100%) poison samples in the training data. We hope this work can help SE researchers and practitioners notice poison attacks and inspire the design of more advanced defense techniques.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4388229767",
    "type": "article"
  },
  {
    "title": "Testing of Deep Reinforcement Learning Agents with Surrogate Models",
    "doi": "https://doi.org/10.1145/3631970",
    "publication_date": "2023-11-11",
    "publication_year": 2023,
    "authors": "Matteo Biagiola; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "Deep Reinforcement Learning (DRL) has received a lot of attention from the research community in recent years. As the technology moves away from game playing to practical contexts, such as autonomous vehicles and robotics, it is crucial to evaluate the quality of DRL agents. In this article, we propose a search-based approach to test such agents. Our approach, implemented in a tool called Indago , trains a classifier on failure and non-failure environment (i.e., pass) configurations resulting from the DRL training process. The classifier is used at testing time as a surrogate model for the DRL agent execution in the environment, predicting the extent to which a given environment configuration induces a failure of the DRL agent under test. The failure prediction acts as a fitness function, guiding the generation towards failure environment configurations, while saving computation time by deferring the execution of the DRL agent in the environment to those configurations that are more likely to expose failures. Experimental results show that our search-based approach finds 50% more failures of the DRL agent than state-of-the-art techniques. Moreover, such failures are, on average, 78% more diverse; similarly, the behaviors of the DRL agent induced by failure configurations are 74% more diverse.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4388596098",
    "type": "article"
  },
  {
    "title": "Vision Transformer Inspired Automated Vulnerability Repair",
    "doi": "https://doi.org/10.1145/3632746",
    "publication_date": "2023-11-13",
    "publication_year": 2023,
    "authors": "Michael C. Fu; Van Nguyen; Chakkrit Tantithamthavorn; Dinh Phung; Trung Le",
    "corresponding_authors": "",
    "abstract": "Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders’ cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders’ self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68% to 32.33%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4388638461",
    "type": "article"
  },
  {
    "title": "Actor-Driven Decomposition of Microservices through Multi-level Scalability Assessment",
    "doi": "https://doi.org/10.1145/3583563",
    "publication_date": "2023-02-15",
    "publication_year": 2023,
    "authors": "Matteo Camilli; Carmine Colarusso; Barbara Russo; Eugenio Zimeo",
    "corresponding_authors": "",
    "abstract": "The microservices architectural style has gained widespread acceptance. However, designing applications according to this style is still challenging. Common difficulties concern finding clear boundaries that guide decomposition while ensuring performance and scalability. With the aim of providing software architects and engineers with a systematic methodology, we introduce a novel actor-driven decomposition strategy to complement the domain-driven design and overcome some of its limitations by reaching a finer modularization yet enforcing performance and scalability improvements. The methodology uses a multi-level scalability assessment framework that supports decision-making over iterative steps. At each iteration, architecture alternatives are quantitatively evaluated at multiple granularity levels. The assessment helps architects to understand the extent to which architecture alternatives increase or decrease performance and scalability. We applied the methodology to drive further decomposition of the core microservices of a real data-intensive smart mobility application and an existing open-source benchmark in the e-commerce domain. The results of an in-depth evaluation show that the approach can effectively support engineers in ( i ) decomposing monoliths or coarse-grained microservices into more scalable microservices and ( ii ) comparing among alternative architectures to guide decision-making for their deployment in modern infrastructures that orchestrate lightweight virtualized execution units.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4320890541",
    "type": "article"
  },
  {
    "title": "GraphPrior: Mutation-based Test Input Prioritization for Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3607191",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Xueqi Dang; Yinghua Li; Mike Papadakis; Jacques Klein; Tegawendé F. Bissyandé; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in a variety of practical applications. Similar to traditional DNNs, GNNs could exhibit incorrect behavior that may lead to severe consequences, and thus testing is necessary and crucial. However, labeling all the test inputs for GNNs can be costly and time-consuming, especially when dealing with large and complex graphs, which seriously affects the efficiency of GNN testing. Existing studies have focused on test prioritization for DNNs, which aims to identify and prioritize fault-revealing tests (i.e., test inputs that are more likely to be misclassified) to detect system bugs earlier in a limited time. Although some DNN prioritization approaches have been demonstrated effective, there is a significant problem when applying them to GNNs: They do not take into account the connections (edges) between GNN test inputs (nodes), which play a significant role in GNN inference. In general, DNN test inputs are independent of each other, while GNN test inputs are usually represented as a graph with complex relationships between each test. In this article, we propose GraphPrior ( GNN -oriented Test Prior itization), a set of approaches to prioritize test inputs specifically for GNNs via mutation analysis. Inspired by mutation testing in traditional software engineering, in which test suites are evaluated based on the mutants they kill, GraphPrior generates mutated models for GNNs and regards test inputs that kill many mutated models as more likely to be misclassified. Then, GraphPrior leverages the mutation results in two ways, killing-based and feature-based methods. When scoring a test input, the killing-based method considers each mutated model equally important, while feature-based methods learn different importance for each mutated model through ranking models. Finally, GraphPrior ranks all the test inputs based on their scores. We conducted an extensive study based on 604 subjects to evaluate GraphPrior on both natural and adversarial test inputs. The results demonstrate that KMGP, the killing-based GraphPrior approach, outperforms the compared approaches in a majority of cases, with an average improvement of 4.76% ~49.60% in terms of APFD. Furthermore, the feature-based GraphPrior approach, RFGP, performs the best among all the GraphPrior approaches. On adversarial test inputs, RFGP outperforms the compared approaches across different adversarial attacks, with the average improvement of 2.95% ~46.69%.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4383102502",
    "type": "article"
  },
  {
    "title": "Non-Autoregressive Line-Level Code Completion",
    "doi": "https://doi.org/10.1145/3649594",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Fang Liu; Zhiyi Fu; Ge Li; Zhi Jin; Hui Liu; Yiyang Hao; Li Zhang",
    "corresponding_authors": "",
    "abstract": "Software developers frequently use code completion tools to accelerate software development by suggesting the following code elements. Researchers usually employ AutoRegressive (AR) decoders to complete code sequences in a left-to-right, token-by-token fashion. To improve the accuracy and efficiency of code completion, we argue that tokens within a code statement have the potential to be predicted concurrently. In this article, we first conduct an empirical study to analyze the dependency among the target tokens in line-level code completion. The results suggest that it is potentially practical to generate all statement tokens in parallel. To this end, we introduce SANAR, a simple and effective syntax-aware non-autoregressive model for line-level code completion. To further improve the quality of the generated code, we propose an adaptive and syntax-aware sampling strategy to boost the model’s performance. The experimental results obtained from two widely used datasets indicate that our model outperforms state-of-the-art code completion approaches of similar model size by a considerable margin, and is faster than these models with up to 9× speed-up. Moreover, the extensive results additionally demonstrate that the enhancements achieved by SANAR become even more pronounced with larger model sizes, highlighting their significance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4392163075",
    "type": "article"
  },
  {
    "title": "Lessons Learned from Developing a Sustainability Awareness Framework for Software Engineering Using Design Science",
    "doi": "https://doi.org/10.1145/3649597",
    "publication_date": "2024-03-08",
    "publication_year": 2024,
    "authors": "Stefanie Betz; Birgit Penzenstadler; Letícia Duboc; Ruzanna Chitchyan; Sedef Akinli Koçak; Ian Brooks; Shola Oyedeji; Jari Porras; Norbert Seyff; Colin C. Venters",
    "corresponding_authors": "",
    "abstract": "To foster a sustainable society within a sustainable environment, we must dramatically reshape our work and consumption activities, most of which are facilitated through software. Yet, most software engineers hardly consider the effects on the sustainability of the IT products and services they deliver. This issue is exacerbated by a lack of methods and tools for this purpose. Despite the practical need for methods and tools that explicitly support consideration of the effects that IT products and services have on the sustainability of their intended environments, such methods and tools remain largely unavailable. Thus, urgent research is needed to understand how to design such tools for the IT community properly. In this article, we describe our experience using design science to create the Sustainability Awareness Framework (SusAF), which supports software engineers in anticipating and mitigating the potential sustainability effects during system development. More specifically, we identify and present the challenges faced during this process. The challenges that we have faced and addressed in the development of the SusAF are likely to be relevant to others who aim to create methods and tools to integrate sustainability analysis into their IT products and services development. Thus, the lessons learned in SusAF development are shared for the benefit of researchers and other professionals who design tools for that end.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4392593558",
    "type": "article"
  },
  {
    "title": "A Novel Refactoring and Semantic Aware Abstract Syntax Tree Differencing Tool and a Benchmark for Evaluating the Accuracy of Diff Tools",
    "doi": "https://doi.org/10.1145/3696002",
    "publication_date": "2024-09-12",
    "publication_year": 2024,
    "authors": "Pouria Alikhanifard; Nikolaos Tsantalis",
    "corresponding_authors": "",
    "abstract": "Software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. Thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. Abstract Syntax Tree (AST) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. Despite the notable improvements brought by AST diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible AST nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. We propose a novel AST diff tool based on RefactoringMiner that resolves all aforementioned limitations. First, we improved RefactoringMiner to increase its statement mapping accuracy, and then we developed an algorithm that generates AST diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by RefactoringMiner. To evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first benchmark of AST node mappings, including 800 bug-fixing commits and 188 refactoring commits. Our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4402483878",
    "type": "article"
  },
  {
    "title": "Diversity’s Double-Edged Sword: Analyzing Race’s Effect on Remote Pair Programming Interactions",
    "doi": "https://doi.org/10.1145/3699601",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Shandler A. Mason; Sandeep Kaur Kuttal",
    "corresponding_authors": "",
    "abstract": "Remote pair programming is widely used in software development, but no research has examined how race affects these interactions between developers. We embarked on this study due to the historical under representation of Black developers in the tech industry, with White developers comprising the majority. Our study involved 24 experienced developers, forming 12 gender-balanced same- and mixed-race pairs. Pairs collaborated on a programming task using the think-aloud method, followed by individual retrospective interviews. Our findings revealed elevated productivity scores for mixed-race pairs, with no differences in code quality between same- and mixed-race pairs. Mixed-race pairs excelled in task distribution, shared decision-making, and role-exchange but encountered communication challenges, discomfort, and anxiety, shedding light on the complexity of diversity dynamics. Our study emphasizes race’s impact on remote pair programming and underscores the need for diverse tools and methods to address racial disparities for collaboration.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4403185898",
    "type": "article"
  },
  {
    "title": "Identifying and Explaining Safety-critical Scenarios for Autonomous Vehicles via Key Features",
    "doi": "https://doi.org/10.1145/3640335",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Neelofar Neelofar; Aldeida Aleti",
    "corresponding_authors": "",
    "abstract": "Ensuring the safety of autonomous vehicles (AVs) is of utmost importance, and testing them in simulated environments is a safer option than conducting in-field operational tests. However, generating an exhaustive test suite to identify critical test scenarios is computationally expensive, as the representation of each test is complex and contains various dynamic and static features, such as the AV under test, road participants (vehicles, pedestrians, and static obstacles), environmental factors (weather and light), and the road’s structural features (lanes, turns, road speed, etc.). In this article, we present a systematic technique that uses Instance Space Analysis (ISA) to identify the significant features of test scenarios that affect their ability to reveal the unsafe behaviour of AVs. ISA identifies the features that best differentiate safety-critical scenarios from normal driving and visualises the impact of these features on test scenario outcomes (safe/unsafe) in two dimensions. This visualisation helps to identify untested regions of the instance space and provides an indicator of the quality of the test suite in terms of the percentage of feature space covered by testing. To test the predictive ability of the identified features, we train five Machine Learning classifiers to classify test scenarios as safe or unsafe. The high precision, recall, and F1 scores indicate that our proposed approach is effective in predicting the outcome of a test scenario without executing it and can be used for test generation, selection, and prioritisation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4390751597",
    "type": "article"
  },
  {
    "title": "Industry Practices for Challenging Autonomous Driving Systems with Critical Scenarios",
    "doi": "https://doi.org/10.1145/3640334",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Qunying Song; Emelie Engström; Per Runeson",
    "corresponding_authors": "",
    "abstract": "Testing autonomous driving systems for safety and reliability is essential, yet complex. A primary challenge is identifying relevant test scenarios, especially the critical ones that may expose hazards or harm to autonomous vehicles and other road users. Although numerous approaches and tools for critical scenario identification are proposed, the industry practices for selection, implementation, and evaluation of approaches, are not well understood. Therefore, we aim at exploring practical aspects of how autonomous driving systems are tested, particularly the identification and use of critical scenarios. We interviewed 13 practitioners from 7 companies in autonomous driving in Sweden. We used thematic modeling to analyse and synthesize the interview data. As a result, we present 9 themes of practices and 4 themes of challenges related to critical scenarios. Our analysis indicates there is little joint effort in the industry, despite every approach has its own limitations, and tools and platforms are lacking. To that end, we recommend the industry and academia combine different approaches, collaborate among different stakeholders, and continuously learn the field. The contributions of our study are exploration and synthesis of industry practices and related challenges for critical scenario identification and testing, and potential increase of industry relevance for future studies.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4390751909",
    "type": "article"
  },
  {
    "title": "Bug Analysis in Jupyter Notebook Projects: An Empirical Study",
    "doi": "https://doi.org/10.1145/3641539",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Taijara Loiola de Santana; Paulo Anselmo da Mota Silveira Neto; Eduardo Santana de Almeida; Iftekhar Ahmed",
    "corresponding_authors": "",
    "abstract": "Computational notebooks, such as Jupyter, have been widely adopted by data scientists to write code for analyzing and visualizing data. Despite their growing adoption and popularity, few studies have been found to understand Jupyter development challenges from the practitioners’ point of view. This article presents a systematic study of bugs and challenges that Jupyter practitioners face through a large-scale empirical investigation. We mined 14,740 commits from 105 GitHub open source projects with Jupyter Notebook code. Next, we analyzed 30,416 StackOverflow posts, which gave us insights into bugs that practitioners face when developing Jupyter Notebook projects. Next, we conducted 19 interviews with data scientists to uncover more details about Jupyter bugs and to gain insight into Jupyter developers’ challenges. Finally, to validate the study results and proposed taxonomy, we conducted a survey with 91 data scientists. We highlight bug categories, their root causes, and the challenges that Jupyter practitioners face.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4391103900",
    "type": "article"
  },
  {
    "title": "Test Input Prioritization for 3D Point Clouds",
    "doi": "https://doi.org/10.1145/3643676",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Yinghua Li; Xueqi Dang; Lei Ma; Jacques Klein; Yves Le Traon; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "3D point cloud applications have become increasingly prevalent in diverse domains, showcasing their efficacy in various software systems. However, testing such applications presents unique challenges due to the high-dimensional nature of 3D point cloud data and the vast number of possible test cases. Test input prioritization has emerged as a promising approach to enhance testing efficiency by prioritizing potentially misclassified test cases during the early stages of the testing process. Consequently, this enables the early labeling of critical inputs, leading to a reduction in the overall labeling cost. However, applying existing prioritization methods to 3D point cloud data is constrained by several factors: (1) inadequate consideration of crucial spatial information, and (2) susceptibility to noises inherent in 3D point cloud data. In this article, we propose PCPrior , the first test prioritization approach specifically designed for 3D point cloud test cases. The fundamental concept behind PCPrior is that test inputs closer to the decision boundary of the model are more likely to be predicted incorrectly. To capture the spatial relationship between a point cloud test and the decision boundary, we propose transforming each test (a point cloud) into a low-dimensional feature vector, toward indirectly revealing the underlying proximity between a test and the decision boundary. To achieve this, we carefully design a group of feature generation strategies, and for each test input, we generate four distinct types of features, namely spatial features, mutation features, prediction features, and uncertainty features. Through a concatenation of the four feature types, PCPrior assembles a final feature vector for each test. Subsequently, a ranking model is employed to estimate the probability of misclassification for each test based on its feature vector. Finally, PCPrior ranks all tests based on their misclassification probabilities. We conducted an extensive study based on 165 subjects to evaluate the performance of PCPrior, encompassing both natural and noisy datasets. The results demonstrate that PCPrior outperforms all of the compared test prioritization approaches, with an average improvement of 10.99% to 66.94% on natural datasets and 16.62% to 53% on noisy datasets.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4391278020",
    "type": "article"
  },
  {
    "title": "Meta-Learning for Multi-Family Android Malware Classification",
    "doi": "https://doi.org/10.1145/3664806",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Yao Li; Dawei Yuan; Tao Zhang; Haipeng Cai; David Lo; Cuiyun Gao; Xiapu Luo; He Jiang",
    "corresponding_authors": "",
    "abstract": "With the emergence of smartphones, Android has become a widely used mobile operating system. However, it is vulnerable when encountering various types of attacks. Every day, new malware threatens the security of users’ devices and private data. Many methods have been proposed to classify malicious applications, utilizing static or dynamic analysis for classification. However, previous methods still suffer from unsatisfactory performance due to two challenges. First, they are unable to address the imbalanced data distribution problem, leading to poor performance for malware families with few members. Second, they are unable to address the zero-day malware (zero-day malware refers to malicious applications that exploit unknown vulnerabilities) classification problem. In this article, we introduce an innovative meta -learning approach for m ulti-family A ndroid m alware c lassification named Meta-MAMC , which uses meta-learning technology to learn meta-knowledge (i.e., the similarities and differences among different malware families) of few-family samples and combines new sampling algorithms to solve the above challenges. Meta-MAMC integrates (i) the meta-knowledge contained within the dataset to guide models in learning to identify unknown malware; and (ii) more accurate and diverse tasks based on novel sampling strategies, as well as directly adapting meta-learning to a new few-sample and zero-sample task to classify families. We have evaluated Meta-MAMC on two popular datasets and a corpus of real-world Android applications. The results demonstrate its efficacy in accurately classifying malicious applications belonging to certain malware families, even achieving 100% classification in some families.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4396860941",
    "type": "article"
  },
  {
    "title": "Automatically Recommend Code Updates: Are We There Yet?",
    "doi": "https://doi.org/10.1145/3678167",
    "publication_date": "2024-07-16",
    "publication_year": 2024,
    "authors": "Yue Liu; Chakkrit Tantithamthavorn; Yonghui Liu; Patanamon Thongtanunam; Li Li",
    "corresponding_authors": "",
    "abstract": "In recent years, large pre-trained Language Models of Code (CodeLMs) have shown promising results on various software engineering tasks. One such task is automatic code update recommendation, which transforms outdated code snippets into their approved and revised counterparts. Although many CodeLM-based approaches have been proposed, claiming high accuracy, their effectiveness and reliability on real-world code update tasks remain questionable. In this paper, we present the first extensive evaluation of state-of-the-art CodeLMs for automatically recommending code updates. We assess their performance on two diverse datasets of paired updated methods, considering factors such as temporal evolution, project specificity, method size, and update complexity. Our results reveal that while CodeLMs exhibit higher performance in settings that ignore temporal information, they struggle in more realistic time-wise scenarios and generalize poorly to new projects. Furthermore, CodeLM performance decreases significantly for larger methods and more complex updates. Furthermore, we observe that many CodeLM-generated “updates” are actually null, especially in time-wise settings, and meaningful edits remain challenging. Our findings highlight the significant gap between the perceived and actual effectiveness of CodeLMs for real-world code update recommendation and emphasize the need for more research on improving their practicality, robustness, and generalizability.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4400690920",
    "type": "article"
  },
  {
    "title": "The Desert environment",
    "doi": "https://doi.org/10.1145/322993.322994",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "Steven P. Reiss",
    "corresponding_authors": "Steven P. Reiss",
    "abstract": "The Desert software engineering environment is a suite of tools developed to enhance programmer productivity through increased tool integration. It introduces an inexpensive form of data integration to provide additional tool capabilities and information sharing among tools, uses a common editor to give high-quality semantic feedback and to integrate different types of software artifacts, and builds virtual files on demand to address specific tasks. All this is done in an open and extensible environment capable of handling large software systems.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2041490592",
    "type": "article"
  },
  {
    "title": "SMC",
    "doi": "https://doi.org/10.1145/350887.350891",
    "publication_date": "2000-04-01",
    "publication_year": 2000,
    "authors": "A. Prasad Sistla; Viktor Gyuris; E. Allen Emerson",
    "corresponding_authors": "",
    "abstract": "The article presents the SMC system. SMC can be used for checking safety and liveness properties of concurrent programs under different fairness assumptions. It is based on explicit state enumeration. It combats the state explosion by exploiting symmetries of the input concurrent program, usually present in the form of identical processes, in two different ways. Firstly, it reduces the number of explored states by identifying those states that are equivalent under the symmetries of the system; this is called process symmetry . Secondly, it reduces the number of edges explored from each state, in0 the reduced state graph, by exploiting the symmetry of a single state; this is called state symmetry . SMC works in an on-the-fly manner; it constructs the reduced state graph as and when it is needed. This method facilitates early termination, speeds up model checking, and reduces memory requirements. We employed SMC to check the correctness of, among other standard examples, the Link Layer part of the IEEE Standard 1394 “Firewire” high-speed serial bus protocol. SMC found deadlocks in the protocol. SMC was also to check certain liveness properties. A report on the case study is included in the article.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2005836640",
    "type": "article"
  },
  {
    "title": "Estimation of software reliability by stratified sampling",
    "doi": "https://doi.org/10.1145/310663.310667",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Andy Podgurski; Wassim A. Masri; Yolanda McCleese; Francis Wolff; Charles Yang",
    "corresponding_authors": "",
    "abstract": "A new approach to software reliability estimation is presented that combines operational testing with stratified sampling in order to reduce the number of program executions that must be checked manually for conformance to requirements. Automatic cluster analysis is applied to execution profiles in order to stratify captured operational executions. Experimental results are reported that suggest this approach can significantly reduce the cost of estimating reliability.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2123218263",
    "type": "article"
  },
  {
    "title": "A simplified domain-testing strategy",
    "doi": "https://doi.org/10.1145/196092.193171",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Bingchiang Jeng; Elaine J. Weyuker",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A simplified domain-testing strategy Authors: Bingchiang Jeng Sun Yat-Sen University Sun Yat-Sen UniversityView Profile , Elaine J. Weyuker New York University New York UniversityView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 3Issue 3July 1994 pp 254–270https://doi.org/10.1145/196092.193171Published:01 July 1994Publication History 49citation1,628DownloadsMetricsTotal Citations49Total Downloads1,628Last 12 Months60Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W1987187562",
    "type": "article"
  },
  {
    "title": "Unified versioning through feature logic",
    "doi": "https://doi.org/10.1145/261640.261654",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "Andreas Zeller; Gregor Snelting",
    "corresponding_authors": "",
    "abstract": "Software configuration management (SCM) suffers from tight coupling between SCM version-ing models and the imposed SCM processes. In order to adapt SCM tools to SCM processes, rather than vice versa, we propose a unified versioning model, the version set model . Version sets denote versions, components, and configurations by feature terms , that is, Boolean terms over ( feature : value )-attributions. Through feature logic , we deduce consistency of abstract configurations as well as features of derived components and describe how features propagate in the SCM process; using feature implications , we integrate change-oriented and version-oriented SCM models. We have implemented the version set model in an SCM system called ICE, for Incremental Configuration Environment . ICE is based on a featured file system (FFS) , where version sets are accessed as virtual files and directories. Using the well-known C preprocessor (CPP) representation, users can view and edit multiple versions simultaneously, while only the differences between versions are stored. It turns out that all major SCM models can be realized and integrated efficiently on top of the FFS, demonstrating the flexible and unifying nature of the version set model.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2115528929",
    "type": "article"
  },
  {
    "title": "A framework for modeling and implementing visual notations with applications to software engineering",
    "doi": "https://doi.org/10.1145/1040291.1040293",
    "publication_date": "2004-10-01",
    "publication_year": 2004,
    "authors": "Gennaro Costagliola; Vincenzo Deufemia; Giuseppe Polese",
    "corresponding_authors": "",
    "abstract": "We present a framework for modeling visual notations and for generating the corresponding visual programming environments. The framework can be used for modeling the diagrammatic notations of software development methodologies, and to generate visual programming environments with CASE tools functionalities. This is accomplished through an underlying modeling process based on the visual notation syntactic model of eXtended Positional Grammars (XPG, for short), and the associated parsing methodology, XpLR. In particular, the process requires the modeling of the basic elements (visual symbols) of a visual notation, their syntactic properties, the relations between them, the syntactic rules to formally define the set of feasible visual sentences, and a set of semantic routines performing additional checks and translation tasks. Such a process is completely supported by the VLDesk system, which enables the automatic generation of an editor for drawing visual sentences, as well as a processor for their recognition, parsing, and translation into other notations.The proposed framework also provides the basis for the definition of a meta-CASE technology. In fact, we can customize the generated visual programming environment in terms of the supported visual notation, its syntactic properties, and the translation rules. We have used this framework to model several diagrammatic notations used in software development methodologies, including those of the Unified Modeling Language.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W1968966764",
    "type": "article"
  },
  {
    "title": "The <i>Pan</i> language-based editing system",
    "doi": "https://doi.org/10.1145/125489.122804",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Robert A. Ballance; Susan L. Graham; Michael L. Van De Vanter",
    "corresponding_authors": "",
    "abstract": "Powerful editing systems for developing complex software documents are difficult to engineer. Besides requiring efficient incremental algorithms and complex data structures, such editors must accommodate flexible editing styles, provide a consistent, coherent, and powerful user interface, support individual variations and projectwide configurations, maintain a sharable database of information concerning the documents being edited, and integrate smoothly with the other tools in the environment. Pan is a language-based editing and browsing system that exhibits these characteristics. This paper surveys the design and engineering of Pan , paying particular attention to a number of issues that pervade the system: incremental checking and analysis, information retention in the presence of change, tolerance for errors and anomalies, and extension facilities.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2148321974",
    "type": "article"
  },
  {
    "title": "Automated abstraction of class diagrams",
    "doi": "https://doi.org/10.1145/606612.606616",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Alexander Egyed",
    "corresponding_authors": "Alexander Egyed",
    "abstract": "Designers can easily become overwhelmed with details when dealing with large class diagrams. This article presents an approach for automated abstraction that allows designers to \"zoom out\" on class diagrams to investigate and reason about their bigger picture. The approach is based on a large number of abstraction rules that individually are not very powerful, but when used together, can abstract complex class structures quickly. This article presents those abstraction rules and an algorithm for applying them. The technique was validated on over a dozen models where it was shown to be well suited for model understanding, consistency checking, and reverse engineering.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2008435703",
    "type": "article"
  },
  {
    "title": "Term rewriting with traversal functions",
    "doi": "https://doi.org/10.1145/941566.941568",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Mark van den Brand; Paul Klint; Jurgen Vinju",
    "corresponding_authors": "",
    "abstract": "Term rewriting is an appealing technique for performing program analysis and program transformation. Tree (term) traversal is frequently used but is not supported by standard term rewriting. We extend many-sorted, first-order term rewriting with traversal functions that automate tree traversal in a simple and type-safe way. Traversal functions can be bottom-up or top-down traversals and can either traverse all nodes in a tree or can stop the traversal at a certain depth as soon as a matching node is found. They can either define sort-preserving transformations or mappings to a fixed sort. We give small and somewhat larger examples of traversal functions and describe their operational semantics and implementation. An assessment of various applications and a discussion conclude the article.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2056351581",
    "type": "article"
  },
  {
    "title": "Object-oriented logical specification of time-critical systems",
    "doi": "https://doi.org/10.1145/174634.174636",
    "publication_date": "1994-01-02",
    "publication_year": 1994,
    "authors": "Angelo Morzenti; Pierluigi San Pietro",
    "corresponding_authors": "",
    "abstract": "We define TRIO + , an object-oriented logical language for modular system specification. TRIO + is based on TRIO, a first-order temporal language that is well suited to the specification of embedded and real-time systems, and that provides an effective support to a variety of validation activities, like specification testing, simulation, and property proof. Unfortunately, TRIO lacks the ability to construct specifications of complex systems in a systematic and modular way. TRIO + combines the use of constructs for hierarchical system decomposition and object-oriented concepts like inheritance and genericity with an expressive and intuitive graphic notation, yielding a specification language that is formal and rigorous, yet still flexible, readable, general, and easily adaptable to the user's needs. After introducing and motivating the main features of the language, we illustrate its application to a nontrivial case study extracted from a real-life industrial application.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W1988608457",
    "type": "article"
  },
  {
    "title": "A slicing-based approach for locating type errors",
    "doi": "https://doi.org/10.1145/366378.366379",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Frank Tip; T. B. Dinesh",
    "corresponding_authors": "",
    "abstract": "The effectiveness of a type-checking tool strongly depends on the accuracy of the positional information that is associated with type errors. We present an approach where the location associated with an error message e is defined as a slice P e of the program P being type-checked. We show that this approach yields highly accurate positional information: P e is a program that contains precisely those program constructs in P that caused error e . Semantically, we have the interesting property that type-checking P e is guaranteed to produce the same error e . Our approach is completely language-independent and has been implemented for a significant subset of Pascal. We also report on experiments with object-oriented type systems, and with a subset of ML.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2029079408",
    "type": "article"
  },
  {
    "title": "A program integration algorithm that accommodates semantics-preserving transformations",
    "doi": "https://doi.org/10.1145/131736.131756",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Wuu Yang; Susan Horwitz; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "Given a program Base and two variants, A and B , each created by modifying separate copies of Base , the goal of program integration is to determine whether the modifications interfere, and if they do not, to create an integrated program that includes both sets of changes as well as the portions of Base preserved in both variants. Text-based integration techniques, such as the one used by the Unix diff 3 utility, are obviously unsatisfactory because one has no guarantees about how the execution behavior of the integrated program relates to the behaviors of Base , A , and B . The first program-integration algorithm to provide such guarantees was developed by Horwitz et al.[13]. However, a limitation of that algorithm is that it incorporates no notion of semantics-preserving transformations. This limitation causes the algorithm to be overly conservative in its definition of interference. For example, if one variant changes the way a computation is performed (without changing the values computed) while the other variant adds code that uses the result of the computation, the algorithm would classify those changes as interfering. This paper describes a new integration algorithm that is able to accommodate semantics-preserving transformations.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2109452325",
    "type": "article"
  },
  {
    "title": "Empirical evaluation of a nesting testability transformation for evolutionary testing",
    "doi": "https://doi.org/10.1145/1525880.1525884",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Phil McMinn; David Binkley; Mark Harman",
    "corresponding_authors": "",
    "abstract": "Evolutionary testing is an approach to automating test data generation that uses an evolutionary algorithm to search a test object's input domain for test data. Nested predicates can cause problems for evolutionary testing, because information needed for guiding the search only becomes available as each nested conditional is satisfied. This means that the search process can overfit to early information, making it harder, and sometimes near impossible, to satisfy constraints that only become apparent later in the search. The article presents a testability transformation that allows the evaluation of all nested conditionals at once. Two empirical studies are presented. The first study shows that the form of nesting handled is prevalent in practice. The second study shows how the approach improves evolutionary test data generation.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2085553299",
    "type": "article"
  },
  {
    "title": "Measuring the strength of information flows in programs",
    "doi": "https://doi.org/10.1145/1571629.1571631",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Wes Masri; Andy Podgurski",
    "corresponding_authors": "",
    "abstract": "Dynamic information flow analysis (DIFA) was devised to enable the flow of information among variables in an executing program to be monitored and possibly regulated. It is related to techniques like dynamic slicing and dynamic impact analysis . To better understand the basis for DIFA, we conducted an empirical study in which we measured the strength of information flows identified by DIFA, using information theoretic and correlation-based methods. The results indicate that in most cases the occurrence of a chain of dynamic program dependences between two variables does not indicate a measurable information flow between them. We also explored the relationship between the strength of an information flow and the length of the corresponding dependence chain, and we obtained results indicating that no consistent relationship exists between the length of an information flow and its strength. Finally, we investigated whether data dependence and control dependence makes equal or unequal contributions to flow strength. The results indicate that flows due to data dependences alone are stronger, on average, than flows due to control dependences alone. We present the details of our study and consider the implications of the results for applications of DIFA and related techniques.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1989081620",
    "type": "article"
  },
  {
    "title": "Conceptual data model-based software size estimation for information systems",
    "doi": "https://doi.org/10.1145/1571629.1571630",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Hee Beng Kuan Tan; Yuan Zhao; Hongyu Zhang",
    "corresponding_authors": "",
    "abstract": "Size estimation plays a key role in effort estimation that has a crucial impact on software projects in the software industry. Some information required by existing software sizing methods is difficult to predict in the early stage of software development. A conceptual data model is widely used in the early stage of requirements analysis for information systems. Lines of code (LOC) is a commonly used software size measure. This article proposes a novel LOC estimation method for information systems from their conceptual data models through using a multiple linear regression model. We have validated the proposed method using samples from both the software industry and open-source systems.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2078490331",
    "type": "article"
  },
  {
    "title": "Bounded satisfiability checking of metric temporal logic specifications",
    "doi": "https://doi.org/10.1145/2491509.2491514",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Matteo Pradella; Angelo Morzenti; Pierluigi San Pietro",
    "corresponding_authors": "",
    "abstract": "We introduce bounded satisfiability checking, a verification technique that extends bounded model checking by allowing also the analysis of a descriptive model , consisting of temporal logic formulae, instead of the more customary operational model , consisting of a state transition system. We define techniques for encoding temporal logic formulae into Boolean logic that support the use of bi-infinite time domain and of metric time operators. In the framework of bounded satisfiability checking, we show how a descriptive model can be refined into an operational one, and how the correctness of such a refinement can be verified for the bounded case, setting the stage for a stepwise system development method based on a bounded model refinement. Finally, we show how the adoption of a modular approach can make the bounded refinement process more manageable and efficient. All introduced concepts are extensively applied to a set of case studies, and thoroughly experimented through Zot, our SAT solver-based verification toolset.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1971998059",
    "type": "article"
  },
  {
    "title": "The value of design rationale information",
    "doi": "https://doi.org/10.1145/2491509.2491515",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Davide Falessi; Lionel Briand; Giovanni Cantone; Rafael Capilla; Philippe Kruchten",
    "corresponding_authors": "",
    "abstract": "A complete and detailed (full) Design Rationale Documentation (DRD) could support many software development activities, such as an impact analysis or a major redesign. However, this is typically too onerous for systematic industrial use as it is not cost effective to write, maintain, or read. The key idea investigated in this article is that DRD should be developed only to the extent required to support activities particularly difficult to execute or in need of significant improvement in a particular context. The aim of this article is to empirically investigate the customization of the DRD by documenting only the information items that will probably be required for executing an activity. This customization strategy relies on the hypothesis that the value of a specific DRD information item depends on its category (e.g., assumptions, related requirements, etc.) and on the activity it is meant to support. We investigate this hypothesis through two controlled experiments involving a total of 75 master students as experimental subjects. Results show that the value of a DRD information item significantly depends on its category and, within a given category, on the activity it supports. Furthermore, on average among activities, documenting only the information items that have been required at least half of the time (i.e., the information that will probably be required in the future) leads to a customized DRD containing about half the information items of a full documentation. We expect that such a significant reduction in DRD information should mitigate the effects of some inhibitors that currently prevent practitioners from documenting design decision rationale.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2018414565",
    "type": "article"
  },
  {
    "title": "Finite satisfiability of UML class diagrams with constrained class hierarchy",
    "doi": "https://doi.org/10.1145/2491509.2491518",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Mira Balaban; Azzam Maraee",
    "corresponding_authors": "",
    "abstract": "Models lie at the heart of the emerging model-driven engineering approach. In order to guarantee precise, consistent, and correct models, there is a need for efficient powerful methods for verifying model correctness. Class diagram is the central language within UML. Its correctness problems involve issues of contradiction, namely the consistency problem, and issues of finite instantiation, namely the finite satisfiability problem. This article analyzes the problem of finite satisfiability of class diagrams with class hierarchy constraints and generalization-set constraints. The article introduces the FiniteSat algorithm for efficient detection of finite satisfiability in such class diagrams, and analyzes its limitations in terms of complex hierarchy structures. FiniteSat is strengthened in two directions. First, an algorithm for identification of the cause for a finite satisfiability problem is introduced. Second, a method for propagation of generalization-set constraints in a class diagram is introduced. The propagation method serves as a preprocessing step that improves FiniteSat performance, and helps developers in clarifying intended constraints. These algorithms are implemented in the FiniteSatUSE tool [BGU Modeling Group 2011b], as part of our ongoing effort for constructing a model-level integrated development environment [BGU Modeling Group 2010a].",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2045430061",
    "type": "article"
  },
  {
    "title": "DARWIN",
    "doi": "https://doi.org/10.1145/2211616.2211622",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Dawei Qi; Abhik Roychoudhury; Zhenkai Liang; Kapil Vaswani",
    "corresponding_authors": "",
    "abstract": "Bugs in programs are often introduced when programs evolve from a stable version to a new version. In this article, we propose a new approach called DARWIN for automatically finding potential root causes of such bugs. Given two programs—a reference program and a modified program—and an input that fails on the modified program, our approach uses symbolic execution to automatically synthesize a new input that (a) is very similar to the failing input and (b) does not fail. We find the potential cause(s) of failure by comparing control-flow behavior of the passing and failing inputs and identifying code fragments where the control flows diverge. A notable feature of our approach is that it handles hard-to-explain bugs, like code missing errors, by pointing to code in the reference program. We have implemented this approach and conducted experiments using several real-world applications, such as the Apache Web server, libPNG (a library for manipulating PNG images), and TCPflow (a program for displaying data sent through TCP connections). In each of these applications, DARWIN was able to localize bugs with high accuracy. Even though these applications contain several thousands of lines of code, DARWIN could usually narrow down the potential root cause(s) to less than ten lines. In addition, we find that the inputs synthesized by DARWIN provide additional value by revealing other undiscovered errors.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2052844069",
    "type": "article"
  },
  {
    "title": "Directed Incremental Symbolic Execution",
    "doi": "https://doi.org/10.1145/2629536",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Guowei Yang; Suzette Person; Neha Rungta; Sarfraz Khurshid",
    "corresponding_authors": "",
    "abstract": "The last few years have seen a resurgence of interest in the use of symbolic execution—a program analysis technique developed more than three decades ago to analyze program execution paths. Scaling symbolic execution to real systems remains challenging despite recent algorithmic and technological advances. An effective approach to address scalability is to reduce the scope of the analysis. For example, in regression analysis, differences between two related program versions are used to guide the analysis. While such an approach is intuitive, finding efficient and precise ways to identify program differences, and characterize their impact on how the program executes has proved challenging in practice. In this article, we present Directed Incremental Symbolic Execution (DiSE), a novel technique for detecting and characterizing the impact of program changes to scale symbolic execution. The novelty of DiSE is to combine the efficiencies of static analysis techniques to compute program difference information with the precision of symbolic execution to explore program execution paths and generate path conditions affected by the differences. DiSE complements other reduction and bounding techniques for improving symbolic execution. Furthermore, DiSE does not require analysis results to be carried forward as the software evolves—only the source code for two related program versions is required. An experimental evaluation using our implementation of DiSE illustrates its effectiveness at detecting and characterizing the effects of program changes.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2006860739",
    "type": "article"
  },
  {
    "title": "Exact scalable sensitivity analysis for the next release problem",
    "doi": "https://doi.org/10.1145/2537853",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Mark Harman; Jens Krinke; Inmaculada Medina‐Bulo; Francisco Palomo‐Lozano; Jian Ren; Shin Yoo",
    "corresponding_authors": "",
    "abstract": "The nature of the requirements analysis problem, based as it is on uncertain and often inaccurate estimates of costs and effort, makes sensitivity analysis important. Sensitivity analysis allows the decision maker to identify those requirements and budgets that are particularly sensitive to misestimation. However, finding scalable sensitivity analysis techniques is not easy because the underlying optimization problem is NP-hard. This article introduces an approach to sensitivity analysis based on exact optimization. We implemented this approach as a tool, O ATSAC , which allowed us to experimentally evaluate the scalability and applicability of Requirements Sensitivity Analysis (RSA). Our results show that O ATSAC scales sufficiently well for practical applications in Requirements Sensitivity Analysis. We also show how the sensitivity analysis can yield insights into difficult and otherwise obscure interactions between budgets, requirements costs, and estimate inaccuracies using a real-world case study.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2059898997",
    "type": "article"
  },
  {
    "title": "Amplifying Tests to Validate Exception Handling Code",
    "doi": "https://doi.org/10.1145/2652483",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Pingyu Zhang; Sebastian Elbaum",
    "corresponding_authors": "",
    "abstract": "Validating code handling exceptional behavior is difficult, particularly when dealing with external resources that may be noisy and unreliable, as it requires (1) systematic exploration of the space of exceptions that may be thrown by the external resources, and (2) setup of the context to trigger specific patterns of exceptions. In this work, we first present a study quantifying the magnitude of the problem by inspecting the bug repositories of a set of popular applications in the increasingly relevant domain of Android mobile applications. The study revealed that 22% of the confirmed and fixed bugs have to do with poor exceptional handling code, and half of those correspond to interactions with external resources. We then present an approach that addresses this challenge by performing an systematic amplification of the program space explored by a test by manipulating the behavior of external resources. Each amplification attempts to expose a program's exception handling constructs to new behavior by mocking an external resource so that it returns normally or throws an exception following a predefined set of patterns. Our assessment of the approach indicates that it can be fully automated, is powerful enough to detect 67% of the faults reported in the bug reports of this kind, and is precise enough that 78% of the detected anomalies are fixed, and it has a great potential to assist developers.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2048901322",
    "type": "article"
  },
  {
    "title": "Guided test generation for database applications via synthesized database interactions",
    "doi": "https://doi.org/10.1145/2491529",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Kai Pan; Xintao Wu; Tao Xie",
    "corresponding_authors": "",
    "abstract": "Testing database applications typically requires the generation of tests consisting of both program inputs and database states. Recently, a testing technique called Dynamic Symbolic Execution (DSE) has been proposed to reduce manual effort in test generation for software applications. However, applying DSE to generate tests for database applications faces various technical challenges. For example, the database application under test needs to physically connect to the associated database, which may not be available for various reasons. The program inputs whose values are used to form the executed queries are not treated symbolically, posing difficulties for generating valid database states or appropriate database states for achieving high coverage of query-result-manipulation code. To address these challenges, in this article, we propose an approach called SynDB that synthesizes new database interactions to replace the original ones from the database application under test. In this way, we bridge various constraints within a database application: query-construction constraints, query constraints, database schema constraints, and query-result-manipulation constraints. We then apply a state-of-the-art DSE engine called Pex for .NET from Microsoft Research to generate both program inputs and database states. The evaluation results show that tests generated by our approach can achieve higher code coverage than existing test generation approaches for database applications.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2118731196",
    "type": "article"
  },
  {
    "title": "Predicting Query Quality for Applications of Text Retrieval to Software Engineering Tasks",
    "doi": "https://doi.org/10.1145/3078841",
    "publication_date": "2017-01-31",
    "publication_year": 2017,
    "authors": "Chris Mills; Gabriele Bavota; Sonia Haiduc; Rocco Oliveto; Andrian Marcus; Andrea De Lucia",
    "corresponding_authors": "",
    "abstract": "Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts. Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself. Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery. Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average. Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2619825482",
    "type": "article"
  },
  {
    "title": "The Effect of Program and Model Structure on the Effectiveness of MC/DC Test Adequacy Coverage",
    "doi": "https://doi.org/10.1145/2934672",
    "publication_date": "2016-07-19",
    "publication_year": 2016,
    "authors": "Gregory Gay; Ajitha Rajan; Matt Staats; Michael W. Whalen; Mats P. E. Heimdahl",
    "corresponding_authors": "",
    "abstract": "Test adequacy metrics defined over the structure of a program, such as Modified Condition and Decision Coverage (MC/DC), are used to assess testing efforts. However, MC/DC can be “cheated” by restructuring a program to make it easier to achieve the desired coverage. This is concerning, given the importance of MC/DC in assessing the adequacy of test suites for critical systems domains. In this work, we have explored the impact of implementation structure on the efficacy of test suites satisfying the MC/DC criterion using four real-world avionics systems. Our results demonstrate that test suites achieving MC/DC over implementations with structurally complex Boolean expressions are generally larger and more effective than test suites achieving MC/DC over functionally equivalent, but structurally simpler, implementations. Additionally, we found that test suites generated over simpler implementations achieve significantly lower MC/DC and fault-finding effectiveness when applied to complex implementations, whereas test suites generated over the complex implementation still achieve high MC/DC and attain high fault finding over the simpler implementation. By measuring MC/DC over simple implementations, we can significantly reduce the cost of testing, but in doing so, we also reduce the effectiveness of the testing process. Thus, developers have an economic incentive to “cheat” the MC/DC criterion, but this cheating leads to negative consequences. Accordingly, we recommend that organizations require MC/DC over a structurally complex implementation for testing purposes to avoid these consequences.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2417128866",
    "type": "article"
  },
  {
    "title": "Detecting the Behavior of Design Patterns through Model Checking and Dynamic Analysis",
    "doi": "https://doi.org/10.1145/3176643",
    "publication_date": "2017-10-31",
    "publication_year": 2017,
    "authors": "Andrea De Lucia; Vincenzo Deufemia; Carmine Gravino; Michele Risi",
    "corresponding_authors": "",
    "abstract": "We present a method and tool (ePAD) for the detection of design pattern instances in source code. The approach combines static analysis, based on visual language parsing and model checking, and dynamic analysis, based on source code instrumentation. Visual language parsing and static source code analysis identify candidate instances satisfying the structural properties of design patterns. Successively, model checking statically verifies the behavioral aspects of the candidates recovered in the previous phase. We encode the sequence of messages characterizing the correct behaviour of a pattern as Linear Temporal Logic (LTL) formulae and the sequence diagram representing the possible interaction traces among the objects involved in the candidates as Promela specifications. The model checker SPIN verifies that candidates satisfy the LTL formulae. Dynamic analysis is then performed on the obtained candidates by instrumenting the source code and monitoring those instances at runtime through the execution of test cases automatically generated using a search-based approach. The effectiveness of ePAD has been evaluated by detecting instances of 12 creational and behavioral patterns from six publicly available systems. The results reveal that ePAD outperforms other approaches by recovering more actual instances. Furthermore, on average ePAD achieves better results in terms of correctness and completeness.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2793523475",
    "type": "article"
  },
  {
    "title": "Multi-Step Learning and Adaptive Search for Learning Complex Model Transformations from Examples",
    "doi": "https://doi.org/10.1145/2904904",
    "publication_date": "2016-06-06",
    "publication_year": 2016,
    "authors": "Islem Baki; Houari Sahraoui",
    "corresponding_authors": "",
    "abstract": "Model-driven engineering promotes models as main development artifacts. As several models may be manipulated during the software-development life cycle, model transformations ensure their consistency by automating model generation and update tasks. However, writing model transformations requires much knowledge and effort that detract from their benefits. To address this issue, Model Transformation by Example (MTBE) aims to learn transformation programs from source and target model pairs supplied as examples. In this article, we tackle the fundamental issues that prevent the existing MTBE approaches from efficiently solving the problem of learning model transformations. We show that, when considering complex transformations, the search space is too large to be explored by naive search techniques. We propose an MTBE process to learn complex model transformations by considering three common requirements: element context and state dependencies and complex value derivation. Our process relies on two strategies to reduce the size of the search space and to better explore it, namely, multi-step learning and adaptive search. We experimentally evaluate our approach on seven model transformation problems. The learned transformation programs are able to produce perfect target models in three transformation cases, whereas precision and recall values larger than 90% are recorded for the four remaining cases.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2409613233",
    "type": "article"
  },
  {
    "title": "Unveiling Elite Developers’ Activities in Open Source Projects",
    "doi": "https://doi.org/10.1145/3387111",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Zhendong Wang; Yang Feng; Yi Wang; James A. Jones; David Redmiles",
    "corresponding_authors": "",
    "abstract": "Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers’ activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on G IT H UB . We investigate elite developers’ contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers’ efforts in nontechnical activities are negatively correlated with the project’s outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers’ activities and can inform an individual’s decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3033375835",
    "type": "article"
  },
  {
    "title": "Quality Indicators in Search-based Software Engineering",
    "doi": "https://doi.org/10.1145/3375636",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Shaukat Ali; Paolo Arcaini; Dipesh Pradhan; Safdar Aqeel Safdar; Tao Yue",
    "corresponding_authors": "",
    "abstract": "Search-Based Software Engineering (SBSE) researchers who apply multi-objective search algorithms (MOSAs) often assess the quality of solutions produced by MOSAs with one or more quality indicators (QIs). However, SBSE lacks evidence providing insights on commonly used QIs, especially about agreements among them and their relations with SBSE problems and applied MOSAs. Such evidence about QIs agreements is essential to understand relationships among QIs, identify redundant QIs, and consequently devise guidelines for SBSE researchers to select appropriate QIs for their specific contexts. To this end, we conducted an extensive empirical evaluation to provide insights on commonly used QIs in the context of SBSE, by studying agreements among QIs with and without considering differences of SBSE problems and MOSAs. In addition, by defining a systematic process based on three common ways of comparing MOSAs in SBSE, we present additional observations that were automatically produced based on the results of our empirical evaluation. These observations can be used by SBSE researchers to gain a better understanding of the commonly used QIs in SBSE, in particular, regarding their agreements. Finally, based on the results, we also provide a set of guidelines for SBSE researchers to select appropriate QIs for their particular context.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3016641541",
    "type": "article"
  },
  {
    "title": "Why Do Smart Contracts Self-Destruct? Investigating the Selfdestruct Function on Ethereum",
    "doi": "https://doi.org/10.1145/3488245",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Jiachi Chen; Xin Xia; David Lo; John Grundy",
    "corresponding_authors": "",
    "abstract": "The selfdestruct function is provided by Ethereum smart contracts to destroy a contract on the blockchain system. However, it is a double-edged sword for developers. On the one hand, using the selfdestruct function enables developers to remove smart contracts ( SCs ) from Ethereum and transfers Ethers when emergency situations happen, e.g., being attacked. On the other hand, this function can increase the complexity for the development and open an attack vector for attackers. To better understand the reasons why SC developers include or exclude the selfdestruct function in their contracts, we conducted an online survey to collect feedback from them and summarize the key reasons. Their feedback shows that 66.67% of the developers will deploy an updated contract to the Ethereum after destructing the old contract. According to this information, we propose a method to find the self-destructed contracts (also called predecessor contracts) and their updated version (successor contracts) by computing the code similarity. By analyzing the difference between the predecessor contracts and their successor contracts, we found five reasons that led to the death of the contracts; two of them (i.e., Unmatched ERC20 Token and Limits of Permission ) might affect the life span of contracts. We developed a tool named LifeScope to detect these problems. LifeScope reports 0 false positives or negatives in detecting Unmatched ERC20 Token . In terms of Limits of Permission , LifeScope achieves 77.89% of F-measure and 0.8673 of AUC in average. According to the feedback of developers who exclude selfdestruct functions, we propose suggestions to help developers use selfdestruct functions in Ethereum smart contracts better.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3025529919",
    "type": "article"
  },
  {
    "title": "Enhancing Search-based Testing with Testability Transformations for Existing APIs",
    "doi": "https://doi.org/10.1145/3477271",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Andrea Arcuri; Juan Pablo Galeotti",
    "corresponding_authors": "",
    "abstract": "Search-based software testing (SBST) has been shown to be an effective technique to generate test cases automatically. Its effectiveness strongly depends on the guidance of the fitness function. Unfortunately, a common issue in SBST is the so-called flag problem , where the fitness landscape presents a plateau that provides no guidance to the search. In this article, we provide a series of novel testability transformations aimed at providing guidance in the context of commonly used API calls (e.g., strings that need to be converted into valid date/time objects). We also provide specific transformations aimed at helping the testing of REST Web Services. We implemented our novel techniques as an extension to EvoMaster , an SBST tool that generates system-level test cases. Experiments on nine open-source REST web services, as well as an industrial web service, show that our novel techniques improve performance significantly.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3202888839",
    "type": "article"
  },
  {
    "title": "Adaptive Hypermutation for Search-Based System Test Generation: A Study on REST APIs with EvoMaster",
    "doi": "https://doi.org/10.1145/3464940",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Man Zhang; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "REST web services are widely popular in industry, and search techniques have been successfully used to automatically generate system-level test cases for those systems. In this article, we propose a novel mutation operator which is designed specifically for test generation at system-level, with a particular focus on REST APIs. In REST API testing, and often in system testing in general, an individual can have a long and complex chromosome. Furthermore, there are two specific issues: (1) fitness evaluation in system testing is highly costly compared with the number of objectives (e.g., testing targets) to optimize for; and (2) a large part of the genotype might have no impact on the phenotype of the individuals (e.g., input data that has no impact on the execution flow in the tested program). Due to these issues, it might be not suitable to apply a typical low mutation rate like 1/ n (where n is the number of genes in an individual), which would lead to mutating only one gene on average. Therefore, in this article, we propose an adaptive weight-based hypermutation, which is aware of the different characteristics of the mutated genes. We developed adaptive strategies that enable the selection and mutation of genes adaptively based on their fitness impact and mutation history throughout the search. To assess our novel proposed mutation operator, we implemented it in the EvoMaster tool, integrated in the MIO algorithm, and further conducted an empirical study with three artificial REST APIs and four real-world REST APIs. Results show that our novel mutation operator demonstrates noticeable improvements over the default MIO. It provides a significant improvement in performance for six out of the seven case studies, where the relative improvement is up to +12.09% for target coverage, +12.69% for line coverage, and +32.51% for branch coverage.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3204593724",
    "type": "article"
  },
  {
    "title": "The Case for Adaptive Security Interventions",
    "doi": "https://doi.org/10.1145/3471930",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Irum Rauf; Marian Petre; Thein Than Tun; Tamara López; D.P. Lunn; Dirk van der Linden; John N. Towse; Helen Sharp; Mark Levine; Awais Rashid; Bashar Nuseibeh",
    "corresponding_authors": "",
    "abstract": "Despite the availability of various methods and tools to facilitate secure coding, developers continue to write code that contains common vulnerabilities. It is important to understand why technological advances do not sufficiently facilitate developers in writing secure code. To widen our understanding of developers' behaviour, we considered the complexity of the security decision space of developers using theory from cognitive and social psychology. Our interdisciplinary study reported in this article (1) draws on the psychology literature to provide conceptual underpinnings for three categories of impediments to achieving security goals, (2) reports on an in-depth meta-analysis of existing software security literature that identified a catalogue of factors that influence developers' security decisions, and (3) characterises the landscape of existing security interventions that are available to the developer during coding and identifies gaps. Collectively, these show that different forms of impediments to achieving security goals arise from different contributing factors. Interventions will be more effective where they reflect psychological factors more sensitively and marry technical sophistication, psychological frameworks, and usability. Our analysis suggests “adaptive security interventions” as a solution that responds to the changing security needs of individual developers and a present a proof-of-concept tool to substantiate our suggestion.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3178592347",
    "type": "article"
  },
  {
    "title": "Predicting Patch Correctness Based on the Similarity of Failing Test Cases",
    "doi": "https://doi.org/10.1145/3511096",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "Haoye Tian; Yinghua Li; Weiguo Pian; Abdoul Kader Kaboré; Kui Liu; Andrew Habib; Jacques Klein; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "How do we know a generated patch is correct? This is a key challenging question that automated program repair (APR) systems struggle to address given the incompleteness of available test suites. Our intuition is that we can triage correct patches by checking whether each generated patch implements code changes (i.e., behavior) that are relevant to the bug it addresses. Such a bug is commonly specified by a failing test case. Towards predicting patch correctness in APR, we propose a novel yet simple hypothesis on how the link between the patch behavior and failing test specifications can be drawn: similar failing test cases should require similar patches . We then propose BATS , an unsupervised learning-based approach to predict patch correctness by checking patch B ehavior A gainst failing T est S pecification. BATS exploits deep representation learning models for code and patches: For a given failing test case, the yielded embedding is used to compute similarity metrics in the search for historical similar test cases to identify the associated applied patches, which are then used as a proxy for assessing the correctness of the APR-generated patches. Experimentally, we first validate our hypothesis by assessing whether ground-truth developer patches cluster together in the same way that their associated failing test cases are clustered. Then, after collecting a large dataset of 1,278 plausible patches (written by developers or generated by 32 APR tools), we use BATS to predict correct patches: BATS achieves AUC between 0.557 to 0.718 and recall between 0.562 and 0.854 in identifying correct patches. Our approach outperforms state-of-the-art techniques for identifying correct patches without the need for large labeled patch datasets—as is the case with machine learning-based approaches. While BATS is constrained by the availability of similar test cases, we show that it can still be complementary to existing approaches: When combined with a recent approach that relies on supervised learning, BATS improves the overall recall in detecting correct patches. We finally show that BATS is complementary to the state-of-the-art PATCH-SIM dynamic approach for identifying correct patches generated by APR tools.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4224244735",
    "type": "article"
  },
  {
    "title": "APIRO: A Framework for Automated Security Tools API Recommendation",
    "doi": "https://doi.org/10.1145/3512768",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Zarrin Tasnim Sworna; Chadni Islam; Muhammad Ali Babar",
    "corresponding_authors": "",
    "abstract": "Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC team’s ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API R ecommendation for security O rchestration, automation, and response, APIRO . To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93%, 23.03%, and 20.87% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7% in terms of Mean Reciprocal Rank (MRR).",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4220837030",
    "type": "article"
  },
  {
    "title": "The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches",
    "doi": "https://doi.org/10.1145/3576039",
    "publication_date": "2022-12-15",
    "publication_year": 2022,
    "authors": "Haoye Tian; Kui Liu; Yinghua Li; Abdoul Kader Kaboré; Anil Koyuncu; Andrew Habib; Li Li; Junhao Wen; Jacques Klein; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4221164274",
    "type": "article"
  },
  {
    "title": "Just-In-Time Defect Prediction on JavaScript Projects: A Replication Study",
    "doi": "https://doi.org/10.1145/3508479",
    "publication_date": "2022-04-19",
    "publication_year": 2022,
    "authors": "Chao Ni; Xin Xia; David Lo; Xiaohu Yang; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "Change-level defect prediction is widely referred to as just-in-time (JIT) defect prediction since it identifies a defect-inducing change at the check-in time, and researchers have proposed many approaches based on the language-independent change-level features. These approaches can be divided into two types: supervised approaches and unsupervised approaches, and their effectiveness has been verified on Java or C++ projects. However, whether the language-independent change-level features can effectively identify the defects of JavaScript projects is still unknown. Additionally, many researches have confirmed that supervised approaches outperform unsupervised approaches on Java or C++ projects when considering inspection effort. However, whether supervised JIT defect prediction approaches can still perform best on JavaScript projects is still unknown. Lastly, prior proposed change-level features are programming language–independent, whether programming language–specific change-level features can further improve the performance of JIT approaches on identifying defect-prone changes is also unknown. To address the aforementioned gap in knowledge, in this article, we collect and label the top-20 most starred JavaScript projects on GitHub. JavaScript is an extremely popular and widely used programming language in the industry. We propose five JavaScript-specific change-level features and conduct a large-scale empirical study (i.e., involving a total of 176,902 changes) and find that (1) supervised JIT defect prediction approaches (i.e., CBS+) still statistically significantly outperform unsupervised approaches on JavaScript projects when considering inspection effort; (2) JavaScript-specific change-level features can further improve the performance of approach built with language-independent features on identifying defect-prone changes; (3) the change-level features in the dimension of size (i.e., LT), diffusion (i.e., NF), and JavaScript-specific (i.e., SO and TC) are the most important features for indicating the defect-proneness of a change on JavaScript projects; and (4) project-related features (i.e., Stars, Branches, Def Ratio, Changes, Files, Defective, and Forks) have a high association with the probability of a change to be a defect-prone one on JavaScript projects.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4224287853",
    "type": "article"
  },
  {
    "title": "Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty",
    "doi": "https://doi.org/10.1145/3550270",
    "publication_date": "2022-07-22",
    "publication_year": 2022,
    "authors": "Thomas Laurent; Stefan Klikovits; Paolo Arcaini; Fuyuki Ishikawa; Anthony Ventresque",
    "corresponding_authors": "",
    "abstract": "Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS’s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS’s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4286488146",
    "type": "article"
  },
  {
    "title": "Uncertainty-aware Prediction Validator in Deep Learning Models for Cyber-physical System Data",
    "doi": "https://doi.org/10.1145/3527451",
    "publication_date": "2022-03-28",
    "publication_year": 2022,
    "authors": "Ferhat Özgür Çatak; Tao Yue; Shaukat Ali",
    "corresponding_authors": "",
    "abstract": "The use of Deep learning in Cyber-Physical Systems (CPSs) is gaining popularity due to its ability to bring intelligence to CPS behaviors. However, both CPSs and deep learning have inherent uncertainty. Such uncertainty, if not handled adequately, can lead to unsafe CPS behavior. The first step toward addressing such uncertainty in deep learning is to quantify uncertainty. Hence, we propose a novel method called NIRVANA (uNcertaInty pRediction ValidAtor iN Ai) for prediction validation based on uncertainty metrics. To this end, we first employ prediction-time Dropout-based Neural Networks to quantify uncertainty in deep learning models applied to CPS data. Second, such quantified uncertainty is taken as the input to predict wrong labels using a support vector machine, with the aim of building a highly discriminating prediction validator model with uncertainty values. In addition, we investigated the relationship between uncertainty quantification and prediction performance and conducted experiments to obtain optimal dropout ratios. We conducted all the experiments with four real-world CPS datasets. Results show that uncertainty quantification is negatively correlated to prediction performance of a deep learning model of CPS data. Also, our dropout ratio adjustment approach is effective in reducing uncertainty of correct predictions while increasing uncertainty of wrong predictions.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4220753921",
    "type": "article"
  },
  {
    "title": "Boosting Compiler Testing via Compiler Optimization Exploration",
    "doi": "https://doi.org/10.1145/3508362",
    "publication_date": "2022-03-05",
    "publication_year": 2022,
    "authors": "Junjie Chen; Chenyao Suo",
    "corresponding_authors": "",
    "abstract": "Compilers are a kind of important software, and similar to the quality assurance of other software, compiler testing is one of the most widely-used ways of guaranteeing their quality. Compiler bugs tend to occur in compiler optimizations. Detecting optimization bugs needs to consider two main factors: (1) the optimization flags controlling the accessability of the compiler buggy code should be turned on; and (2) the test program should be able to trigger the buggy code. However, existing compiler testing approaches only consider the latter to generate effective test programs, but just run them under several pre-defined optimization levels (e.g., -O0 , -O1 , -O2 , -O3 , -Os in GCC). To better understand the influence of compiler optimizations on compiler testing, we conduct the first empirical study, and find that (1) all the bugs detected under the widely-used optimization levels are also detected under the explored optimization settings (we call a combination of optimization flags turned on for compilation an optimization setting ), while 83.54% of bugs are only detected under the latter; (2) there exist both inhibition effect and promotion effect among optimization flags for compiler testing, indicating the necessity and challenges of considering the factor of compiler optimizations in compiler testing. We then propose the first approach, called COTest , by considering both factors to test compilers. Specifically, COTest first adopts machine-learning (the XGBoost algorithm) to model the relationship between test programs and optimization settings, to predict the bug-triggering probability of a test program under an optimization setting. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. The experiments on GCC and LLVM demonstrate its effectiveness, especially COTest detects 17 previously unknown bugs, 11 of which have been fixed or confirmed by developers.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4220800565",
    "type": "article"
  },
  {
    "title": "Time-travel Investigation: Toward Building a Scalable Attack Detection Framework on Ethereum",
    "doi": "https://doi.org/10.1145/3505263",
    "publication_date": "2022-04-09",
    "publication_year": 2022,
    "authors": "Siwei Wu; Lei Wu; Yajin Zhou; Runhuai Li; Zhi Wang; Xiapu Luo; Cong Wang; Kui Ren",
    "corresponding_authors": "",
    "abstract": "Ethereum has been attracting lots of attacks, hence there is a pressing need to perform timely investigation and detect more attack instances. However, existing systems suffer from the scalability issue due to the following reasons. First, the tight coupling between malicious contract detection and blockchain data importing makes them infeasible to repeatedly detect different attacks. Second, the coarse-grained archive data makes them inefficient to replay transactions. Third, the separation between malicious contract detection and runtime state recovery consumes lots of storage. In this article, we propose a scalable attack detection framework named EthScope , which overcomes the scalability issue by neatly re-organizing the Ethereum state and efficiently locating suspicious transactions. It leverages the fine-grained state to support the replay of arbitrary transactions and proposes a well-designed schema to optimize the storage consumption. The performance evaluation shows that EthScope can solve the scalability issue, i.e., efficiently performing a large-scale analysis on billions of transactions, and a speedup of around \\( \\text{2,300}\\times \\) when replaying transactions. It also has lower storage consumption compared with existing systems. Further analysis shows that EthScope can help analysts understand attack behaviors and detect more attack instances.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4226411519",
    "type": "article"
  },
  {
    "title": "Some Seeds Are Strong: Seeding Strategies for Search-based Test Case Selection",
    "doi": "https://doi.org/10.1145/3532182",
    "publication_date": "2022-05-11",
    "publication_year": 2022,
    "authors": "Aitor Arrieta; Pablo Valle; Joseba Andoni Agirre; Goiuria Sagardui",
    "corresponding_authors": "",
    "abstract": "The time it takes software systems to be tested is usually long. Search-based test selection has been a widely investigated technique to optimize the testing process. In this article, we propose a set of seeding strategies for the test case selection problem that generates the initial population of Pareto-based multi-objective algorithms, with the goals of (1) helping to find an overall better set of solutions and (2) enhancing the convergence of the algorithms. The seeding strategies were integrated with four state-of-the-art multi-objective search algorithms and applied into two contexts where regression-testing is paramount: (1) Simulation-based testing of Cyber-physical Systems and (2) Continuous Integration. For the first context, we evaluated our approach by using six fitness function combinations and six independent case studies, whereas in the second context, we derived a total of six fitness function combinations and employed four case studies. Our evaluation suggests that some of the proposed seeding strategies are indeed helpful for solving the multi-objective test case selection problem. Specifically, the proposed seeding strategies provided a higher convergence of the algorithms towards optimal solutions in 96% of the studied scenarios and an overall cost-effectiveness with a standard search budget in 85% of the studied scenarios.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4280587448",
    "type": "article"
  },
  {
    "title": "Monitoring Constraints and Metaconstraints with Temporal Logics on Finite Traces",
    "doi": "https://doi.org/10.1145/3506799",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Giuseppe De Giacomo; Riccardo De Masellis; Fabrizio Maria Maggi; Marco Montali",
    "corresponding_authors": "",
    "abstract": "Runtime monitoring is a central operational decision support task in business process management. It helps process executors to check on-the-fly whether a running process instance satisfies business constraints of interest, providing an immediate feedback when deviations occur. We study runtime monitoring of properties expressed in ltl f , a variant of the classical ltl (Linear-time Temporal Logic) that is interpreted over finite traces, and in its extension ldl f , a powerful logic obtained by combining ltl f with regular expressions. We show that ldl f is able to declaratively express, in the logic itself, not only the constraints to be monitored, but also the de facto standard rv -LTL monitors. On the one hand, this enables us to directly employ the standard characterization of ldl f based on finite-state automata to monitor constraints in a fine-grained way. On the other hand, it provides the basis for declaratively expressing sophisticated metaconstraints that predicate on the monitoring state of other constraints, and to check them by relying on standard logical services instead of ad hoc algorithms. We then report on how this approach has been effectively implemented using Java to manipulate ldl f formulae and their corresponding monitors, and the RuM rule mining suite as underlying infrastructure.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3014360398",
    "type": "article"
  },
  {
    "title": "Security Responses in Software Development",
    "doi": "https://doi.org/10.1145/3563211",
    "publication_date": "2022-09-12",
    "publication_year": 2022,
    "authors": "Tamara López; Helen Sharp; Arosha K. Bandara; Thein Thun Tun; Mark Levine; Bashar Nuseibeh",
    "corresponding_authors": "",
    "abstract": "The pressure on software developers to produce secure software has never been greater. But what does security look like in environments that do not produce security-critical software? In answer to this question, this multi-sited ethnographic study characterizes security episodes and identifies five typical behaviors in software development. Using theory drawn from information security and motivation research in software engineering, this article characterizes key ways in which individual developers form security responses to meet the demands of particular circumstances, providing a framework managers and teams can use to recognize, understand, and alter security activity in their environments.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4295292680",
    "type": "article"
  },
  {
    "title": "Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study",
    "doi": "https://doi.org/10.1145/3571854",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Fiorella Zampetti; Damian A. Tamburri; Sebastiano Panichella; Annibale Panichella; Gerardo Canfora; Massimiliano Di Penta",
    "corresponding_authors": "",
    "abstract": "Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4309563112",
    "type": "article"
  },
  {
    "title": "COMET: Coverage-guided Model Generation For Deep Learning Library Testing",
    "doi": "https://doi.org/10.1145/3583566",
    "publication_date": "2023-02-08",
    "publication_year": 2023,
    "authors": "Meiziniu Li; Jialun Cao; Yongqiang Tian; Tsz On Li; Ming Wen; Shing-Chi Cheung",
    "corresponding_authors": "",
    "abstract": "Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques. Because of this limitation, we propose COMET to effectively generate DL models with diverse layer API calls for DL library testing. COMET: (1) designs a set of mutation operators and a coverage-based search algorithm to diversify layer inputs, layer parameter values, and layer sequences in DL models. (2) proposes a model synthesis method to boost the test efficiency without compromising the layer API call diversity. Our evaluation result shows that COMET outperforms baselines by covering twice as many layer inputs (69.7% vs. 34.1%), layer parameter values (50.2% vs. 25.9%), and layer sequences (39.0% vs. 15.6%) as those by the state-of-the-art. Moreover, COMET covers 3.4% more library branches than those by existing techniques. Finally, COMET detects 32 new bugs in the latest version of eight popular DL libraries, including TensorFlow and MXNet, with 21 of them confirmed by DL library developers and seven of those confirmed bugs have been fixed by developers.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4319594569",
    "type": "article"
  },
  {
    "title": "Generation-based Differential Fuzzing for Deep Learning Libraries",
    "doi": "https://doi.org/10.1145/3628159",
    "publication_date": "2023-10-18",
    "publication_year": 2023,
    "authors": "Jiawei Liu; Yuheng Huang; Zhijie Wang; Lei Ma; Chunrong Fang; Mingzheng Gu; Xufan Zhang; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) libraries have become the key component in developing and deploying DL-based software nowadays. With the growing popularity of applying DL models in both academia and industry across various domains, any bugs inherent in the DL libraries can potentially cause unexpected server outcomes. As such, there is an urgent demand for improving the software quality of DL libraries. Although there are some existing approaches specifically designed for testing DL libraries, their focus is usually limited to one specific domain, such as computer vision (CV). It is still not very clear how the existing approaches perform in detecting bugs of different DL libraries regarding different task domains and to what extent. To bridge this gap, we first conduct an empirical study on four representative and state-of-the-art DL library testing approaches. Our empirical study results reveal that it is hard for existing approaches to generalize to other task domains. We also find that the test inputs generated by these approaches usually lack diversity, with only a few types of bugs. What is worse, the false-positive rate of existing approaches is also high ( up to 58% ). To address these issues, we propose a guided differential fuzzing approach based on generation , namely, Gandalf . To generate testing inputs across diverse task domains effectively, Gandalf adopts the context-free grammar to ensure validity and utilizes a Deep Q-Network to maximize the diversity. Gandalf also includes 15 metamorphic relations to make it possible for the generated test cases to generalize across different DL libraries. Such a design can decrease the false positives because of the semantic difference for different APIs. We evaluate the effectiveness of Gandalf on nine versions of three representative DL libraries, covering 309 operators from computer vision, natural language processing, and automated speech recognition. The evaluation results demonstrate that Gandalf can effectively and efficiently generate diverse test inputs. Meanwhile, Gandalf successfully detects five categories of bugs with only 3.1% false-positive rates. We report all 49 new unique bugs found during the evaluation to the DL libraries’ developers, and most of these bugs have been confirmed. Details about our empirical study and evaluation results are available on our project website. 1",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4387735187",
    "type": "article"
  },
  {
    "title": "A Comparative Study on Method Comment and Inline Comment",
    "doi": "https://doi.org/10.1145/3582570",
    "publication_date": "2023-02-13",
    "publication_year": 2023,
    "authors": "Yuan Huang; Hanyang Guo; Xi Ding; Junhuai Shu; Xiangping Chen; Xiapu Luo; Zibin Zheng; Xiaocong Zhou",
    "corresponding_authors": "",
    "abstract": "Code comments are one of the important documents to help developers review and comprehend source code. In recent studies, researchers have proposed many deep learning models to generate the method header comments (i.e., method comment), which have achieved encouraging results. The comments in the method, which is called inline comment, are also important for program comprehension. Unfortunately, they have not received enough attention in automatic generation when comparing with the method comments. In this paper, we compare and analyze the similarities and differences between the method comments and the inline comments. By applying the existing models of generating method comments to the inline comment generation, we find that these existing models perform worse on the task of inline comment generation. We then further explore the possible reasons and obtain a number of new observations. For example, we find that there are a lot of templates (i.e., comments with the same or similar structures) in the method comment dataset, which makes the models perform better. Some terms were thought to be important (e.g., API calls) in the comment generation by previous study does not significantly affect the quality of the generated comments, which seems counter-intuitive. Our findings may give some implications for building the approaches of method comment or inline comment generation in the future.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4320490961",
    "type": "article"
  },
  {
    "title": "Tiny, Always-on, and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows",
    "doi": "https://doi.org/10.1145/3591867",
    "publication_date": "2023-04-11",
    "publication_year": 2023,
    "authors": "Wiebke Hutiri; Aaron Yi Ding; Fahim Kawsar; Akhil Mathur",
    "corresponding_authors": "",
    "abstract": "Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias . Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4364381409",
    "type": "article"
  },
  {
    "title": "Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis",
    "doi": "https://doi.org/10.1145/3604608",
    "publication_date": "2023-06-17",
    "publication_year": 2023,
    "authors": "Shouguo Yang; Zhengzi Xu; Yang Xiao; Zhe Lang; W. Tang; Yang Liu; Zhiqiang Shi; Hong Li; Limin Sun",
    "corresponding_authors": "",
    "abstract": "Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin , to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4381050443",
    "type": "article"
  },
  {
    "title": "The Human Side of Fuzzing: Challenges Faced by Developers during Fuzzing Activities",
    "doi": "https://doi.org/10.1145/3611668",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Olivier Nourry; Yutaro Kashiwa; Bin Lin; Gabriele Bavota; Michele Lanza; Yasutaka Kamei",
    "corresponding_authors": "",
    "abstract": "Fuzz testing, also known as fuzzing, is a software testing technique aimed at identifying software vulnerabilities. In recent decades, fuzzing has gained increasing popularity in the research community. However, existing studies led by fuzzing experts mainly focus on improving the coverage and performance of fuzzing techniques. That is, there is still a gap in empirical knowledge regarding fuzzing, especially about the challenges developers face when they adopt fuzzing. Understanding these challenges can provide valuable insights to both practitioners and researchers on how to further improve fuzzing processes and techniques. We conducted a study to understand the challenges encountered by developers during fuzzing. More specifically, we first manually analyzed 829 randomly sampled fuzzing-related GitHub issues and constructed a taxonomy consisting of 39 types of challenges (22 related to the fuzzing process itself, 17 related to using external fuzzing providers). We then surveyed 106 fuzzing practitioners to verify the validity of our taxonomy and collected feedback on how the fuzzing process can be improved. Our taxonomy, accompanied with representative examples and highlighted implications, can serve as a reference point on how to better adopt fuzzing techniques for practitioners, and indicates potential directions researchers can work on toward better fuzzing approaches and practices.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4385492587",
    "type": "article"
  },
  {
    "title": "Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees",
    "doi": "https://doi.org/10.1145/3611663",
    "publication_date": "2023-08-07",
    "publication_year": 2023,
    "authors": "Jeho Oh; Don Batory; Rubén Heradio",
    "corresponding_authors": "",
    "abstract": "A Software Product Line ( SPL ) is a family of similar programs. Each program is defined by a unique set of features, called a configuration , that satisfies all feature constraints. “What configuration achieves the best performance for a given workload?” is the SPL Optimization ( SPLO ) challenge. SPLO is daunting: just 80 unconstrained features yield 10 24 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4385616879",
    "type": "article"
  },
  {
    "title": "Characterizing and Detecting WebAssembly Runtime Bugs",
    "doi": "https://doi.org/10.1145/3624743",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Yixuan Zhang; Shangtong Cao; Haoyu Wang; Zhenpeng Chen; Xiapu Luo; Dongliang Mu; Yun Ma; Gang Huang; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "WebAssembly (abbreviated WASM) has emerged as a promising language of the Web and also been used for a wide spectrum of software applications such as mobile applications and desktop applications. These applications, named WASM applications, commonly run in WASM runtimes. Bugs in WASM runtimes are frequently reported by developers and cause the crash of WASM applications. However, these bugs have not been well studied. To fill in the knowledge gap, we present a systematic study to characterize and detect bugs in WASM runtimes. We first harvest a dataset of 311 real-world bugs from hundreds of related posts on GitHub. Based on the collected high-quality bug reports, we distill 31 bug categories of WASM runtimes and summarize their common fix strategies. Furthermore, we develop a pattern-based bug detection framework to automatically detect bugs in WASM runtimes. We apply the detection framework to seven popular WASM runtimes and successfully uncover 60 bugs that have never been reported previously, among which 13 have been confirmed and 9 have been fixed by runtime developers.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4386889449",
    "type": "article"
  },
  {
    "title": "Enablers and Barriers of Empathy in Software Developer and User Interactions: A Mixed Methods Case Study",
    "doi": "https://doi.org/10.1145/3641849",
    "publication_date": "2024-01-23",
    "publication_year": 2024,
    "authors": "Hashini Gunatilake; John Grundy; Rashina Hoda; Ingo Mueller",
    "corresponding_authors": "",
    "abstract": "Software engineering (SE) requires developers to collaborate with stakeholders, and understanding their emotions and perspectives is often vital. Empathy is a concept characterising a person’s ability to understand and share the feelings of another. However, empathy continues to be an under-researched human aspect in SE. We studied how empathy is practised between developers and end users using a mixed methods case study. We used an empathy test, observations, and interviews to collect data and socio-technical grounded theory and descriptive statistics to analyse data. We identified the nature of awareness required to trigger empathy and enablers of empathy. We discovered barriers to empathy and a set of potential strategies to overcome these barriers. We report insights on emerging relationships and present a set of recommendations and potential future works on empathy and SE for software practitioners and SE researchers.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4391145674",
    "type": "article"
  },
  {
    "title": "Test Optimization in DNN Testing: A Survey",
    "doi": "https://doi.org/10.1145/3643678",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Qiang Hu; Yuejun Guo; Xiaofei Xie; Maxime Cordy; Lei Ma; Mike Papadakis; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "This article presents a comprehensive survey on test optimization in deep neural network (DNN) testing. Here, test optimization refers to testing with low data labeling effort. We analyzed 90 papers, including 43 from the software engineering (SE) community, 32 from the machine learning (ML) community, and 15 from other communities. Our study: (i) unifies the problems as well as terminologies associated with low-labeling cost testing, (ii) compares the distinct focal points of SE and ML communities, and (iii) reveals the pitfalls in existing literature. Furthermore, we highlight the research opportunities in this domain.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4391277903",
    "type": "article"
  },
  {
    "title": "Characterizing Deep Learning Package Supply Chains in PyPI: Domains, Clusters, and Disengagement",
    "doi": "https://doi.org/10.1145/3640336",
    "publication_date": "2024-01-10",
    "publication_year": 2024,
    "authors": "Kai Gao; Runzhi He; Bing Xie; Minghui Zhou",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) frameworks have become the cornerstone of the rapidly developing DL field. Through installation dependencies specified in the distribution metadata, numerous packages directly or transitively depend on DL frameworks, layer after layer, forming DL package supply chains (SCs), which are critical for DL frameworks to remain competitive. However, vital knowledge on how to nurture and sustain DL package SCs is still lacking. Achieving this knowledge may help DL frameworks formulate effective measures to strengthen their SCs to remain competitive and shed light on dependency issues and practices in the DL SC for researchers and practitioners. In this paper, we explore the domains, clusters, and disengagement of packages in two representative PyPI DL package SCs to bridge this knowledge gap. We analyze the metadata of nearly six million PyPI package distributions and construct version-sensitive SCs for two popular DL frameworks: TensorFlow and PyTorch. We find that popular packages (measured by the number of monthly downloads) in the two SCs cover 34 domains belonging to eight categories. Applications , Infrastructure , and Sciences categories account for over 85% of popular packages in either SC and TensorFlow and PyTorch SC have developed specializations on Infrastructure and Applications packages, respectively. We employ the Leiden community detection algorithm and detect 131 and 100 clusters in the two SCs. The clusters mainly exhibit four shapes: Arrow, Star, Tree, and Forest with increasing dependency complexity. Most clusters are Arrow or Star, while Tree and Forest clusters account for most packages (Tensorflow SC: 70.7%, PyTorch SC: 92.9%). We identify three groups of reasons why packages disengage from the SC (i.e., remove the DL framework and its dependents from their installation dependencies): dependency issues, functional improvements, and ease of installation. The most common reason in TensorFlow SC is dependency incompatibility and in PyTorch SC is to simplify functionalities and reduce installation size. Our study provides rich implications for DL framework vendors, researchers, and practitioners on the maintenance and dependency management practices of PyPI DL SCs.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4390722138",
    "type": "article"
  },
  {
    "title": "KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation",
    "doi": "https://doi.org/10.1145/3643675",
    "publication_date": "2024-01-29",
    "publication_year": 2024,
    "authors": "Wei Tao; Yucheng Zhou; Yanlin Wang; Hongyu Zhang; Haofen Wang; Wenqiang Zhang",
    "corresponding_authors": "",
    "abstract": "Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391323412",
    "type": "article"
  },
  {
    "title": "Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?",
    "doi": "https://doi.org/10.1145/3654443",
    "publication_date": "2024-03-27",
    "publication_year": 2024,
    "authors": "Emanuele Iannone; Giulia Sellitto; Emanuele Iaccarino; Filomena Ferrucci; Andrea De Lucia; Fabio Palomba",
    "corresponding_authors": "",
    "abstract": "With the rate of discovered and disclosed vulnerabilities escalating, researchers have been experimenting with machine learning to predict whether a vulnerability will be exploited. Existing solutions leverage information unavailable when a CVE is created, making them unsuitable just after the disclosure. This paper experiments with early exploitability prediction models driven exclusively by the initial CVE record, i.e., the original description and the linked online discussions. Leveraging NVD and Exploit Database, we evaluate 72 prediction models trained using six traditional machine learning classifiers, four feature representation schemas, and three data balancing algorithms. We also experiment with five pre-trained large language models (LLMs). The models leverage seven different corpora made by combining three data sources, i.e., CVE description, Security Focus , and BugTraq . The models are evaluated in a realistic , time-aware fashion by removing the training and test instances that cannot be labeled “neutral” with sufficient confidence. The validation reveals that CVE descriptions and Security Focus discussions are the best data to train on. Pre-trained LLMs do not show the expected performance, requiring further pre-training in the security domain. We distill new research directions, identify possible room for improvement, and envision automated systems assisting security experts in assessing the exploitability.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4393233304",
    "type": "article"
  },
  {
    "title": "A Survey of Source Code Search: A 3-Dimensional Perspective",
    "doi": "https://doi.org/10.1145/3656341",
    "publication_date": "2024-04-06",
    "publication_year": 2024,
    "authors": "Weisong Sun; Chunrong Fang; Yifei Ge; Yuling Hu; Yuchen Chen; Quanjun Zhang; Xiuting Ge; Yang Liu; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "(Source) code search is widely concerned by software engineering researchers because it can improve the productivity and quality of software development. Given a functionality requirement usually described in a natural language sentence, a code search system can retrieve code snippets that satisfy the requirement from a large-scale code corpus, e.g., GitHub. To realize effective and efficient code search, many techniques have been proposed successively. These techniques improve code search performance mainly by optimizing three core components, including query understanding component, code understanding component, and query-code matching component. In this article, we provide a 3-dimensional perspective survey for code search. Specifically, we categorize existing code search studies into query-end optimization techniques, code-end optimization techniques, and match-end optimization techniques according to the specific components they optimize. These optimization techniques are proposed to enhance the performance of specific components, and thus the overall performance of code search. Considering that each end can be optimized independently and contributes to the code search performance, we treat each end as a dimension. Therefore, this survey is 3-dimensional in nature, and it provides a comprehensive summary of each dimension in detail. To understand the research trends of the three dimensions in existing code search studies, we systematically review 68 relevant literatures. Different from existing code search surveys that only focus on the query end or code end or introduce various aspects shallowly (including codebase, evaluation metrics, modeling technique, etc.), our survey provides a more nuanced analysis and review of the evolution and development of the underlying techniques used in the three ends. Based on a systematic review and summary of existing work, we outline several open challenges and opportunities at the three ends that remain to be addressed in future work.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4394010871",
    "type": "article"
  },
  {
    "title": "Replication in Requirements Engineering: The NLP for RE Case",
    "doi": "https://doi.org/10.1145/3658669",
    "publication_date": "2024-04-15",
    "publication_year": 2024,
    "authors": "Sallam Abualhaija; Fatma Başak Aydemir; Fabiano Dalpiaz; Davide Dell’Anna; Alessio Ferrari; Xavier Franch; Davide Fucci",
    "corresponding_authors": "",
    "abstract": "Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Despite its empirical vocation, RE research has given limited attention to replication of NLP for RE studies. Replication is hampered by several factors, including the context specificity of the studies, the heterogeneity of the tasks involving NLP, the tasks’ inherent hairiness , and, in turn, the heterogeneous reporting structure. To address these issues, we propose a new artifact, referred to as ID-Card , whose goal is to provide a structured summary of research papers emphasizing replication-relevant information. We construct the ID-Card through a structured, iterative process based on design science. In this article: (i) we report on hands-on experiences of replication; (ii) we review the state-of-the-art and extract replication-relevant information: (iii) we identify, through focus groups, challenges across two typical dimensions of replication: data annotation and tool reconstruction; and (iv) we present the concept and structure of the ID-Card to mitigate the identified challenges. This study aims to create awareness of replication in NLP for RE. We propose an ID-Card that is intended to foster study replication but can also be used in other contexts, e.g., for educational purposes.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4394809155",
    "type": "article"
  },
  {
    "title": "FunFuzz: A Function-oriented Fuzzer for Smart Contract Vulnerability Detection with High Effectiveness and Efficiency",
    "doi": "https://doi.org/10.1145/3674725",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Mingxi Ye; Yuhong Nan; Hong‐Ning Dai; Shuo Yang; Xiapu Luo; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "With the increasing popularity of Decentralized Applications (DApps) in blockchain, securing smart contracts has been a long-term, high-priority subject in the domain. Among the various research directions for vulnerability detection, fuzzing has received extensive attention because of its high effectiveness. However, with the increasing complexity of smart contracts, existing fuzzers may waste substantial time exploring locations irrelevant to smart contract vulnerabilities. In this article, we present FunFuzz, a function-oriented fuzzer, which is dedicatedly tailored for detecting smart contract vulnerability with high effectiveness and efficiency. The key observation in our research is that most smart contract vulnerabilities exist in specific functions rather than randomly distributed in all program code like other traditional software. To this end, unlike traditional fuzzers which mainly target code coverage, FunFuzz identifies risky functions while pruning non-risky ones in smart contracts. In this way, it significantly narrows down the exploration scope during the fuzzing process. In addition, FunFuzz employs three unique strategies to direct itself toward effectively discovering vulnerabilities specific to smart contracts (e.g., reentrancy, block dependency, and gasless send). Extensive experiments on 170 real-world contracts demonstrate that FunFuzz outperforms state-of-the-art fuzzers in terms of effectiveness and efficiency.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4400117505",
    "type": "article"
  },
  {
    "title": "<scp>AceCoder</scp> : An Effective Prompting Technique Specialized in Code Generation",
    "doi": "https://doi.org/10.1145/3675395",
    "publication_date": "2024-07-04",
    "publication_year": 2024,
    "authors": "Jia Li; Yunfei Zhao; Yongmin Li; Ge Li; Zhi Jin",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) have shown great success in code generation. LLMs take as the input a prompt and output the code. How to make prompts (i.e., Prompting Techniques ) is a key question. Existing prompting techniques are designed for natural language generation and have low accuracy in code generation. In this article, we propose a new prompting technique named AceCoder . Our motivation is that code generation meets two unique challenges (i.e., requirement understanding and code implementation). AceCoder contains two novel mechanisms (i.e., guided code generation and example retrieval) to solve these challenges. ❶ Guided code generation asks LLMs first to analyze requirements and output an intermediate preliminary (e.g., test cases). The preliminary clarifies requirements and tells LLMs “what to write.” ❷ Example retrieval selects similar programs as examples in prompts, which provide lots of relevant content (e.g., algorithms, APIs) and teach LLMs “how to write.” We apply AceCoder to four LLMs (e.g., GPT-3.5, CodeGeeX) and evaluate it on three public benchmarks using the Pass@ \\(k\\) . Results show that AceCoder can significantly improve the performance of LLMs on code generation. In terms of Pass@1, AceCoder outperforms the SOTA baseline by up to 56.4% in MBPP, 70.7% in MBJP, and 88.4% in MBJSP . AceCoder is effective in LLMs with different sizes (i.e., 6B–13B) and different languages (i.e., Python, Java, and JavaScript). Human evaluation shows human developers prefer programs from AceCoder .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4400318591",
    "type": "article"
  },
  {
    "title": "Revisiting Sentiment Analysis for Software Engineering in the Era of Large Language Models",
    "doi": "https://doi.org/10.1145/3697009",
    "publication_date": "2024-09-24",
    "publication_year": 2024,
    "authors": "Ting Zhang; Ivana Clairine Irsan; Ferdian Thung; David Lo",
    "corresponding_authors": "",
    "abstract": "Software development involves collaborative interactions where stakeholders express opinions across various platforms. Recognizing the sentiments conveyed in these interactions is crucial for the effective development and ongoing maintenance of software systems. For software products, analyzing the sentiment of user feedback, e.g., reviews, comments, and forum posts can provide valuable insights into user satisfaction and areas for improvement. This can guide the development of future updates and features. However, accurately identifying sentiments in software engineering datasets remains challenging. This study investigates bigger large language models (bLLMs) in addressing the labeled data shortage that hampers fine-tuned smaller large language models (sLLMs) in software engineering tasks. We conduct a comprehensive empirical study using five established datasets to assess three open-source bLLMs in zero-shot and few-shot scenarios. Additionally, we compare them with fine-tuned sLLMs, using sLLMs to learn contextual embeddings of text from software platforms. Our experimental findings demonstrate that bLLMs exhibit state-of-the-art performance on datasets marked by limited training data and imbalanced distributions. bLLMs can also achieve excellent performance under a zero-shot setting. However, when ample training data is available or the dataset exhibits a more balanced distribution, fine-tuned sLLMs can still achieve superior results.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4402759247",
    "type": "article"
  },
  {
    "title": "Large Language Model Supply Chain: A Research Agenda",
    "doi": "https://doi.org/10.1145/3708531",
    "publication_date": "2024-12-13",
    "publication_year": 2024,
    "authors": "Shenao Wang; Yanjie Zhao; Xinyi Hou; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, introducing unprecedented capabilities in natural language processing and multimodal content generation. However, the increasing complexity and scale of these models have given rise to a multifaceted supply chain that presents unique challenges across infrastructure, foundation models, and downstream applications. This paper provides the first comprehensive research agenda of the LLM supply chain, offering a structured approach to identify critical challenges and opportunities through the dual lenses of software engineering (SE) and security &amp; privacy (S&amp;P). We begin by establishing a clear definition of the LLM supply chain, encompassing its components and dependencies. We then analyze each layer of the supply chain, presenting a vision for robust and secure LLM development, reviewing the current state of practices and technologies, and identifying key challenges and research opportunities. This work aims to bridge the existing research gap in systematically understanding the multifaceted issues within the LLM supply chain, offering valuable insights to guide future efforts in this rapidly evolving domain.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4405366282",
    "type": "article"
  },
  {
    "title": "A Roadmap for Integrating Sustainability into Software Engineering Education",
    "doi": "https://doi.org/10.1145/3708526",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Ana Moreira; Patricia Lago; Rogardt Heldal; Stefanie Betz; Ian Brooks; Rafael Capilla; Vlad C. Coroamă; Letícia Duboc; João Paulo Fernandes; Ola Leifler; Jari Porras; Ngoc-Thanh Nguyen; Shola Oyedeji; Birgit Penzenstadler; Anne-Kathrin Peters; Colin C. Venters",
    "corresponding_authors": "",
    "abstract": "The world faces escalating crises: record-breaking temperatures, widespread fires, severe flooding, increased oceanic microplastics, and unequal resource distribution. Academia introduces courses around sustainability to meet the new demand, but software engineering education lags behind. While software systems contribute to environmental issues through high energy consumption, they also hold the potential for solutions, such as more efficient and equitable resource management. Yet, sustainability remains a low priority for many businesses, including those in the digital sector. Business as usual is no longer viable. A transformational change in software engineering education is urgently needed. We must move beyond traditional curriculum models and fully integrate sustainability into every aspect of software development. By embedding sustainability as a core competency, we can equip future engineers not only to minimise harm but also to innovate solutions that drive positive, sustainable change. Only with such a shift can software engineering education meet the demands of a world in crisis and prepare students to lead the next generation of sustainable technology. This paper discusses a set of challenges and proposes a customisable education roadmap for integrating sustainability into the software engineering curricula. These challenges reflect our perspective on key considerations, stemming from regular, intensive discussions in regular workshops among the authors and the community, as well as our extensive research and teaching experience in the field.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4405543550",
    "type": "article"
  },
  {
    "title": "Efficient and Green Large Language Models for Software Engineering: Vision and the Road Ahead",
    "doi": "https://doi.org/10.1145/3708525",
    "publication_date": "2024-12-20",
    "publication_year": 2024,
    "authors": "Jieke Shi; Zhou Yang; David Lo",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have recently shown remarkable capabilities in various software engineering tasks, spurring the rapid growth of the Large Language Models for Software Engineering (LLM4SE) area. However, limited attention has been paid to developing efficient LLM4SE techniques that demand minimal computational cost, time, and memory resources, as well as green LLM4SE solutions that reduce energy consumption, water usage, and carbon emissions. This paper aims to redirect the focus of the research community towards the efficiency and greenness of LLM4SE, while also sharing potential research directions to achieve this goal. It commences with a brief overview of the significance of LLM4SE and highlights the need for efficient and green LLM4SE solutions. Subsequently, the paper presents a vision for a future where efficient and green LLM4SE revolutionizes the LLM-based software engineering tool landscape, benefiting various stakeholders, including industry, individual practitioners, and society. The paper then delineates a roadmap for future research, outlining specific research paths and potential solutions for the research community to pursue. While not intended to be a definitive guide, the paper aims to inspire further progress, with the ultimate goal of establishing efficient and green LLM4SE as a central element in the future of software engineering.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4405640497",
    "type": "article"
  },
  {
    "title": "How Low Can We Go? Minimizing Interaction Samples for Configurable Systems",
    "doi": "https://doi.org/10.1145/3712193",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Dominik Krupke; Ahmad Moradi; Michael Perk; Phillip Keldenich; Gabriel Gehrke; Sebastian Krieter; Thomas Thüm; Sándor P. Fekete",
    "corresponding_authors": "",
    "abstract": "Modern software systems are typically configurable, a fundamental prerequisite for wide applicability and reusability. This flexibility poses an extraordinary challenge for quality assurance, as the enormous number of possible configurations makes it impractical to test each of them separately. This is where t-wise interaction sampling can be used to systematically cover the configuration space and detect unknown feature interactions. Over the last two decades, numerous algorithms for computing small interaction samples have been studied, providing improvements for a range of heuristic results; nevertheless, it has remained unclear how much these results can still be improved. We present a significant breakthrough: a fundamental framework, based on the mathematical principle of duality , for combining near-optimal solutions with provable lower bounds on the required sample size. This implies that we no longer need to work on heuristics with marginal or no improvement, but can certify the solution quality by establishing a limit on the remaining gap; in many cases, we can even prove optimality of achieved solutions. This theoretical contribution also provides extensive practical improvements: Our algorithm SampLNS was tested on 47 small and medium-sized configurable systems from the existing literature. SampLNS can reliably find samples of smaller size than previous methods in 85 % of the cases; moreover, we can achieve and prove optimality of solutions for 63 % of all instances. This makes it possible to avoid cumbersome efforts of minimizing samples by researchers as well as practitioners, and substantially save testing resources for most configurable systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406308381",
    "type": "article"
  },
  {
    "title": "Software Engineering for Collective Cyber-Physical Ecosystems",
    "doi": "https://doi.org/10.1145/3712004",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Roberto Casadei; Gianluca Aguzzi; Giorgio Audrito; Ferruccio Damiani; Danilo Pianini; Giordano Scarso; Gianluca Torta; Mirko Viroli",
    "corresponding_authors": "",
    "abstract": "Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focuses on treating these systems as “composites” (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as “collectives” (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this “collective computing paradigm” in software engineering. In particular, it discusses its peculiar challenges, implied by characteristics like distribution, situatedness, large scale, and cooperative nature. These challenges outline significant directions for future research in software engineering, touching on aspects such as macro-programming, collective intelligence, self-adaptive middleware, learning/synthesis of collective behaviour, human involvement, safety and security in collective cyber-physical ecosystems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406324483",
    "type": "article"
  },
  {
    "title": "Finding Information Leaks with Information Flow Fuzzing - RCR Report",
    "doi": "https://doi.org/10.1145/3711905",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Bernd Gruner; Clemens-Alexander Brust; Andreas Zeller",
    "corresponding_authors": "",
    "abstract": "This is the Replicated Computational Results (RCR) Report for our ACM TOSEM paper, ”Finding Information Leaks with Information Flow Fuzzing,” in which we propose information flow fuzzing . This approach directs fuzzers toward detecting information leaks . We introduce a novel leak oracle and employ information flow as guidance for the fuzzer to identify information leaks effectively. As part of this RCR, we provide a replication package that enables the complete replication of all our results and simplifies the reuse of our FLOWFUZZ fuzzer.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406385974",
    "type": "article"
  },
  {
    "title": "Making Software Development More Diverse and Inclusive: Key Themes, Challenges, and Future Directions",
    "doi": "https://doi.org/10.1145/3711904",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Sonja M. Hyrynsalmi; Sebastian Baltes; Chris Brown; Rafael Prikladnicki; Gema Rodríguez-Pérez; Alexander Serebrenik; Jocelyn Simmonds; Bianca Trinkenreich; Yi Wang; Grischa Liebel",
    "corresponding_authors": "",
    "abstract": "Introduction : Digital products increasingly reshape industries, influencing human behavior and decision-making. However, the software development teams developing these systems often lack diversity, which may lead to designs that overlook the needs, equal treatment or safety of diverse user groups. These risks highlight the need for fostering diversity and inclusion in software development to create safer, more equitable technology. Method : This research is based on insights from an academic meeting in June 2023 involving 23 software engineering researchers and practitioners. We used the collaborative discussion method 1-2-4-ALL as a systematic research approach and identified six themes around the theme “challenges and opportunities to improve Software Developer Diversity and Inclusion (SDDI).” We identified benefits, harms, and future research directions for the four main themes. Then, we discuss the remaining two themes, AI &amp; SDDI and AI &amp; Computer Science education, which have a cross-cutting effect on the other themes. Results : This research explores the key challenges and research opportunities for promoting SDDI, providing a roadmap to guide both researchers and practitioners. We underline that research around SDDI requires a constant focus on maximizing benefits while minimizing harms, especially to vulnerable groups. As a research community, we must strike this balance in a responsible way.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406465537",
    "type": "article"
  },
  {
    "title": "Uncovering Community Smells in Machine Learning-Enabled Systems: Causes, Effects, and Mitigation Strategies",
    "doi": "https://doi.org/10.1145/3712198",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Giusy Annunziata; Stefano Lambiase; Damian A. Tamburri; Willem‐Jan van den Heuvel; Fabio Palomba; Gemma Catolino; Filomena Ferrucci; Andrea De Lucia",
    "corresponding_authors": "",
    "abstract": "Successful software development hinges on effective communication and collaboration, which are significantly influenced by human and social dynamics. Poor management of these elements can lead to the emergence of ‘community smells’, i.e., negative patterns in socio-technical interactions that gradually accumulate as ‘social debt’. This issue is particularly pertinent in machine learning-enabled systems, where diverse actors such as data engineers and software engineers interact at various levels. The unique collaboration context of these systems presents an ideal setting to investigate community smells and their impact on development communities. This paper addresses a gap in the literature by identifying the types, causes, effects, and potential mitigation strategies of community smells in machine learning-enabled systems. Using Partial Least Squares Structural Equation Modeling (PLS-SEM), we developed hypotheses based on existing literature and interviews, and conducted a questionnaire-based study to collect data. Our analysis resulted in the construction and validation of five models that represent the causes, effects, and strategies for five specific community smells. These models can help practitioners identify and address community smells within their organizations, while also providing valuable insights for future research on the socio-technical aspects of machine learning-enabled system communities.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406494502",
    "type": "article"
  },
  {
    "title": "Innovating for Tomorrow: The Convergence of Software Engineering and Green AI",
    "doi": "https://doi.org/10.1145/3712007",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Luís Cruz; Xavier Franch; Silverio Martínez‐Fernández",
    "corresponding_authors": "",
    "abstract": "The latest advancements in machine learning, specifically in foundation models, are revolutionizing the frontiers of existing software engineering (SE) processes. This is a bi-directional phenomena, where 1) software systems are now challenged to provide AI-enabled features to their users, and 2) AI is used to automate tasks within the software development lifecycle. In an era where sustainability is a pressing societal concern, our community needs to adopt a long-term plan enabling a conscious transformation that aligns with environmental sustainability values. In this paper, we reflect on the impact of adopting environmentally friendly practices to create AI-enabled software systems and make considerations on the environmental impact of using foundation models for software development.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406494538",
    "type": "article"
  },
  {
    "title": "Finding Information Leaks with Information Flow Fuzzing",
    "doi": "https://doi.org/10.1145/3711902",
    "publication_date": "2025-01-20",
    "publication_year": 2025,
    "authors": "Bernd Gruner; Clemens-Alexander Brust; Andreas Zeller",
    "corresponding_authors": "",
    "abstract": "We present information flow fuzzing , an approach that guides fuzzers towards detecting information leaks — information that reaches a third party, but should not. The approach detects information flow by means of mutations , checking whether and how mutations to (secret) data affect output and execution: — First, the fuzzer uses information flow as a leak oracle. To this end, for each input, the fuzzer first runs the program regularly. Then, it mutates secret data such as a certificate or a password, and re-runs the program giving the original input. If the output changes, the fuzzer has revealed an information leak. — Second, the fuzzer uses information flow as guidance. The fuzzer not only maximizes coverage, but also changes in coverage and changes in data between the two runs. This increases the likelihood that a mutation will spread to the output. We have implemented a tool named flowfuzz that wraps around a C program under test to provide information flow based oracles and guidance, allowing for integration with all common fuzzers for C programs. Using a set of subjects representing common information leaks, we investigate (1) whether oracles based on information flow detect information leaks in our subjects; and (2) whether guidance based on information flow improves over standard coverage guidance. All data and tools are available for replication and reproduction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406620166",
    "type": "article"
  },
  {
    "title": "Empowering Agile-Based Generative Software Development through Human-AI Teamwork",
    "doi": "https://doi.org/10.1145/3702987",
    "publication_date": "2025-01-22",
    "publication_year": 2025,
    "authors": "Sai Zhang; Zhenchang Xing; Ronghui Guo; Fangzhou Xu; L. Chen; Zhaoyuan Zhang; Xiaowang Zhang; Zhiyong Feng; Zhiqiang Zhuang",
    "corresponding_authors": "",
    "abstract": "In software development, the raw requirements proposed by users are frequently incomplete, which impedes the complete implementation of software functionalities. With the emergence of large language models, the exploration of generating software through user requirements has attracted attention. Recent methods with the top-down waterfall model employ a questioning approach for requirement completion, attempting to explore further user requirements. However, users, constrained by their domain knowledge, result in a lack of effective acceptance criteria during the requirement completion, failing to fully capture the implicit needs of the user. Moreover, the cumulative errors of the waterfall model can lead to discrepancies between the generated code and user requirements. The Agile methodologies reduce cumulative errors of the waterfall model through lightweight iteration and collaboration with users, but the challenge lies in ensuring semantic consistency between user requirements and the code generated by the agent. To address these challenges, we propose AgileGen, an agile-based generative software development through human-AI teamwork. Unlike existing questioning agents, AgileGen adopts a novel collaborative approach that breaks free from the constraints of domain knowledge by initiating the end-user perspective to complete the acceptance criteria. By introducing the Gherkin language, AgileGen attempts for the first time to use testable requirement descriptions as a bridge for semantic consistency between requirements and code, aiming to ensure that software products meet actual user requirements by defining user scenarios that include acceptance criteria. Additionally, we innovate in the human-AI teamwork model, allowing users to participate in decision-making processes they do well and significantly enhancing the completeness of software functionality. To ensure semantic consistency between requirements and generated code, we derive consistency factors from Gherkin to drive the subsequent software code generation. Finally, to improve the reliability of user scenarios, we also introduce a memory pool mechanism, collecting user decision-making scenarios and recommending them to new users with similar requirements. AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4% and garnered higher user satisfaction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406698707",
    "type": "article"
  },
  {
    "title": "Signal Feature Coverage and Testing for CPS Dataflow Models",
    "doi": "https://doi.org/10.1145/3714467",
    "publication_date": "2025-01-22",
    "publication_year": 2025,
    "authors": "Ezio Bartocci; Leonardo Mariani; Dejan Ničković; Drishti Yadav",
    "corresponding_authors": "",
    "abstract": "Design of cyber-physical systems (CPS) typically involves dataflow modelling. The structure of dataflow models differs from the traditional software, making standard coverage metrics not appropriate for measuring the thoroughness of testing. To address this limitation, this paper proposes signal feature coverage as a new coverage metric for systematically testing CPS dataflow models. We derive signal feature coverage by leveraging signal features. We developed a testing framework in Simulink ® , a popular dataflow modelling and simulation environment, that automates the generation and execution of test cases based on the defined coverage metric. We evaluated the effectiveness of our approach by carrying out experiments on five Simulink ® models tested against ten Signal Temporal Logic specifications. We compared our coverage-based testing approach to adaptive random testing, falsification testing, output diversity-based approaches, and testing using MathWorks’ Simulink ® Design Verifier™. The results demonstrate that our coverage-based testing approach outperforms the conventional techniques regarding fault detection capability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406698759",
    "type": "article"
  },
  {
    "title": "With Great Power Comes Great Responsibility: The Role of Software Engineers",
    "doi": "https://doi.org/10.1145/3715112",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Stefanie Betz; Birgit Penzenstadler",
    "corresponding_authors": "",
    "abstract": "The landscape of Software Engineering evolves rapidly amidst digital transformation and the ascendancy of AI, leading to profound shifts in the role and responsibilities of Software Engineers. This evolution encompasses both immediate changes, such as the adoption of Large Language Model-based approaches to coding, and deeper shifts driven by the profound societal and environmental impacts of technology. Despite the urgency, there persists a lag in adapting to these evolving roles. This roadmap paper proposes 10 research challenges to develop a new generation of Software Engineers equipped to navigate the technical and social complexities as well as ethical considerations inherent in their evolving profession. Furthermore, the challenges target role definition, integration of AI, education transformation, standards evolution, and impact assessment to equip future Software Engineers to skillfully and responsibly handle the obstacles within their transforming discipline.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406866191",
    "type": "article"
  },
  {
    "title": "Finding Safety Violations of AI-Enabled Control Systems through the Lens of Synthesized Proxy Programs",
    "doi": "https://doi.org/10.1145/3715105",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Jieke Shi; Zhou Yang; Junda He; Bowen Xu; Dongsun Kim; DongGyun Han; David Lo",
    "corresponding_authors": "",
    "abstract": "Given the increasing adoption of modern AI-enabled control systems, ensuring their safety and reliability has become a critical task in software testing. One prevalent approach to testing control systems is falsification, which aims to find an input signal that causes the control system to violate a formal safety specification using optimization algorithms. However, applying falsification to AI-enabled control systems poses two significant challenges: (1) it requires the system to execute numerous candidate test inputs, which can be time-consuming, particularly for systems with AI models that have many parameters, and (2) multiple safety requirements are typically defined as a conjunctive specification, which is difficult for existing falsification approaches to comprehensively cover. This paper introduces Synthify , a falsification framework tailored for AI-enabled control systems, i.e., control systems equipped with AI controllers. Our approach performs falsification in a two-phase process. At the start, Synthify synthesizes a program that implements one or a few linear controllers to serve as a proxy for the AI controller. This proxy program mimics the AI controller's functionality but is computationally more efficient. Then, Synthify employs the \\(\\epsilon\\) -greedy strategy to sample a promising sub-specification from the conjunctive safety specification. It then uses a Simulated Annealing-based falsification algorithm to find violations of the sampled sub-specification for the control system. To evaluate Synthify , we compare it to PSY-TaLiRo , a state-of-the-art and industrial-strength falsification tool, on 8 publicly available control systems. On average, Synthify achieves a 83.5% higher success rate in falsification compared to PSY-TaLiRo with the same budget of falsification trials. Additionally, our method is 12.8 \\(\\times\\) faster in finding a single safety violation than the baseline. The safety violations found by Synthify are also more diverse than those found by PSY-TaLiRo , covering 137.7% more sub-specifications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406866240",
    "type": "article"
  },
  {
    "title": "Introducing Phylogenetics in Search-based Software Engineering: Phylogenetics-aware SBSE",
    "doi": "https://doi.org/10.1145/3715002",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Daniel Blasco; Antonio Iglesias; Jorge Echeverría; Francisca Pérez; Carlos Cetina",
    "corresponding_authors": "",
    "abstract": "Phylogenetics studies the relationships, in terms of biological history and kinship, of a set of taxa (e.g., species). We argue that in Search-based Software Engineering (SBSE), the individuals of an evolutionary computation-driven population could be considered as taxa for which the leverage of Phylogenetic Inference might be beneficial. In this work, we present our Phylogenetics-aware SBSE approach. Our approach introduces a novel Phylogenetic Operation to promote results which are sufficiently aligned (in terms of lineage) with a certain reference given by the domain expert. Our approach is evaluated in two heterogeneous industrial case studies: Procedural Content Generation from Game Software Engineering, and Feature Location from Software Maintenance. The results are analyzed using quality-of-the-solution and acceptance-by-developers measurements. We performed a statistical analysis to determine whether the impact on the results is significant compared to baselines that do not leverage Phylogenetics. The results show that our approach significantly outperforms two baselines in both case studies. Furthermore, two focus groups confirmed the acceptance of our approach and stressed that solution acceptance may make the difference in industrial environments. Our work has the potential to motivate a new breed of research work on Phylogenetics awareness to produce better results in Software Engineering.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406866346",
    "type": "article"
  },
  {
    "title": "HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent",
    "doi": "https://doi.org/10.1145/3715109",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Jie JW Wu; Fatemeh H. Fard",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. The most recent trend is using LLM-based agents to iterate the code generation process. Based on the observation that top-level software engineers often ask clarifying questions to reduce Ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. For this purpose, we define the communication skills of LLMs as “being able to ask clarifying questions when the description of the code generation problem has issues”. In this study, we restrict these issues to three matters from the software requirement engineering field: inconsistent requirements, ambiguous requirements, and incomplete requirements. By asking probing questions about the requirements of problem descriptions before generating the final code, the challenges of programming with LLMs, such as unclear intent specification may be alleviated, resulting to a correct code in the initial iterations. In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues mentioned above, Inconsistency , Ambiguity , Incompleteness . We then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, C o de C l a rificatio n a nd G eneration A ge n t (Okanagan), to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. In the evaluation, we introduced an LLM-based evaluator and created Communication Rate and Good Question Rate as the evaluation metrics to represent the ratio of questions asked and questions with good quality in responses. We found that more than 60% of responses from Code LLMs still generate code rather than ask questions when the problem descriptions are manually modified according to different clarification categories. The Pass@1 and Test Pass Rate of most Code LLMs drop by 35% \\(\\sim\\) 52% and by 17% \\(\\sim\\) 35% respectively, with statistical significance in each category for over 75% numbers. Okanagan, as an LLM agent approach that uses LLM such as ChatGPT 3.5, effectively increases the Communication Rate and Good Question Rate by an absolute 58% and 38%, respectively. Thus, Okanagan boosts Pass@1 and Test Pass Rate by an absolute 8% and 7%, respectively, when the problem descriptions are modified based on given clarification categories. This result indicates the potential for achieving more effective communication capability using LLM agent. Our benchmark and full code are publicly available at https://github.com/jie-jw-wu/human-eval-comm .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406866661",
    "type": "article"
  },
  {
    "title": "Assessing the Robustness of Test Selection Methods for Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3715693",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Qiang Hu; Yuejun Guo; Xiaofei Xie; Maxime Cordy; Wei Ma; Mike Papadakis; Lei Ma; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Regularly testing deep learning-powered systems on newly collected data is critical to ensure their reliability, robustness, and efficacy in real-world applications. This process is demanding due to the significant time and human effort required for labeling new data. While test selection methods alleviate manual labor by labeling and evaluating only a subset of data while meeting testing criteria, we observe that such methods with reported promising results are simply evaluated, e.g., testing on original test data. The question arises: are they always reliable? In this paper, we explore when and to what extent test selection methods fail. First, we identify potential pitfalls of 11 selection methods based on their construction. Second, we conduct a study to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suffer from data that are: 1) correctly classified but uncertain, or 2) misclassified but confident. Remarkably, the test relative coverage achieved by such methods drops by up to 86.85%. Besides, methods for performance estimation are sensitive to the choice of intermediate-layer output. The effectiveness of such methods can be even worse than random selection when using an inappropriate layer.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406957873",
    "type": "article"
  },
  {
    "title": "<scp>DrWASI</scp> : LLM-assisted Differential Testing for WebAssembly System Interface Implementations",
    "doi": "https://doi.org/10.1145/3716379",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Yixuan Zhang; Ningyu He; Jianting Gao; Shangtong Cao; Kaibo Liu; Haoyu Wang; Yun Ma; Gang Huang; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "WebAssembly (Wasm) is an emerging binary format that serves as a compilation target for over 40 programming languages. Wasm runtimes provide execution environments that enhance portability by abstracting away operating systems and hardware details. A key component in these runtimes is the WebAssembly System Interface (WASI), which manages interactions with operating systems, like file operations. Considering the critical role of Wasm runtimes, the community has aimed to detect their implementation bugs. However, no work has focused on WASI-specific bugs that can affect the original functionalities of running Wasm binaries and cause unexpected results. To fill the void, we present DrWASI , the first general-purpose differential testing framework for WASI implementations. Our approach uses a large language model to generate seeds and applies variant and environment mutation strategies to expand and enrich the test case corpus. We then perform differential testing across major Wasm runtimes. By leveraging dynamic and static information collected during and after the execution, DrWASI can identify bugs. Our evaluation shows that DrWASI uncovered 33 unique bugs, with all confirmed and 7 fixed by developers. This research represents a pioneering step in exploring a promising yet under-explored area of the Wasm ecosystem, providing valuable insights for stakeholders.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407353004",
    "type": "article"
  },
  {
    "title": "<i>SCOPE</i> : Performance Testing for Serverless Computing",
    "doi": "https://doi.org/10.1145/3717609",
    "publication_date": "2025-02-14",
    "publication_year": 2025,
    "authors": "Jinfeng Wen; Zhenpeng Chen; J. Q. Zhao; Federica Sarro; Haodi Ping; Ying Zhang; Shangguang Wang; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "Serverless computing is a popular cloud computing paradigm that has found widespread adoption across various online workloads. It allows software engineers to develop cloud applications as a set of functions (called serverless functions ). However, accurately measuring the performance (i.e., end-to-end response latency) of serverless functions is challenging due to the highly dynamic nature of the environment in which they run. To tackle this problem, a potential solution is to apply checks of performance testing techniques to determine how many repetitions of a given serverless function across a range of inputs are needed to cater to the performance fluctuation. However, the available literature lacks performance testing approaches designed explicitly for serverless computing. In this paper, we propose SCOPE , the first s erverless c omputing- o riented p erformance t e sting approach. SCOPE takes into account the unique performance characteristics of serverless functions, such as their short execution durations and on-demand triggering. As such, SCOPE is designed as a fine-grained analysis approach. SCOPE incorporates the accuracy check and the consistency check to obtain the accurate and reliable performance of serverless functions. The evaluation shows that SCOPE provides testing results with 97.25% accuracy, 33.83 percentage points higher than the best currently available technique. Moreover, the superiority of SCOPE over the state-of-the-art holds on all functions that we study.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407580499",
    "type": "article"
  },
  {
    "title": "Enhancing Android Malware Detection: The Influence of ChatGPT on Decision-centric Task",
    "doi": "https://doi.org/10.1145/3720541",
    "publication_date": "2025-02-27",
    "publication_year": 2025,
    "authors": "Yao Li; Sen Fang; Tao Zhang; Haipeng Cai",
    "corresponding_authors": "",
    "abstract": "With the rise of large language models, such as ChatGPT, non-decisional models have been applied to various tasks. Moreover, ChatGPT has drawn attention to the traditional decision-centric task of Android malware detection. Despite effective detection methods proposed by scholars, they face low interpretability issues. Specifically, while these methods excel in classifying applications as benign or malicious and can detect malicious behavior, they often fail to provide detailed explanations for the decisions they make. This challenge raises concerns about the reliability of existing detection schemes and questions their true ability to understand complex data. In this study, we investigate the influence of the non-decisional model, ChatGPT, on the traditional decision-centric task of Android malware detection. We choose three state-of-the-art solutions, \\(Drebin\\) , \\(XM_{AL}\\) , and \\(MaMaDroid\\) , conduct a series of experiments on publicly available datasets, and carry out a comprehensive comparison and analysis. Our findings indicate that these decision-driven solutions primarily rely on statistical patterns within datasets to make decisions, rather than genuinely understanding the underlying data. In contrast, ChatGPT, as a non-decisional model, excels in providing comprehensive analysis reports, substantially enhancing interpretability. Furthermore, we conduct surveys among experienced developers. The result highlights developers’ preference for ChatGPT, as it offers in-depth insights and enhances efficiency and understanding of challenges. Meanwhile, these studies and analyses offer profound insights, presenting developers with a novel perspective on Android malware detection—enhancing the reliability of detection results from a non-decisional perspective.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408021713",
    "type": "article"
  },
  {
    "title": "Large Language Model-Aware In-Context Learning for Code Generation",
    "doi": "https://doi.org/10.1145/3715908",
    "publication_date": "2025-02-28",
    "publication_year": 2025,
    "authors": "Jia Li; Chongyang Tao; Jia Li; Ge Li; Zhi Jin; Huangzhao Zhang; Zheng Fang; Fang Liu",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have shown impressive In-Context Learning (ICL) ability in code generation. LLMs take a prompt context consisting of a few demonstration examples and a new requirement as input, and output new programs without any parameter update. Existing studies have found that the performance of ICL-based code generation heavily depends on the quality of demonstration examples and thus arises research on selecting demonstration examples: given a new requirement, a few demonstration examples are selected from a candidate pool, where LLMs are expected to learn the pattern hidden in these selected demonstration examples. Existing approaches are mostly based on heuristics or randomly selecting examples. However, the distribution of randomly selected examples usually varies greatly, making the performance of LLMs less robust. The heuristics retrieve examples by only considering textual similarities of requirements, leading to sub-optimal performance. To fill this gap, we propose a L arge language model- A ware selection approach for I n-context- L earning-based code generation named LAIL. LAIL uses LLMs themselves to select examples. It requires LLMs themselves to label a candidate example as a positive example or a negative example for a requirement. Positive examples are helpful for LLMs to generate correct programs, while negative examples are trivial and should be ignored. Based on the labeled positive and negative data, LAIL trains a model-aware retriever to learn the preference of LLMs and select demonstration examples that LLMs need. During the inference, given a new requirement, LAIL uses the trained retriever to select a few examples and feed them into LLMs to generate desired programs. We apply LAIL to four widely used LLMs and evaluate it on five code generation datasets. Extensive experiments demonstrate that LAIL outperforms the state-of-the-art (SOTA) baselines by 11.58%, 3.33%, and 5.07% on CodeGen-Multi-16B, 1.32%, 2.29%, and 1.20% on CodeLlama-34B, and achieves 4.38%, 2.85%, and 2.74% improvements on Text-davinci-003 in terms of Pass@1 at MBJP, MBPP, and MBCPP, respectively. In addition to function-level code generation, LAIL improves the performance of LLMs on DevEval, a repository-level code generation dataset, which achieves 10.04%, 8.12%, and 4.63% improvements compared to the SOTA baselines at Pass@1, 3, and 5 on CodeLlama-7B. Human evaluation further verifies that the generated programs of LAIL are superior in correctness, code quality, and maintainability. Besides, LAIL has satisfactory transferability across different LLMs and datasets, where the retriever learned on one LLM (dataset) can be transferred to other LLMs (datasets).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408028112",
    "type": "article"
  },
  {
    "title": "Unraveling Code Clone Dynamics in Deep Learning Frameworks",
    "doi": "https://doi.org/10.1145/3721125",
    "publication_date": "2025-02-28",
    "publication_year": 2025,
    "authors": "Maram Assi; Safwat Hassan; Ying Zou",
    "corresponding_authors": "",
    "abstract": "Deep Learning (DL) frameworks play a critical role in advancing artificial intelligence, and their rapid growth underscores the need for a comprehensive understanding of software quality and maintainability. DL frameworks, like other systems, are prone to code clones. Code clones refer to identical or highly similar source code fragments within the same project or even across different projects. Code cloning can have positive and negative implications for software development, influencing maintenance, readability, and bug propagation. While the existing studies focus on studying clones in DL-based applications, to our knowledge, no work has been done investigating clones, their evolution and their impact on the maintenance of DL frameworks. In this paper, we aim to address the knowledge gap concerning the evolutionary dimension of code clones in DL frameworks and the extent of code reuse across these frameworks. We empirically analyze code clones in nine popular DL frameworks, i.e., TensorFlow , Paddle , PyTorch , Aesara , Ray , MXNet , Keras , Jax and BentoML , to investigate (1) the characteristics of the long-term code cloning evolution over releases in each framework, (2) the short-term, i.e., within-release, code cloning patterns and their influence on the long-term trends, and (3) the file-level code clones within the DL frameworks. Our findings reveal that DL frameworks adopt four distinct cloning trends: “Serpentine” , “Rise and Fall” , “Decreasing” , and “Stable” and that these trends present some common and distinct characteristics. For instance, bug-fixing activities persistently happen in clones irrespective of the clone evolutionary trend but occur more in the “Serpentine” trend. Moreover, the within-release level investigation demonstrates that short-term code cloning practices impact long-term cloning trends. The cross-framework code clone investigation reveals the presence of functional and architectural adaptation file-level cross-framework code clones across the nine studied frameworks. We provide insights that foster robust clone practices and collaborative maintenance in the development of DL frameworks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408054024",
    "type": "article"
  },
  {
    "title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation",
    "doi": "https://doi.org/10.1145/3722108",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Catherine Tony; Nicolás E. Díaz Ferreyra; Markus Mutas; Salem Dhif; Riccardo Scandariato",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. Objective: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. Method : First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. Results: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408214684",
    "type": "article"
  },
  {
    "title": "DRLMutation: A Comprehensive Framework for Mutation Testing in Deep Reinforcement Learning Systems",
    "doi": "https://doi.org/10.1145/3721978",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Jiapeng Li; Zheng Zheng; Xiaoting Du; Haoyu Wang; Yanwen Liu",
    "corresponding_authors": "",
    "abstract": "Deep reinforcement learning (DRL) systems have been increasingly applied in various domains. Testing them, however, remains a major open research problem. Mutation testing is a popular test suite evaluation technique that analyzes the extent to which test suites detect injected faults. It has been widely researched in both traditional software and the field of deep learning. However, due to the fundamental differences between DRL systems and traditional software, as well as deep learning systems, in aspects such as environment interaction, network decision-making, and data efficiency, previous mutation testing techniques cannot be directly applied to DRL systems. In this paper, we proposed a comprehensive mutation testing framework specifically designed for DRL systems, DRLMutation , to further fill this gap. We first considered the characteristics of DRL, and based on both the training process and the model of trained agent, examined combinations from three dimensions: objects, operation methods, and injection methods. This approach led to a more comprehensive design methodology for DRL mutation operators. After filtering, we identified a total of 107 applicable DRL mutation operators. Then, in the realm of evaluation, we formulated a set of metrics tailored to assess test suites. Finally, we validated the stealthiness and effectiveness of the proposed mutation operators in the Cart Pole , Mountain Car Continuous , Lunar Lander , Breakout and CARLA environments. We show inspiring findings that the majority of these designed DRL mutation operators potentially undermine the decision-making capabilities of the agent without affecting normal training. The varying degrees of disruption achieved by these mutation operators can be used to assess the quality of different test suites.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408214696",
    "type": "article"
  },
  {
    "title": "V <scp>ex</scp> IR2V <scp>ec</scp> : An Architecture-Neutral Embedding Framework for Binary Similarity",
    "doi": "https://doi.org/10.1145/3721481",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "S. VenkataKeerthy; Shantimoy Banerjee; S. Dey; Yashas Andaluri; Raghul PS; Subrahmanyam Kalyanasundaram; Fernando Magno Quintão Pereira; Ramakrishna Upadrasta",
    "corresponding_authors": "",
    "abstract": "Binary similarity involves determining whether two binary programs exhibit similar functionality with applications in vulnerability detection, malware analysis, and copyright detection. However, variations in compiler settings, target architectures, and deliberate code obfuscations significantly complicate the similarity measurement by effectively altering the syntax, semantics, and structure of the underlying binary. To address these challenges, we propose V ex IR2V ec , a robust, architecture-neutral approach based on VEX-IR to solve binary similarity tasks. V ex IR2V ec consists of three key components: a peephole extractor, a normalization engine (V ex INE), and an embedding model (V ex N et ). The process to build program embeddings starts with the extraction of sequences of basic blocks, or peepholes , from control-flow graphs via random walks, capturing structural information. These generated peepholes are then normalized using V ex INE, which applies compiler-inspired transformations to reduce architectural and compiler-induced variations. Embeddings of peepholes are generated using representation learning techniques, avoiding Out-Of-Vocabulary (OOV) issues. These embeddings are then fine-tuned with V ex N et , a feed-forward Siamese network that maps functions into a high dimensional space for diffing and searching tasks in an application-independent manner. We evaluate V ex IR2V ec against five baselines — BinDiff, DeepBinDiff, SAFE, BinFinder, and histograms of opcodes — on a dataset comprising \\(2.7M\\) functions and \\(15.5K\\) binaries from \\(7\\) projects compiled across \\(12\\) compilers targeting x86 and ARM architectures. The experiments span four adversarial settings — cross-optimization, cross-compilation, cross-architecture, and obfuscations — that are typically exploited by malware and vulnerabilities. In diffing experiments, V ex IR2V ec outperforms the nearest baseline in these four scenarios by \\(40\\%\\) , \\(18\\%\\) , \\(21\\%\\) , and \\(60\\%\\) , respectively. In the searching experiment, V ex IR2V ec achieves a mean average precision of \\(0.76\\) , the nearest baseline, by \\(46\\%\\) . Our framework is highly scalable and is built as a lightweight, multi-threaded, parallel library using only open-source tools. V ex IR2V ec is \\(\\approx 3.1\\) – \\(3.5\\times\\) faster than the closest baselines and orders-of-magnitude faster than other tools.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408219068",
    "type": "article"
  },
  {
    "title": "Efficient Generation of Test Cases for MPI Program Path Coverage through Elite Individual Selection",
    "doi": "https://doi.org/10.1145/3723354",
    "publication_date": "2025-03-14",
    "publication_year": 2025,
    "authors": "Yong Wang; Wenzhong Cui; Gai‐Ge Wang; Jian Wang; Dunwei Gong",
    "corresponding_authors": "",
    "abstract": "In the field of message-passing interface (MPI) program path coverage test case generation, evolutionary algorithms (EAs) have been frequently utilized to generate test cases. However, relying solely on EAs will incur excessive computational costs. In this article, we improve the efficiency and quality of MPI program path coverage test cases generated by EAs based on elite individual selection. First, data within the data domain is sampled and fitness is calculated to form a shared set. Then, the population data is initialized using EAs, and the fitness of individuals is predicted using the neighbor value sharing algorithm (NVSA). Subsequently, individuals are ranked using rank-based elite selection (RES). Finally, elite individuals are chosen through ranking to run the program and verify the generation of test cases. In order to reduce computational costs, data dimensionality reduction operations are added to the above process. We demonstrate that the proposed method can effectively generate test data and reduce test costs by comparing it with several excellent methods on seven representative MPI programs. Among them, NVSA has a maximum improvement of 42.2%, RES has a maximum improvement of 31.5%, dimensionality reduction can increase by 20.2%, and the overall method has a maximum improvement of 47.4%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408421851",
    "type": "article"
  },
  {
    "title": "Evaluating API-Level Deep Learning Fuzzers: A Comprehensive Benchmarking Study",
    "doi": "https://doi.org/10.1145/3729533",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Nima Shiri Harzevili; Moshi Wei; Mohammad Mahdi Mohajer; Song Wang; Hung Viet Pham",
    "corresponding_authors": "",
    "abstract": "In recent years, the practice of fuzzing Deep Learning (DL) APIs has received significant attention in the software engineering community. Many API-level DL fuzzers have been proposed to test individual DL APIs by generating malformed input. Although these fuzzers have been effective in detecting bugs and outperforming prior work, there remains a gap in bench-marking them against ground-truth, real-world bugs in DL libraries. Existing comparisons among these API-level DL fuzzers primarily focus on the bugs detected but do not offer a comprehensive, in-depth evaluation of the fuzzers’ effectiveness. In this work, we perform the first in-depth evaluation of state-of-the-art API-level DL fuzzers that generate tests for single DL APIs, focusing on their effectiveness against real-world bugs. We manually created an extensive benchmark dataset, including 517 real-world DL bugs collected from PyTorch and TensorFlow libraries that can be triggered by malformed inputs. We then apply seven state-of-the-art DL fuzzers— FreeFuzz, DeepRel, NablaFuzz, DocTer, ACETest, TitanFuzz, and FuzzGPT—to our benchmark dataset, following their respective instructions. Our results show that these fuzzers detect only 6.5% (34 out of 517) of the unique real-world bugs in the dataset. Our analysis identifies two dominant factors that impact the effectiveness of these fuzzers in detecting real-world bugs. These findings suggest opportunities for improving the performance of fuzzers in future work. Overall, this study extends previous work on DL fuzzers by providing an extensive evaluation and benchmarking platform for fuzzing DL libraries.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409478867",
    "type": "article"
  },
  {
    "title": "Efficient Fuzzing Infrastructure for Pointer-to-Object Association",
    "doi": "https://doi.org/10.1145/3730580",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Hao Ling; Heqing Huang; Yuandao Cai; Charles Zhang",
    "corresponding_authors": "",
    "abstract": "Runtime feedback is at the heart of efficient greybox fuzzing, and the collection of runtime feedback is the most important infrastructure for greybox fuzzing. However, existing fuzzers have difficulty collecting runtime feedback for the memory, which is the most important and vulnerable components of a running program. The operating system does not support associative queries between arbitrary pointers and runtime objects. Therefore, existing works only capture aggregate statistics (e.g., memory usage) or random quantities (e.g., the random addresses stored in pointers) to provide low-precision memory-related feedback. This paper presents Spinel , a greybox fuzzer equipped with a brand-new infrastructure for memory feedback collection. It introduces an almost zero-overhead runtime system for associating arbitrary pointers with the corresponding runtime objects and offers spatial distance information as memory-related fuzzing feedback. To avoid introducing accumulated overhead upon silent error detectors (e.g., sanitizers that are used to detect memory safety violations), we introduce the post-execution validation technique to remove the expensive runtime safety checks while maintaining the same error detection ability. Our experiments on \\(33\\) real-world programs show that Spinel detects 1.30x—2.33x unique bugs compared to state-of-the-art fuzzers. Furthermore, according to the restricted mean survival time, Spinel achieves 1.56x-8.21x speed up in triggering ground-truth bugs collected by the Magma benchmark.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409584084",
    "type": "article"
  },
  {
    "title": "VEglue: Testing Visual Entailment Systems via Object-Aligned Joint Erasing",
    "doi": "https://doi.org/10.1145/3731244",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Zhiyuan Chang; Mingyang Li; Junjie Wang; Cheng Li; Qing Wang",
    "corresponding_authors": "",
    "abstract": "Visual entailment (VE) is a multimodal reasoning task consisting of image-sentence pairs whereby a promise is defined by an image, and a sentence describes a hypothesis. The goal is to predict whether the image semantically entails the sentence. VE systems have been widely adopted in many downstream tasks such as image caption and visual question answering. However, the robustness of VE systems still faces significant challenges. One of the reasons is that the VE system suffers object-confusing defect when some similar objects exist. It outputs a positive prediction inferred by an erroneous object relationship, which will result in a fault negative prediction if the noised object does not exist. Previous approaches generate tests primarily relied on some general perturbations, such as simulating noise or weather interference in images, or substituting synonyms or rewriting sentences in texts. To test the object-confusing defect in VE systems, it requires perceiving and understanding key objects and entities, and maintain the semantic relevance between cross-modal inputs inputs, making it challenging to generate effective tests with high-quality. Therefore, we propose VEglue , an object-aligned joint erasing approach for VE systems testing. It first aligns the object regions in the premise and object descriptions in the hypothesis to identify linked and un-linked objects. Then, based on the alignment information, three Metamorphic Relations are designed to jointly erase the objects of the two modalities. We evaluate VEglue on four widely-used VE systems involving two public datasets, and the results demonstrate that VEglue could detect 11,609 issues on average with a 52.5% Issue Finding Rate (IFR). Furthermore, we leverage the tests generated by VEglue to retrain the VE systems, which largely improves model performance (50.8% increase in accuracy) on newly generated tests without sacrificing the accuracy on the original test set.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409584661",
    "type": "article"
  },
  {
    "title": "NeuSemSlice: Towards Effective DNN Model Maintenance via Neuron-level Semantic Slicing",
    "doi": "https://doi.org/10.1145/3731556",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "Shide Zhou; Tianlin Li; Yihao Huang; Ling Shi; Kailong Wang; Yang Liu; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "Deep Neural networks (DNNs), extensively applied across diverse disciplines, are characterized by their integrated and monolithic architectures, setting them apart from conventional software systems. This architectural difference introduces particular challenges to maintenance tasks, such as model restructure ( e.g ., model compression), re-adaptation ( e.g ., fitting new samples), and incremental development ( e.g ., continual knowledge accumulation). Prior research addresses these challenges by identifying task-critical neuron layers, and dividing neural networks into semantically-similar sequential modules. However, such layer-level approaches fail to precisely identify and manipulate neuron-level semantic components, restricting their applicability to finer-grained model maintenance tasks. In this work, we implement NeuSemSlice, a novel framework that introduces the semantic slicing technique to effectively identify critical neuron-level semantic components in DNN models for semantic-aware model maintenance tasks. Specifically, semantic slicing identifies, categorizes and merges critical neurons across different categories and layers according to their semantic similarity, enabling their flexibility and effectiveness in the subsequent tasks. For semantic-aware model maintenance tasks, we provide a series of novel strategies based on semantic slicing to enhance NeuSemSlice. They include semantic components ( i.e ., critical neurons) preservation for model restructure, critical neuron tuning for model re-adaptation, and non-critical neuron training for model incremental development. A thorough evaluation has demonstrated that NeuSemSlice significantly outperforms baselines in all three tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409730458",
    "type": "article"
  },
  {
    "title": "Addressing Data Scarcity with Synthetic Data: A Secure and GDPR-compliant Cloud-Based Platform",
    "doi": "https://doi.org/10.1145/3732937",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Nemania Borovits; Gianluigi Bardelloni; Hourieh Hashemi; Masoom Tulsiani; Damian A. Tamburri; Willem‐Jan van den Heuvel",
    "corresponding_authors": "",
    "abstract": "This study presents a cloud-based platform for synthetic data generation, validation and evaluation, developed to address data scarcity in the telecommunications sector while ensuring compliance with the General Data Protection Regulation (GDPR). In collaboration with a Dutch telecommunications provider facing data scarcity due to low user-consent rates, we developed a platform that allows synthetic data vendors to securely generate synthetic data based on schema input without accessing sensitive information. Vendors uploaded containerized executables for synthetic data generation and the platform automated infrastructure provisioning, ensuring no access to personal data. A validation mechanism minimized the risk of re-identification by ensuring that the synthetic data did not inadvertently replicate real data points. We mutually agreed with the vendors on five evaluation metrics and the platform logged and calculated performance for each, allowing them to refine their algorithms. To validate the platform’s performance, we conducted an offline study with the TV viewership team, using each vendor’s synthetic data to generate viewership categories. The vendor with the best evaluation metrics also produced categories most similar to the real data, confirming the platform’s effectiveness. This study, involving two vendors and a telecommunications company, demonstrated the platform’s applicability in addressing business challenges while ensuring privacy compliance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409921949",
    "type": "article"
  },
  {
    "title": "Runtime Verification via Rational Monitor with Imperfect Information",
    "doi": "https://doi.org/10.1145/3735130",
    "publication_date": "2025-05-10",
    "publication_year": 2025,
    "authors": "Angelo Ferrando; Vadim Malvone",
    "corresponding_authors": "",
    "abstract": "Trusting software systems, particularly autonomous ones, is challenging. To address this, formal verification techniques can ensure these systems behave as expected. Runtime Verification (RV) is a leading, lightweight method for verifying system behaviour during execution. However, traditional RV assumes perfect information, meaning the monitoring component perceives everything accurately. This assumption often fails, especially with autonomous systems operating in real-world environments where sensors might be faulty. Additionally, traditional RV considers the monitor to be passive, lacking the capability to interpret the system's information and thus unable to address incomplete data. In this work, we extend standard RV of Linear Temporal Logic properties to accommodate scenarios where the monitor has imperfect information and behaves rationally. We outline the necessary engineering steps to update the verification pipeline and demonstrate our implementation in a case study involving robotic systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410263610",
    "type": "article"
  },
  {
    "title": "Less is More: DocString Compression in Code Generation",
    "doi": "https://doi.org/10.1145/3735636",
    "publication_date": "2025-05-14",
    "publication_year": 2025,
    "authors": "Guang Yang; Yu Zhou; Wei Cheng; Xiangyu Zhang; Xiang Chen; Terry Yue Zhuo; Ke Liu; Xin Zhou; David Lo; Taolue Chen",
    "corresponding_authors": "",
    "abstract": "The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc , dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters) and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25–40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410361283",
    "type": "article"
  },
  {
    "title": "Do Current Language Models Support Code Intelligence for R Programming Language?",
    "doi": "https://doi.org/10.1145/3735635",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Zixiao Zhao; Fatemeh H. Fard",
    "corresponding_authors": "",
    "abstract": "Recent advancements in developing Pre-trained Language Models for Code (Code-PLMs) have urged many areas of Software Engineering (SE) and brought breakthrough results for many SE tasks. Though these models have achieved the state-of-the-art performance for SE tasks for many popular programming languages, such as Java and Python, the Scientific Software and its related languages like R programming language have rarely benefited or even been evaluated with the Code-PLMs. Research has shown that R has many differences with other programming languages and requires specific techniques. In this study, we provide the first insights for code intelligence for R. For this purpose, we collect and open source an R dataset, and evaluate Code-PLMs for the two tasks of code summarization and method name prediction using several settings and strategies, including the differences in two R styles, Tidy-verse and Base R. Our results demonstrate that the studied models have experienced varying degrees of performance degradation when processing R programming language code, which is supported by human evaluation. Additionally, not all models show performance improvement in R-specific tasks even after multi-language fine-tuning. The dual syntax paradigms in R significantly impact the models’ performance, particularly in code summarization tasks. Furthermore, the project-specific context inherent in R codebases significantly impacts the performance when attempting cross-project training. Interestingly, even when Large Language Models like CodeLlama and StarCoder2 are used for code generation, the Pass@K ( \\(K=1,5,10\\) ) results lags signigicantly behind Python scores. Our research shows that R as a low resource language requires different techniques to collect a high quality data. Specifically separating the two R styles has a great impact on the results and the separate dataset could increase the performance of the models. Our research sheds light on the capabilities of Code-PLMs and opens new research directions for researchers and practitioners for developing code intelligence tools and techniques for R. With R's widespread use and popularity, the results of our study can potentially benefit a large community of R developers, both in research and industry.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410610020",
    "type": "article"
  },
  {
    "title": "CI/CD Configuration Practices in Open-Source Android Apps: An Empirical Study",
    "doi": "https://doi.org/10.1145/3736758",
    "publication_date": "2025-05-23",
    "publication_year": 2025,
    "authors": "Taher Ahmed Ghaleb; Osamah Abduljalil; Safwat Hassan",
    "corresponding_authors": "",
    "abstract": "Continuous Integration and Continuous Delivery (CI/CD) is a well-established practice that automatically builds, tests, packages, and deploys software systems. To adopt CI/CD, software developers need to configure their projects using dedicated YML configuration files. Mobile apps have distinct characteristics with respect to CI/CD practices, such as testing on various emulators and deploying to app stores. However, little is known about the challenges and added value of adopting CI/CD in mobile apps and how developers maintain such a practice. In this paper, we conduct an empirical study on CI/CD practices in \\(2,557\\) Android apps adopting four popular CI/CD services, namely GitHub Actions , Travis CI , CircleCI , and GitLab CI/CD . We also compare our findings with those reported in prior research on general CI/CD practices to situate them within broader trends. We observe a lack of commonality and standardization across CI/CD services and Android apps, leading to complex YML configurations and associated maintenance efforts. We also observe that CI/CD configurations focus primarily on the build setup, with around half of the projects performing standard testing and only \\(9\\%\\) incorporating deployment. In addition, we find that CI/CD configurations are changed bi-monthly on average, with frequent maintenance correlating with active issue tracking, project size/age, and community engagement. Our qualitative analysis of commits uncovered \\(11\\) themes in CI/CD maintenance activities, with over a third of the changes focusing on improving workflows and fixing build issues, whereas another third involves updating the build environment, tools, and dependencies. Our study emphasizes the necessity for automation and AI-powered tools to improve CI/CD processes for mobile apps and advocates creating adaptable open-source tools to efficiently manage resources, especially in testing and deployment.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410644286",
    "type": "article"
  },
  {
    "title": "The Havoc Paradox in Generator-Based Fuzzing",
    "doi": "https://doi.org/10.1145/3742894",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Ao Li; Madonna Huang; Vasudev Vikram; Caroline Lemieux; Rohan Padhye",
    "corresponding_authors": "",
    "abstract": "Parametric generators combine coverage-guided and generator-based fuzzing for testing programs requiring structured inputs. They function as decoders that transform arbitrary byte sequences into structured inputs, allowing mutations on byte sequences to map directly to mutations on structured inputs, without requiring specialized mutators. However, this technique is prone to the havoc effect , where small mutations on the byte sequence cause large, destructive mutations to the structured input. This paper investigates the paradoxical nature of the havoc effect for generator-based fuzzing in Java. In particular, we measure mutation characteristics and confirm the existence of the havoc effect, as well as scenarios where it may be more detrimental. Our evaluation across 7 real-world Java applications compares various techniques that perform context-aware, finer-grained mutations on parametric byte sequences, such as JQF-EI, BeDivFuzz, and Zeugma. We find that these techniques exhibit better control over input mutations and consistently reduce the havoc effect compared to our coverage-guided fuzzer baseline Zest. While we find that context-aware mutation approaches can achieve significantly higher code coverage, we see that destructive mutations still play a valuable role in discovering inputs that increase code coverage. Specialized mutation strategies, while effective, impose substantial computational overhead—revealing practical trade-offs in mitigating the havoc effect.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4411087888",
    "type": "article"
  },
  {
    "title": "FLITSR: Improved Spectrum-Based Localization of Multiple Faults by Iterative Test Suite Reduction",
    "doi": "https://doi.org/10.1145/3745027",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Dylan Callaghan; Bernd Fischer",
    "corresponding_authors": "",
    "abstract": "Spectrum-based fault localization (SBFL) works well for single-fault programs but its accuracy decays for increasing fault numbers. We present FLITSR (Fault Localization by Iterative Test Suite Reduction), a novel SBFL approach that improves the localization of a given SBFL base metric specifically in the presence of multiple faults. FLITSR iteratively selects reduced versions of the test suite that better localize the individual faults in the system. This allows it to identify and re-rank faults ranked too low by the base metric because they were masked by other program elements. Through this process, FLITSR returns a set of highly suspicious program elements (called a basis), where the execution of each failing test involves at least one basis element, considered as the cause of the failure. We implemented the FLITSR algorithm in an open-source toolset and extensively evaluated it over three true multi-fault datasets, varying the fault type, coverage granularity and programming language. Our evaluation shows that FLITSR consistently outperforms existing localization metrics and methods, including those designed to handle multiple faults such as ARTEMIS and parallel debugging. For the Defects4J method-level faults, FLITSR also substantially outperforms GRACE, a state-of-the-art learning-based fault localizer.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4411488284",
    "type": "article"
  },
  {
    "title": "On the Utility of Domain Modeling Assistance with Large Language Models",
    "doi": "https://doi.org/10.1145/3744920",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Meriem Ben Chaaben; Loli Burgueño; István Dávid; Houari Sahraoui",
    "corresponding_authors": "",
    "abstract": "Model-driven engineering (MDE) simplifies software development through abstraction, yet challenges such as time constraints, incomplete domain understanding, and adherence to syntactic constraints hinder the design process. This paper presents a study to evaluate the usefulness of a novel approach utilizing large language models (LLMs) and few-shot prompt learning to assist in domain modeling. The aim of this approach is to overcome the need for extensive training of traditional AI-based completion algorithms on domain-specific datasets and to offer versatile support for various modeling activities, providing valuable recommendations to software modelers. To support this approach, we developed MAGDA, a user-friendly tool, through which we conduct a user study and assess the real-world applicability of our approach in the context of domain modeling, offering valuable insights into its usability and effectiveness.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4411488587",
    "type": "article"
  },
  {
    "title": "LLM-Based Misconfiguration Detection for AWS Serverless Computing",
    "doi": "https://doi.org/10.1145/3745766",
    "publication_date": "2025-06-23",
    "publication_year": 2025,
    "authors": "Jinfeng Wen; Zhenpeng Chen; Zongyang Zhu; Federica Sarro; Yi Liu; Haodi Ping; Shangguang Wang",
    "corresponding_authors": "",
    "abstract": "Serverless computing is a popular cloud computing paradigm that enables developers to build applications at the function level, known as serverless applications. The Serverless Application Model (AWS SAM) is the most widely adopted configuration schema. However, misconfigurations pose a significant challenge due to the complexity of serverless configurations and the limitations of traditional data-driven techniques. Recent advancements in Large Language Models (LLMs), pre-trained on large-scale public data, offer promising potential for identifying and explaining misconfigurations. In this paper, we present SlsDetector , the first framework that harnesses the capabilities of LLMs to perform static misconfiguration detection in serverless applications. SlsDetector utilizes effective prompt engineering with zero-shot prompting to identify configuration issues. It designs multi-dimensional constraints aligned with serverless configuration characteristics and leverages the Chain of Thought technique to enhance LLM inferences, alongside generating structured responses. We evaluate SlsDetector on a curated dataset of 110 configuration files, which includes correct configurations, real-world misconfigurations, and intentionally injected errors. Our results show that SlsDetector , based on ChatGPT-4o (one of the most representative LLMs), achieves a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%, outperforming state-of-the-art data-driven methods by 53.82, 17.40, and 49.72 percentage points, respectively. We further investigate the generalization capability of SlsDetector across recent LLMs, including Llama 3.1 (405B) Instruct Turbo, Gemini 1.5 Pro, and DeepSeek V3, with consistently high effectiveness.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4411563239",
    "type": "article"
  },
  {
    "title": "Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners’ Perspectives",
    "doi": "https://doi.org/10.1145/3748721",
    "publication_date": "2025-07-16",
    "publication_year": 2025,
    "authors": "Lidiany Cerqueira; João Pedro Silva Bastos; Danilo Ferreira Neves; Glauco de Figueiredo Carneiro; Rodrigo Spínola; Sávio Freire; José Amâncio Macedo Santos; Manoel Mendonça",
    "corresponding_authors": "",
    "abstract": "Context . Empathy, a key social skill, is essential for communication and collaboration in SE but remains an under-researched topic. Aims . This study investigates empathy in SE from practitioners’ perspectives, aiming to characterize its meaning, identify barriers, discuss practices to overcome them, and explore its effects. Method . A qualitative content analysis was conducted on 55 web articles from DEV and Medium, two communities widely used by practitioners. To strengthen our findings, we conducted a follow-up survey with empathy experts. Results . The study proposes a definition of empathy in SE, identifies barriers such as toxic culture and excessive technical focus, practices to foster empathy in teams, and outcomes, including improved collaboration, communication, and reduced anxiety, frustration, and stress. These findings are synthesized into a conceptual framework. Conclusion . Survey results indicate the framework is clear, valuable, and raises empathy awareness, with suggestions for improvements and integration into training. This study paves the way for improving team dynamics by addressing barriers and offering strategies to cultivate empathy. Future work will explore empathy's broader implications in SE practice.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4412477610",
    "type": "article"
  },
  {
    "title": "Detecting Outdated Screenshot from GUI Document",
    "doi": "https://doi.org/10.1145/3750041",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Ye Tang; Aoyang Yan; Hui Liu; Na Meng; Hao Zhong",
    "corresponding_authors": "",
    "abstract": "In software development, many documents ( e.g. , tutorials for tools and mobile application websites) contain screenshots of graphical user interfaces (GUIs) to illustrate functionalities. Although screenshots are critical in such documents, screenshots can become outdated, especially if document developers forget to update them. Outdated screenshots can mislead users and diminish the credibility of documentation. Identifying screenshots manually is tedious and error-prone, especially when documents are numerous. However, no existing tools are proposed to detect outdated screenshots in GUI documents. To mitigate manual efforts, we propose DOSUD, a novel approach for detecting outdated screenshots. It is challenging to identify outdated screenshots since the differences are subtle and only specific areas are useful to identify such screenshots. To address the challenges, DOSUD automatically extracts and labels screenshots and trains a classification model to identify outdated screenshots. As the first exploration, we focus on Android applications and the most popular IDE, VS Code. We evaluated DOSUD on a benchmark comprising 10 popular applications, achieving high F1-scores. When applied in the wild, DOSUD identified 20 outdated screenshots across 50 Android application websites and 17 outdated screenshots in VS Code documentation. VS Code developers have confirmed and fixed all our bug reports.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4412602610",
    "type": "article"
  },
  {
    "title": "Fault Localization from the Semantic Code Search Perspective",
    "doi": "https://doi.org/10.1145/3757915",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Yihao Qin; Shangwen Wang; Yan Lei; Zhuo Zhang; Bo Lin; Xin Peng; Jun Ma; Liqian Chen; Xiaoguang Mao",
    "corresponding_authors": "",
    "abstract": "The software development process is characterized by an iterative cycle of continuous functionality implementation and debugging, essential for the enhancement of software quality and adaptability to changing requirements. This process incorporates two isolatedly studied tasks: Code Search (CS), which retrieves reference code from a code corpus to aid in code implementation, and Fault Localization (FL), which identifies code entities responsible for bugs within the software project to boost software debugging. The basic observation of this study is that these two tasks exhibit similarities since they both address search problems. Notably, CS techniques have demonstrated greater effectiveness than FL ones, possibly because of the precise semantic details of the required code offered by natural language queries, which are not readily accessible to FL methods. Drawing inspiration from this, we hypothesize that a fault localizer could achieve greater proficiency if semantic information about the buggy methods were made available. Based on this idea, we propose \\(\\mathtt{CosFL}\\) , an FL approach that decomposes the FL task into two steps: query generation , which describes the functionality of the problematic code in natural language, and fault retrieval , which uses CS to find program elements semantically related to the query, allowing for finishing the FL task from a CS perspective. Specifically, to depict the buggy functionalities and generate high-quality queries, \\(\\mathtt{CosFL}\\) extensively harnesses the code analysis, semantic comprehension, text generation, and decision-making capabilities of LLMs. Moreover, to enhance the accuracy of CS, \\(\\mathtt{CosFL}\\) captures varying levels of context information and employs a multi-granularity code search strategy, which facilitates a more precise identification of buggy methods from a holistic view. The evaluation on 835 real bugs from 23 Java projects shows that \\(\\mathtt{CosFL}\\) successfully localizes 324 bugs within Top-1, which significantly outperforms the state-of-the-art approaches by 26.6%-57.3%. The ablation study and sensitivity analysis further validate the importance of different components and the robustness of \\(\\mathtt{CosFL}\\) across different backend models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4413635666",
    "type": "article"
  },
  {
    "title": "A Survey on LLM-based Code Generation for Low-Resource and Domain-Specific Programming Languages",
    "doi": "https://doi.org/10.1145/3770084",
    "publication_date": "2025-10-07",
    "publication_year": 2025,
    "authors": "Sathvik Joel; Jie JW Wu; Fatemeh H. Fard",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in code generation for popular programming languages. However, their performance in Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs) remains a critical challenge. This gap affects millions of developers - with Rust alone having 3.5 million users - who are currently unable to fully leverage LLM capabilities. LRPLs and DSLs face unique challenges, including severe data scarcity and, for DSLs, highly specialized syntax and semantics that are poorly represented in general-purpose datasets. Addressing these challenges is crucial as LRPLs and DSLs significantly enhance development efficiency in specialized domains and applications, including financial and scientific works. While several surveys on LLMs for software engineering and code exist, none comprehensively address the challenges and opportunities specific to LRPLs and DSLs. Our survey fills this gap by providing a systematic review of the current state, methodologies, and challenges in leveraging LLMs for code generation in LRPL and DSL. We filtered 111 papers from over 27,000 published studies from 2020 – 2024 to understand the capabilities and limitations of LLMs in these specialized domains. We also expanded our literature search to include 5 recent papers from 2024 – 2025. We report LLMs used, benchmarks, and metrics to evaluate code generation in LRPLs and DSLs, as well as strategies used to enhance LLM performance, and the collected datasets and curation methods in this context. We identified four main evaluation techniques used in the literature, along with several metrics to assess code generation in LRPL and DSL. We categorized the methods used for LLM improvement into six main groups and summarized the novel methods and architectures proposed by the researchers. We also classified different approaches used for data collection and preparation. While different techniques, metrics, and datasets are used, there is a lack of a standard approach and a benchmark dataset to evaluate code generation in several LRPLs and DSLs. We discuss several distinctions of the studied approaches with the ones used in high-resource programming languages (HRPLs), as well as several challenges unique to these languages, especially DSLs. The challenges stem from the scarcity of data, the unique requirements, and specialized domains, which often need expertise guidelines or domain-specific tools. Accordingly, we provide insights into different research opportunities for the studied aspects. This survey serves as a comprehensive resource for researchers and practitioners working at the intersection of LLMs, software engineering, and specialized programming languages, providing a foundation for future advancements in LRPL and DSL code generation. A GitHub repository was created to organize the papers of this survey at https://github.com/jie-jw-wu/Survey-CodeLLM4LowResource-DSL .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4414913062",
    "type": "article"
  },
  {
    "title": "PRIME—toward process-integrated modeling environments",
    "doi": "https://doi.org/10.1145/322993.322995",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "Klaus Pohl; K. Weidenhaupt; Ralf Dömges; Peter Haumer; Matthias Jarke; Ralf Klamma",
    "corresponding_authors": "",
    "abstract": "Research in process-centered environments (PCEs) has focused on project management support and has neglected method guidance for the engineers performing the (software) engineering process. It has been dominated by the search for suitable process-modeling languages and enactment mechanisms. The consequences of process orientation on the computer-based engineering environments, i.e., the interactive tools used during process performance, have been studied much less. In this article, we present the PRIME (Process Integrated Modeling Environments) framework which empowers method guidance through process-integrated tools. In contrast to the tools of PCEs, the process-integrated tools of PRIME adjust their behavior according to the current process situation and the method definitions. Process integration of PRIME tools is achieved through (1) the definition of tool models; (2) the integration of the tool models and the method definitions; (3) the interpretation of the integrated environment model by the tools, the process-aware control integration mechanism, and the enactment mechanism; and (4) the synchronization of the tools and the enactment mechanism based on a comprehensive interaction protocol. We sketch the implementation of PRIME as a reusable implementation framework which facilitates the realization of process-integrated tools as well as the process integration of external tools. We define a six-step procedure for building a PRIME-based process-integrated environment (PIE) and illustrate how PRIME facilitates change integration on an easy-to-adapt modeling level.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W1987634577",
    "type": "article"
  },
  {
    "title": "From formal models to formally based methods",
    "doi": "https://doi.org/10.1145/295558.295566",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Emanuele Ciapessoni; P. Mirandola; Alberto Coen‐Porisini; Dino Mandrioli; Angelo Morzenti",
    "corresponding_authors": "",
    "abstract": "We address the problem of increasing the impact of formal methods in the practice of industrial computer applications. We summarize the reasons why formal methods so far did not gain widespead use within the industrial environment despite several promising experiences. We suggest an evolutionary rather than revolutionary attitude in the introduction of formal methods in the practice of industrial applications, and we report on our long-standing experience which involves an academic institution. Politecnico di Milano, two main industrial partners, ENEL and CISE, and occasionally a few other industries. Our approach aims at augmenting an existing and fairly deeply rooted informal industrial methodology with our original formalism, the logic specification language TRIO. On the basis of the experiences we gained we argue that our incremental attitude toward the introduction of formal methods within the industry could be effective largely independently from the chosen formalism.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1965539666",
    "type": "article"
  },
  {
    "title": "Hybrid slicing",
    "doi": "https://doi.org/10.1145/261640.261644",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "Rajiv Gupta; Mary Lou Soffa; John Howard",
    "corresponding_authors": "",
    "abstract": "Program slicing is an effective techniqe for narrowing the focus of attention to the relevant parts of a program during the debugging process. However, imprecision is a problem in static slices, since they are based on all possible executions that reach a given program point rather than the specific execution under which the program is being debugged. Dynamic slices, based on the specific execution being debugged, are precise but incur high run-time overhead due to the tracing information that is collected during the program's execution. We present a hybrid slicing technique that integrates dynamic information from a specific execution into a static slice analysis. The hybrid slice produced is more precise that the static slice and less costly that the dynamic slice. The technique exploits dynamic information that is readily available during debugging—namely, breakpoint information and the dynamic call graph. This information is integrated into a static slicing analysis to more accurately estimate the potential paths taken by the program. The breakpoints and call/return points, used as reference points, divide the execution path into intervals. By associating each statement in the slice with an execution interval, hybrid slicing provides information as to when a statement was encountered during execution. Another attractive feature of our approach is that it allows the user to control the cost of hybrid slicing by limiting the amount of dynamic information used in computing the slice. We implemented the hybrid slicing technique to demonstrate the feasibility of our approach.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W1969962573",
    "type": "article"
  },
  {
    "title": "Feature specification and automated conflict detection",
    "doi": "https://doi.org/10.1145/839268.839270",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Amy Felty; Kedar S. Namjoshi",
    "corresponding_authors": "",
    "abstract": "Large software systems, especially in the telecommunications field, are often specified as a collection of features. We present a formal specification language for describing features, and a method of automatically detecting conflicts (\"undesirable interactions\") amongst features at the specification stage. Conflict detection at this early stage can help prevent costly and time consuming problem fixes during implementation. Features are specified using temporal logic; two features conflict essentially if their specifications are mutually inconsistent under axioms about the underlying system behavior. We show how this inconsistency check may be performed automatically with existing model checking tools. In addition, the model checking tools can be used to provide witness scenarios, both when two features conflict as well as when the features are mutually consistent. Both types of witnesses are useful for refining the specifications. We have implemented a conflict detection tool, FIX (Feature Interaction eXtractor), which uses the model checker COSPAN for the inconsistency check. We describe our experience in applying this tool to a collection of telecommunications feature specifications obtained from the Telcordia (Bellcore) standards. Using FIX, we were able to detect most known interactions and some new ones, fully automatically, in a few hours processing time.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2068726498",
    "type": "article"
  },
  {
    "title": "On fault classes and error detection capability of specification-based testing",
    "doi": "https://doi.org/10.1145/504087.504089",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Tatsuhiro Tsuchiya; Tohru Kikuno",
    "corresponding_authors": "",
    "abstract": "In a previous paper, Kuhn [1999] showed that faults in Boolean specifications constitute a hierarchy with respect to detectability, and drew the conclusion that missing condition faults should be hypothesized to generate tests. However this conclusion was premature, since the relationships between missing condition faults and faults in other classes have not been sufficiently analyzed. In this note, we investigate such relationships, aiming to complement the work of Kuhn. As a result, we obtain an extended hierarchy of fault classes and reach a different conclusion.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2028828983",
    "type": "article"
  },
  {
    "title": "A model-checking verification environment for mobile processes",
    "doi": "https://doi.org/10.1145/990010.990013",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Gian-Luigi Ferrari; Stefania Gnesi; Ugo Montanari; Marco Pistore",
    "corresponding_authors": "",
    "abstract": "This article presents a semantic-based environment for reasoning about the behavior of mobile systems. The verification environment, called HAL, exploits a novel automata-like model that allows finite-state verification of systems specified in the π-calculus. The HAL system is able to interface with several efficient toolkits (e.g. model-checkers) to determine whether or not certain properties hold for a given specification. We report experimental results on some case studies.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2070368511",
    "type": "article"
  },
  {
    "title": "Validating real-time systems by history-checking TRIO specifications",
    "doi": "https://doi.org/10.1145/201024.201034",
    "publication_date": "1994-10-01",
    "publication_year": 1994,
    "authors": "Miguel Felder; Angelo Morzenti",
    "corresponding_authors": "",
    "abstract": "We emphasize the importance of formal executable specifications in the development of real-time systems, as a means to assess the adequacy of the requirements before a costly development process takes place. TRIO is a first-order temporal logic language for executable specification of real-time systems that deals with time in a quantitative way by providing a metric to indicate distance in time between events and length of time intervals. We summarize the language and its model-parametric semantics. Then we present an algorithm to perform history checking, i.e., to check that a history of the system satisfies the specification. This algorithm can be used as a basis for an effective specification testing tool. The algorithm is described; an estimation of its complexity is provided; and the main functionalities of the tool are presented, together with sample test cases. Finally, we draw conclusions and indicate directions of future research.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2161166453",
    "type": "article"
  },
  {
    "title": "Interprocedural static analysis of sequencing constraints",
    "doi": "https://doi.org/10.1145/125489.122822",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "K. Olender; Leon J. Osterweil",
    "corresponding_authors": "",
    "abstract": "This paper describes a system that automatically performs static interprocedural sequencing analysis from programmable constraint specifications. We describe the algorithms used for interprocedural analysis, relate the problems arising from the analysis of real-world programs, and show how these difficulties were overcome. Finally, we sketch the architecture of our prototype analysis system (called Cesar) and describe our experiences to date with its use, citing performance and error detection characteristics.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2122875187",
    "type": "article"
  },
  {
    "title": "The impact of software engineering research on modern programming languages",
    "doi": "https://doi.org/10.1145/1101815.1101818",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "Barbara G. Ryder; Mary Lou Soffa; Margaret Burnett",
    "corresponding_authors": "",
    "abstract": "Software engineering research and programming language design have enjoyed a symbiotic relationship, with traceable impacts since the 1970s, when these areas were first distinguished from one another. This report documents this relationship by focusing on several major features of current programming languages: data and procedural abstraction, types, concurrency, exceptions, and visual programming mechanisms. The influences are determined by tracing references in publications in both fields, obtaining oral histories from language designers delineating influences on them, and tracking cotemporal research trends and ideas as demonstrated by workshop topics, special issue publications, and invited talks in the two fields. In some cases there is conclusive data supporting influence. In other cases, there are circumstantial arguments (i.e., cotemporal ideas) that indicate influence. Using this approach, this study provides evidence of the impact of software engineering research on modern programming language design and documents the close relationship between these two fields.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2133452779",
    "type": "article"
  },
  {
    "title": "Integrating automated test generation into the WYSIWYT spreadsheet testing methodology",
    "doi": "https://doi.org/10.1145/1131421.1131423",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "Marc Fisher; Gregg Rothermel; Darren Brown; Mingming Cao; Curtis R. Cook; Margaret Burnett",
    "corresponding_authors": "",
    "abstract": "Spreadsheet languages, which include commercial spreadsheets and various research systems, have had a substantial impact on end-user computing. Research shows, however, that spreadsheets often contain faults. Thus, in previous work we presented a methodology that helps spreadsheet users test their spreadsheet formulas. Our empirical studies have shown that end users can use this methodology to test spreadsheets more adequately and efficiently; however, the process of generating test cases can still present a significant impediment. To address this problem, we have been investigating how to incorporate automated test case generation into our testing methodology in ways that support incremental testing and provide immediate visual feedback. We have used two techniques for generating test cases, one involving random selection and one involving a goal-oriented approach. We describe these techniques and their integration into our testing environment, and report results of an experiment examining their effectiveness and efficiency.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2134600833",
    "type": "article"
  },
  {
    "title": "Foundations of incremental aspect model-checking",
    "doi": "https://doi.org/10.1145/1217295.1217296",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Shriram Krishnamurthi; Kathi Fisler",
    "corresponding_authors": "",
    "abstract": "Programs are increasingly organized around features, which are encapsulated using aspects and other linguistic mechanisms. Despite their growing popularity amongst developers, there is a dearth of techniques for computer-aided verification of programs that employ these mechanisms. We present the theoretical underpinnings for applying model checking to programs (expressed as state machines) written using these mechanisms. The analysis is incremental, examining only components that change rather than verifying the entire system every time one part of it changes. Our technique assumes that the set of pointcut designators is known statically, but the actual advice can vary. It handles both static and dynamic pointcut designators. We present the algorithm, prove it sound, and address several subtleties that arise, including cascading advice application and problems of circular reasoning.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2075807120",
    "type": "article"
  },
  {
    "title": "Extracting and answering why and why not questions about Java program output",
    "doi": "https://doi.org/10.1145/1824760.1824761",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Amy J. Ko; Brad A. Myers",
    "corresponding_authors": "",
    "abstract": "When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of “why did and why didn't” questions extracted from the program's code and execution. The tool then finds one or more possible explanations for the output in question. These explanations are derived using a static and dynamic slicing, precise call graphs, reachability analyses, and new algorithms for determining potential sources of values. Evaluations of the tool on two debugging tasks showed that developers with the Whyline were three times more successful and twice as fast at debugging, compared to developers with traditional breakpoint debuggers. The tool has the potential to simplify debugging and program understanding in many software development contexts.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1973924171",
    "type": "article"
  },
  {
    "title": "Modular aspect-oriented design with XPIs",
    "doi": "https://doi.org/10.1145/1824760.1824762",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Kevin Sullivan; William G. Griswold; Hridesh Rajan; Yuanyuan Song; Yuanfang Cai; Macneil Shonle; Nishit Tewari",
    "corresponding_authors": "",
    "abstract": "The emergence of aspect-oriented programming (AOP) languages has provided software designers with new mechanisms and strategies for decomposing programs into modules and composing modules into systems. What we do not yet fully understand is how best to use such mechanisms consistent with common modularization objectives such as the comprehensibility of programming code, its parallel development, dependability, and ease of change. The main contribution of this work is a new form of information-hiding interface for AOP that we call the crosscut programming interface, or XPI. XPIs abstract crosscutting behaviors and make these abstractions explicit. XPIs can be used, albeit with limited enforcement of interface rules, with existing AOP languages, such as AspectJ. To evaluate our notion of XPIs, we have applied our XPI-based design methodology to a medium-sized network overlay application called Hypercast. A qualitative and quantitative analysis of existing AO design methods and XPI-based design method shows that our approach produces improvements in program comprehensibility, in opportunities for parallel development, and in the ease when code can be developed and changed.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2157407613",
    "type": "article"
  },
  {
    "title": "Synthesizing hierarchical state machines from expressive scenario descriptions",
    "doi": "https://doi.org/10.1145/1656250.1656252",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Jon Whittle; Praveen Jayaraman",
    "corresponding_authors": "",
    "abstract": "There are many examples in the literature of algorithms for synthesizing state machines from scenario-based models. The motivation for these is to automate the transition from scenario-based requirements to early behavioral design models. To date, however, these synthesis algorithms have tended to generate flat state machines which can be difficult to understand or adapt for practical systems. One of the reasons for this is that relationships between scenarios are often not taken into account during synthesis—either because the relationships are not explicitly defined or because the synthesis algorithms are not sophisticated enough to cope with them. If relationships are not considered, it is impossible for a synthesis algorithm to know, for example, where one scenario stops and another continues. Furthermore, the lack of relationships makes it difficult to introduce structure into the generated state machines. With the introduction of interaction overview diagrams (IODs) in UML2.0, relationships such as continuation and concurrency can now be specified between scenarios in a way that conforms to the UML standard. But synthesis algorithms do not currently exist that take into account all of these relationships. This article presents a novel synthesis algorithm for an extended version of interaction overview diagram. This algorithm takes into account not only continuation and concurrency, but also preemption, suspension and the notion of a negative scenario. Furthermore, the synthesis algorithm generates well-structured state machines. These state machines are executable and can therefore be used to validate the scenarios. The hierarchy generated aids readability and so the state machines are more amenable to subsequent design steps. Our IOD extensions have a formal semantics and are supported by a synthesis and execution tool, UCSIM, which is currently provided as a plug-in to IBM Rational Software Modeler.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2044303677",
    "type": "article"
  },
  {
    "title": "FlagRemover",
    "doi": "https://doi.org/10.1145/2000791.2000796",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "David Binkley; Mark Harman; Kiran Lakhotia",
    "corresponding_authors": "",
    "abstract": "Search-Based Testing is a widely studied technique for automatically generating test inputs, with the aim of reducing the cost of software engineering activities that rely upon testing. However, search-based approaches degenerate to random testing in the presence of flag variables, because flags create spikes and plateaux in the fitness landscape. Both these features are known to denote hard optimization problems for all search-based optimization techniques. Several authors have studied flag removal transformations and fitness function refinements to address the issue of flags, but the problem of loop-assigned flags remains unsolved. This article introduces a testability transformation along with a tool that transforms programs with loop-assigned flags into flag-free equivalents, so that existing search-based test data generation approaches can successfully be applied. The article presents the results of an empirical study that demonstrates the effectiveness and efficiency of the testability transformation on programs including those made up of open source and industrial production code, as well as test data generation problems specifically created to denote hard optimization problems.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2071365726",
    "type": "article"
  },
  {
    "title": "A logical verification methodology for service-oriented computing",
    "doi": "https://doi.org/10.1145/2211616.2211619",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Alessandro Fantechi; Stefania Gnesi; Alessandro Lapadula; Franco Mazzanti; Rosario Pugliese; Francesco Tiezzi",
    "corresponding_authors": "",
    "abstract": "We introduce a logical verification methodology for checking behavioral properties of service-oriented computing systems. Service properties are described by means of SocL, a branching-time temporal logic that we have specifically designed for expressing in an effective way distinctive aspects of services, such as, acceptance of a request, provision of a response, correlation among service requests and responses, etc. Our approach allows service properties to be expressed in such a way that they can be independent of service domains and specifications. We show an instantiation of our general methodology that uses the formal language COWS to conveniently specify services and the expressly developed software tool CMC to assist the user in the task of verifying SocL formulas over service specifications. We demonstrate the feasibility and effectiveness of our methodology by means of the specification and analysis of a case study in the automotive domain.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2142693631",
    "type": "article"
  },
  {
    "title": "Generating API Call Rules from Version History and Stack Overflow Posts",
    "doi": "https://doi.org/10.1145/2990497",
    "publication_date": "2017-01-09",
    "publication_year": 2017,
    "authors": "Shams Azad; Peter C. Rigby; Latifa Guerrouj",
    "corresponding_authors": "",
    "abstract": "Researchers have shown that related functions can be mined from groupings of functions found in the version history of a system. Our first contribution is to expand this approach to a community of applications and set of similar applications. Android developers use a set of application programming interface (API) calls when creating apps. These API calls are used in similar ways across multiple applications. By clustering co-changing API calls used by 230 Android apps across 12k versions, we are able to predict the API calls that individual app developers will use with an average precision of 75% and recall of 22%. When we make predictions from the same category of app, such as Finance, we attain precision and recall of 81% and 28%, respectively. Our second contribution can be characterized as “programmers who discussed these functions were also interested in these functions.” Informal discussions on Stack Overflow provide a rich source of information about related API calls as developers provide solutions to common problems. By grouping API calls contained in each positively voted answer posts, we are able to create rules that predict the calls that app developers will use in their own apps with an average precision of 66% and recall of 13%. For comparison purposes, we developed a baseline by clustering co-changing API calls for each individual app and generated association rules from them. The baseline predicts API calls used by app developers with a precision and recall of 36% and 23%, respectively.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2570857834",
    "type": "article"
  },
  {
    "title": "Weak Alphabet Merging of Partial Behavior Models",
    "doi": "https://doi.org/10.1145/2089116.2089119",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Dario Fischbein; Nicolás D’Ippolito; Greg Brunet; Marsha Chećhik; Sebastián Uchitel",
    "corresponding_authors": "",
    "abstract": "Constructing comprehensive operational models of intended system behavior is a complex and costly task, which can be mitigated by the construction of partial behavior models, providing early feedback and subsequently elaborating them iteratively. However, how should partial behavior models with different viewpoints covering different aspects of behavior be composed? How should partial models of component instances of the same type be put together? In this article, we propose model merging of modal transition systems (MTSs) as a solution to these questions. MTS models are a natural extension of labelled transition systems that support explicit modeling of what is currently unknown about system behavior. We formally define model merging based on weak alphabet refinement, which guarantees property preservation, and show that merging consistent models is a process that should result in a minimal common weak alphabet refinement (MCR). In this article, we provide theoretical results and algorithms that support such a process. Finally, because in practice MTS merging is likely to be combined with other operations over MTSs such as parallel composition, we also study the algebraic properties of merging and apply these, together with the algorithms that support MTS merging, in a case study.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1988506401",
    "type": "article"
  },
  {
    "title": "Automated Comparison of State-Based Software Models in Terms of Their Language and Structure",
    "doi": "https://doi.org/10.1145/2430545.2430549",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Neil Walkinshaw; Kirill Bogdanov",
    "corresponding_authors": "",
    "abstract": "State machines capture the sequential behavior of software systems. Their intuitive visual notation, along with a range of powerful verification and testing techniques render them an important part of the model-driven software engineering process. There are several situations that require the ability to identify and quantify the differences between two state machines (e.g. to evaluate the accuracy of state machine inference techniques is measured by the similarity of a reverse-engineered model to its reference model). State machines can be compared from two complementary perspectives: (1) In terms of their language -- the externally observable sequences of events that are permitted or not, and (2) in terms of their structure -- the actual states and transitions that govern the behavior. This article describes two techniques to compare models in terms of these two perspectives. It shows how the difference can be quantified and measured by adapting existing binary classification performance measures for the purpose. The approaches have been implemented by the authors, and the implementation is openly available. Feasibility is demonstrated via a case study to compare two real state machine inference approaches. Scalability and accuracy are assessed experimentally with respect to a large collection of randomly synthesized models.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2067007638",
    "type": "article"
  },
  {
    "title": "Detecting missing method calls as violations of the majority rule",
    "doi": "https://doi.org/10.1145/2430536.2430541",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Martin Monperrus; Mira Mezini",
    "corresponding_authors": "",
    "abstract": "When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this article, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system that searches for missing method calls in software based on the other method calls that are observable. Our key insight is that the voting theory concept of majority rule holds for method calls: a call is likely to be missing if there is a majority of similar pieces of code where this call is present. The evaluation shows that the system predictions go further missing method calls and often reveal different kinds of code smells (e.g., violations of API best practices).",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3125551030",
    "type": "article"
  },
  {
    "title": "Multi-Objective Optimization of Energy Consumption of GUIs in Android Apps",
    "doi": "https://doi.org/10.1145/3241742",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "Mario Linares‐Vásquez; Gabriele Bavota; Carlos Bernal-Cárdenas; Massimiliano Di Penta; Rocco Oliveto; Denys Poshyvanyk",
    "corresponding_authors": "",
    "abstract": "The number of mobile devices sold worldwide has exponentially increased in recent years, surpassing that of personal computers in 2011. Such devices daily download and run millions of apps that take advantage of modern hardware features (e.g., multi-core processors, large Organic Light-Emitting Diode—OLED—screens, etc.) to offer exciting user experiences. Clearly, there is a cost to pay in terms of energy consumption and, in particular, of reduced battery life. This has pushed researchers to investigate how to reduce the energy consumption of apps, for example, by optimizing the color palette used in the app’s GUI. Whilst past research in this area aimed at optimizing energy while keeping an acceptable level of contrast, this article proposes an approach, named Gui Energy Multi-objective optiMization for Android apps (GEMMA), for generating color palettes using a multi-objective optimization technique, which produces color solutions optimizing energy consumption and contrast while using consistent colors with respect to the original color palette. The empirical evaluation demonstrates (i) substantial improvements in terms of the three different objectives, (ii) a concrete reduction of the energy consumption as assessed by a hardware power monitor, (iii) the attractiveness of the generated color compositions for apps’ users, and (iv) the suitability of GEMMA to be adopted in industrial contexts.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2894365172",
    "type": "article"
  },
  {
    "title": "Documenting Design-Pattern Instances",
    "doi": "https://doi.org/10.1145/2699696",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Giuseppe Scanniello; Carmine Gravino; Michele Risi; Genoveffa Tortora; Gabriella Dodero",
    "corresponding_authors": "",
    "abstract": "Design patterns are recognized as a means to improve software maintenance by furnishing an explicit specification of class and object interactions and their underlying intent [Gamma et al. 1995]. Only a few empirical investigations have been conducted to assess whether the kind of documentation for design patterns implemented in source code affects its comprehensibility. To investigate this aspect, we conducted a family of four controlled experiments with 88 participants having different experience (i.e., professionals and Bachelor, Master, and PhD students). In each experiment, the participants were divided into three groups and asked to comprehend a nontrivial chunk of an open-source software system. Depending on the group, each participant was, or was not, provided with graphical or textual representations of the design patterns implemented within the source code. We graphically documented design-pattern instances with UML class diagrams. Textually documented instances are directly reported source code as comments. Our results indicate that documenting design-pattern instances yields an improvement in correctness of understanding source code for those participants with an adequate level of experience.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2205176590",
    "type": "article"
  },
  {
    "title": "The Effectiveness of Test Coverage Criteria for Relational Database Schema Integrity Constraints",
    "doi": "https://doi.org/10.1145/2818639",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Phil McMinn; Chris J. Wright; Gregory M. Kapfhammer",
    "corresponding_authors": "",
    "abstract": "Despite industry advice to the contrary, there has been little work that has sought to test that a relational database's schema has correctly specified integrity constraints. These critically important constraints ensure the coherence of data in a database, defending it from manipulations that could violate requirements such as “usernames must be unique” or “the host name cannot be missing or unknown.” This article is the first to propose coverage criteria, derived from logic coverage criteria, that establish different levels of testing for the formulation of integrity constraints in a database schema. These range from simple criteria that mandate the testing of successful and unsuccessful INSERT statements into tables to more advanced criteria that test the formulation of complex integrity constraints such as multi-column PRIMARY KEYs and arbitrary CHECK constraints. Due to different vendor interpretations of the structured query language (SQL) specification with regard to how integrity constraints should actually function in practice, our criteria crucially account for the underlying semantics of the database management system (DBMS). After formally defining these coverage criteria and relating them in a subsumption hierarchy, we present two approaches for automatically generating tests that satisfy the criteria. We then describe the results of an empirical study that uses mutation analysis to investigate the fault-finding capability of data generated when our coverage criteria are applied to a wide variety of relational schemas hosted by three well-known and representative DBMSs—HyperSQL, PostgreSQL, and SQLite. In addition to revealing the complementary fault-finding capabilities of the presented criteria, the results show that mutation scores range from as low as just 12% of mutants being killed with the simplest of criteria to 96% with the most advanced.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2266149187",
    "type": "article"
  },
  {
    "title": "The State of Empirical Evaluation in Static Feature Location",
    "doi": "https://doi.org/10.1145/3280988",
    "publication_date": "2018-12-05",
    "publication_year": 2018,
    "authors": "Abdul Razzaq; Asanka Wasala; Chris Exton; Jim Buckley",
    "corresponding_authors": "",
    "abstract": "Feature location (FL) is the task of finding the source code that implements a specific, user-observable functionality in a software system. It plays a key role in many software maintenance tasks and a wide variety of Feature Location Techniques (FLTs), which rely on source code structure or textual analysis, have been proposed by researchers. As FLTs evolve and more novel FLTs are introduced, it is important to perform comparison studies to investigate “Which are the best FLTs?” However, an initial reading of the literature suggests that performing such comparisons would be an arduous process, based on the large number of techniques to be compared, the heterogeneous nature of the empirical designs, and the lack of transparency in the literature. This article presents a systematic review of 170 FLT articles, published between the years 2000 and 2015. Results of the systematic review indicate that 95% of the articles studied are directed towards novelty, in that they propose a novel FLT. Sixty-nine percent of these novel FLTs are evaluated through standard empirical methods but, of those, only 9% use baseline technique(s) in their evaluations to allow cross comparison with other techniques. The heterogeneity of empirical evaluation is also clearly apparent: altogether, over 60 different FLT evaluation metrics are used across the 170 articles, 272 subject systems have been used, and 235 different benchmarks employed. The review also identifies numerous user input formats as contributing to the heterogeneity. Analysis of the existing research also suggests that only 27% of the FLTs presented might be reproduced from the published material. These findings suggest that comparison across the existing body of FLT evaluations is very difficult. We conclude by providing guidelines for empirical evaluation of FLTs that may ultimately help to standardise empirical research in the field, cognisant of FLTs with different goals, leveraging common practices in existing empirical evaluations and allied with rationalisations. This is seen as a step towards standardising evaluation in the field, thus facilitating comparison across FLTs.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2903382629",
    "type": "article"
  },
  {
    "title": "Monotone Precision and Recall Measures for Comparing Executions and Specifications of Dynamic Systems",
    "doi": "https://doi.org/10.1145/3387909",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Artem Polyvyanyy; Andreas Solti; Matthias Weidlich; Claudio Di Ciccio; Jan Mendling",
    "corresponding_authors": "",
    "abstract": "The behavioural comparison of systems is an important concern of software engineering research. For example, the areas of specification discovery and specification mining are concerned with measuring the consistency between a collection of execution traces and a program specification. This problem is also tackled in process mining with the help of measures that describe the quality of a process specification automatically discovered from execution logs. Though various measures have been proposed, it was recently demonstrated that they neither fulfil essential properties, such as monotonicity , nor can they handle infinite behaviour. In this article, we address this research problem by introducing a new framework for the definition of behavioural quotients. We prove that corresponding quotients guarantee desired properties that existing measures have failed to support. We demonstrate the application of the quotients for capturing precision and recall measures between a collection of recorded executions and a system specification. We use a prototypical implementation of these measures to contrast their monotonic assessment with measures that have been defined in prior research.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2903974159",
    "type": "article"
  },
  {
    "title": "An Active Learning Approach for Improving the Accuracy of Automated Domain Model Extraction",
    "doi": "https://doi.org/10.1145/3293454",
    "publication_date": "2019-01-07",
    "publication_year": 2019,
    "authors": "Chetan Arora; Mehrdad Sabetzadeh; Shiva Nejati; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Domain models are a useful vehicle for making the interpretation and elaboration of natural-language requirements more precise. Advances in natural-language processing (NLP) have made it possible to automatically extract from requirements most of the information that is relevant to domain model construction. However, alongside the relevant information, NLP extracts from requirements a significant amount of information that is superfluous (not relevant to the domain model). Our objective in this article is to develop automated assistance for filtering the superfluous information extracted by NLP during domain model extraction. To this end, we devise an active-learning-based approach that iteratively learns from analysts’ feedback over the relevance and superfluousness of the extracted domain model elements and uses this feedback to provide recommendations for filtering superfluous elements. We empirically evaluate our approach over three industrial case studies. Our results indicate that, once trained, our approach automatically detects an average of ≈ 45% of the superfluous elements with a precision of ≈ 96%. Since precision is very high, the automatic recommendations made by our approach are trustworthy. Consequently, analysts can dispose of a considerable fraction – nearly half – of the superfluous elements with minimal manual work. The results are particularly promising, as they should be considered in light of the non-negligible subjectivity that is inherently tied to the notion of relevance.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2909731777",
    "type": "article"
  },
  {
    "title": "Verifying and Quantifying Side-channel Resistance of Masked Software Implementations",
    "doi": "https://doi.org/10.1145/3330392",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Pengfei Gao; Jun Zhang; Fu Song; Chao Wang",
    "corresponding_authors": "",
    "abstract": "Power side-channel attacks, capable of deducing secret data using statistical analysis, have become a serious threat. Random masking is a widely used countermeasure for removing the statistical dependence between secret data and side-channel information. Although there are techniques for verifying whether a piece of software code is perfectly masked, they are limited in accuracy and scalability. To bridge this gap, we propose a refinement-based method for verifying masking countermeasures. Our method is more accurate than prior type-inference-based approaches and more scalable than prior model-counting-based approaches using SAT or SMT solvers. Indeed, our method can be viewed as a gradual refinement of a set of type-inference rules for reasoning about distribution types. These rules are kept abstract initially to allow fast deduction and then made concrete when the abstract version is not able to resolve the verification problem. We also propose algorithms for quantifying the amount of side-channel information leakage from a software implementation using the notion of quantitative masking strength. We have implemented our method in a software tool and evaluated it on cryptographic benchmarks including AES and MAC-Keccak. The experimental results show that our method significantly outperforms state-of-the-art techniques in terms of accuracy and scalability.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2997262806",
    "type": "article"
  },
  {
    "title": "KLEESpectre",
    "doi": "https://doi.org/10.1145/3385897",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Guanhua Wang; Sudipta Chattopadhyay; Arnab Kumar Biswas; Tulika Mitra; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Spectre-style attacks disclosed in early 2018 expose data leakage scenarios via cache side channels. Specifically, speculatively executed paths due to branch mis-prediction may bring secret data into the cache, which are then exposed via cache side channels even after the speculative execution is squashed. Symbolic execution is a well-known test generation method to cover program paths at the level of the application software. In this article, we extend symbolic execution with modeling of cache and speculative execution. Our tool KLEE SPECTRE , built on top of the KLEE symbolic execution engine, can thus provide a testing engine to check for data leakage through the cache side channel as shown via Spectre attacks. Our symbolic cache model can verify whether the sensitive data leakage due to speculative execution can be observed by an attacker at a given program point. Our experiments show that KLEE SPECTRE can effectively detect data leakage along speculatively executed paths and our cache model can make the leakage detection more precise.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2972034624",
    "type": "article"
  },
  {
    "title": "Automated Patch Transplantation",
    "doi": "https://doi.org/10.1145/3412376",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Ridwan Shariffdeen; Shin Hwei Tan; Mingyuan Gao; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Automated program repair is an emerging area that attempts to patch software errors and vulnerabilities. In this article, we formulate and study a problem related to automated repair, namely automated patch transplantation. A patch for an error in a donor program is automatically adapted and inserted into a “similar” target program. We observe that despite standard procedures for vulnerability disclosures and publishing of patches, many un-patched occurrences remain in the wild. One of the main reasons is the fact that various implementations of the same functionality may exist and, hence, published patches need to be modified and adapted. In this article, we therefore propose and implement a workflow for transplanting patches. Our approach centers on identifying patch insertion points, as well as namespaces translation across programs via symbolic execution. Experimental results to eliminate five classes of errors highlight our ability to fix recurring vulnerabilities across various programs through transplantation. We report that in 20 of 24 fixing tasks involving eight application subjects mostly involving file processing programs, we successfully transplanted the patch and validated the transplantation through differential testing. Since the publication of patches make an un-patched implementation more vulnerable, our proposed techniques should serve a long-standing need in practice.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3116924855",
    "type": "article"
  },
  {
    "title": "An Empirical Study of Developer Discussions in the Gitter Platform",
    "doi": "https://doi.org/10.1145/3412378",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Osama Ehsan; Safwat Hassan; Mariam El Mezouar; Ying Zou",
    "corresponding_authors": "",
    "abstract": "Developer chatrooms (e.g., the Gitter platform) are gaining popularity as a communication channel among developers. In developer chatrooms, a developer ( asker ) posts questions and other developers ( respondents ) respond to the posted questions. The interaction between askers and respondents results in a discussion thread . Recent studies show that developers use chatrooms to inquire about issues, discuss development ideas, and help each other. However, prior work focuses mainly on analyzing individual messages of a chatroom without analyzing the discussion thread in a chatroom. Developer chatroom discussions are context-sensitive, entangled, and include multiple participants that make it hard to accurately identify threads. Therefore, prior work has limited capability to show the interactions among developers within a chatroom by analyzing only individual messages. In this article, we perform an in-depth analysis of the Gitter platform (i.e., developer chatrooms) by analyzing 6,605,248 messages of 709 chatrooms. To analyze the characteristics of the posted questions and the impact on the response behavior (e.g., whether the posted questions get responses), we propose an approach that identifies discussion threads in chatrooms with high precision (i.e., 0.81 F-score). Our results show that inactive members responded more often and unique questions take longer discussion time than simple questions. We also find that clear and concise questions are more likely to be responded to than poorly written questions. We further manually analyze a randomly selected sample of 384 threads to examine how respondents resolve the raised questions. We observe that more than 80% of the studied threads are resolved. Advanced-level/beginner-level questions along with the edited questions are the mostly resolved questions. Our results can help the project maintainers understand the nature of the discussion threads (e.g., the topic trends). Project maintainers can also benefit from our thread identification approach to spot the common repeated threads and use these threads as frequently asked questions (FAQs) to improve the documentation of their projects.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3117161066",
    "type": "article"
  },
  {
    "title": "Technical Q8A Site Answer Recommendation via Question Boosting",
    "doi": "https://doi.org/10.1145/3412845",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Zhipeng Gao; Xin Xia; David Lo; John Grundy",
    "corresponding_authors": "",
    "abstract": "Software developers have heavily used online question and answer platforms to seek help to solve their technical problems. However, a major problem with these technical Q&A sites is \"answer hungriness\" i.e., a large number of questions remain unanswered or unresolved, and users have to wait for a long time or painstakingly go through the provided answers with various levels of quality. To alleviate this time-consuming problem, we propose a novel DeepAns neural network-based approach to identify the most relevant answer among a set of answer candidates. Our approach follows a three-stage process: question boosting, label establishment, and answer recommendation. Given a post, we first generate a clarifying question as a way of question boosting. We automatically establish the positive, neutral+, neutral- and negative training samples via label establishment. When it comes to answer recommendation, we sort answer candidates by the matching scores calculated by our neural network-based model. To evaluate the performance of our proposed model, we conducted a large scale evaluation on four datasets, collected from the real world technical Q&A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python and Stack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user study results demonstrate that our approach is effective in solving the answer hungry problem by recommending the most relevant answers from historical archives.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3115599191",
    "type": "article"
  },
  {
    "title": "Many-Objective Test Suite Generation for Software Product Lines",
    "doi": "https://doi.org/10.1145/3361146",
    "publication_date": "2020-01-30",
    "publication_year": 2020,
    "authors": "Robert M. Hierons; Miqing Li; Xiaohui Liu; José Antonio Parejo; Sergio Segura; Xin Yao",
    "corresponding_authors": "",
    "abstract": "A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2990116723",
    "type": "article"
  },
  {
    "title": "How Software Refactoring Impacts Execution Time",
    "doi": "https://doi.org/10.1145/3485136",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Luca Traini; Daniele Di Pompeo; Michele Tucci; Bin Lin; Simone Scalabrino; Gabriele Bavota; Michele Lanza; Rocco Oliveto; Vittorio Cortellessa",
    "corresponding_authors": "",
    "abstract": "Refactoring aims at improving the maintainability of source code without modifying its external behavior. Previous works proposed approaches to recommend refactoring solutions to software developers. The generation of the recommended solutions is guided by metrics acting as proxy for maintainability (e.g., number of code smells removed by the recommended solution). These approaches ignore the impact of the recommended refactorings on other non-functional requirements, such as performance, energy consumption, and so forth. Little is known about the impact of refactoring operations on non-functional requirements other than maintainability. We aim to fill this gap by presenting the largest study to date to investigate the impact of refactoring on software performance, in terms of execution time. We mined the change history of 20 systems that defined performance benchmarks in their repositories, with the goal of identifying commits in which developers implemented refactoring operations impacting code components that are exercised by the performance benchmarks. Through a quantitative and qualitative analysis, we show that refactoring operations can significantly impact the execution time. Indeed, none of the investigated refactoring types can be considered “safe” in ensuring no performance regression. Refactoring types aimed at decomposing complex code entities (e.g., Extract Class/Interface, Extract Method) have higher chances of triggering performance degradation, suggesting their careful consideration when refactoring performance-critical code.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4200249747",
    "type": "article"
  },
  {
    "title": "History-based Model Repair Recommendations",
    "doi": "https://doi.org/10.1145/3419017",
    "publication_date": "2021-01-03",
    "publication_year": 2021,
    "authors": "Manuel Ohrndorf; Christopher Pietsch; Udo Kelter; Lars Grunske; Timo Kehrer",
    "corresponding_authors": "",
    "abstract": "Models in Model-driven Engineering are primary development artifacts that are heavily edited in all stages of software development and that can become temporarily inconsistent during editing. In general, there are many alternatives to resolve an inconsistency, and which one is the most suitable depends on a variety of factors. As also proposed by recent approaches to model repair, it is reasonable to leave the actual choice and approval of a repair alternative to the discretion of the developer. Model repair tools can support developers by proposing a list of the most promising repairs. Such repair recommendations will be only accepted in practice if the generated proposals are plausible and understandable, and if the set as a whole is manageable. Current approaches, which mostly focus on exhaustive search strategies, exploring all possible model repairs without considering the intention of historic changes, fail in meeting these requirements. In this article, we present a new approach to generate repair proposals that aims at inconsistencies that have been introduced by past incomplete edit steps that can be located in the version history of a model. Such an incomplete edit step is either undone or it is extended to a full execution of a consistency-preserving edit operation. The history-based analysis of inconsistencies as well as the generation of repair recommendations are fully automated, and all interactive selection steps are supported by our repair tool called R E V ISION . We evaluate our approach using histories of real-world models obtained from popular open-source modeling projects hosted in the Eclipse Git repository, including the evolution of the entire UML meta-model. Our experimental results confirm our hypothesis that most of the inconsistencies, namely, 93.4, can be resolved by complementing incomplete edits. 92.6% of the generated repair proposals are relevant in the sense that their effect can be observed in the models’ histories. 94.9% of the relevant repair proposals are ranked at the topmost position.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3118705258",
    "type": "article"
  },
  {
    "title": "Killing Stubborn Mutants with Symbolic Execution",
    "doi": "https://doi.org/10.1145/3425497",
    "publication_date": "2021-01-03",
    "publication_year": 2021,
    "authors": "Thierry Titcheu Chekam; Mike Papadakis; Maxime Cordy; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "We introduce SEMu , a Dynamic Symbolic Execution technique that generates test inputs capable of killing stubborn mutants (killable mutants that remain undetected after a reasonable amount of testing). SEMu aims at mutant propagation (triggering erroneous states to the program output) by incrementally searching for divergent program behaviors between the original and the mutant versions. We model the mutant killing problem as a symbolic execution search within a specific area in the programs’ symbolic tree. In this framework, the search area is defined and controlled by parameters that allow scalable and cost-effective mutant killing. We integrate SEMu in KLEE and experimented with Coreutils (a benchmark frequently used in symbolic execution studies). Our results show that our modeling plays an important role in mutant killing. Perhaps more importantly, our results also show that, within a two-hour time limit, SEMu kills 37% of the stubborn mutants, where KLEE kills none and where the mutant infection strategy (strategy suggested by previous research) kills 17%.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3119014038",
    "type": "article"
  },
  {
    "title": "Psychometrics in Behavioral Software Engineering: A Methodological Introduction with Guidelines",
    "doi": "https://doi.org/10.1145/3469888",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Daniel Graziotin; Per Lenberg; Robert Feldt; Stefan Wagner",
    "corresponding_authors": "",
    "abstract": "A meaningful and deep understanding of the human aspects of software engineering (SE) requires psychological constructs to be considered. Psychology theory can facilitate the systematic and sound development as well as the adoption of instruments (e.g., psychological tests, questionnaires) to assess these constructs. In particular, to ensure high quality, the psychometric properties of instruments need evaluation. In this article, we provide an introduction to psychometric theory for the evaluation of measurement instruments for SE researchers. We present guidelines that enable using existing instruments and developing new ones adequately. We conducted a comprehensive review of the psychology literature framed by the Standards for Educational and Psychological Testing. We detail activities used when operationalizing new psychological constructs, such as item pooling, item review, pilot testing, item analysis, factor analysis, statistical property of items, reliability, validity, and fairness in testing and test bias. We provide an openly available example of a psychometric evaluation based on our guideline. We hope to encourage a culture change in SE research towards the adoption of established methods from psychology. To improve the quality of behavioral research in SE, studies focusing on introducing, validating, and then using psychometric instruments need to be more common.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3202205940",
    "type": "article"
  },
  {
    "title": "Applying Bayesian Analysis Guidelines to Empirical Software Engineering Data: The Case of Programming Languages and Code Quality",
    "doi": "https://doi.org/10.1145/3490953",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Carlo A. Furia; Richard Torkar; Robert Feldt",
    "corresponding_authors": "",
    "abstract": "Statistical analysis is the tool of choice to turn data into information and then information into empirical knowledge. However, the process that goes from data to knowledge is long, uncertain, and riddled with pitfalls. To be valid, it should be supported by detailed, rigorous guidelines that help ferret out issues with the data or model and lead to qualified results that strike a reasonable balance between generality and practical relevance. Such guidelines are being developed by statisticians to support the latest techniques for Bayesian data analysis. In this article, we frame these guidelines in a way that is apt to empirical research in software engineering. To demonstrate the guidelines in practice, we apply them to reanalyze a GitHub dataset about code quality in different programming languages. The dataset’s original analysis [Ray et al. 55 ] and a critical reanalysis [Berger et al. 6 ] have attracted considerable attention—in no small part because they target a topic (the impact of different programming languages) on which strong opinions abound. The goals of our reanalysis are largely orthogonal to this previous work, as we are concerned with demonstrating, on data in an interesting domain, how to build a principled Bayesian data analysis and to showcase its benefits. In the process, we will also shed light on some critical aspects of the analyzed data and of the relationship between programming languages and code quality—such as the impact of project-specific characteristics other than the used programming language. The high-level conclusions of our exercise will be that Bayesian statistical techniques can be applied to analyze software engineering data in a way that is principled, flexible, and leads to convincing results that inform the state-of-the-art while highlighting the boundaries of its validity. The guidelines can support building solid statistical analyses and connecting their results. Thus, they can help buttress continued progress in empirical software engineering research.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3128940561",
    "type": "article"
  },
  {
    "title": "Microservice Security Metrics for Secure Communication, Identity Management, and Observability",
    "doi": "https://doi.org/10.1145/3532183",
    "publication_date": "2022-05-11",
    "publication_year": 2022,
    "authors": "Uwe Zdun; Pierre-Jean Quéval; Georg Simhandl; Riccardo Scandariato; S.P. Chakravarty; Martina Jelić; Aleksandar Jovanović",
    "corresponding_authors": "",
    "abstract": "Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems’ dataset through statistical methods.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4280556079",
    "type": "article"
  },
  {
    "title": "Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts",
    "doi": "https://doi.org/10.1145/3560264",
    "publication_date": "2022-08-30",
    "publication_year": 2022,
    "authors": "Fuchen Ma; Meng Ren; Lerong Ouyang; Yuanliang Chen; Juan Zhu; Ting Chen; Yingli Zheng; Dai Xiao; Yu Jiang; Jiaguang Sun",
    "corresponding_authors": "",
    "abstract": "With the development of decentralized networks, smart contracts, especially those for ERC tokens, are attracting more and more Dapp users to implement their applications. There are some functions in ERC token contracts that only a specific group of accounts could invoke. Among those functions, some even can influence other accounts or the whole system without prior notice or permission. These functions are referred to as contract backdoors. Once exploited by an attacker, they can cause property losses and harm users’ privacy. In this work, we propose Pied-Piper, a hybrid analysis method that integrates datalog analysis and directed fuzzing to detect backdoor threats in Ethereum ERC token contracts. First, datalog analysis is applied to abstract the data structures and identification rules related to the threats for preliminary static detection. Then, directed fuzzing is applied to eliminate false positives caused by the static analysis. We first evaluated Pied-Piper on 200 smart contracts, which are injected with different types of backdoors. It reported all problems without false positives, and none of the injected problems was missed. Then, we applied Pied-Piper on 13,484 real token contracts deployed on Ethereum. Pied-Piper reported 189 confirmed problems, four of which have been assigned unique CVE ids while others are still in the review process. Each contract takes 8.03 seconds for datalog analysis on average, and the fuzzing engine can eliminate the false positives within one minute.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4293545213",
    "type": "article"
  },
  {
    "title": "How Do Successful and Failed Projects Differ? A Socio-Technical Analysis",
    "doi": "https://doi.org/10.1145/3504003",
    "publication_date": "2022-02-08",
    "publication_year": 2022,
    "authors": "Mitchell Joblin; Sven Apel",
    "corresponding_authors": "",
    "abstract": "Software development is at the intersection of the social realm , involving people who develop the software, and the technical realm , involving artifacts (code, docs, etc.) that are being produced. It has been shown that a socio-technical perspective provides rich information about the state of a software project. In particular, we are interested in socio-technical factors that are associated with project success . For this purpose, we frame the task as a network classification problem. We show how a set of heterogeneous networks composed of social and technical entities can be jointly embedded in a single vector space enabling mathematically sound comparisons between distinct software projects. Our approach is specifically designed using intuitive metrics stemming from network analysis and statistics to ease the interpretation of results in the context of software engineering wisdom. Based on a selection of 32 open source projects, we perform an empirical study to validate our approach considering three prediction scenarios to test the classification model’s ability generalizing to (1) randomly held-out project snapshots, (2) future project states, and (3) entirely new projects. Our results provide evidence that a socio-technical perspective is superior to a pure social or technical perspective when it comes to early indicators of future project success. To our surprise, the methodology proposed here even shows evidence of being able to generalize to entirely novel (project hold-out set) software projects reaching predication accuracies of 80%, which is a further testament to the efficacy of our approach and beyond what has been possible so far. In addition, we identify key features that are strongly associated with project success. Our results indicate that even relatively simple socio-technical networks capture highly relevant and interpretable information about the early indicators of future project success.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4210942585",
    "type": "article"
  },
  {
    "title": "Testing the Plasticity of Reinforcement Learning-based Systems",
    "doi": "https://doi.org/10.1145/3511701",
    "publication_date": "2022-03-28",
    "publication_year": 2022,
    "authors": "Matteo Biagiola; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "The dataset available for pre-release training of a machine-learning based system is often not representative of all possible execution contexts that the system will encounter in the field. Reinforcement Learning (RL) is a prominent approach among those that support continual learning, i.e., learning continually in the field, in the post-release phase. No study has so far investigated any method to test the plasticity of RL-based systems, i.e., their capability to adapt to an execution context that may deviate from the training one. We propose an approach to test the plasticity of RL-based systems. The output of our approach is a quantification of the adaptation and anti-regression capabilities of the system, obtained by computing the adaptation frontier of the system in a changed environment. We visualize such frontier as an adaptation/anti-regression heatmap in two dimensions, or as a clustered projection when more than two dimensions are involved. In this way, we provide developers with information on the amount of changes that can be accommodated by the continual learning component of the system, which is key to decide if online, in-the-field learning can be safely enabled or not.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4220663349",
    "type": "article"
  },
  {
    "title": "L2S: A Framework for Synthesizing the Most Probable Program under a Specification",
    "doi": "https://doi.org/10.1145/3487570",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Yingfei Xiong; Bo Wang",
    "corresponding_authors": "",
    "abstract": "In many scenarios, we need to find the most likely program that meets a specification under a local context, where the local context can be an incomplete program, a partial specification, natural language description, and so on. We call such a problem program estimation . In this article, we propose a framework, LingLong Synthesis Framework (L2S) , to address this problem. Compared with existing work, our work is novel in the following aspects. (1) We propose a theory of expansion rules to describe how to decompose a program into choices. (2) We propose an approach based on abstract interpretation to efficiently prune off the program sub-space that does not satisfy the specification. (3) We prove that the probability of a program is the product of the probabilities of choosing expansion rules, regardless of the choosing order. (4) We reduce the program estimation problem to a pathfinding problem, enabling existing pathfinding algorithms to solve this problem. L2S has been applied to program generation and program repair. In this article, we report our instantiation of this framework for synthesizing conditional expressions (L2S-Cond) and repairing conditional statements (L2S-Hanabi). The experiments on L2S-Cond show that each option enabled by L2S, including the expansion rules, the pruning technique, and the use of different pathfinding algorithms, plays a major role in the performance of the approach. The default configuration of L2S-Cond correctly predicts nearly 60% of the conditional expressions in the top 5 candidates. Moreover, we evaluate L2S-Hanabi on 272 bugs from two real-world Java defects benchmarks, namely Defects4J and Bugs.jar. L2S-Hanabi correctly fixes 32 bugs with a high precision of 84%. In terms of repairing conditional statement bugs, L2S-Hanabi significantly outperforms all existing approaches in both precision and recall.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4220884961",
    "type": "article"
  },
  {
    "title": "Dealing with Belief Uncertainty in Domain Models",
    "doi": "https://doi.org/10.1145/3542947",
    "publication_date": "2022-06-08",
    "publication_year": 2022,
    "authors": "Loli Burgueño; Paula Muñoz; Robert Clarisó; Jordi Cabot; Sébastien Gérard; Antonio Vallecillo",
    "corresponding_authors": "",
    "abstract": "There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholders’ beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4281767374",
    "type": "article"
  },
  {
    "title": "Storage State Analysis and Extraction of Ethereum Blockchain Smart Contracts",
    "doi": "https://doi.org/10.1145/3548683",
    "publication_date": "2022-07-29",
    "publication_year": 2022,
    "authors": "Maha Ayub; Tania Saleem; Muhammad Umar Janjua; Talha Ahmad",
    "corresponding_authors": "",
    "abstract": "In migrating and upgrading an Ethereum smart contract, it is necessary to transfer both the code as well as the stored data. Various methods attempt to migrate or upgrade a smart contract, but they are mostly manual, error-prone, and applicable only before deployment. Further, they have challenges in extracting the storage state of complex mapping data structures along with their keys. In this work, we present Smartmuv as an automatic source-code-based static analysis tool to analyze and extract the state from the storage-trie of smart contracts. Based on the abstract syntax tree and the control flow graphs of the Solidity source code, the tool analyzes each state variable including mapping types along the inheritance hierarchy. It also provides the upgrade algorithm that initializes the extracted state in the constructor of new smart contract. Smartmuv safely approximates the origin of the keys used in the mapping to extract values and has been able to extract the mapping state of 23,673 smart contracts with 95.7% overall precision. Moreover, we also validate the Smartmuv’s extracted state with the third-party tool Etherscan.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4288701128",
    "type": "article"
  },
  {
    "title": "An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities",
    "doi": "https://doi.org/10.1145/3554732",
    "publication_date": "2022-08-05",
    "publication_year": 2022,
    "authors": "Imen Sayar; Alexandre Bartel; Eric Bodden; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Nowadays, an increasing number of applications uses deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASP's list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class -- such as making it public -- can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5% are not patched, leaving gadgets available for future attacks. For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4289939589",
    "type": "article"
  },
  {
    "title": "What’s (Not) Working in Programmer User Studies?",
    "doi": "https://doi.org/10.1145/3587157",
    "publication_date": "2023-03-14",
    "publication_year": 2023,
    "authors": "Matthew C. Davis; Emad Aghayi; Thomas D. LaToza; Xiaoyin Wang; Brad A. Myers; Joshua Sunshine",
    "corresponding_authors": "",
    "abstract": "A key goal of software engineering research is to improve the environments, tools, languages, and techniques programmers use to efficiently create quality software. Successfully designing these tools and demonstrating their effectiveness involves engaging with tool users—software engineers. Researchers often want to conduct user studies of software engineers to collect direct evidence. However, running user studies can be difficult, and researchers may lack solution strategies to overcome the barriers, so they may avoid user studies. To understand the challenges researchers face when conducting programmer user studies, we interviewed 26 researchers. Based on the analysis of interview data, we contribute (i) a taxonomy of 18 barriers researchers encounter; (ii) 23 solution strategies some researchers use to address 8 of the 18 barriers in their own studies; and (iii) 4 design ideas, which we adapted from the behavioral science community, that may lower 8 additional barriers. To validate the design ideas, we held an in-person all-day focus group with 16 researchers.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4324157328",
    "type": "article"
  },
  {
    "title": "XCoS: Explainable Code Search Based on Query Scoping and Knowledge Graph",
    "doi": "https://doi.org/10.1145/3593800",
    "publication_date": "2023-04-22",
    "publication_year": 2023,
    "authors": "Chong Wang; Xin Peng; Zhenchang Xing; Yue Zhang; Mingwei Liu; Rong Luo; Xiujie Meng",
    "corresponding_authors": "",
    "abstract": "When searching code, developers may express additional constraints (e.g., functional constraints and nonfunctional constraints) on the implementations of desired functionalities in the queries. Existing code search tools treat the queries as a whole and ignore the different implications of different parts of the queries. Moreover, these tools usually return a ranked list of candidate code snippets without any explanations. Therefore, the developers often find it hard to choose the desired results and build confidence on them. In this article, we conduct a developer survey to better understand and address these issues and induct some insights from the survey results. Based on the insights, we propose XCoS, an explainable code search approach based on query scoping and knowledge graph. XCoS extracts a background knowledge graph from general knowledge bases like Wikidata and Wikipedia. Given a code search query, XCoS identifies different parts (i.e., functionalities, functional constraints, nonfunctional constraints) from it and use the expressions of functionalities and functional constraints to search the codebase. It then links both the query and the candidate code snippets to the concepts in the background knowledge graph and generates explanations based on the association paths between these two parts of concepts together with relevant descriptions. XCoS uses an interactive user interface that allows the user to better understand the associations between candidate code snippets and the query from different aspects and choose the desired results. Our evaluation shows that the quality of the extracted background knowledge and the concept linkings in codebase is generally high. Furthermore, the generated explanations are considered complete, concise, and readable, and the approach can help developers find the desired code snippets more accurately and confidently.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4366770033",
    "type": "article"
  },
  {
    "title": "Survey of Code Search Based on Deep Learning",
    "doi": "https://doi.org/10.1145/3628161",
    "publication_date": "2023-10-19",
    "publication_year": 2023,
    "authors": "Yutao Xie; Jiayi Lin; Hande Dong; Lei Zhang; Zhonghai Wu",
    "corresponding_authors": "",
    "abstract": "Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given natural language query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework that maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-step process: query semantics modeling, code semantics modeling, and matching modeling, which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4387769581",
    "type": "article"
  },
  {
    "title": "Challenges of Working from Home in Software Development During Covid-19 Lockdowns",
    "doi": "https://doi.org/10.1145/3579636",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Katharina Müller; Christian Koch; Dirk Riehle; Michael Stops; Nikolay Harutyunyan",
    "corresponding_authors": "",
    "abstract": "The COVID-19 pandemic in 2020/2021/2022 and the resulting lockdowns forced many companies to switch to working from home, swiftly, on a large scale, and without preparation. This situation created unique challenges for software development, where individual software professionals had to shift instantly from working together at a physical venue to working remotely from home. Our research questions focus on the challenges of software professionals who work from home due to the COVID-19 pandemic, which we studied empirically at a German bank. We conducted a case study employing a mixed methods approach. We aimed to cover both the breadth of challenges via a quantitative survey, as well as a deeper understanding of these challenges via the follow-up qualitative analysis of 15 semi-structured interviews. In this article, we present the key impediments employees faced during the crisis, as well as their similarities and differences to the known challenges in distributed software development (DSD). We also analyze the employees’ job satisfaction and how the identified challenges impact job satisfaction. In our study, we focus on challenges in communication, collaboration, tooling, and management. The findings of the study provide insights into this emerging topic of high industry relevance. At the same time, the study contributes to the existing academic research on work from home and on the COVID-19 pandemic aftermath.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4316813573",
    "type": "article"
  },
  {
    "title": "Dependency Update Strategies and Package Characteristics",
    "doi": "https://doi.org/10.1145/3603110",
    "publication_date": "2023-06-01",
    "publication_year": 2023,
    "authors": "Abbas Javan Jafari; Diego Elias Costa; Emad Shihab; Rabe Abdalkareem",
    "corresponding_authors": "",
    "abstract": "Managing project dependencies is a key maintenance issue in software development. Developers need to choose an update strategy that allows them to receive important updates and fixes while protecting them from breaking changes. Semantic Versioning was proposed to address this dilemma, but many have opted for more restrictive or permissive alternatives. This empirical study explores the association between package characteristics and the dependency update strategy selected by its dependents to understand how developers select and change their update strategies. We study over 112,000 Node Package Manager (npm) packages and use 19 characteristics to build a prediction model that identifies the common dependency update strategy for each package. Our model achieves a minimum improvement of 72% over the baselines and is much better aligned with community decisions than the npm default strategy. We investigate how different package characteristics can influence the predicted update strategy and find that dependent count, age, and release status to be the highest influencing features. We complement the work with qualitative analyses of 160 packages to investigate the evolution of update strategies. While the common update strategy remains consistent for many packages, certain events such as the release of the 1.0.0 version or breaking changes influence the selected update strategy over time.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4379014622",
    "type": "article"
  },
  {
    "title": "Testing Causality in Scientific Modelling Software",
    "doi": "https://doi.org/10.1145/3607184",
    "publication_date": "2023-07-12",
    "publication_year": 2023,
    "authors": "Andrew G. Clark; Michael S. Foster; Benedikt Prifling; Neil Walkinshaw; Robert M. Hierons; Volker Schmidt; Robert D. Turner",
    "corresponding_authors": "",
    "abstract": "From simulating galaxy formation to viral transmission in a pandemic, scientific models play a pivotal role in developing scientific theories and supporting government policy decisions that affect us all. Given these critical applications, a poor modelling assumption or bug could have far-reaching consequences. However, scientific models possess several properties that make them notoriously difficult to test, including a complex input space, long execution times, and non-determinism, rendering existing testing techniques impractical. In fields such as epidemiology, where researchers seek answers to challenging causal questions, a statistical methodology known as Causal inference has addressed similar problems, enabling the inference of causal conclusions from noisy, biased, and sparse data instead of costly experiments. This article introduces the causal testing framework: a framework that uses causal inference techniques to establish causal effects from existing data, enabling users to conduct software testing activities concerning the effect of a change, such as metamorphic testing, a posteriori . We present three case studies covering real-world scientific models, demonstrating how the causal testing framework can infer metamorphic test outcomes from reused, confounded test data to provide an efficient solution for testing scientific modelling software.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4384030289",
    "type": "article"
  },
  {
    "title": "Learning to Detect Memory-related Vulnerabilities",
    "doi": "https://doi.org/10.1145/3624744",
    "publication_date": "2023-09-18",
    "publication_year": 2023,
    "authors": "Sicong Cao; Xiaobing Sun; Lili Bo; Rongxin Wu; Bin Li; Xiaoxue Wu; Chuanqi Tao; Tao Zhang; Wei Liu",
    "corresponding_authors": "",
    "abstract": "Memory-related vulnerabilities can result in performance degradation or even program crashes, constituting severe threats to the security of modern software. Despite the promising results of deep learning (DL)-based vulnerability detectors, there exist three main limitations: (1) rich contextual program semantics related to vulnerabilities have not yet been fully modeled; (2) multi-granularity vulnerability features in hierarchical code structure are still hard to be captured; and (3) heterogeneous flow information is not well utilized. To address these limitations, in this article, we propose a novel DL-based approach, called MVD+ , to detect memory-related vulnerabilities at the statement-level. Specifically, it conducts both intraprocedural and interprocedural analysis to model vulnerability features, and adopts a hierarchical representation learning strategy, which performs syntax-aware neural embedding within statements and captures structured context information across statements based on a novel Flow-Sensitive Graph Neural Networks, to learn both syntactic and semantic features of vulnerable code. To demonstrate the performance, we conducted extensive experiments against eight state-of-the-art DL-based approaches as well as five well-known static analyzers on our constructed dataset with 6,879 vulnerabilities in 12 popular C/C++ applications. The experimental results confirmed that MVD+ can significantly outperform current state-of-the-art baselines and make a great trade-off between effectiveness and efficiency.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4386830545",
    "type": "article"
  },
  {
    "title": "Aspect-level Information Discrepancies across Heterogeneous Vulnerability Reports: Severity, Types and Detection Methods",
    "doi": "https://doi.org/10.1145/3624734",
    "publication_date": "2023-10-16",
    "publication_year": 2023,
    "authors": "Jiamou Sun; Zhenchang Xing; Xin Xia; Qinghua Lu; Xiwei Xu; Liming Zhu",
    "corresponding_authors": "",
    "abstract": "Vulnerable third-party libraries pose significant threats to software applications that reuse these libraries. At an industry scale of reuse, manual analysis of third-party library vulnerabilities can be easily overwhelmed by the sheer number of vulnerabilities continually collected from diverse sources for thousands of reused libraries. Our study of four large-scale, actively maintained vulnerability databases (NVD, IBM X-Force, ExploitDB, and Openwall) reveals the wide presence of information discrepancies, in terms of seven vulnerability aspects, i.e., product, version, component, vulnerability type, root cause, attack vector, and impact, between the reports for the same vulnerability from heterogeneous sources. It would be beneficial to integrate and cross-validate multi-source vulnerability information, but it demands automatic aspect extraction and aspect discrepancy detection. In this work, we experimented with a wide range of NLP methods to extract named entities (e.g., product) and free-form phrases (e.g., root cause) from textual vulnerability reports and to detect semantically different aspect mentions between the reports. Our experiments confirm the feasibility of applying NLP methods to automate aspect-level vulnerability analysis and identify the need for domain customization of general NLP methods. Based on our findings, we propose a discrepancy-aware, aspect-level vulnerability knowledge graph and a KG-based web portal that integrates diversified vulnerability key aspect information from heterogeneous vulnerability databases. Our conducted user study proves the usefulness of our web portal. Our study opens the door to new types of vulnerability integration and management, such as vulnerability portraits of a product and explainable prediction of silent vulnerabilities.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4387664630",
    "type": "article"
  },
  {
    "title": "Safety of Perception Systems for Automated Driving: A Case Study on Apollo",
    "doi": "https://doi.org/10.1145/3631969",
    "publication_date": "2023-11-08",
    "publication_year": 2023,
    "authors": "Sangeeth Kochanthara; Tajinder Singh; Alexandru Forrai; Loek Cleophas",
    "corresponding_authors": "",
    "abstract": "The automotive industry is now known for its software-intensive and safety-critical nature. The industry is on a path to the holy grail of completely automating driving, starting from relatively simple operational areas like highways. One of the most challenging, evolving, and essential parts of automated driving is the software that enables understanding of surroundings and the vehicle’s own as well as surrounding objects’ relative position, otherwise known as the perception system. Current generation perception systems are formed by a combination of traditional software and machine learning-related software. With automated driving systems transitioning from research to production, it is imperative to assess their safety. We assess the safety of Apollo, the most popular open-source automotive software, at the design level for its use on a Dutch highway. We identified 58 safety requirements, 38 of which are found to be fulfilled at the design level. We observe that all requirements relating to traditional software are fulfilled, while most requirements specific to machine learning systems are not. This study unveils issues that need immediate attention; and directions for future research to make automated driving safe.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4388499465",
    "type": "article"
  },
  {
    "title": "Try with Simpler - An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection",
    "doi": "https://doi.org/10.1145/3644386",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Lin Yang; Junjie Chen; Shutao Gao; Zhihao Gong; Hongyu Zhang; Yue Kang; H.H. Li",
    "corresponding_authors": "",
    "abstract": "With the rapid development of deep learning (DL), the recent trend of log-based anomaly detection focuses on extracting semantic information from log events (i.e., templates of log messages) and designing more advanced DL models for anomaly detection. Indeed, the effectiveness of log-based anomaly detection can be improved, but these DL-based techniques further suffer from the limitations of more heavy dependency on training data (such as data quality or data labels) and higher costs in time and resources due to the complexity and scale of DL models, which hinder their practical use. On the contrary, the techniques based on traditional machine learning or data mining algorithms are less dependent on training data and more efficient but produce worse effectiveness than DL-based techniques, which is mainly caused by the problem of unseen log events (some log events in incoming log messages are unseen in training data) confirmed by our motivating study. Intuitively, if we can improve the effectiveness of traditional techniques to be comparable with advanced DL-based techniques, then log-based anomaly detection can be more practical. Indeed, an existing study in the other area (i.e., linking questions posted on Stack Overflow) has pointed out that traditional techniques with some optimizations can indeed achieve comparable effectiveness with the state-of-the-art DL-based technique, indicating the feasibility of enhancing traditional log-based anomaly detection techniques to some degree. Inspired by the idea of “try-with-simpler,” we conducted the first empirical study to explore the potential of improving traditional techniques for more practical log-based anomaly detection. In this work, we optimized the traditional unsupervised PCA (Principal Component Analysis) technique by incorporating a lightweight semantic-based log representation in it, called SemPCA , and conducted an extensive study to investigate the potential of SemPCA for more practical log-based anomaly detection. By comparing seven log-based anomaly detection techniques (including four DL-based techniques, two traditional techniques, and SemPCA ) on both public and industrial datasets, our results show that SemPCA achieves comparable effectiveness as advanced supervised/semi-supervised DL-based techniques while being much more stable under insufficient training data and more efficient, demonstrating that the traditional technique can still excel after small but useful adaptation.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391614207",
    "type": "article"
  },
  {
    "title": "On the Way to SBOMs: Investigating Design Issues and Solutions in Practice",
    "doi": "https://doi.org/10.1145/3654442",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Tingting Bi; Boming Xia; Zhenchang Xing; Qinghua Lu; Liming Zhu",
    "corresponding_authors": "",
    "abstract": "The increase of software supply chain threats has underscored the necessity for robust security mechanisms, among which the Software Bill of Materials (SBOM) stands out as a promising solution. SBOMs, by providing a machine-readable inventory of software composition details, play a crucial role in enhancing transparency and traceability within software supply chains. This empirical study delves into the practical challenges and solutions associated with the adoption of SBOMs through an analysis of 4,786 GitHub discussions across 510 SBOM-related projects. Through repository mining and analysis, this research delineates key topics, challenges, and solutions intrinsic to the effective utilization of SBOMs. Furthermore, we shed light on commonly used tools and frameworks for SBOM generation, exploring their respective strengths and limitations. This study underscores a set of findings, for example, there are four phases of the SBOM life cycle, and each phase has a set of SBOM development activities and issues; in addition, this study emphasizes the role SBOM play in ensuring resilient software development practices and the imperative of their widespread adoption and integration to bolster supply chain security. The insights of our study provide vital input for future work and practical advancements in this topic.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4393192529",
    "type": "article"
  },
  {
    "title": "GENOA—a customizable, front-end-retargetable source code analysis framework",
    "doi": "https://doi.org/10.1145/304399.304402",
    "publication_date": "1999-04-01",
    "publication_year": 1999,
    "authors": "Prémkumar Dévanbu",
    "corresponding_authors": "Prémkumar Dévanbu",
    "abstract": "Code analysis tools provide support for such software engineering tasks as program understanding, software metrics, testing, and reengineering. In this article we describe GENOA, the framework underlying application generators such as Aria and GEN++ which have been used to generate a wide range of practical code analysis tools. This experience illustrates front-end retargetability of GENOA; we describe the features of the GENOA framework that allow it to be used with different front ends. While permitting arbitrary parse tree computations, the GENOA specification language has special, compact iteration operators that are tuned for expressing simple, polynomial-time analysis programs; in fact, there is a useful sublanguage of the GENOA language that can express precisely all (and only) polynomial-time (PTIME) analysis programs on parse trees. Thus, we argue that the GENOA language is a simple and convenient vehicle for implementing a range of analysis tools. We also argue that the “front-and reuse” approach of GENOA offers an important advantage for tools aimed at large software projects: the reuse of complex, expensive build procedures to run generated tools over large source bases. In this article, we describe the GENOA framework and our experiences with it.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2074315105",
    "type": "article"
  },
  {
    "title": "Automatic testing equivalence verification of spi calculus specifications",
    "doi": "https://doi.org/10.1145/941566.941570",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Luca Durante; Riccardo Sisto; Adriano Valenzano",
    "corresponding_authors": "",
    "abstract": "Testing equivalence is a powerful means for expressing the security properties of cryptographic protocols, but its formal verification is a difficult task because of the quantification over contexts on which it is based. Previous articles have provided insights into using theorem-proving for the verification of testing equivalence of spi calculus specifications. This article addresses the same verification problem, but uses a state exploration approach. The verification technique is based on the definition of an environment-sensitive, labeled transition system representing a spi calculus specification. Trace equivalence defined on such a transition system coincides with testing equivalence. Symbolic techniques are used to keep the set of traces finite. If a difference in the traces of two spi descriptions (typically a specification and the corresponding implementation of a protocol) is found, it can be used to automatically build the spi calculus description of an intruder process that can exploit the difference.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2159435827",
    "type": "article"
  },
  {
    "title": "Applying GQM in an industrial software factory",
    "doi": "https://doi.org/10.1145/292182.292197",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "Alfonso Fuggetta; Luigi Lavazza; Sandro Morasca; Stefano Cinti; Giandomenico Oldano; Elena Orazi",
    "corresponding_authors": "",
    "abstract": "Goal/Question/Metric GQM) is a paradigm for the systematic definition, establishment, and exploitation of measurement programs supporting the quantitative evaluation of softare processes and products. Although GQM is a quite well-known method, detailed guidelines for establishing a GQM program in an industrial environment are still limited. Also, there are few reported experiences on the application of GQM to industrial cases. Finally, the technological support for GQM is still inadequate. This article describes the experience we have gained in applying GQM at Digital Laboratries in Italy. The procedures, experiences, and technology that have been employed in this study are largely reusable by other industrial organizations willing to introduce a GQM-based measurement program in their development environments.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W1975674099",
    "type": "article"
  },
  {
    "title": "A graphical environment for the design of concurrent real-time systems",
    "doi": "https://doi.org/10.1145/237432.237438",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "L.E. Moser; Y. S. Ramakrishna; G. Kutty; P. M. Melliar‐Smith; Laura K. Dillon",
    "corresponding_authors": "",
    "abstract": "Concurrent real-time systems are among the most difficult systems to design because of the many possible interleavings of events and because of the timing requirements that must be satisfied. We have developed a graphical environment based on Real-Time Graphical Interval Logic (RTGIL) for specifying and reasoning about the designs of concurrent real-time systems. Specifications in the logic have an intuitive graphical representation that resembles the timing diagrams drawn by software and hardware engineers, with real-time constraints that bound the durations of intervals. The syntax-directed editor of the RTGIL environment enables the user to compose and edit graphical formulas on a workstation display; the automated theorem prover mechanically checks the validity of proofs in the logic; and the database and proof manager tracks proof dependencies and allows formulas to be stored and retrieved. This article describes the logic, methodology, and tools that comprise the prototype RTGIL environment and illustrates the use of the environment with an example application.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2001594070",
    "type": "article"
  },
  {
    "title": "A generic model for reflective design",
    "doi": "https://doi.org/10.1145/350887.350895",
    "publication_date": "2000-04-01",
    "publication_year": 2000,
    "authors": "Πάνος Λουρίδας; Pericles Loucopoulos",
    "corresponding_authors": "",
    "abstract": "Rapid technological change has had an impact on the nature of software. This has led to new exigencies and to demands for software engineering paradigms that pay particular atttention to meeting them. We advocate that such demands can be met, at least in large parts, through the adoption of software engineering processes that are founded on a reflective stance. To this end, we turn our attention to the field of Design Rationale. We analyze and characterize Design Rationale approaches and show that despite surface differences between different approaches, they all tend to be variants of a relatively small set of static and dynamic affinities. We use the synthesis of static and dynamic affinities to develop a generic model for reflective design. The model is nonprescriptive and affects minimally the design process. It is context-independent and is intended to be used as a facilitator in participative design, supporting group communication and deliberation. The potential utility of the model is demonstrated through two examples, one from the world of business design and the other from programming language design",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2090436708",
    "type": "article"
  },
  {
    "title": "Functional specification of time-sensitive communicating systems",
    "doi": "https://doi.org/10.1145/151299.151302",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Manfred Broy",
    "corresponding_authors": "Manfred Broy",
    "abstract": "A formal model and a logical framework for the functional specification of time-sensitive communicating systems and their interacting components are outlined. The specification method is modular with respect to sequential composition, parallel composition, and communication feedback. Nondeterminism is included by underspecification. The application of the specification method to timed communicating functions is demonstrated. Abstractions from time are studied. In particular, a rational is given for the chosen concepts of the functional specification technique. The relationship between system models based on nondeterminism and system models based on explicit time notions is investigated. Forms of reasoning are considered. The alternating bit protocol is used as a running example.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2103225125",
    "type": "article"
  },
  {
    "title": "The cost of selective recompilation and environment processing",
    "doi": "https://doi.org/10.1145/174634.174637",
    "publication_date": "1994-01-02",
    "publication_year": 1994,
    "authors": "Rolf Adams; Walter F. Tichy; Annette Weinert",
    "corresponding_authors": "",
    "abstract": "When a single software module in a large system is modified, a potentially large number of other modules may have to be recompiled. By reducing both the number of compilations and the amount of input processed by each compilation run, the turnaround time after changes can be reduced significantly. Potential time savings are measured in a medium-sized, industrial software project over a three-year period. The results indicate that a large number of compilations caused by traditional compilation unit dependencies may be redundant. On the available data, a mechanism that compares compiler output saves about 25 percent, smart recompilation saves 50 percent, and smartest recompilation may save up to 80 percent of compilation work. Furthermore, all compilation methods other than smartest recompilation process large amounts of unused environment data. In the project analyzed, the average environment symbols are actually used. Reading only the actually used symbols would reduce total compiler input by about 50 percent. Combining smart recompilation with a reduction in environment processing might double to triple perceived compilation speed and double linker speed, without sacrificing static type safety.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2169954431",
    "type": "article"
  },
  {
    "title": "Application and experimental evaluation of state space reduction methods for deadlock analysis in Ada",
    "doi": "https://doi.org/10.1145/201024.201038",
    "publication_date": "1994-10-01",
    "publication_year": 1994,
    "authors": "Sastry Duri; Ugo Buy; Ramesh Devarapalli; Sol M. Shatz",
    "corresponding_authors": "",
    "abstract": "An emerging challenge for software engineering is the development of the methods and tools to aid design and analysis of concurrent and distributed software. Over the past few years, a number of analysis methods that focus on Ada tasking have been developed. Many of these methods are based on some form of reachability analysis, which has the advantage of being conceptually simple, but the disadvantage of being computationally expensive. We explore the effectiveness of various Petri net-based techniques for the automated deadlock analysis of Ada programs. Our experiments consider a variety of state space reduction methods both individually and in various combinations. The experiments are applied to a number of classical concurrent programs as well as a set of “real-world” programs. The results indicate that Petri net reduction and reduced state space generation are mutually beneficial techniques, and that combined approaches based on Petri net models are quite effective, compared to alternative analysis approaches.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W1986332631",
    "type": "article"
  },
  {
    "title": "Formalizing space shuttle software requirements",
    "doi": "https://doi.org/10.1145/287000.287023",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Judith Crow; Ben Di Vito",
    "corresponding_authors": "",
    "abstract": "This article describes four case studies in which requirements for new flight software subsystems on NASA's Space Shuttle were analyzed using mechanically supported formal methods. Three of the studies used standard formal specification and verification techniques, and the fourth used state exploration. These applications illustrate two thesis: (1) formal methods complement conventional requirements analysis processes effectively and (2) formal methods confer benefits even when only selectively adopted and applied. The studies also illustrate the interplay of application maturity level and formal methods strategy, especially in areas such as technology transfer, legacy applications, and rapid formalization, and they raise interesting issues in problem domain modeling and in tailoring formal techniques to applications.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2020968148",
    "type": "article"
  },
  {
    "title": "Automated deductive requirements analysis of critical systems",
    "doi": "https://doi.org/10.1145/383876.383877",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Angelo Gargantini; Angelo Morzenti",
    "corresponding_authors": "",
    "abstract": "We advocate the need for automated support to System Requirement Analysis in the development of time- and safety-critical computer-based systems. To this end we pursue an approach based on deductive analysis: high-level, real-world entities and notions, such as events, states, finite variability, cause-effect relations, are modeled through the temporal logic TRIO, and the resulting deductive system is implemented by means of the theorem prover PVS. Throughout the paper, the constructs and features of the deductive system are illustrated and validated by applying them to the well-known example of the Generalized Railway Crossing.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W1981998034",
    "type": "article"
  },
  {
    "title": "Software process modeling and execution within virtual environments",
    "doi": "https://doi.org/10.1145/268411.268415",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "John C. Doppke; Dennis Heimbigner; Alexander Wolf",
    "corresponding_authors": "",
    "abstract": "In the past, multiuser virtual environments have been developed as venues for entertainment and social interaction. Recent research focuses instead on their utility in carrying out work in the real world. This research has identified the importance of a mapping between the real and the virtual that permits the representation of real tasks in the virtual environment. We investigate the use of virtual environments—in particular, MUDs (Multi-User Dimensions)—in the domain of software process. In so doing, we define a mapping, or metaphor , that permits the representation of software processes within a MUD. The system resulting from this mapping, called Promo , permits the modeling and execution of software processes by geographically dispersed agents.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W1988323104",
    "type": "article"
  },
  {
    "title": "Composite model-checking",
    "doi": "https://doi.org/10.1145/332740.332746",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Tevfik Bultan; R. Gerber; Christopher League",
    "corresponding_authors": "",
    "abstract": "There has been a surge of progress in automated verification methods based on state exploration. In areas like hardware design, these technologies are rapidly augmenting key phases of testing and validation. To date, one of the most successful of these methods has been symbolic model-checking, in which large finite-state machines are encoded into compact data structures such as Binary Decision Diagrams (BDDs), and are then checked for safety and liveness properties. However, these techniques have not realized the same success on software systems. One limitation is their inability to deal with infinite-state programs, even those with a single unbounded integer. A second problem is that of finding efficient representations for various variable types. We recently proposed a model-checker for integer-based systems that uses arithmetic constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems, it proved inefficient in its treatment of boolean and (unordered) enumerated types—which are not efficiently representable using arithmetic constraints. In this article we present a new technique that combines the strengths of both BDD and arithmetic constraint representations. Our composite model merges multiple type-specific symbolic representations in a single model-checker. A system's transitions and fixpoint computations are encoded using both BDD (for boolean and enumerated types) and arithmetic constraints (for integers) representations, where the choice depends on the variable types. Our composite model-checking strategy can be extended to other symbolic representations provided that they support operations such as intersection, union, complement, equivalence checking, and relational image computation. We also present conservative approximation techniques for composite representations to address the undecidability of model-checking on infinite-state systems. We demonstrate the effectiveness of our approach by analyzing two example software specifications which include a mixture of booleans, integers, and enumerated types. One of them is a requirements specification for the control software of a nuclear reactor's cooling system, and the other one is a protocol specification.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2054424606",
    "type": "article"
  },
  {
    "title": "Coupling and cohesion metrics for knowledge-based systems using frames and rules",
    "doi": "https://doi.org/10.1145/1027092.1027094",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Stefan Krämer; Hermann Kaindl",
    "corresponding_authors": "",
    "abstract": "Software systems and in particular also knowledge-based systems (KBS) become increasingly large and complex. In response to this challenge, software engineering has a long tradition of advocating modularity. This has also heavily influenced object-oriented development. For measuring certain important aspects of modularity, coupling and cohesion metrics have been developed. Metrics have also attracted considerable attention for object-oriented development. For KBS development, however, no such metrics are available yet. This article presents the core of the first metrics suite for KBS development, its coupling and cohesion metrics. These metrics measure modularity in terms of the relations induced between slots of frames through their common references in rules. We show the soundness of these metrics according to theory and report on their usefulness in practice. As a consequence, we propose using our metrics in order to improve KBS development, and developing other important metrics and assessing their theoretical soundness along these lines.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W1986538131",
    "type": "article"
  },
  {
    "title": "Higher-order architectural connectors",
    "doi": "https://doi.org/10.1145/839268.839272",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Antónia Lopes; Michel Wermelinger; José Luiz Fiadeiro",
    "corresponding_authors": "",
    "abstract": "We develop a notion of higher-order connector towards supporting the systematic construction of architectural connectors for software design. A higher-order connector takes connectors as parameters and allows for services such as security protocols and fault-tolerance mechanisms to be superposed over the interactions that are handled by the connectors passed as actual arguments. The notion is first illustrated over CommUnity, a parallel program design language that we have been using for formalizing aspects of architectural design. A formal, algebraic semantics is then presented which is independent of any Architectural Description Language. Finally, we discuss how our results can impact software design methods and tools.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1963507242",
    "type": "article"
  },
  {
    "title": "Structural testing of rule-based expert systems",
    "doi": "https://doi.org/10.1145/128894.128896",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "James D. Kiper",
    "corresponding_authors": "James D. Kiper",
    "abstract": "Testing of rule-based expert systems has become a high priority for many organizations as the use of such systems proliferates. Traditional software teting techniques apply to some components of rule-based systems, e.g., the inference engine. However, to structurally test the rule base component requires new techniques or adaptations of existing ones. This paper describes one such adaptation: an extension of data flow path selection in which a graphical representation of a rule base is defined and evaluated. This graphical form, called a logical path graph, captures logical paths through a rule base. These logical paths create precisely the abstractions needed in the testing process. An algorithm for the construction of logical path graphs are analyzed.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2024732842",
    "type": "article"
  },
  {
    "title": "Avoiding coincidental correctness in boundary value analysis",
    "doi": "https://doi.org/10.1145/1151695.1151696",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Robert M. Hierons",
    "corresponding_authors": "Robert M. Hierons",
    "abstract": "In partition analysis we divide the input domain to form subdomains on which the system's behaviour should be uniform. Boundary value analysis produces test inputs near each subdomain's boundaries to find failures caused by incorrect implementation of the boundaries. However, boundary value analysis can be adversely affected by coincidental correctness---the system produces the expected output, but for the wrong reason. This article shows how boundary value analysis can be adapted in order to reduce the likelihood of coincidental correctness. The main contribution is to cases of automated test data generation in which we cannot rely on the expertise of a tester.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2093775031",
    "type": "article"
  },
  {
    "title": "Three empirical studies on estimating the design effort of Web applications",
    "doi": "https://doi.org/10.1145/1276933.1276936",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Luciano Baresi; Sandro Morasca",
    "corresponding_authors": "",
    "abstract": "Our research focuses on the effort needed for designing modern Web applications. The design effort is an important part of the total development effort, since the implementation can be partially automated by tools. We carried out three empirical studies with students of advanced university classes enrolled in engineering and communication sciences curricula. The empirical studies are based on the use of W2000, a special-purpose design notation for the design of Web applications, but the hypotheses and results may apply to a wider class of modeling notations (e.g., OOHDM, WebML, or UWE). We started by investigating the relative importance of each design activity. We then assessed the accuracy of a priori design effort predictions and the influence of a few process-related factors on the effort needed for each design activity. We also analyzed the impact of attributes like the size and complexity of W2000 design artifacts on the total effort needed to design the user experience of web applications. In addition, we carried out a finer-grain analysis, by studying which of these attributes impact the effort devoted to the steps of the design phase that are followed when using W2000.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2069756886",
    "type": "article"
  },
  {
    "title": "The role of outcome feedback in improving the uncertainty assessment of software development effort estimates",
    "doi": "https://doi.org/10.1145/13487689.13487693",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "T. Gruschke; Magne Jørgensen",
    "corresponding_authors": "",
    "abstract": "Previous studies report that software developers are over-confident in the accuracy of their effort estimates. Aim: This study investigates the role of outcome feedback, that is, feedback about the discrepancy between the estimated and the actual effort, in improving the uncertainty assessments. Method: We conducted two in-depth empirical studies on uncertainty assessment learning. Study 1 included five student developers and Study 2, 10 software professionals. In each study the developers repeatedly assessed the uncertainty of their effort estimates of a programming task, solved the task, and received estimation accuracy outcome feedback. Results: We found that most, but not all, developers were initially over-confident in the accuracy of their effort estimates and remained over-confident in spite of repeated and timely outcome feedback. One important, but not sufficient, condition for improvement based on outcome feedback seems to be the use of explicitly formulated, instead of purely intuition-based, uncertainty assessment strategies.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2043380925",
    "type": "article"
  },
  {
    "title": "The impact of research on the development of middleware technology",
    "doi": "https://doi.org/10.1145/13487689.13487692",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Wolfgang Emmerich; Mikio Aoyama; Joe Sventek",
    "corresponding_authors": "",
    "abstract": "The middleware market represents a sizable segment of the overall Information and Communication Technology market. In 2005, the annual middleware license revenue was reported by Gartner to be in the region of $8.5 billion. In this article we address the question whether research had any involvement in the creation of the technology that is being sold in this market? We attempt a scholarly discourse. We present the research method that we have applied to answer this question. We then present a brief introduction into the key middleware concepts that provide the foundation for this market. It would not be feasible to investigate any possible impact that research might have had. Instead we select a few very successful technologies that are representative for the middleware market as a whole and show the existence of impact of research results in the creation of these technologies. We investigate the origins of Web services middleware, distributed transaction processing middleware, message-oriented middleware, distributed object middleware and remote procedure call systems. For each of these technologies we are able to show ample influence of research and conclude that without the research conducted by PhD students and researchers in university computer science labs at Brown, CMU, Cambridge, Newcastle, MIT, Vrije, and University of Washington as well as research in industrial labs at APM, AT&amp;T Bell Labs, DEC Systems Research, HP Labs, IBM Research, and Xerox PARC we would not have middleware technology in its current form. We summarise the article by distilling lessons that can be learnt from this evidenced impact for future technology transfer undertakings.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2049187722",
    "type": "article"
  },
  {
    "title": "Composing expressive runtime security policies",
    "doi": "https://doi.org/10.1145/1525880.1525882",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Lujo Bauer; Jay Ligatti; David Walker",
    "corresponding_authors": "",
    "abstract": "Program monitors enforce security policies by interposing themselves into the control flow of untrusted software whenever that software attempts to execute security-relevant actions. At the point of interposition, a monitor has authority to permit or deny (perhaps conditionally) the untrusted software's attempted action. Program monitors are common security enforcement mechanisms and integral parts of operating systems, virtual machines, firewalls, network auditors, and antivirus and antispyware tools. Unfortunately, the runtime policies we require program monitors to enforce grow more complex, both as the monitored software is given new capabilities and as policies are refined in response to attacks and user feedback. We propose dealing with policy complexity by organizing policies in such a way as to make them composable, so that complex policies can be specified more simply as compositions of smaller subpolicy modules. We present a fully implemented language and system called Polymer that allows security engineers to specify and enforce composable policies on Java applications. We formalize the central workings of Polymer by defining an unambiguous semantics for our language. Using this formalization, we state and prove an uncircumventability theorem which guarantees that monitors will intercept all security-relevant actions of untrusted software.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2028824503",
    "type": "article"
  },
  {
    "title": "A revisit of fault class hierarchies in general boolean specifications",
    "doi": "https://doi.org/10.1145/2000791.2000797",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Zhenyu Chen; Tsong Yueh Chen; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "Recently, Kapoor and Bowen [2007] have extended the works by Kuhn [1999], Tsuchiya and Kikuno [2002], and Lau and Yu [2005]. However, their proofs overlook the possibility that a mutant of the Boolean specifications under test may be equivalent. Hence, each of their fault relationships is either incorrect or has an incorrect proof. In this article, we give counterexamples to the incorrect fault relationships and provide new proofs for the valid fault relationships. Furthermore, a co-stronger fault relation is introduced to establish a new fault class hierarchy for general Boolean specifications.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2054341442",
    "type": "article"
  },
  {
    "title": "Feasibility of Stepwise Design of Multitolerant Programs",
    "doi": "https://doi.org/10.1145/2063239.2063240",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Ali Ebnenasir; Sandeep S. Kulkarni",
    "corresponding_authors": "",
    "abstract": "The complexity of designing programs that simultaneously tolerate multiple classes of faults, called multitolerant programs, is in part due to the conflicting nature of the fault tolerance requirements that must be met by a multitolerant program when different types of faults occur. To facilitate the design of multitolerant programs, we present sound and (deterministically) complete algorithms for stepwise design of two families of multitolerant programs in a high atomicity program model, where a process can read and write all program variables in an atomic step. We illustrate that if one needs to design failsafe (respectively, nonmasking) fault tolerance for one class of faults and masking fault tolerance for another class of faults, then a multitolerant program can be designed in separate polynomial-time (in the state space of the fault-intolerant program) steps regardless of the order of addition. This result has a significant methodological implication in that designers need not be concerned about unknown fault tolerance requirements that may arise due to unanticipated types of faults. Further, we illustrate that if one needs to design failsafe fault tolerance for one class of faults and nonmasking fault tolerance for a different class of faults, then the resulting problem is NP-complete in program state space. This is a counterintuitive result in that designing failsafe and nonmasking fault tolerance for the same class of faults can be done in polynomial time. We also present sufficient conditions for polynomial-time design of failsafe-nonmasking multitolerance. Finally, we demonstrate the stepwise design of multitolerance for a stable disk storage system, a token ring network protocol and a repetitive agreement protocol that tolerates Byzantine and transient faults. Our automatic approach decreases the design time from days to a few hours for the token ring program that is our largest example with 200 million reachable states and 8 processes.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1984688105",
    "type": "article"
  },
  {
    "title": "An algebra of design patterns",
    "doi": "https://doi.org/10.1145/2491509.2491517",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Hong Zhu; Ian Bayley",
    "corresponding_authors": "",
    "abstract": "In a pattern-oriented software design process, design decisions are made by selecting and instantiating appropriate patterns, and composing them together. In our previous work, we enabled these decisions to be formalized by defining a set of operators on patterns with which instantiations and compositions can be represented. In this article, we investigate the algebraic properties of these operators. We provide and prove a complete set of algebraic laws so that equivalence between pattern expressions can be proven. Furthermore, we define an always-terminating normalization of pattern expression to a canonical form which is unique modulo equivalence in first-order logic. By a case study, the pattern-oriented design of an extensible request-handling framework, we demonstrate two practical applications of the algebraic framework. First, we can prove the correctness of a finished design with respect to the design decisions made and the formal specification of the patterns. Second, we can even derive the design from these components.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2017264764",
    "type": "article"
  },
  {
    "title": "Model-based synthesis of control software from system-level formal specifications",
    "doi": "https://doi.org/10.1145/2559934",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Federico Mari; Igor Melatti; Ivano Salvo; Enrico Tronci",
    "corresponding_authors": "",
    "abstract": "Many embedded systems are indeed software-based control systems , that is, control systems whose controller consists of control software running on a microcontroller device. This motivates investigation on formal model-based design approaches for automatic synthesis of embedded systems control software. We present an algorithm, along with a tool QKS implementing it, that from a formal model (as a discrete-time linear hybrid system ) of the controlled system ( plant ), implementation specifications (that is, number of bits in the Analog-to-Digital , AD, conversion) and system-level formal specifications (that is, safety and liveness requirements for the closed loop system ) returns correct-by-construction control software that has a Worst-Case Execution Time (WCET) linear in the number of AD bits and meets the given specifications. We show feasibility of our approach by presenting experimental results on using it to synthesize control software for a buck DC-DC converter, a widely used mixed-mode analog circuit, and for the inverted pendulum.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1999929714",
    "type": "article"
  },
  {
    "title": "GreASE",
    "doi": "https://doi.org/10.1145/2560563",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Nicoletta De Francesco; Giuseppe Lettieri; Antonella Santone; Gigliola Vaglini",
    "corresponding_authors": "",
    "abstract": "Equivalence checking plays a crucial role in formal verification to ensure the correctness of concurrent systems. However, this method cannot be scaled as easily with the increasing complexity of systems due to the state explosion problem. This article presents an efficient procedure, based on heuristic search, for checking Milner's strong and weak equivalence; to achieve higher efficiency, we actually search for a difference between two processes to be discovered as soon as possible, thus the heuristics aims to find a counterexample, even if not the minimum one, to prove nonequivalence. The presented algorithm builds the system state graph on-the-fly, during the checking, and the heuristics promotes the construction of the more promising subgraph. The heuristic function is syntax based, but the approach can be applied to different specification languages such as CCS, LOTOS, and CSP, provided that the language semantics is based on the concept of transition. The algorithm to explore the search space of the problem is based on a greedy technique; GreASE (Greedy Algorithm for System Equivalence), the tool supporting the approach, is used to evaluate the achieved reduction of both state-space size and time with respect to other verification environments.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2126506540",
    "type": "article"
  },
  {
    "title": "Precise memory leak detection for java software using container profiling",
    "doi": "https://doi.org/10.1145/2491509.2491511",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Guoqing Xu; Atanas Rountev",
    "corresponding_authors": "",
    "abstract": "A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to detect because static analysis typically cannot precisely identify these redundant references, and existing dynamic leak detection tools track and report fine-grained information about individual objects, producing results that are usually hard to interpret and lack precision. In this article we introduce a novel container-based heap-tracking technique, based on the fact that many memory leaks in Java programs occur due to incorrect uses of containers, leading to containers that keep references to unused data entries. The novelty of the described work is twofold: (1) instead of tracking arbitrary objects and finding leaks by analyzing references to unused objects, the technique tracks only containers and directly identifies the source of the leak, and (2) the technique computes a confidence value for each container based on a combination of its memory consumption and its elements' staleness (time since last retrieval), while previous approaches do not consider such combined metrics. Our experimental results show that the reports generated by the proposed technique can be very precise: for two bugs reported by Sun, a known bug in SPECjbb 2000, and an example bug from IBM developerWorks, the top containers in the reports include the containers that leak memory.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2157579724",
    "type": "article"
  },
  {
    "title": "Software Effort Interval Prediction via Bayesian Inference and Synthetic Bootstrap Resampling",
    "doi": "https://doi.org/10.1145/3295700",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Liyan Song; Leandro L. Minku; Xin Yao",
    "corresponding_authors": "",
    "abstract": "Software effort estimation (SEE) usually suffers from inherent uncertainty arising from predictive model limitations and data noise. Relying on point estimation only may ignore the uncertain factors and lead project managers (PMs) to wrong decision making. Prediction intervals (PIs) with confidence levels (CLs) present a more reasonable representation of reality, potentially helping PMs to make better-informed decisions and enable more flexibility in these decisions. However, existing methods for PIs either have strong limitations or are unable to provide informative PIs. To develop a “better” effort predictor, we propose a novel PI estimator called Synthetic Bootstrap ensemble of Relevance Vector Machines (SynB-RVM) that adopts Bootstrap resampling to produce multiple RVM models based on modified training bags whose replicated data projects are replaced by their synthetic counterparts. We then provide three ways to assemble those RVM models into a final probabilistic effort predictor, from which PIs with CLs can be generated. When used as a point estimator, SynB-RVM can either significantly outperform or have similar performance compared with other investigated methods. When used as an uncertain predictor, SynB-RVM can achieve significantly narrower PIs compared to its base learner RVM. Its hit rates and relative widths are no worse than the other compared methods that can provide uncertain estimation.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2908625311",
    "type": "article"
  },
  {
    "title": "psc2code",
    "doi": "https://doi.org/10.1145/3392093",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Lingfeng Bao; Zhenchang Xing; Xin Xia; David Lo; Minghui Wu; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Programming screencasts have become a pervasive resource on the Internet, which help developers learn new programming technologies or skills. The source code in programming screencasts is an important and valuable information for developers. But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts. Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily. However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups. Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images. The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts. In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts. First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames. Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected sub-windows, it identifies and crops the screen region that is most likely to be a code editor. Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code. We conduct an experiment on 1,142 programming screencasts from YouTube. We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames. We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words. Based on the source code denoised by psc2code , we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool. Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the precision@5, 10, and 20 of 0.93, 0.81, and 0.63, respectively. We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants. This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3033209696",
    "type": "article"
  },
  {
    "title": "On the Monitoring of Decentralized Specifications",
    "doi": "https://doi.org/10.1145/3355181",
    "publication_date": "2020-01-30",
    "publication_year": 2020,
    "authors": "Antoine El-Hokayem; Ylìès Falcone",
    "corresponding_authors": "",
    "abstract": "We introduce two complementary approaches to monitor decentralized systems. The first approach relies on systems with a centralized specification, i.e., when the specification is written for the behavior of the entire system. To do so, our approach introduces a data structure that (i) keeps track of the execution of an automaton (ii) has predictable parameters and size, and (iii) guarantees strong eventual consistency. The second approach defines decentralized specifications wherein multiple specifications are provided for separate parts of the system. We study two properties of decentralized specifications pertaining to monitorability and compatibility between specification and architecture. We also present a general algorithm for monitoring decentralized specifications. We map three existing algorithms to our approaches and provide a framework for analyzing their behavior. Furthermore, we present THEMIS, a framework for designing such decentralized algorithms and simulating their behavior. We demonstrate the usage of THEMIS to compare multiple algorithms and validate the trends predicted by the analysis in two scenarios: a synthetic benchmark and the Chiron user interface.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2996770247",
    "type": "article"
  },
  {
    "title": "CodeMatcher: Searching Code Based on Sequential Semantics of Important Query Words",
    "doi": "https://doi.org/10.1145/3465403",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Chao Liu; Xin Xia; David Lo; Z. Liu; Ahmed E. Hassan; Shanping Li",
    "corresponding_authors": "",
    "abstract": "To accelerate software development, developers frequently search and reuse existing code snippets from a large-scale codebase, e.g., GitHub. Over the years, researchers proposed many information retrieval based models for code search, but they fail to connect the semantic gap between query and code. An early successful deep learning based model DeepCS solved this issue by learning the relationship between pairs of code methods and corresponding natural language descriptions. Two major advantages of DeepCS are the capability of understanding irrelevant/noisy keywords and capturing sequential relationships between words in query and code. In this paper, we proposed an IR-based model CodeMatcher that inherits the advantages of DeepCS, while it can leverage the indexing technique in the IR-based model to accelerate the search response time substantially. CodeMatcher first collects metadata for query words to identify irrelevant/noisy ones, then iteratively performs fuzzy search with important query words on the codebase that is indexed by the Elasticsearch tool, and finally reranks a set of returned candidate code according to how the tokens in the candidate code snippet sequentially matched the important words in a query. We verified its effectiveness on a large-scale codebase with ~41k repositories. Experimental results showed that CodeMatcher achieves an MRR of 0.60, outperforming DeepCS, CodeHow, and UNIF by 82%, 62%, and 46% respectively. Our proposed model is over 1.2k times faster than DeepCS. Moreover, CodeMatcher outperforms GitHub and Google search by 46% and 33% respectively in terms of MRR. We also observed that: fusing the advantages of IR-based and DL-based models is promising; improving the quality of method naming helps code search, since method name plays an important role in connecting query and code.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3203911409",
    "type": "article"
  },
  {
    "title": "Test Data Generation for Path Coverage of MPI Programs Using SAEO",
    "doi": "https://doi.org/10.1145/3423132",
    "publication_date": "2021-01-03",
    "publication_year": 2021,
    "authors": "Dunwei Gong; Baicai Sun; Xiangjuan Yao; Tian Tian",
    "corresponding_authors": "",
    "abstract": "Message-passing interface (MPI) programs, a typical kind of parallel programs, have been commonly used in various applications. However, it generally takes exhaustive computation to run these programs when generating test data to test them. In this article, we propose a method of test data generation for path coverage of MPI programs using surrogate-assisted evolutionary optimization, which can efficiently generate test data with high quality. We first divide a sample set of a program into a number of clusters according to the multi-mode characteristic of the coverage problem, with each cluster training a surrogate model. Then, we estimate the fitness of each individual using one or more surrogate models when generating test data through evolving a population. Finally, a small number of representative individuals are selected to execute the program, with the purpose of obtaining their real fitness, to guide the subsequent evolution of the population. We apply the proposed method to seven benchmark MPI programs and compare it with several state-of-the-art approaches. The experimental results show that the proposed method can generate test data with reduced computation, thus improving the testing efficiency.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3119972155",
    "type": "article"
  },
  {
    "title": "Toward an Objective Measure of Developers’ Cognitive Activities",
    "doi": "https://doi.org/10.1145/3434643",
    "publication_date": "2021-03-09",
    "publication_year": 2021,
    "authors": "Zohreh Sharafi; Yu Huang; Kevin Leach; Westley Weimer",
    "corresponding_authors": "",
    "abstract": "Understanding how developers carry out different computer science activities with objective measures can help to improve productivity and guide the use and development of supporting tools in software engineering. In this article, we present two controlled experiments involving 112 students to explore multiple computing activities (code comprehension, code review, and data structure manipulations) using three different objective measures including neuroimaging (functional near-infrared spectroscopy (fNIRS) and functional magnetic resonance imaging (fMRI)) and eye tracking. By examining code review and prose review using fMRI, we find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity, and those task distinctions are modulated by expertise. We leverage insights from the psychological notion of spatial ability to decode the neural representations of several fundamental data structures and their manipulations using fMRI, fNIRS, and eye tracking. We examine list, array, tree, and mental rotation tasks and find that data structure and spatial operations use the same focal regions of the brain but to different degrees: they are related but distinct neural tasks. We demonstrate best practices and describe the implication and tradeoffs between fMRI, fNIRS, eye tracking, and self-reporting for software engineering research.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3135365795",
    "type": "article"
  },
  {
    "title": "Predicting Performance Anomalies in Software Systems at Run-time",
    "doi": "https://doi.org/10.1145/3440757",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Guoliang Zhao; Safwat Hassan; Ying Zou; Derek Truong; Toby Corbin",
    "corresponding_authors": "",
    "abstract": "High performance is a critical factor to achieve and maintain the success of a software system. Performance anomalies represent the performance degradation issues (e.g., slowing down in system response times) of software systems at run-time. Performance anomalies can cause a dramatically negative impact on users’ satisfaction. Prior studies propose different approaches to detect anomalies by analyzing execution logs and resource utilization metrics after the anomalies have happened. However, the prior detection approaches cannot predict the anomalies ahead of time; such limitation causes an inevitable delay in taking corrective actions to prevent performance anomalies from happening. We propose an approach that can predict performance anomalies in software systems and raise anomaly warnings in advance. Our approach uses a Long-Short Term Memory neural network to capture the normal behaviors of a software system. Then, our approach predicts performance anomalies by identifying the early deviations from the captured normal system behaviors. We conduct extensive experiments to evaluate our approach using two real-world software systems (i.e., Elasticsearch and Hadoop). We compare the performance of our approach with two baselines. The first baseline is one state-to-the-art baseline called Unsupervised Behavior Learning. The second baseline predicts performance anomalies by checking if the resource utilization exceeds pre-defined thresholds. Our results show that our approach can predict various performance anomalies with high precision (i.e., 97–100%) and recall (i.e., 80–100%), while the baselines achieve 25–97% precision and 93–100% recall. For a range of performance anomalies, our approach can achieve sufficient lead times that vary from 20 to 1,403 s (i.e., 23.4 min). We also demonstrate the ability of our approach to predict the performance anomalies that are caused by real-world performance bugs. For predicting performance anomalies that are caused by real-world performance bugs, our approach achieves 95–100% precision and 87–100% recall, while the baselines achieve 49–83% precision and 100% recall. The obtained results show that our approach outperforms the existing anomaly prediction approaches and is able to predict performance anomalies in real-world systems.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3161148246",
    "type": "article"
  },
  {
    "title": "Evaluation of Software Architectures under Uncertainty",
    "doi": "https://doi.org/10.1145/3464305",
    "publication_date": "2021-08-03",
    "publication_year": 2021,
    "authors": "Dalia Sobhy; Rami Bahsoon; Leandro L. Minku; Rick Kazman",
    "corresponding_authors": "",
    "abstract": "Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3192541372",
    "type": "article"
  },
  {
    "title": "Automatic Fault Detection for Deep Learning Programs Using Graph Transformations",
    "doi": "https://doi.org/10.1145/3470006",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Amin Nikanjam; Houssem Ben Braiek; Mohammad Mehdi Morovati; Foutse Khomh",
    "corresponding_authors": "",
    "abstract": "Nowadays, we are witnessing an increasing demand in both corporates and academia for exploiting Deep Learning ( DL ) to solve complex real-world problems. A DL program encodes the network structure of a desirable DL model and the process by which the model learns from the training dataset. Like any software, a DL program can be faulty, which implies substantial challenges of software quality assurance, especially in safety-critical domains. It is therefore crucial to equip DL development teams with efficient fault detection techniques and tools. In this article, we propose NeuraLint , a model-based fault detection approach for DL programs, using meta-modeling and graph transformations. First, we design a meta-model for DL programs that includes their base skeleton and fundamental properties. Then, we construct a graph-based verification process that covers 23 rules defined on top of the meta-model and implemented as graph transformations to detect faults and design inefficiencies in the generated models (i.e., instances of the meta-model). First, the proposed approach is evaluated by finding faults and design inefficiencies in 28 synthesized examples built from common problems reported in the literature. Then NeuraLint successfully finds 64 faults and design inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts and GitHub repositories. The results show that NeuraLint effectively detects faults and design issues in both synthesized and real-world examples with a recall of 70.5% and a precision of 100%. Although the proposed meta-model is designed for feedforward neural networks, it can be extended to support other neural network architectures such as recurrent neural networks. Researchers can also expand our set of verification rules to cover more types of issues in DL programs.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3203969037",
    "type": "article"
  },
  {
    "title": "Towards a Consistent Interpretation of AIOps Models",
    "doi": "https://doi.org/10.1145/3488269",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Yingzhe Lyu; Gopi Krishnan Rajbahadur; Dayi Lin; Boyuan Chen; Zhen Ming Jiang",
    "corresponding_authors": "",
    "abstract": "Artificial Intelligence for IT Operations (AIOps) has been adopted in organizations in various tasks, including interpreting models to identify indicators of service failures. To avoid misleading practitioners, AIOps model interpretations should be consistent (i.e., different AIOps models on the same task agree with one another on feature importance). However, many AIOps studies violate established practices in the machine learning community when deriving interpretations, such as interpreting models with suboptimal performance, though the impact of such violations on the interpretation consistency has not been studied. In this article, we investigate the consistency of AIOps model interpretation along three dimensions: internal consistency, external consistency, and time consistency. We conduct a case study on two AIOps tasks: predicting Google cluster job failures and Backblaze hard drive failures. We find that the randomness from learners, hyperparameter tuning, and data sampling should be controlled to generate consistent interpretations. AIOps models with AUCs greater than 0.75 yield more consistent interpretation compared to low-performing models. Finally, AIOps models that are constructed with the Sliding Window or Full History approaches have the most consistent interpretation with the trends presented in the entire datasets. Our study provides valuable guidelines for practitioners to derive consistent AIOps model interpretation.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3211945175",
    "type": "article"
  },
  {
    "title": "Nudge: Accelerating Overdue Pull Requests toward Completion",
    "doi": "https://doi.org/10.1145/3544791",
    "publication_date": "2022-06-25",
    "publication_year": 2022,
    "authors": "Chandra Maddila; Sai Surya Upadrasta; Chetan Bansal; Nachiappan Nagappan; Georgios Gousios; Arie van Deursen",
    "corresponding_authors": "",
    "abstract": "Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests towards completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue, but for which sufficient action is taking place nonetheless. Lastly, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge's ability to scale to thousands of repositories. Lastly, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3109196559",
    "type": "article"
  },
  {
    "title": "Correlating Automated and Human Evaluation of Code Documentation Generation Quality",
    "doi": "https://doi.org/10.1145/3502853",
    "publication_date": "2022-05-23",
    "publication_year": 2022,
    "authors": "Xing Hu; Qiuyuan Chen; Haoye Wang; Xin Xia; David Lo; Thomas Zimmermann",
    "corresponding_authors": "",
    "abstract": "Automatic code documentation generation has been a crucial task in the field of software engineering. It not only relieves developers from writing code documentation but also helps them to understand programs better. Specifically, deep-learning-based techniques that leverage large-scale source code corpora have been widely used in code documentation generation. These works tend to use automatic metrics (such as BLEU, METEOR, ROUGE, CIDEr, and SPICE) to evaluate different models. These metrics compare generated documentation to reference texts by measuring the overlapping words. Unfortunately, there is no evidence demonstrating the correlation between these metrics and human judgment. We conduct experiments on two popular code documentation generation tasks, code comment generation and commit message generation, to investigate the presence or absence of correlations between these metrics and human judgments. For each task, we replicate three state-of-the-art approaches and the generated documentation is evaluated automatically in terms of BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. We also ask 24 participants to rate the generated documentation considering three aspects (i.e., language, content, and effectiveness). Each participant is given Java methods or commit diffs along with the target documentation to be rated. The results show that the ranking of generated documentation from automatic metrics is different from that evaluated by human annotators. Thus, these automatic metrics are not reliable enough to replace human evaluation for code documentation generation tasks. In addition, METEOR shows the strongest correlation (with moderate Pearson correlation r about 0.7) to human evaluation metrics. However, it is still much lower than the correlation observed between different annotators (with a high Pearson correlation r about 0.8) and correlations that are reported in the literature for other tasks (e.g., Neural Machine Translation [ 39 ]). Our study points to the need to develop specialized automated evaluation metrics that can correlate more closely to human evaluation metrics for code generation tasks.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4281400706",
    "type": "article"
  },
  {
    "title": "Evaluating Surprise Adequacy for Deep Learning System Testing",
    "doi": "https://doi.org/10.1145/3546947",
    "publication_date": "2022-07-06",
    "publication_year": 2022,
    "authors": "Jin-Han Kim; Robert Feldt; Shin Yoo",
    "corresponding_authors": "",
    "abstract": "The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4283830352",
    "type": "article"
  },
  {
    "title": "Making Sense of the Unknown: How Managers Make Cyber Security Decisions",
    "doi": "https://doi.org/10.1145/3548682",
    "publication_date": "2022-08-01",
    "publication_year": 2022,
    "authors": "Benjamin Shreeve; Catarina Gralha; Awais Rashid; Jo�ão Araújo; Miguel Goulão",
    "corresponding_authors": "",
    "abstract": "Managers rarely have deep knowledge of cyber security and yet are expected to make decisions with cyber security implications for software-based systems. We investigate the decision-making conversations of seven teams of senior managers from the same organisation as they complete the Decisions &amp; Disruptions cyber security exercise. We use grounded theory to situate our analysis of their decision-making and help us explore how these complex socio-cognitive interactions occur. We have developed a goal-model (using iStar 2.0) of the teams’ dialogue that illustrates what cyber security goals teams identify and how they operationalise their decisions to reach these goals. We complement this with our model of cyber security reasoning that describes how these teams make their decisions, showing how each team members’ experience, intuition, and understanding affects the team’s overall shared reasoning and decision-making. Our findings show how managers with little cyber security expertise are able to use logic and traditional risk management thinking to make cyber security decisions. Despite their lack of cyber security–specific training, they demonstrate reasoning that closely resembles the decision-making approaches espoused in cyber security–specific standards (e.g., NIST/ISO). Our work demonstrates how organisations and practitioners can enrich goal modelling to capture not only what security goals an organisation has (and how they can operationalise them) but also how and why these goals have been identified. Ultimately, non–cyber security experts can develop their cyber security model based on their current context (and update it when new requirements appear or new incidents happen), whilst capturing their reasoning at every stage.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4289225773",
    "type": "article"
  },
  {
    "title": "Turnover of Companies in OpenStack: Prevalence and Rationale",
    "doi": "https://doi.org/10.1145/3510849",
    "publication_date": "2022-03-21",
    "publication_year": 2022,
    "authors": "Yuxia Zhang; Hui Liu; Xin Tan; Minghui Zhou; Zhi Jin; Jiaxin Zhu",
    "corresponding_authors": "",
    "abstract": "To achieve commercial goals, companies have made substantial contributions to large open-source software (OSS) ecosystems such as OpenStack and have become the main contributors. However, they often withdraw their employees for a variety of reasons, which may affect the sustainability of OSS projects. While the turnover of individual contributors has been extensively investigated, there is a lack of knowledge about the nature of companies’ withdrawal. To this end, we conduct a mixed-methods empirical study on OpenStack to reveal how common company withdrawals were, to what degree withdrawn companies made contributions, and what the rationale behind withdrawals was. By analyzing the commit data of 18 versions of OpenStack, we find that the number of companies that have left is increasing and even surpasses the number of companies that have joined in later versions. Approximately 12% of the companies in each version have exited by the next version. Compared to the sustaining companies that joined in the same version, the withdrawn companies tend to have a weaker contribution intensity but contribute to a similar scope of repositories in OpenStack. Through conducting a developer survey, we find four aspects of reasons for companies’ withdrawal from OpenStack: company, community, developer, and project. The most common reasons lie in the company aspect, i.e., the company either achieved its goals or failed to do so. By fitting the survival analysis model, we find that commercial goals are associated with the probability of the company’s withdrawal, and that a company’s contribution intensity and scale are positively correlated with its retention. Maintaining good retention is important but challenging for OSS ecosystems, and our results may shed light on potential approaches to improve company retention and reduce the negative impact of company withdrawal.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4293146358",
    "type": "article"
  },
  {
    "title": "1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis",
    "doi": "https://doi.org/10.1145/3561385",
    "publication_date": "2022-09-06",
    "publication_year": 2022,
    "authors": "Ang Jia; Ming Fan; Wuxia Jin; Xi Xu; Zhaohui Zhou; Qiyi Tang; Sen Nie; Shi Wu; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. “ 1-to-1 ” mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of “ 1-to-n ” (one binary function matches multiple source functions or binary functions) or even “ n-to-n ” (multiple binary functions match multiple binary functions) due to the existence of function inlining , different from traditional understanding. In this article, we investigate the effect of function inlining on binary similarity analysis. We carry out three studies to investigate the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Firstly, a scalable and lightweight identification method is designed to recover function inlining in binaries. 88 projects (compiled in 288 versions and resulting in 32,460,156 binary functions) are collected and analyzed to construct four inlining-oriented datasets for four security tasks in the software supply chain, including code search, OSS (Open Source Software) reuse detection, vulnerability detection, and patch presence test. Datasets reveal that the proportion of function inlining ranges from 30–40% when using O3 and sometimes can reach nearly 70%. Then, we evaluate four existing works on our dataset. Results show most existing works neglect inlining and use the “1-to-1” mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, most inlined functions would be ignored during OSS reuse detection and patch presence test, thus leaving these functions risky. Finally, we analyze two inlining-simulation strategies on our dataset. It is shown that they miss nearly 40% of the inlined functions, and there is still a large space for promotion. By precisely recovering when function inlining happens, we discover that inlining is usually cumulative when optimization increases. Thus, conditional inlining and incremental inlining are recommended to design a low-cost and high-coverage inlining-simulation strategy.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4294733240",
    "type": "article"
  },
  {
    "title": "Video Game Bad Smells: What They Are and How Developers Perceive Them",
    "doi": "https://doi.org/10.1145/3563214",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Vittoria Nardone; Biruk Asmare Muse; Mouna Abidi; Foutse Khomh; Massimiliano Di Penta",
    "corresponding_authors": "",
    "abstract": "Video games represent a substantial and increasing share of the software market. However, their development is particularly challenging as it requires multi-faceted knowledge, which is not consolidated in computer science education yet. This article aims at defining a catalog of bad smells related to video game development. To achieve this goal, we mined discussions on general-purpose and video game-specific forums. After querying such a forum, we adopted an open coding strategy on a statistically significant sample of 572 discussions, stratified over different forums. As a result, we obtained a catalog of 28 bad smells, organized into five categories, covering problems related to game design and logic, physics, animation, rendering, or multiplayer. Then, we assessed the perceived relevance of such bad smells by surveying 76 game development professionals. The survey respondents agreed with the identified bad smells but also provided us with further insights about the discussed smells. Upon reporting results, we discuss bad smell examples, their consequences, as well as possible mitigation/fixing strategies and trade-offs to be pursued by developers. The catalog can be used not only as a guideline for developers and educators but also can pave the way toward better automated tool support for video game developers.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4296132134",
    "type": "article"
  },
  {
    "title": "On Wasted Contributions: Understanding the Dynamics of Contributor-Abandoned Pull Requests–A Mixed-Methods Study of 10 Large Open-Source Projects",
    "doi": "https://doi.org/10.1145/3530785",
    "publication_date": "2022-05-11",
    "publication_year": 2022,
    "authors": "SayedHassan Khatoonabadi; Diego Elias Costa; Rabe Abdalkareem; Emad Shihab",
    "corresponding_authors": "",
    "abstract": "Pull-based development has enabled numerous volunteers to contribute to open-source projects with fewer barriers. Nevertheless, a considerable amount of pull requests (PRs) with valid contributions are abandoned by their contributors, wasting the effort and time put in by both the contributors and maintainers. To better understand the underlying dynamics of contributor-abandoned PRs, we conduct a mixed-methods study using both quantitative and qualitative methods. We curate a dataset consisting of 265,325 PRs including 4,450 abandoned ones from ten popular and mature GitHub projects and measure 16 features characterizing PRs, contributors, review processes, and projects. Using statistical and machine learning techniques, we find that complex PRs, novice contributors, and lengthy reviews have a higher probability of abandonment and the rate of PR abandonment fluctuates alongside the projects' maturity or workload. To identify why contributors abandon their PRs, we also manually examine a random sample of 354 abandoned PRs. We observe that the most frequent abandonment reasons are related to the obstacles faced by contributors, followed by the hurdles imposed by maintainers during the review process. Finally, we survey the top core maintainers of the studied projects to understand their perspectives on dealing with PR abandonment and on our findings.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3209468076",
    "type": "article"
  },
  {
    "title": "Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering",
    "doi": "https://doi.org/10.1145/3550271",
    "publication_date": "2022-07-22",
    "publication_year": 2022,
    "authors": "Mohammed Oualid Attaoui; Hazem Fahmy; Fabrizio Pastore; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this paper, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4286485305",
    "type": "article"
  },
  {
    "title": "<tt>sem2vec</tt> : Semantics-aware Assembly Tracelet Embedding",
    "doi": "https://doi.org/10.1145/3569933",
    "publication_date": "2022-10-28",
    "publication_year": 2022,
    "authors": "Huaijin Wang; Pingchuan Ma; Shuai Wang; Qiyi Tang; Sen Nie; Shi Wu",
    "corresponding_authors": "",
    "abstract": "Binary code similarity is the foundation of many security and software engineering applications. Recent works leverage deep neural networks (DNN) to learn a numeric vector representation (namely, embeddings ) of assembly functions, enabling similarity analysis in the numeric space. However, existing DNN-based techniques capture syntactic-, control flow-, or data flow-level information of assembly code, which is too coarse-grained to represent program functionality. These methods can suffer from low robustness to challenging settings such as compiler optimizations and obfuscations. We present sem2vec , a binary code embedding framework that learns from semantics . Given the control-flow graph (CFG), 34 pages. of an assembly function, we divide it into tracelets , denoting continuous and short execution traces that are reachable from the function entry point. We use symbolic execution to extract symbolic constraints and other auxiliary information on each tracelet. We then train masked language models to compute embeddings of symbolic execution outputs. Last, we use graph neural networks, to aggregate tracelet embeddings into the CFG-level embedding for a function. Our evaluation shows that sem2vec extracts high-quality embedding and is robust against different compilers, optimizations, architectures, and popular obfuscation methods including virtualization obfuscation. We further augment a vulnerability search application with embeddings computed by sem2vec and demonstrate a significant improvement in vulnerability search accuracy.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4307510779",
    "type": "article"
  },
  {
    "title": "Do Performance Aspirations Matter for Guiding Software Configuration Tuning? An Empirical Investigation under Dual Performance Objectives",
    "doi": "https://doi.org/10.1145/3571853",
    "publication_date": "2022-11-24",
    "publication_year": 2022,
    "authors": "Tao Chen; Miqing Li",
    "corresponding_authors": "",
    "abstract": "Configurable software systems can be tuned for better performance. Leveraging on some Pareto optimizers, recent work has shifted from tuning for a single, time-related performance objective to two intrinsically different objectives that assess distinct performance aspects of the system, each with varying aspirations to be satisfied, e.g., “ the latency is less than 10s ” while “ the memory usage is no more than 1GB ”. Before we design better optimizers, a crucial engineering decision to make therein is how to handle the performance requirements with clear aspirations in the tuning process. For this, the community takes two alternative optimization models: either quantifying and incorporating the aspirations into the search objectives that guide the tuning, or not considering the aspirations during the search but purely using them in the later decision-making process only. However, despite being a crucial decision that determines how an optimizer can be designed and tailored, there is a rather limited understanding of which optimization model should be chosen under what particular circumstance, and why. In this article, we seek to close this gap. Firstly, we do that through a review of over 426 articles in the literature and 14 real-world requirements datasets, from which we summarize four performance requirement patterns that quantify the aspirations in the configuration tuning. Drawing on these, we then conduct a comprehensive empirical study that covers 15 combinations of the state-of-the-art performance requirement patterns, four types of aspiration space, three Pareto optimizers, and eight real-world systems/environments, leading to 1,296 cases of investigation. Our findings reveal that (1) the realism of aspirations is the key factor that determines whether they should be used to guide the tuning; (2) the given patterns and the position of the realistic aspirations in the objective landscape are less important for the choice, but they do matter to the extents of improvement; (3) the available tuning budget can also influence the choice for unrealistic aspirations but it is insignificant under realistic ones. To promote open science practice, we make our code and dataset publicly available at: https://github.com/ideas-labo/aspiration-study .",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4309916036",
    "type": "article"
  },
  {
    "title": "I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages",
    "doi": "https://doi.org/10.1145/3576037",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Daniel Venturini; Filipe R. Cogo; Ivanilton Polato; Marco Aurélio Gerosa; Igor Wiese",
    "corresponding_authors": "",
    "abstract": "Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm ) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes , dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages’ builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider’s version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4317209725",
    "type": "article"
  },
  {
    "title": "Estimating Software Functional Size via Machine Learning",
    "doi": "https://doi.org/10.1145/3582575",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "Luigi Lavazza; Angela Locoro; Geng Liu; Roberto Meli",
    "corresponding_authors": "",
    "abstract": "Measuring software functional size via standard Function Points Analysis (FPA) requires the availability of fully specified requirements and specific competencies. Most of the time, the need to measure software functional size occurs well in advance with respect to these ideal conditions, under the lack of complete information or skilled experts. To work around the constraints of the official measurement process, several estimation methods for FPA have been proposed and are commonly used. Among these, the International Function Points User Group (IFPUG) has adopted the “High-level FPA” method (also known as the NESMA method). This method avoids weighting each data and transaction function by using fixed weights instead. Applying High-level FPA, or similar estimation methods, is faster and easier than carrying out the official measurement process but inevitably yields an approximation in the measures. In this article, we contribute to the problem of estimating software functional size measures by using machine learning. To the best of our knowledge, machine learning methods were never applied to the early estimation of software functional size. Our goal is to understand whether machine learning techniques yield estimates of FPA measures that are more accurate than those obtained with High-level FPA or similar methods. An empirical study on a large dataset of functional size predictors was carried out to train and test three of the most popular and robust machine learning methods, namely Random Forests, Support Vector Regression , and Neural Networks. A systematic experimental phase, with cycles of dataset filtering and splitting, parameter tuning, and model training and validation, is presented. The estimation accuracy of the obtained models was then evaluated and compared to that of fixed-weight models (e.g., High-level FPA) and linear regression models, also using a second dataset as the test set. We found that Support Vector Regression yields quite accurate estimation models. However, the obtained level of accuracy does not appear significantly better with respect to High-level FPA or to models built via ordinary least squares regression. Noticeably, fairly good accuracy levels were obtained by models that do not even require discerning among different types of transactions and data.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4318618274",
    "type": "article"
  },
  {
    "title": "Faire: Repairing Fairness of Neural Networks via Neuron Condition Synthesis",
    "doi": "https://doi.org/10.1145/3617168",
    "publication_date": "2023-08-24",
    "publication_year": 2023,
    "authors": "Tianlin Li; Xiaofei Xie; Jian Wang; Qing Guo; Aishan Liu; Lei Ma; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) have achieved tremendous success in many applications, while it has been demonstrated that DNNs can exhibit some undesirable behaviors on concerns such as robustness, privacy, and other trustworthiness issues. Among them, fairness (i.e., non-discrimination) is one important property, especially when they are applied to some sensitive applications (e.g., finance and employment). However, DNNs easily learn spurious correlations between protected attributes (e.g., age, gender, race) and the classification task and develop discriminatory behaviors if the training data is imbalanced. Such discriminatory decisions in sensitive applications would introduce severe social impacts. To expose potential discrimination problems in DNNs before putting them in use, some testing techniques have been proposed to identify the discriminatory instances (i.e., instances that show defined discrimination 1 ). However, how to repair DNNs after detecting such discrimination is still challenging. Existing techniques mainly rely on retraining on a large number of discriminatory instances generated by testing methods, which requires huge time overhead and makes the repairing inefficient. In this work, we propose the method Faire to effectively and efficiently repair the fairness issues of DNNs, without using additional data (e.g., discriminatory instances). Our basic idea is inspired by the traditional program repair method that synthesizes proper condition checking. To repair traditional programs, a typical method is to localize the program defects and repair the program logic by adding condition checking. Similarly, for DNNs, we try to understand the unfair logic and reformulate it with well-designed condition checking. In this article, we synthesize the condition that can reduce the effect of features relevant to the protected attributes in the DNN. Specifically, we first perform the neuron-based analysis and check the functionalities of neurons to identify neurons whose outputs could be regarded as features relevant to protected attributes and original tasks. Then a new condition layer is added after each hidden layer to penalize neurons that are accountable for the protected features (i.e., intermediate features relevant to protected attributes) and promote neurons that are accountable for the non-protected features (i.e., intermediate features relevant to original tasks). In sum, the repair rate 2 of Faire reaches up to more than 99%, which outperforms other methods, and the whole repairing process only takes no more than 340 s. The evaluation results demonstrate that our approach can effectively and efficiently repair the individual discriminatory instances of the target model.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4386136237",
    "type": "article"
  },
  {
    "title": "Search-Based Software Testing Driven by Automatically Generated and Manually Defined Fitness Functions",
    "doi": "https://doi.org/10.1145/3624745",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Federico Formica; Tony Fan; Claudio Menghi",
    "corresponding_authors": "",
    "abstract": "Search-based software testing (SBST) typically relies on fitness functions to guide the search exploration toward software failures. There are two main techniques to define fitness functions: (a) automated fitness function computation from the specification of the system requirements, and (b) manual fitness function design. Both techniques have advantages. The former uses information from the system requirements to guide the search toward portions of the input domain more likely to contain failures. The latter uses the engineers’ domain knowledge. We propose ATheNA , a novel SBST framework that combines fitness functions automatically generated from requirements specifications and those manually defined by engineers. We design and implement ATheNA-S , an instance of ATheNA that targets Simulink ® models. We evaluate ATheNA-S by considering a large set of models from different domains. Our results show that ATheNA-S generates more failure-revealing test cases than existing baseline tools and that the difference between the runtime performance of ATheNA-S and the baseline tools is not statistically significant. We also assess whether ATheNA-S could generate failure-revealing test cases when applied to two representative case studies: one from the automotive domain and one from the medical domain. Our results show that ATheNA-S successfully revealed a requirement violation in our case studies.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4386889431",
    "type": "article"
  },
  {
    "title": "LibAM: An Area Matching Framework for Detecting Third-Party Libraries in Binaries",
    "doi": "https://doi.org/10.1145/3625294",
    "publication_date": "2023-09-26",
    "publication_year": 2023,
    "authors": "Siyuan Li; Yongpan Wang; Chaopeng Dong; Shouguo Yang; Hong Li; Hao Sun; Zhe Lang; Zuxin Chen; Weijie Wang; Hongsong Zhu; Limin Sun",
    "corresponding_authors": "",
    "abstract": "Third-party libraries (TPLs) are extensively utilized by developers to expedite the software development process and incorporate external functionalities. Nevertheless, insecure TPL reuse can lead to significant security risks. Existing methods, which involve extracting strings or conducting function matching, are employed to determine the presence of TPL code in the target binary. However, these methods often yield unsatisfactory results due to the recurrence of strings and the presence of numerous similar non-homologous functions. Furthermore, the variation in C/C++ binaries across different optimization options and architectures exacerbates the problem. Additionally, existing approaches struggle to identify specific pieces of reused code in the target binary, complicating the detection of complex reuse relationships and impeding downstream tasks. And, we call this issue the poor interpretability of TPL detection results. In this article, we observe that TPL reuse typically involves not just isolated functions but also areas encompassing several adjacent functions on the Function Call Graph (FCG). We introduce LibAM, a novel Area Matching framework that connects isolated functions into function areas on FCG and detects TPLs by comparing the similarity of these function areas, significantly mitigating the impact of different optimization options and architectures. Furthermore, LibAM is the first approach capable of detecting the exact reuse areas on FCG and offering substantial benefits for downstream tasks. To validate our approach, we compile the first TPL detection dataset for C/C++ binaries across various optimization options and architectures. Experimental results demonstrate that LibAM outperforms all existing TPL detection methods and provides interpretable evidence for TPL detection results by identifying exact reuse areas. We also evaluate LibAM’s scalability on large-scale, real-world binaries in IoT firmware and generate a list of potential vulnerabilities for these devices. Our experiments indicate that the Area Matching framework performs exceptionally well in the TPL detection task and holds promise for other binary similarity analysis tasks. Last but not least, by analyzing the detection results of IoT firmware, we make several interesting findings, for instance, different target binaries always tend to reuse the same code area of TPL. The datasets and source code used in this article are available at https://github.com/Siyuan-Li201/LibAM .",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4387055195",
    "type": "article"
  },
  {
    "title": "Causality-driven Testing of Autonomous Driving Systems",
    "doi": "https://doi.org/10.1145/3635709",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Luca Giamattei; Antonio Guerriero; Roberto Pietrantuono; Stefano Russo",
    "corresponding_authors": "",
    "abstract": "Testing Autonomous Driving Systems (ADS) is essential for safe development of self-driving cars. For thorough and realistic testing, ADS are usually embedded in a simulator and tested in interaction with the simulated environment. However, their high complexity and the multiple safety requirements lead to costly and ineffective testing. Recent techniques exploit many-objective strategies and ML to efficiently search the huge input space. Despite the indubitable advances, the need for smartening the search keep being pressing. This article presents CART ( CAusal-Reasoning-driven Testing ), a new technique that formulates testing as a causal reasoning task. Learning causation, unlike correlation, allows assessing the effect of actively changing an input on the output, net of possible confounding variables. CART first infers the causal relations between test inputs and outputs, then looks for promising tests by querying the learnt model. Only tests suggested by the model are run on the simulator. An extensive empirical evaluation, using Pylot as ADS and CARLA as simulator, compares CART with state-of-the-art algorithms used recently on ADS. CART shows a significant gain in exposing more safety violations and does so more efficiently. More broadly, the work opens to a wider exploitation of causal learning beside (or on top of) ML for testing-related tasks.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4389337793",
    "type": "article"
  },
  {
    "title": "<scp>Octopus</scp> : Scaling Value-Flow Analysis via Parallel Collection of Realizable Path Conditions",
    "doi": "https://doi.org/10.1145/3632743",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Wensheng Tang; Dejun Dong; Shijie Li; Chengpeng Wang; Peisen Yao; Jinguo Zhou; Charles Zhang",
    "corresponding_authors": "",
    "abstract": "Value-flow analysis is a fundamental technique in program analysis, benefiting various clients, such as memory corruption detection and taint analysis. However, existing efforts suffer from the low potential speedup that leads to a deficiency in scalability. In this work, we present a parallel algorithm Octopus to collect path conditions for realizable paths efficiently. Octopus builds on the realizability decomposition to collect the intraprocedural path conditions of different functions simultaneously on-demand and obtain realizable path conditions by concatenation, which achieves a high potential speedup in parallelization. We implement Octopus as a tool and evaluate it over 15 real-world programs. The experiment shows that Octopus significantly outperforms the state-of-the-art algorithms. Particularly, it detects NULL-pointer-dereference bugs for the project llvm with 6.3 MLoC within 6.9 minutes under the 40-thread setting. We also state and prove several theorems to demonstrate the soundness, completeness, and high potential speedup of Octopus . Our empirical and theoretical results demonstrate the great potential of Octopus in supporting various program analysis clients. The implementation has officially deployed at Ant Group, scaling the nightly code scan for massive FinTech applications.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391169686",
    "type": "article"
  },
  {
    "title": "Understanding Real-time Collaborative Programming: a Study of Visual Studio Live Share",
    "doi": "https://doi.org/10.1145/3643672",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Xin Tan; Xinyue Lv; Jing Jiang; Li Zhang",
    "corresponding_authors": "",
    "abstract": "Real-time collaborative programming (RCP) entails developers working simultaneously, regardless of their geographic locations. RCP differs from traditional asynchronous online programming methods, such as Git or SVN, where developers work independently and update the codebase at separate times. Although various real-time code collaboration tools (e.g., Visual Studio Live Share , Code with Me , and Replit ) have kept emerging in recent years, none of the existing studies explicitly focus on a deep understanding of the processes or experiences associated with RCP. To this end, we combine interviews and an e-mail survey with the users of Visual Studio Live Share , aiming to understand (i) the scenarios, (ii) the requirements, and (iii) the challenges when developers participate in RCP. We find that developers participate in RCP in 18 different scenarios belonging to six categories, e.g., pair programming , group debugging , and code review . However, existing users’ attitudes toward the usefulness of the current RCP tools in these scenarios were significantly more negative than the expectations of potential users. As for the requirements, the most critical category is live editing , followed by the need for sharing terminals to enable hosts and guests to run commands and see the results, as well as focusing and following , which involves “following” the host’s edit location and “focusing” the guests’ attention on the host with a notification. Under these categories, we identify 17 requirements, but most of them are not well supported by current tools. In terms of challenges, we identify 19 challenges belonging to seven categories. The most severe category of challenges is lagging followed by permissions and conflicts . The above findings indicate that the current RCP tools and even collaborative environment need to be improved greatly and urgently. Based on these findings, we discuss the recommendations for different stakeholders, including practitioners, tool designers, and researchers.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391278012",
    "type": "article"
  },
  {
    "title": "Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks",
    "doi": "https://doi.org/10.1145/3644387",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Jiaxiang Liu; Yunhan Xing; Xiaomu Shi; Fu Song; Zhiwu Xu; Zhong Ming",
    "corresponding_authors": "",
    "abstract": "As a new programming paradigm, deep neural networks (DNNs) have been increasingly deployed in practice, but the lack of robustness hinders their applications in safety-critical domains. While there are techniques for verifying DNNs with formal guarantees, they are limited in scalability and accuracy. In this article, we present a novel counterexample-guided abstraction refinement (CEGAR) approach for scalable and exact verification of DNNs. Specifically, we propose a novel abstraction to break down the size of DNNs by over-approximation. The result of verifying the abstract DNN is conclusive if no spurious counterexample is reported. To eliminate each spurious counterexample introduced by abstraction, we propose a novel counterexample-guided refinement that refines the abstract DNN to exclude the spurious counterexample while still over-approximating the original one, leading to a sound, complete yet efficient CEGAR approach. Our approach is orthogonal to and can be integrated with many existing verification techniques. For demonstration, we implement our approach using two promising tools, Marabou and Planet , as the underlying verification engines, and evaluate on widely used benchmarks for three datasets ACAS , Xu , MNIST , and CIFAR-10 . The results show that our approach can boost their performance by solving more problems in the same time limit, reducing on average 13.4%–86.3% verification time of Marabou on almost all the verification tasks, and reducing on average 8.3%–78.0% verification time of Planet on all the verification tasks. Compared to the most relevant CEGAR-based approach, our approach is 11.6–26.6 times faster.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391568841",
    "type": "article"
  },
  {
    "title": "DinoDroid: Testing Android Apps Using Deep Q-Networks",
    "doi": "https://doi.org/10.1145/3652150",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Yu Zhao; Brent Harrison; Tingting Yu",
    "corresponding_authors": "",
    "abstract": "The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers need to guarantee the quality of mobile apps before it is released to the market. There have been many approaches using different strategies to test the GUI of mobile apps. However, they still need improvement due to their limited effectiveness. In this article, we propose DinoDroid, an approach based on deep Q-networks to automate testing of Android apps. DinoDroid learns a behavior model from a set of existing apps and the learned model can be used to explore and generate tests for new apps. DinoDroid is able to capture the fine-grained details of GUI events (e.g., the content of GUI widgets) and use them as features that are fed into deep neural network, which acts as the agent to guide app exploration. DinoDroid automatically adapts the learned model during the exploration without the need of any modeling strategies or pre-defined rules. We conduct experiments on 64 open-source Android apps. The results showed that DinoDroid outperforms existing Android testing tools in terms of code coverage and bug detection.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4392806505",
    "type": "article"
  },
  {
    "title": "Automatic Repair of Quantum Programs via Unitary Operation",
    "doi": "https://doi.org/10.1145/3664604",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Yuechen Li; Hanyu Pei; Lin Huang; Beibei Yin; Kai-Yuan Cai",
    "corresponding_authors": "",
    "abstract": "With the continuous advancement of quantum computing (QC), the demand for high-quality quantum programs (QPs) is growing. To avoid program failure, in software engineering, the technology of automatic program repair (APR) employs appropriate patches to remove potential bugs without the intervention of a human. However, the method tailored for repairing defective QPs is still absent. This article proposes, to the best of our knowledge, a new APR method named UnitAR that can repair QPs via unitary operation automatically. Based on the characteristics of superposition and entanglement in QC, the article constructs an algebraic model and adopts a generate-and-validate approach for the repair procedure. Furthermore, the article presents two schemes that can respectively promote the efficiency of generating patches and guarantee the effectiveness of applying patches. For the purpose of evaluating the proposed method, the article selects 29 mutated versions as well as five real-world buggy programs as the objects and introduces two traditional APR approaches GenProg and TBar as baselines. According to the experiments, UnitAR can fix 23 buggy programs, and this method demonstrates the highest efficiency and effectiveness among three APR approaches. Besides, the experimental results further manifest the crucial roles of two constituents involved in the framework of UnitAR .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4396826966",
    "type": "article"
  },
  {
    "title": "Supporting Emotional Intelligence, Productivity and Team Goals while Handling Software Requirements Changes",
    "doi": "https://doi.org/10.1145/3664600",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Kashumi Madampe; Rashina Hoda; John Grundy",
    "corresponding_authors": "",
    "abstract": "Background: Research shows that emotional intelligence (EI) should be used alongside cognitive intelligence during requirements change (RC) handling in Software Engineering (SE), especially in agile settings. Objective: We wanted to study the role of EI in-depth during RC handling. Method: We conducted a mixed-methods study (an interview study followed by a survey study) with 124 software practitioners. Findings: We found the causal condition, intervening condition and causes lead to key direct consequences of regulating own emotions, managing relationships, and extended consequences of sustaining productivity, setting and sustaining team goals. We found several strategies of supporting EI during RC handling. Further, we found strong correlations between six strategies and one being aware of own emotions, regulating own emotions, sustaining team productivity, and setting and sustaining team goals. Conclusion: Empathising with others and tracking commitments and decisions as a team are key strategies that have strong correlations between managing emotions, between sustaining team productivity, and between setting and sustaining team goals. To the best of our knowledge, the framework we present in this paper is the first theoretical framework on EI in SE research. We provide recommendations for software practitioners to consider during RC handling.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4396827103",
    "type": "article"
  },
  {
    "title": "Focused Test Generation for Autonomous Driving Systems",
    "doi": "https://doi.org/10.1145/3664605",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Tahereh Zohdinasab; Vincenzo Riccio; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "Testing Autonomous Driving Systems (ADSs) is crucial to ensure their reliability when navigating complex environments. ADSs may exhibit unexpected behaviours when presented, during operation, with driving scenarios containing features inadequately represented in the training dataset. To address this shift from development to operation, developers must acquire new data with the newly observed features. This data can be then utilised to fine tune the ADS, so as to reach the desired level of reliability in performing driving tasks. However, the resource-intensive nature of testing ADSs requires efficient methodologies for generating targeted and diverse tests. In this work, we introduce a novel approach, DeepAtash-LR , that incorporates a surrogate model into the focused test generation process. This integration significantly improves focused testing effectiveness and applicability in resource-intensive scenarios. Experimental results show that the integration of the surrogate model is fundamental to the success of DeepAtash-LR . Our approach was able to generate an average of up to 60× more targeted, failure-inducing inputs compared to the baseline approach. Moreover, the inputs generated by DeepAtash-LR were useful to significantly improve the quality of the original ADS through fine tuning.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4396860999",
    "type": "article"
  },
  {
    "title": "Harmonising Contributions: Exploring Diversity in Software Engineering through CQA Mining on Stack Overflow",
    "doi": "https://doi.org/10.1145/3672453",
    "publication_date": "2024-06-18",
    "publication_year": 2024,
    "authors": "Elijah Zolduoarrati; Sherlock A. Licorish; Nigel Stanger",
    "corresponding_authors": "",
    "abstract": "The need for collective intelligence in technology means that online Q&amp;A platforms, such as Stack Overflow and Reddit, have become invaluable in building the global knowledge ecosystem. Despite literature demonstrating a prevalence of inclusion and contribution disparities in online communities, studies investigating the underlying reasons behind such fluctuations remain scarce. The current study examines Stack Overflow users’ contribution profiles, both in isolation and relative to various diversity metrics, including GDP and access to electricity. This study also examines whether such profiles propagate to the city and state levels, supplemented by granular data such as per capita income and education, before validating quantitative findings using content analysis. We selected 143 countries and compared the profiles of their respective users to assess implicit diversity-related complications that impact how users contribute. Results show that countries with high GDP, prominent R&amp;D presence, less wealth inequality and sufficient access to infrastructure tend to have more users, regardless of their development status. Similarly, cities and states where technology is more prevalent (e.g., San Francisco and New York) have more users who tend to contribute more often. Qualitative analysis reveals distinct communication styles based on users’ locations. Urban users exhibited assertive, solution-oriented behaviour, actively sharing information. Conversely, rural users engaged through inquiries and discussions, incorporating personal anecdotes, gratitude and conciliatory language. Findings from this study may benefit scholars and practitioners, allowing them to develop sustainable mechanisms to bridge the inclusion and diversity gaps.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4399782414",
    "type": "article"
  },
  {
    "title": "Cleaning Up Confounding: Accounting for Endogeneity Using Instrumental Variables and Two-Stage Models",
    "doi": "https://doi.org/10.1145/3674730",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Lorenz Graf‐Vlachy; Stefan Wagner",
    "corresponding_authors": "",
    "abstract": "Studies in empirical software engineering are often most useful if they make causal claims because this allows practitioners to identify how they can purposefully influence (rather than only predict) outcomes of interest. Unfortunately, many non-experimental studies suffer from potential endogeneity, for example, through omitted confounding variables, which precludes claims of causality. In this conceptual tutorial, we aim to transfer the proven solution of instrumental variables and two-stage models as a means to account for endogeneity from econometrics to the field of empirical software engineering. To this end, we discuss causality and causal inference, provide a definition of endogeneity, explain its causes, and lay out the conceptual idea behind instrumental variable approaches and two-stage models. We also provide an extensive illustration with simulated data and a brief illustration with real data to demonstrate the approach, offering Stata and R code to allow researchers to replicate our analyses and apply the techniques to their own research projects. We close with concrete recommendations and a guide for researchers on how to deal with endogeneity.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4400117331",
    "type": "article"
  },
  {
    "title": "A Disruptive Research Playbook for Studying Disruptive Innovations",
    "doi": "https://doi.org/10.1145/3678172",
    "publication_date": "2024-07-15",
    "publication_year": 2024,
    "authors": "Margaret‐Anne Storey; Daniel Russo; Nicole Novielli; Takashi Kobayashi; Dong Wang",
    "corresponding_authors": "",
    "abstract": "As researchers today, we are witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative Artificial Intelligence (AI), Augmented Reality (AR) and Virtual Reality (VR). In particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its socio-technical nature. In this article, we reflect on the importance of formulating and addressing research problems in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. We propose a research playbook with the aim of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. We showcase how to apply the research playbook. Firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, Stack Overflow, and its impact on software development. Secondly, we show how it can be used to question the impact of two current disruptive technologies: AI and AR/VR. Finally, we introduce a specialized GPT model to support the researcher in framing future investigations. We conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4400652308",
    "type": "article"
  },
  {
    "title": "Reusing d-DNNFs for Efficient Feature-Model Counting",
    "doi": "https://doi.org/10.1145/3680465",
    "publication_date": "2024-07-30",
    "publication_year": 2024,
    "authors": "Chico Sundermann; Heiko Raab; Tobias Heß; Thomas Thüm; Ina Schaefer",
    "corresponding_authors": "",
    "abstract": "Feature models are commonly used to specify valid configurations of a product line. In industry, feature models are often complex due to numerous features and constraints. Thus, a multitude of automated analyses have been proposed. Many of those rely on computing the number of valid configurations, which typically depends on solving a #SAT problem, a computationally expensive operation. Even worse, most counting-based analyses require evaluation for multiple features or partial configurations resulting in numerous #SAT computations on the same feature model. Instead of repetitive computations on highly similar formulas, we aim to improve the performance by reusing knowledge between these computations. In this work, we are the first to propose reusing d-DNNFs for performing repetitive counting queries on features and partial configurations. In our experiments, reusing d-DNNFs saved up-to \\(\\sim\\) 99.98% compared to repetitive invocations of #SAT solvers even when including compilation times. Overall, our tool ddnnife combined with the d-DNNF compiler d4 appears to be the most promising option when dealing with many repetitive feature-model counting queries.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4401118021",
    "type": "article"
  },
  {
    "title": "Fine-tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review",
    "doi": "https://doi.org/10.1145/3695993",
    "publication_date": "2024-09-14",
    "publication_year": 2024,
    "authors": "Yongda Yu; Guoping Rong; Haifeng Shen; He Zhang; Dong Shao; Min Wang; Zhao Wei; Yong Xu; Juhong Wang",
    "corresponding_authors": "",
    "abstract": "As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able to detect issues in the code under review. However, human intervention still remains inevitable since results produced by automated code review are not 100% correct. To assist human reviewers in making their final decisions on automatically generated review comments, the comprehensibility of the comments underpinned by accurate localization and relevant explanations for the detected issues with repair suggestions is paramount. However, this has largely been neglected in the existing research. Large language models (LLMs) have the potential to generate code review comments that are more readable and comprehensible by humans thanks to their remarkable processing and reasoning capabilities. However, even mainstream LLMs perform poorly in detecting the presence of code issues because they have not been specifically trained for this binary classification task required in code review. In this paper, we contribute Carllm (Comprehensibility of Automated Code Review using Large Language Models), a novel fine-tuned LLM that has the ability to improve not only the accuracy but, more importantly, the comprehensibility of automated code review, as compared to state-of-the-art pre-trained models and general LLMs.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4402533615",
    "type": "article"
  },
  {
    "title": "Bias Behind the Wheel: Fairness Testing of Autonomous Driving Systems",
    "doi": "https://doi.org/10.1145/3702989",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Xinyue Li; Zhenpeng Chen; Jie M. Zhang; Federica Sarro; Ying Zhang; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "This paper conducts fairness testing of automated pedestrian detection, a crucial but under-explored issue in autonomous driving systems. We evaluate eight state-of-the-art deep learning-based pedestrian detectors across demographic groups on large-scale real-world datasets. To enable thorough fairness testing, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues, particularly related to age. The proportion of undetected children is 20.14% higher compared to adults. Furthermore, we explore how various driving scenarios affect the fairness of pedestrian detectors. We find that pedestrian detectors demonstrate significant gender biases during night time, potentially exacerbating the prevalent societal issue of female safety concerns during nighttime out. Moreover, we observe that pedestrian detectors can demonstrate both enhanced fairness and superior performance under specific driving conditions, which challenges the fairness-performance trade-off theory widely acknowledged in the fairness literature. We publicly release the code, data, and results to support future research on fairness in autonomous driving.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4403997126",
    "type": "article"
  },
  {
    "title": "Requirements are All You Need: The Final Frontier for End-User Software Engineering",
    "doi": "https://doi.org/10.1145/3708524",
    "publication_date": "2024-12-14",
    "publication_year": 2024,
    "authors": "Diana Robinson; Christian Cabrera; Andrew D. Gordon; Neil D. Lawrence; Lars Mennen",
    "corresponding_authors": "",
    "abstract": "What if end users could own the software development life cycle from conception to deployment using only requirements expressed in language, images, video or audio? We explore this idea, building on the capabilities that Generative Artificial Intelligence brings to software generation and maintenance techniques. How could designing software in this way better serve end users? What are the implications of this process for the future of end-user software engineering and the software development life cycle? We discuss the research needed to bridge the gap between where we are today and these imagined systems of the future.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4405396181",
    "type": "article"
  },
  {
    "title": "A hierarchy-aware approach to faceted classification of objected-oriented components",
    "doi": "https://doi.org/10.1145/310663.310665",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Ernesto Damiani; Mariagrazia Fugini; Carlo Bellettini",
    "corresponding_authors": "",
    "abstract": "This article presents a hierarchy-aware classification schema for obje ct-oriented code, where software components are classified according to their behavioral characteristics , such as provided services, employed algorithms, and needed data. In the case of reusable application frameworks, these characteristics are constructured from their model , i.e., from the description of the abstract classes specifying both the framework structure and purpose. In conventional object libraries, the characteristics are extracted semiautomatically from class interfaces. Characteristics are term pairs, weighted to represent “how well” they describe component behavior. The set of characteristics associated with a given component forms its software descriptor . A descriptor base is presented where descriptors are organized on the basis of structured relationships, such as similarity and composition. The classification is supported by a thesaurus acting as a language-independent unified lexicon. The descriptor base is conceived for developers who, besides conventionally browsing the descriptors hierarchy, can query the system, specifying a set of desired functionalities and getting a ranked set of adaptable candidates. User feedback is taken into account in order to progressively ameliorate the quality of the descriptors according to the views of the user community. Feedback is made dependent of the user typology through a user profile . Experimental results in terms of recall and precision of the retrieval mechanism against a sample code base are reported.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W1968672338",
    "type": "article"
  },
  {
    "title": "Eliciting software process models with the<i>E</i><sup>3</sup>language",
    "doi": "https://doi.org/10.1145/292182.292194",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "Letizia Jaccheri; Gian Pietro Picco; Patricia Lago",
    "corresponding_authors": "",
    "abstract": "Software processes are complex entities that demand careful understand ing and improvement as they determine the quality of the resulting product. A necessary step toward the improvement of an organization's process is a clear description of the entities involved and of their mutual relationships. Process model elicitation aims at constructing this description under the shape of a software process model. The model is constructed by gathering, from several sources, process information which is often incomplete, inconsistent, and ambiguous. A process modeling language can be used to represent the model being elicited. However, elicitation requires process models to be understandable and well structured. These requirements are often not satisfied by available process modeling languages becuase of their bias toward process enaction rather than process description. This article presents a process modeling language and a support tool which are conceived especially for process model elicitation. The E 3 language is an object-oriented modeling language with a graphical notation. In E 3 , associations are a means to express constraints and facilitate reuse. The E 3 p-draw tool supports the creation and management of E 3 models and provides a view mechanism that enables inspection of models according to different perspectives.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2091079249",
    "type": "article"
  },
  {
    "title": "A compiler for analyzing cryptographic protocols using noninterference",
    "doi": "https://doi.org/10.1145/363516.363532",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Antonio Durante; Riccardo Focardi; Roberto Gorrieri",
    "corresponding_authors": "",
    "abstract": "The Security Process Algebra (SPA) is a CCS-like specification languag e where actions belong to two different levels of confidentiality. It has been used to define several noninterference-like security properties whose verification has been automated by the tool CoSeC. In recent years, a method for analyzing security protocols using SPA and CoSeC has been developed. Even if it has been useful in analyzing small security protocols, this method has shown to be error-prone, as it requires the protocol description and its environment to be written by hand. This problem has been solved by defining a protocol specification language more abstract than SPA, called VSP, and a compiler CVS that automatically generates the SPA specification for a given protocol described in VSP. The VSP/CVS technology is very powerful, and its usefulness is shown with some case studies: the Woo-Lam one-way authentication protocol, for which a new attack to authentication is found, and the Wide Mouthed Frog protocol, where different kinds of attack are detected and analyzed.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1991972246",
    "type": "article"
  },
  {
    "title": "Testing by means of inductive program learning",
    "doi": "https://doi.org/10.1145/227607.227611",
    "publication_date": "1996-04-01",
    "publication_year": 1996,
    "authors": "Francesco Bergadano; Daniele Gunetti",
    "corresponding_authors": "",
    "abstract": "article Share on Testing by means of inductive program learning Authors: Francesco Bergadano University of Torino University of TorinoView Profile , Daniele Gunetti University of Torino University of TorinoView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 5Issue 2April 1996 pp 119–145https://doi.org/10.1145/227607.227611Online:01 April 1996Publication History 37citation739DownloadsMetricsTotal Citations37Total Downloads739Last 12 Months21Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2026387030",
    "type": "article"
  },
  {
    "title": "A formal model for reasoning about adaptive QoS-enabled middleware",
    "doi": "https://doi.org/10.1145/1005561.1005564",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Nalini Venkatasubramanian; Carolyn Talcott; Gul Agha",
    "corresponding_authors": "",
    "abstract": "Systems that provide distributed multimedia services are subject to constant evolution; customizable middleware is required to effectively manage this change. Middleware services for resource management execute concurrently with each other, and with application activities, and can, therefore, potentially interfere with each other. To ensure cost-effective QoS in distributed multimedia systems, safe composability of resource management services is essential. In this article, we present a meta-architectural framework, the Two-Level Actor Model (TLAM) for customizable QoS-based middleware, based on the actor model of concurrent active objects. Using TLAM, a semantic model for specifying and reasoning about components of open distributed systems, we show how a QoS brokerage service can be used to coordinate multimedia resource management services in a safe, flexible, and efficient manner. In particular, we show a system in which the multimedia actor behaviors satisfy the specified requirements and provide the required multimedia service. The behavior specification leaves open the possibility of a variety of algorithms for resource management. Furthermore, constraints are identified that are sufficient to guarantee noninterference among the multiple broker resource management services, as well as providing guidelines for the safe composition of additional services.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2123608402",
    "type": "article"
  },
  {
    "title": "A scalable formal method for design and automatic checking of user interfaces",
    "doi": "https://doi.org/10.1145/1061254.1061256",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "Jean Berstel; Stefano Crespi Reghizzi; Gilles Roussel; Pierluigi San Pietro",
    "corresponding_authors": "",
    "abstract": "The article addresses the formal specification, design and implementation of the behavioral component of graphical user interfaces. The complex sequences of visual events and actions that constitute dialogs are specified by means of modular, communicating grammars called VEG (Visual Event Grammars), which extend traditional BNF grammars to make them more convenient to model dialogs.A VEG specification is independent of the actual layout of the GUI, but it can easily be integrated with various layout design toolkits. Moreover, a VEG specification may be verified with the model checker SPIN, in order to test consistency and correctness, to detect deadlocks and unreachable states, and also to generate test cases for validation purposes.Efficient code is automatically generated by the VEG toolkit, based on compiler technology. Realistic applications have been specified, verified and implemented, like a Notepad-style editor, a graph construction library and a large real application to medical software. It is also argued that VEG can be used to specify and test voice interfaces and multimodal dialogs. The major contribution of our work is blending together a set of features coming from GUI design, compilers, software engineering and formal verification. Even though we do not claim novelty in each of the techniques adopted for VEG, they have been united into a toolkit supporting all GUI design phases, that is, specification, design, verification and validation, linking to applications and coding.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2039811450",
    "type": "article"
  },
  {
    "title": "Analysis and applications of timed service protocols",
    "doi": "https://doi.org/10.1145/1734229.1734230",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Julien Ponge; Boualem Benatallah; Fabio Casati; Farouk Toumani",
    "corresponding_authors": "",
    "abstract": "Web services are increasingly gaining acceptance as a framework for facilitating application-to-application interactions within and across enterprises. It is commonly accepted that a service description should include not only the interface, but also the business protocol supported by the service. The present work focuses on the formalization of an important category of protocols that includes time-related constraints (called timed protocols ), and the impact of time on compatibility and replaceability analysis. We formalized the following timing constraints: C-Invoke constraints define time windows within which a service operation can be invoked while M-Invoke constraints define expiration deadlines. We extended techniques for compatibility and replaceability analysis between timed protocols by using a semantic-preserving mapping between timed protocols and timed automata, leading to the identification of a novel class of timed automata, called protocol timed automata (PTA). PTA exhibit a particular kind of silent transition that strictly increase the expressiveness of the model, yet they are closed under complementation, making every type of compatibility or replaceability analysis decidable. Finally, we implemented our approach in the context of a larger project called ServiceMosaic, a model-driven framework for Web service life-cycle management.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1969500205",
    "type": "article"
  },
  {
    "title": "QVM",
    "doi": "https://doi.org/10.1145/2063239.2063241",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Matthew Arnold; Martin Vechev; Eran Yahav",
    "corresponding_authors": "",
    "abstract": "Coping with software defects that occur in the post-deployment stage is a challenging problem: bugs may occur only when the system uses a specific configuration and only under certain usage scenarios. Nevertheless, halting production systems until the bug is tracked and fixed is often impossible. Thus, developers have to try to reproduce the bug in laboratory conditions. Often, the reproduction of the bug takes most of the debugging effort. In this paper we suggest an approach to address this problem by using a specialized runtime environment called Quality Virtual Machine (QVM). QVM efficiently detects defects by continuously monitoring the execution of the application in a production setting. QVM enables the efficient checking of violations of user-specified correctness properties, that is, typestate safety properties, Java assertions, and heap properties pertaining to ownership. QVM is markedly different from existing techniques for continuous monitoring by using a novel overhead manager which enforces a user-specified overhead budget for quality checks. Existing tools for error detection in the field usually disrupt the operation of the deployed system. QVM, on the other hand, provides a balanced trade-off between the cost of the monitoring process and the maintenance of sufficient accuracy for detecting defects. Specifically, the overhead cost of using QVM instead of a standard JVM, is low enough to be acceptable in production environments. We implemented QVM on top of IBM’s J9 Java Virtual Machine and used it to detect and fix various errors in real-world applications.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2041653169",
    "type": "article"
  },
  {
    "title": "Architecture-centric support for adaptive service collaborations",
    "doi": "https://doi.org/10.1145/2559937",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Robrecht Haesevoets; Danny Weyns; Tom Holvoet",
    "corresponding_authors": "",
    "abstract": "In today's volatile business environments, collaboration between information systems, both within and across company borders, has become essential to success. An efficient supply chain, for example, requires the collaboration of distributed and heterogeneous systems of multiple companies. Developing such collaborative applications and building the supporting information systems poses several engineering challenges. A key challenge is to manage the ever-growing design complexity. In this article, we argue that software architecture should play a more prominent role in the development of collaborative applications. This can help to better manage design complexity by modularizing collaborations and separating concerns. State-of-the-art solutions, however, often lack proper abstractions for modeling collaborations at architectural level or do not reify these abstractions at detailed design and implementation level. Developers, on the other hand, rely on middleware, business process management, and Web services, techniques that mainly focus on low-level infrastructure. To address the problem of managing the design complexity of collaborative applications, we present Macodo. Macodo consists of three complementary parts: (1) a set of abstractions for modeling adaptive collaborations, (2) a set of architectural views, the main contribution of this article, that reify these abstractions at architectural level, and (3) a proof-of-concept middleware infrastructure that supports the architectural abstractions at design and implementation level. We evaluate the architectural views in a controlled experiment. Results show that the use of Macodo can reduce fault density and design complexity, and improve reuse and productivity. The main contributions of this article are illustrated in a supply chain management case.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2089765811",
    "type": "article"
  },
  {
    "title": "<i>A Posteriori</i> Typing for Model-Driven Engineering",
    "doi": "https://doi.org/10.1145/3063384",
    "publication_date": "2017-05-05",
    "publication_year": 2017,
    "authors": "Juan de Lara; Esther Guerra",
    "corresponding_authors": "",
    "abstract": "Model-Driven Engineering (MDE) is founded on the ability to create and process models conforming to a meta-model. In this context, classes in a meta-model are used in two ways: as templates to create objects and as (static) classifiers for them. These two aspects are inherently tied in most meta-modelling approaches, which results in unnecessarily rigid systems and hinders reusability of MDE artefacts. In this work, we discuss the benefits of decoupling object creation from typing in MDE. Thus, we rely on standard mechanisms for object creation, and propose a posteriori typing as a means to retype objects and enable multiple, partial, dynamic typings. This approach enhances flexibility; permits unanticipated reuse, as model management operations defined for a meta-model can be reused with other models once they get reclassified; and enables bidirectional model transformation by reclassification. In particular, we propose two mechanisms to realise model retyping and show their underlying theory and analysis methods. We show the feasibility of the approach by an implementation atop our meta-modelling tool M eta D epth and present several applications of retypings (transformations, reuse, and dynamicity).",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2612065447",
    "type": "article"
  },
  {
    "title": "Fixing Faults in C and Java Source Code",
    "doi": "https://doi.org/10.1145/3104029",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Giuseppe Scanniello; Michele Risi; Porfirio Tramontana; Simone Romano",
    "corresponding_authors": "",
    "abstract": "We carried out a family of controlled experiments to investigate whether the use of abbreviated identifier names, with respect to full-word identifier names, affects fault fixing in C and Java source code. This family consists of an original (or baseline) controlled experiment and three replications. We involved 100 participants with different backgrounds and experiences in total. Overall results suggested that there is no difference in terms of effort, effectiveness, and efficiency to fix faults, when source code contains either only abbreviated or only full-word identifier names. We also conducted a qualitative study to understand the values, beliefs, and assumptions that inform and shape fault fixing when identifier names are either abbreviated or full-word. We involved in this qualitative study six professional developers with 1--3 years of work experience. A number of insights emerged from this qualitative study and can be considered a useful complement to the quantitative results from our family of experiments. One of the most interesting insights is that developers, when working on source code with abbreviated identifier names, adopt a more methodical approach to identify and fix faults by extending their focus point and only in a few cases do they expand abbreviated identifiers.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2742136054",
    "type": "article"
  },
  {
    "title": "Platys",
    "doi": "https://doi.org/10.1145/2729976",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Pradeep K. Murukannaiah; Munindar P. Singh",
    "corresponding_authors": "",
    "abstract": "We introduce a high-level abstraction of location called place . A place derives its meaning from a user's physical space, activities, or social context. In this manner, place can facilitate improved user experience compared to the traditional representation of location, which is spatial coordinates. We propose the Platys framework as a way to address the special challenges of place-aware application development. The core of Platys is a middleware that (1) learns a model of places specific to each user via active learning , a machine learning paradigm that seeks to reduce the user-effort required for training the middleware, and (2) exposes the learned user-specific model of places to applications at run time, insulating application developers from dealing with both low-level sensors and user idiosyncrasies in perceiving places. We evaluated Platys via two studies. First, we collected place labels and Android phone sensor readings from 10 users. We applied Platys' active learning approach to learn each user's places and found that Platys (1) requires fewer place labels to learn a user's places with a desired accuracy than do two traditional supervised approaches, and (2) learns places with higher accuracy than two unsupervised approaches. Second, we conducted a developer study to evaluate Platys' efficiency in assisting developers and its effectiveness in enabling usable applications. In this study, 46 developers employed either Platys or the Android location API to develop a place-aware application. Our results indicate that application developers employing Platys, when compared to those employing the Android API, (1) develop a place-aware application faster and perceive reduced difficulty and (2) produce applications that are easier to understand (for developers) and potentially more usable and privacy preserving (for application users).",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2266696196",
    "type": "article"
  },
  {
    "title": "Conditional Commitments",
    "doi": "https://doi.org/10.1145/2685613",
    "publication_date": "2014-12-23",
    "publication_year": 2014,
    "authors": "Warda El Kholy; Jamal Bentahar; Mohamed El Menshawy; Hongyang Qu; Rachida Dssouli",
    "corresponding_authors": "",
    "abstract": "While modeling interactions using social commitments provides a fundamental basis for capturing flexible and declarative interactions and helps in addressing the challenge of ensuring compliance with specifications, the designers of the system cannot guarantee that an agent complies with its commitments as it is supposed to, or at least an agent doesn't want to violate its commitments. They may still wish to develop efficient and scalable algorithms by which model checking conditional commitments, a natural and universal frame of social commitments, is feasible at design time. However, distinguishing between different but related types of conditional commitments, and developing dedicated algorithms to tackle the problem of model checking conditional commitments, is still an active research topic. In this article, we develop the temporal logic CTL cc that extends Computation Tree Logic (CTL) with new modalities which allow representing and reasoning about two types of communicating conditional commitments and their fulfillments using the formalism of interpreted systems. We introduce a set of rules to reason about conditional commitments and their fulfillments. The verification technique is based on developing a new symbolic model checking algorithm to address this verification problem. We analyze the computational complexity and present the full implementation of the developed algorithm on top of the MCMAS model checker. We also evaluate the algorithm's effectiveness and scalability by verifying the compliance of the NetBill protocol, taken from the business domain, and the process of breast cancer diagnosis and treatment, taken from the health-care domain, with specifications expressed in CTL cc . We finally compare the experimental results with existing proposals.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2052401040",
    "type": "article"
  },
  {
    "title": "Scalable Runtime Bloat Detection Using Abstract Dynamic Slicing",
    "doi": "https://doi.org/10.1145/2560047",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Guoqing Xu; Nick Mitchell; Matthew Arnold; Atanas Rountev; Edith Schonberg; Gary Sevitsky",
    "corresponding_authors": "",
    "abstract": "Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent improvement, performance experts analyze deployed applications and regularly find gains of 2× or more. Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Our experience shows that, in order to identify large performance bottlenecks in a program, it is more important to understand its dynamic dataflow than traditional performance metrics, such as running time. This article presents a general framework for designing and implementing scalable analysis algorithms to find causes of bloat in Java programs. At the heart of this framework is a generalized form of runtime dependence graph computed by abstract dynamic slicing , a semantics-aware technique that achieves high scalability by performing dynamic slicing over bounded abstract domains. The framework is instantiated to create two independent dynamic analyses, copy profiling and cost-benefit analysis , that help programmers identify performance bottlenecks by identifying, respectively, high-volume copy activities and data structures that have high construction cost but low benefit for the forward execution. We have successfully applied these analyses to large-scale and long-running Java applications. We show that both analyses are effective at detecting inefficient operations that can be optimized for better performance. We also demonstrate that the general framework is flexible enough to be instantiated for dynamic analyses in a variety of application domains.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2123059332",
    "type": "article"
  },
  {
    "title": "Estimating Semantic Relatedness in Source Code",
    "doi": "https://doi.org/10.1145/2824251",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Anas Mahmoud; Gary Bradshaw",
    "corresponding_authors": "",
    "abstract": "Contemporary software engineering tools exploit semantic relations between individual code terms to aid in code analysis and retrieval tasks. Such tools employ word similarity methods, often used in natural language processing ( nlp ), to analyze the textual content of source code. However, the notion of similarity in source code is different from natural language. Source code often includes unnatural domain-specific terms (e.g., abbreviations and acronyms), and such terms might be related due to their structural relations rather than linguistic aspects. Therefore, applying natural language similarity methods to source code without adjustment can produce low-quality and error-prone results. Motivated by these observations, we systematically investigate the performance of several semantic-relatedness methods in the context of software. Our main objective is to identify the most effective semantic schemes in capturing association relations between source code terms. To provide an unbiased comparison, different methods are compared against human-generated relatedness information using terms from three software systems. Results show that corpus-based methods tend to outperform methods that exploit external sources of semantic knowledge. However, due to inherent code limitations, the performance of such methods is still suboptimal. To address these limitations, we propose Normalized Software Distance ( nsd ), an information-theoretic method that captures semantic relatedness in source code by exploiting the distributional cues of code terms across the system. nsd overcomes data sparsity and lack of context problems often associated with source code, achieving higher levels of resemblance to the human perception of relatedness at the term and the text levels of code.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2264942984",
    "type": "article"
  },
  {
    "title": "Differential Testing of Certificate Validation in SSL/TLS Implementations",
    "doi": "https://doi.org/10.1145/3355048",
    "publication_date": "2019-10-09",
    "publication_year": 2019,
    "authors": "Cong Tian; Chu Chen; Zhenhua Duan; Liang Zhao",
    "corresponding_authors": "",
    "abstract": "Certificate validation in Secure Sockets Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS implementations is correctly implemented. With this motivation, we propose a novel differential testing approach that is based on the standard Request for Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e., certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-targeted, since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations. In addition, by providing seed certificates for mutation approaches with RFCcert, the ability of mutation approaches in finding distinct discrepancies is significantly enhanced.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2980273018",
    "type": "article"
  },
  {
    "title": "Prove it! Inferring Formal Proof Scripts from CafeOBJ Proof Scores",
    "doi": "https://doi.org/10.1145/3208951",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Adrián Riesco; Kazuhiro Ogata",
    "corresponding_authors": "",
    "abstract": "CafeOBJ is a language for writing formal specifications for a wide variety of software and hardware systems and for verifying their properties. CafeOBJ makes it possible to verify properties by using either proof scores, which consists of reducing goal-related terms in user-defined modules, or by using theorem proving. While the former is more flexible, it lacks the formal support to ensure that a property has been really proven. On the other hand, theorem proving might be too strict, since only a predefined set of commands can be applied to the current goal; hence, it hardens the verification of properties. In order to take advantage of the benefits of both techniques, we have extended CafeInMaude, a CafeOBJ interpreter implemented in Maude, with the CafeInMaude Proof Assistant (CiMPA) and the CafeInMaude Proof Generator (CiMPG). CiMPA is a proof assistant for proving inductive properties on CafeOBJ specifications that uses Maude metalevel features to allow programmers to create and manipulate CiMPA proofs. On the other hand, CiMPG provides a minimal set of annotations for identifying proof scores and generating CiMPA scripts for these proof scores. In this article, we present the CiMPA and CiMPG, detailing the behavior of the CiMPA and the algorithm underlying the CiMPG and illustrating the power of the approach by using the QLOCK protocol. Finally, we present some benchmarks that give us confidence in the matureness and usefulness of these tools.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2884080291",
    "type": "article"
  },
  {
    "title": "Fine-grained Code Coverage Measurement in Automated Black-box Android Testing",
    "doi": "https://doi.org/10.1145/3395042",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Aleksandr Pilgun; Olga Gadyatskaya; Yury Zhauniarovich; Stanislav Dashevskyi; Artsiom Kushniarou; Sjouke Mauw",
    "corresponding_authors": "",
    "abstract": "Today, there are millions of third-party Android applications. Some of them are buggy or even malicious. To identify such applications, novel frameworks for automated black-box testing and dynamic analysis are being developed by the Android community. Code coverage is one of the most common metrics for evaluating effectiveness of these frameworks. Furthermore, code coverage is used as a fitness function for guiding evolutionary and fuzzy testing techniques. However, there are no reliable tools for measuring fine-grained code coverage in black-box Android app testing. We present the Android Code coVerage Tool, ACVTool for short, that instruments Android apps and measures code coverage in the black-box setting at class, method and instruction granularity. ACVTool has successfully instrumented 96.9% of apps in our experiments. It introduces a negligible instrumentation time overhead, and its runtime overhead is acceptable for automated testing tools. We demonstrate practical value of ACVTool in a large-scale experiment with Sapienz, a state-of-the-art automated testing tool. Using ACVTool on the same cohort of apps, we have compared different coverage granularities applied by Sapienz in terms of the found amount of crashes. Our results show that none of the applied coverage granularities clearly outperforms others in this aspect.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3039757350",
    "type": "article"
  },
  {
    "title": "What You See is What it Means! Semantic Representation Learning of Code based on Visualization and Transfer Learning",
    "doi": "https://doi.org/10.1145/3485135",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Patrick Keller; Abdoul Kader Kaboré; Laura Plein; Jacques Klein; Yves Le Traon; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "Recent successes in training word embeddings for Natural Language Processing ( NLP ) tasks have encouraged a wave of research on representation learning for source code, which builds on similar NLP methods. The overall objective is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding approach based on the intuition that source code has visual patterns of semantics. We further use these patterns to address the outstanding challenge of identifying semantic code clones. We propose the WySiWiM ( ‘ ‘What You See Is What It Means ” ) approach where visual representations of source code are fed into powerful pre-trained image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning. We evaluate the proposed embedding approach on the task of vulnerable code prediction in source code and on two variations of the task of semantic code clone identification: code clone detection (a binary classification problem), and code classification (a multi-classification problem). We show with experiments on the BigCloneBench (Java), Open Judge (C) that although simple, our WySiWiM approach performs as effectively as state-of-the-art approaches such as ASTNN or TBCNN. We also showed with data from NVD and SARD that WySiWiM representation can be used to learn a vulnerable code detector with reasonable performance (accuracy ∼90%). We further explore the influence of different steps in our approach, such as the choice of visual representations or the classification algorithm, to eventually discuss the promises and limitations of this research direction.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3005276469",
    "type": "article"
  },
  {
    "title": "Predictive Mutation Analysis via the Natural Language Channel in Source Code",
    "doi": "https://doi.org/10.1145/3510417",
    "publication_date": "2022-03-21",
    "publication_year": 2022,
    "authors": "Jin-Han Kim; Juyoung Jeon; Shin Hong; Shin Yoo",
    "corresponding_authors": "",
    "abstract": "Mutation analysis can provide valuable insights into both the system under test and its test suite. However, it is not scalable due to the cost of building and testing a large number of mutants. Predictive Mutation Testing (PMT) has been proposed to reduce the cost of mutation testing, but it can only provide statistical inference about whether a mutant will be killed or not by the entire test suite. We propose Seshat, a Predictive Mutation Analysis (PMA) technique that can accurately predict the entire kill matrix , not just the Mutation Score (MS) of the given test suite. Seshat exploits the natural language channel in code, and learns the relationship between the syntactic and semantic concepts of each test case and the mutants it can kill, from a given kill matrix. The learnt model can later be used to predict the kill matrices for subsequent versions of the program, even after both the source and test code have changed significantly. Empirical evaluation using the programs in Defects4J shows that Seshat can predict kill matrices with an average F-score of 0.83 for versions that are up to years apart. This is an improvement in F-score by 0.14 and 0.45 points over the state-of-the-art PMT technique and a simple coverage-based heuristic, respectively. Seshat also performs as well as PMT for the prediction of the MS only. When applied to a mutant-based fault localisation technique, the predicted kill matrix by Seshat is successfully used to locate faults within the top 10 position, showing its usefulness beyond prediction of MS. Once Seshat trains its model using a concrete mutation analysis, the subsequent predictions made by Seshat are on average 39 times faster than actual test-based analysis. We also show that Seshat can be successfully applied to automatically generated test cases with an experiment using EvoSuite.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3153266893",
    "type": "article"
  },
  {
    "title": "Super-optimization of Smart Contracts",
    "doi": "https://doi.org/10.1145/3506800",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Elvira Albert; Pablo Gordillo; Alejandro Hernández-Cerezo; Albert Rubio; Maria A. Schett",
    "corresponding_authors": "",
    "abstract": "Smart contracts are programs deployed on a blockchain. They are executed for a monetary fee paid in gas —a clear optimization target for smart contract compilers. Because smart contracts are a young, fast-moving field without (manually) fine-tuned compilers, they highly benefit from automated and adaptable approaches, especially as smart contracts are effectively immutable, and as such need a high level of assurance. This makes them an ideal domain for applying formal methods. Super-optimization is a technique to find the best translation of a block of instructions by trying all possible sequences of instructions that produce the same result. We present a framework for super-optimizing smart contracts based on Max-SMT with two main ingredients: (1) a stack functional specification extracted from the basic blocks of a smart contract, which is simplified using rules capturing the semantics of arithmetic, bit-wise, and relational operations, and (2) the synthesis of optimized blocks , which finds—by means of an efficient SMT encoding—basic blocks with minimal gas cost whose stack functional specification is equal (modulo commutativity) to the extracted one. We implemented our framework in the tool syrup 2.0 . Through large-scale experiments on real-world smart contracts, we analyze performance improvements for different SMT encodings, as well as tradeoffs between quality of optimizations and required optimization time.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4289313229",
    "type": "article"
  },
  {
    "title": "Input Distribution Coverage: Measuring Feature Interaction Adequacy in Neural Network Testing",
    "doi": "https://doi.org/10.1145/3576040",
    "publication_date": "2022-12-12",
    "publication_year": 2022,
    "authors": "Swaroopa Dola; Matthew B. Dwyer; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "Testing deep neural networks (DNNs) has garnered great interest in the recent years due to their use in many applications. Black-box test adequacy measures are useful for guiding the testing process in covering the input domain. However, the absence of input specifications makes it challenging to apply black-box test adequacy measures in DNN testing. The Input Distribution Coverage (IDC) framework addresses this challenge by using a variational autoencoder to learn a low dimensional latent representation of the input distribution, and then using that latent space as a coverage domain for testing. IDC applies combinatorial interaction testing on a partitioning of the latent space to measure test adequacy. Empirical evaluation demonstrates that IDC is cost-effective, capable of detecting feature diversity in test inputs, and more sensitive than prior work to test inputs generated using different DNN test generation methods. The findings demonstrate that IDC overcomes several limitations of white-box DNN coverage approaches by discounting coverage from unrealistic inputs and enabling the calculation of test adequacy metrics that capture the feature diversity present in the input space of DNNs.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4311159382",
    "type": "article"
  },
  {
    "title": "An Empirical Study of the Effectiveness of an Ensemble of Stand-alone Sentiment Detection Tools for Software Engineering Datasets",
    "doi": "https://doi.org/10.1145/3491211",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Gias Uddin; Yann-Gaël Guéhénuc; Foutse Khomh; Chanchal K. Roy",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis in software engineering (SE) has shown promise to analyze and support diverse development activities. Recently, several tools are proposed to detect sentiments in software artifacts. While the tools improve accuracy over off-the-shelf tools, recent research shows that their performance could still be unsatisfactory. A more accurate sentiment detector for SE can help reduce noise in analysis of software scenarios where sentiment analysis is required. Recently, combinations, i.e., hybrids of stand-alone classifiers are found to offer better performance than the stand-alone classifiers for fault detection. However, we are aware of no such approach for sentiment detection for software artifacts. We report the results of an empirical study that we conducted to determine the feasibility of developing an ensemble engine by combining the polarity labels of stand-alone SE-specific sentiment detectors. Our study has two phases. In the first phase, we pick five SE-specific sentiment detection tools from two recently published papers by Lin et al. [ 29 , 30 ], who first reported negative results with stand alone sentiment detectors and then proposed an improved SE-specific sentiment detector, POME [ 29 ]. We report the study results on 17,581 units (sentences/documents) coming from six currently available sentiment benchmarks for software engineering. We find that the existing tools can be complementary to each other in 85-95% of the cases, i.e., one is wrong but another is right. However, a majority voting-based ensemble of those tools fails to improve the accuracy of sentiment detection. We develop Sentisead, a supervised tool by combining the polarity labels and bag of words as features. Sentisead improves the performance (F1-score) of the individual tools by 4% (over Senti4SD [ 5 ]) – 100% (over POME [ 29 ]). The initial development of Sentisead occurred before we observed the use of deep learning models for SE-specific sentiment detection. In particular, recent papers show the superiority of advanced language-based pre-trained transformer models (PTM) over rule-based and shallow learning models. Consequently, in a second phase, we compare and improve Sentisead infrastructure using the PTMs. We find that a Sentisead infrastructure with RoBERTa as the ensemble of the five stand-alone rule-based and shallow learning SE-specific tools from Lin et al. [ 29 , 30 ] offers the best F1-score of 0.805 across the six datasets, while a stand-alone RoBERTa shows an F1-score of 0.801.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3211925781",
    "type": "article"
  },
  {
    "title": "Testing Feedforward Neural Networks Training Programs",
    "doi": "https://doi.org/10.1145/3529318",
    "publication_date": "2022-05-10",
    "publication_year": 2022,
    "authors": "Houssem Ben Braiek; Foutse Khomh",
    "corresponding_authors": "",
    "abstract": "At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this article, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design TheDeepChecker , an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger ( SMD ). Results show that TheDeepChecker ’s on-execution validation of DNN-based program’s properties through three sequential phases (pre-, on-, and post-fitting) succeeds in revealing several coding bugs and system misconfigurations errors early on and at a low cost. Moreover, our property-based approach outperforms the SMD ’s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4280562623",
    "type": "article"
  },
  {
    "title": "Mutation Testing in Evolving Systems: Studying the Relevance of Mutants to Code Evolution",
    "doi": "https://doi.org/10.1145/3530786",
    "publication_date": "2022-05-11",
    "publication_year": 2022,
    "authors": "Miloš Ojdanić; Ezekiel Soremekun; Renzo Degiovanni; Mike Papadakis; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Context: When software evolves, opportunities for introducing faults appear. Therefore, it is important to test the evolved program behaviors during each evolution cycle. However, while software evolves, its complexity is also evolving, introducing challenges to the testing process. To deal with this issue, testing techniques should be adapted to target the effect of the program changes instead of the entire program functionality. To this end, commit-aware mutation testing , a powerful testing technique, has been proposed. Unfortunately, commit-aware mutation testing is challenging due to the complex program semantics involved. Hence, it is pertinent to understand the characteristics, predictability, and potential of the technique. Objective: We conduct an exploratory study to investigate the properties of commit-relevant mutants , i.e., the test elements of commit-aware mutation testing, by proposing a general definition and an experimental approach to identify them. We thus aim at investigating the prevalence, location, and comparative advantages of commit-aware mutation testing over time (i.e., the program evolution). We also investigate the predictive power of several commit-related features in identifying and selecting commit-relevant mutants to understand the essential properties for its best-effort application case. Method: Our commit-relevant definition relies on the notion of observational slicing, approximated by higher-order mutation. Specifically, our approach utilizes the impact of mutants, effects of one mutant on another in capturing and analyzing the implicit interactions between the changed and unchanged code parts. The study analyses millions of mutants (over 10 million), 288 commits, five (5) different open-source software projects involving over 68,213 CPU days of computation and sets a ground truth where we perform our analysis. Results: Our analysis shows that commit-relevant mutants are located mainly outside of program commit change (81%), suggesting a limitation in previous work. We also note that effective selection of commit-relevant mutants has the potential of reducing the number of mutants by up to 93%. In addition, we demonstrate that commit relevant mutation testing is significantly more effective and efficient than state-of-the-art baselines, i.e., random mutant selection and analysis of only mutants within the program change. In our analysis of the predictive power of mutants and commit-related features (e.g., number of mutants within a change, mutant type, and commit size) in predicting commit-relevant mutants, we found that most proxy features do not reliably predict commit-relevant mutants . Conclusion: This empirical study highlights the properties of commit-relevant mutants and demonstrates the importance of identifying and selecting commit-relevant mutants when testing evolving software systems.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4280606181",
    "type": "article"
  },
  {
    "title": "Guaranteeing Timed Opacity using Parametric Timed Model Checking",
    "doi": "https://doi.org/10.1145/3502851",
    "publication_date": "2022-07-09",
    "publication_year": 2022,
    "authors": "Étienne André; Didier Lime; Dylan Marinho; Jun Sun",
    "corresponding_authors": "",
    "abstract": "Information leakage can have dramatic consequences on systems security. Among harmful information leaks, the timing information leakage occurs whenever an attacker successfully deduces confidential internal information. In this work, we consider that the attacker has access (only) to the system execution time. We address the following timed opacity problem: given a timed system, a private location and a final location, synthesize the execution times from the initial location to the final location for which one cannot deduce whether the system went through the private location. We also consider the full timed opacity problem, asking whether the system is opaque for all execution times. We show that these problems are decidable for timed automata (TAs) but become undecidable when one adds parameters, yielding parametric timed automata (PTAs). We identify a subclass with some decidability results. We then devise an algorithm for synthesizing PTAs parameter valuations guaranteeing that the resulting TA is opaque. We finally show that our method can also apply to program analysis.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4282966423",
    "type": "article"
  },
  {
    "title": "<scp>HybridCISave</scp> : A Combined Build and Test Selection Approach in Continuous Integration",
    "doi": "https://doi.org/10.1145/3576038",
    "publication_date": "2022-12-13",
    "publication_year": 2022,
    "authors": "Xianhao Jin; Francisco Servant",
    "corresponding_authors": "",
    "abstract": "Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice—Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique ( HybridCISave ) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4311349067",
    "type": "article"
  },
  {
    "title": "<scp>QuoTe</scp> : Quality-oriented Testing for Deep Learning Systems",
    "doi": "https://doi.org/10.1145/3582573",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Jialuo Chen; Jingyi Wang; Xingjun Ma; Youcheng Sun; Jun Sun; Peixin Zhang; Peng Cheng",
    "corresponding_authors": "",
    "abstract": "Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing—that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Qu ality- o riented Te sting). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality—that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4319963669",
    "type": "article"
  },
  {
    "title": "Automatic Core-Developer Identification on GitHub: A Validation Study",
    "doi": "https://doi.org/10.1145/3593803",
    "publication_date": "2023-04-22",
    "publication_year": 2023,
    "authors": "Thomas Böck; Nils Alznauer; Mitchell Joblin; Sven Apel",
    "corresponding_authors": "",
    "abstract": "Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4366768319",
    "type": "article"
  },
  {
    "title": "An Empirical Study on GitHub Pull Requests’ Reactions",
    "doi": "https://doi.org/10.1145/3597208",
    "publication_date": "2023-05-13",
    "publication_year": 2023,
    "authors": "Mohamed Amine Batoun; Ka Lai Yung; Yuan Tian; Mohammed Sayagh",
    "corresponding_authors": "",
    "abstract": "The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., “Thumbs-up”, “Laugh”, “Hooray”, “Heart”, “Rocket”, “Thumbs-down”, “Confused”, and “Eyes”. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests’ comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process. To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., “Thumbs-up”, “Hooray”, “Heart”, “Rocket”, and “Laugh”). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the “end-users” of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull request’s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requests’ reactions.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4376505229",
    "type": "article"
  },
  {
    "title": "Revisiting the Identification of the Co-evolution of Production and Test Code",
    "doi": "https://doi.org/10.1145/3607183",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Weifeng Sun; Meng Yan; Zhongxin Liu; Xin Xia; Yan Lei; David Lo",
    "corresponding_authors": "",
    "abstract": "Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample . However, the validity of this assumption has never been investigated. To fill this gap, we present an empirical study, investigating the reasons for test code updates occurring after the associated production code changes, and revealing the pervasive existence of noise in the production-test co-evolution samples identified based on the aforementioned assumption by existing works. We define a taxonomy of such noise, including six categories (i.e., adaptive maintenance, perfective maintenance, corrective maintenance, indirectly related production code update, indirectly related test code update, and other reasons). Guided by the empirical findings, we propose CHOSEN (an identifi C ation met H od O f production-te S t co- E volutio N ) based on a two-stage strategy. CHOSEN takes a test code change and its associated production code change as input, aiming to determine whether the production-test change pair is a production-test co-evolution sample. Such identified samples are the basis of or are useful for various downstream tasks. We conduct a series of experiments to evaluate our method. Results show that (1) CHOSEN achieves an AUC of 0.931 and an F1-score of 0.928, significantly outperforming existing identification methods, and (2) CHOSEN can help researchers and practitioners draw more accurate conclusions on studies related to the co-evolution of production and test code. For the task of Just-In-Time (JIT) obsolete test code detection, which can help detect whether a piece of test code should be updated when developers modify the production code, the test set constructed by CHOSEN can help measure the detection method’s performance more accurately, only leading to 0.76% of average error compared with ground truth. In addition, the dataset constructed by CHOSEN can be used to train a better obsolete test code detection model, of which the average improvements on accuracy, precision, recall, and F1-score are 12.00%, 17.35%, 8.75%, and 13.50% respectively.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4382987312",
    "type": "article"
  },
  {
    "title": "Seed Selection for Testing Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3607190",
    "publication_date": "2023-07-07",
    "publication_year": 2023,
    "authors": "Yuhan Zhi; Xiaofei Xie; Chao Shen; Jun Sun; Xiaoyu Zhang; Xiaohong Guan",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) has been applied in many applications. Meanwhile, the quality of DL systems is becoming a big concern. To evaluate the quality of DL systems, a number of DL testing techniques have been proposed. To generate test cases, a set of initial seed inputs are required. Existing testing techniques usually construct seed corpus by randomly selecting inputs from training or test dataset. Till now, there is no study on how initial seed inputs affect the performance of DL testing and how to construct an optimal one. To fill this gap, we conduct the first systematic study to evaluate the impact of seed selection strategies on DL testing. Specifically, considering three popular goals of DL testing (i.e., coverage, failure detection, and robustness), we develop five seed selection strategies, including three based on single-objective optimization (SOO) and two based on multi-objective optimization (MOO). We evaluate these strategies on seven testing tools. Our results demonstrate that the selection of initial seed inputs greatly affects the testing performance. SOO-based selection can construct the best seed corpus that can boost DL testing with respect to the specific testing goal. MOO-based selection strategies can construct seed corpus that achieve balanced improvement on multiple objectives.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4383555034",
    "type": "article"
  },
  {
    "title": "Smart Contract Code Repair Recommendation based on Reinforcement Learning and Multi-metric Optimization",
    "doi": "https://doi.org/10.1145/3637229",
    "publication_date": "2023-12-11",
    "publication_year": 2023,
    "authors": "Hanyang Guo; Y. Chen; Xiangping Chen; Yuan Huang; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "A smart contract is a kind of code deployed on the blockchain that executes automatically once an event triggers a clause in the contract. Since smart contracts involve businesses such as asset transfer, they are more vulnerable to attacks, so it is crucial to ensure the security of smart contracts. Because a smart contract cannot be tampered with once deployed on the blockchain, for smart contract developers, it is necessary to fix vulnerabilities before deployment. Compared with many vulnerability detection tools for smart contracts, the amount of automatic fix approaches for smart contracts is relatively limited. These approaches mainly use defined pattern-based methods or heuristic search algorithms for vulnerability repairs. In this article, we propose RLRep , a reinforcement learning-based approach to provide smart contract repair recommendations for smart contract developers automatically. This approach adopts an agent to provide repair action suggestions based on the vulnerable smart contract without any supervision, which can solve the problem of missing labeled data in machine learning-based repair methods. We evaluate our approach on a dataset containing 853 smart contract programs (programming language: Solidity) with different kinds of vulnerabilities. We split them into training and test sets. The result shows that our approach can provide 54.97% correct repair recommendations for smart contracts.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4389560241",
    "type": "article"
  },
  {
    "title": "Measuring and Clustering Heterogeneous Chatbot Designs",
    "doi": "https://doi.org/10.1145/3637228",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Pablo C. Cañizares; Jose María López-Morales; Sara Pérez-Soler; Esther Guerra; Juan de Lara",
    "corresponding_authors": "",
    "abstract": "Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies. To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob , which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4389675247",
    "type": "article"
  },
  {
    "title": "Automated Mapping of Vulnerability Advisories onto their Fix Commits in Open Source Repositories",
    "doi": "https://doi.org/10.1145/3649590",
    "publication_date": "2024-03-04",
    "publication_year": 2024,
    "authors": "Daan Hommersom; Antonino Sabetta; Bonaventura Coppola; Dario Di Nucci; Damian A. Tamburri",
    "corresponding_authors": "",
    "abstract": "The lack of comprehensive sources of accurate vulnerability data represents a critical obstacle to studying and understanding software vulnerabilities (and their corrections). In this article, we present an approach that combines heuristics stemming from practical experience and machine-learning (ML)—specifically, natural language processing (NLP)—to address this problem. Our method consists of three phases. First, we construct an advisory record object containing key information about a vulnerability that is extracted from an advisory, such as those found in the National Vulnerability Database (NVD). These advisories are expressed in natural language. Second, using heuristics, a subset of candidate fix commits is obtained from the source code repository of the affected project, by filtering out commits that can be identified as unrelated to the vulnerability at hand. Finally, for each of the remaining candidate commits, our method builds a numerical feature vector reflecting the characteristics of the commit that are relevant to predicting its match with the advisory at hand. Based on the values of these feature vectors, our method produces a ranked list of candidate fixing commits. The score attributed by the ML model to each feature is kept visible to the users, allowing them to easily interpret the predictions. We implemented our approach and we evaluated it on an open data set, built by manual curation, that comprises 2,391 known fix commits corresponding to 1,248 public vulnerability advisories. When considering the top-10 commits in the ranked results, our implementation could successfully identify at least one fix commit for up to 84.03% of the vulnerabilities (with a fix commit on the first position for 65.06% of the vulnerabilities). Our evaluation shows that our method can reduce considerably the manual effort needed to search open-source software (OSS) repositories for the commits that fix known vulnerabilities.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3137480023",
    "type": "article"
  },
  {
    "title": "<scp>ARCTURUS</scp> : Full Coverage Binary Similarity Analysis with Reachability-Guided Emulation",
    "doi": "https://doi.org/10.1145/3640337",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Anshunkang Zhou; Yikun Hu; Xiangzhe Xu; Charles Zhang",
    "corresponding_authors": "",
    "abstract": "Binary code similarity analysis is extremely useful, since it provides rich information about an unknown binary, such as revealing its functionality and identifying reused libraries. Robust binary similarity analysis is challenging, as heavy compiler optimizations can make semantically similar binaries have gigantic syntactic differences. Unfortunately, existing semantic-based methods still suffer from either incomplete coverage or low accuracy. In this article, we propose ARCTURUS , a new technique that can achieve high code coverage and high accuracy simultaneously by manipulating program execution under the guidance of code reachability. Our key insight is that the compiler must preserve program semantics (e.g., dependences between code fragments) during compilation; therefore, the code reachability, which implies the interdependence between code, is invariant across code transformations. Based on the above insight, our key idea is to leverage the stability of code reachability to manipulate the program execution such that deep code logic can also be covered in a consistent way. Experimental results show that ARCTURUS achieves an average precision of 87.8% with 100% block coverage, outperforming compared methods by 38.4%, on average. ARCTURUS takes only 0.15 second to process one function, on average, indicating that it is efficient for practical use.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390751813",
    "type": "article"
  },
  {
    "title": "Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors",
    "doi": "https://doi.org/10.1145/3641543",
    "publication_date": "2024-02-01",
    "publication_year": 2024,
    "authors": "Baijun Cheng; Shengming Zhao; Kailong Wang; Meizhen Wang; Guangdong Bai; Ruitao Feng; Yao Guo; Lei Ma; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in domains such as computer vision and natural language processing. Unfortunately, there is still a lack of in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is insufficent for evaluating these approaches, as fidelity incurs significant fluctuations across different datasets and detectors. We subsequently check the precision of the vulnerability-related code lines reported by the explanation approaches, and find poor accuracy in this task among all of them. This can be attributed to the inefficiency of explainers in selecting important features and the presence of irrelevant artifacts learned by DL-based detectors.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391444574",
    "type": "article"
  },
  {
    "title": "Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches",
    "doi": "https://doi.org/10.1145/3643671",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Mohammed Oualid Attaoui; Hazem Fahmy; Fabrizio Pastore; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this article, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively capturing images of the same failure scenario, thus facilitating root cause analysis. Further, it generates distinct clusters for each root cause of failure, thus enabling engineers to detect all the unsafe scenarios. Interestingly, these results hold even for failure scenarios that are only observed in a small percentage of the failing images.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391614448",
    "type": "article"
  },
  {
    "title": "RE Methods for Virtual Reality Software Product Development: A Mapping Study",
    "doi": "https://doi.org/10.1145/3649595",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Sai Anirudh Karre; Y. Raghu Reddy; Raghav Mittal",
    "corresponding_authors": "",
    "abstract": "Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392166345",
    "type": "article"
  },
  {
    "title": "Generating Python Type Annotations from Type Inference: How Far Are We?",
    "doi": "https://doi.org/10.1145/3652153",
    "publication_date": "2024-03-11",
    "publication_year": 2024,
    "authors": "Yimeng Guo; Zhifei Chen; Lin Chen; Wenjie Xu; Yanhui Li; Yuming Zhou; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "In recent years, dynamic languages such as Python have become popular due to their flexibility and productivity. The lack of static typing makes programs face the challenges of fixing type errors, early bug detection, and code understanding. To alleviate these issues, PEP 484 introduced optional type annotations for Python in 2014, but unfortunately, a large number of programs are still not annotated by developers. Annotation generation tools can utilize type inference techniques. However, several important aspects of type annotation generation are overlooked by existing works, such as in-depth effectiveness analysis, potential improvement exploration, and practicality evaluation. And it is unclear how far we have been and how far we can go. In this paper, we set out to comprehensively investigate the effectiveness of type inference tools for generating type annotations, applying three categories of state-of-the-art tools on a carefully-cleaned dataset. First, we use a comprehensive set of metrics and categories, finding that existing tools have different effectiveness and cannot achieve both high accuracy and high coverage. Then, we summarize six patterns to present the limitations in type annotation generation. Next, we implement a simple but effective tool to demonstrate that existing tools can be improved in practice. Finally, we conduct a controlled experiment showing that existing tools can reduce the time spent annotating types and determine more precise types, but cannot reduce subjective difficulty. Our findings point out the limitations and improvement directions in type annotation generation, which can inspire future work.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392645437",
    "type": "article"
  },
  {
    "title": "The IDEA of Us: An Identity-Aware Architecture for Autonomous Systems",
    "doi": "https://doi.org/10.1145/3654439",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Carlos Gavidia-Calderon; Anastasia Kordoni; Amel Bennaceur; Mark Levine; Bashar Nuseibeh",
    "corresponding_authors": "",
    "abstract": "Autonomous systems, such as drones and rescue robots, are increasingly used during emergencies. They deliver services and provide situational awareness that facilitate emergency management and response. To do so, they need to interact and cooperate with humans in their environment. Human behaviour is uncertain and complex, so it can be difficult to reason about it formally. In this article, we propose IDEA: an adaptive software architecture that enables cooperation between humans and autonomous systems, by leveraging the social identity approach. This approach establishes that group membership drives human behaviour. Identity and group membership are crucial during emergencies, as they influence cooperation among survivors. IDEA systems infer the social identity of surrounding humans, thereby establishing their group membership. By reasoning about groups, we limit the number of cooperation strategies the system needs to explore. IDEA systems select a strategy from the equilibrium analysis of game-theoretic models that represent interactions between group members and the IDEA system. We demonstrate our approach using a search-and-rescue scenario, in which an IDEA rescue robot optimises evacuation by collaborating with survivors. Using an empirically validated agent-based model, we show that the deployment of the IDEA system can reduce median evacuation time by 13.6%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4393278036",
    "type": "article"
  },
  {
    "title": "<scp>MR-Scout</scp> : Automated Synthesis of Metamorphic Relations from Existing Test Cases",
    "doi": "https://doi.org/10.1145/3656340",
    "publication_date": "2024-06-29",
    "publication_year": 2024,
    "authors": "Congying Xu; Valerio Terragni; Hengcheng Zhu; Jiarong Wu; Shing-Chi Cheung",
    "corresponding_authors": "",
    "abstract": "Metamorphic Testing (MT) alleviates the oracle problem by defining oracles based on metamorphic relations (MRs) that govern multiple related inputs and their outputs. However, designing MRs is challenging, as it requires domain-specific knowledge. This hinders the widespread adoption of MT. We observe that developer-written test cases can embed domain knowledge that encodes MRs. Such encoded MRs could be synthesized for testing not only their original programs but also other programs that share similar functionalities. In this article, we propose MR-Scout to automatically synthesize MRs from test cases in open-source software (OSS) projects. MR-Scout first discovers MR-encoded test cases (MTCs), and then synthesizes the encoded MRs into parameterized methods (called codified MRs ), and filters out MRs that demonstrate poor quality for new test case generation. MR-Scout discovered over 11,000 MTCs from 701 OSS projects. Experimental results show that over 97% of codified MRs are of high quality for automated test case generation, demonstrating the practical applicability of MR-Scout . Furthermore, codified-MRs-based tests effectively enhance the test adequacy of programs with developer-written tests, leading to 13.52% and 9.42% increases in line coverage and mutation score, respectively. Our qualitative study shows that 55.76% to 76.92% of codified MRs are easily comprehensible for developers.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4394606321",
    "type": "article"
  },
  {
    "title": "Synthesis and Verification of Mission Plans for Multiple Autonomous Agents under Complex Road Conditions",
    "doi": "https://doi.org/10.1145/3672445",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Rong Gu; Eduard Baranov; Afshin Ameri; Cristina Seceleanu; Eduard Paul Enoiu; Baran Çürüklü; Axel Legay; Kristina Lundqvist",
    "corresponding_authors": "",
    "abstract": "Mission planning for multi-agent autonomous systems aims to generate feasible and optimal mission plans that satisfy given requirements. In this article, we propose a tool-supported mission-planning methodology that combines (i) a path-planning algorithm for synthesizing path plans that are safe in environments with complex road conditions, and (ii) a task-scheduling method for synthesizing task plans that schedule the tasks in the right and fastest order, taking into account the planned paths. The task-scheduling method is based on model checking, which provides means of automatically generating task execution orders that satisfy the requirements and ensure the correctness and efficiency of the plans by construction. We implement our approach in a tool named MALTA, which offers a user-friendly GUI for configuring mission requirements, a module for path planning, an integration with the model checker UPPAAL, and functions for automatic generation of formal models, and parsing of the execution traces of models. Experiments with the tool demonstrate its applicability and performance in various configurations of an industrial case study of an autonomous quarry. We also show the adaptability of our tool by employing it in a special case of an industrial case study.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4399557603",
    "type": "article"
  },
  {
    "title": "Automated Commit Intelligence by Pre-training",
    "doi": "https://doi.org/10.1145/3674731",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Shangqing Liu; Yanzhou Li; Xiaofei Xie; Wei Ma; Guozhu Meng; Yang Liu",
    "corresponding_authors": "",
    "abstract": "GitHub commits, which record the code changes with natural language messages for description, play a critical role in software developers’ comprehension of software evolution. Due to their importance in software development, several learning-based works are conducted for GitHub commits, such as commit message generation and security patch identification. However, most existing works focus on customizing specialized neural networks for different tasks. Inspired by the superiority of code pre-trained models, which has confirmed their effectiveness across different downstream tasks, to promote the development of open-source software community, we first collect a large-scale commit benchmark including over 7.99 million commits across 7 programming languages. Based on this benchmark, we present CommitBART, a pre-trained encoder-decoder Transformer model for GitHub commits. The model is pre-trained by three categories (i.e., denoising objectives, cross-modal generation, and contrastive learning) for six pre-training tasks to learn commit fragment representations. Our model is evaluated on one understanding task and three generation tasks for commits. The comprehensive experiments on these tasks demonstrate that CommitBART significantly outperforms previous pre-trained works for code. Further analysis also reveals that each pre-training task enhances the model performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4400190916",
    "type": "article"
  },
  {
    "title": "HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources",
    "doi": "https://doi.org/10.1145/3674726",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Zhouruixing Zhu; Cheryl Lee; Xiaoying Tang; Pinjia He",
    "corresponding_authors": "",
    "abstract": "Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA) , for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7% and 74% on average, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4400212265",
    "type": "article"
  },
  {
    "title": "Enhancing Energy-Awareness in Deep Learning through Fine-Grained Energy Measurement",
    "doi": "https://doi.org/10.1145/3680470",
    "publication_date": "2024-07-26",
    "publication_year": 2024,
    "authors": "Saurabhsingh Rajput; Tim Widmayer; Ziyuan Shang; Maria Kechagia; Federica Sarro; Tushar Sharma",
    "corresponding_authors": "",
    "abstract": "With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of dl systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at fine granularity ( e.g ., at the API level) hinders progress in this area. This paper introduces FECoM ( F ine-grained E nergy Co nsumption M eter ), a framework for fine-grained DL energy consumption measurement. FECoM enables researchers and developers to profile DL APIS from energy perspective. FECoM addresses the challenges of fine-grained energy measurement using static instrumentation while considering factors such as computational load and temperature stability. We assess FECoM’s capability for fine-grained energy measurement for one of the most popular open-source DL frameworks, namely TENSORFLOW. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TENSORFLOW APIS’ energy profiles. Furthermore, we elaborate on the considerations and challenges while designing and implementing a fine-grained energy measurement tool. This work will facilitate further advances in dl energy measurement and the development of energy-aware practices for DL systems.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4401021697",
    "type": "article"
  },
  {
    "title": "Test Case Minimization with Quantum Annealers",
    "doi": "https://doi.org/10.1145/3680467",
    "publication_date": "2024-07-27",
    "publication_year": 2024,
    "authors": "Xinyi Wang; Asmar Muqeet; Tao Yue; Shaukat Ali; Paolo Arcaini",
    "corresponding_authors": "",
    "abstract": "Quantum annealers are specialized quantum computers for solving combinatorial optimization problems with special quantum computing characteristics, e.g., superposition and entanglement. Theoretically, quantum annealers can outperform classic computers. However, current quantum annealers are constrained by a limited number of qubits and cannot demonstrate quantum advantages. Nonetheless, research is needed to develop novel mechanisms to formulate combinatorial optimization problems for quantum annealing (QA). However, QA applications in software engineering remain unexplored. Thus, we propose BootQA , the very first effort at solving test case minimization (TCM) problems on classical software with QA. We provide a novel TCM formulation for QA and utilize bootstrap sampling to optimize the qubit usage. We also implemented our TCM formulation in three other optimization processes: simulated annealing (SA), QA without problem decomposition, and QA with an existing D-Wave problem decomposition strategy, and conducted an empirical evaluation with three real-world TCM datasets. Results show that BootQA outperforms QA without problem decomposition and QA with the existing decomposition strategy regarding effectiveness. Moreover, BootQA ’s effectiveness is similar to SA. Finally, BootQA has higher efficiency in terms of time when solving large TCM problems than the other three optimization processes.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4401048182",
    "type": "article"
  },
  {
    "title": "Interpretable Failure Localization for Microservice Systems Based on Graph Autoencoder",
    "doi": "https://doi.org/10.1145/3695999",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Yongqian Sun; Zihan Lin; Binpeng Shi; Shenglin Zhang; Shiyu Ma; Pengxiang Jin; Zhenyu Zhong; Lemeng Pan; Yicheng Guo; Dan Pei",
    "corresponding_authors": "",
    "abstract": "Accurate and efficient localization of root cause instances in large-scale microservice systems is of paramount importance. Unfortunately, prevailing methods face several limitations. Notably, some recent methods rely on supervised learning which necessitates a substantial amount of labeled data. However, labeling root cause instances is time-consuming and laborious, especially with multiple modalities of data including logs, traces, metrics, etc. Moreover, some approaches favor deep learning for localization but lack interpretability and continuous improvement mechanisms. To address the above challenges, we propose DeepHunt, a novel root cause localization method based on multimodal data analysis. Firstly, DeepHunt introduces Root Cause Score (RCS) by integrating reconstruction errors and failure propagation patterns (upstream-downstream relationships), imparting interpretability to the localization of root causes. Then, it embraces Graph Autoencoder (GAE) to address the limitation imposed by scarce labeled data. It employs data augmentation to mitigate the adverse effects of insufficient historical training samples. We evaluate DeepHunt on two open-source datasets, and it outperforms existing methods when facing a zero-label cold start. DeepHunt can be further improved by continuously fine-tuning through a feedback mechanism.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4402516174",
    "type": "article"
  },
  {
    "title": "Exploring Automated Assertion Generation via Large Language Models",
    "doi": "https://doi.org/10.1145/3699598",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Quanjun Zhang; Weifeng Sun; Chunrong Fang; Bowen Yu; Hongyan Li; Meng Yan; Jianyi Zhou; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Unit testing aims to validate the correctness of software system units and has become an essential practice in software development and maintenance. However, it is incredibly time-consuming and labor-intensive for testing experts to write unit test cases manually, including test inputs ( i.e., prefixes) and test oracles ( i.e., assertions). Very recently, some techniques have been proposed to apply Large Language Models (LLMs) to generate unit assertions and have proven the potential in reducing manual testing efforts. However, there has been no systematic comparison of the effectiveness of these LLMs, and their pros and cons remain unexplored. To bridge this gap, we perform the first extensive study on applying various LLMs to automated assertion generation. The experimental results on two independent datasets show that studied LLMs outperform six state-of-the-art techniques with a prediction accuracy of 51.82% \\(\\sim\\) 58.71% and 38.72% \\(\\sim\\) 48.19%. The improvements achieve 29.60% and 12.47% on average. Besides, as a representative LLM, CodeT5 consistently outperforms all studied LLMs and all baselines on both datasets, with an average improvement of 13.85% and 26.64%, respectively. We also explore the performance of generated assertions in detecting real-world bugs, and find LLMs are able to detect 32 bugs from Defects4J on average, with an improvement of 52.38% against the most recent approach EditAS . Inspired by the findings, we construct a simplistic retrieval-and-repair-enhanced LLM-based approach by transforming the assertion generation problem into a program repair task for retrieved similar assertions. Surprisingly, such a simplistic approach can further improve the prediction accuracy of LLMs by 9.40% on average, leading to new records on both datasets. Besides, we provide additional discussions from different aspects ( e.g., the impact of assertion types and test lengths) to illustrate the capacity and limitations of LLM-based approaches. Finally, we further pinpoint various practical guidelines ( e.g., the improvement of multiple candidate assertions) for advanced LLM-based assertion generation in the near future. Overall, our work underscores the promising future of adopting off-the-shelf LLMs to generate accurate and meaningful assertions in real-world test cases and reduce the manual efforts of unit testing experts in practical scenarios.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4403186335",
    "type": "article"
  },
  {
    "title": "Less is More: Unlocking Semi-Supervised Deep Learning for Vulnerability Detection",
    "doi": "https://doi.org/10.1145/3699602",
    "publication_date": "2024-10-29",
    "publication_year": 2024,
    "authors": "Xiao Yu; Guancheng Lin; Xing Hu; Jacky Keung; Xin Xia",
    "corresponding_authors": "",
    "abstract": "Deep learning has demonstrated its effectiveness in software vulnerability detection, but acquiring a large number of labeled code snippets for training deep learning models is challenging due to labor-intensive annotation. With limited labeled data, complex deep learning models often suffer from overfitting and poor performance. To address this limitation, semi-supervised deep learning offers a promising approach by annotating unlabeled code snippets with pseudo-labels and utilizing limited labeled data together as training sets to train vulnerability detection models. However, applying semi-supervised deep learning for accurate vulnerability detection comes with several challenges. One challenge lies in how to select correctly pseudo-labeled code snippets as training data, while another involves mitigating the impact of potentially incorrectly pseudo-labeled training code snippets during model training. To address these challenges, we propose the Semi-Supervised Vulnerability Detection (SSVD) approach. SSVD leverages the information gain of model parameters as the certainty of the correctness of pseudo-labels and prioritizes high-certainty pseudo-labeled code snippets as training data. Additionally, it incorporates the proposed noise-robust triplet loss to maximize the separation between vulnerable and non-vulnerable code snippets to better propagate labels from labeled code snippets to nearby unlabeled snippets, and utilizes the proposed noise-robust cross-entropy loss for gradient clipping to mitigate the error accumulation caused by incorrect pseudo-labels. We evaluate SSVD with nine semi-supervised approaches on four widely-used public vulnerability datasets. The results demonstrate that SSVD outperforms the baselines with an average of 29.82% improvement in terms of F1-score and 56.72% in terms of MCC. In addition, SSVD trained on a certain proportion of labeled data can outperform or closely match the performance of fully supervised LineVul and ReVeal vulnerability detection models trained on 100% labeled data in most scenarios. This indicates that SSVD can effectively learn from limited labeled data to enhance vulnerability detection performance, thereby reducing the effort required for labeling a large number of code snippets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4403854610",
    "type": "article"
  },
  {
    "title": "Towards Better Comprehension of Breaking Changes in the NPM Ecosystem",
    "doi": "https://doi.org/10.1145/3702991",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Dezhen Kong; Jiakun Liu; Lingfeng Bao; David Lo",
    "corresponding_authors": "",
    "abstract": "Code evolution is prevalent in software ecosystems, which can provide many benefits, such as new features, bug fixes, security patches, etc., while still introducing breaking changes that make downstream projects fail to work. Breaking changes cause a lot of effort to both downstream and upstream developers: downstream developers need to adapt to breaking changes and upstream developers are responsible for identifying and documenting them. In the NPM ecosystem, characterized by frequent code changes and a high tolerance for making breaking changes, the effort is larger. For better comprehension of breaking changes in the NPM ecosystem and to enhance breaking change detection tools, we conduct a large-scale empirical study to investigate breaking changes in the NPM ecosystem. We construct a dataset of explicitly documented breaking changes from 381 popular NPM projects. We find that 95.4% of the detected breaking changes can be covered by developers’ documentation, and 19% of the breaking changes cannot be detected by regression testing. Then in the process of investigating source code of our collected breaking changes, we yield a taxonomy of JavaScript and TypeScript-specific syntactic breaking changes and a taxonomy of major types of behavioral breaking changes. Additionally, we investigate the reasons why developers make breaking changes in NPM and find three major reasons, i.e., to reduce code redundancy, to improve identifier names, and to improve API design, and each category contains several sub-items. We provide actionable implications for future research, e.g., automatic naming and renaming techniques should be applied in JavaScript projects to improve identifier names, future research can try to detect more types of behavioral breaking changes. By presenting the implications, we also discuss the weakness of automatic renaming and breaking change detection approaches, such as the lack of support for public identifiers and various types of breaking changes.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4404002912",
    "type": "article"
  },
  {
    "title": "GUing: A Mobile GUI Search Engine using a Vision-Language Model",
    "doi": "https://doi.org/10.1145/3702993",
    "publication_date": "2024-11-08",
    "publication_year": 2024,
    "authors": "Jialiang Wei; Anne-Lise Courbis; Thomas Lambolais; Binbin Xu; Pierre Louis Bernard; Gérard Dray; Walid Maalej",
    "corresponding_authors": "",
    "abstract": "Graphical User Interfaces (GUIs) are central to app development projects. App developers may use the GUIs of other apps as a means of requirements refinement and rapid prototyping or as a source of inspiration for designing and improving their own apps. Recent research has thus suggested retrieving relevant GUI designs that match a certain text query from screenshot datasets acquired through crowdsourced or automated exploration of GUIs. However, such text-to-GUI retrieval approaches only leverage the textual information of the GUI elements, neglecting visual information such as icons or background images. In addition, retrieved screenshots are not steered by app developers and lack app features that require particular input data. To overcome these limitations, this paper proposes GUing, a GUI search engine based on a vision-language model called GUIClip, which we trained specifically for the problem of designing app GUIs. For this, we first collected from Google Play app introduction images which display the most representative screenshots and are often captioned (i.e. labelled) by app vendors. Then, we developed an automated pipeline to classify, crop, and extract the captions from these images. This resulted in a large dataset which we share with this paper: including 303k app screenshots, out of which 135k have captions. We used this dataset to train a novel vision-language model, which is, to the best of our knowledge, the first of its kind for GUI retrieval. We evaluated our approach on various datasets from related work and in a manual experiment. The results demonstrate that our model outperforms previous approaches in text-to-GUI retrieval achieving a Recall@10 of up to 0.69 and a HIT@10 of 0.91. We also explored the performance of GUIClip for other GUI tasks including GUI classification and sketch-to-GUI retrieval with encouraging results.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4404182948",
    "type": "article"
  },
  {
    "title": "Killing Two Birds with One Stone: Malicious Package Detection in NPM and PyPI using a Single Model of Malicious Behavior Sequence",
    "doi": "https://doi.org/10.1145/3705304",
    "publication_date": "2024-11-26",
    "publication_year": 2024,
    "authors": "J. S. Zhang; Kaifeng Huang; Yiheng Huang; Bihuan Chen; Ruisi Wang; Chong Wang; Xin Peng",
    "corresponding_authors": "",
    "abstract": "Open-source software (OSS) supply chain enlarges the attack surface of a software system, which makes package registries attractive targets for attacks. Recently, multiple package registries have received intensified attacks with malicious packages. Of those package registries, NPM and PyPI are two of the most severe victims. Existing malicious package detectors are developed with features from a list of packages of the same ecosystem and deployed within the same ecosystem exclusively, which is infeasible to utilize the knowledge of a new malicious NPM package detected recently to detect the new malicious package in PyPI. Moreover, existing detectors lack support to model malicious behavior of OSS packages in a sequential way To address the two limitations, we propose a single detection model using malicious behavior sequence, named Cerebro , to detect malicious packages in NPM and PyPI. We curate a feature set based on a high-level abstraction of malicious behavior to enable multi-lingual knowledge fusing. We organize extracted features into a behavior sequence to model sequential malicious behavior. We fine-tune the pre-trained language model to understand the semantics of malicious behavior. Extensive evaluation has demonstrated the effectiveness of Cerebro over the state-of-the-art as well as the practically acceptable efficiency. Cerebro has detected 683 and 799 new malicious packages in PyPI and NPM, and received 707 thank letters from the official PyPI and NPM teams.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4404711543",
    "type": "article"
  },
  {
    "title": "Supporting the restructuring of data abstractions through manipulation of a program visualization",
    "doi": "https://doi.org/10.1145/279310.279312",
    "publication_date": "1998-04-01",
    "publication_year": 1998,
    "authors": "Robert W. Bowdidge; William G. Griswold",
    "corresponding_authors": "",
    "abstract": "With a meaning-preserving restructuring tool, a software engineer can change a program's structure to ease future modifications. However, deciding how to restructure the program requires a global understanding of the program's structure, which cannot be derived easily by directly inspecting the source code. We describe a manipulable program visualization—the star diagram —that supports the restructuring task of encapsulating a global data structure. The star diagram graphically displays information pertinent to encapsulation, and direct manipulation of the diagram causes the underlying program to be restructured. The visualization compactly presents all statements in the program that use the given global data structure, helping the programmer to choose the functions that completely encapsulate it. Additionally, the visualization elides code unrelated to the data structure and to the task and collapses similar expressions to help the programmer identify frequently occurring code fragments and manipulate them together. The visualization is mapped directly to the program text, so manipulation of the visualization also restructures the program. We present the star diagram concept and describe an implementation of the star diagram built upon a meaning-preserving restructuring tool for Scheme. We also describe our creation of star diagram generators for C programs, and we test the scalability of the star diagram using large C and MUMPS programs.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2091469532",
    "type": "article"
  },
  {
    "title": "Comparing test sets and criteria in the presence of test hypotheses and fault domains",
    "doi": "https://doi.org/10.1145/606612.606615",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Robert M. Hierons",
    "corresponding_authors": "Robert M. Hierons",
    "abstract": "A number of authors have considered the problem of comparing test sets and criteria. Ideally test sets are compared using a preorder with the property that test set T 1 is at least as strong as T 2 if whenever T 2 determines that an implementation p is faulty, T 1 will also determine that p is faulty. This notion can be extended to test criteria. However, it has been noted that very few test sets and criteria are comparable under such an ordering; instead orderings are based on weaker properties such as subsumes. This article explores an alternative approach, in which comparisons are made in the presence of a test hypothesis or fault domain. This approach allows strong statements about fault detecting ability to be made and yet for a number of test sets and criteria to be comparable. It may also drive incremental test generation.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1980574120",
    "type": "article"
  },
  {
    "title": "Graph models for reachability analysis of concurrent programs",
    "doi": "https://doi.org/10.1145/210134.210180",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Mauro Pezzè; Richard N. Taylor; Michal Young",
    "corresponding_authors": "",
    "abstract": "The problem of analyzing concurrent systems has been investigated by many researchers, and several solutions have been proposed. Among the proposed techniques, reachability analysis—systematic enumeration of reachable states in a finite-state model—is attractive because it is conceptually simple and relatively straightforward to automate and can be used in conjunction with model-checking procedures to check for application-specific as well as general properties. This article shows that the nature of the translation from source code to a modeling formalism is of greater practical importance than the underlying formalism. Features identified as pragmatically important are the representation of internal choice, selection of a dynamic or static matching rule, and the ease of applying reductions. Since combinatorial explosion is the primary impediment to application of reachability analysis, a particular concern in choosing a model is facilitating divide-and-conquer analysis of large programs. Recently, much interest in finite-state verification systems has centered on algebraic theories of concurrency. Algebraic structure can be used to decompose reachability analysis based on a flowgraph model. The semantic equivalence of graph and Petri net-based models suggests that one ought to be able to apply a similar strategy for decomposing Petri nets. We describe how category-theoretic treatments of Petri nets provide a basis for decomposition of Petri net reachability analysis.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1979669672",
    "type": "article"
  },
  {
    "title": "On statecharts with overlapping",
    "doi": "https://doi.org/10.1145/136586.136589",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "David Harel; Chaim-arie Kahana",
    "corresponding_authors": "",
    "abstract": "The problem of extending the language of statecharts to include overlapping states is considered. The need for such an extension is motivated and the subtlety of the problem is illustrated by exhibiting the shortcomings of naive approaches. The syntax and formal semantics of our extension are then presented, showing in the process that the definitions for conventional statecharts constitute a special case. Our definitions are rather complex, a fact that we feel points to the inherent difficulty of such an extension. We thus prefer to leave open the question of whether or not it should be adopted in practice.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1985660524",
    "type": "article"
  },
  {
    "title": "Wrapper-based evolution of legacy information systems",
    "doi": "https://doi.org/10.1145/1178625.1178626",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "Philippe Thiran; Jean-Luc Hainaut; Geert‐Jan Houben; Djamal Benslimane",
    "corresponding_authors": "",
    "abstract": "System evolution most often implies the integration of legacy components, such as databases, with newly developed ones, leading to mixed architectures that suffer from severe heterogeneity problems. For instance, incorporating a new program in a legacy database application can create an integrity mismatch, since the database model and the program data view can be quite different (e.g. standard file model versus OO model). In addition, neither the legacy DBMS (too weak to address integrity issues correctly) nor the new program (that relies on data server responsibility) correctly cope with data integrity management. The component that can reconciliate these mismatched subsystems is the R/W wrapper , which allows any client program to read, but also to update the legacy data, while controlling the integrity constraints that are ignored by the legacy DBMS.This article describes a generic, technology-independent, R/W wrapper architecture, a methodology for specifying them in a disciplined way, and a CASE tool for generating most of the corresponding code.The key concept is that of implicit construct , which is a structure or a constraint that has not been declared in the database, but which is controlled by the legacy application code. The implicit constructs are elicited through reverse engineering techniques, and then translated into validation code in the wrapper. For instance, a wrapper can be generated for a collection of COBOL files in order to allow external programs to access them through a relational, object-oriented or XML interface, while offering referential integrity control. The methodology is based on a transformational approach that provides a formal way to build the wrapper schema and to specify inter-schema mappings.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2109328702",
    "type": "article"
  },
  {
    "title": "Polychronous design of embedded real-time applications",
    "doi": "https://doi.org/10.1145/1217295.1217298",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Abdoulaye Gamatié; Thierry Gautier; Paul Le Guernic; Jean-Pierre Talpin",
    "corresponding_authors": "",
    "abstract": "Embedded real-time systems consist of hardware and software that controls the behavior of a device or plant. They are ubiquitous in today's technological landscape and found in domains such as telecommunications, nuclear power, avionics, and medical technology. These systems are difficult to design and build because they must satisfy both functional and timing requirements to work correctly in their intended environment. Furthermore, embedded systems are often critical systems, where failure can lead to loss of life, loss of mission, or serious financial consequences. Because of the difficulty in creating these systems and the consequences of failure, they require rigorous and reliable design approaches. The synchronous approach is one possible answer to this demand. Its mathematical basis provides formal concepts that favor the trusted design of embedded real-time systems. The multiclock or polychronous model stands out from other synchronous specification models by its capability to enable the design of systems where each component holds its own activation clock as well as single-clocked systems in a uniform way. A great advantage is its convenience for component-based design approaches that enable modular development of increasingly complex modern systems. The expressiveness of its underlying semantics allows dealing with several issues of real-time design. This article exposes insights gained during recent years from the design of real-time applications within the polychronous framework. In particular, it shows promising results about the design of applications from the avionics domain.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2061616795",
    "type": "article"
  },
  {
    "title": "Test conditions for fault classes in Boolean specifications",
    "doi": "https://doi.org/10.1145/1243987.1243988",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "Kalpesh Kapoor; Jonathan P. Bowen",
    "corresponding_authors": "",
    "abstract": "Fault-based testing of software checks the software implementation for a set of faults. Two previous papers on fault-based testing [Kuhn 1999; Tsuchiya and Kikuno 2002] represent the required behavior of the software as a Boolean specification represented in Disjunctive Normal Form (DNF) and then show that faults may be organized in a hierarchy. This article extends these results by identifying necessary and sufficient conditions for fault-based testing. Unlike previous solutions, the formal analysis used to derive these conditions imposes no restrictions (such as DNF) on the form of the Boolean specification.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1987811078",
    "type": "article"
  },
  {
    "title": "Path exploration based on symbolic output",
    "doi": "https://doi.org/10.1145/2522920.2522925",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Dawei Qi; Hoang Duong Thien Nguyen; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Efficient program path exploration is important for many software engineering activities such as testing, debugging, and verification. However, enumerating all paths of a program is prohibitively expensive. In this article, we develop a partitioning of program paths based on the program output. Two program paths are placed in the same partition if they derive the output similarly, that is, the symbolic expression connecting the output with the inputs is the same in both paths. Our grouping of paths is gradually created by a smart path exploration. Our experiments show the benefits of the proposed path exploration in test-suite construction. Our path partitioning produces a semantic signature of a program—describing all the different symbolic expressions that the output can assume along different program paths. To reason about changes between program versions, we can therefore analyze their semantic signatures. In particular, we demonstrate the applications of our path partitioning in testing and debugging of software regressions.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2071067812",
    "type": "article"
  },
  {
    "title": "Enabledness-based program abstractions for behavior validation",
    "doi": "https://doi.org/10.1145/2491509.2491519",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Guido de Caso; Vı́ctor Braberman; Diego Garbervetsky; Sebastián Uchitel",
    "corresponding_authors": "",
    "abstract": "Code artifacts that have nontrivial requirements with respect to the ordering in which their methods or procedures ought to be called are common and appear, for instance, in the form of API implementations and objects. This work addresses the problem of validating if API implementations provide their intended behavior when descriptions of this behavior are informal, partial, or nonexistent. The proposed approach addresses this problem by generating abstract behavior models which resemble typestates. These models are statically computed and encode all admissible sequences of method calls. The level of abstraction at which such models are constructed has shown to be useful for validating code artifacts and identifying findings which led to the discovery of bugs, adjustment of the requirements expected by the engineer to the requirements implicit in the code, and the improvement of available documentation.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2002561159",
    "type": "article"
  },
  {
    "title": "Combining Genetic Algorithms and Constraint Programming to Support Stress Testing of Task Deadlines",
    "doi": "https://doi.org/10.1145/2818640",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Stefano Di Alesio; Lionel Briand; Shiva Nejati; Arnaud Gotlieb",
    "corresponding_authors": "",
    "abstract": "Tasks in real-time embedded systems (RTES) are often subject to hard deadlines that constrain how quickly the system must react to external inputs. These inputs and their timing vary in a large domain depending on the environment state and can never be fully predicted prior to system execution. Therefore, approaches for stress testing must be developed to uncover possible deadline misses of tasks for different input arrival times. In this article, we describe stress-test case generation as a search problem over the space of task arrival times. Specifically, we search for worst-case scenarios maximizing deadline misses, where each scenario characterizes a test case. In order to scale our search to large industrial-size problems, we combine two state-of-the-art search strategies, namely, genetic algorithms (GA) and constraint programming (CP). Our experimental results show that, in comparison with GA and CP in isolation, GA+CP achieves nearly the same effectiveness as CP and the same efficiency and solution diversity as GA, thus combining the advantages of the two strategies. In light of these results, we conclude that a combined GA+CP approach to stress testing is more likely to scale to large and complex systems.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2233888282",
    "type": "article"
  },
  {
    "title": "Shadow Symbolic Execution for Testing Software Patches",
    "doi": "https://doi.org/10.1145/3208952",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "Tomasz Kuchta; Hristina Palikareva; Cristian Cadar",
    "corresponding_authors": "",
    "abstract": "While developers are aware of the importance of comprehensively testing patches, the large effort involved in coming up with relevant test cases means that such testing rarely happens in practice. Furthermore, even when test cases are written to cover the patch, they often exercise the same behaviour in the old and the new version of the code. In this article, we present a symbolic execution-based technique that is designed to generate test inputs that cover the new program behaviours introduced by a patch. The technique works by executing both the old and the new version in the same symbolic execution instance, with the old version shadowing the new one. During this combined shadow execution, whenever a branch point is reached where the old and the new version diverge, we generate a test input exercising the divergence and comprehensively test the new behaviours of the new version. We evaluate our technique on the Coreutils patches from the CoREBench suite of regression bugs, and show that it is able to generate test inputs that exercise newly added behaviours and expose some of the regression bugs.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2894151403",
    "type": "article"
  },
  {
    "title": "Automatic Workarounds",
    "doi": "https://doi.org/10.1145/2755970",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Antonio Carzaniga; Alessandra Gorla; Nicolò Perino; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "Despite the best intentions, the competence, and the rigorous methods of designers and developers, software is often delivered and deployed with faults. To cope with imperfect software, researchers have proposed the concept of self-healing for software systems. The ambitious goal is to create software systems capable of detecting and responding “autonomically” to functional failures, or perhaps even preempting such failures, to maintain a correct functionality, possibly with acceptable degradation. We believe that self-healing can only be an expression of some form of redundancy, meaning that, to automatically fix a faulty behavior, the correct behavior must be already present somewhere, in some form, within the software system either explicitly or implicitly. One approach is to deliberately design and develop redundant systems, and in fact this kind of deliberate redundancy is the essential ingredient of many fault tolerance techniques. However, this type of redundancy is also generally expensive and does not always satisfy the time and cost constraints of many software projects. With this article we take a different approach. We observe that modern software systems naturally acquire another type of redundancy that is not introduced deliberately but rather arises intrinsically as a by-product of modern modular software design. We formulate this notion of intrinsic redundancy and we propose a technique to exploit it to achieve some level of self-healing. We first demonstrate that software systems are indeed intrinsically redundant. Then we develop a way to express and exploit this redundancy to tolerate faults with automatic workarounds. In essence, a workaround amounts to replacing some failing operations with alternative operations that are semantically equivalent in their intended effect, but that execute different code and ultimately avoid the failure. The technique we propose finds such workarounds automatically. We develop this technique in the context of Web applications. In particular, we implement this technique within a browser extension, which we then use in an evaluation with several known faults and failures of three popular Web libraries. The evaluation demonstrates that automatic workarounds are effective: out of the nearly 150 real faults we analyzed, 100 could be overcome with automatic workarounds, and half of these workarounds found automatically were not publicly known before.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1923648912",
    "type": "article"
  },
  {
    "title": "Is Static Analysis Able to Identify Unnecessary Source Code?",
    "doi": "https://doi.org/10.1145/3368267",
    "publication_date": "2020-01-30",
    "publication_year": 2020,
    "authors": "Roman Haas; Rainer Niedermayr; Tobias Roehm; Sven Apel",
    "corresponding_authors": "",
    "abstract": "Grown software systems often contain code that is not necessary anymore. Such unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data in this way. We investigate to what extent a static analysis approach, which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our approach, we conducted a study involving 14 open-source and closed-source software systems. As there is no perfect oracle for unnecessary code, we compared recommendations for unnecessary code with historical cleanups, runtime usage data, and feedback from 25 developers of five software projects. Our study shows that recommendations generated from stability and centrality information point to unnecessary code that cannot be identified by dead code detectors. Developers confirmed that 34% of recommendations were indeed unnecessary and deleted 20% of the recommendations shortly after our interviews. Overall, our results suggest that static analysis can provide quick feedback on unnecessary code and is useful in practice.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3008981372",
    "type": "article"
  },
  {
    "title": "D <scp>ia</scp> P <scp>ro</scp>",
    "doi": "https://doi.org/10.1145/2894751",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Haipeng Cai; Raúl Santelices; Douglas Thain",
    "corresponding_authors": "",
    "abstract": "Impact analysis not only assists developers with change planning and management, but also facilitates a range of other client analyses, such as testing and debugging. In particular, for developers working in the context of specific program executions, dynamic impact analysis is usually more desirable than static approaches, as it produces more manageable and relevant results with respect to those concrete executions. However, existing techniques for this analysis mostly lie on two extremes: either fast, but too imprecise, or more precise, yet overly expensive. In practice, both more cost-effective techniques and variable cost-effectiveness trade-offs are in demand to fit a variety of usage scenarios and budgets of impact analysis. This article aims to fill the gap between these two extremes with an array of cost-effective analyses and, more broadly, to explore the cost and effectiveness dimensions in the design space of impact analysis. We present the development and evaluation of D ia P ro , a framework that unifies a series of impact analyses, including three new hybrid techniques that combine static and dynamic analyses. Harnessing both static dependencies and multiple forms of dynamic data including method-execution events, statement coverage, and dynamic points-to sets, D ia P ro prunes false-positive impacts with varying strength for variant effectiveness and overheads. The framework also facilitates an in-depth examination of the effects of various program information on the cost-effectiveness of impact analysis. We applied D ia P ro to ten Java applications in diverse scales and domains, evaluating it thoroughly on both arbitrary and repository-based queries from those applications. We show that the three new analyses are all significantly more effective than existing alternatives while remaining efficient, and the D ia P ro framework, as a whole, provides flexible cost-effectiveness choices for impact analysis with the best options for variable needs and budgets. Our study results also suggest that hybrid techniques tend to be much more cost-effective than purely dynamic approaches, in general, and that statement coverage has mostly stronger effects than dynamic points-to sets on the cost-effectiveness of dynamic impact analysis, while static dependencies have even stronger effects than both forms of dynamic data.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2314633885",
    "type": "article"
  },
  {
    "title": "The Impact of Dormant Defects on Defect Prediction: A Study of 19 Apache Projects",
    "doi": "https://doi.org/10.1145/3467895",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Davide Falessi; Aalok Ahluwalia; Massimiliano Di Penta",
    "corresponding_authors": "",
    "abstract": "Defect prediction models can be beneficial to prioritize testing, analysis, or code review activities, and has been the subject of a substantial effort in academia, and some applications in industrial contexts. A necessary precondition when creating a defect prediction model is the availability of defect data from the history of projects. If this data is noisy, the resulting defect prediction model could result to be unreliable. One of the causes of noise for defect datasets is the presence of “dormant defects,” i.e., of defects discovered several releases after their introduction. This can cause a class to be labeled as defect-free while it is not, and is, therefore “snoring.” In this article, we investigate the impact of snoring on classifiers' accuracy and the effectiveness of a possible countermeasure, i.e., dropping too recent data from a training set. We analyze the accuracy of 15 machine learning defect prediction classifiers, on data from more than 4,000 defects and 600 releases of 19 open source projects from the Apache ecosystem. Our results show that on average across projects (i) the presence of dormant defects decreases the recall of defect prediction classifiers, and (ii) removing from the training set the classes that in the last release are labeled as not defective significantly improves the accuracy of the classifiers. In summary, this article provides insights on how to create defects datasets by mitigating the negative effect of dormant defects on defect prediction.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3202129885",
    "type": "article"
  },
  {
    "title": "Are Comments on Stack Overflow Well Organized for Easy Retrieval by Developers?",
    "doi": "https://doi.org/10.1145/3434279",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "Haoxiang Zhang; Shaowei Wang; Tse-Hsun Chen; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "Many Stack Overflow answers have associated informative comments that can strengthen them and assist developers. A prior study found that comments can provide additional information to point out issues in their associated answer, such as the obsolescence of an answer. By showing more informative comments (e.g., the ones with higher scores) and hiding less informative ones, developers can more effectively retrieve information from the comments that are associated with an answer. Currently, Stack Overflow prioritizes the display of comments, and, as a result, 4.4 million comments (possibly including informative comments) are hidden by default from developers. In this study, we investigate whether this mechanism effectively organizes informative comments. We find that (1) the current comment organization mechanism does not work well due to the large amount of tie-scored comments (e.g., 87% of the comments have 0-score) and (2) in 97.3% of answers with hidden comments, at least one comment that is possibly informative is hidden while another comment with the same score is shown (i.e., unfairly hidden comments). The longest unfairly hidden comment is more likely to be informative than the shortest one. Our findings highlight that Stack Overflow should consider adjusting the comment organization mechanism to help developers effectively retrieve informative comments. Furthermore, we build a classifier that can effectively distinguish informative comments from uninformative comments. We also evaluate two alternative comment organization mechanisms (i.e., the Length mechanism and the Random mechanism) based on text similarity and the prediction of our classifier.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3131995106",
    "type": "article"
  },
  {
    "title": "Automatically Identifying the Quality of Developer Chats for Post Hoc Use",
    "doi": "https://doi.org/10.1145/3450503",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Preetha Chatterjee; Kostadin Damevski; Nicholas A. Kraft; Lori Pollock",
    "corresponding_authors": "",
    "abstract": "Software engineers are crowdsourcing answers to their everyday challenges on Q&amp;A forums (e.g., Stack Overflow) and more recently in public chat communities such as Slack, IRC, and Gitter. Many software-related chat conversations contain valuable expert knowledge that is useful for both mining to improve programming support tools and for readers who did not participate in the original chat conversations. However, most chat platforms and communities do not contain built-in quality indicators (e.g., accepted answers, vote counts). Therefore, it is difficult to identify conversations that contain useful information for mining or reading, i.e., conversations of post hoc quality. In this article, we investigate automatically detecting developer conversations of post hoc quality from public chat channels. We first describe an analysis of 400 developer conversations that indicate potential characteristics of post hoc quality, followed by a machine learning-based approach for automatically identifying conversations of post hoc quality. Our evaluation of 2,000 annotated Slack conversations in four programming communities (python, clojure, elm, and racket) indicates that our approach can achieve precision of 0.82, recall of 0.90, F-measure of 0.86, and MCC of 0.57. To our knowledge, this is the first automated technique for detecting developer conversations of post hoc quality.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3186527712",
    "type": "article"
  },
  {
    "title": "Classifying Mobile Applications Using Word Embeddings",
    "doi": "https://doi.org/10.1145/3474827",
    "publication_date": "2021-11-17",
    "publication_year": 2021,
    "authors": "Fahimeh Ebrahimi; Miroslav Tushev; Anas Mahmoud",
    "corresponding_authors": "",
    "abstract": "Modern application stores enable developers to classify their apps by choosing from a set of generic categories, or genres, such as health, games, and music. These categories are typically static—new categories do not necessarily emerge over time to reflect innovations in the mobile software landscape. With thousands of apps classified under each category, locating apps that match a specific consumer interest can be a challenging task. To overcome this challenge, in this article, we propose an automated approach for classifying mobile apps into more focused categories of functionally related application domains. Our aim is to enhance apps visibility and discoverability. Specifically, we employ word embeddings to generate numeric semantic representations of app descriptions. These representations are then classified to generate more cohesive categories of apps. Our empirical investigation is conducted using a dataset of 600 apps, sampled from the Education, Health&amp;Fitness, and Medical categories of the Apple App Store. The results show that our classification algorithms achieve their best performance when app descriptions are vectorized using GloVe, a count-based model of word embeddings. Our findings are further validated using a dataset of Sharing Economy apps and the results are evaluated by 12 human subjects. The results show that GloVe combined with Support Vector Machines can produce app classifications that are aligned to a large extent with human-generated classifications.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3211399334",
    "type": "article"
  },
  {
    "title": "Blindspots in Python and Java APIs Result in Vulnerable Code",
    "doi": "https://doi.org/10.1145/3571850",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Yuriy Brun; Tian Lin; Jessie Somerville; Elisha Myers; Natalie C. Ebner",
    "corresponding_authors": "",
    "abstract": "Blindspots in APIs can cause software engineers to introduce vulnerabilities, but such blindspots are, unfortunately, common. We study the effect APIs with blindspots have on developers in two languages by replicating a 109-developer, 24-Java-API controlled experiment. Our replication applies to Python and involves 129 new developers and 22 new APIs. We find that using APIs with blindspots statistically significantly reduces the developers’ ability to correctly reason about the APIs in both languages, but that the effect is more pronounced for Python. Interestingly, for Java, the effect increased with complexity of the code relying on the API, whereas for Python, the opposite was true. This suggests that Python developers are less likely to notice potential for vulnerabilities in complex code than in simple code, whereas Java developers are more likely to recognize the extra complexity and apply more care, but are more careless with simple code. Whether the developers considered API uses to be more difficult, less clear, and less familiar did not have an effect on their ability to correctly reason about them. Developers with better long-term memory recall were more likely to correctly reason about APIs with blindspots, but short-term memory, processing speed, episodic memory, and memory span had no effect. Surprisingly, professional experience and expertise did not improve the developers’ ability to reason about APIs with blindspots across both languages, with long-term professionals with many years of experience making mistakes as often as relative novices. Finally, personality traits did not significantly affect the Python developers’ ability to reason about APIs with blindspots, but less extroverted and more open developers were better at reasoning about Java APIs with blindspots. Overall, our findings suggest that blindspots in APIs are a serious problem across languages, and that experience and education alone do not overcome that problem, suggesting that tools are needed to help developers recognize blindspots in APIs as they write code that uses those APIs.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3135096732",
    "type": "article"
  },
  {
    "title": "Coverage-Based Debloating for Java Bytecode",
    "doi": "https://doi.org/10.1145/3546948",
    "publication_date": "2022-07-06",
    "publication_year": 2022,
    "authors": "César Soto-Valero; Thomas Durieux; Nicolas Harrand; Benoît Baudry",
    "corresponding_authors": "",
    "abstract": "Software bloat is code that is packaged in an application but is actually not necessary to run the application. The presence of software bloat is an issue for security, performance, and for maintenance. In this article, we introduce a novel technique for debloating, which we call coverage-based debloating. We implement the technique for one single language: Java bytecode. We leverage a combination of state-of-the-art Java bytecode coverage tools to precisely capture what parts of a project and its dependencies are used when running with a specific workload. Then, we automatically remove the parts that are not covered, in order to generate a debloated version of the project. We succeed to debloat 211 library versions from a dataset of 94 unique open-source Java libraries. The debloated versions are syntactically correct and preserve their original behaviour according to the workload. Our results indicate that 68.3% of the libraries’ bytecode and 20.3% of their total dependencies can be removed through coverage-based debloating. For the first time in the literature on software debloating, we assess the utility of debloated libraries with respect to client applications that reuse them. We select 988 client projects that either have a direct reference to the debloated library in their source code or which test suite covers at least one class of the libraries that we debloat. Our results show that 81.5% of the clients, with at least one test that uses the library, successfully compile and pass their test suite when the original library is replaced by its debloated version.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4283826179",
    "type": "article"
  },
  {
    "title": "Uncertainty-Aware Robustness Assessment of Industrial Elevator Systems",
    "doi": "https://doi.org/10.1145/3576041",
    "publication_date": "2022-12-12",
    "publication_year": 2022,
    "authors": "Liping Han; Shaukat Ali; Tao Yue; Aitor Arrieta; Maite Arratibel",
    "corresponding_authors": "",
    "abstract": "Industrial elevator systems are commonly used software systems in our daily lives, which operate in uncertain environments such as unpredictable passenger traffic, uncertain passenger attributes and behaviors, and hardware delays. Understanding and assessing the robustness of such systems under various uncertainties enable system designers to reason about uncertainties, especially those leading to low system robustness, and consequently improve their designs and implementations in terms of handling uncertainties. To this end, we present a comprehensive empirical study conducted with industrial elevator systems provided by our industrial partner Orona, which focuses on assessing the robustness of a dispatcher—that is, a software component responsible for elevators’ optimal scheduling. In total, we studied 90 industrial dispatchers in our empirical study. Based on the experience gained from the study, we derived an uncertainty-aware robustness assessment method (named UncerRobua ) comprising a set of guidelines on how to conduct the robustness assessment and a newly proposed ranking algorithm, for supporting the robustness assessment of industrial elevator systems against uncertainties.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4311411307",
    "type": "article"
  },
  {
    "title": "Influential Global and Local Contexts Guided Trace Representation for Fault Localization",
    "doi": "https://doi.org/10.1145/3576043",
    "publication_date": "2022-12-15",
    "publication_year": 2022,
    "authors": "Zhuo Zhang; Yan Lei; Ting Su; Meng Yan; Xiaoguang Mao; Yue Yu",
    "corresponding_authors": "",
    "abstract": "Trace data is critical for fault localization (FL) to analyze suspicious statements potentially responsible for a failure. However, existing trace representation meets its bottleneck mainly in two aspects: (1) the trace information of a statement is restricted to a local context (i.e., a test case) without the consideration of a global context (i.e., all test cases of a test suite); (2) it just uses the ‘occurrence’ for representation without strong FL semantics. Thus, we propose UNITE : an infl U ential co N text-Gu I ded T race r E presentation, representing the trace from both global and local contexts with influential semantics for FL. UNITE embodies and implements two key ideas: (1) UNITE leverages the widely used weighting capability from local and global contexts of information retrieval to reflect how important a statement (a word) is to a test case (a document) in all test cases of a test suite (a collection), where a test case (a document) and all test cases of a test suite (a collection) represent local and global contexts respectively; (2) UNITE further elaborates the trace representation from ‘occurrence’ (weak semantics) to ‘influence’ (strong semantics) by combing program dependencies. The large-scale experiments on 12 FL techniques and 20 programs show that UNITE significantly improves FL effectiveness.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4311805847",
    "type": "article"
  },
  {
    "title": "Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?",
    "doi": "https://doi.org/10.1145/3582572",
    "publication_date": "2023-02-01",
    "publication_year": 2023,
    "authors": "Zhaoqiang Guo; Shiran Liu; Xutong Liu; Wei Lai; Mingliang Ma; Xu Zhang; Chao Ni; Yibiao Yang; Yanhui Li; Lin Chen; Guoqiang Zhou; Yuming Zhou",
    "corresponding_authors": "",
    "abstract": "Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program. Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin. Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively. Method. We propose a simple heuristic baseline solution GLANCE (aimin G at contro L - AN d C ompl E x-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT). Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators. Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4318776432",
    "type": "article"
  },
  {
    "title": "Client-Specific Upgrade Compatibility Checking via Knowledge-Guided Discovery",
    "doi": "https://doi.org/10.1145/3582569",
    "publication_date": "2023-02-01",
    "publication_year": 2023,
    "authors": "Chenguang Zhu; Mengshi Zhang; Xiuheng Wu; Xiufeng Xu; Yi Li",
    "corresponding_authors": "",
    "abstract": "Modern software systems are complex, and they heavily rely on external libraries developed by different teams and organizations. Such systems suffer from higher instability due to incompatibility issues caused by library upgrades. In this article, we address the problem by investigating the impact of a library upgrade on the behaviors of its clients. We developed CompCheck , an automated upgrade compatibility checking framework that generates incompatibility-revealing tests based on previous examples. CompCheck first establishes an offline knowledge base of incompatibility issues by mining from open source projects and their upgrades. It then discovers incompatibilities for a specific client project, by searching for similar library usages in the knowledge base and generating tests to reveal the problems. We evaluated CompCheck on 202 call sites of 37 open source projects and the results show that CompCheck successfully revealed incompatibility issues on 76 call sites, 72.7% and 94.9% more than two existing techniques, confirming CompCheck ’s applicability and effectiveness.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4318831784",
    "type": "article"
  },
  {
    "title": "<scp>Katana</scp> : Dual Slicing Based Context for Learning Bug Fixes",
    "doi": "https://doi.org/10.1145/3579640",
    "publication_date": "2023-02-06",
    "publication_year": 2023,
    "authors": "Mifta Sintaha; Noor Nashid; Ali Mesbah",
    "corresponding_authors": "",
    "abstract": "Contextual information plays a vital role for software developers when understanding and fixing a bug. Consequently, deep learning based program repair techniques leverage context for bug fixes. However, existing techniques treat context in an arbitrary manner, by extracting code in close proximity of the buggy statement within the enclosing file, class, or method, without any analysis to find actual relations with the bug. To reduce noise, they use a predefined maximum limit on the number of tokens to be used as context. We present a program slicing based approach, in which instead of arbitrarily including code as context, we analyze statements that have a control or data dependency on the buggy statement. We propose a novel concept called dual slicing , which leverages the context of both buggy and fixed versions of the code to capture relevant repair ingredients. We present our technique and tool called Katana , the first to apply slicing-based context for a program repair task. The results show that Katana effectively preserves sufficient information for a model to choose contextual information while reducing noise. We compare against four recent state-of-the-art context-aware program repair techniques. Our results show that Katana fixes between 1.5 and 3.7 times more bugs than existing techniques.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4319296048",
    "type": "article"
  },
  {
    "title": "Fine-grained Coverage-based Fuzzing",
    "doi": "https://doi.org/10.1145/3587158",
    "publication_date": "2023-03-14",
    "publication_year": 2023,
    "authors": "Wei‐Cheng Wu; Bernard Nongpoh; Marwan Nour; Michaël Marcozzi; Sébastien Bardin; Christophe Hauser",
    "corresponding_authors": "",
    "abstract": "Fuzzing is a popular software testing method that discovers bugs by massively feeding target applications with automatically generated inputs. Many state-of-the-art fuzzers use branch coverage as a feedback metric to guide the fuzzing process. The fuzzer retains inputs for further mutation only if branch coverage is increased. However, branch coverage only provides a shallow sampling of program behaviors and hence may discard interesting inputs to mutate. This work aims to take advantage of the large body of research in defining finer-grained code coverage metrics (such as control-flow, data-flow, or mutation coverage) and to evaluate how fuzzing performance is impacted when using these metrics to select interesting inputs for mutation. We propose to make branch coverage-based fuzzers support most fine-grained coverage metrics out of the box (i.e., without changing fuzzer internals). We achieve this by making the test objectives defined by these metrics (such as conditions to activate or mutants to kill) explicit as new branches in the target program. Fuzzing such a modified target is then equivalent to fuzzing the original target, but the fuzzer will also retain inputs covering the additional metric objectives for mutation. In addition, all the fuzzer mechanisms to penetrate hard-to-cover branches will help in covering the additional metric objectives. We use this approach to evaluate the impact of supporting two fine-grained coverage metrics (multiple condition coverage and weak mutation) over the performance of two state-of-the-art fuzzers (AFL++ and QSYM) with the standard LAVA-M and MAGMA benchmarks. This evaluation suggests that our mechanism for runtime fuzzer guidance, where the fuzzed code is instrumented with additional branches, is effective and could be leveraged to encode guidance from human users or static analyzers. Our results also show that the impact of fine-grained metrics over fuzzing performance is hard to predict before fuzzing and most of the time either neutral or negative. As a consequence, we do not recommend using them to guide fuzzers, except maybe in some possibly favorable circumstances yet to be investigated, like for limited parts of the code or to complement classical fuzzing campaigns.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4324192566",
    "type": "article"
  },
  {
    "title": "<scp>TestSGD</scp> : Interpretable Testing of Neural Networks against Subtle Group Discrimination",
    "doi": "https://doi.org/10.1145/3591869",
    "publication_date": "2023-04-10",
    "publication_year": 2023,
    "authors": "Mengdi Zhang; Jun Sun; Jingyi Wang; Bing Sun",
    "corresponding_authors": "",
    "abstract": "Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden , is much less studied, compared with identifying individual discrimination . In this work, we propose TestSGD , an interpretable testing approach that systematically identifies and measures hidden (which we call “subtle”) group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes . Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4363677096",
    "type": "article"
  },
  {
    "title": "<i>ArchRepair</i> : Block-Level Architecture-Oriented Repairing for Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3585005",
    "publication_date": "2023-07-24",
    "publication_year": 2023,
    "authors": "Hua Xiao Qi; Zhijie Wang; Qing Guo; Jianlang Chen; Felix Juefei-Xu; Fuyuan Zhang; Lei Ma; Jianjun Zhao",
    "corresponding_authors": "",
    "abstract": "Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level). We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization , where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing . Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons’ status and weights’ gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair , and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4385222146",
    "type": "article"
  },
  {
    "title": "Hierarchical Distribution-aware Testing of Deep Learning",
    "doi": "https://doi.org/10.1145/3625290",
    "publication_date": "2023-09-24",
    "publication_year": 2023,
    "authors": "Wei Huang; Xingyu Zhao; Alec Banks; Victoria Cox; Xiaowei Huang",
    "corresponding_authors": "",
    "abstract": "With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs) , are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution–agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model’s dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm–based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386998279",
    "type": "article"
  },
  {
    "title": "How Are Multilingual Systems Constructed: Characterizing Language Use and Selection in Open-Source Multilingual Software",
    "doi": "https://doi.org/10.1145/3631967",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Wen Li; Austin Marino; Haoran Yang; Na Meng; Li Li; Haipeng Cai",
    "corresponding_authors": "",
    "abstract": "For many years now, modern software is known to be developed in multiple languages (hence termed as multilingual or multi-language software). Yet, to date, we still only have very limited knowledge about how multilingual software systems are constructed. For instance, it is not yet really clear how different languages are used, selected together, and why they have been so in multilingual software development. Given the fact that using multiple languages in a single software project has become a norm, understanding language use and selection (i.e., language profile ) as a basic element of the multilingual construction in contemporary software engineering is an essential first step. In this article, we set out to fill this gap with a large-scale characterization study on language use and selection in open-source multilingual software. We start with presenting an updated overview of language use in 7,113 GitHub projects spanning the 5 past years by characterizing overall statistics of language profiles, followed by a deeper look into the functionality relevance/justification of language selection in these projects through association rule mining. We proceed with an evolutionary characterization of 1,000 GitHub projects for each of the 10 past years to provide a longitudinal view of how language use and selection have changed over the years, as well as how the association between functionality and language selection has been evolving. Among many other findings, our study revealed a growing trend of using three to five languages in one multilingual software project and the noticeable stableness of top language selections. We found a non-trivial association between language selection and certain functionality domains, which was less stable than that with individual languages over time. In a historical context, we also have observed major shifts in these characteristics of multilingual systems both in contrast to earlier peer studies and along the evolutionary timeline. Our findings offer essential knowledge on the multilingual construction in modern software development. Based on our results, we also provide insights and actionable suggestions for both researchers and developers of multilingual systems.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4388421852",
    "type": "article"
  },
  {
    "title": "Attack as Detection: Using Adversarial Attack Methods to Detect Abnormal Examples",
    "doi": "https://doi.org/10.1145/3631977",
    "publication_date": "2023-11-10",
    "publication_year": 2023,
    "authors": "Zhe Zhao; Guangke Chen; Tong Liu; Taishan Li; Fu Song; Jingyi Wang; Jun Sun",
    "corresponding_authors": "",
    "abstract": "As a new programming paradigm, deep learning (DL) has achieved impressive performance in areas such as image processing and speech recognition, and has expanded its application to solve many real-world problems. However, neural networks and DL are normally black-box systems; even worse, DL-based software are vulnerable to threats from abnormal examples, such as adversarial and backdoored examples constructed by attackers with malicious intentions as well as unintentionally mislabeled samples. Therefore, it is important and urgent to detect such abnormal examples. Although various detection approaches have been proposed respectively addressing some specific types of abnormal examples, they suffer from some limitations; until today, this problem is still of considerable interest. In this work, we first propose a novel characterization to distinguish abnormal examples from normal ones based on the observation that abnormal examples have significantly different (adversarial) robustness from normal ones. We systemically analyze those three different types of abnormal samples in terms of robustness and find that they have different characteristics from normal ones. As robustness measurement is computationally expensive and hence can be challenging to scale to large networks, we then propose to effectively and efficiently measure robustness of an input sample using the cost of adversarially attacking the input, which was originally proposed to test robustness of neural networks against adversarial examples. Next, we propose a novel detection method, named attack as detection (A 2 D for short), which uses the cost of adversarially attacking an input instead of robustness to check if it is abnormal. Our detection method is generic, and various adversarial attack methods could be leveraged. Extensive experiments show that A 2 D is more effective than recent promising approaches that were proposed to detect only one specific type of abnormal examples. We also thoroughly discuss possible adaptive attack methods to our adversarial example detection method and show that A 2 D is still effective in defending carefully designed adaptive adversarial attack methods—for example, the attack success rate drops to 0% on CIFAR10.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4388574628",
    "type": "article"
  },
  {
    "title": "The Lost World: Characterizing and Detecting Undiscovered Test Smells",
    "doi": "https://doi.org/10.1145/3631973",
    "publication_date": "2023-11-20",
    "publication_year": 2023,
    "authors": "Yanming Yang; Xing Hu; Xin Xia; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Test smell refers to poor programming and design practices in testing and widely spreads throughout software projects. Considering test smells have negative impacts on the comprehension and maintenance of test code and even make code-under-test more defect-prone, it thus has great importance in mining, detecting, and refactoring them. Since Deursen et al. introduced the definition of “test smell”, several studies worked on discovering new test smells from test specifications and software practitioners’ experience. Indeed, many bad testing practices are “observed” by software developers during creating test scripts rather than through academic research and are widely discussed in the software engineering community (e.g., Stack Overflow) [ 70 , 94 ]. However, no prior studies explored new bad testing practices from software practitioners’ discussions, formally defined them as new test smell types, and analyzed their characteristics, which plays a bad role for developers in knowing these bad practices and avoiding using them during test code development. Therefore, we pick up those challenges and act by working on systematic methods to explore new test smell types from one of the most mainstream developers’ Q&amp;A platforms, i.e., Stack Overflow. We further investigate the harmfulness of new test smells and analyze possible solutions for eliminating them. We find that some test smells make it hard for developers to fix failed test cases and trace their failing reasons. To exacerbate matters, we have identified two types of test smells that pose a risk to the accuracy of test cases. Next, we develop a detector to detect test smells from software. The detector is composed of six detection methods for different smell types. These detection methods are both wrapped with a set of syntactic rules based on the code patterns extracted from different test smells and developers’ code styles. We manually construct a test smell dataset from seven popular Java projects and evaluate the effectiveness of our detector on it. The experimental results show that our detector achieves high performance in precision, recall, and F1 score. Then, we utilize our detector to detect smells from 919 real-world Java projects to explore whether the six test smells are prevalent in practice. We observe that these test smells are widely spread in 722 out of 919 Java projects, which demonstrates that they are prevalent in real-world projects. Finally, to validate the usefulness of test smells in practice, we submit 56 issue reports to 53 real-world projects with different smells. Our issue reports achieve 76.4% acceptance by conducting sentiment analysis on developers’ replies. These evaluations confirm the effectiveness of our detector and the prevalence and practicality of new test smell types on real-world projects.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4388827066",
    "type": "article"
  },
  {
    "title": "Using shape analysis to reduce finite-state models of concurrent Java programs",
    "doi": "https://doi.org/10.1145/332740.332741",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "James C. Corbett",
    "corresponding_authors": "James C. Corbett",
    "abstract": "Finite-state verification (e.g., model checking) provides a powerful means to detect concurrency errors, which are often subtle and difficult to reproduce. Nevertheless, widespread use of this technology by developers is unlikely until tools provide automated support for extracting the required finite-state models directly from program source. Unfortunately, the dynamic features of modern languages such as Java complicate the construction of compact finite-state models for verification. In this article, we show how shape analysis, which has traditionally been used for computing alias information in optimizers, can be used to greatly reduce the size of finite-state models of concurrent Java programs by determining which heap-allocated variables are accessible only by a single thread, and which shared variables are protected by locks. We also provide several other state-space reductions based on the semantics of Java monitors. A prototype of the reductions demonstrates their effectiveness.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1980487816",
    "type": "article"
  },
  {
    "title": "Composition and refinement of discrete real-time systems",
    "doi": "https://doi.org/10.1145/295558.295560",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Jonathan S. Ostroff",
    "corresponding_authors": "Jonathan S. Ostroff",
    "abstract": "Reactive systems exhibit ongoing, possibly nonterminating, interaction with the environment. Real-time systems are reactive systems that must satisfy quantitative timing constraints. This paper presents a structured compositional design method for discrete real-time systems that can be used to combat the combinatorial explosion of states in the verification of large systems. A composition rule describes how the correctness of the system can be determined from the correctness of its modules, without knowledge of their internal structure. The advantage of compositional verification is clear. Each module is both simpler and smaller than the system itself. Composition requires the use of both model-checking and deductive techniques. A refinement rule guarantees that specifications of high-level modules are preserved by their implementations. The StateTime toolset is used to automate parts of compositional designs using a combination of model-checking and simulation. The design method is illustrated using a reactor shutdown system that cannot be verified using the StateTime toolset (due to the combinatorial explosion of states) without compositional reasoning. The reactor example also illustrates the use of the refinement rule.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1978707903",
    "type": "article"
  },
  {
    "title": "The Larch/Smalltalk interface specification language",
    "doi": "https://doi.org/10.1145/196092.195325",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Yoonsik Cheon; Gary T. Leavens",
    "corresponding_authors": "",
    "abstract": "article The Larch/Smalltalk interface specification language Share on Authors: Yoonsik Cheon Iowa State University Iowa State UniversityView Profile , Gary T. Leavens Iowa State University Iowa State UniversityView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 3Issue 3July 1994 pp 221–153https://doi.org/10.1145/196092.195325Online:01 July 1994Publication History 19citation518DownloadsMetricsTotal Citations19Total Downloads518Last 12 Months5Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2097310563",
    "type": "article"
  },
  {
    "title": "Classifying data dependences in the presence of pointers for program comprehension, testing, and debugging",
    "doi": "https://doi.org/10.1145/1018210.1018212",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "Alessandro Orso; Saurabh Sinha; Mary Jean Harrold",
    "corresponding_authors": "",
    "abstract": "Understanding data dependences in programs is important for many software-engineering activities, such as program understanding, impact analysis, reverse engineering, and debugging. The presence of pointers can cause subtle and complex data dependences that can be difficult to understand. For example, in languages such as C, an assignment made through a pointer dereference can assign a value to one of several variables, none of which may appear syntactically in that statement. In the first part of this article, we describe two techniques for classifying data dependences in the presence of pointer dereferences. The first technique classifies data dependences based on definition type, use type, and path type. The second technique classifies data dependences based on span. We present empirical results to illustrate the distribution of data-dependence types and spans for a set of real C programs. In the second part of the article, we discuss two applications of the classification techniques. First, we investigate different ways in which the classification can be used to facilitate data-flow testing. We outline an approach that uses types and spans of data dependences to determine the appropriate verification technique for different data dependences; we present empirical results to illustrate the approach. Second, we present a new slicing approach that computes slices based on types of data dependences. Based on the new approach, we define an incremental slicing technique that computes a slice in multiple steps. We present empirical results to illustrate the sizes of incremental slices and the potential usefulness of incremental slicing for debugging.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1978068196",
    "type": "article"
  },
  {
    "title": "A framework and tool support for the systematic testing of model-based specifications",
    "doi": "https://doi.org/10.1145/990010.990012",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Tim Miller; Paul Strooper",
    "corresponding_authors": "",
    "abstract": "Formal specifications can precisely and unambiguously define the required behavior of a software system or component. However, formal specifications are complex artifacts that need to be verified to ensure that they are consistent, complete, and validated against the requirements. Specification testing or animation tools exist to assist with this by allowing the specifier to interpret or execute the specification. However, currently little is known about how to do this effectively.This article presents a framework and tool support for the systematic testing of formal, model-based specifications. Several important generic properties that should be satisfied by model-based specifications are first identified. Following the idea of mutation analysis, we then use variants or mutants of the specification to check that these properties are satisfied. The framework also allows the specifier to test application-specific properties. All properties are tested for a range of states that are defined by the tester in the form of a testgraph, which is a directed graph that partially models the states and transitions of the specification being tested. Tool support is provided for the generation of the mutants, for automatically traversing the testgraph and executing the test cases, and for reporting any errors. The framework is demonstrated on a small specification and its application to three larger specifications is discussed. Experience indicates that the framework can be used effectively to test small to medium-sized specifications and that it can reveal a significant number of problems in these specifications.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1989169215",
    "type": "article"
  },
  {
    "title": "Efficient Analysis of DynAlloy Specifications",
    "doi": "https://doi.org/10.1145/1314493.1314497",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Marcelo F. Frias; Carlos G. López Pombo; Juan Pablo Galeotti; Nazareno Aguirre",
    "corresponding_authors": "",
    "abstract": "DynAlloy is an extension of Alloy to support the definition of actions and the specification of assertions regarding execution traces. In this article we show how we can extend the Alloy tool so that DynAlloy specifications can be automatically analyzed in an efficient way. We also demonstrate that DynAlloy's semantics allows for a sound technique that we call program atomization , which improves the analyzability of properties regarding execution traces by considering certain programs as atomic steps in a trace. We present the foundations, case studies, and empirical results indicating that the analysis of DynAlloy specifications can be performed efficiently.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2029009274",
    "type": "article"
  },
  {
    "title": "Verdict functions in testing with a fault domain or test hypotheses",
    "doi": "https://doi.org/10.1145/1538942.1538944",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "Robert M. Hierons",
    "corresponding_authors": "Robert M. Hierons",
    "abstract": "In state-based testing, it is common to include verdicts within test cases, the result of the test case being the verdict reached by the test run. In addition, approaches that reason about test effectiveness or produce tests that are guaranteed to find certain classes of faults are often based on either a fault domain or a set of test hypotheses. This article considers how the presence of a fault domain or test hypotheses affects our notion of a test verdict. The analysis reveals the need for new verdicts that provide more information than the current verdicts and for verdict functions that return a verdict based on a set of test runs rather than a single test run. The concepts are illustrated in the contexts of testing from a nondeterministic finite state machine and the testing of a datatype specified using an algebraic specification language but are potentially relevant whenever fault domains or test hypotheses are used.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2060561921",
    "type": "article"
  },
  {
    "title": "Unifying aspect- and object-oriented design",
    "doi": "https://doi.org/10.1145/1555392.1555396",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Hridesh Rajan; Kevin Sullivan",
    "corresponding_authors": "",
    "abstract": "The contribution of this work is the design and evaluation of a programming language model that unifies aspects and classes as they appear in AspectJ-like languages. We show that our model preserves the capabilities of AspectJ-like languages, while improving the conceptual integrity of the language model and the compositionality of modules. The improvement in conceptual integrity is manifested by the reduction of specialized constructs in favor of uniform orthogonal constructs. The enhancement in compositionality is demonstrated by better modularization of integration and higher-order crosscutting concerns.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2069173317",
    "type": "article"
  },
  {
    "title": "Tools and experiments supporting a testing-based theory of component composition",
    "doi": "https://doi.org/10.1145/1525880.1525885",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Dick Hamlet",
    "corresponding_authors": "Dick Hamlet",
    "abstract": "Development of software using off-the-shelf components seems to offer a chance for improving product quality and developer productivity. This article reviews a foundational testing-based theory of component composition, describes tools that implement the theory, and presents experiments with functional and nonfunctional component/system properties that validate the theory and illuminate issues in component composition. The context for this work is an ideal form of Component-Based Software Development (CBSD) supported by tools. Component developers describe their components by measuring approximations to functional and nonfunctional behavior on a finite collection of subdomains. Systems designers describe an application-system structure by the component connections that form it. From measured component descriptions and a system structure, a CAD tool synthesizes the system properties, predicting how the system will behave. The system is not built, nor are any test executions performed. Neither the component sources nor executables are needed by systems designers. From CAD calculations a designer can learn (approximately) anything that could be learned by testing an actual system implementation. The CAD tool is often more efficient than it would be to assemble and execute an actual system. Using tools that support an ideal separation between component- and system development, experiments were conducted to investigate two related questions: (1) To what extent can unit (that is, component) testing replace system testing? (2) What properties of software and subdomains influence the quality of subdomain testing?",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2088618231",
    "type": "article"
  },
  {
    "title": "Inferring Extended Probabilistic Finite-State Automaton Models from Software Executions",
    "doi": "https://doi.org/10.1145/3196883",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Sepideh Emam; James Miller",
    "corresponding_authors": "",
    "abstract": "Behavioral models are useful tools in understanding how programs work. Although several inference approaches have been introduced to generate extended finite-state automatons from software execution traces, they suffer from accuracy, flexibility, and decidability issues. In this article, we apply a hybrid technique to use both reinforcement learning and stochastic modeling to generate an extended probabilistic finite state automaton from software traces. Our approach—ReHMM (Reinforcement learning-based Hidden Markov Modelling)—is able to address the problems of inflexibility and un-decidability reported in other state-of-the-art approaches. Experimental results indicate that ReHMM outperforms other inference algorithms.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2807405309",
    "type": "article"
  },
  {
    "title": "Refactoring Multi-Level Models",
    "doi": "https://doi.org/10.1145/3280985",
    "publication_date": "2018-10-31",
    "publication_year": 2018,
    "authors": "Juan de Lara; Esther Guerra",
    "corresponding_authors": "",
    "abstract": "Multi-level modelling promotes flexibility in modelling by enabling the use of several meta-levels instead of just two, as is the case in mainstream two-level modelling approaches. While this approach leads to simpler models for some scenarios, it introduces an additional degree of freedom as designers can decide the meta-level where an element should reside, having to ascertain the suitability of such decisions. In this respect, model refactorings have been successfully applied in the context of two-level modelling to rearrange the elements of a model while preserving its meaning. Following this idea, we propose a catalogue of 17 novel refactorings specific to multi-level models. Their objective is to help designers in rearranging elements across and within meta-levels and exploring the consequences. In this article, we detail each refactoring in the catalogue, show a classification across different dimensions, and describe the support we provide in our M eta D epth tool. We present two experiments to assess two aspects of our refactorings. The first one validates the predicted semantic side effects of the refactorings on the basis of more than 210.000 refactoring applications. The second one measures the impact of refactorings on three quality attributes of multi-level models.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2900555028",
    "type": "article"
  },
  {
    "title": "Accounting for defect characteristics in evaluations of testing techniques",
    "doi": "https://doi.org/10.1145/2211616.2211620",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Jaymie Strecker; Atif M. Memon",
    "corresponding_authors": "",
    "abstract": "As new software-testing techniques are developed, before they can achieve widespread acceptance, their effectiveness at detecting defects must be evaluated. The most common way of evaluating testing techniques is with empirical studies, in which one or more techniques are tried out on software with known defects. However, the defects used can affect the performance of the techniques. To complicate matters, it is not even clear how to effectively describe or characterize defects. To address these problems, this article describes an experiment architecture for empirically evaluating testing techniques which takes both defect and test-suite characteristics into account. As proof of concept, an experiment on GUI-testing techniques is conducted. It provides evidence that the defect characteristics proposed do help explain defect detection, at least for GUI testing, and it explores the relationship between the coverage of defective code and the detection of defects.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2005064209",
    "type": "article"
  },
  {
    "title": "A Compiler for Multimodal Scenarios",
    "doi": "https://doi.org/10.1145/2000799.2000804",
    "publication_date": "2011-09-01",
    "publication_year": 2011,
    "authors": "Shahar Maoz; David Harel; Asaf Kleinbort",
    "corresponding_authors": "",
    "abstract": "We exploit the main similarity between the aspect-oriented programming paradigm and the inter-object, scenario-based approach to specification, in order to construct a new way of executing systems based on the latter. Specifically, we transform multimodal scenario-based specifications, given in the visual language of live sequence charts (LSC), into what we call scenario aspects , implemented in AspectJ. Unlike synthesis approaches, which attempt to take the inter-object scenarios and construct intra-object state-based per-object specifications or a single controller automaton, we follow the ideas behind the LSC play-out algorithm to coordinate the simultaneous monitoring and direct execution of the specified scenarios. Thus, the structure of the specification is reflected in the structure of the generated code; the high-level inter-object requirements and their structure are not lost in the translation. The transformation/compilation scheme is fully implemented in a UML2-compliant tool we term the S2A compiler (for Scenarios to Aspects), which provides full code generation of reactive behavior from inter-object multimodal scenarios. S2A supports advanced scenario-based programming features, such as multiple instances and exact and symbolic parameters. We demonstrate our work with an application whose inter-object behaviors are specified using LSCs. We discuss advantages and challenges of the compilation scheme in the context of the more general vision of scenario-based programming.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2040371171",
    "type": "article"
  },
  {
    "title": "Exception handlers for healing component-based systems",
    "doi": "https://doi.org/10.1145/2522920.2522923",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Hervé Chang; Leonardo Mariani; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "To design effective exception handlers, developers must predict at design time the exceptional events that may occur at runtime, and must implement the corresponding handlers on the basis of their predictions. Designing exception handlers for component-based software systems is particularly difficult because the information required to build handlers is distributed between component and application developers. Component developers know the internal details of the components but ignore the applications, while application developers own the applications but cannot access the details required to implement handlers in components. This article addresses the problem of automatically healing the infield failures that are caused by faulty integration of OTS components. In the article, we propose a technique and a methodology to decouple the tasks of component and application developers, who will be able to share information asynchronously and independently, and communicate implicitly by developing and deploying what we call healing connectors. Component developers implement healing connectors on the basis of information about the integration problems frequently experienced by application developers. Application developers easily and safely install healing connectors in their applications without knowing the internal details of the connectors. Healing connectors heal failures activated by exceptions raised in the OTS components actually deployed in the system. The article defines healing connectors, introduces a methodology to develop and deploy healing connectors, and presents several case studies that indicate that healing connectors are effective, reusable and efficient.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2146741138",
    "type": "article"
  },
  {
    "title": "Assessing the Refactoring of Brain Methods",
    "doi": "https://doi.org/10.1145/3191314",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Santiago Vidal; Iñaki berra; Santiago Zulliani; Claudia Marcos; J. Andrés Díaz Pace",
    "corresponding_authors": "",
    "abstract": "Code smells are a popular mechanism for identifying structural design problems in software systems. Several tools have emerged to support the detection of code smells and propose some refactorings. However, existing tools do not guarantee that a smell will be automatically fixed by means of refactorings. This article presents Bandago, an automated approach to fix a specific type of code smell called Brain Method . A Brain Method centralizes the intelligence of a class and manifests itself as a long and complex method that is difficult to understand and maintain by developers. For each Brain Method , Bandago recommends several refactoring solutions to remove the smell using a search strategy based on simulated annealing. Our approach has been evaluated with several open-source Java applications, and the results show that Bandago can automatically fix more than 60% of Brain Methods . Furthermore, we conducted a survey with 35 industrial developers that showed evidence about the usefulness of the refactorings proposed by Bandago. Also, we compared the performance of the Bandago against that of a third-party refactoring tool.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2801360630",
    "type": "article"
  },
  {
    "title": "Oracles for Testing Software Timeliness with Uncertainty",
    "doi": "https://doi.org/10.1145/3280987",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Chunhui Wang; Fabrizio Pastore; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Uncertainty in timing properties (e.g., detection time of external events) is a common occurrence in embedded software systems, since these systems interact with complex physical environments. Such time uncertainty leads to non-determinism. For example, time-triggered operations may either generate different valid outputs across different executions or experience failures (e.g., results not being generated in the expected time window) that occur only occasionally over many executions. For these reasons, time uncertainty makes the generation of effective test oracles for timing requirements a challenging task. To address the above challenge, we propose Stochastic Testing with Unique Input Output Sequences, an approach for the automated generation of stochastic oracles that verify the capability of a software system to fulfill timing constraints in the presence of time uncertainty. Such stochastic oracles entail the statistical analysis of repeated test case executions based on test output probabilities predicted by means of statistical model checking. Results from two industrial case studies in the automotive domain demonstrate that this approach improves the fault detection effectiveness of tests suites derived from timed automata compared to traditional approaches.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2900774402",
    "type": "article"
  },
  {
    "title": "Test Case Prioritization Using Extended Digraphs",
    "doi": "https://doi.org/10.1145/2789209",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Sepideh Emam; James Miller",
    "corresponding_authors": "",
    "abstract": "Although many test case prioritization techniques exist, their performance is far from perfect. Hence, we propose a new fault-based test case prioritization technique to promote fault-revealing test cases in model-based testing (MBT) procedures. We seek to improve the fault detection rate—a measure of how fast a test suite is able to detect faults during testing—in scenarios such as regression testing. We propose an extended digraph model as the basis of this new technique. The model is realized using a novel reinforcement-learning (RL)- and hidden-Markov-model (HMM)-based technique which is able to prioritize test cases for regression testing objectives. We present a method to initialize and train an HMM based upon RL concepts applied to an application's digraph model. The model prioritizes test cases based upon forward probabilities, a new test case prioritization approach. In addition, we also propose an alternative approach to prioritizing test cases according to the amount of change they cause in applications. To evaluate the effectiveness of the proposed techniques, we perform experiments on graphical user interface (GUI)-based applications and compare the results with state-of-the-art test case prioritization approaches. The experimental results show that the proposed technique is able to detect faults early within test runs.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2271850540",
    "type": "article"
  },
  {
    "title": "Automatically Generating SystemC Code from HCSP Formal Models",
    "doi": "https://doi.org/10.1145/3360002",
    "publication_date": "2020-01-30",
    "publication_year": 2020,
    "authors": "Gaogao Yan; Jiao Li; Shuling Wang; Lingtai Wang; Naijun Zhan",
    "corresponding_authors": "",
    "abstract": "In model-driven design of embedded systems, how to generate code from high-level control models seamlessly and correctly is challenging. This is because hybrid systems are involved with continuous evolution, discrete jumps, and the complicated entanglement between them, while code only contains discrete actions. In this article, we investigate the code generation from Hybrid Communicating Sequential Processes (HCSP), a formal hybrid control model, to SystemC. We first introduce the notion of approximate bisimulation as a criterion to check the consistency between two different systems, especially between the original control model and the final generated code. We prove that it is decidable whether two HCSPs are approximately bisimilar in bounded time and unbounded time with some conditions, respectively. For both the cases, we present two sets of rules correspondingly for discretizing HCSPs and prove that the original HCSP model and the corresponding discretization are approximately bisimilar. Furthermore, based on the discretization, we define a transformation function to map a discretized HCSP model to SystemC code such that they are also approximately bisimilar. We finally implement a tool to automatically realize the translation from HCSP to SystemC code and illustrate our approach through some case studies.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3004215995",
    "type": "article"
  },
  {
    "title": "Verification of Distributed Systems via Sequential Emulation",
    "doi": "https://doi.org/10.1145/3490387",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Luca Di Stefano; Rocco De Nicola; Omar Inverso",
    "corresponding_authors": "",
    "abstract": "Sequential emulation is a semantics-based technique to automatically reduce property checking of distributed systems to the analysis of sequential programs. An automated procedure takes as input a formal specification of a distributed system, a property of interest, and the structural operational semantics of the specification language and generates a sequential program whose execution traces emulate the possible evolutions of the considered system. The problem as to whether the property of interest holds for the system can then be expressed either as a reachability or as a termination query on the program. This allows to immediately adapt mature verification techniques developed for general-purpose languages to domain-specific languages, and to effortlessly integrate new techniques as soon as they become available. We test our approach on a selection of concurrent systems originated from different contexts from population protocols to models of flocking behaviour. By combining a comprehensive range of program verification techniques, from traditional symbolic execution to modern inductive-based methods such as property-directed reachability, we are able to draw consistent and correct verification verdicts for the considered systems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4220773921",
    "type": "article"
  },
  {
    "title": "<scp>XCode</scp> : Towards Cross-Language Code Representation with Large-Scale Pre-Training",
    "doi": "https://doi.org/10.1145/3506696",
    "publication_date": "2022-04-09",
    "publication_year": 2022,
    "authors": "Zehao Lin; Guodun Li; Jingfeng Zhang; Yue Deng; Xiangji Zeng; Yin Zhang; Yao Wan",
    "corresponding_authors": "",
    "abstract": "Source code representation learning is the basis of applying artificial intelligence to many software engineering tasks such as code clone detection, algorithm classification, and code summarization. Recently, many works have tried to improve the performance of source code representation from various perspectives, e.g., introducing the structural information of programs into latent representation. However, when dealing with rapidly expanded unlabeled cross-language source code datasets from the Internet, there are still two issues. Firstly, deep learning models for many code-specific tasks still suffer from the lack of high-quality labels. Secondly, the structural differences among programming languages make it more difficult to process multiple languages in a single neural architecture. To address these issues, in this article, we propose a novel Cross -language Code representation with a large-scale pre-training ( XCode ) method. Concretely, we propose to use several abstract syntax trees and ELMo-enhanced variational autoencoders to obtain multiple pre-trained source code language models trained on about 1.5 million code snippets. To fully utilize the knowledge across programming languages, we further propose a Shared Encoder-Decoder (SED) architecture which uses the multi-teacher single-student method to transfer knowledge from the aforementioned pre-trained models to the distilled SED. The pre-trained models and SED will cooperate to better represent the source code. For evaluation, we examine our approach on three typical downstream cross-language tasks, i.e., source code translation, code clone detection, and code-to-code search, on a real-world dataset composed of programming exercises with multiple solutions. Experimental results demonstrate the effectiveness of our proposed approach on cross-language code representations. Meanwhile, our approach performs significantly better than several code representation baselines on different downstream tasks in terms of multiple automatic evaluation metrics.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4226463316",
    "type": "article"
  },
  {
    "title": "On the Discoverability of npm Vulnerabilities in Node.js Projects",
    "doi": "https://doi.org/10.1145/3571848",
    "publication_date": "2022-11-21",
    "publication_year": 2022,
    "authors": "Mahmoud Alfadel; Diego Elias Costa; Emad Shihab; Bram Adams",
    "corresponding_authors": "",
    "abstract": "The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42%) depend on undisclosed vulnerable packages, 206 (4.63%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4309529433",
    "type": "article"
  },
  {
    "title": "Open Source License Inconsistencies on GitHub",
    "doi": "https://doi.org/10.1145/3571852",
    "publication_date": "2022-12-08",
    "publication_year": 2022,
    "authors": "Thomas Wolter; Ann Barcomb; Dirk Riehle; Nikolay Harutyunyan",
    "corresponding_authors": "",
    "abstract": "Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository and the licenses found in the code. We analyzed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately 10% represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not just look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4311938462",
    "type": "article"
  },
  {
    "title": "Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair",
    "doi": "https://doi.org/10.1145/3579637",
    "publication_date": "2023-01-20",
    "publication_year": 2023,
    "authors": "Kui Liu; Jingtang Zhang; Li Li; Anil Koyuncu; Dongsun Kim; Chunpeng Ge; Zhe Liu; Jacques Klein; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar , which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4317536042",
    "type": "article"
  },
  {
    "title": "Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse",
    "doi": "https://doi.org/10.1145/3597206",
    "publication_date": "2023-05-22",
    "publication_year": 2023,
    "authors": "Qing Huang; Dianshu Liao; Zhenchang Xing; Zhengkang Zuo; Changjing Wang; Xin Xia",
    "corresponding_authors": "",
    "abstract": "Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4377238767",
    "type": "article"
  },
  {
    "title": "A First Look at Dark Mode in Real-world Android Apps",
    "doi": "https://doi.org/10.1145/3604607",
    "publication_date": "2023-06-24",
    "publication_year": 2023,
    "authors": "Suyu Ma; Chunyang Chen; Hourieh Khalajzadeh; John Grundy",
    "corresponding_authors": "",
    "abstract": "Android apps often have a “dark mode” option used in low-light situations, for those who find the conventional color palette problematic, or because of personal preferences. Typically developers add a dark mode option for their apps with different backgrounds, text, and sometimes iconic forms. We wanted to understand the actual provision of this dark mode in real-world Android apps through an empirical study of posts from Stack Overflow and real-world Android app analysis. Using these approaches, we identified the aspects of dark mode that developers implemented as well as the key difficulties they experienced in implementing it. We performed a quantitative analysis using open-coding of more than 300 discussion threads to create a taxonomy regarding the aspects discussed by developers with respect to dark mode in Android. Our quantitative analysis of over 6,000 Android apps highlights which dark mode features are typically provided in Android apps and which aspects developers care about during dark mode design. We also examined four app development support tools to see how well they aid Android app development for dark mode. From our analysis, we distilled some key lessons to guide further research and actions in aiding developers with supporting users who require such assistive features. For example, developers should be aware of the potential risks in using unsuitable dark mode design schema and researchers should take dark mode features into consideration when developing app development support tools.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4381890405",
    "type": "article"
  },
  {
    "title": "Framework for SQL Error Message Design: A Data-Driven Approach",
    "doi": "https://doi.org/10.1145/3607180",
    "publication_date": "2023-07-05",
    "publication_year": 2023,
    "authors": "Toni Taipalus; Hilkka Grahn",
    "corresponding_authors": "",
    "abstract": "Software developers use a significant amount of time reading and interpreting error messages. However, error messages have often been based on either anecdotal evidence or expert opinion, disregarding novices, who arguably are the ones who benefit the most from effective error messages. Furthermore, the usability aspects of Structured Query Language (SQL) error messages have not received much scientific attention. In this mixed-methods study, we coded a total of 128 error messages from eight database management systems (DBMS), and using data from 311 participants, analysed 4,796 queries using regression analysis to find out if and how acknowledged error message qualities explain SQL syntax error fixing success rates. Additionally, we performed a conventional content analysis on 1,505 suggestions on how to improve SQL error messages, and based on the analysis, formulated a framework consisting of nine guidelines for SQL error message design. The results indicate that general error message qualities do not necessarily explain query fixing success in the context of SQL syntax errors and that even some novel NewSQL systems fail to account for basic error message design guidelines. The error message design framework and examples of its practical applications shown in this study are applicable in educational contexts as well as by DBMS vendors in understanding novice perspectives in error message design.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4383223779",
    "type": "article"
  },
  {
    "title": "Towards Causal Analysis of Empirical Software Engineering Data: The Impact of Programming Languages on Coding Competitions",
    "doi": "https://doi.org/10.1145/3611667",
    "publication_date": "2023-08-19",
    "publication_year": 2023,
    "authors": "Carlo A. Furia; Richard Torkar; Robert Feldt",
    "corresponding_authors": "",
    "abstract": "There is abundant observational data in the software engineering domain, whereas running large-scale controlled experiments is often practically impossible. Thus, most empirical studies can only report statistical correlations —instead of potentially more insightful and robust causal relations. To support analyzing purely observational data for causal relations and to assess any differences between purely predictive and causal models of the same data, this article discusses some novel techniques based on structural causal models (such as directed acyclic graphs of causal Bayesian networks). Using these techniques, one can rigorously express, and partially validate, causal hypotheses and then use the causal information to guide the construction of a statistical model that captures genuine causal relations—such that correlation does imply causation. We apply these ideas to analyzing public data about programmer performance in Code Jam, a large world-wide coding contest organized by Google every year. Specifically, we look at the impact of different programming languages on a participant’s performance in the contest. While the overall effect associated with programming languages is weak compared to other variables—regardless of whether we consider correlational or causal links—we found considerable differences between a purely associational and a causal analysis of the very same data. The takeaway message is that even an imperfect causal analysis of observational data can help answer the salient research questions more precisely and more robustly than with just purely predictive techniques—where genuine causal effects may be confounded.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386001521",
    "type": "article"
  },
  {
    "title": "On the Caching Schemes to Speed Up Program Reduction",
    "doi": "https://doi.org/10.1145/3617172",
    "publication_date": "2023-09-05",
    "publication_year": 2023,
    "authors": "Yongqiang Tian; Xueyan Zhang; Yiwen Dong; Zhenyang Xu; Mengxiao Zhang; Yu Jiang; Shing-Chi Cheung; C. P. Sun",
    "corresponding_authors": "",
    "abstract": "Program reduction is a highly practical, widely demanded technique to help debug language tools, such as compilers, interpreters and debuggers. Given a program P that exhibits a property ψ, conceptually, program reduction iteratively applies various program transformations to generate a vast number of variants from P by deleting certain tokens and returns the minimal variant preserving ψ as the result. A program reduction process inevitably generates duplicate variants, and the number of them can be significant. Our study reveals that on average 61.8% and 24.3% of the generated variants in two representative program reducers HDD and Perses, respectively, are duplicates. Checking them against ψ is thus redundant and unnecessary, which wastes time and computation resources. Although it seems that simply caching the generated variants can avoid redundant property tests, such a trivial method is impractical in the real world due to the significant memory footprint. Therefore, a memory-efficient caching scheme for program reduction is in great demand. This study is the first effort to conduct a systematic, extensive analysis of memory-efficient caching schemes for program reduction. We first propose to use two well-known compression methods, ZIP and SHA , to compress the generated variants before they are stored in the cache. Furthermore, our keen understanding on the program reduction process motivates us to propose a novel, domain-specific, both memory and computation-efficient caching scheme, R efreshable C ompact C aching ( RCC ). Our key insight is two-fold: ① by leveraging the correlation between variants and the original program P , we losslessly encode each variant into an equivalent , compact , canonical representation; ② periodically, stale cache entries, which will never be accessed, are timely removed to minimize the memory footprint over time. Our extensive evaluation on 31 real-world C compiler bugs demonstrates that caching schemes help avoid issuing redundant queries by 61.8% and 24.3% in HDD and Perses, respectively; correspondingly, the runtime performance is notably boosted by 22.8% and 18.2%. With regard to the memory efficiency, all three methods use less memory than the state-of-the-art string-based scheme STR . Specifically, ZIP and SHA cut down the memory footprint by more than 80% and 90% in both Perses and HDD compared to STR ; moreover, the highly-scalable, domain-specific RCC dominates peer schemes, and outperforms the SHA by 96.4% and 91.74% in HDD and Perses, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386442953",
    "type": "article"
  },
  {
    "title": "Automatically Detecting Incompatible Android APIs",
    "doi": "https://doi.org/10.1145/3624737",
    "publication_date": "2023-09-18",
    "publication_year": 2023,
    "authors": "Pei Liu; Yanjie Zhao; Mattia Fazzini; Haipeng Cai; John Grundy; Li Li",
    "corresponding_authors": "",
    "abstract": "Fragmentation is a serious problem in the Android ecosystem, which is mainly caused by the fast evolution of the system itself and the various system customizations. Many efforts have attempted to mitigate its impact via approaches to automatically pinpointing compatibility issues in Android apps. We conducted a literature review to identify all the currently available approaches to addressing this issue. Within the nine identified approaches, the four issue detection tools and one incompatible API harvesting tool could be successfully executed. We tried to reproduce them based on their original datasets and then empirically compared those approaches against common datasets. Our experimental results show that existing tool capabilities are quite distinct with only a small overlap in the compatibility issues being identified. Moreover, these detection tools commonly detect compatibility issues via two separate steps including incompatible APIs gathering and compatibility issues (induced by the incorrect invocations of the identified incompatible APIs) determination. To help developers better identify compatibility issues in Android apps, we developed a new approach, AndroMevol , to systematically spot incompatible APIs as they play a crucial role in issue detection. AndroMevol was able to pinpoint 397,678 incompatible APIs against the full history of the official Android framework and 52 customized Android frameworks spanning five popular device manufacturers. Our approach could enhance the ability of the state-of-the-art detection tools by identifying many more incompatible APIs that may cause compatibility issues in Android apps and foster more advanced approaches to pinpointing all types of compatibility issues.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386830538",
    "type": "article"
  },
  {
    "title": "Variable-based Fault Localization via Enhanced Decision Tree",
    "doi": "https://doi.org/10.1145/3624741",
    "publication_date": "2023-09-18",
    "publication_year": 2023,
    "authors": "Jiajun Jiang; Yumeng Wang; Junjie Chen; Delin Lv; Mengjiao Liu",
    "corresponding_authors": "",
    "abstract": "Fault localization, aiming at localizing the root cause of the bug under repair, has been a longstanding research topic. Although many approaches have been proposed in past decades, most of the existing studies work at coarse-grained statement or method levels with very limited insights about how to repair the bug ( granularity problem ), but few studies target the finer-grained fault localization. In this article, we target the granularity problem and propose a novel finer-grained variable-level fault localization technique. Specifically, the basic idea of our approach is that fault-relevant variables may exhibit different values in failed and passed test runs, and variables that have higher discrimination ability have a larger possibility to be the root causes of the failure. Based on this, we propose a program-dependency-enhanced decision tree model to boost the identification of fault-relevant variables via discriminating failed and passed test cases based on the variable values. To evaluate the effectiveness of our approach, we have implemented it in a tool called VarDT and conducted an extensive study over the Defects4J benchmark. The results show that VarDT outperforms the state-of-the-art fault localization approaches with at least 268.4% improvement in terms of bugs located at Top-1, and the average improvement is 351.3%. Besides, to investigate whether our finer-grained fault localization result can further improve the effectiveness of downstream APR techniques, we have adapted VarDT to the application of patch filtering, where we use the variables located by VarDT to filter incorrect patches. The results denote that VarDT outperforms the state-of-the-art PATCH-SIM and BATS by filtering 14.8% and 181.8% more incorrect patches, respectively, demonstrating the effectiveness of our approach. It also provides a new way of thinking for improving automatic program repair techniques.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386830539",
    "type": "article"
  },
  {
    "title": "PTM-APIRec: Leveraging Pre-trained Models of Source Code in API Recommendation",
    "doi": "https://doi.org/10.1145/3632745",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Zhihao Li; Chuanyi Li; Ze Tang; Wanhong Huang; Jidong Ge; Bin Luo; Vincent Ng; Ting Wang; Yucheng Hu; Xiaopeng Zhang",
    "corresponding_authors": "",
    "abstract": "Recommending APIs is a practical and essential feature of IDEs. Improving the accuracy of API recommendations is an effective way to improve coding efficiency. With the success of deep learning in software engineering, the state-of-the-art (SOTA) performance of API recommendation is also achieved by deep-learning-based approaches. However, existing SOTAs either only consider the API sequences in the code snippets or rely on complex operations for extracting hand-crafted features, all of which have potential risks in under-encoding the input code snippets and further resulting in sub-optimal recommendation performance. To this end, this article proposes to utilize the code understanding ability of existing general code P re- T raining M odels to fully encode the input code snippet to improve the accuracy of API Rec ommendation, namely, PTM-APIRec . To ensure that the code semantics of the input are fully understood and the API recommended actually exists, we use separate vocabularies for the input code snippet and the APIs to be predicted. The experimental results on the JDK and Android datasets show that PTM-APIRec surpasses existing approaches. Besides, an effective way to improve the performance of PTM-APIRec is to enhance the pre-trained model with more pre-training data (which is easier to obtain than API recommendation datasets).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4388671832",
    "type": "article"
  },
  {
    "title": "Improving Automated Program Repair with Domain Adaptation",
    "doi": "https://doi.org/10.1145/3631972",
    "publication_date": "2023-11-21",
    "publication_year": 2023,
    "authors": "Armin Zirak; Hadi Hemmati",
    "corresponding_authors": "",
    "abstract": "Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies, the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set’s (“Domain Shift”). In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning , TuningWithLightWeightAdapterLayers , and CurriculumLearning and two APR models on 2,672 bugs from 12 projects. The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05% and CodeXGLUE by 48.78%, in terms of “Exact Match”. Through experiments, we also show that the framework provides high efficiency and reliability (in terms of “Exposure Bias”). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76% and 17.62% for TFix and CodeXGLUE, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4388848588",
    "type": "article"
  },
  {
    "title": "Understanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic",
    "doi": "https://doi.org/10.1145/3638244",
    "publication_date": "2023-12-22",
    "publication_year": 2023,
    "authors": "Daniel Russo; Paul H. P. Hanel; Niels van Berkel",
    "corresponding_authors": "",
    "abstract": "The COVID-19 pandemic has brought significant and enduring shifts in various aspects of life, including increased flexibility in work arrangements. In a longitudinal study, spanning 24 months with six measurement points from April 2020 to April 2022, we explore changes in well-being, productivity, social contacts, and needs of software engineers during this time. Our findings indicate systematic changes in various variables. For example, well-being and quality of social contacts increased while emotional loneliness decreased as lockdown measures were relaxed. Conversely, people's boredom and productivity remained stable. Furthermore, a preliminary investigation into the future of work at the end of the pandemic revealed a consensus among developers for a preference of hybrid work arrangements. We also discovered that prior job changes and low job satisfaction were consistently linked to intentions to change jobs if current work conditions do not meet developers' needs. This highlights the need for software organizations to adapt to various work arrangements to remain competitive employers. Building upon our findings and the existing literature, we introduce the Integrated Job Demands-Resources and Self-Determination (IJARS) Model as a comprehensive framework to explain the well-being and productivity of software engineers during the COVID-19 pandemic.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4390105448",
    "type": "article"
  },
  {
    "title": "Patch Correctness Assessment: A Survey",
    "doi": "https://doi.org/10.1145/3702972",
    "publication_date": "2024-11-08",
    "publication_year": 2024,
    "authors": "Zhongxiang Fei; Jidong Ge; Chuanyi Li; Tianqi Wang; Y. Li; H. Zhang; LiGuo Huang; Bin Luo",
    "corresponding_authors": "",
    "abstract": "Most automated program repair methods rely on test cases to determine the correctness of the generated patches. However, due to the incompleteness of available test suites, some patches that pass all the test cases may still be incorrect. This issue is known as the patch overfitting problem. Overfitting problem is a longstanding problem in automated program repair. Due to overfitting patches, the patches obtained by automated program repair tools require further validation to determine their correctness. Researchers have proposed many methods to automatically assess the correctness of patches, but no systematic review provides a detailed introduction to this problem, the existing solutions, and the challenges. To address this deficiency, we systematically review the existing approaches to patch correctness assessment. We first offer a few examples of overfitting patches to acquire a more detailed understanding of this problem. We then propose a comprehensive categorization of publicly available techniques and datasets, examine the commonly used evaluation metrics, and perform an in-depth analysis of the effectiveness of the existing models in addressing the challenge of overfitting. Based on our analysis, we provided the difficulties encountered by current methodologies, alongside the possible avenues for future research exploration.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404183048",
    "type": "article"
  },
  {
    "title": "Generating testing and analysis tools with Aria",
    "doi": "https://doi.org/10.1145/226155.226157",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Prémkumar Dévanbu; David S. Rosenblum; Alexander L. Wolf",
    "corresponding_authors": "",
    "abstract": "Many software testing and analysis tools manipulate graph representations of programs, such as abstract syntax trees or abstract semantics graphs. Handcrafting such tools in conventional programming languages can be difficult, error prone, and time consuming. Our approach is to use application generators targeted for the domain of graph-representation-based testing and analysis tools. Moreover, we generate the generators themselves, so that the development of tools based on different languages and/or representations can also be supported better. In this article we report on our experiences in developing and using a system called Aria that generates testing and analysis tools based on an abstract semantics graph representation for C and C++ called Reprise. Aria itself was generated by the Genoa system. We demonstrate the utility of Aria and, thereby, the power of our approach, by showing Aria's use in the development of a number of useful testing and analysis tools.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1968136518",
    "type": "article"
  },
  {
    "title": "Distributed real-time system specification and verification in APTL",
    "doi": "https://doi.org/10.1145/158431.158434",
    "publication_date": "1993-10-01",
    "publication_year": 1993,
    "authors": "Farn Wang; Aloysius K. Mok; E. Allen Emerson",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a language, Asynchronous Propositional Temporal Logic (APTL), for the specification and verification of distributed hard real-time sytems. APTL extends the logic TPTL by dealing explicitly with multiple local clocks. We propose a distributed-system model which permits definition of inequalities asserting the temporal precedence of local clock readings. We show the expressiveness of APTL through two nontrivial examples. Our logic can be used to specify and reason about such important properties as bounded clock rate drifting. We then give a 2 2 0(n) tableau-based decision procedure for determining APTL satisfiability, where n is the size (number of bits) of the input formula.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2123410838",
    "type": "article"
  },
  {
    "title": "A compiler for analyzing cryptographic protocols using non-interference",
    "doi": null,
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Antonio Durante; Riccardo Focardi; Roberto Gorrieri",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2289364018",
    "type": "article"
  },
  {
    "title": "Coordinating rule-based software processes with ESP",
    "doi": "https://doi.org/10.1145/152388.152393",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Paolo Ciancarini",
    "corresponding_authors": "Paolo Ciancarini",
    "abstract": "ESP is a language for modeling rule-based software processes that take place in a distributed software development environment. It is based on PoliS, an abstract coordination model that relies on Multiple Tuple Spaces, i.e., collections of tuples a la Linda. PoliS extends Linda aiming at the specification and coordination of logically distributed systems. ESP (Extended Shared Prolog) combines the PoliS mechanisms to deal with concurrency and distribution, with the logic-programming language Prolog, to deal with rules and deduction. Such a combination of a coordination model and a logic language provides a powerful framework in which experiments about rule-based software process programming can be performed and evaluated.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2020942621",
    "type": "article"
  },
  {
    "title": "Detection of linear errors via domain testing",
    "doi": "https://doi.org/10.1145/136586.136590",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Steven J. Zeil; Faten H. Afifi; Lee White",
    "corresponding_authors": "",
    "abstract": "Domain testing attempts to find errors in the numeric expressions affecting the flow of control through a program. Intuitively, domain testing provides a systematic form of boundary value testing for the conditional statements within a program. Several forms of domain testing have been proposed, all dealing with the detection of linear errors in linear functions. Perturbation analysis has been previously developed as a measure of the volume of faults, from within a selected space of possible faults, left undetected by a test set. It is adapted here to errors and error spaces. The adapted form is used to show that the different forms of domain testing are closer in error detection ability than had been supposed. They may all be considered effective for finding linear errors in linear predicate functions. A simple extension is proposed, which allows them to detect linear errors in nonlinear predicate functions using only a single additional test point.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2046546418",
    "type": "article"
  },
  {
    "title": "On software component co-installability",
    "doi": "https://doi.org/10.1145/2522920.2522927",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Jérôme Vouillon; Roberto Di Cosmo",
    "corresponding_authors": "",
    "abstract": "Modern software systems are built by composing components drawn from large repositories , whose size and complexity is increasing at a very fast pace. A fundamental challenge for the maintainability and the scalability of such software systems is the ability to quickly identify the components that can or cannot be installed together: this is the co-installability problem, which is related to boolean satisfiability and is known to be algorithmically hard. This article develops a novel theoretical framework, based on formally certified semantic preserving graph-theoretic transformations, that allows us to associate to each concrete component repository a much smaller one with a simpler structure, that we call strongly flat , with equivalent co-installability properties. This flat repository can be displayed in a way that provides a concise view of the co-installability issues in the original repository, or used as a basis for various algorithms related to co-installability, like the efficient computation of strong conflicts between components. The proofs contained in this work have been machine checked using the Coq proof assistant.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2093486322",
    "type": "article"
  },
  {
    "title": "A methodology for testing CPU emulators",
    "doi": "https://doi.org/10.1145/2522920.2522922",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Lorenzo Martignoni; Roberto Paleari; Alessandro Reina; Giampaolo Fresi Roglia; Danilo Bruschi",
    "corresponding_authors": "",
    "abstract": "A CPU emulator is a software system that simulates a hardware CPU. Emulators are widely used by computer scientists for various kind of activities (e.g., debugging, profiling, and malware analysis). Although no theoretical limitation prevents developing an emulator that faithfully emulates a physical CPU, writing a fully featured emulator is a very challenging and error prone task. Modern CISC architectures have a very rich instruction set, some instructions lack proper specifications, and others may have undefined effects in corner cases. This article presents a testing methodology specific for CPU emulators, based on fuzzing. The emulator is “stressed” with specially crafted test cases, to verify whether the CPU is properly emulated or not. Improper behaviors of the emulator are detected by running the same test case concurrently on the emulated and on the physical CPUs and by comparing the state of the two after the execution. Differences in the final state testify defects in the code of the emulator. We implemented this methodology in a prototype (named as EmuFuzzer), analyzed five state-of-the-art IA-32 emulators (QEMU, Valgrind, Pin, BOCHS, and JPC), and found several defects in each of them, some of which can prevent proper execution of programs.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2061290744",
    "type": "article"
  },
  {
    "title": "Scaling predictive analysis of concurrent programs by removing trace redundancy",
    "doi": "https://doi.org/10.1145/2430536.2430542",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Jeff Huang; Jinguo Zhou; Charles Zhang",
    "corresponding_authors": "",
    "abstract": "Predictive trace analysis (PTA) of concurrent programs is powerful in finding concurrency bugs unseen in past program executions. Unfortunately, existing PTA solutions face considerable challenges in scaling to large traces. In this article, we identify that a large percentage of events in the trace are redundant for presenting useful analysis results to the end user. Removing them from the trace can significantly improve the scalability of PTA without affecting the quality of the results. We present a trace redundancy theorem that specifies a redundancy criterion and the soundness guarantee that the PTA results are preserved after removing the redundancy. Based on this criterion, we design and implement TraceFilter, an efficient algorithm that automatically removes redundant events from a trace for the PTA of general concurrency access anomalies. We evaluated TraceFilter on a set of popular concurrent benchmarks as well as real world large server programs. Our experimental results show that TraceFilter is able to significantly improve the scalability of PTA by orders of magnitude, without impairing the analysis result.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1978493229",
    "type": "article"
  },
  {
    "title": "ConMem",
    "doi": "https://doi.org/10.1145/2430545.2430546",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Wei Zhang; Chong Sun; Junghee Lim; Shan Lu; Thomas Reps",
    "corresponding_authors": "",
    "abstract": "Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and nondeterministic interleaving space. In reality, without the resources to thoroughly check the interleaving space, critical concurrency bugs can slip into production versions and cause failures in the field. Approaches to making the best use of the limited resources and exposing severe concurrency bugs before software release would be desirable. Unlike previous work that focuses on bugs caused by specific interleavings (e.g., races and atomicity violations), this article targets concurrency bugs that result in one type of severe effect: program crashes. Our study of the error-propagation process of real-world concurrency bugs reveals a common pattern (50% in our nondeadlock concurrency bug set) that is highly correlated with program crashes. We call this pattern concurrency-memory bugs: buggy interleavings directly cause memory bugs (NULL-pointer-dereferences, dangling-pointers, buffer-overflows, uninitialized-reads) on shared memory objects. Guided by this study, we built ConMem to monitor program execution, analyze memory accesses and synchronizations, and predictively detect these common and severe concurrency-memory bugs. We also built a validator,ConMem-v, to automatically prune false positives by enforcing potential bug-triggering interleavings. We evaluated ConMem using 7 open-source programs with 10 real-world concurrency bugs. ConMem detects more tested bugs (9 out of 10 bugs) than a lock-set-based race detector and an unserializable-interleaving detector, which detect 4 and 6 bugs, respectively, with a false-positive rate about one tenth of the compared tools. ConMem-v further prunes out all the false positives. ConMem has reasonable overhead suitable for development usage.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2117564107",
    "type": "article"
  },
  {
    "title": "Required behavior of sequence diagrams",
    "doi": "https://doi.org/10.1145/2523108",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Lunjin Lu; Dae‐Kyoo Kim",
    "corresponding_authors": "",
    "abstract": "Many reusable software artifacts such as design patterns and design aspects make use of UML sequence diagrams to describe interaction behaviors. When a pattern or an aspect is reused in an application, it is important to ensure that the sequence diagrams for the application conform to the corresponding sequence diagrams for the pattern or aspect. Reasoning about conformance relationship between sequence diagrams has not been addressed adequately in literature. In this article, we focus on required behaviors specified by a UML sequence diagram and provide a semantic-based formalization of conformance relationships between sequence diagrams. A novel trace semantics is first given that captures precisely required behaviors. A refinement relation between sequence diagrams is then defined based on the semantics. The refinement relation allows a sequence diagram to be refined by changing its structure so long as its required behaviors are preserved. A conformance relation between sequence diagrams is finally given that includes the refinement relation as a special case. It allows one to introduce and rename lifelines, messages, and system variables when reusing sequence diagrams. Properties of the semantics, refinement, and conformance relations are studied. Two case studies are provided to illustrate the efficacy of semantic-based conformance reasoning.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2963452129",
    "type": "article"
  },
  {
    "title": "How Understandable Are Pattern-based Behavioral Constraints for Novice Software Designers?",
    "doi": "https://doi.org/10.1145/3306608",
    "publication_date": "2019-02-26",
    "publication_year": 2019,
    "authors": "Christoph Czepa; Uwe Zdun",
    "corresponding_authors": "",
    "abstract": "This article reports a controlled experiment with 116 participants on the understandability of representative graphical and textual pattern-based behavioral constraint representations from the viewpoint of novice software designers. Particularly, graphical and textual behavioral constraint patterns present in the declarative business process language Declare and textual behavioral constraints based on Property Specification Patterns are the subjects of this study. In addition to measuring the understandability construct, this study assesses subjective aspects such as perceived difficulties regarding learning and application of the tested approaches. An interesting finding of this study is the overall low achieved correctness in the experimental tasks, which seems to indicate that pattern-based behavioral constraint representations are hard to understand for novice software designers in the absence of additional supportive measures. The results of the descriptive statistics regarding achieved correctness are slightly in favor of the textual representations, but the inference statistics do not indicate any significant differences in terms of understandability between graphical and textual behavioral constraint representations.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2997300846",
    "type": "article"
  },
  {
    "title": "Computing Alignments of Well-Formed Process Models using Local Search",
    "doi": "https://doi.org/10.1145/3394056",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Farbod Taymouri; Josep Carmona",
    "corresponding_authors": "",
    "abstract": "The alignment of observed and modeled behavior is an essential element for organizations, since it opens the door for conformance checking and enhancement of processes. The state-of-the-art technique for computing alignments has exponential time and space complexity, hindering its applicability for medium and large instances. In this article, a novel approach is presented to tackle the challenge of computing an alignment for large-problem instances that correspond to well-formed process models. Given an observed trace, first it uses a novel replay technique to find an initial candidate trace in the model. Then a local search framework is applied to try to improve the alignment until no further improvement is possible. The implementation of the presented technique reveals a magnificent reduction both in computation time and in memory usage. Moreover, although the proposed technique does not guarantee the derivation of an alignment with minimal cost, the experiments show that in practice the quality of the obtained solutions is close to optimal.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3010475891",
    "type": "article"
  },
  {
    "title": "Automated cookie collection testing",
    "doi": "https://doi.org/10.1145/2559936",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Andrew F. Tappenden; James Miller",
    "corresponding_authors": "",
    "abstract": "Cookies are used by over 80% of Web applications utilizing dynamic Web application frameworks. Applications deploying cookies must be rigorously verified to ensure that the application is robust and secure. Given the intense time-to-market pressures faced by modern Web applications, testing strategies that are low cost and automatable are required. Automated Cookie Collection Testing (CCT) is presented, and is empirically demonstrated to be a low-cost and highly effective automated testing solution for modern Web applications. Automatable test oracles and evaluation metrics specifically designed for Web applications are presented, and are shown to be significant diagnostic tests. Automated CCT is shown to detect faults within five real-world Web applications. A case study of over 580 test results for a single application is presented demonstrating that automated CCT is an effective testing strategy. Moreover, CCT is found to detect security bugs in a Web application released into full production.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1977244105",
    "type": "article"
  },
  {
    "title": "Architecture-Level Configuration of Large-Scale Embedded Software Systems",
    "doi": "https://doi.org/10.1145/2581376",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Razieh Behjati; Shiva Nejati; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2003519084",
    "type": "article"
  },
  {
    "title": "A Defect Estimator for Source Code",
    "doi": "https://doi.org/10.1145/3384517",
    "publication_date": "2020-04-29",
    "publication_year": 2020,
    "authors": "Ritu Kapur; Balwinder Sodhi",
    "corresponding_authors": "",
    "abstract": "An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost. We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PRO gramming CON struct (PROCON) metrics, and (ii) a novel M achine- L earning (ML)-based system, called D efect E stimator for S ource Co de (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python. The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3021986703",
    "type": "article"
  },
  {
    "title": "Uncertainty-wise Requirements Prioritization with Search",
    "doi": "https://doi.org/10.1145/3408301",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Huihui Zhang; Man Zhang; Tao Yue; Shaukat Ali; Yan Li",
    "corresponding_authors": "",
    "abstract": "Requirements review is an effective technique to ensure the quality of requirements in practice, especially in safety-critical domains (e.g., avionics systems, automotive systems). In such contexts, a typical requirements review process often prioritizes requirements, due to limited time and monetary budget, by, for instance, prioritizing requirements with higher implementation cost earlier in the review process. However, such a requirement implementation cost is typically estimated by stakeholders who often lack knowledge about (future) requirements implementation scenarios, which leads to uncertainty in cost overrun. In this article, we explicitly consider such uncertainty (quantified as cost overrun probability) when prioritizing requirements based on the assumption that a requirement with higher importance, a higher number of dependencies to other requirements, and higher implementation cost will be reviewed with the higher priority. Motivated by this, we formulate four objectives for uncertainty-wise requirements prioritization: maximizing the importance of requirements, requirements dependencies, the implementation cost of requirements, and cost overrun probability. These four objectives are integrated as part of our search-based uncertainty-wise requirements prioritization approach with tool support, named as URP. We evaluated six Multi-Objective Search Algorithms (MOSAs) (i.e., NSGA-II, NSGA-III, MOCell, SPEA2, IBEA, and PAES ) together with Random Search ( RS ) using three real-world datasets (i.e., the RALIC, Word, and ReleasePlanner datasets) and 19 synthetic optimization problems. Results show that all the selected MOSAs can solve the requirements prioritization problem with significantly better performance than RS . Among them, IBEA was over 40% better than RS in terms of permutation effectiveness for the first 10% of prioritized requirements in the prioritization sequence of all three datasets. In addition, IBEA achieved the best performance in terms of the convergence of solutions, and NSGA-III performed the best when considering both the convergence and diversity of nondominated solutions.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3113500266",
    "type": "article"
  },
  {
    "title": "Taming Reflection",
    "doi": "https://doi.org/10.1145/3440033",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Xiaoyu Sun; Li Li; Tegawendé F. Bissyandé; Jacques Klein; Damien Octeau; John G. Grundy",
    "corresponding_authors": "",
    "abstract": "Android developers heavily use reflection in their apps for legitimate reasons. However, reflection is also significantly used for hiding malicious actions. Unfortunately, current state-of-the-art static analysis tools for Android are challenged by the presence of reflective calls, which they usually ignore. Thus, the results of their security analysis, e.g., for private data leaks, are incomplete, given the measures taken by malware writers to elude static detection. We propose a new instrumentation-based approach to address this issue in a non-invasive way. Specifically, we introduce to the community a prototype tool called DroidRA, which reduces the resolution of reflective calls to a composite constant propagation problem and then leverages the COAL solver to infer the values of reflection targets. After that, it automatically instruments the app to replace reflective calls with their corresponding Java calls in a traditional paradigm. Our approach augments an app so that it can be more effectively statically analyzable, including by such static analyzers that are not reflection-aware. We evaluate DroidRA on benchmark apps as well as on real-world apps, and we demonstrate that it can indeed infer the target values of reflective calls and subsequently allow state-of-the-art tools to provide more sound and complete analysis results.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3161202781",
    "type": "article"
  },
  {
    "title": "Leveraging Control Flow Knowledge in SMT Solving of Program Verification",
    "doi": "https://doi.org/10.1145/3446211",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Jianhui Chen; Fei He",
    "corresponding_authors": "",
    "abstract": "Satisfiability modulo theories (SMT) solvers have been widely applied as the reasoning engine for diverse software analysis and verification technologies. The efficiency of the SMT solver has significant effects on the performance of these technologies. However, current SMT solvers are designed for the general purpose of constraint solving. Lots of useful knowledge of programs cannot be utilized during SMT solving. As a result, the SMT solver may spend much effort to explore redundant search space. In this article, we propose a novel approach to utilizing control-flow knowledge in SMT solving. With this technique, the search space can be considerably reduced, and the efficiency of SMT solving is observably improved. We conducted extensive experiments on credible benchmarks. The results show significant improvements of our approach.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3163732942",
    "type": "article"
  },
  {
    "title": "Eagle",
    "doi": "https://doi.org/10.1145/3450492",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Jingbo Lu; Dongjie He; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Object sensitivity is widely used as a context abstraction for computing the points-to information context-sensitively for object-oriented programming languages such as Java. Due to the combinatorial explosion of contexts in large object-oriented programs, k -object-sensitive pointer analysis (under k -limiting), denoted k -obj , is often inefficient even when it is scalable for small values of k , where k ⩽ 2 holds typically. A recent popular approach for accelerating k -obj trades precision for efficiency by instructing k -obj to analyze only some methods in a program context-sensitively, determined heuristically by a pre-analysis. In this article, we investigate how to develop a fundamentally different approach, Eagle , for designing a pre-analysis that can make k -obj run significantly faster while maintaining its precision. The novelty of Eagle is to enable k -obj to analyze a method with partial context sensitivity (i.e., context-sensitively for only some of its selected variables/allocation sites) by solving a context-free-language (CFL) reachability problem based on a new CFL-reachability formulation of k -obj . By regularizing one CFL for specifying field accesses and using another CFL for specifying method calls, we have formulated Eagle as a fully context-sensitive taint analysis (without k -limiting) that is both effective (by selecting the variables/allocation sites to be analyzed by k -obj context-insensitively so as to reduce the number of context-sensitive facts inferred by k -obj in the program) and efficient (by running linearly in terms of the number of pointer assignment edges in the program). As Eagle represents the first precision-preserving pre-analysis, our evaluation focuses on demonstrating its significant performance benefits in accelerating k -obj for a set of popular Java benchmarks and applications, with call graph construction, may-fail-casting, and polymorphic call detection as three important client analyses.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3185514416",
    "type": "article"
  },
  {
    "title": "Automatic API Usage Scenario Documentation from Technical Q&amp;A Sites",
    "doi": "https://doi.org/10.1145/3439769",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Gias Uddin; Foutse Khomh; Chanchal K. Roy",
    "corresponding_authors": "",
    "abstract": "The online technical Q&amp;A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3159568147",
    "type": "article"
  },
  {
    "title": "Fold2Vec: Towards a Statement-Based Representation of Code for Code Comprehension",
    "doi": "https://doi.org/10.1145/3514232",
    "publication_date": "2022-04-06",
    "publication_year": 2022,
    "authors": "Francesco Bertolotti; Walter Cazzola",
    "corresponding_authors": "",
    "abstract": "We introduce a novel approach to source code representation to be used in combination with neural networks. Such a representation is designed to permit the production of a continuous vector for each code statement. In particular, we present how the representation is produced in the case of Java source code. We test our representation for three tasks: code summarization , statement separation , and code search . We compare with the state-of-the-art non-autoregressive and end-to-end models for these tasks. We conclude that all tasks benefit from the proposed representation to boost their performance in terms of F1-score, accuracy, and mean reciprocal rank, respectively. Moreover, we show how models trained on code summarization and models trained on statement separation can be combined to address methods with tangled responsibilities, meaning that these models can be used to detect code misconduct.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4225809646",
    "type": "article"
  },
  {
    "title": "Retrieving API Knowledge from Tutorials and Stack Overflow Based on Natural Language Queries",
    "doi": "https://doi.org/10.1145/3565799",
    "publication_date": "2022-10-07",
    "publication_year": 2022,
    "authors": "Di Wu; Xiao‐Yuan Jing; Hongyu Zhang; Yang Feng; Haowen Chen; Yuming Zhou; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "When encountering unfamiliar APIs, developers tend to seek help from API tutorials and Stack Overflow (SO). API tutorials help developers understand the API knowledge in a general context, while SO often explains the API knowledge in a specific programming task. Thus, tutorials and SO posts together can provide more API knowledge. However, it is non-trivial to retrieve API knowledge from both API tutorials and SO posts based on natural language queries. Two major problems are irrelevant API knowledge in two different resources and the lexical gap between the queries and documents. In this article, we regard a fragment in tutorials and a Question and Answering (Q&amp;A) pair in SO as a knowledge item (KI). We generate ⟨ API, FRA ⟩ pairs (FRA stands for fragment) from tutorial fragments and APIs and build ⟨ API, QA ⟩ pairs based on heuristic rules of SO posts. We fuse ⟨ API, FRA ⟩ pairs and ⟨ API, QA ⟩ pairs to generate API knowledge (AK for short) datasets, where each data item is an ⟨ API, KI ⟩ pair. We propose a novel approach, called PLAN, to automatically retrieve API knowledge from both API tutorials and SO posts based on natural language queries. PLAN contains three main stages: (1) API knowledge modeling, (2) query mapping, and (3) API knowledge retrieving. It first utilizes a deep-transfer-metric-learning-based relevance identification (DTML) model to effectively find relevant ⟨ API, KI ⟩ pairs containing two different knowledge items (⟨ API, QA ⟩ pairs and ⟨ API, FRA ⟩ pairs) simultaneously. Then, PLAN generates several potential APIs as a way to reduce the lexical gap between the query and ⟨ API, KI ⟩ pairs. According to potential APIs, we can select relevant ⟨ API, KI ⟩ pairs to generate potential results. Finally, PLAN returns a list of ranked ⟨ API, KI ⟩ pairs that are related to the query. We evaluate the effectiveness of PLAN with 270 queries on Java and Android AK datasets containing 10,072 ⟨ API, KI ⟩ pairs. Our experimental results show that PLAN is effective and outperforms the state-of-the-art approaches. Our user study further confirms the effectiveness of PLAN in locating useful API knowledge.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4303427119",
    "type": "article"
  },
  {
    "title": "Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems",
    "doi": "https://doi.org/10.1145/3569935",
    "publication_date": "2022-10-29",
    "publication_year": 2022,
    "authors": "Hazem Fahmy; Fabrizio Pastore; Lionel Briand; Thomas Stifter",
    "corresponding_authors": "",
    "abstract": "When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error-prone. To support such safety analysis practices, we propose SEDE, a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators towards the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN. With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4307811455",
    "type": "article"
  },
  {
    "title": "What is the intended usage context of this model? - An exploratory study of pre-trained models on various model repositories",
    "doi": "https://doi.org/10.1145/3569934",
    "publication_date": "2022-10-29",
    "publication_year": 2022,
    "authors": "Lina Gong; Jingxuan Zhang; Mingqiang Wei; Haoxiang Zhang; Zhiqiu Huang",
    "corresponding_authors": "",
    "abstract": "There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4307812665",
    "type": "article"
  },
  {
    "title": "Fuzzing Configurations of Program Options",
    "doi": "https://doi.org/10.1145/3580597",
    "publication_date": "2023-02-09",
    "publication_year": 2023,
    "authors": "Zenong Zhang; George Klees; Eric Wang; Michael Hicks; Shiyi Wei",
    "corresponding_authors": "",
    "abstract": "While many real-world programs are shipped with configurations to enable/disable functionalities, fuzzers have mostly been applied to test single configurations of these programs. In this work, we first conduct an empirical study to understand how program configurations affect fuzzing performance. We find that limiting a campaign to a single configuration can result in failing to cover a significant amount of code. We also observe that different program configurations contribute differing amounts of code coverage, challenging the idea that each one can be efficiently fuzzed individually. Motivated by these two observations, we propose ConfigFuzz , which can fuzz configurations along with normal inputs. ConfigFuzz transforms the target program to encode its program options within part of the fuzzable input, so existing fuzzers’ mutation operators can be reused to fuzz program configurations. We instantiate ConfigFuzz on six configurable, common fuzzing targets, and integrate their executions in FuzzBench. In our evaluation, ConfigFuzz outperforms two baseline fuzzers in four targets, while the results are mixed in the other targets due to program size and configuration space. We also analyze the options fuzzed by ConfigFuzz and how they affect the performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4319736431",
    "type": "article"
  },
  {
    "title": "Extraction of Phrase-based Concepts in Vulnerability Descriptions through Unsupervised Labeling",
    "doi": "https://doi.org/10.1145/3579638",
    "publication_date": "2023-02-09",
    "publication_year": 2023,
    "authors": "Sofonias Yitagesu; Zhenchang Xing; Xiaowang Zhang; Zhiyong Feng; Xiaohong Li; Linyi Han",
    "corresponding_authors": "",
    "abstract": "Software vulnerabilities, once disclosed, can be documented in vulnerability databases, which have great potential to advance vulnerability analysis and security research. People describe the key characteristics of software vulnerabilities in natural language mixed with domain-specific names and concepts. This textual nature poses a significant challenge for the automatic analysis of vulnerability knowledge embedded in text. Automatic extraction of key vulnerability aspects is highly desirable but demands significant effort to manually label data for model training. In this article, we propose unsupervised methods to label and extract important vulnerability concepts in textual vulnerability descriptions (TVDs). We focus on six types of phrase-based vulnerability concepts (vulnerability type, vulnerable component, root cause, attacker type, impact, and attack vector) as they are much more difficult to label and extract than name- or number-based entities (i.e., vendor, product, and version). Our approach is based on a key observation that the same-type of phrases, no matter how they differ in sentence structures and phrase expressions, usually share syntactically similar paths in the sentence parsing trees. Specifically, we present a source-target neural architecture that learns the Part-of-Speech (POS) tagging to identify a token’s functional role within TVDs, where the source neural model is trained to capture common features found in the TVD corpus, and the target model is trained to identify linguistically malformed words specific to the security domain. Our evaluation confirms that the proposed tagger outperforms (4.45%–5.98%) the taggers designed on natural language notions and identifies a broad set of TVDs and natural language contents. Then, based on the key observations, we propose two path representations (absolute paths and relative paths) and use an auto-encoder to encode such syntactic similarities. To address the discrete nature of our paths, we enhance the traditional Variational Auto-encoder (VAE) with Gumble-Max trick for categorical data distribution and thus create a Categorical VAE (CaVAE). In the latent space of absolute and relative paths, we further apply unsupervised clustering techniques to generate clusters of the same-type of concepts. Our evaluation confirms the effectiveness of our CaVAE, which achieves a small (85.85) log-likelihood for encoding path representations and the accuracy (83%–89%) of vulnerability concepts in the resulting clusters. The resulting clusters accurately label six types of vulnerability concepts from a TVD corpus in an unsupervised way. Furthermore, these labeled vulnerability concepts can be mapped back to the corresponding phrases in the original TVDs, which produce labels of six types of vulnerability concepts. The resulting labeled TVDs can be used to train concept extraction models for other TVD corpora. In this work, we present two concept extraction methods (concept classification and sequence labeling model) to demonstrate the utility of the unsupervisedly labeled concepts. Our study shows that models trained with our unsupervisedly labeled vulnerability concepts outperform (3.9%–5.14%) those trained with the two manually labeled TVD datasets from previous work due to the consistent boundary and typing by our unsupervised labeling method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4319736724",
    "type": "article"
  },
  {
    "title": "PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing",
    "doi": "https://doi.org/10.1145/3591870",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Yuheng Huang; Lei Ma; Yuanchun Li",
    "corresponding_authors": "",
    "abstract": "Vision Transformer (ViT) is known to be highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial patch perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g. 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes (up to 25%) by simply changing the masking strategy.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4362721716",
    "type": "article"
  },
  {
    "title": "Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective",
    "doi": "https://doi.org/10.1145/3597204",
    "publication_date": "2023-05-13",
    "publication_year": 2023,
    "authors": "Xuanzhe Liu; Diandian Gu; Zhenpeng Chen; Jinfeng Wen; Zili Zhang; Yun Ma; Haoyu Wang; Xin Jin",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) has become a key component of modern software. In the “ big model ” era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training , or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers’ issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers’ issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4376505605",
    "type": "article"
  },
  {
    "title": "An Interleaving Guided Metamorphic Testing Approach for Concurrent Programs",
    "doi": "https://doi.org/10.1145/3607182",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Chang‐ai Sun; Hepeng Dai; Ning Geng; Huai Liu; Tsong Yueh Chen; Peng Wu; Yan Cai; Jinqiu Wang",
    "corresponding_authors": "",
    "abstract": "Concurrent programs are normally composed of multiple concurrent threads sharing memory space. These threads are often interleaved, which may lead to some non-determinism in execution results, even for the same program input. This poses huge challenges to the testing of concurrent programs, especially on the test result verification—that is, the prevalent existence of the oracle problem. In this article, we investigate the application of metamorphic testing (MT), a mainstream technique to address the oracle problem, into the testing of concurrent programs. Based on the unique features of interleaved executions in concurrent programming, we propose an extended notion of metamorphic relations, the core part of MT, which are particularly designed for the testing of concurrent programs. A comprehensive testing approach, namely ConMT , is thus developed and a tool is built to automate its implementation on concurrent programs written in Java. Empirical studies have been conducted to evaluate the performance of ConMT, and the experimental results show that in addition to addressing the oracle problem, ConMT outperforms the baseline traditional testing techniques with respect to a higher degree of automation, better bug detection capability, and shorter testing time. It is clear that ConMT can significantly improve the cost-effectiveness for the testing of concurrent programs and thus advances the state of the art in the field. The study also brings novelty into MT, hence promoting the fundamental research of software testing.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4382987235",
    "type": "article"
  },
  {
    "title": "A Systematic Review of Automated Query Reformulations in Source Code Search",
    "doi": "https://doi.org/10.1145/3607179",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Mohammad Masudur Rahman; Chanchal K. Roy",
    "corresponding_authors": "",
    "abstract": "Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4383067302",
    "type": "review"
  },
  {
    "title": "Poracle: Testing Patches under Preservation Conditions to Combat the Overfitting Problem of Program Repair",
    "doi": "https://doi.org/10.1145/3625293",
    "publication_date": "2023-09-26",
    "publication_year": 2023,
    "authors": "Elkhan Ismayilzada; Md Mazba Ur Rahman; Dongsun Kim; Jooyong Yi",
    "corresponding_authors": "",
    "abstract": "To date, the users of test-driven program repair tools suffer from the overfitting problem; a generated patch may pass all available tests without being correct. In the existing work, users are treated as merely passive consumers of the tests. However, what if they are willing to modify the test to better assess the patches obtained from a repair tool? In this work, we propose a novel semi-automatic patch-classification methodology named Poracle . Our key contributions are three-fold. First, we design a novel lightweight specification method that reuses the existing test. Specifically, the users extend the existing failing test with a preservation condition —the condition under which the patched and pre-patched versions should produce the same output. Second, we develop a fuzzer that performs differential fuzzing with a test containing a preservation condition. Once we find an input that satisfies a specified preservation condition but produces different outputs between the patched and pre-patched versions, we classify the patch as incorrect with high confidence. We show that our approach is more effective than the four state-of-the-art patch classification approaches. Last, we show through a user study that the users find our semi-automatic patch assessment method more effective and preferable than the manual assessment.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4387055370",
    "type": "article"
  },
  {
    "title": "ALL: Supporting Experiential Accessibility Education and Inclusive Software Development",
    "doi": "https://doi.org/10.1145/3625292",
    "publication_date": "2023-09-26",
    "publication_year": 2023,
    "authors": "Weishi Shi; Heather Moses; Qi Yu; Samuel A. Malachowsky; Daniel E. Krutz",
    "corresponding_authors": "",
    "abstract": "Creating accessible software is imperative for making software inclusive for all users.Unfortunately, the topic of accessibility is frequently excluded from computing education, leading to scenarios where students are unaware of either how to develop accessible software or see the need to create it. To address this challenge, we have created a set of educational labs that are systematically designed to not only inform students about fundamental topics in producing accessible software but also demonstrate its importance. Over the previous year, these labs were included in several Computer Science 2 offerings at the Rochester Institute of Technology, comprising a total of 500 student participants. This article discusses instructional observations from these offerings, some of which include the following: (i) many of the research findings from previous efforts remain true with the larger, more diverse evaluation; (ii) our created material and format reduced students’ belief that creating accessible software was difficult in relation to the baseline,; (iii) we observed that our created material and format benefited student opinion that creating accessible software is important, and (iv) computing majors may not be uniformly impacted by experiential educational accessibility material. The educational labs are publicly available on the project website (https://all.rit.edu).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4387055376",
    "type": "article"
  },
  {
    "title": "CLFuzz: Vulnerability Detection of Cryptographic Algorithm Implementation via Semantic-aware Fuzzing",
    "doi": "https://doi.org/10.1145/3628160",
    "publication_date": "2023-10-16",
    "publication_year": 2023,
    "authors": "Yuanhang Zhou; Fuchen Ma; Yuanliang Chen; Meng Ren; Yu Jiang",
    "corresponding_authors": "",
    "abstract": "Cryptography is a core component of many security applications, and flaws hidden in its implementation will affect the functional integrity or, more severely, pose threats to data security. Hence, guaranteeing the correctness of the implementation is important. However, the semantic characteristics (e.g., diverse input data and complex functional transformation) challenge those traditional program validation techniques (e.g., static analysis and dynamic fuzzing). In this article, we propose CLFuzz, a semantic-aware fuzzer for the vulnerability detection of cryptographic algorithm implementation. CLFuzz first extracts the semantic information of targeted algorithms including their cryptographic-specific constraints and function signatures. Based on them, CLFuzz generates high-quality input data adaptively to trigger error-prone situations efficiently. Furthermore, CLFuzz applies innovative logical cross-check that strengthens the logical bug detection ability. We evaluate CLFuzz on the widely used implementations of 54 cryptographic algorithms. It outperforms state-of-the-art cryptographic fuzzing tools. For example, compared with Cryptofuzz, it achieves a coverage speedup of 3.4× and increases the final coverage by 14.4%. Furthermore, CLFuzz has detected 12 previously unknown implementation bugs in 8 cryptographic algorithms (e.g., CMAC in OpenSSL and Message Digest in SymCrypt), most of which are security-critical and have been successfully collected in the national vulnerability database (7 in NVD/CNVD) and is awarded by the Microsoft bounty program (2 for $1,000).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4387664578",
    "type": "article"
  },
  {
    "title": "Deep Is Better? An Empirical Comparison of Information Retrieval and Deep Learning Approaches to Code Summarization",
    "doi": "https://doi.org/10.1145/3631975",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Tingwei Zhu; Zhong Li; Minxue Pan; Chaoxuan Shi; Tian Zhang; Yu Pei; Xuandong Li",
    "corresponding_authors": "",
    "abstract": "Code summarization aims to generate short functional descriptions for source code to facilitate code comprehension. While Information Retrieval (IR) approaches that leverage similar code snippets and corresponding summaries have led the early research, Deep Learning (DL) approaches that use neural models to capture statistical properties between code and summaries are now mainstream. Although some preliminary studies suggest that IR approaches are more effective in some cases, it is currently unclear how effective the existing approaches can be in general, where and why IR/DL approaches perform better, and whether the integration of IR and DL can achieve better performance. Consequently, there is an urgent need for a comprehensive study of the IR and DL code summarization approaches to provide guidance for future development in this area. This article presents the first large-scale empirical study of 18 IR, DL, and hybrid code summarization approaches on five benchmark datasets. We extensively compare different types of approaches using automatic metrics, we conduct quantitative and qualitative analyses of where and why IR and DL approaches perform better, respectively, and we also study hybrid approaches for assessing the effectiveness of integrating IR and DL. The study shows that the performance of IR approaches should not be underestimated, that while DL models perform better in predicting tokens from method signatures and capturing structural similarities in code, simple IR approaches tend to perform better in the presence of code with high similarity or long reference summaries, and that existing hybrid approaches do not perform as well as individual approaches in their respective areas of strength. Based on our findings, we discuss future research directions for better code summarization.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4388412504",
    "type": "article"
  },
  {
    "title": "Do Code Summarization Models Process Too Much Information? Function Signature May Be All What Is Needed",
    "doi": "https://doi.org/10.1145/3652156",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Xi Ding; Rui Peng; Xiangping Chen; Yuan Huang; Jing Bian; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "With the fast development of large software projects, automatic code summarization techniques, which summarize the main functionalities of a piece of code using natural languages as comments, play essential roles in helping developers understand and maintain large software projects. Many research efforts have been devoted to building automatic code summarization approaches. Typical code summarization approaches are based on deep learning models. They transform the task into a sequence-to-sequence task, which inputs source code and outputs summarizations in natural languages. All code summarization models impose different input size limits, such as 50 to 10,000, for the input source code. However, how the input size limit affects the performance of code summarization models still remains under-explored. In this article, we first conduct an empirical study to investigate the impacts of different input size limits on the quality of generated code comments. To our surprise, experiments on multiple models and datasets reveal that setting a low input size limit, such as 20, does not necessarily reduce the quality of generated comments. Based on this finding, we further propose to use function signatures instead of full source code to summarize the main functionalities first and then input the function signatures into code summarization models. Experiments and statistical results show that inputs with signatures are, on average, more than 2 percentage points better than inputs without signatures and thus demonstrate the effectiveness of involving function signatures in code summarization. We also invite programmers to do a questionnaire to evaluate the quality of code summaries generated by two inputs with different truncation levels. The results show that function signatures generate, on average, 9.2% more high-quality comments than full code.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392806399",
    "type": "article"
  },
  {
    "title": "<scp>DiPri</scp> : Distance-based Seed Prioritization for Greybox Fuzzing",
    "doi": "https://doi.org/10.1145/3654440",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Ruixiang Qian; Quanjun Zhang; Chunrong Fang; Ding Yang; Shun Li; Bohao Li; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Greybox fuzzing is a powerful testing technique. Given a set of initial seeds, greybox fuzzing continuously generates new test inputs to execute the program under test and drives executions with code coverage as feedback. Seed prioritization is an important step of greybox fuzzing that helps greybox fuzzing choose promising seeds for input generation in priority. However, mainstream greybox fuzzers like AFL++ and Zest tend to neglect the importance of seed prioritization. They may pick seeds plainly according to the sequential order of the seeds being queued or an order produced with a random-based approach, which may consequently degrade their performance in exploring code and exposing bugs. In the meantime, existing state-of-the-art techniques like Alphuzz and K-Scheduler adopt complex strategies to schedule seeds. Although powerful, such strategies also inevitably incur great overhead and will reduce the scalability of the proposed technique. In this paper, we propose a novel distance-based seed prioritization approach named DiPri to facilitate greybox fuzzing. Specifically, DiPri evaluates the queued seeds according to seed distances and chooses the outlier ones, which are the farthest from the others, in priority to improve the probabilities of discovering previously unexplored code regions. To make a profound evaluation of DiPri , we prototype DiPri on AFL++ and conduct large-scale experiments with four baselines and 24 C/C++ fuzz targets, where eight are from widely adopted real-world projects, eight are from the coverage-based benchmark FuzzBench, and eight are from the bug-based benchmark Magma. The results obtained through a fuzzing exceeding 50,000 CPU hours suggest that DiPri can (1) insignificantly influence the host fuzzer’s capability of code coverage by slightly improving the branch coverage on the eight targets from real-world projects and slightly reducing the branch coverage on the eight targets from FuzzBench, and (2) improve the host fuzzer’s capability of finding bugs by triggering five more Magma bugs. Besides the evaluation with the three C/C++ benchmarks, we integrate DiPri into the Java fuzzer Zest and conduct experiments on a Java benchmark composed of five real-world programs for more than 8,000 CPU hours to empirically study the scalability of DiPri . The results with the Java benchmark demonstrate that DiPri is pretty scalable and can help the host fuzzer find bugs more consistently.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4393192468",
    "type": "article"
  },
  {
    "title": "Fairness Testing of Machine Translation Systems",
    "doi": "https://doi.org/10.1145/3664608",
    "publication_date": "2024-05-09",
    "publication_year": 2024,
    "authors": "Zeyu Sun; Zhenpeng Chen; Jie M. Zhang; Dan Hao",
    "corresponding_authors": "",
    "abstract": "Machine translation is integral to international communication and extensively employed in diverse human-related applications. Despite remarkable progress, fairness issues persist within current machine translation systems. In this article, we propose FairMT, an automated fairness testing approach tailored for machine translation systems. FairMT operates on the assumption that translations of semantically similar sentences, containing protected attributes from distinct demographic groups, should maintain comparable meanings. It comprises three key steps: (1) test input generation, producing inputs covering various demographic groups; (2) test oracle generation, identifying potential unfair translations based on semantic similarity measurements; and (3) regression, discerning genuine fairness issues from those caused by low-quality translation. Leveraging FairMT, we conduct an empirical study on three leading machine translation systems–Google Translate, T5, and Transformer. Our investigation uncovers up to 832, 1,984, and 2,627 unfair translations across the three systems, respectively. Intriguingly, we observe that fair translations tend to exhibit superior translation performance, challenging the conventional wisdom of a fairness-performance tradeoff prevalent in the fairness literature.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396762170",
    "type": "article"
  },
  {
    "title": "LLMEffiChecker:Understanding and Testing Efficiency Degradation of Large Language Models",
    "doi": "https://doi.org/10.1145/3664812",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "X.L. Feng; Xiaohong Han; S. H. Chen; Wei Yang",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of LLMs, which is of paramount importance due to often vast generation demands and real-time requirements, has surprisingly received little attention. In this article, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art LLMs. By analyzing the working mechanism and implementation of 20,543 public-accessible LLMs, we observe a fundamental property in LLMs that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of LLMs instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime-generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that LLMs would have to go through enough iterations to satisfy the pre-configured threshold. We present LLMEffiChecker , which can work under both white-box setting and black-box setting. In the white-box scenario, LLMEffiChecker develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level. In the black-box scenario, LLMEffiChecker employs a causal inference-based approach to find critical tokens and similarly applies three levels of imperceptible perturbation to them. Both the white-box and black-box settings effectively delay the appearance of EOS, compelling these inputs to reach the naturally unreachable threshold. To demonstrate the effectiveness of LLMEffiChecker , we conduct a systematic evaluation on nine publicly available LLMs: Google T5, AllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL translator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT, and Salesforce CodeGen. Experimental results show that LLMEffiChecker can increase on average LLMs’ response latency and energy consumption by 325% to 3,244% and 344% to 3,616%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by LLMEffiChecker significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396861092",
    "type": "article"
  },
  {
    "title": "MET-MAPF: A Metamorphic Testing Approach for Multi-Agent Path Finding Algorithms",
    "doi": "https://doi.org/10.1145/3669663",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Xiao-Yi Zhang; Yang Liu; Paolo Arcaini; Mingyue Jiang; Zheng Zheng",
    "corresponding_authors": "",
    "abstract": "The Multi-Agent Path Finding (MAPF) problem, i.e., the scheduling of multiple agents to reach their destinations, has been widely investigated. Testing MAPF systems is challenging, due to the complexity and variety of scenarios and the agents’ distribution and interaction. Moreover, MAPF testing suffers from the oracle problem, i.e., it is not always clear whether a test shows a failure or not. Indeed, only considering whether the agents reach their destinations without collision is not sufficient. Other properties related to the “quality” of the generated paths should be assessed, e.g., an agent should not follow an unnecessarily long path. To tackle this issue, this paper proposes MET-MAPF, a Metamorphic Testing approach for MAPF systems. We identified ten Metamorphic Relations (MRs) that a MAPF system should guarantee, designed over the environment in which agents operate, the behaviour of the single agents, and the interactions among agents. Starting from the different MRs, MET-MAPF automatically generates test cases addressing them, so possibly exposing different types of failures. Experimental results show that MET-MAPF can indeed find MR violations not exposed by approaches that only consider the completion of the mission as test oracle. Moreover, experiments show that different MRs expose different types of violations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399557806",
    "type": "article"
  },
  {
    "title": "Effective, Platform-Independent GUI Testing via Image Embedding and Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3674728",
    "publication_date": "2024-06-22",
    "publication_year": 2024,
    "authors": "Shengcheng Yu; Chunrong Fang; X J Li; Yuchen Ling; Zhenyu Chen; Zhendong Su",
    "corresponding_authors": "",
    "abstract": "Software applications (apps) have been playing an increasingly important role in various aspects of society. In particular, mobile apps and web apps are the most prevalent among all applications and are widely used in various industries as well as in people’s daily lives. To help ensure mobile and web app quality, many approaches have been introduced to improve app GUI testing via automated exploration, including random testing, model-based testing, learning-based testing, and so on. Despite the extensive effort, existing approaches are still limited in reaching high code coverage, constructing high-quality models, and being generally applicable. Reinforcement learning-based approaches, as a group of representative and advanced approaches for automated GUI exploration testing, are faced with difficult challenges, including effective app state abstraction, reward function design, and so on. Moreover, they heavily depend on the specific execution platforms (i.e., Android or Web), thus leading to poor generalizability and being unable to adapt to different platforms. This work specifically tackles these challenges based on the high-level observation that apps from distinct platforms share commonalities in GUI design. Indeed, we propose PIRLTest , an effective platform-independent approach for app testing. Specifically, PIRLTest utilizes computer vision and reinforcement learning techniques in a novel, synergistic manner for automated testing. It extracts the GUI widgets from GUI pages and characterizes the corresponding GUI layouts, embedding the GUI pages as states. The app GUI state combines the macroscopic perspective (app GUI layout) and the microscopic perspective (app GUI widget) and attaches the critical semantic information from GUI images. This enables PIRLTest to be platform-independent and makes the testing approach generally applicable on different platforms. PIRLTest explores apps with the guidance of a curiosity-driven strategy, which uses a Q-network to estimate the values of specific state-action pairs to encourage more exploration in uncovered pages without platform dependency. The exploration will be assigned with rewards for all actions, which are designed considering both the app GUI states and the concrete widgets, to help the framework explore more uncovered pages. We conduct an empirical study on 20 mobile apps and 5 web apps, and the results show that PIRLTest is zero-cost when being adapted to different platforms, and can perform better than the baselines, covering 6.3–41.4% more code on mobile apps and 1.5–51.1% more code on web apps. PIRLTest is capable of detecting 128 unique bugs on mobile and web apps, including 100 bugs that cannot be detected by the baselines.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399920862",
    "type": "article"
  },
  {
    "title": "A Comprehensive View on TD Prevention Practices and Reasons for not Preventing It",
    "doi": "https://doi.org/10.1145/3674727",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Sávio Freire; Alexia Pacheco; Nicolli Rios; Boris Pérez; Camilo Castellanos; Darío Correal; Robert Ramač; Vladimir Mandić; Nebojša Taušan; Gustavo López; Manoel Mendonça; Davide Falessi; Clemente Izurieta; Carolyn Seaman; Rodrigo Spínola",
    "corresponding_authors": "",
    "abstract": "Context . Technical debt (TD) prevention allows software practitioners to apply practices to avoid potential TD items in their projects. Aims . To uncover and prioritize, from the point of view of software practitioners, the practices that could be used to avoid TD items, the relations between these practices and the causes of TD, and the practice avoidance reasons (PARs) that could explain the failure to prevent TD. Method . We analyze data collected from six replications of a global industrial family of surveys on TD, totaling 653 answers. We also conducted a follow up survey to understand the importance level of analyzed data. Results . Most practitioners indicated that TD could be prevented, revealing 89 prevention practices and 23 PARs for explaining the failure to prevent TD. The article identifies statistically significant relationships between preventive practices and certain causes of TD. Further, it prioritizes the list of practices, PARs, and relationships regarding their level of importance for TD prevention based on the opinion of software practitioners. Conclusion . This work organizes TD prevention practices and PARs in a conceptual map and the relationships between practices and causes of TD in a Sankey diagram to help the visualization of the body of knowledge reported in this study.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400113459",
    "type": "article"
  },
  {
    "title": "Let’s Discover More API Relations: A Large Language Model-based AI Chain for Unsupervised API Relation Inference",
    "doi": "https://doi.org/10.1145/3680469",
    "publication_date": "2024-07-23",
    "publication_year": 2024,
    "authors": "Qing Huang; Yanbang Sun; Zhenchang Xing; Yuanlong Cao; Jieshan Chen; Xiwei Xu; Huan Jin; Jiaxing Lu",
    "corresponding_authors": "",
    "abstract": "APIs have intricate relations that can be described in text and represented as knowledge graphs to aid software engineering tasks. Existing relation extraction methods have limitations, such as limited API text corpus and affected by the characteristics of the input text. To address these limitations, we propose utilizing large language models (LLMs) (e.g., gpt-3.5) as a neural knowledge base for API relation inference. This approach leverages the entire Web used to pre-train LLMs as a knowledge base and is insensitive to the context and complexity of input texts. To ensure accurate inference, we design an AI chain consisting of three AI modules: API Fully Qualified Name (FQN) Parser, API Knowledge Extractor, and API Relation Decider. The accuracy of the API FQN Parser and API Relation Decider is 0.81 and 0.83, respectively. Using the generative capacity of the LLM and our approach’s inference capability, we achieve an average F1 value of 0.76 under the three datasets, significantly higher than the state-of-the-art method’s average F1 value of 0.40. Compared to the original CoT and modularized CoT methods, our AI chain design has improved the performance of API relation inference by 71% and 49%, respectively. Meanwhile, the prompt ensembling strategy enhances the performance of our approach by 32%. The API relations inferred by our method can be further organized into structured forms to provide support for other software engineering tasks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400914497",
    "type": "article"
  },
  {
    "title": "My Fuzzers Won’t Build: An Empirical Study of Fuzzing Build Failures",
    "doi": "https://doi.org/10.1145/3688842",
    "publication_date": "2024-08-21",
    "publication_year": 2024,
    "authors": "Olivier Nourry; Yutaro Kashiwa; Weiyi Shang; Honglin Shu; Yasutaka Kamei",
    "corresponding_authors": "",
    "abstract": "Fuzzing is an automated software testing technique used to find software vulnerabilities that works by sending large amounts of inputs to a software system to trigger bad behaviors. In recent years, the open-source software ecosystem has seen a significant increase in the adoption of fuzzing to avoid spreading vulnerabilities throughout the ecosystem. While fuzzing can uncover vulnerabilities, there is currently a lack of knowledge regarding the challenges of conducting fuzzing activities over time. Specifically, fuzzers are very complex tools to set up and build before they can be used. We set out to empirically find out how challenging is build maintenance in the context of fuzzing. We mine over 1.2 million build logs from Google's OSS-Fuzz service to investigate fuzzing build failures. We first conduct a quantitative analysis to quantify the prevalence of fuzzing build failures. We then manually investigate 677 failing fuzzing builds logs and establish a taxonomy of 25 root causes of build failures. We finally train a machine learning model to recognize common failure patterns in failing build logs. Our taxonomy can serve as a reference for practitioners conducting fuzzing build maintenance. Our modeling experiment shows the potential of using automation to simplify the process of fuzzing.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401724691",
    "type": "article"
  },
  {
    "title": "T-Rec: Fine-Grained Language-Agnostic Program Reduction Guided by Lexical Syntax",
    "doi": "https://doi.org/10.1145/3690631",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Zhenyang Xu; Yongqiang Tian; Mengxiao Zhang; Jiarui Zhang; Puzhuo Liu; Yu Jiang; C. P. Sun",
    "corresponding_authors": "",
    "abstract": "Program reduction strives to eliminate bug-irrelevant code elements from a bug-triggering program, so that (1) a smaller and more straightforward bug-triggering program can be obtained, (2) and the difference among duplicates ( i.e. , different programs that trigger the same bug) can be minimized or even eliminated. With such reduction and canonicalization functionality, program reduction facilitates debugging for software, especially language toolchains, such as compilers, interpreters, and debuggers. While many program reduction techniques have been proposed, most of them (especially the language-agnostic ones) overlooked the potential reduction opportunities hidden within tokens. Therefore, their capabilities in terms of reduction and canonicalization are significantly restricted. To fill this gap, we propose T-Rec, a fine-grained language-agnostic program reduction technique guided by lexical syntax. Instead of treating tokens as atomic and irreducible components, T-Rec introduces a fine-grained reduction process that leverages the lexical syntax of programming languages to effectively explore the reduction opportunities in tokens. Through comprehensive evaluations with versatile benchmark suites, we demonstrate that T-Rec significantly improves the reduction and canonicalization capability of two existing language-agnostic program reducers ( i.e. , Perses and Vulcan). T-Rec enables Perses and Vulcan to further eliminate 1,294 and 1,315 duplicates in a benchmark suite that contains 3,796 test cases that triggers 46 unique bugs. Additionally, T-Rec can also reduce up to 65.52% and 53.73% bytes in the results of Perses and Vulcan on our multi-lingual benchmark suite, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402048137",
    "type": "article"
  },
  {
    "title": "Systematic Literature Review of Commercial Participation in Open Source Software",
    "doi": "https://doi.org/10.1145/3690632",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Xuetao Li; Yuxia Zhang; Cailean Osborne; Minghui Zhou; Zhi Jin; Hui Liu",
    "corresponding_authors": "",
    "abstract": "Open source software (OSS) has been playing a fundamental role in not only information technology but also our social lives. Attracted by various advantages of OSS, increasing commercial companies are participating extensively in open source development, and this has had a broad impact. Enormous research efforts have been devoted to understanding this phenomenon and trying to pursue a win-win result. To characterize the current research achievement and identify challenges, this paper provides a comprehensive systematic literature review (SLR) of existing research on company participation in OSS. We collected 105 papers and organized them based on their research topics, which cover three main directions, i.e., participation motivation, contribution model, and impact on OSS development. We found that companies have diverse motivations from economic, technological, and social aspects, and no one study covered all the motivation categories. Existing studies categorize five main companies’ contribution models in OSS projects through their objectives and how they shape OSS communities. Researchers also explored how commercial participation affects OSS development, including companies, developers, and OSS projects. This study contributes to a comprehensive understanding of commercial participation in OSS development. Based on our findings, we present a set of research challenges and promising directions for companies’ better participation in OSS.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402048570",
    "type": "article"
  },
  {
    "title": "Identifying the Failure-Revealing Test Cases in Metamorphic Testing: A Statistical Approach",
    "doi": "https://doi.org/10.1145/3695990",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Zheng Zheng; Daixu Ren; Huai Liu; Tsong Yueh Chen; T. T. Li",
    "corresponding_authors": "",
    "abstract": "Metamorphic testing, thanks to its high failure-detection effectiveness especially in the absence of test oracle, has been widely applied in both the traditional context of software testing and other relevant fields such as fault localization and program repair. Its core element is a set of metamorphic relations, which are the necessary properties of the target algorithm in the form of the relationships among multiple inputs and corresponding expected outputs. When a relation is violated by the outputs of a group of test cases, namely metamorphic group of test cases, that are constructed based on the relation, a failure is said to be revealed. Traditionally, the primary task of software testing is to reveal failures. Therefore, from the perspective of software testing, it may not need to know which test case(s) in the metamorphic group cause the violation and thus the failure. However, such information is definitely helpful for other software engineering activities, such as software debugging. The current literature of metamorphic testing lacks a systematic mechanism of identifying the actual failure-revealing test cases, which hinders its applicability and effectiveness in other relevant fields. In this paper, we propose a new technique for the FAILure-revealing Test case Identification in Metamorphic testing, namely FAILTIM. The approach is based on a novel application of statistical methods. More specifically, we leverage and adapt the basic ideas of spectrum-based techniques, which are originally used in fault localization, and propose the utilization of a set of risk formulas to estimate the suspiciousness of each individual test case in metamorphic groups. Failure-revealing test cases are then suggested according to their suspiciousness. A series of experiments have been conducted to evaluate the effectiveness and efficiency of FAILTIM using nine subject programs and 30 risk formulas. The experimental results showed that the new approach can achieve a high accuracy in identifying the actual failure-revealing test cases in metamorphic testing. Consequently, our study will help boost the applicability and performance of metamorphic testing beyond testing to other software engineering areas. The present work also unfolds a number of research directions for further advancing the theory of metamorphic testing and more broadly, software testing.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402516172",
    "type": "article"
  },
  {
    "title": "Decision Support Model for Selecting the Optimal Blockchain Oracle Platform: An Evaluation of Key Factors",
    "doi": "https://doi.org/10.1145/3697011",
    "publication_date": "2024-10-26",
    "publication_year": 2024,
    "authors": "Sabreen Ahmadjee; Carlos Mera‐Gómez; Siamak Farshidi; Rami Bahsoon; Rick Kazman",
    "corresponding_authors": "",
    "abstract": "Smart contract-based applications are executed in a blockchain environment, and they cannot directly access data from external systems, which is required for the service provision of these applications. Instead, smart contracts use agents known as blockchain oracles to collect and provide data feeds to the contracts. The functionality and compatibility with smart contract applications need to be considered when selecting the best-fit oracle platform. As the number of oracle alternatives and their features increases, the decision-making process becomes increasingly complex. Selecting the wrong or sub-optimal oracle is costly and may lead to severe security risks. This paper provides a decision support model for the oracle selection problem. The model supports smart contract decision-makers in selecting a secure, cost-effective, and feasible oracle platform for their applications. We interviewed oracle co-founders and smart contracts experts to refine and validate the decision model. Two real-world smart contract application case studies were used to evaluate the model. Our model prioritises and suggests more than one possible oracle platform based on the developer's required criteria, security assessment, and cost analysis. Moreover, this guided decision model serves to reveal issues that may go unnoticed if done haphazardly, reduce decision-making efforts, and provide a cost-effective solution.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403783131",
    "type": "article"
  },
  {
    "title": "Deep Configuration Performance Learning: A Systematic Survey and Taxonomy",
    "doi": "https://doi.org/10.1145/3702986",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Jingzhi Gong; Tao Chen",
    "corresponding_authors": "",
    "abstract": "Performance is arguably the most crucial attribute that reflects the quality of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 1,206 searched papers spanning six indexing services, based on which 99 primary papers were extracted and analyzed. Our results outline key statistics, taxonomy, strengths, weaknesses, and optimal usage scenarios for techniques related to the preparation of configuration data, the construction of deep learning performance models, the evaluation of these models, and their utilization in various software configuration-related tasks. We also identify the good practices and potentially problematic phenomena from the studies surveyed, together with a comprehensive summary of actionable suggestions and insights into future opportunities within the field. To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404059931",
    "type": "article"
  },
  {
    "title": "Detecting Refactoring Commits in Machine Learning Python Projects: A Machine Learning-Based Approach",
    "doi": "https://doi.org/10.1145/3705309",
    "publication_date": "2024-11-22",
    "publication_year": 2024,
    "authors": "Shayan Noei; Heng Li; Ying Zou",
    "corresponding_authors": "",
    "abstract": "Refactoring aims to improve the quality of software without altering its functional behaviors. Understanding developers’ refactoring activities is essential to improve software maintainability. The use of machine learning (ML) libraries and frameworks in software systems has significantly increased in recent years, making the maximization of their maintainability crucial. Due to the data-driven nature of ML libraries and frameworks, they often undergo a different development process compared to traditional projects. As a result, they may experience various types of refactoring, such as those related to the data. The state-of-the-art refactoring detection tools have not been tested in the ML technical domain, and they are not specifically designed to detect ML-specific refactoring types (e.g., data manipulation) in ML projects; therefore, they may not adequately find all potential refactoring operations, specifically the ML-specific refactoring operations. Furthermore, a vast number of ML libraries and frameworks are written in Python, which has limited tooling support for refactoring detection. PyRef, a rule-based and state-of-the-art tool for Python refactoring detection, can identify 11 types of refactoring operations with relatively high precision. In contrast, for other languages such as Java, state-of-the-art tools are capable of detecting a much more comprehensive list of refactorings. For example, Rminer can detect 99 types of refactoring for Java projects. Inspired by previous work that leverages commit messages to detect refactoring, we introduce MLRefScanner, a prototype tool that applies machine-learning techniques to detect refactoring commits in ML Python projects. MLRefScanner detects commits involving both ML-specific refactoring operations and additional refactoring operations beyond the scope of state-of-the-art refactoring detection tools. To demonstrate the effectiveness of our approach, we evaluate MLRefScanner on 199 ML open-source libraries and frameworks and compare MLRefScanner against other refactoring detection tools for Python projects. Our findings show that MLRefScanner outperforms existing tools in detecting refactoring-related commits, achieving an overall precision of 94% and recall of 82% for identifying refactoring-related commits. MLRefScanner can identify commits with ML-specific and additional refactoring operations compared to state-of-the-art refactoring detection tools. When combining MLRefScanner with PyRef, we can further increase the precision and recall to 95% and 99%, respectively. MLRefScanner provides a valuable contribution to the Python ML community, as it allows ML developers to detect refactoring-related commits more effectively in their ML Python projects. Our study sheds light on the promising direction of leveraging machine learning techniques to detect refactoring activities for other programming languages or technical domains where the commonly used rule-based refactoring detection approaches are not sufficient.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404635154",
    "type": "article"
  },
  {
    "title": "Automatic Programming: Large Language Models and Beyond",
    "doi": "https://doi.org/10.1145/3708519",
    "publication_date": "2024-12-14",
    "publication_year": 2024,
    "authors": "Michael R. Lyu; Baishakhi Ray; Abhik Roychoudhury; Shin Hwei Tan; Patanamon Thongtanunam",
    "corresponding_authors": "",
    "abstract": "Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4405396136",
    "type": "article"
  },
  {
    "title": "The Impact of Generative AI on Creativity in Software Development: A Research Agenda",
    "doi": "https://doi.org/10.1145/3708523",
    "publication_date": "2024-12-19",
    "publication_year": 2024,
    "authors": "Victoria Jackson; Bogdan Vasilescu; Daniel Russo; Paul Ralph; Rafael Prikladnicki; Maliheh Izadi; Sarah D’Angelo; Sarah Inman; Anielle Lisboa; André van der Hoek",
    "corresponding_authors": "",
    "abstract": "As GenAI becomes embedded in developer toolchains and practices, and routine code is increasingly generated, human creativity will be increasingly important for generating competitive advantage. This paper uses the McLuhan tetrad alongside scenarios of how GenAI may disrupt software development more broadly, to identify potential impacts GenAI may have on creativity within software development. The impacts are discussed along with a future research agenda comprising five connected themes that consider how individual capabilities, team capabilities, the product, unintended consequences, and society. can be affected.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4405570253",
    "type": "article"
  },
  {
    "title": "From Today’s Code to Tomorrow’s Symphony: The AI Transformation of Developer’s Routine by 2030",
    "doi": "https://doi.org/10.1145/3709353",
    "publication_date": "2024-12-24",
    "publication_year": 2024,
    "authors": "Ketai Qiu; Niccolò Puccinelli; Matteo Ciniselli; Luca Di Grazia",
    "corresponding_authors": "",
    "abstract": "In the rapidly evolving landscape of software engineering, the integration of Artificial Intelligence (AI) into the Software Development Life-Cycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift towards AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI’s ChatGPT, which have become a crucial element for coding, debugging, and software design. In this paper we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers’ roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant , an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers’ capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security and creativity.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4405758166",
    "type": "article"
  },
  {
    "title": "Software Fairness Debt: Building a Research Agenda for Addressing Bias in AI Systems",
    "doi": "https://doi.org/10.1145/3709357",
    "publication_date": "2024-12-30",
    "publication_year": 2024,
    "authors": "Ronnie de Souza Santos; Felipe Fronchetti; Sávio Freire; Rodrigo Spínola",
    "corresponding_authors": "",
    "abstract": "Ensuring fairness in software systems has become a critical concern in software engineering. Motivated by this challenge, this paper explores the multifaceted nature of bias in software systems, providing a comprehensive understanding of its origins, manifestations, and impacts. Through a scoping study, we identified the primary causes of fairness deficiencies in software development and highlighted their adverse effects on individuals and communities, including instances of discrimination and the perpetuation of inequalities. Our investigation culminated in the introduction of the concept of software fairness debt. In addition to defining fairness debt, we propose a socio-technical roadmap that addresses broader aspects of fairness in AI-driven systems. This roadmap is structured around six goals: bridging the gap between research and real-world applications, developing a framework for fairness debt, equipping practitioners with tools and knowledge, improving bias mitigation, integrating fairness tools into industry practice, and enhancing explainability and transparency in AI systems. This roadmap provides a holistic approach to managing biases in software systems through software fairness debt, offering actionable steps for both research and practice. By guiding researchers and practitioners, our roadmap aims to foster the development of more equitable and socially responsible software systems, ensuring fairness is embedded throughout the software lifecycle.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4405891407",
    "type": "article"
  },
  {
    "title": "Using failure cost information for testing and reliability assessment",
    "doi": "https://doi.org/10.1145/227607.227608",
    "publication_date": "1996-04-01",
    "publication_year": 1996,
    "authors": "Elaine J. Weyuker",
    "corresponding_authors": "Elaine J. Weyuker",
    "abstract": "A technique for incorporating failure cost information into algorithms designed to automatically generate software-load-testing suites is presented. A previously introduced reliability measure is also modified to incorporate this cost information. examples are presented to show the usefulness of including cost information when testing or assessing software.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1974055215",
    "type": "article"
  },
  {
    "title": "A concurrency analysis tool suite for Ada programs",
    "doi": "https://doi.org/10.1145/201055.201080",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Michal Young; Richard N. Taylor; David L. Levine; Kari Nies; D. Brodbeck",
    "corresponding_authors": "",
    "abstract": "Cats (Concurrency Analysis Tool Suite) is designed to satisfy several criteria: it must analyze implementation-level Ada source code and check user-specified conditions associated with program source code; it must be modularized in a fashion that supports flexible composition with other tool components, including integration with a variety of testing and analysis techniques; and its performance and capacity must be sufficient for analysis of real application programs. Meeting these objectives together is significantly more difficult than meeting any of them alone. We describe the design and rationale of Cats and report experience with an implementation. The issues addressed here are primarily practical concerns for modularizing and integrating tools for analysis of actual source programs. We also report successful application of Cats to major subsystems of a (nontoy) highly concurrent user interface system.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2030837954",
    "type": "article"
  },
  {
    "title": "A reduced test suite for protocol conformance testing",
    "doi": "https://doi.org/10.1145/196092.196088",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Philip J. Bernhard",
    "corresponding_authors": "Philip J. Bernhard",
    "abstract": "article Free Access Share on A reduced test suite for protocol conformance testing Author: Philip J. Bernhard Harris Space Systems Corporation Harris Space Systems CorporationView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 3Issue 3July 1994 pp 201–220https://doi.org/10.1145/196092.196088Online:01 July 1994Publication History 22citation637DownloadsMetricsTotal Citations22Total Downloads637Last 12 Months4Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2052877962",
    "type": "article"
  },
  {
    "title": "Abstracting dependencies between software configuration items",
    "doi": "https://doi.org/10.1145/332740.332743",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Carl A. Gunter",
    "corresponding_authors": "Carl A. Gunter",
    "abstract": "This article studies an abstract model of dependencies between software configuration items based on a theory of concurrent computation over a class of Petri nets called production nets. A general theory of build optimizations and their correctness is developed based on a form of abstract interpretation called a build abstraction ; these are created during a build and are used to optimize subsequent builds. Various examples of such optimizations are discussed. The theory is used to show how properties can be characterized and proved, and how optimizations can be composed and compared.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2016611270",
    "type": "article"
  },
  {
    "title": "Assembly instruction level reverse execution for debugging",
    "doi": "https://doi.org/10.1145/1018210.1018211",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "Tankut Akgül; Vincent J. Mooney",
    "corresponding_authors": "",
    "abstract": "Assembly instruction level reverse execution provides a programmer with the ability to return a program to a previous state in its execution history via execution of a \"reverse program.\" The ability to execute a program in reverse is advantageous for shortening software development time. Conventional techniques for recovering a state rely on saving the state into a record before the state is destroyed. However, state-saving causes significant memory and time overheads during forward execution.The proposed method introduces a reverse execution methodology at the assembly instruction level with low memory and time overheads. The methodology generates, from a program, a reverse program by which a destroyed state is almost always regenerated rather than being restored from a record. This significantly reduces state-saving.The methodology has been implemented on a PowerPC processor with a custom made debugger. As compared to previous work, all of which heavily use state-saving techniques, the experimental results show from 2X to 2206X reduction in run-time memory usage, from 1.5X to 403X reduction in forward execution time overhead and from 1.2X to 2.32X reduction in forward execution time for the tested benchmarks. Furthermore, due to the reduction in memory usage, our method can provide reverse execution in many cases where other methods run out of available memory. However, for cases where there is enough memory available, our method results in 1.16X to 1.89X slow down in reverse execution.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1992329154",
    "type": "article"
  },
  {
    "title": "Developing and debugging algebraic specifications for Java classes",
    "doi": "https://doi.org/10.1145/1363102.1363105",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Johannes Henkel; Christoph Reichenbach; Amer Diwan",
    "corresponding_authors": "",
    "abstract": "Modern programs make extensive use of reusable software libraries. For example, a study of a number of large Java applications shows that between 17% and 30% of the classes in those applications use container classes defined in the java.util package. Given this extensive code reuse in Java programs, it is important for the interfaces of reusable classes to be well documented. An interface is well documented if it satisfies the following requirements: (1) the documentation completely describes how to use the interface; (2) the documentation is clear; (3) the documentation is unambiguous; and (4) any deviation between the documentation and the code is machine detectable. Unfortunately, documentation in natural language, which is the norm, does not satisfy the above requirements. Formal specifications can satisfy them but they are difficult to develop, requiring significant effort on the part of programmers. To address the practical difficulties with formal specifications, we describe and evaluate a tool to help programmers write and debug algebraic specifications. Given an algebraic specification of a class, our interpreter generates a prototype that can be used within an application like a regular Java class. When running an application that uses the prototype, the interpreter prints error messages that tell the developer in which way the specification is incomplete or inconsistent with a hand-coded implementation of the class. We use case studies to demonstrate the usefulness of our system.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1993509980",
    "type": "article"
  },
  {
    "title": "Domain-specific languages and program generation with meta-AspectJ",
    "doi": "https://doi.org/10.1145/1416563.1416566",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Shan Huang; David Zook; Yannis Smaragdakis",
    "corresponding_authors": "",
    "abstract": "Meta-AspectJ (MAJ) is a language for generating AspectJ programs using code templates. MAJ itself is an extension of Java, so users can interleave arbitrary Java code with AspectJ code templates. MAJ is a structured metaprogramming tool: a well-typed generator implies a syntactically correct generated program. MAJ promotes a methodology that combines aspect-oriented and generative programming. A valuable application is in implementing small domain-specific language extensions as generators using unobtrusive annotations for syntax extension and AspectJ as a back-end. The advantages of this approach are twofold. First, the generator integrates into an existing software application much as a regular API or library, instead of as a language extension. Second, a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language. In addition to its practical value, MAJ offers valuable insights to metaprogramming tool designers. It is a mature metaprogramming tool for AspectJ (and, by extension, Java): a lot of emphasis has been placed on context-sensitive parsing and error reporting. As a result, MAJ minimizes the number of metaprogramming (quote/unquote) operators and uses type inference to reduce the need to remember type names for syntactic entities.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2023878728",
    "type": "article"
  },
  {
    "title": "Parallel Algorithms for Generating Distinguishing Sequences for Observable Non-deterministic FSMs",
    "doi": "https://doi.org/10.1145/3051121",
    "publication_date": "2017-01-31",
    "publication_year": 2017,
    "authors": "Robert M. Hierons; Uraz Cengiz Türker",
    "corresponding_authors": "",
    "abstract": "A distinguishing sequence (DS) for a finite-state machine (FSM) is an input sequence that distinguishes every pair of states of the FSM. There are techniques that generate a test sequence with guaranteed fault detection power, and it has been found that shorter test sequences can be produced if DSs are used. Despite these benefits, however, until recently the only published DS generation algorithms have been for deterministic FSMs. This article develops a massively parallel algorithm, which can be used in Graphics Processing Units (GPUs) Computing, to generate DSs from partial observable non-deterministic FSMs. We also present the results of experiments using randomly generated FSMs and some benchmark FSMs. The results are promising and indicate that the proposed algorithm can derive DSs from partial observable non-deterministic FSMs with 32,000 states in an acceptable amount of time.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2579287322",
    "type": "article"
  },
  {
    "title": "Combining Centralised and Distributed Testing",
    "doi": "https://doi.org/10.1145/2661296",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Robert M. Hierons",
    "corresponding_authors": "Robert M. Hierons",
    "abstract": "Many systems interact with their environment at distributed interfaces (ports) and sometimes it is not possible to place synchronised local testers at the ports of the system under test (SUT). There are then two main approaches to testing: having independent local testers or a single centralised tester that interacts asynchronously with the SUT. The power of using independent testers has been captured using implementation relation dioco. In this article, we define implementation relation diococ for the centralised approach and prove that dioco and dioco c are incomparable. This shows that the frameworks detect different types of faults and so we devise a hybrid framework and define an implementation relation diocos for this. We prove that the hybrid framework is more powerful than the distributed and centralised approaches. We then prove that the Oracle problem is NP-complete for diococ and diocos but can be solved in polynomial time if we place an upper bound on the number of ports. Finally, we consider the problem of deciding whether there is a test case that is guaranteed to force a finite state model into a particular state or to distinguish two states, proving that both problems are undecidable for the centralised and hybrid frameworks.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2081825954",
    "type": "article"
  },
  {
    "title": "S <scp>EADS</scp>",
    "doi": "https://doi.org/10.1145/3379345",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Xiaoqin Fu; Haipeng Cai; Wen Li; Li Li",
    "corresponding_authors": "",
    "abstract": "Distributed software systems are increasingly developed and deployed today. Many of these systems are supposed to run continuously. Given their critical roles in our society and daily lives, assuring the quality of distributed systems is crucial. Analyzing runtime program dependencies has long been a fundamental technique underlying numerous tool support for software quality assurance. Yet conventional approaches to dynamic dependence analysis face severe scalability barriers when they are applied to real-world distributed systems, due to the unbounded executions to be analyzed in addition to common efficiency challenges suffered by dynamic analysis in general. In this article, we present S EADS , a distributed , online , and cost-effective dynamic dependence analysis framework that aims at scaling the analysis to real-world distributed systems. The analysis itself is distributed to exploit the distributed computing resources (e.g., a cluster) of the system under analysis; it works online to overcome the problem with unbounded execution traces while running continuously with the system being analyzed to provide timely querying of analysis results (i.e., runtime dependence set of any given query). Most importantly, given a user-specified time budget, the analysis automatically adjusts itself to better cost-effectiveness tradeoffs (than otherwise) while respecting the budget by changing various analysis parameters according to the time being spent by the dependence analysis. At the core of the automatic adjustment is our application of a reinforcement learning method for the decision making—deciding which configuration to adjust to according to the current configuration and its associated analysis cost with respect to the user budget. We have implemented S EADS for Java and applied it to eight real-world distributed systems with continuous executions. Our empirical results revealed the efficiency and scalability advantages of our framework over a conventional dynamic analysis, at least for dynamic dependence computation at method level. While we demonstrate it in the context of dynamic dependence analysis in this article, the methodology for achieving and maintaining scalability and greater cost-effectiveness against continuously running systems is more broadly applicable to other dynamic analyses.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3040719252",
    "type": "article"
  },
  {
    "title": "Learning Weighted Assumptions for Compositional Verification of Markov Decision Processes",
    "doi": "https://doi.org/10.1145/2907943",
    "publication_date": "2016-06-06",
    "publication_year": 2016,
    "authors": "Fei He; Xiaowei Gao; Miaofei Wang; Bow-Yaw Wang; Lijun Zhang",
    "corresponding_authors": "",
    "abstract": "Probabilistic models are widely deployed in various systems. To ensure their correctness, verification techniques have been developed to analyze probabilistic systems. We propose the first sound and complete learning-based compositional verification technique for probabilistic safety properties on concurrent systems where each component is an Markov decision process. Different from previous works, weighted assumptions are introduced to attain completeness of our framework. Since weighted assumptions can be implicitly represented by multiterminal binary decision diagrams (MTBDDs), we give an &gt;i&lt;L&gt;/i&lt;*-based learning algorithm for MTBDDs to infer weighted assumptions. Experimental results suggest promising outlooks for our compositional technique.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2411900593",
    "type": "article"
  },
  {
    "title": "Automating App Review Response Generation Based on Contextual Knowledge",
    "doi": "https://doi.org/10.1145/3464969",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Cuiyun Gao; Wenjie Zhou; Xin Xia; David Lo; Qi Xie; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "User experience of mobile apps is an essential ingredient that can influence the user base and app revenue. To ensure good user experience and assist app development, several prior studies resort to analysis of app reviews, a type of repository that directly reflects user opinions about the apps. Accurately responding to the app reviews is one of the ways to relieve user concerns and thus improve user experience. However, the response quality of the existing method relies on the pre-extracted features from other tools, including manually labelled keywords and predicted review sentiment, which may hinder the generalizability and flexibility of the method. In this article, we propose a novel neural network approach, named CoRe, with the contextual knowledge naturally incorporated and without involving external tools. Specifically, CoRe integrates two types of contextual knowledge in the training corpus, including official app descriptions from app store and responses of the retrieved semantically similar reviews, for enhancing the relevance and accuracy of the generated review responses. Experiments on practical review data show that CoRe can outperform the state-of-the-art method by 12.36% in terms of BLEU-4, an accuracy metric that is widely used to evaluate text generation systems.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3209079307",
    "type": "article"
  },
  {
    "title": "A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms",
    "doi": "https://doi.org/10.1145/3533021",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Hichem Belgacem; Xiaochen Li; Domenico Bianculli; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options. In this paper, we propose LAFF, a learning-based automated approach for filling categorical fields in data entry forms. LAFF first builds Bayesian Network models by learning field dependencies from a set of historical input instances, representing the values of the fields that have been filled in the past. To improve its learning ability, LAFF uses local modeling to effectively mine the local dependencies of fields in a cluster of input instances. During the form filling phase, LAFF uses such models to predict possible values of a target field, based on the values in the already-filled fields of the form and their dependencies; the predicted values (endorsed based on field dependencies and prediction confidence) are then provided to the end-user as a list of suggestions. We evaluated LAFF by assessing its effectiveness and efficiency in form filling on two datasets, one of them proprietary from the banking domain. Experimental results show that LAFF is able to provide accurate suggestions with a Mean Reciprocal Rank value above 0.73. Furthermore, LAFF is efficient, requiring at most 317 ms per suggestion.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4225143092",
    "type": "article"
  },
  {
    "title": "Using Personality Detection Tools for Software Engineering Research: How Far Can We Go?",
    "doi": "https://doi.org/10.1145/3491039",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Fabio Calefato; Filippo Lanubile",
    "corresponding_authors": "",
    "abstract": "Assessing the personality of software engineers may help to match individual traits with the characteristics of development activities such as code review and testing, as well as support managers in team composition. However, self-assessment questionnaires are not a practical solution for collecting multiple observations on a large scale. Instead, automatic personality detection, while overcoming these limitations, is based on off-the-shelf solutions trained on non-technical corpora, which might not be readily applicable to technical domains like Software Engineering (SE). In this paper, we first assess the performance of general-purpose personality detection tools when applied to a technical corpus of developers' emails retrieved from the public archives of the Apache Software Foundation. We observe a general low accuracy of predictions and an overall disagreement among the tools. Second, we replicate two previous research studies in SE by replacing the personality detection tool used to infer developers' personalities from pull-request discussions and emails. We observe that the original results are not confirmed, i.e., changing the tool used in the original study leads to diverging conclusions. Our results suggest a need for personality detection tools specially targeted for the software engineering domain.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4225644729",
    "type": "article"
  },
  {
    "title": "<scp>Anchor</scp> : Fast and Precise Value-flow Analysis for Containers via Memory Orientation",
    "doi": "https://doi.org/10.1145/3565800",
    "publication_date": "2022-10-04",
    "publication_year": 2022,
    "authors": "Chengpeng Wang; Wenyang Wang; Peisen Yao; Qingkai Shi; Jinguo Zhou; Xiao Xiao; Charles Zhang",
    "corresponding_authors": "",
    "abstract": "Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered. This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1% as its false-positive ratio, while the smashing-based analysis reports 66.7% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4301395468",
    "type": "article"
  },
  {
    "title": "Morescient GAI for Software Engineering",
    "doi": "https://doi.org/10.1145/3709354",
    "publication_date": "2025-01-03",
    "publication_year": 2025,
    "authors": "Marcus Kessel; Colin Atkinson",
    "corresponding_authors": "",
    "abstract": "The ability of Generative AI (GAI) technology to automatically check, synthesize and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with over a hundred LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness – they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of “Morescient” GAI is needed that is “aware” of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating large quantities of execution observations in a structured and readily analyzable way. In this paper, we present a vision and roadmap for how such “Morescient” GAI models can be engineered, evolved and disseminated according to the principles of open science.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406025986",
    "type": "article"
  },
  {
    "title": "Unified and Split Symbolic Execution for Exposing Semantic Differences",
    "doi": "https://doi.org/10.1145/3705299",
    "publication_date": "2025-01-06",
    "publication_year": 2025,
    "authors": "Hongliang Liang; Luming Yin; Wenying Hu; Y. Q. Li; Wuwei Shen",
    "corresponding_authors": "",
    "abstract": "Software evolution is an important activity during a software development life cycle. Understanding semantic differences between two versions of a software system is a crucial yet challenging task, especially in many safety critical sectors. Consequently, various techniques have been proposed to check semantic differences between a program and its evolution. But, many current techniques are still far from being satisfactory in terms of the accuracy and efficiency. In this paper, we propose a novel framework, called US \\({}^{2}\\) E , which can efficiently and effectively generate the minimal number of test cases that reveal as many semantic differences across two versions as possible. Specifically, given a unified control flow graph that denotes two versions of a program, US \\({}^{2}\\) E executes as many common nodes as possible and leaves execution of non-common nodes separately in a single concolic execution instance. We evaluate US \\({}^{2}\\) E on 86 pairs of C programs from four benchmarks, and experimental results show that US \\({}^{2}\\) E can efficiently and effectively generate test cases demonstrating the semantic differences across two versions, with better performance than six baseline tools.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406085804",
    "type": "article"
  },
  {
    "title": "How are We Detecting Inconsistent Method Names? An Empirical Study from Code Review Perspective",
    "doi": "https://doi.org/10.1145/3711901",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Kisub Kim; Xin Zhou; Dongsun Kim; Julia Lawall; Kui Liu; Tegawendé F. Bissyandé; Jacques Klein; Jaekwon Lee; David Lo",
    "corresponding_authors": "",
    "abstract": "Proper naming of methods can make program code easier to understand, and thus enhance software maintainability. Yet, developers may use inconsistent names due to poor communication or a lack of familiarity with conventions within the software development lifecycle. To address this issue, much research effort has been invested into building automatic tools that can check for method name inconsistency and recommend consistent names. However, existing datasets generally do not provide precise details about why a method name was deemed improper and required to be changed. Such information can give useful hints on how to improve the recommendation of adequate method names. Accordingly, we construct a sample method-naming benchmark, ReName4J, by matching name changes with code reviews. We then present an empirical study on how state-of-the-art techniques perform in detecting or recommending consistent and inconsistent method names based on ReName4J. The main purpose of the study is to reveal a different perspective based on reviewed names rather than proposing a complete benchmark. We find that the existing techniques underperform on our review-driven benchmark, both in inconsistent checking and the recommendation. We further identify potential biases in the evaluation of existing techniques, which future research should consider thoroughly.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406308383",
    "type": "article"
  },
  {
    "title": "You Don’t Have to Say Where to Edit! jLED – Joint Learning to Localize and Edit Source Code",
    "doi": "https://doi.org/10.1145/3712187",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Weiguo Pian; Yinghua Li; Haoye Tian; Tiezhu Sun; Yewei Song; Xunzhu Tang; Andrew Habib; Jacques Klein; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "Learning to edit code automatically is becoming more and more feasible. Thanks to recent advances in Neural Machine Translation (NMT) , various case studies are being investigated where patches are automatically produced and assessed either automatically (using test suites) or by developers themselves. An appealing setting remains when the developer must provide a natural language input of the requirement for the code change. A recent proof of concept in the literature showed that it is indeed feasible to translate these natural language requirements into code changes. A recent advancement, MODIT, has shown promising results in code editing by leveraging natural language, code context, and location information as input. However, it struggles when location information is unavailable. While several studies have demonstrated the ability to edit source code without explicitly specifying the edit location, they still tend to generate edits with less accuracy at the line level. In this work, we address the challenge of generating code edits without precise location information, a scenario we consider crucial for the practical adoption of NMT in code development. To that end, we develop a novel joint training approach for both localization and source code editions. Building a benchmark based on over 70k commits (patches and messages), we demonstrate that our joint Localize and EDit ( jLED) approach is effective. An ablation study further demonstrates the importance of our design choice in joint training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406324690",
    "type": "article"
  },
  {
    "title": "Introducing Interactions in Multi-Objective Optimization of Software Architectures",
    "doi": "https://doi.org/10.1145/3712185",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Vittorio Cortellessa; J. Andrés Díaz‐Pace; Daniele Di Pompeo; Sebastian Frank; Pooyan Jamshidi; Michele Tucci; André van Hoorn",
    "corresponding_authors": "",
    "abstract": "Software architecture optimization aims to enhance non-functional attributes like performance and reliability while meeting functional requirements. Multi-objective optimization employs metaheuristic search techniques, such as genetic algorithms, to explore feasible architectural changes and propose alternatives to designers. However, this resource-intensive process may not always align with practical constraints. This study investigates the impact of designer interactions on multi-objective software architecture optimization. Designers can intervene at intermediate points in the fully automated optimization process, making choices that guide exploration towards more desirable solutions. Through several controlled experiments as well as an initial user study (14 subjects), we compare this interactive approach with a fully automated optimization process, which serves as a baseline. The findings demonstrate that designer interactions lead to a more focused solution space, resulting in improved architectural quality. By directing the search towards regions of interest, the interaction uncovers architectures that remain unexplored in the fully automated process. In the user study, participants found that our interactive approach provides a better trade-off between sufficient exploration of the solution space and the required computation time.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406325819",
    "type": "article"
  },
  {
    "title": "Recommending Variable Names for Extract Local Variable Refactorings",
    "doi": "https://doi.org/10.1145/3712191",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Taiming Wang; Hui Liu; Yuxia Zhang; Yanjie Jiang",
    "corresponding_authors": "",
    "abstract": "Extract local variable is one of the most popular refactorings. It is frequently employed to replace occurrences of a complex expression with simple accesses to a newly introduced variable that is initialized by the original complex expression. Consequently, most IDEs and refactoring tools provide automated support for this refactoring, e.g., to suggest names for the newly extracted variables. However, we find approximately 70% of the names recommended by these IDEs are different from what developers manually constructed, adding additional renaming burdens to developers and providing limited assistance. In this paper, we introduce VarNamer , an automated approach designed to recommend variable names for extract local variable refactorings. Through a large-scale empirical study, we identify key contexts, such as variable initializations and homogeneous variables (variables whose initializations are identical to that of the newly extracted variable), that are useful for composing variable names. Leveraging these insights, we developed a set of heuristic rules through program static analysis techniques, e.g., lexical analysis, syntax analysis, control flow analysis, and data flow analysis, and employ data mining techniques, i.e., FP-growth algorithm, to recommend variable names effectively. Notably, some of our heuristic rules have been successfully integrated into Eclipse , where they are now distributed with the latest releases of the IDE. Evaluation of VarNamer on a dataset of 27,158 real-world extract local variable refactorings in Java applications demonstrates its superiority over state-of-the-art IDEs. Specifically, VarNamer significantly increases the chance of exact match by 52.6% compared to Eclipse and 40.7% compared to IntelliJ IDEA . We also evaluated the proposed approach with real-world extract local variable refactorings conducted in C++ projects, and the results suggest that the approach can achieve comparable performance on programming languages besides Java. It may suggest the generalizability of VarNamer . Finally, we designed and conducted a user study to investigate the impact of VarNamer on developers’ productivity. The results of the user study suggest that our approach can speed up the refactoring by 27.8% and reduce 49.3% edits on the recommended variable names.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406325921",
    "type": "article"
  },
  {
    "title": "Automatically Learning a Precise Measurement for Fault Diagnosis Capability of Test Cases",
    "doi": "https://doi.org/10.1145/3712189",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Yifan Zhao; Zeyu Sun; Guoqing Wang; Qingyuan Liang; Yakun Zhang; Yiling Lou; Dan Hao; Lu Zhang",
    "corresponding_authors": "",
    "abstract": "Prevalent Fault Localization (FL) techniques rely on tests to localize buggy program elements. Tests could be treated as fuel to further boost FL by providing more debugging information. Therefore, it is highly valuable to measure the Fault Diagnosis Capability (FDC) of a test for diagnosing faults, so as to select or generate tests to better help FL (i.e., FL-oriented test selection or FL-oriented test generation). To this end, researchers have proposed many FDC metrics, which serve as the selection criterion in FL-oriented test selection or the fitness function in FL-oriented test generation. Existing FDC metrics can be classified into result-agnostic and result-aware metrics depending on whether they take test results (i.e., passing or failing) as input. Although result-aware metrics perform better in test selection, they have restricted applications due to the input of test results, e.g., they cannot be applied to guide test generation. Moreover, all the existing FDC metrics are designed based on some predefined heuristics and have achieved limited FL performance due to their inaccuracy. To address these issues, in this paper, we reconsider result-agnostic metrics (i.e., metrics that do not take test results as input), and propose a novel result-agnostic metric RLFDC which predicts FDC values of tests through reinforcement learning. In particular, we treat FL results as reward signals, and train an FDC prediction model with the direct FL feedback to automatically learn a more accurate measurement rather than design one based on predefined heuristics. Finally, we evaluate the proposed RLFDC on Defects4J by applying the studied metrics to test selection and generation. According to the experimental results, the proposed RLFDC outperforms all the result-agnostic metrics in both test selection and generation, e.g., when applied to selecting human-written tests, RLFDC achieves 28.2% and 21.6% higher acc@1 and mAP values compared to the state-of-the-art result-agnostic metric TfD. Besides, RLFDC even achieves competitive performance compared to the state-of-the-art result-aware metric FDG in test selection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406385977",
    "type": "article"
  },
  {
    "title": "On the Practicability of Deep Learning based Anomaly Detection for Modern Online Software Systems: A Pre-Train-and-Align Framework",
    "doi": "https://doi.org/10.1145/3712195",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Zilong He; Pengfei Chen; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "Operation and maintenance are critical activities in the whole life cycle of modern online software systems, and anomaly detection is a crucial step of these activities. Recent studies mainly develop deep learning techniques to complete this task. Notably, though these techniques have achieved promising results in experimental evaluations, there are still several practicality gaps for them to be successfully applied in a real-world online system, including the scalability gap, availability gap and alignment gap. To bridge these gaps, we propose an anomaly detection framework, namely ShareAD , based on a pre-train-and-align paradigm. Specifically, we argue that pre-training a shared model for anomaly detection is an effective way to bridge the scalability gap and the availability gap. To support this argument, we systematically study the necessity and feasibility of model sharing for online system maintenance. We further propose a novel model based upon Transformer encoder layers and Base layers, which works well for anomaly detection pre-training. Then, to bridge the alignment gap, we propose ShareAD alignment to align the pre-trained model with operator preference by jointly considering the local observation context and sensitivity of each monitor entity. Extensive experiments on two real-world large-scale datasets demonstrate the effectiveness and practicality of ShareAD .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406385978",
    "type": "article"
  },
  {
    "title": "System Safety Monitoring of Learned Components Using Temporal Metric Forecasting",
    "doi": "https://doi.org/10.1145/3712196",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Sepehr Sharifi; Andrea Stocco; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "In learning-enabled autonomous systems, safety monitoring of learned components is crucial to ensure their outputs do not lead to system safety violations, given the operational context of the system. However, developing a safety monitor for practical deployment in real-world applications is challenging. This is due to limited access to internal workings and training data of the learned component. Furthermore, safety monitors should predict safety violations with low latency, while consuming a reasonable computation resource amount. To address the challenges, we propose a safety monitoring method based on probabilistic time series forecasting. Given the learned component outputs and an operational context, we empirically investigate different Deep Learning (DL)-based probabilistic forecasting to predict the objective measure capturing the satisfaction or violation of a safety requirement ( safety metric ). We empirically evaluate safety metric and violation prediction accuracy, and inference latency and resource usage of four state-of-the-art models, with varying horizons, using autonomous aviation and autonomous driving case studies. Our results suggest that probabilistic forecasting of safety metrics, given learned component outputs and scenarios, is effective for safety monitoring. Furthermore, for both case studies, the Temporal Fusion Transformer (TFT) was the most accurate model for predicting imminent safety violations, with acceptable latency and resource consumption.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406385999",
    "type": "article"
  },
  {
    "title": "Novelty Not Found: Exploring Input Shadowing in Fuzzing through Adaptive Fuzzer Restarts - RCR Report",
    "doi": "https://doi.org/10.1145/3712590",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Nico Schiller; Xinyi Xu; Lukas Bernhard; Nils Bars; Moritz Schloegel; Thorsten Holz",
    "corresponding_authors": "",
    "abstract": "Greybox fuzzing enhances software security through unprecedented effectiveness in automated fault detection. Its success lies in the coverage feedback extracted from the system under test, guiding the fuzzer to explore different program parts. The most ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406465878",
    "type": "article"
  },
  {
    "title": "Novelty Not Found: Exploring Input Shadowing in Fuzzing through Adaptive Fuzzer Restarts",
    "doi": "https://doi.org/10.1145/3712186",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Nico Schiller; Xinyi Xu; Lukas Bernhard; Nils Bars; Moritz Schloegel; Thorsten Holz",
    "corresponding_authors": "",
    "abstract": "Greybox fuzzing enhances software security through unprecedented effectiveness in automated fault detection. Its success lies in the coverage feedback extracted from the system under test, guiding the fuzzer to explore different program parts. The most prominent way to use this feedback is novelty search , where the fuzzer keeps only new inputs exercising a new program edge. However, this approach—by design—ignores input shadowing , in which interesting inputs are discarded if they do not contribute to new coverage. This limits the accepted input space and may overlook bugs that shadowed inputs could trigger with mutations. In this work, we present a comprehensive analysis of input shadowing and demonstrate that multiple fuzzing runs of the same target exhibit a different basic block hit frequency distribution despite overlapping code coverage. We propose fuzzer restarts to effectively redistribute basic block hit frequencies and show that this increases the overall achieved coverage on 15 evaluated targets on average by \\(9.5\\%\\) and up to \\(25.0\\%\\) . Furthermore, restarts help to find more bugs and trigger them more reliably. Overall, our results highlight the importance of considering input shadowing in the fuzzers’ design and the potential benefits of a restart-based strategy to enhance the performance of complex fuzzing methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406466697",
    "type": "article"
  },
  {
    "title": "Backsolver: Adapting Preceding Execution Paths to Solve Constraints for Concolic Execution",
    "doi": "https://doi.org/10.1145/3712194",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Y. J. Zeng; Zhanwei Song; Gang Lv; Yu Zhou; Hongsong Zhu; Limin Sun",
    "corresponding_authors": "",
    "abstract": "Concolic execution follows the execution paths of concrete inputs, capable of generating new inputs for unexplored code by solving negated path constraints. However, implicit flows can hinder concolic execution, reducing the code coverage. Implicit flows occur when inputs influence control flow, and the control flow variation affects the values of some variables. During concolic execution, the preceding path selections limit the potential values of these variables. This limitation may result in unsolvable constraints, subsequently restricting the generation of new inputs for unexplored paths. Our insight is that following the same preceding paths is unnecessary, and we can adapt preceding paths to make the latest constraints solvable. We divide states into general states and implicit-flow-solving states (IFSSs). We utilize the general states to perform concolic execution. When solving constraints influenced by implicit flows, we switch to the IFSSs. We use the IFSSs to explore the relevant code region and adapt paths. To mitigate path explosion and construct the relation between inputs and the variables, we merge the IFSSs. State merging does not burden the general states, and we limit the code regions for the IFSSs to minimize the introduced overhead. Finally, we replace the variable symbols in the target constraints with new expressions and attempt to solve the new constraints. We implement our approach in Backsolver and build a test suite to evaluate it. Backsolver successfully identifies all the implicit flows in the test suite and resolves most of them. When evaluated on 6 real-world binaries, Backsolver resolves the highest number of branches related to implicit flows in total. Besides, Backsolver has the highest code coverage in PlutoSVG and finds a 0-day vulnerability. We reported the vulnerability and obtained a CVE ID.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406466719",
    "type": "article"
  },
  {
    "title": "Making Fault Localization in Online Service Systems More Actionable and Interpretable",
    "doi": "https://doi.org/10.1145/3714466",
    "publication_date": "2025-01-22",
    "publication_year": 2025,
    "authors": "Ke Xv; Shikai Guo; Hui Li; C. H. Li; Rong Chen; Xiaochen Li; He Jiang",
    "corresponding_authors": "",
    "abstract": "Online service systems struggle with accurately and quickly pinpointing and resolving failures within their intricate systems, and it therefore emerges the solutions for fault localization in the code. However, the previous fault localization models suffer from low localization accuracy and poor interpretability due to the complex dependencies among fault characteristics in industrial practice. To address this issue, challenges brought by the long-distance dependencies among fault features and the unbalanced distribution of fault knowledge, and to improve the interpretability of the model, we present a fault localization model in online service systems more actionable and interpretable, named FL-AIer. Specifically, FL-AIer consists of two components: the feature encoding component and the fault localization component. The feature encoding component utilizes graph attention networks to capture the complex spatio-temporal dependencies within fault features. Then, the fault localization component adopts a three-stage approach, leveraging a multi-attention mechanism to identify and prioritize the most relevant fault features for precise localization. Additionally, the Fault Knowledge Balancing module it contains introduces a weighted Kullback-Leibler divergence loss function to ensure that the model pays adequate attention to all fault features, addressing the issue of imbalanced fault knowledge distribution and enhancing localization performance. We conducted extensive experiments on four datasets, and the results demonstrated that FL-AIer effectively addressing the challenges of fault localization in online system environments, and consistently outperforms the state-of-the-art methods across various evaluation metrics such as A@1, A@2, A@3, A@5, and MAR. For instance, FL-AIer achieves significant improvements of 5.82%, 10.77%, 4.20% and 15.56% on the A@1 metric, respectively. These results fully demonstrate the excellent effectiveness of FL-AIer in effectively addressing the challenges of fault localization in online system environments, surpassing the performance of existing state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406698715",
    "type": "article"
  },
  {
    "title": "<i>TAEFuzz</i> : Automatic Fuzzing for Image-based Deep Learning Systems via Transferable Adversarial Examples",
    "doi": "https://doi.org/10.1145/3714463",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Shunhui Ji; Congling Huang; Bin Ren; Hai Dong; Lars Grunske; Yan Xiao; Pengcheng Zhang",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) components have been broadly applied in diverse applications. Similar to traditional software engineering, effective test case generation methods are needed by industry to enhance the quality and robustness of these deep learning components. To this end, we propose a novel automatic software testing technique, TAEFuzz (Automatic Fuzz -Testing via T ransferable A dversarial E xamples), which aims to automatically assess and enhance the robustness of image-based deep learning (DL) systems based on test cases generated by transferable adversarial examples. TAEFuzz alleviates the over-fitting problem during optimized test case generation and prevents test cases from prematurely falling into local optima. In addition, TAEFuzz enhances the visual quality of test cases through constraining perturbations inserted into sensitive areas of the images. For a system with low robustness, TAEFuzz trains a low-cost denoising module to reduce the impact of perturbations in transferable adversarial examples on the system. Experimental results demonstrate that the test cases generated by TAEFuzz can discover up to 46.1% more errors in the targeted systems, and ensure the visual quality of test cases. Compared to existing techniques, TAEFuzz also enhances the robustness of the target systems against transferable adversarial examples with the perturbation denoising module.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406909213",
    "type": "article"
  },
  {
    "title": "µOpTime: Statically Reducing the Execution Time of Microbenchmark Suites Using Stability Metrics",
    "doi": "https://doi.org/10.1145/3715322",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Nils Japke; Martin Grambow; Christoph Laaber; David Bermbach",
    "corresponding_authors": "",
    "abstract": "Performance regressions have a tremendous impact on the quality of software. One way to catch regressions before they reach production is executing performance tests before deployment, e.g., using microbenchmarks, which measure performance at subroutine level. In projects with many microbenchmarks, this may take several hours due to repeated execution to get accurate results, disqualifying them from frequent use in CI/CD pipelines. We propose µOpTime, a static approach to reduce the execution time of microbenchmark suites by configuring the number of repetitions for each microbenchmark. Based on the results of a full, previous microbenchmark suite run, µOpTime determines the minimal number of (measurement) repetitions with statistical stability metrics that still lead to accurate results. We evaluate µOpTime with an experimental study on 14 open-source projects written in two programming languages and five stability metrics. Our results show that (i) µOpTime reduces the total suite execution time (measurement phase) by up to 95.83% (Go) and 94.17% (Java), (ii) the choice of stability metric depends on the project and programming language, (iii) microbenchmark warmup phases have to be considered for Java projects (potentially leading to higher reductions), and (iv) µOpTime can be used to reliably detect performance regressions in CI/CD pipelines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406957886",
    "type": "article"
  },
  {
    "title": "Beyond Decision: Android Malware Description Generation through Profiling Malicious Behavior Trajectory",
    "doi": "https://doi.org/10.1145/3715909",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Changhao Wu; Sen Chen; Jiaming Li; Renchao Chai; Lingling Fan; Xiaofei Xie; Ruitao Feng",
    "corresponding_authors": "",
    "abstract": "Malware family labels and key features used for the decision-making of Android malware detection models fall short of precise comprehension of malicious behaviors due to their coarse granularity. To solve these problems, in this paper, we first introduce the concept of the malicious behavior trajectory ( MBT ) and propose an innovative approach called ProMal . ProMal aims to automatically generate malware descriptions with fine granularity through extracted MBTs from malware for users. Specifically, a labeled dataset of MBTs is constructed through substantial human efforts to build a behavioral knowledge graph ( BxKG ). The BxKG is scalable and can be automatically updated using two strategies to ensure its completeness and timeliness: 1) taking into consideration the evolution of Android SDKs, and 2) mining new MBTs by leveraging the widely-used malware datasets. We highlight that the knowledge graph is essential in ProMal , which can reason new MBTs based on existing MBTs because of its structured data representation and semantic relation modeling, and thus helps effectively extract real MBTs in Android malware. We evaluated ProMal on a recent malware dataset where researcher-crafted malware descriptions are available, and the Precision, Recall, and F1-Score of MBT identification based on BxKG reached 96.97%, 91.43%, and 0.94, respectively, outperforming the state-of-the-art approaches. Taking MBTs identified from Android malware as inputs, precise, fine-grained, and human-readable descriptions can be generated using the large language model, whose readability and usability are verified through a user study. The generated descriptions play a significant role in interpreting and comprehending malware behaviors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407029455",
    "type": "article"
  },
  {
    "title": "Beyond Dependencies: The Role of Copy-Based Reuse in Open Source Software Development",
    "doi": "https://doi.org/10.1145/3715907",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Mahmoud Jahanshahi; David Reid; Audris Mockus",
    "corresponding_authors": "",
    "abstract": "In Open Source Software, resources of any project are open for reuse by introducing dependencies or copying the resource itself. In contrast to dependency-based reuse, the infrastructure to systematically support copy-based reuse appears to be entirely missing. Our aim is to enable future research and tool development to increase efficiency and reduce the risks of copy-based reuse. We seek a better understanding of such reuse by measuring its prevalence and identifying factors affecting the propensity to reuse. To identify reused artifacts and trace their origins, our method exploits World of Code infrastructure. We begin with a set of theory-derived factors related to the propensity to reuse, sample instances of different reuse types, and survey developers to better understand their intentions. Our results indicate that copy-based reuse is common, with many developers being aware of it when writing code. The propensity for a file to be reused varies greatly among languages and between source code and binary files, consistently decreasing over time. Files introduced by popular projects are more likely to be reused, but at least half of reused resources originate from “small” and “medium” projects. Developers had various reasons for reuse but were generally positive about using a package manager.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407030199",
    "type": "article"
  },
  {
    "title": "Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing",
    "doi": "https://doi.org/10.1145/3716167",
    "publication_date": "2025-02-04",
    "publication_year": 2025,
    "authors": "Thanh Le-Cong; Thanh-Dat Nguyen; Xuan-Bach D. Le; Toby Murray",
    "corresponding_authors": "",
    "abstract": "Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20%-35% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact. In this paper, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations, and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60% of these transformations are considered natural, while 20% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing. Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using Large Language Models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407137448",
    "type": "article"
  },
  {
    "title": "An Empirical Study on the Relationship Between Defects and Source Code’s Unnaturalness",
    "doi": "https://doi.org/10.1145/3718083",
    "publication_date": "2025-02-18",
    "publication_year": 2025,
    "authors": "Yanjie Jiang; Hui Liu; J. Liu; Yuxia Zhang; Weixing Ji; Hao Zhong; Lu Zhang",
    "corresponding_authors": "",
    "abstract": "Natural languages are “natural” in that texts in natural languages are repetitive and predictable. Recent research indicates that programming languages share similar characteristics (naturalness), with source code displaying patterns of repetition and predictability. Notably, studies have shown that buggy code deviates from these natural patterns in that buggy code is significantly less natural than bug-free one. In this paper, we conduct a large-scale and extensive empirical study to investigate whether code defects lead to unnaturalness of source code. Different from existing studies, we leverage multiple large-scale and high-quality bug repositories where bug-irrelevant changes in bug-fixing commits have been explicitly excluded. The leveraged software applications cover different programming languages, and the empirical study involves real-world software defects as well as defects injected automatically with well-known mutation operators. On one side, our evaluation results confirm existing studies in that buggy source code lines are often less natural than bug-free ones. On the other side, our evaluation reveals some interesting new findings. First, fixing bugs does not significantly improve the naturalness of code lines and the fixed lines on average are as unnatural as buggy ones. This finding may suggest that software defects are not the root causes of source code’s unnaturalness although there does existing statistically significant correlation between software defects and source code’s naturalness. Second, defects in different programming languages have similar effect on source code’s naturalness. The conclusions (i.e., buggy code is less natural but fixing the bugs cannot improve source code’s naturalness) hold regardless of the programming languages. Third, injecting defects automatically by well-known mutation operators does not significantly reduce the naturalness of involved source code lines. This suggests that automatically injected defects may have a similar impact on the naturalness of source code as real-world defects inadvertently introduced by developers. Fourth, the detects’ impact on source code’s naturalness varies slightly among different categories of software defects. Although fixing bugs on average does not significantly improve the naturalness of involved source code, fixing ”checking” related bugs does significantly improve the naturalness of source code. Finally, locating buggy code lines according to naturalness alone is inaccurate, resulting in extremely low precision (less than one percent).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407698941",
    "type": "article"
  },
  {
    "title": "PATCH: Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Simulation for Automatic Bug Fixing",
    "doi": "https://doi.org/10.1145/3718739",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Yuwei Zhang; Zhi Jin; Ying Xing; Ge Li; Fang Liu; Jiaxin Zhu; Wensheng Dou; Jun Wei",
    "corresponding_authors": "",
    "abstract": "Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs. We implement PATCH by employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407766864",
    "type": "article"
  },
  {
    "title": "An Empirical Study of Code Simplification Methods in Code Intelligence Tasks",
    "doi": "https://doi.org/10.1145/3720540",
    "publication_date": "2025-02-27",
    "publication_year": 2025,
    "authors": "Zongwen Shen; Y. Li; Jidong Ge; Xiang Chen; Chuanyi Li; LiGuo Huang; Bin Luo",
    "corresponding_authors": "",
    "abstract": "In recent years, pre-trained language models have seen significant success in natural language processing and have been increasingly applied to code-related tasks. Code intelligence tasks have shown promising performance with the support of code pre-trained language models. Pre-processing code simplification methods have been introduced to prune code tokens from the model’s input while maintaining task effectiveness. These methods improve the efficiency of code intelligence tasks while reducing computational costs. Post-prediction code simplification methods provide explanations for code intelligence task outcomes, enhancing the reliability and interpretability of model predictions. However, comprehensive evaluations of these methods across diverse code pre-trained model architectures and code intelligence tasks are lacking. To assess the effectiveness of code simplification methods, we conduct an empirical study integrating these code simplification methods with various pre-trained code models across multiple code intelligence tasks. Our empirical findings suggest that developing task-specific code simplification methods would be beneficial. Then, we recommend leveraging post-prediction methods to summarize prior knowledge, which can pre-process code simplification strategies. Moreover, establishing more evaluation mechanisms for code simplification is crucial. Finally, we propose incorporating code simplification methods into the pre-training phase of code pre-trained models to enhance their program comprehension and code representation capabilities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408021172",
    "type": "article"
  },
  {
    "title": "Characterizing Smart Contract Evolution",
    "doi": "https://doi.org/10.1145/3719004",
    "publication_date": "2025-02-27",
    "publication_year": 2025,
    "authors": "Xiangping Chen; Z. Qian; Peiyong Liao; Yuan Huang; C Yang; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "Smart contracts are programs that permanently store and automatically execute on the blockchain system such as Ethereum. Due to the non-tamperable nature of the underlying blockchain, smart contracts are difficult to update once deployed, which requires redeploying the contracts and migrating the data. It means that the observation of smart contract evolution in the real world makes more sense. Hence, in this paper, we conducted the first large-scale empirical study to characterize the evolution of smart contracts in Ethereum. For evolution identification, we presented a contract similarity-based search algorithm, digEvolution, and evaluated its effectiveness with five different search strategies. Then we applied this algorithm to 80,152 on-chain contracts we collected from Ethereum, to dig out the evolution among these contracts. We then explored three research questions. We first studied whether the evolution of smart contracts is common (RQ1), then we studied how do the Gas consumption (RQ2) and the vulnerability (RQ3) of smart contracts vary during the evolution. Our research results show that the evolution of smart contracts is not very common. There are some contract components that have vulnerability but still be called by users. The Gas consumption of most smart contracts doesn’t vary during the evolution, contract is Gas-efficient before and after the evolution. The vulnerability of most smart contracts doesn’t vary during the evolution, both are secure before and after the evolution.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408021714",
    "type": "article"
  },
  {
    "title": "Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs",
    "doi": "https://doi.org/10.1145/3721127",
    "publication_date": "2025-02-28",
    "publication_year": 2025,
    "authors": "Adam Alami; Victor Vadmand Jensen; Neil Ernst",
    "corresponding_authors": "",
    "abstract": "Accountability is an innate part of social systems. It maintains stability and ensures positive pressure on individuals’ decision-making. As actors in a social system, software developers are accountable to their team and organization for their decisions. However, the drivers of accountability and how it changes behavior in software development are less understood. In this study, we look at how the social aspects of code review affect software engineers’ sense of accountability for code quality. Since software engineering (SE) is increasingly involving Large Language Models (LLM) assistance, we also evaluate the impact on accountability when introducing LLM-assisted code reviews. We carried out a two-phased sequential qualitative study ( \\(\\textbf{interviews}\\rightarrow\\textbf{focus groups}\\) ). In Phase I (16 interviews), we sought to investigate the intrinsic drivers of software engineers influencing their sense of accountability for code quality, relying on self-reported claims. In Phase II, we tested these traits in a more natural setting by simulating traditional peer-led reviews with focus groups and then LLM-assisted review sessions. We found that there are four key intrinsic drivers of accountability for code quality: personal standards , professional integrity , pride in code quality , and maintaining one’s reputation . In a traditional peer-led review, we observed a transition from individual to collective accountability when code reviews are initiated. We also found that the introduction of LLM-assisted reviews disrupts this accountability process, challenging the reciprocity of accountability taking place in peer-led evaluations, i.e., one cannot be accountable to an LLM. Our findings imply that the introduction of AI into SE must preserve social integrity and collective accountability mechanisms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408028155",
    "type": "article"
  },
  {
    "title": "Experimental Evaluation of Parameter-Efficient Fine-Tuning for Software Engineering Tasks",
    "doi": "https://doi.org/10.1145/3722107",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Wentao Zou; Zongwen Shen; Qi Li; Jidong Ge; Chuanyi Li; Xiang Chen; Xiaoyu Shen; LiGuo Huang; Bin Luo",
    "corresponding_authors": "",
    "abstract": "Pre-trained models (PTMs) have succeeded in various software engineering (SE) tasks following the “pre-train then fine-tune” paradigm. As fully fine-tuning all parameters of PTMs can be computationally expensive, a potential solution is parameter-efficient fine-tuning (PEFT), which freezes PTMs while introducing extra parameters. Although PEFT methods have been applied to SE tasks, researchers often focus on specific scenarios and lack a comprehensive comparison of PTMs from different aspects such as field, size, and architecture. To fill this gap, we have conducted an empirical study on six PEFT methods, eight PTMs, and four SE tasks. The experimental results reveal several noteworthy findings. For example, model architecture has little impact on PTM performance when using PEFT methods. Additionally, we provide a comprehensive discussion of PEFT methods from three perspectives. First, we analyze the effectiveness and efficiency of PEFT methods. Second, we explore the impact of the scaling factor hyperparameter. Finally, we investigate the application of PEFT methods on the latest open-source large language model, Llama 3.2. These findings provide valuable insights to guide future researchers in effectively applying PEFT methods to SE tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408214724",
    "type": "article"
  },
  {
    "title": "Distinguishing GUI Component States for Blind Users using Large Language Models",
    "doi": "https://doi.org/10.1145/3722106",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Mengxi Zhang; Huaxiao Liu; Changhao Du; Tengmei Wang; Han Li; Pei Huang; Chunyang Chen",
    "corresponding_authors": "",
    "abstract": "Graphical User Interfaces (GUIs) serve as the primary medium for user interaction with mobile applications (apps). Within these GUIs, editable text views, buttons, and other visual elements exhibit different states following user actions. However, developers often present these states only in various colors without providing textual hints for blind users. This results in significant difficulties for blind users to discern the transitions in component states, thereby hindering their ability to proceed with subsequent actions. Traditional rule-based methods and attribute settings often struggle to adapt to diverse component styles and fail to address the component state changes influenced by context. Recently, pre-trained large language models (LLMs) have demonstrated their generalization ability to various downstream tasks. In this work, we leverage LLMs and propose a tool called CasGPT ( C omponent st a te s distinguishing GPT) to automatically distinguish component states in GUIs and provide corresponding textual hints, thereby aiding blind users in app usage. Our experiments demonstrate that CasGPT is a lightweight approach capable of accurately distinguishing component states (accuracy=86.5%). The usefulness of our method is validated through a user study, where participants expressed positive attitudes towards it. Also, we compare and find that our method outperforms other open-source LLMs and different versions of GPT.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408231987",
    "type": "article"
  },
  {
    "title": "Test Script Intention Generation for Mobile Application via GUI Image and Code Understanding",
    "doi": "https://doi.org/10.1145/3722105",
    "publication_date": "2025-03-11",
    "publication_year": 2025,
    "authors": "Shengcheng Yu; Chunrong Fang; Jia Liu; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Testing is the most direct and effective technique to ensure software quality. Test scripts always play a more important role in mobile app testing than test cases for source code, due to the GUI-intensive and event-driven characteristics of mobile applications (app). Test scripts focus on user interactions and the corresponding response events, which is significant for testing the target app functionalities. Therefore, it is critical to understand the test scripts for better script maintenance and modification. There exist some mature code understanding ( i.e., code comment generation, code summarization) technologies that can be directly applied to functionality source code with business logic. However, such technologies will have difficulties when being applied to test scripts, because test scripts are loosely linked to apps under test (AUT) by widget selectors, and do not contain business logic themselves. In order to solve the test script understanding gap, this paper presents a novel approach, namely TestIntention , to infer the intention of GUI test scripts. Test intention refers to the user expectations of app behaviors for specific operations. TestIntention formalizes test scripts with an operation sequence model. For each operation within the sequence, TestIntention extracts the target widget selector and links the selector to the GUI layout information or the corresponding response events. For widgets identified by XPath , TestIntention utilizes the image understanding technologies to explore the detailed information of the widget images, the intention of which is understood with a deep learning model. For widgets identified by ID , TestIntention first maps the selectors to the response methods with business logic, and then adopts code understanding technologies to describe code in natural language form. Results of all operations are combined to generate test intention for test scripts. An empirical experiment including different metrics proves the outstanding performance of TestIntention , outperforming baselines by much. Also, it is shown that TestIntention can save about 80% developers’ time to understand test scripts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408303204",
    "type": "article"
  },
  {
    "title": "Leveraging Risk Models to Improve Productivity for Effective Code Un-Freeze at Scale",
    "doi": "https://doi.org/10.1145/3722216",
    "publication_date": "2025-03-11",
    "publication_year": 2025,
    "authors": "Audris Mockus; Rui Abreu; Peter C. Rigby; David Amsallem; Parveen Bansal; Kaavya Chinniah; Brian Ellis; Peng Fan; Jun Ge; Bingjie He; Kohei HIRANO; S. Praneeth Kumar; Ajay Lingapuram; Andrew Loe; Megh Mehta; Venus Montes; Maher Saba; Gursharan Singh; M. Steiner; Weiyan Sun; Siri Uppalapati; Nachiappan Nagappan",
    "corresponding_authors": "",
    "abstract": "Changing software is essential to add needed functionality and to fix problems, but changes may introduce defects that lead to outages. This motivates one of the oldest software quality control techniques: a temporary prevention of non-critical changes to the codebase — code freeze. Despite its widespread use in practice, research literature is scant. Historically, code freezes were used as a way to improve software quality by preventing changes during periods before software releases, but code freezes significantly slow down development. To address this shortcoming we develop and evaluate a family of code un-freeze (permitting changes) strategies tailored to different occasions and products at Meta. They are designed to un-freeze the maximum amount of code without compromising quality. The three primary dimensions to un-freeze involve a) the exact timing of (and the reasoning behind it) the code freezes, b) the parts of the organization or the codebase where the codebase freeze is applied to, and c) the method of screening of the code diffs during the code freeze with the aim to allow low risk diffs and prevent only the most risky diffs. To operationalize the drivers of outages, we consider the entire network of interdependencies among different parts of the source code, the engineers that modify the code, code complexity, and the coordination dependencies and authors’ expertise. Since the code freeze is a balancing act between reducing outages and allowing software development to proceed unimpeded, the performance of the various approaches to code un-freeze is evaluated based on the fraction of flagged/gated changes to measure overhead and the fraction of all outage-causing changes contained within the set of flagged set of changes to measure the ability of the code un-freeze to delay (or prevent) outages. We found that taking into account the risk posed by modifying individual files and the properties of the change we could un-freeze two and \\(2.5\\) times more changes correspondingly. The change level model is used by Meta in production. For example, during the winter 2023 code freeze, we see that only 16% of changes are gated. Although 42% more changes landed (were integrated into the codebase) compared to the prior year, there was a 52% decrease in outages. This reduction meant less impact on users and less strain on engineers during the holiday period. The risk model has been enormously effective at allowing low risk changes to proceed while gating high risk changes and reducing outages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408303354",
    "type": "article"
  },
  {
    "title": "Variable Renaming-Based Adversarial Test Generation for Code Model: Benchmark and Enhancement",
    "doi": "https://doi.org/10.1145/3723353",
    "publication_date": "2025-03-14",
    "publication_year": 2025,
    "authors": "Wen Jin; Qiang Hu; Yuejun Guo; Maxime Cordy; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Robustness testing is essential for evaluating deep learning models, particularly under unforeseen circumstances. Adversarial test generation, a fundamental approach in robustness testing, is prevalent in computer vision and natural language processing, and it has gained considerable attention in code tasks recently. The Variable Renaming-Based Adversarial Test Generation (VRTG), which deceives models by altering variable names, is a key focus. VRTG involves substitution construction and variable name searching, but its systematic design remains a challenge due to the empirical nature of these components. This paper introduces the first benchmark to examine the impact of various substitutions and search algorithms on VRTG effectiveness, exploring improvements for existing VRTGs. Our benchmark includes three substitution construction types, six substitution position rank ways and seven search algorithms. Analysis of four code understanding tasks and three pre-trained code models using our benchmark reveals that combining RNNS and Genetic Algorithm with code-based substitution is more effective for VRTG construction. Notably, this method outperforms the advanced black-box variable renaming test generation technique, ALERT, by up to 22.57%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408488372",
    "type": "article"
  },
  {
    "title": "Evaluating Explanations Needs in Blockchain Smart Contracts to Reconcile Surprises",
    "doi": "https://doi.org/10.1145/3721283",
    "publication_date": "2025-03-18",
    "publication_year": 2025,
    "authors": "Hanouf Al Ghanmi; Sabreen Ahmadjee; Rami Bahsoon",
    "corresponding_authors": "",
    "abstract": "Smart contracts on the blockchain play an important role in decentralised systems by automating and executing agreements without the need for intermediaries. As these contracts become integral to various domains, ensuring users’ understanding of their functioning is paramount. This article investigates the need for explanations in smart contracts, drawing inspiration from contract law principles and established practices in Explainable Artificial intelligence (XAI). It introduces key purposes—justification, clarification, compliance and consent to design explainability. Additionally, the study proposes a novel assessment framework informed by the Metacognitive Explanation-Based (MEB) theory to systematically evaluate surprise potential in smart contracts lacking explanations. We use surprise as a guiding factor to systematically identify areas requiring improvement in terms of justification, clarification, compliance and consent. To demonstrate the utility of the assessment approach, we evaluate two decentralised lending projects, uncovering potential surprises. One of the key observations is the lack of setting information, especially concerning compliance, consent and decision justification. This absence of information has heightened the potential for surprises. In the process of validating the explanation purposes, we implement techniques to improve the design of the assessed smart contracts. Further, the research explores the trade-offs involved in integrating explanations, providing nuanced insights into economic implications such as increased deployment and execution costs. This work contributes to the broader comprehension of smart contract explainability requirements and lays out a theoretical foundation for a generic evaluation method. It aims to facilitate the development of more human-centric and comprehensible smart contracts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408561535",
    "type": "article"
  },
  {
    "title": "SILVA: A Scalable Incremental Layered Sparse Value-Flow Analysis",
    "doi": "https://doi.org/10.1145/3725214",
    "publication_date": "2025-03-20",
    "publication_year": 2025,
    "authors": "Jiayi Wang; Yu Wang; Ke Wang; Linzhang Wang",
    "corresponding_authors": "",
    "abstract": "Layered sparse value-flow analysis (SVFA) is a prominent static analysis for resolving program dependencies. Despite the significant progress, SVFA still suffers from scalability issue. In light of the natural, continuous evolution of software, we introduce SILVA, the first incremental layered sparse value-flow analysis that scales to large, real-world programs efficiently. At the core of SILVA lies a novel incremental pointer analysis and incremental Mod-Ref analysis. Our extensive experiments on large-scale, real-world C/C++ programs demonstrate its effectiveness: SILVA achieves nearly a 7x speedup over SVF, the state-of-the-art layered sparse value-flow analysis, without losing any precision. Moreover, our incremental pointer and Mod-Ref analysis algorithms are 12x and 5x faster than existing methods, respectively. Regarding the impact of the size of the code changes on SILVA’s effectiveness, we find that SILVA outperforms SVF for changes up to 10K lines—well beyond the typical scope of code commits in real-world software development.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408652505",
    "type": "article"
  },
  {
    "title": "Wenwang: Toward Effectively Generating Code Beyond Standalone Functions via Generative Pre-trained Models",
    "doi": "https://doi.org/10.1145/3725213",
    "publication_date": "2025-03-20",
    "publication_year": 2025,
    "authors": "Hao Yu; Bo Shen; J. Y. Zhang; Shaoxin Lin; Lin Li; Guangtai Liang; Ying Li; Qianxiang Wang; Tao Xie",
    "corresponding_authors": "",
    "abstract": "Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. After being pre-trained on a large-scale corpus of code, a model is further fine-tuned with datasets specifically for the target downstream task, e.g., generating code from natural language description. The target code being generated can be classified into two types: a standalone function, i.e., a function that invokes or accesses only built-in functions and standard libraries, and a non-standalone function, i.e., a function that invokes or accesses user-defined functions or third-party libraries. To effectively generate code especially non-standalone functions (largely ignored by existing work), in this article, we present Wenwang, an approach to improving the capability of a pre-trained model on generating code beyond standalone functions. Wenwang consists of two components: a fine-tuning dataset named WenwangData and a fine-tuned model named WenwangCoder. Compared with existing fine-tuning datasets, WenwangData additionally covers non-standalone functions. Besides the docstring and code snippet for a function, WenwangData also includes its contextual information collected via program analysis. Based on PanGu-Coder, we produce WenwangCoder by fine-tuning PanGu-Coder on WenwangData with our context-aware fine-tuning technique so that the contextual information can be fully leveraged during code generation. On CoderEval and HumanEval, WenwangCoder outperforms three state-of-the-art models with similar parameter sizes (at the scale of around 300M), namely CodeGen, PanGu-Coder, and PanGu-FT. Although WenwangCoder does not outperform ChatGPT on HumanEval, WenwangCoder with smaller model parameter sizes can achieve similar effects to ChatGPT on CoderEval. Our experimental results also shed light on a number of promising optimization directions based on existing pre-trained models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408652593",
    "type": "article"
  },
  {
    "title": "LEAM++: Learning for Selective Mutation Fault Construction",
    "doi": "https://doi.org/10.1145/3725528",
    "publication_date": "2025-03-21",
    "publication_year": 2025,
    "authors": "Zhao Tian; Junjie Chen; Dong Wang; Qihao Zhu; Xingyu Fan; Lingming Zhang",
    "corresponding_authors": "",
    "abstract": "Mutation faults are the core of mutation testing and have been widely used in many software testing tasks. Hence, efficiently constructing high-quality mutation faults is critical. To address the effectiveness limitations of traditional and deep learning-based mutation techniques, we first proposed LEAM , utilizing a syntax-guided encoder-decoder architecture with extended grammar rules. While LEAM significantly enhances the effectiveness, it does not consider the associated testing cost. To further improve the efficiency of LEAM , we propose LEAM++ , adopting a novel selective mutation fault construction module based on the probability of grammar rule sequences and the similarity of mutation faults. We extensively evaluate LEAM++ using the Defects4J. Regarding effectiveness, the results demonstrate that the mutation faults constructed by LEAM++ can better represent real faults than two traditional techniques ( Major and PIT ) and the deep learning-based technique ( DeepMutation ), and substantially boost three downstream applications, i.e., mutation-based test case prioritization, mutation-based fault localization, and mutation-based bug detection. Regarding efficiency, LEAM++ demonstrates superiority over the four selective mutation testing techniques across three scenarios, i.e., mutation testing, mutation-based test case prioritization, and mutation-based fault localization. Our work serves as an important step toward the efficiently automated construction of mutation faults.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408694922",
    "type": "article"
  },
  {
    "title": "Why Personalizing Deep Learning-Based Code Completion Tools Matters",
    "doi": "https://doi.org/10.1145/3725732",
    "publication_date": "2025-03-26",
    "publication_year": 2025,
    "authors": "Alessandro Giagnorio; Alberto Martin-Lopez; Gabriele Bavota",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL)-based code completion tools have revolutionized software development by providing unprecedented code generation capabilities. The DL models behind these tools are usually trained on large amounts of code from thousands of software repositories. This makes them good in learning natural coding patterns observed across many training instances. However, little is known about the extent to which additional training effort (fine-tuning) aimed at specializing the models towards the code base of a given organization/developer further benefits their code completion capabilities. In this work, we fill this gap by presenting solid empirical evidence answering this question. More specifically, we consider 136 developers from two organizations (Apache and Spring), two model architectures (T5 and Code Llama), and three model sizes (60M, 750M, and 7B trainable parameters). For T5 models (60M, 750M), we pre-train and fine-tune them on over 2,000 open source projects, making sure that code from the two subject organizations is not part of their training sets. Then, we compare their completion capabilities against the same models further fine-tuned on organization- and developer-specific datasets. For the Code Llama model (7B), we compare the performance of the already pre-trained model publicly available online with the same model fine-tuned via parameter-efficient fine-tuning on organization- and developer-specific datasets. Our results show that there is a boost in prediction capabilities provided by both an organization-specific and a developer-specific additional fine-tuning, with the former being particularly performant. Such a finding generalizes across (i) the two subject organizations ( i.e. , Apache and Spring) and (ii) models of completely different magnitude (from 60M to 7B trainable parameters). Finally, we show that DL models fine-tuned on an organization-specific dataset achieve the same completion performance of pre-trained code models used out of the box and being \\(\\sim\\) 10 \\(\\times\\) larger, with consequent savings in terms of deployment and inference cost ( e.g. , smaller GPUs needed).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408851842",
    "type": "article"
  },
  {
    "title": "Towards On-The-Fly Code Performance Profiling",
    "doi": "https://doi.org/10.1145/3725212",
    "publication_date": "2025-03-26",
    "publication_year": 2025,
    "authors": "Xing Hu; W.C. Lin; Zhuang Liu; Xin Xia; Michael Ling; Yuan Wang; David Lo",
    "corresponding_authors": "",
    "abstract": "Improving the performance of software applications is one of the most important tasks in software evolution and maintenance. In the Intel Microarchitecture, CPUs employ pipelining to utilize resources as effectively as possible. Some types of software patterns or algorithms can have implications on the underlying CPU pipelines and result in inefficiencies. Therefore, analyzing how well the CPU’s pipeline(s) are being utilized while running an application is important in software performance analysis. Existing techniques, such as Intel VTune Profiler, usually detect software performance issues from CPU pipeline metrics after the software enters production and during the running time. These techniques require developers to manually analyze monitoring data and perform additional test runs to obtain relevant information about performance problems. It costs a lot of time and human effort for developers to build, deploy, test, execute, and monitor the software. To alleviate these problems, we propose a novel approach named PGProf to predict the CPU pipeline before execution and provide the profiling feedback during the development process. PGProf exploits the graph neural networks to learn semantic and structural representations for C functions and then predict the fraction of pipeline slots in each category for them during the development process. Given a code snippet, we fuse different types of code structures, e.g., Abstract Syntax Tree (AST), Data Flow Graph (DFG), and Control Flow Graph (CFG) into one program graph. During offline learning, we first leverage the gated graph neural network to capture representations of C functions. PGProf then automatically estimates the final pipeline values according to the learned semantic and structural features. For online prediction, we predict pipeline metrics with four category values by leveraging the offline trained model. We build our dataset from C projects in GitHub and use Intel VTune profiler to get profiling information by running them. Extensive experimental results show the promising performance of our model. We achieved absolute result of 49.90% and 79.44% in terms of \\(Acc@5\\%\\) and \\(Acc@10\\%\\) with improvements of 8.0%-42.7% and 7.8%-20.1% over a set of baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408851891",
    "type": "article"
  },
  {
    "title": "Test Oracle Generation for REST APIs",
    "doi": "https://doi.org/10.1145/3726524",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Juan C. Alonso; Michael D. Ernst; Sergio Segura; Antonio Ruiz–Cortés",
    "corresponding_authors": "",
    "abstract": "The number and complexity of test case generation tools for REST APIs have significantly increased in recent years. These tools excel in automating input generation but are limited by their test oracles, which can only detect crashes, regressions, and violations of API specifications or design best practices. This article introduces AGORA+, an approach for generating test oracles for REST APIs through the detection of invariants—output properties that should always hold. AGORA+ learns the expected behavior of an API by analyzing API requests and their corresponding responses. We enhanced the Daikon tool for dynamic detection of likely invariants, adding new invariant types and creating a front-end called Beet. Beet translates any OpenAPI specification and a set of API requests and responses into Daikon inputs. AGORA+ can detect 106 different types of invariants in REST APIs. We also developed PostmanAssertify, which converts the invariants identified by AGORA+ into executable JavaScript assertions. AGORA+ achieved a precision of 80% on 25 operations from 20 industrial APIs. It also identified 48% of errors systematically seeded in the outputs of the APIs under test. AGORA+ uncovered 32 bugs in popular APIs, including Amadeus, Deutschebahn, GitHub, Marvel, NYTimesBooks, and YouTube, leading to fixes and documentation updates.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408938118",
    "type": "article"
  },
  {
    "title": "Understanding Mirror Bugs in Multiple-Language Projects",
    "doi": "https://doi.org/10.1145/3727145",
    "publication_date": "2025-03-31",
    "publication_year": 2025,
    "authors": "Ye Tang; Honghao Chen; Z. He; Hao Zhong",
    "corresponding_authors": "",
    "abstract": "As software is widely used in daily life, bugs can introduce catastrophic consequences. Researchers have conducted empirical studies to delve into bug characteristics, exploring topics such as buggy locations, symptoms, causes, and repair patterns. To attract users, many applications have implementations in different languages. If an implementation has a bug, other implementations can have similar bugs. In this paper, we call such cross-language clone bugs mirror bugs. Understanding mirror bugs is crucial, as they offer insights into broader bug patterns. Still, no prior study has explored mirror bugs, leaving several research questions unanswered. For instance, can bug fixes in one language help detect and repair bugs in other languages? To address these questions, we conducted the first empirical study analyzing mirror bugs. Our investigation focused on 638 bugs from four projects, implemented in both Java and C#. Our study presents answers to five interesting research questions. For instance, some programmers actively fix mirror bugs even without tool support. Consequently, there is a timely need for tools that assist in detecting mirror bugs. Following this insight, we manually identified and implemented the patches of 9 new mirror bugs. Among them, 5 patches are already accepted by programmers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408998386",
    "type": "article"
  },
  {
    "title": "Autonomous Driving System Testing via Diversity-Oriented Driving Scenario Exploration",
    "doi": "https://doi.org/10.1145/3727875",
    "publication_date": "2025-04-09",
    "publication_year": 2025,
    "authors": "Xinyu Ji; Lei Xue; Zhijian He; Xiapu Luo",
    "corresponding_authors": "",
    "abstract": "Testing Autonomous Driving Systems (ADS) is critical for validating their safety in operational environments. High-fidelity simulators enable the testing of ADS through virtual driving scenarios, especially those that are hazardous to replicate in real-world settings. However, existing testing approaches suffer from inadequate coverage of real-world traffic situations due to over-simplified modeling of vehicle movements (e.g., insufficient diversity in driving styles), resulting in undetected critical ADS failures. In this paper, we propose a testing framework to discover diverse failures of ADS in driving scenarios that embody real-world traffic complexity. The framework leverages advanced traffic simulation methods to encode vehicle movements and generates realistic yet safety-critical driving scenarios for ADS by mutating vehicle movements. To efficiently explore driving scenarios that pose different challenges for ADS and expose diverse ADS failures, this framework further leverages a dynamic prioritization mechanism that prioritizes vehicle movements likely to trigger unique ADS behaviors. Specifically, we propose a method to estimate the possibility based on encoded vehicle movements. We implement this framework and evaluate it with three representative ADS from the famous CARLA leaderboard. Empirical evaluation demonstrates that the proposed approach discovers more unique failures of ADS than existing testing frameworks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409312638",
    "type": "article"
  },
  {
    "title": "Improving Test Efficacy for Large-Scale Android Applications by Exploiting GUI and Functional Equivalence",
    "doi": "https://doi.org/10.1145/3729225",
    "publication_date": "2025-04-11",
    "publication_year": 2025,
    "authors": "Yifei Lu; Minxue Pan; Haochuan Lu; Yuetang Deng; Tian Zhang; Linzhang Wang; Xuandong Li",
    "corresponding_authors": "",
    "abstract": "Large-scale Android apps that provide complex functions are gradually becoming the mainstream in Android app markets. They tend to display many GUI widgets on a single GUI page, which, unfortunately, can cause more redundant test actions—actions with similar functions—to automatic testing approaches. The effectiveness of existing testing approaches is still limited, suggesting the necessity of reducing the test effort on redundant actions. In this paper, we first identify three types of GUI structures that can cause redundant actions and then propose a novel approach, called action equivalence evaluation, to find the actions with similar functions by exploiting both GUI structure and functionality. By integrating this approach with existing testing tools, the test efficacy can be improved. We conducted experiments on 17 large-scale Android apps, including three industrial apps Google News , Messenger , and WeChat . The results show that more instructions can be covered, and more crashes can be detected, compared to the state-of-the-art Android testing tools. 29 real bugs were found in our experiment, and moreover, 760 bugs over 40 versions of WeChat had been detected in the real test environment during a three-month testing period.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409359698",
    "type": "article"
  },
  {
    "title": "Resolving Conditional Implicit Calls to Improve Static and Dynamic Analysis in Android Apps",
    "doi": "https://doi.org/10.1145/3729168",
    "publication_date": "2025-04-17",
    "publication_year": 2025,
    "authors": "Jordan Samhi; René Just; Michael D. Ernst; Tegawendé F. Bissyandé; Jacques Klein",
    "corresponding_authors": "",
    "abstract": "An implicit call is a mechanism that triggers the execution of a method \\(m\\) without a direct call to \\(m\\) in the code being analyzed. For instance, in Android apps the Thread.start() method implicitly executes the Thread.run() method. These implicit calls can be conditionally triggered by programmer-specified constraints that are evaluated at run time. For instance, the JobScheduler.schedule() method can be called to implicitly execute the JobService.onStartJob() method only if the device's battery is charging. Such conditional implicit calls can effectively disguise logic bombs , posing significant challenges for both static and dynamic software analyses. Conservative static analysis may produce false-positive alerts due to over-approximation, while less conservative approaches might overlook potential covert behaviors, a serious concern in security analysis. Dynamic analysis may fail to generate the specific inputs required to activate these implicit call targets. To address these challenges, we introduce Archer, a tool designed to resolve conditional implicit calls and extract the constraints triggering execution control transfer. Our evaluation reveals that ① implicit calls are prevalent in Android apps; ② Archer enhances app models’ soundness beyond existing static analysis methods; and ③ Archer successfully infers constraint values, enabling dynamic analyzers to detect (i.e., thanks to better code coverage) and assess conditionally triggered implicit calls.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409538210",
    "type": "article"
  },
  {
    "title": "Privacy in Chatbot Conversation-Driven Development: A Comprehensive Review and Requirements Proposal",
    "doi": "https://doi.org/10.1145/3730578",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Geovana Ramos Sousa Silva; Edna Dias Canedo",
    "corresponding_authors": "",
    "abstract": "Ensuring data privacy is a major challenge for software developers, especially in chatbots, where balancing privacy protection with response quality is key, given the need for conversation-driven development and data protection regulations. This research identifies privacy requirements and techniques for chatbot development through a literature review, privacy policy analysis, and a practitioner survey. The methodology includes a Systematic Literature Review (SLR), an adapted Gray Literature Review (GLR), privacy requirement formulation, and validation via a survey. Based on the SLR and GLR, eight privacy requirements are proposed, covering personal information protection, user authentication, access control, secure communication, database safety, user rights empowerment, decentralized storage, and reliable infrastructure. Survey results highlight foundational measures like secure communication and scalable infrastructures as priorities, while advanced measures such as decentralized storage or privacy rights implementation scored lower due to complexity and cost. Practitioners also stressed clarity and verifiability, citing gaps in definitions, examples, and validation criteria as challenges to adoption.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409584068",
    "type": "review"
  },
  {
    "title": "FairGenerate: Enhancing Fairness Through Synthetic Data Generation and Two-Fold Biased Labels Removal",
    "doi": "https://doi.org/10.1145/3730579",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Hem Chandra Joshi; Sandeep Kumar",
    "corresponding_authors": "",
    "abstract": "We are increasingly using machine learning (ML) software to make autonomous decisions such as identifying credit risks, criminal sentencing, discharge of a patient, predicting heart diseases, hiring employees, etc. However, the biased decision made by ML software against specific social groups based on protected/sensitive attributes (e.g., sex) has raised concern among software engineering (SE) and ML communities. ML software builds its decision logic from the training data. Consequently, if it is trained on biased data, it can make biased decisions. Previous studies have reported ‘biased labels’ and ‘imbalanced data’ as the root causes of biases in the training dataset. In this study, we propose FairGenerate, a pre-processing method that (a) balances the internal distribution of training datasets based on class labels and sensitive attributes by generating synthetic data samples using differential evolution and (b) identifies the biased labels through situation testing and removes them before and after synthetic data generation, hence the development of fair ML software. The experiments carried out in this study show that our proposed approach, FairGenerate, can attain markedly improved fairness (measured across various metrics) without compromising (original) the model performance over five benchmark methods. FairGenerate outperforms the fairness and performance trade-off baseline set by the benchmarking tool Fairea in 65% of cases, compared to the state-of-the-art method, which achieves this in only 59% of cases. To promote open science, we provide all the scripts and data utilized in this work at https://github.com/xyz745/FairGenerate .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409584136",
    "type": "article"
  },
  {
    "title": "PriCod: Prioritizing Test Inputs for Compressed Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3730435",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Yinghua Li; Xueqi Dang; Jacques Klein; Yves Le Traon; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "The widespread adoption of deep neural networks (DNNs) has brought remarkable advances in machine learning. However, the computational and memory demands of complex DNNs hinder their deployment in resource-constrained environments. To address this challenge, compressed DNN models have emerged, offering a compromise between efficiency and accuracy. Nonetheless, assessing the performance of these compressed models can demand extensive testing, typically requiring high manual labeling costs, rendering the process resource-intensive and time-consuming. To mitigate these challenges, test input prioritization has emerged as a promising technique aimed at reducing labeling costs by prioritizing inputs that are more likely to be misclassified. This enables the early identification of bug-revealing tests with reduced time and manual labeling effort. In this paper, we propose PriCod, a novel test prioritization approach designed for compressed DNNs. PriCod leverages the behavior disparities caused by model compression, along with the embeddings of test inputs, to effectively prioritize potentially misclassified tests. It operates on the premises that significant behavior disparities between the models indicate potential misclassifications and that inputs near decision boundaries are more likely to be misclassified. To this end, PriCod generates two types of features for each test input (i.e., deviation features and embedding features) to capture the prediction deviation caused by model compression and the proximity to decision boundaries, respectively. By combining these features, PriCod predicts the probability of misclassification for each test, ranking tests accordingly. We conduct an extensive study to evaluate the effectiveness of PriCod, comparing it with multiple test prioritization approaches. The experimental results demonstrate the effectiveness of PriCod, with average improvements of 7.43%~55.89% on natural test inputs, 7.92%~52.91% on noisy test inputs, and 7.03%~51.59% on adversarial test inputs, compared with existing test prioritization approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409584649",
    "type": "article"
  },
  {
    "title": "Is Fault Localization Effective on Industrial Software? A Case Study on Computer-Aided Engineering Projects",
    "doi": "https://doi.org/10.1145/3731448",
    "publication_date": "2025-04-21",
    "publication_year": 2025,
    "authors": "Zhilei Ren; Yue Ma; Xiaochen Li; Shikai Guo; He Jiang",
    "corresponding_authors": "",
    "abstract": "In software engineering, empirical studies on automated fault localization (FL) methods mainly focus on general software, and substantial progress have been made. However, the applicability and efficacy of these methods in specialized, domain-specific software like industrial software remains under-explored. Such specialized software is usually characterized by complex inputs and iterative computing paradigms, which could significantly influence the effectiveness of existing FL methods. To address this gap, this study takes a typical categorical of industrial software (i.e., computer-aided engineering (CAE) projects) as a case study, to investigate the feasibility and effectiveness of state-of-the-art FL methods within CAE projects. Through the reproduction of 76 real-world bugs from three widely used CAE projects (i.e., FDS, deal.II, and MFEM), we find that even the most precise FL methods require developers to examine on average 467.18 statements before finding bugs, and can take 208.13 hours to execute. The complex inputs and long-term computation characteristics of CAE projects further increase the difficulty of FL. Moreover, FL on CAE also faces challenges, such as insufficient differentiation of coverage information and missing CAE-specific FL features. Based on our findings, we improve FL on CAE projects by proposing a set of CAE main module based features, which improve the best-performed FL method in this study (i.e., DeepFL) by 35.93% and 45%, in terms of MAR and MFR , respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409643784",
    "type": "article"
  },
  {
    "title": "FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs",
    "doi": "https://doi.org/10.1145/3731449",
    "publication_date": "2025-04-22",
    "publication_year": 2025,
    "authors": "Xiuwei Shang; Guoqiang Chen; Shaoyin Cheng; Shikai Guo; Yanming Zhang; Weiming Zhang; Nenghai Yu",
    "corresponding_authors": "",
    "abstract": "Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task, which is crucial in software security fields such as malware analysis and legacy code inspection. However, the inherent high logical complexity of cryptographic algorithms makes their analysis more difficult than that of ordinary code, and the general absence of symbolic information in binaries exacerbates this challenge. Existing methods for cryptographic algorithm identification frequently rely on data or structural pattern matching, which limits their generality and effectiveness while requiring substantial manual effort. In response to these challenges, we present FoC ( F igure o ut the C ryptographic functions), a novel framework that leverages large language models (LLMs) to identify and analyze cryptographic functions in stripped binaries. In FoC, we first build an LLM-based generative model ( FoC-BinLLM ) to summarize the semantics of cryptographic functions in natural language form, which is intuitively readable to analysts. Subsequently, based on the semantic insights provided by FoC-BinLLM, we further develop a binary code similarity detection model ( FoC-Sim ), which allows analysts to effectively retrieve similar implementations of unknown cryptographic functions from a library of known cryptographic functions. The predictions of generative model like FoC-BinLLM are inherently difficult to reflect minor alterations in binary code, such as those introduced by vulnerability patches. In contrast, the change-sensitive representations generated by FoC-Sim compensate for the shortcomings to some extent. To support the development and evaluation of these models, and to facilitate further research in this domain, we also construct a comprehensive cryptographic binary dataset and introduce an automatic method to create semantic labels for extensive binary functions. Our evaluation results are promising. FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score, demonstrating superior capability in summarizing the semantics of cryptographic functions. FoC-Sim also surpasses previous best methods with a 52% higher Recall@1 in retrieving similar cryptographic functions. Beyond these metrics, our method has proven its practical utility in real-world scenarios, including cryptographic-related virus analysis and 1-day vulnerability detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409664068",
    "type": "article"
  },
  {
    "title": "A 2030 Roadmap for Software Engineering",
    "doi": "https://doi.org/10.1145/3731559",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Mauro Pezzè; Silvia Abrahão; Birgit Penzenstadler; Denys Poshyvanyk; Abhik Roychoudhury; Tao Yue",
    "corresponding_authors": "",
    "abstract": "The landscape of software engineering has dramatically changed in recent years. The impressive advances of artificial intelligence are just the latest and most disruptive innovation that has remarkably changed the software engineering research and practice. This special issue shares a roadmap to guide the software engineering community in this confused era. This roadmap is the outcome of a two-day intensive discussion at the 2030 Software Engineering workshop. The roadmap spotlights and discusses seven main landmarks in the new software engineering landscape: artificial intelligence and software engineering, human aspects of software engineering, software security, verification and validation, sustainable software engineering, and quantum software engineering. This editorial summarizes the core aspects discussed in the 37 papers that comprise the seven sections of the special issue and guides the interested readers throughout the issue. This roadmap is a living body that we will refine with follow-up workshops that will update the roadmap for a series of forthcoming ACM TOSEM special issues.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409797578",
    "type": "article"
  },
  {
    "title": "LogUpdater: Automated Detection and Repair of Specific Defects in Logging Statements",
    "doi": "https://doi.org/10.1145/3731754",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Renyi Zhong; Yichen Li; Jinxi Kuang; Wenwei Gu; Yintong Huo; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Developers write logging statements to monitor software runtime behaviors and system state. However, poorly constructed or misleading log messages can inadvertently obfuscate actual program execution patterns, thereby impeding effective software maintenance. Existing research on analyzing issues within logging statements is limited, primarily focusing on detecting a singular type of defect and relying on manual intervention for fixes rather than automated solutions. To address the limitation, we initiate a systematic study that pinpoints four specific types of defects in logging statements (i.e., statement code inconsistency, static dynamic inconsistency, temporal relation inconsistency, and readability issues) through the analysis of real-world log-centric changes. We then propose LogUpdater , a two-stage framework for automatically detecting and updating logging statements for these specific defects. In the offline stage, LogUpdater constructs a similarity-based classifier on a set of synthetic defective logging statements to identify specific defect types. During the online testing phase, this classifier first evaluates logging statements in a given code snippet to determine the necessity and type of improvements required. Then, LogUpdater constructs type-aware prompts from historical logging update changes for an LLM-based recommendation framework to suggest updates addressing these specific defects. We evaluate the effectiveness of LogUpdater on a dataset containing real-world logging changes, a synthetic dataset, and a new real-world project dataset. The results indicate that our approach is highly effective in detecting logging defects, achieving an F1 score of 0.625. Additionally, it exhibits significant improvements in suggesting precise static text and dynamic variables, with enhancements of 48.12% and 24.90%, respectively. Furthermore, LogUpdater achieves a 61.49% success rate in recommending correct updates on new real-world projects. We reported 40 problematic logging statements and their fixes to GitHub via pull requests, resulting in 25 changes confirmed and merged across 11 different projects.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409797596",
    "type": "article"
  },
  {
    "title": "Bipartite-Grammar Aware Pretraining for XML-SQL Code Updating",
    "doi": "https://doi.org/10.1145/3731752",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Qingyuan Liang; Zeyu Sun; Yifan Zhao; Zhihao Gong; Guoqing Wang; Yizhou Chen; Lu Zhang; Guangtai Liang; Qianxiang Wang",
    "corresponding_authors": "",
    "abstract": "The e X tensible M arkup L anguage (XML) is a file format widely used for data transmission in modern software development. In recent years, embedding SQL statements in XML files (i.e., XML-SQL) has become a popular way for developing applications with database access capability. Typically, XML-SQL code snippets demonstrate similar functionalities and structures, leading to repetitive programming work. Therefore, leveraging pre-trained code models for automated code generation presents a promising way to alleviate duplicated efforts and enhance the efficiency of developing XML-SQL code. However, XML-SQL code has strong domain-specific characteristics that general pre-trained code models typically struggle to fully harness, thereby leading to limited overall performance of general pre-trained code models. In this paper, we aim to address the challenge of handling this domain-specific knowledge. First, we propose a code updating task and construct the corresponding TwinXSQL dataset to better evaluate the model’s code generation performance in the XML-SQL domain. Then, we leverage the common characteristics of XML-SQL and other programming languages (i.e., all programming languages impose grammar constraints on behaviour) to design a bipartite-grammar-aware training framework (named BGA) for unsupervised pre-training, thereby improving the transfer of general-purpose code models to the XML-SQL domain. Specifically, we divide the XML-SQL code into two types of grammatical components: structure components and value components. During pre-training, we undertake three tasks, each designed to learn the internal information of these grammatical components and the relationships between them, enabling the pre-training process to better incorporate previously unlearned domain-specific knowledge of XML-SQL code. Our experimental results show that our trained model XSQLT5-base (220M) improves accuracy by 13.8% compared to the similarly sized CodeT5-base (220M). Additionally, our experiments reveal that ChatGPT, due to its inability to fully learn the XML-SQL domain knowledge, achieves a much lower generation accuracy even with few-shot samples compared to our XSQLT5-base (220M) model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409920472",
    "type": "article"
  },
  {
    "title": "Fuzzing: On Benchmarking Outcome as a Function of Benchmark Properties",
    "doi": "https://doi.org/10.1145/3732936",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Dylan Wolff; Marcel Böhme; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "In a typical experimental design in fuzzing, we would run two or more fuzzers on an appropriate set of benchmark programs plus seed corpora and consider their ranking in terms of code coverage or bugs found as outcome. However, the specific characteristics of the benchmark setup clearly can have some impact on the benchmark outcome. If the programs were larger, or these initial seeds were chosen differently, the same fuzzers may be ranked differently; the benchmark outcome would change. In this paper, we explore two methodologies to quantify the impact of the specific properties on the benchmarking outcome . This allows us to report the benchmarking outcome counter-factually, e.g., “If the benchmark had larger programs, this fuzzer would outperform all others”. Our first methodology is the controlled experiment to identify a causal relationship between a single property in isolation and the benchmarking outcome. The controlled experiment requires manually altering the fuzzer or system under test to vary that property while holding all other variables constant. By repeating this controlled experiment for multiple fuzzer implementations, we can gain detailed insights to the different effects this property has on various fuzzers. However, due to the large number of properties and the difficulty of realistically manipulating one property exactly, control may not always be practical or possible. Hence, our second methodology is randomization and non-parametric regression to identify the strength of the relationship between arbitrary benchmark properties (i.e., covariates) and outcome. Together, these two fundamental aspects of experimental design, control and randomization , can provide a comprehensive picture of the impact of various properties of the current benchmark on the fuzzer ranking. These analyses can be used to guide fuzzer developers towards areas of improvement in their tools and allow researchers to make more nuanced claims about fuzzer effectiveness. We instantiate each approach on a subset of properties suspected of impacting the relative effectiveness of fuzzers and quantify the effects of these properties on the evaluation outcome. In doing so, we identify multiple properties, such as the coverage of the initial seed-corpus and the program execution speed, which can have statistically significant effect on the relative effectiveness of fuzzers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409922298",
    "type": "article"
  },
  {
    "title": "When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair",
    "doi": "https://doi.org/10.1145/3733599",
    "publication_date": "2025-05-01",
    "publication_year": 2025,
    "authors": "Wenqiang Luo; Jacky Keung; Boyang Yang; He Ye; Claire Le Goues; Tegawendé F. Bissyandé; Haoye Tian; Xuan-Bach D. Le",
    "corresponding_authors": "",
    "abstract": "Software systems have been evolving rapidly and inevitably introducing bugs at an increasing rate, leading to significant maintenance costs. While large language models (LLMs) have demonstrated remarkable potential in enhancing software development and maintenance practices, particularly in automated program repair (APR), they rely heavily on high-quality code repositories. Most code repositories are proprietary assets that capture the diversity and nuances of real-world industry software practices, which public datasets cannot fully represent. However, obtaining such data from various industries is hindered by data privacy concerns, as companies are reluctant to share their proprietary codebases. There has also been no in-depth investigation of collaborative software development by learning from private and decentralized data while preserving data privacy for program repair. To address the gap, we investigate federated learning as a privacy-preserving method for fine-tuning LLMs on proprietary and decentralized data to boost collaborative software development and maintenance. We use the private industrial dataset TutorCode for fine-tuning and the EvalRepair-Java benchmark for evaluation, and assess whether federated fine-tuning enhances program repair. We then further explore how code heterogeneity (i.e., variations in coding style, complexity, and embedding) and different federated learning algorithms affect bug fixing to provide practical implications for real-world software development collaboration. Our evaluation reveals that federated fine-tuning can significantly enhance program repair, achieving increases of up to 16.67% for Top@10 and 18.44% for Pass@10, even comparable to the bug-fixing capabilities of centralized learning. Moreover, the negligible impact of code heterogeneity implies that industries can effectively collaborate despite diverse data distributions. Different federated algorithms also demonstrate unique strengths across LLMs, suggesting that tailoring the optimization process to specific LLM characteristics can further improve program repair.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410008025",
    "type": "article"
  },
  {
    "title": "Exploring JVM Garbage Collector Testing with Event-Coverage",
    "doi": "https://doi.org/10.1145/3733598",
    "publication_date": "2025-05-01",
    "publication_year": 2025,
    "authors": "Kai Zheng; Zan Wang; Yingquan Zhao; Junjie Chen; Hanmo You; Haoyu Wang; Yong Du; Tianchang Gao",
    "corresponding_authors": "",
    "abstract": "Garbage Collection (GC) in the Java Virtual Machine (JVM) serves as an automatic memory management mechanism, efficiently reclaiming unused memory space in different production scenarios. To optimize JVM performance, developers typically fine-tune the garbage collector by identifying an optimal set of GC configurations for specific scenarios. Despite the sophisticated design of garbage collectors, they still have the potential for bugs in different settings, and these bugs can result in more severe consequences. Hence, comprehensive testing of these garbage collectors is imperative before their release. Code coverage criteria are typically employed to assess the comprehensiveness of a test suite. However, traditional code coverage metrics, such as branch coverage, are hardly applicable for GC testing due to their inherent concurrency. Additionally, existing JVM testing techniques do not adequately consider the characteristics of garbage collectors, making it difficult to test these garbage collectors sufficiently. In this paper, we make the first effort to design coverage criteria against garbage collectors based on the events of GC called Event-Coverage . Its key insight is to measure the diversity of GC executions for testing purposes by assessing the range of GC events these executions cover. Furthermore, we design a new testing method for maximizing Event-Coverage called GCFuzz. GCFuzz conducts an exhaustive investigation of the memory state space of GC and thoroughly explores the memory state under various GC configurations. To enhance GCFuzz’s efficiency in achieving higher Event-Coverage, we have further designed a coverage-driven strategy for preserving candidate seed programs and selecting GC configurations. Extensive evaluations demonstrate a positive correlation between Event-Coverage and the bug-revealing efficiency. Moreover, GCFuzz outperforms state-of-the-art techniques in detecting unique GC-related inconsistencies and achieving higher Event-Coverage. Remarkably, GCFuzz has identified 20 previously undetected GC bugs, with 15 of them already confirmed or fixed by developers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410008053",
    "type": "article"
  },
  {
    "title": "Stress Testing Control Loops in Cyber-Physical Systems - RCR Report",
    "doi": "https://doi.org/10.1145/3733715",
    "publication_date": "2025-05-05",
    "publication_year": 2025,
    "authors": "Claudio Mandrioli; Seung Yeob Shin; Martina Maggio; Domenico Bianculli; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "This is the Replicated Computational Results (RCR) Report for the article “ Stress Testing Control Loops in Cyber-Physical Systems .” The article proposes a novel approach for testing Cyber-Physical Systems (CPS) based on the integration of the guarantees that can be provided with the control theoretical models into the software testing practices. This RCR report describes how to reproduce the empirical results of the article. We make available the different scripts needed to fully replicate the results obtained in our article.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410089993",
    "type": "article"
  },
  {
    "title": "Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss",
    "doi": "https://doi.org/10.1145/3728639",
    "publication_date": "2025-05-05",
    "publication_year": 2025,
    "authors": "Guang Yang; Yu Zhou; Xiangyu Zhang; Xiang Chen; Terry Yue Zhuo; David Lo; Taolue Chen",
    "corresponding_authors": "",
    "abstract": "Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of “early learning” as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks. To evaluate the effectiveness of our defense method, we select four code-related tasks as our experiments scenes and conduct experimental analyses on both natural language and two programming languages (Java and Python). Our experiments across multiple models with different sizes (from 125M to 7B) and poisoning ratios demonstrate the applicability and effectiveness of DeCE in enhancing the security of CLMs. The findings emphasize the potential of DeCE as a novel defense mechanism for CLMs, effectively tackling the challenge of securing models against backdoor threats.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410090047",
    "type": "article"
  },
  {
    "title": "TVDiag: A Task-oriented and View-invariant Failure Diagnosis Framework for Microservice-based Systems with Multimodal Data",
    "doi": "https://doi.org/10.1145/3734868",
    "publication_date": "2025-05-08",
    "publication_year": 2025,
    "authors": "Shuaiyu Xie; Jian Wang; Hanbin He; Z. Wang; Yuqi Zhao; Neng Zhang; Bing Li",
    "corresponding_authors": "",
    "abstract": "Microservice-based systems often suffer from reliability issues due to their intricate interactions and expanding scale. With the rapid growth of observability techniques, various methods have been proposed to achieve failure diagnosis, including root cause localization and failure type identification, by leveraging diverse monitoring data such as logs, metrics, or traces. However, traditional failure diagnosis methods that use single-modal data can hardly cover all failure scenarios due to the restricted information. Several failure diagnosis methods have been recently proposed to integrate multimodal data based on deep learning. These methods, however, tend to combine modalities indiscriminately and treat them equally in failure diagnosis, ignoring the relationship between specific modalities and different diagnostic tasks. This oversight hinders the effective utilization of the unique advantages offered by each modality. To address the limitation, we propose TVDiag , a multimodal failure diagnosis framework for locating culprit microservice instances and identifying their failure types (e.g., Net-packets Corruption) in microservice-based systems. TVDiag employs task-oriented learning to enhance the potential advantages of each modality and establishes cross-modal associations based on contrastive learning to extract view-invariant failure information. Furthermore, we develop a graph-level data augmentation strategy that randomly inactivates the observability of some normal microservice instances to mitigate the shortage of training data. Experimental results on four datasets show that TVDiag outperforms state-of-the-art methods in multimodal failure diagnosis by at least 20.16% and 3.08% in terms of \\(HR@1\\) and F1-score, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410192168",
    "type": "article"
  },
  {
    "title": "MORepair: Teaching LLMs to Repair Code via Multi-Objective Fine-Tuning",
    "doi": "https://doi.org/10.1145/3735129",
    "publication_date": "2025-05-10",
    "publication_year": 2025,
    "authors": "Boyang Yang; Haoye Tian; Jiadong Ren; Hongyu Zhang; Jacques Klein; Tegawendé F. Bissyandé; Claire Le Goues; Shunfu Jin",
    "corresponding_authors": "",
    "abstract": "Within the realm of software engineering, specialized tasks on code, such as program repair, present unique challenges, necessitating fine-tuning Large language models (LLMs) to unlock state-of-the-art performance. Fine-tuning approaches proposed in the literature for LLMs on program repair tasks generally overlook the need to reason about the logic behind code changes, beyond syntactic patterns in the data. High-performing fine-tuning experiments also usually come at very high computational costs. With MORepair , we propose a novel perspective on the learning focus of LLM fine-tuning for program repair: we not only adapt the LLM parameters to the syntactic nuances of the task of code transformation (objective ➊), but we also specifically fine-tune the LLM with respect to the logical reason behind the code change in the training data (objective ➋). Such a multi-objective fine-tuning will instruct LLMs to generate high-quality patches. We apply MORepair to fine-tune four open-source LLMs with different sizes and architectures. Experimental results on function-level and repository-level repair benchmarks show that the implemented fine-tuning effectively boosts LLM repair performance by 11.4% to 56.0%. We further show that our fine-tuning strategy yields superior performance compared to the state-of-the-art approaches, including standard fine-tuning, Fine-tune-CoT, and RepairLLaMA.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410263438",
    "type": "article"
  },
  {
    "title": "AutoAdapt: On the Application of AutoML for Parameter-Efficient Fine-Tuning of Pre-Trained Code Models",
    "doi": "https://doi.org/10.1145/3734867",
    "publication_date": "2025-05-10",
    "publication_year": 2025,
    "authors": "Amal Akli; Maxime Cordy; Mike Papadakis; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have demonstrated their ability to solve tasks across various domains, including software engineering. However, their extensive number of parameters makes full fine-tuning computationally prohibitive. While Parameter-efficient fine-tuning (PEFT) methods, such as adapter fine-tuning, have been proposed to address this issue; yet, they typically employ default configurations that use the same adapter settings across all layers. Concurrently, Automated Machine Learning (AutoML) has demonstrated success in hyperparameter optimization, while Neural Architecture Search (NAS) has proven effective in optimizing neural network architectures. Building on these successes, we introduce AutoAdapt, a novel approach that leverages NAS to automatically discover task-specific, layer-wide adapter configurations, allowing each layer to adopt distinct adapter parameters. AutoAdapt defines a search space tailored for adapter-based fine-tuning and employs an evolutionary algorithm to explore a diverse range of configurations, thereby evaluating the benefits of customizing each layer individually. We evaluate AutoAdapt on well-established software engineering tasks, including vulnerability detection, code clone detection, and code search. Our empirical results demonstrate that AutoAdapt outperforms manually engineered adapter configurations, achieving up to a 5% improvement in F1-score for clone detection and defect detection, and up to a 25% improvement in MRR for code search. Additionally, it surpasses other PEFT techniques, such as Prefix Tuning and LoRA. Furthermore, AutoAdapt is capable of identifying configurations that outperform even full fine-tuning, while training less than 2.5% of the model parameters. A comprehensive analysis reveals that factors such as selective layer adaptation, module selection (e.g., attention versus feed-forward layers), normalization, and dropout significantly influence performance across different tasks. Additionally, our findings suggest the possibility of transferring adapter configurations to similar datasets and tasks, thus simplifying the search for optimal PEFT settings. Our code and data are available for access at: https://github.com/serval-uni-lu/AutoAdapt",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410263855",
    "type": "article"
  },
  {
    "title": "Integrating Path Selection for Symbolic Execution and Variable Selection for Constraint Solving",
    "doi": "https://doi.org/10.1145/3735552",
    "publication_date": "2025-05-13",
    "publication_year": 2025,
    "authors": "Shunkai Zhu; Jun Sun; Jingyi Wang; Zhenbang Chen; Peng Cheng",
    "corresponding_authors": "",
    "abstract": "Symbolic execution is a powerful technique that can accurately synthesize program inputs for program testing through constraint solving. Applying symbolic execution effectively means that we must solve two searching problems efficiently. One is to search through the many program paths and the other is, given a particular path condition, to search through the numerous variable assignments to identify one satisfying solution. With few exceptions, existing symbolic execution engines treat constraint solvers as black boxes. As a result, the two searches are completely separated, which results in much redundancy (i.e., the same variable assignments may be tried for solving many program paths). Existing attempts on addressing this issue include those approaches based on constrained Horn clauses (in which the whole program is encoded as one constraint) and one preliminary attempt on caching and reusing partial solving results from the constraint solver. In this work, we propose SEC, which systematically computes the reward of concretizing a program path (for symbolic execution) and a variable (for constraint solving) and uses the reward as guide for integrating the two searches. We implemented SEC based on KLEE and evaluate it on a diverse set of programs. The results show that SEC is effective, i.e., achieving 15% more code coverage than the state-of-the-art baseline symbolic execution engines. Furthermore, we show that SEC can be readily combined with a state-of-the-art concolic testing engine to improve its performance",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410343899",
    "type": "article"
  },
  {
    "title": "Large Language Models for Automated Web-Form-Test Generation: An Empirical Study",
    "doi": "https://doi.org/10.1145/3735553",
    "publication_date": "2025-05-13",
    "publication_year": 2025,
    "authors": "Tao Li; Chenhui Cui; Rubing Huang; Dave Towey; Lei Ma",
    "corresponding_authors": "",
    "abstract": "Testing web forms is an essential activity for ensuring the quality of web applications. It typically involves evaluating the interactions between users and forms. Automated test-case generation remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests. Large Language Models (LLMs) have shown great potential for contextual text generation. This motivated us to explore how they could generate automated tests for web forms, making use of the contextual information within form elements. To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation. To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications. In addition, we propose three HTML-structure-pruning methods to extract key contextual information. The experimental results show that different LLMs can achieve different testing effectiveness, with the GPT-4, GLM-4, and Baichuan2 LLMs generating the best web-form tests. Compared with GPT-4, the other LLMs had difficulty generating appropriate tests for the web forms: Their successfully-submitted rates (SSRs) — the proportions of the LLMs-generated web-form tests that could be successfully inserted into the web forms and submitted — decreased by 9.10% to 74.15%. Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated. Specifically, when using Parser-Processed HTML for Task Prompt (PH-P), the SSR averaged 70.63%, higher than the 60.21% for Raw HTML for Task Prompt (RH-P) and 50.27% for LLM-Processed HTML for Task Prompt (LH-P). With RH-P, GPT-4’s SSR was 98.86%, outperforming models like LLaMa2 (7B) with 34.47% and GLM-4V with 0%. Similarly, with PH-P, GPT-4 reached an SSR of 99.54%, the highest among all models and prompt types. Finally, this paper also highlights strategies for selecting LLMs based on performance metrics, and for optimizing the prompt design to improve the quality of the web-form tests.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410344404",
    "type": "article"
  },
  {
    "title": "Enhancing Differential Testing With LLMs For Testing Deep Learning Libraries",
    "doi": "https://doi.org/10.1145/3735637",
    "publication_date": "2025-05-14",
    "publication_year": 2025,
    "authors": "Meiziniu Li; D. Li; J. Liu; Jialun Cao; Yongqiang Tian; Shing-Chi Cheung",
    "corresponding_authors": "",
    "abstract": "Differential testing offers a promising strategy to alleviate the test oracle problem by comparing the test results between alternative implementations. However, existing differential testing techniques for deep learning (DL) libraries are limited by the key challenges of finding alternative implementations (called \\(counterparts\\) ) for a given API and subsequently generating diverse test inputs. To address the two challenges, this paper introduces DLL ens , an LLM-enhanced differential testing technique for DL libraries. The first challenge is addressed by an observation that DL libraries are commonly designed to support the computation of a similar set of DL algorithms. Therefore, the counterpart of a given API’s computation could be successfully synthesized through certain composition and adaptation of the APIs from another DL library. DLL ens incorporates a novel counterpart synthesis workflow, leveraging a large language model (LLM) to search for valid counterparts for differential testing. To address the second challenge, DLL ens incorporates a static analysis technique that extracts the path constraints from the implementations of a given API and its counterpart to guide diverse test input generation. The extraction is facilitated by LLM’s knowledge of the concerned DL library and its upstream libraries. DLL ens incorporates validation mechanisms to manage the LLM’s hallucinations in counterpart synthesis and path constraint extraction. We evaluate DLL ens on two popular DL libraries, TensorFlow and PyTorch. Our evaluation shows that DLL ens synthesizes counterparts for 1.84 times as many APIs as those found by state-of-the-art techniques on these libraries. Moreover, under the same time budget, DLL ens covers 7.23% more branches and detects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly sampled APIs. DLL ens has successfully detected 71 bugs in recent TensorFlow and PyTorch libraries. Among them, 59 are confirmed by developers, including 46 confirmed as previously unknown bugs, and 10 of these previously unknown bugs have been fixed in the latest version of TensorFlow and PyTorch.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410361381",
    "type": "article"
  },
  {
    "title": "PonziHunter: Hunting Ethereum Ponzi Contract via Static Analysis and Contrastive Learning on the Bytecode Level",
    "doi": "https://doi.org/10.1145/3735971",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "J. Chen; Jieli Liu; Jianlin Wu; Dan Lin; Jiajing Wu; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "In recent years, blockchain technology has developed rapidly and received widespread attention. However, its pseudonymous and decentralized nature has also attracted many criminal activities. Ponzi schemes, a kind of classic financial scam, also hide their true face in smart contracts, causing massive financial losses to blockchain users. Although several methods have been proposed to detect Ponzi contracts, there are still limitations in broad applicability, semantics understanding, and adversarial robustness. In this paper, we propose PonziHunter, an intelligent framework for hunting Ponzi contracts on Ethereum. To tackle the problem of broad applicability, we train a detection model that does not require expert experience based on publicly available on-chain bytecode and off-chain contract labels. To tackle the problem of semantics understanding, we employ cross-function control flows and state variable dependencies to understand the logic of Ponzi contracts. Specifically, we decompile bytecodes into higher-order representations to analyze control flows and state variable dependencies and model the information as graph data. By combining the idea of code slicing, we identify the basic blocks related to Ponzi contract recognition. To tackle the problem of adversarial robustness, we model Ponzi contract recognition as a graph classification problem based on contrastive pre-training. We propose a data augmentation method for control flow graphs (CFGs), which preserves the basic blocks related to Ponzi contract recognition as much as possible during data perturbation. Experimental results show that PonziHunter outperforms state-of-the-art tools with average improvements of at least 4.77% on real-world ground-truth data, and can newly discover 85 Ponzi contracts in the wild. More importantly, PonziHunter is robust against adversarial examples and can locate the critical basic blocks for smart Ponzi detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410430635",
    "type": "article"
  },
  {
    "title": "White-Box Test Input Generation for Enhancing Deep Neural Network Models through Suspicious Neuron Awareness",
    "doi": "https://doi.org/10.1145/3736305",
    "publication_date": "2025-05-19",
    "publication_year": 2025,
    "authors": "Hongjing Guo; Chuanqi Tao; Zhiqiu Huang; Weiqin Zou",
    "corresponding_authors": "",
    "abstract": "Deep Neural Network (DNN) testing has emerged as an effective way of uncovering erroneous behaviors in DNN models and further enhancing their performance. Research on test input generation has gained much attention from both researchers and practitioners, aiming to expose faults in models. The newly generated inputs subsequently serve as additional training instances for model refinement through retraining. Existing approaches generate test inputs by optimizing an objective function based on testing metrics such as neuron coverage and property-related metrics, and the gradient of the objective is used to perturb seed inputs. However, these approaches pay limited attention to the model’s decision logic, particularly the erroneous decision patterns learned during training. Furthermore, they primarily focus on detecting faults without considering the diversity of detected misbehaviors, which limits the models’ ability to learn diverse features through retraining. To address these limitations, this paper introduces SUNTest, a novel test input generation approach designed to detect diverse faults and enhance the robustness of DNN models. SUNTest focuses on erroneous decision-making by localizing suspicious neurons responsible for misbehaviors through the execution spectrum analysis of neurons. To guide input mutations toward inducing diverse faults, SUNTest designs a hybrid fitness function that incorporates two types of feedback derived from neuron behaviors, including the fault-revealing capability of test inputs guided by suspicious neurons and the diversity of test inputs. Additionally, SUNTest adopts an adaptive selection strategy for mutation operators to prioritize operators likely to induce new fault types and improve the fitness value in each iteration. Experiments conducted on eight DNN models demonstrate the effectiveness of SUNTest in fault localization and test input generation. It outperforms existing test input generators in the number of detected faults, uncovering up to 80.9 more distinct fault types. In terms of model enhancement, SUNTest increases the average accuracy improvement by up to 8.04% compared to baseline approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410497808",
    "type": "article"
  },
  {
    "title": "Exploring Fine-Grained Bug Report Categorization with Large Language Models and Prompt Engineering: An Empirical Study",
    "doi": "https://doi.org/10.1145/3736408",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Anil Koyuncu",
    "corresponding_authors": "Anil Koyuncu",
    "abstract": "Accurate classification of issues is essential for effective project management and timely responses, as the volume of issue reports continues to grow. Manual classification is labor-intensive and error-prone, necessitating automated solutions. While large language models (LLMs) show promise in automated issue labeling, most research focuses on broad categorization (e.g., bugs, feature requests), with limited attention to fine-grained categorization. Understanding specific bug types is crucial, as different bugs require tailored resolution strategies. This study addresses this gap by evaluating LLMs and prompt engineering strategies for fine-grained bug report categorization. We analyze 221,184 fine-grained bug report category labels generated by selected LLMs using various prompt engineering strategies for 1,024 bug reports. We examine how LLMs and prompt engineering influence output characteristics, control over outputs, and categorization performance. Our findings highlight that LLMs and prompt engineering significantly impact output consistency and classification capability, with some yielding consistent results and others introducing variability. Based on these findings, we analyze the agreements and disagreements between LLM-generated labels and human annotations to assess category correctness. Our results suggest that examining label consistency and discrepancies can serve as a complementary method for validating bug report categories, identifying unclear reports, and detecting misclassifications in human annotations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410537964",
    "type": "article"
  },
  {
    "title": "No Country for Indie Developers: A Study of Google Play’s Closed Testing Requirements for New Personal Developer Accounts",
    "doi": "https://doi.org/10.1145/3736578",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Grishma Shrestha; Shristi Shrestha; Anas Mahmoud",
    "corresponding_authors": "",
    "abstract": "In November 2023, Google Play introduced new closed testing requirements for apps submitted by developers operating with personal accounts, or indie app developers. These requirements mandate that at least 20 testers must remain opted-in (use the app) for at least 14 consecutive days before the app can be published on the Play Store. According to Google, these new requirements aim to ensure the quality and security of submitted apps. However, for individual developers operating without organizational support, adhering to such requirements can pose logistical challenges and lead to production delays. To understand these challenges, in this paper, we qualitatively analyze app developers’ discussions of Google Play’s new closed testing requirements on Reddit. Additionally, we report insights from a survey of 14 indie app developers who recently passed the requirements or are actively seeking compliance. Our results show that Google Play’s closed testing requirements for indie apps are commonly perceived as discriminatory, imposing logistical and bureaucratic barriers on small-scale creators in their quest to compete in the mobile app market. Our analysis also uncovers the strategies the Android developer community has adapted to navigate such requirements. Based on our findings, we propose several guidelines to help indie app developers integrate the testing requirements into their workflow. We further suggest design strategies to mitigate the impact of such requirements on innovation, fairness, and competition in the mobile app market.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410560470",
    "type": "article"
  },
  {
    "title": "Testing Abstractions for Cyber-Physical Control Systems - RCR Report",
    "doi": "https://doi.org/10.1145/3736577",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Claudio Mandrioli; Max Nyberg Carlsson; Martina Maggio",
    "corresponding_authors": "",
    "abstract": "This is the Replicated Computational Results (RCR) Report for the article “ Testing Abstractions for Cyber-Physical Control Systems .” The article empirically studies how substituting different components in Cyber-Physical Systems (CPSs) testing with simulators impacts the fault-exposition. This RCR report describes the artefacts used in the paper, how to use the testing setups used in the article, and how to reproduce the empirical results of the article.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410560480",
    "type": "article"
  },
  {
    "title": "HyRES: Recovering Data Structures in Binaries via Semantic Enhanced Hybrid Reasoning",
    "doi": "https://doi.org/10.1145/3736719",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Zihan Sha; Hui Shu; Hao Wang; Zeyu Gao; Lan Yang; Chao Zhang",
    "corresponding_authors": "",
    "abstract": "Binary reverse engineering is pivotal in the realm of cybersecurity, enabling critical applications such as malware analysis, legacy code hardening, and vulnerability detection. However, the challenge of recovering structural information from binaries, especially stripped ones, persists due to the significant loss of variable boundaries, types, names and data flow information during compilation. In this paper, we introduce HyRES ( Hy brid RE asoning For S tructure Recovery), an innovative hybrid reasoning technique that energizes static analysis, large language model (LLM), and heuristic methods to recover data structures from stripped binaries. It analyzes the structure layout and proficiently infer its semantics via LLM, and utilizes semantics to perform semantic-enhanced structure aggregation, which overcomes the need for complete data flow. HyRES outperforms state-of-the-art (SOTA) solutions in terms of structure pointer identification and layout recovery. Specifically, HyRES achieves 65.1% higher recall and 33.4% higher accuracy than the SOTA, while also being 64.2% faster than existing SOTA solutions. Comprehensive experiments demonstrate HyRES's superior performance and practical utility in real-world reverse engineering tasks, marking a significant advancement in binary analysis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410610032",
    "type": "article"
  },
  {
    "title": "Preparation and Utilization of Mixed States for Testing Quantum Programs",
    "doi": "https://doi.org/10.1145/3736757",
    "publication_date": "2025-05-23",
    "publication_year": 2025,
    "authors": "Yuechen Li; Kai-Yuan Cai; Beibei Yin",
    "corresponding_authors": "",
    "abstract": "Due to the growing demand for high-quality quantum programs (QPs), unit testing is employed to check the behavior of QPs. As for quantum inputs of testing, most studies limit test inputs to pure states, whereas mixed states representing probabilistic mixtures of pure states are almost excluded from the test process. Besides, when achieving the input domain coverage, lots of pure-state test cases (PSTCs) with pure states as inputs should be employed, leading to high time costs for testing. To handle that, this paper explores using mixed states as test inputs for better utilization of quantum information. From the perspective of input domain coverage, applying mixed-state test cases (MSTCs) replacing PSTCs can simplify the test suite and accordingly promote test efficiency. Owing to the mixture of multiple pure states, a single MSTC is more likely to detect a fault than a PSTC, thereby enhancing test effectiveness. This paper then proposes a unit testing framework, including generation and execution of MSTCs. Also, this paper presents two guidelines and two parameterized quantum circuits to prepare desired mixed states. Empirical studies evaluate the performance of MSTCs and the experimental results demonstrate that MSCTs generally consume less time and detect more faults than PSTCs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410642317",
    "type": "article"
  },
  {
    "title": "Improving Code Reviewer Recommendation: Accuracy, Latency, Workload, and Bystanders",
    "doi": "https://doi.org/10.1145/3736405",
    "publication_date": "2025-05-27",
    "publication_year": 2025,
    "authors": "Peter C. Rigby; Seth Rogers; Sadruddin Saleem; Parth Suresh; Daniel Suskin; Patrick Riggs; Chandra Maddila; Nachiappan Nagappan; Audris Mockus",
    "corresponding_authors": "",
    "abstract": "Aim. The code review team at Meta is continuously improving the code review process. In this work, we report on three randomized controlled experimental trials to improve code reviewer recommendation. Method. To evaluate the recommenders, we conduct three A/B tests which are a type of randomized controlled experimental trial. The unit is either the code diff (Meta’s term for a pull-request) or all the diffs that an author creates during the experimental period. We set goal metrics, i.e. those we expect to improve, and guardrail metrics, those that we do not want to negatively impact, i.e. analogous to safety metrics in medical trials. We test the outcomes using a t-test, Wilcoxon test, or Fisher test depending on the type of data. Expt 1. We developed a new recommender, RevRecV2 , based on features that had been successfully used in the literature and that could be calculated with low latency. In an A/B test on 82k diffs in Spring of 2022, we found that the new recommender was more accurate and had lower latency. The new recommender did not impact the amount of time a diff was under review. The results allowed us to roll-out the recommender in Summer of 2022 to all of Meta. Expt 2. Reviewer workload is not evenly distributed, our goal was to reduce the workload of top reviewers. Based on the literature, and using historical data, we conducted backtests to determine the best measure of reviewer workload. We then ran an A/B test on 28k diff authors in Winter 2023 on a workload balanced recommender, RevRecWL . Our A/B test led to mixed results. When a low workload reviewer had reasonable expertise, authors selected them, however, the top recommended low workload reviewer was often not selected. There was no impact on our guardrail metrics of the amount of time to perform a review. This workload balancing replaced the recommender from the first experiment as the recommender in production at Meta. Expt 3. Engineers at Meta often select a team rather than an individual reviewer to review a diff. We suspected the bystander effect might be slowing down reviews of these diffs because no single individual was assigned the review. On diffs that only had a team assigned, we randomly selected one of the top three recommended reviewers to review the diff with BystanderRecRnd . We conducted an A/B test on 12.5k authors in Spring 2023 and found a large decrease in the amount of time it took for diffs to be reviewed. We did not find that reviewers rushed reviews. The results were strong enough to roll this recommender out to all diffs that only have a team assigned for review. Implications. Aside from the direct findings from our work, our findings suggest there can be a discrepancy between historical back-testing and A/B test experimental findings, and that more A/B tests are necessary to test recommenders in production. Outcome measures beyond accuracy are important. This is especially true in understanding how recommenders change a reviewer’s workload. We also see that the latency in displaying a recommendation can have a large impact on how often authors select recommendations making the reporting of latency an important metric for future work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410771641",
    "type": "article"
  },
  {
    "title": "Modeling and Verification of Hybrid Systems by Extending AADL",
    "doi": "https://doi.org/10.1145/3737698",
    "publication_date": "2025-05-29",
    "publication_year": 2025,
    "authors": "Xiong Xu; Ehsan Ahmad; Shuling Wang; Xiangyu Jin; Bohua Zhan; Naijun Zhan",
    "corresponding_authors": "",
    "abstract": "System level design, and dependability prediction of safety-critical systems demand integration of architectural and analysis artifacts in a single development environment. Hybrid systems, with mutual dependencies and extensive interactions between the control portion and its physical environment, further intensify this need. Architecture Analysis &amp; Design Language (AADL) is a model-based engineering language for the architectural design and analysis of embedded control systems. Core AADL has been extended with sub-languages for modeling and analysis of discrete behavior of the control portion, but not for continuous behavior of the physical environment. In a previous work, we have introduced Hybrid Annex for continuous behavior modeling as part of initial findings of an ongoing research effort on fulfilling the need for integrated modeling of the computing system along with its physical environment. In this paper, we first detail complete structure of the Hybrid Annex along with appropriate examples for each section. Then, we present formal semantics of the synchronous subset of AADL models annotated with Hybrid Annex specifications using Hybrid Communicating Sequential Processes (HCSP). Formal semantics are used to verify correctness of AADL models (with Hybrid Annex specifications) using Hybrid Hoare Logic (HHL). A case study on a realistically-scaled automatic cruise control system is provided to demonstrate modeling and verification of hybrid systems using AADL with the proposed extension.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410856798",
    "type": "article"
  },
  {
    "title": "Code Health Meter: A Quantitative and Graph-Theoretic Foundation for Automated Code Quality and Architecture Assessment",
    "doi": "https://doi.org/10.1145/3737670",
    "publication_date": "2025-05-30",
    "publication_year": 2025,
    "authors": "Ben Khalfallah Hela",
    "corresponding_authors": "Ben Khalfallah Hela",
    "abstract": "Quantifying code quality and architectural soundness remains a persistent challenge in modern software engineering. Existing tools often rely on isolated or superficial metrics, lacking architectural awareness and actionable insight. This paper introduces Code Health Meter (CHM), a fully automated, referentially transparent framework for quantitative and graph-theoretic code quality assessment. CHM statically analyzes source code and produces a six-dimensional signature per module, capturing semantic complexity (Maintainability Index, Halstead Volume, Cyclomatic Complexity), architectural structure (graph centrality, modularity), and redundancy (code duplication via Rabin–Karp fingerprinting). The framework is designed to be deterministic, explainable, and amenable to longitudinal analysis. We validate CHM on a 14,000-line JavaScript system by tracking its evolution across multiple versions. Our analysis reveals increasing architectural fragmentation, rising code duplication, and declining maintainability. These findings demonstrate CHM’s capability to surface actionable architectural insights, quantify technical debt, and support evidence-driven refactoring decisions. CHM bridges the gap between classical software metrics and architectural reasoning, offering a robust foundation for integrating automated code health monitoring into engineering workflows.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410891834",
    "type": "article"
  },
  {
    "title": "C <scp>om</scp> C <scp>at</scp> : Expertise-Guided Context Generation to Enhance Code Comprehension",
    "doi": "https://doi.org/10.1145/3742475",
    "publication_date": "2025-06-04",
    "publication_year": 2025,
    "authors": "Skyler Grandel; Scott Thomas Andersen; Yu Huang; Kevin Leach",
    "corresponding_authors": "",
    "abstract": "Software maintenance constitutes a substantial portion of the total lifetime costs of software, with a significant portion attributed to code comprehension. Software comprehension is eased by documentation such as comments that summarize and explain code. We present ComCat , an approach to automate comment generation by augmenting Large Language Models (LLMs) with expertise-guided context to target the annotation of source code with comments that improve comprehension. Our approach enables the selection of the most relevant and informative comments for a given snippet or file containing source code. We develop the ComCat pipeline to comment C/C++ files by (1) automatically identifying suitable locations in which to place comments, (2) predicting the most helpful type of comment for each location, and (3) generating a comment based on the selected location and comment type. In a human subject evaluation, we demonstrate that ComCat -generated comments significantly improve developer code comprehension across three indicative software engineering tasks by up to 13% for 80% of participants. In addition, we demonstrate that ComCat -generated comments are at least as accurate and readable as human-generated comments and are preferred over standard ChatGPT-generated comments for up to 92% of snippets of code. Furthermore, we develop and release a dataset containing source code snippets, human-written comments, and human-annotated comment categories. ComCat leverages LLMs to offer a significant improvement in code comprehension across a variety of human software engineering tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411022490",
    "type": "article"
  },
  {
    "title": "Feature Disentanglement Based Heterogeneous Defect Prediction",
    "doi": "https://doi.org/10.1145/3742474",
    "publication_date": "2025-06-05",
    "publication_year": 2025,
    "authors": "Xu Yu; Jiaqi Yan; Qinqin Gao; Peng QingTao; Bin Yu; Junwei Du; Ying Xing; Dunwei Gong",
    "corresponding_authors": "",
    "abstract": "Cross-project defect prediction (CPDP) utilizes the existing labeled data in the source project to assist with the prediction of unlabeled projects in the target dataset, which effectively improves the prediction performance and has become a research hotspot in software engineering. At present, CPDP can be categorized into homogeneous cross-project defect prediction and heterogeneous cross-project defect prediction (HDP), in which HDP doesn’t require that the source project and the target project have the same feature space, thus, it is more widely used in the actual CPDP. Most of current HDP methods map the original features to the latent feature space and reduce the inter-project variation by transferring domain-independent features, but the transferring process ignores the use of domain-related features, which affects the prediction performance of the model. Moreover, the mapped latent features are not conducive to the model’s interpretability. Based on these, this paper proposes a heterogeneous defect prediction method based on feature disentanglement (FD-HDP). We disentangle the features using domain-related and domain-independent feature extractors, respectively, to improve the interpretability of the model by maximizing the domain adversarial loss during training and guiding the feature extractors to produce accurate domain-related and domain-independent features. The weighted sum of the prediction results from domain-related and domain-independent predictors is used as the final prediction result of the project during the prediction process, which realizes the combination of domain-independent and domain-related features and effectively improves the prediction performance. In this paper, we conducted experiments using four publicly available defect datasets to construct heterogeneous scenarios. The results demonstrate that the FD-HDP model shows significant advantages over state-of-the-art methods in six metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411059043",
    "type": "article"
  },
  {
    "title": "Systematic Literature Review on Software Security Vulnerability Information Extraction",
    "doi": "https://doi.org/10.1145/3745026",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Sofonias Yitagesu; Zhenchang Xing; Xiaowang Zhang; Zhiyong Feng; Tingting Bi; Linyi Han; Xiaohong Li",
    "corresponding_authors": "",
    "abstract": "Background. Software vulnerabilities are increasing in complexity and scale, posing great security risks to many software systems. Extracting information about software vulnerabilities is a critical area of research that aims to identify and create a structured representation of vulnerability-related information. This structured data helps software systems better understand vulnerabilities and provides security professionals with timely information to mitigate the impact of rapidly growing vulnerabilities while guiding future research to develop more secure systems. However, this process relies on the effectiveness of information extraction to transform manual vulnerability analysis from security experts to digital solutions. Despite its importance, the unique nature of vulnerability information and the fast pace at which machine learning-based extraction methods and techniques have evolved make it challenging to assess the current successes, failures, challenges, and opportunities within this research area. This study presents a systematic literature review aimed at clarifying this complex landscape. Methods. In this study, we conduct a systematic literature review (SLR) to explore existing research focusing on extracting information about software security vulnerabilities. We search for 829 primary studies on security vulnerability information extraction from seven widely used online digital libraries, focusing on top peer-reviewed journals and conferences published between 2001 and 2024. After applying our inclusion and exclusion criteria and the snowballing technique, we narrowed our selection to 87 studies for in-depth analysis and addressed four main research questions. We collect qualitative and quantitative data from each study, identifying 34 components such as research problems, methods, contributions, evaluation metrics, results, types of extracted vulnerability information, challenges, and limitations. We use meta-analysis, statistical machine learning, and text-mining techniques to identify themes, patterns, and trends across the primary studies and visualize findings. Results. The study provides an overview of the security vulnerability data landscape, identifies key resources, and guides efforts to improve vulnerability information extraction and analysis. The study finds a diverse landscape of learning algorithms used in security vulnerability information extraction, with Bidirectional Encoder Representations from Transformers (BERT), Long Short-term Memory (LSTM), and Support Vector Machine (SVM) being the most dominant. The study identifies key challenges, including feature engineering complexity, lack of a gold-standard corpus, preprocessing errors, generating accurate training data, addressing imbalanced data, multimodality fusion, and graph sparsity in security knowledge graphs. Insights for Future Research Directions. The study underscores the need for advanced extraction approaches, robust datasets, automated annotation methods, and advanced machine learning algorithms to improve the extraction of security vulnerability information. This study also suggests using large language models (LLMs) and transformer models to facilitate the automatic extraction of security-related words, terms, concepts, and phrases and introduce new filtering parameters for user requirements. We provide all our implementations; it can be found at https://bitbucket.org/slr-svie/vulnerability-information-extraction/src/master/ .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411488389",
    "type": "article"
  },
  {
    "title": "Anomaly Detection Services for Blockchain Smart Contracts with Unknown Vulnerabilities",
    "doi": "https://doi.org/10.1145/3744709",
    "publication_date": "2025-06-23",
    "publication_year": 2025,
    "authors": "Chunhong Liu; Zihang Sang; Li Duan; Jingxiong Wang; Wei Ni; Wei Wang",
    "corresponding_authors": "",
    "abstract": "Security vulnerabilities in smart contracts can have severe economic consequences. Existing smart contract vulnerability detection methods rely primarily on rigid rules defined by experts and have difficulty in detecting unknown vulnerabilities. This paper proposes a new Anomalous Smart Contract Detector, named ASCD, to effectively detect known and unknown vulnerabilities in smart contracts. This is achieved by interpreting unknown vulnerabilities as code anomalies and detecting them with an anomaly detection technique named DeepSVDD. This is also attributed to a new design of feature extraction, in which we compile smart contract source codes into opcodes, extract semantic features from opcode sequences, and control flow features from control flow graphs. By joining LSTM and GIN, the semantic and control flow features are fused to offer a comprehensive representation of smart contracts suitable for anomaly detection. Extensive experiments were conducted to verify the ASCD model, and more than 30,000 smart contracts were tested. The new model offers a significantly better F1-score than existing methods in detecting known vulnerabilities and achieves a high accuracy of 77% in detecting unknown vulnerabilities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411563155",
    "type": "article"
  },
  {
    "title": "The Sustainability Face of Automated Program Repair Tools",
    "doi": "https://doi.org/10.1145/3744900",
    "publication_date": "2025-06-24",
    "publication_year": 2025,
    "authors": "Matías Martínez; Silverio Martínez‐Fernández; Xavier Franch",
    "corresponding_authors": "",
    "abstract": "Automated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of maintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains unstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and use software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity. We state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly repair bugs and the amount of energy consumed during such process. We measure the energy consumption of ten traditional APR tools for Java and eleven fine-tuned Large-Language Models (LLM) trying to repair real bugs from Defects4J. The results of this study show the existing trade-off between energy consumption and repairability. In particular, APR tools such as TBar and RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as SimFix and the LLM CodeT5-Large, provide a good trade-off between energy consumption and repairability. We also present guidelines consisting of a set of recommendations for developing greener APR.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411604867",
    "type": "article"
  },
  {
    "title": "Enhancing Security and Acuity of Smart Contract Vulnerability Detection based on Federated Learning and BiLSTM-Attention",
    "doi": "https://doi.org/10.1145/3746061",
    "publication_date": "2025-06-25",
    "publication_year": 2025,
    "authors": "Bin Jia; Xiaosong Zhang; Xinze Zhang; Ting Chen; Wenjuan Lian",
    "corresponding_authors": "",
    "abstract": "Over the course of more than a decade, blockchain technology has made significant advancements and found applications in various domains. Smart contract, as an integral component of blockchain technology, plays a pivotal role in ensuring the security and robustness of blockchain’s development and diverse applications. Currently, smart contract vulnerabilities have caused millions of dollars in economic losses. Due to the inherent immutability of blockchain technology, once smart contracts are deployed on the blockchain, effecting changes becomes a formidable task. Most of the vulnerability detection tools currently available employ traditional security technologies, which require high expertise and have unsatisfactory detection results. In recent years, deep learning technologies have emerged. Although they do not require extensive expert knowledge, they do require a large amount of labeled data for training. The biggest issue in this field is the lack of a large-scale, accurately annotated public dataset. Hence, we propose a method for detecting smart contract vulnerabilities by leveraging federated learning and BiLSTM, called FASCVD. Our approach not only utilizes federated learning technology to aggregate multiple small datasets while ensuring data privacy but also introduces a bidirectional information extraction technique based on BiLSTM, thereby significantly enhancing the accuracy of vulnerability detection. The experimental results show that our method has already surpassed the best existing methods in terms of accuracy, precision, recall, F1-score, etc., with an accuracy rate of 95.04%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411644227",
    "type": "article"
  },
  {
    "title": "Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach",
    "doi": "https://doi.org/10.1145/3742785",
    "publication_date": "2025-06-25",
    "publication_year": 2025,
    "authors": "Yao Wan; G. Wan; Shijie Zhang; Hongyu Zhang; Yulei Sui; Pan Zhou; Hai Jin; Lichao Sun",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed significant progress in developing deep learning-based models for automated code completion. Examples of such models include CodeGPT and StarCoder. These models are typically trained from a large amount of source code collected from open-source communities such as GitHub. Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement. In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model? To this end, we tailor a membership inference approach (termed CodeMI ) that was originally crafted for classification tasks to a more challenging task of code completion. In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior. The acquired posteriors from these shadow models are subsequently employed to train a membership classifier. After that, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model. We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, ( i.e. , LSTM-based, CodeGPT, CodeGen, and StarCoder). Experimental results demonstrate that our approach effectively detects data membership, achieving accuracies of 0.842 and 0.730 for LSTM-based and CodeGPT models, respectively. Interestingly, our experiments also show that the data membership of current large language models of code, e.g. , CodeGen and StarCoder, is difficult to detect, leaving ample space for further improvement. Finally, we also try to explain the findings from the perspective of model memorization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411644230",
    "type": "article"
  },
  {
    "title": "Towards Automating Domain-Specific Data Generation for Text-to-SQL: A Comprehensive Approach",
    "doi": "https://doi.org/10.1145/3746226",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Salmane Chafik; Saad Ezzini; Ismaïl Berrada",
    "corresponding_authors": "",
    "abstract": "As software systems increasingly rely on natural language interfaces, ensuring the reliability of these systems is crucial. One critical component is the ability to accurately translate natural language queries into corresponding SQL queries, a field known as Text-to-SQL. However, the scarcity of high-quality, large-scale, and domain-specific Text-to-SQL datasets hinders the development of reliable and robust models. To tackle these challenges, we propose SelectCraft , a novel automatic generation approach designed to create realistic Text-to-SQL datasets tailored to specific domains. Our method leverages existing databases and their structures to generate complex text-SQL pairs that mirror real-world usage scenarios. As a proof of concept, we have successfully generated a substantial financial Text-to-SQL dataset, denominated as BanQies , encompassing over 1 million samples utilizing our proposed approach. Moreover, we introduce BanQL , a new large language model (LLM) based on StarCoder2 , a state-of-the-art code-based LLM, and fine-tuned on our newly created dataset. We evaluate BanQL performance against several state-of-the-art models, demonstrating significant enhancements in accuracy and generalizability, highlighting the advantages of incorporating domain-specific data in Text-to-SQL tasks. We firmly believe that our contributions have the potential to improve the overall reliability of Text-to-SQL software systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411673827",
    "type": "article"
  },
  {
    "title": "Automatically Deriving Developers’ Technical Expertise from the GitHub Social Network",
    "doi": "https://doi.org/10.1145/3746451",
    "publication_date": "2025-06-30",
    "publication_year": 2025,
    "authors": "Yanchun Sun; Jiawei Wu; Xiaohan Zhao; Haizhou Xu; Ye Zhu; Zhenpeng Chen; Sihan Wang; Huizhen Jiang; Gang Huang",
    "corresponding_authors": "",
    "abstract": "Developers’ technical expertise is crucial for numerous tasks within open-source communities, such as identifying suitable developers and maintainers. Despite its significance, GitHub, the world’s largest open-source code hosting platform, does not explicitly display developers’ technical expertise. Existing methods fall short in capturing the multifaceted and dynamic nature of developers’ skills and knowledge. To address this gap, we propose a novel approach that leverages graph neural networks (GNNs) to express developers’ technical expertise. Our method constructs a comprehensive GitHub social network that integrates various social and development activities. We then employ a GNN model to learn a low-dimensional representation vector for each developer, encapsulating their technical expertise across different dimensions. We assess the effectiveness of our model by comparing it against five baselines on three GitHub social relationship recommendation tasks, including SimDeveloper, ContributionRepo, and RepoMaintainer. Our proposed method outperforms these baselines, achieving improvements of 5.6%-9.5% on Hit Ratio@10 and 3.4%-11.1% on F1 score. These results demonstrate promising performance in predicting technical preferences for both repositories and developers. This research contributes to a more nuanced understanding of developer expertise in open-source communities and has potential implications for improving collaboration and project management on platforms like GitHub.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411805258",
    "type": "article"
  },
  {
    "title": "The Effects of Learning from Failure and Team Cognitive Factors on Software Process Tailoring",
    "doi": "https://doi.org/10.1145/3747179",
    "publication_date": "2025-07-03",
    "publication_year": 2025,
    "authors": "Chung‐Yang Chen; Jung-Chieh Lee",
    "corresponding_authors": "",
    "abstract": "Software project teams often plan and replan their development processes to accommodate a project’s particularities and volatility. This continual planning is called software process tailoring (SPT). In SPT, software teams often experiment with innovative process solutions to address changes. Because mistakes are inevitable in development and innovation, learning from failure is critical for team members to acknowledge, reflect on, and rectify process mistakes. This paper focuses on failure-based learning behavior (FLB) in SPT and explores how a team’s FLB can be facilitated to foster SPT performance. Based on social cognitive theory, three team cognitive factors, self-efficacy, self-esteem, and identification, are examined. The research also explores the moderating role of servant leadership in the effects of these factors on FLB. We used a survey research method and collected data from 65 software teams to test the model. The results show that a team’s FLB significantly fosters SPT performance. Higher team self-efficacy and team identification increase team FLB, whereas higher team self-esteem decreases FLB. Servant leadership significantly moderates the effects of team self-esteem and identification on FLB, but this leadership does not have a significant effect on the relationship between team self-efficacy and FLB.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412013266",
    "type": "article"
  },
  {
    "title": "Towards Understanding Refactoring Engine Bugs",
    "doi": "https://doi.org/10.1145/3747289",
    "publication_date": "2025-07-04",
    "publication_year": 2025,
    "authors": "Haibo Wang; Zejiang Xu; Huaien Zhang; Nikolaos Tsantalis; Shin Hwei Tan",
    "corresponding_authors": "",
    "abstract": "Refactoring is a critical process in software development, aiming at improving the internal structure of code while preserving its external behavior. Refactoring engines are integral components of modern Integrated Development Environments (IDEs) and can automate or semi-automate this process to enhance code readability, reduce complexity, and improve the maintainability of software products. Like traditional software systems, refactoring engines can generate incorrect refactored programs, resulting in unexpected behaviors. In this paper, we present the first systematic study of refactoring engine bugs by analyzing bugs arising in three popular refactoring engines (i.e., Eclipse , IntelliJ IDEA, and Netbeans ). We analyzed these bugs according to their refactoring types, symptoms, root causes, and triggering conditions. We obtained 12 findings and provided a series of valuable guidelines for future work on refactoring bug detection and debugging. Furthermore, our transferability study revealed 134 new bugs in the latest version of those refactoring engines. Among the 22 bugs we submitted, 11 bugs are confirmed by their developers, and seven of them have already been fixed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412033360",
    "type": "article"
  },
  {
    "title": "Bypassing Guardrails: Lessons Learned from Red Teaming ChatGPT",
    "doi": "https://doi.org/10.1145/3747288",
    "publication_date": "2025-07-04",
    "publication_year": 2025,
    "authors": "Terry Yue Zhuo; Yujin Huang; Chunyang Chen; Xiaoning Du; Zhenchang Xing",
    "corresponding_authors": "",
    "abstract": "Warning: this paper may contain content that is offensive or upsetting. Ethical and social risks persist as a crucial yet challenging topic in human-artificial Intelligence interactions, especially in ensuring the safe usage of natural language processing (NLP). The emergence of large language models (LLMs) like ChatGPT introduces the potential for exacerbating this concern. However, prior works on the ethics and risks of emergent LLMs either overlook the practical implications in real-world scenarios, lag behind rapid NLP advancements, lack user consensus on ethical risks, or fail to holistically address the entire spectrum of ethical considerations. In this paper, we comprehensively evaluate, qualitatively explore and catalog ethical dilemmas and risks in ChatGPT through benchmarking with eight representative datasets and red teaming involving diverse case studies. Our findings show that while ChatGPT demonstrates superior safety performance on benchmark datasets, its guardrails can be bypassed via our manually curated examples, revealing not only the limitations of current benchmarks for risk assessment but also unexplored risks in five distinct scenarios, including social bias in code generation, bias in cross-lingual question answering, toxic language in personalized dialogue, misleading information from hallucination, and prompt injections for unethical behaviors. We conclude with implications from red teaming ChatGPT and recommendations for designing future responsible large language models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412033371",
    "type": "article"
  },
  {
    "title": "Understanding Open Source Contributor Profiles in Popular Machine Learning Libraries",
    "doi": "https://doi.org/10.1145/3747347",
    "publication_date": "2025-07-07",
    "publication_year": 2025,
    "authors": "Jiawen Liu; Haoxiang Zhang; Ying Zou",
    "corresponding_authors": "",
    "abstract": "With the increasing popularity of machine learning (ML), many open source software (OSS) contributors are attracted to developing and adopting ML approaches. Comprehensive understanding of ML contributors is crucial for successful ML OSS development and maintenance. Without such knowledge, there is a risk of inefficient resource allocation and hindered collaboration in ML OSS projects. Existing research focuses on understanding the difficulties and challenges perceived by ML contributors through user surveys. There is a lack of understanding of ML contributors based on their activities recorded in the software repositories. In this paper, we aim to understand ML contributors by identifying contributor profiles in ML libraries. We further study contributors’ OSS engagement from four aspects: workload composition, work preferences, technical importance, and ML-specific vs SE contributions. By investigating 11,949 contributors from 8 popular ML libraries (i.e., TensorFlow, PyTorch, scikit-learn, Keras, MXNet, Theano/Aesara, ONNX, and deeplearning4j), we categorize them into four contributor profiles: Core-Nighttime , Core-Daytime , Peripheral-Nighttime , and Peripheral-Daytime . We find that: 1) project experience, authored files, collaborations, pull requests comments received and approval ratio, and geographical location are significant features of all profiles; 2) contributors in Core profiles exhibit significantly different OSS engagement compared to Peripheral profiles; 3) contributors’ work preferences and workload compositions are significantly correlated with project popularity; and 4) long-term contributors evolve towards making fewer, constant, balanced and less technical contributions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412070868",
    "type": "article"
  },
  {
    "title": "IMPACT: Identifying and Classifying Multiple Sourced and Categorized Self-Admitted Technical Debts",
    "doi": "https://doi.org/10.1145/3747180",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "Qiaoxing Li; Z. Yin; Yimin Yang; Chuanyi Li; Zongwen Shen; Jidong Ge; Wenkang Zhong; Bin Luo; Vincent Ng",
    "corresponding_authors": "",
    "abstract": "Self-Admitted Technical Debt (SATD) refers to suboptimal solutions deliberately introduced to accelerate the software development process, often at the expense of software maintainability and sustainability. Therefore, timely identification and repayment of the SATD is critical for the software system. As exploration deepens, it is found that effectively prioritizing the repayment of SATD with more significant impacts on software quality requires not only identifying SATD but also further classifying it. However, existing SATD identification and classification approaches face the following challenges: (1) SATDs originate from diverse sources. Code comments are a widespread source, but recent research has revealed that SATDs can originate from other sources, such as pull requests, issues, and commit messages. Nonetheless, existing approaches primarily target code comments, lacking the capability to analyze SATDs from other sources effectively. (2) SATDs fall into diverse categories. Nonetheless, existing SATD classification approaches fail to address all SATD categories comprehensively and show inadequate performance. (3) Imbalance of existing SATD datasets. Real-world SATD data is scarce, making dataset collection challenging. Moreover, SATD distribution across different sources is uneven, further complicating the construction of high-quality datasets. To alleviate these challenges, this paper presents an SATD identification and classification framework named IMPACT . First, IMPACT employs ChatGPT to construct an augmented dataset. Subsequently, it utilizes a pipeline with two fine-tuned language models of different parameter sizes to identify and classify SATD separately. To evaluate the effectiveness of IMPACT, we compare it with three state-of-the-art SATD classification methods and its two foundation models. Experimental results demonstrate that IMPACT outperforms state-of-the-art methods by a large margin, even surpasses its foundation model GLM-4-9B-Chat. It achieves the optimal average F1 score of 0.697 on the source of pull requests, the most challenging data source. Moreover, experiments on the cross-project test set show that IMPACT demonstrates strong generalizability on unseen project data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412172429",
    "type": "article"
  },
  {
    "title": "SPOLRE: Semantic Preserving Object Layout Reconstruction for Image Captioning System Testing",
    "doi": "https://doi.org/10.1145/3748306",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "Yi Liu; Guanyu Wang; Xinyi Zheng; Gelei Deng; Kailong Wang; Yang Liu; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "Image captioning (IC) systems, including Microsoft Azure Cognitive Service, are commonly utilized to convert image content into descriptive natural language. However, inaccuracies in caption generation can lead to serious misinterpretations. Advanced testing techniques such as MetaIC and ROME have been developed to mitigate these issues, yet they encounter notable challenges. Firstly, these strategies demand intensive labor, relying on detailed manual annotations like bounding box data of objects to create test cases. Secondly, the realism of the generated images is compromised, with MetaIC adding unrelated objects and ROME failing to remove objects effectively. Finally, the capability to generate diversified test suites is restricted. MetaIC is limited to only inserting specific objects to prevent overlap, whereas ROME can generate only \\(3^{n}-2^{n}\\) variations of test cases from an original seed image containing \\(n\\) objects. In this study, we present SPOLRE, a novel automated tool designed for semantic preserving object layout reconstruction in image captioning system testing. SPOLRE is based on the insight that modifying the arrangement of objects within an image does not alter its inherent semantics. We utilize four semantic-preserving transformation techniques—translation, rotation, mirroring, and scaling—to modify object layouts autonomously, eliminating the need for manual annotation. This approach enables the creation of realistic and varied test suites for IC system testing. Our extensive testing demonstrates that more than 75% of survey respondents find the images produced by SPOLRE more realistic compared to those generated by SOTA methods. Additionally, SPOLRE exhibits outstanding performance in identifying caption errors, detecting 31,544 incorrect captions across seven IC systems with an average precision of 91.62%. This significantly outperforms other methods, which only achieve 85.65% accuracy on average and identify 17,160 incorrect captions. Notably, SPOLRE exposes 6,236 unique issues within Microsoft Azure Cognitive Service, highlighting its effectiveness against one of the most advanced IC systems available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412202048",
    "type": "article"
  },
  {
    "title": "Sustainability in the Field of Software Engineering: A Tertiary Study",
    "doi": "https://doi.org/10.1145/3747178",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "Gabriel Alberto García‐Mireles; Ma Ángeles Moraga; Félix García; Coral Calero",
    "corresponding_authors": "",
    "abstract": "Sustainability in software and green software have increased attention in software engineering (SE), given the potential of software to contribute positively to sustainable development initiatives. This trend is substantiated by the numerous systematic reviews (SRs) that have been published on the matter. However, a significant gap in understanding remains concerning the sustainability concepts and their dimensions within the field. This tertiary study aims to construct a comprehensive map of sustainability topics in SE. As a result, 80 SRs were selected in this report. A notable lack of consensus exists regarding SE sustainability definitions and the dimensions involved. The environmental dimension is the most frequently reported, capturing 55% of the research focus, with a predominant emphasis on energy efficiency. Approximately 52.5% of studies delve into products related to system performance and evolvability and software architecture. However, limited attention is given to the social, economic, and individual dimensions. Despite the considerable focus on the green dimension, there is a necessity for further research in both empirical primary studies and SRs to enhance the reliability of the findings. Achieving consensus on definitions of terminology and implementing standardized measurement methods are essential steps to advance understanding the sustainable SE.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412202077",
    "type": "article"
  },
  {
    "title": "Towards a feasible evaluation function for search-based merge conflict resolution",
    "doi": "https://doi.org/10.1145/3748256",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Heleno de S. Campos; Gleiph Ghiotto; Márcio de Oliveira Barros; André van der Hoek; Leonardo Murta",
    "corresponding_authors": "",
    "abstract": "Resolving merge conflicts manually is a tedious and complex task. While automated approaches exist, many challenges persist. One promising yet underexplored solution is the use of search-based optimization algorithms, which require an evaluation function to measure the quality of intermediary solutions. However, using code compilation and test execution for this purpose is computationally expensive. This study investigates the relationship between conflict resolutions and conflicting content to identify a metric for guiding search-based optimization techniques. We analyzed 9,998 conflict chunks from 1,062 open-source projects, focusing on the similarity of resolutions to their parents and the correlation between randomly generated candidates, parent versions, and the resolution. Our findings reveal that conflict resolutions are, on average, 70% similar to both parents. A strong median correlation ( \\(\\rho=0.791\\) ) exists between candidate-parent and candidate-resolution similarities when aggregating parent similarities with the mean function. Based on these findings, we propose and evaluate SBCR, a Search-Based Conflict Resolution approach that uses parent similarity as a guiding function. We found that the resolution candidates generated by SBCR have a median of 86.5% similarity to the expected resolution, achieving 100% of similarity in 25.2% of the conflicts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412400182",
    "type": "article"
  },
  {
    "title": "One Size Does Not Fit All: Investigating Efficacy of Perplexity in Detecting LLM-Generated Code",
    "doi": "https://doi.org/10.1145/3748506",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Jinwei Xu; He Zhang; Yanjing Yang; Lanxin Yang; Z. Cheng; Jun Lyu; Bohan Liu; Xin Zhou; Alberto Bacchelli; Yin Kia Chiam; Thiam Kian Chiew",
    "corresponding_authors": "",
    "abstract": "Large language model-generated code (LLMgCode) has become increasingly common in software development. So far LLMgCode has more quality issues than human-authored code (HaCode). It is common for LLMgCode to mix with HaCode in a code change, while the change is signed by only human developers, without being carefully examined. Many automated methods have been proposed to detect LLMgCode from HaCode, in which the perplexity-based method (Perplexity for short) is the state-of-the-art method. However, the efficacy evaluation of Perplexity has focused on detection accuracy. Yet it is unclear whether Perplexity is good enough in a wider range of realistic evaluation settings. To this end, we carry out a family of experiments to compare Perplexity against feature- and pre-training-based methods from three perspectives: detection accuracy , detection speed , and generalization capability . The experimental results show that Perplexity has the best generalization capability while having limited detection accuracy and detection speed. Based on that, we discuss the strengths and limitations of Perplexity , e . g ., Perplexity is unsuitable for high-level programming languages. Finally, we provide recommendations to improve Perplexity and apply it in practice. As the first large-scale investigation on detecting LLMgCode from HaCode, this article provides a wide range of findings for future improvement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412400230",
    "type": "article"
  },
  {
    "title": "Advancing Code Coverage: Incorporating Program Analysis with Large Language Models",
    "doi": "https://doi.org/10.1145/3748505",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Chen Yang; Junjie Chen; Bin Lin; Z. Y. Wang; Jianyi Zhou",
    "corresponding_authors": "",
    "abstract": "Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-enhanced techniques, achieving an average improvement of 34.10% and 25.93% in terms of branch coverage.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412400243",
    "type": "article"
  },
  {
    "title": "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration of Large and Small Language Model",
    "doi": "https://doi.org/10.1145/3749840",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Lipeng Ma; Weidong Yang; Yixuan Li; Ben Fei; Mingjie Zhou; Shuhao Li; Sihang Jiang; Bo Xu; Yanghua Xiao",
    "corresponding_authors": "",
    "abstract": "Automated log analysis is crucial to ensure the high availability and reliability of complex systems. The advent of large language models (LLMs) in natural language processing (NLP) has ushered in a new era of language model-driven automated log analysis, garnering significant interest. Within this field, two primary paradigms based on language models for log analysis have become prominent. Small Language Models (SLMs) (such as BERT) follow the pre-train and fine-tune paradigm, focusing on the specific log analysis task through fine-tuning on supervised datasets. On the other hand, LLMs (such as ChatGPT) following the in-context learning paradigm, analyze logs by providing a few examples in prompt contexts without updating parameters. Despite their respective strengths, both models exhibit inherent limitations. By comparing SLMs and LLMs, we notice that SLMs are more cost-effective but less powerful, whereas LLMs with large parameters are highly powerful but expensive and inefficient. To trade-off between the performance and inference costs of both models in automated log analysis, this paper introduces an adaptive log analysis framework known as AdaptiveLog, which effectively reduces the costs associated with LLM while ensuring superior results. This framework collaborates an LLM and a small language model, strategically allocating the LLM to tackle complex logs while delegating simpler logs to the SLM. Specifically, to efficiently query the LLM, we propose an adaptive selection strategy based on the uncertainty estimation of the SLM, where the LLM is invoked only when the SLM is uncertain. In addition, to enhance the reasoning ability of the LLM in log analysis tasks, we propose a novel prompt strategy by retrieving similar error-prone cases as the reference, enabling the model to leverage past error experiences and learn solutions from these cases. We evaluate AdaptiveLog on different log analysis tasks, extensive experiments demonstrate that AdaptiveLog achieves state-of-the-art results across different tasks, elevating the overall accuracy of log analysis while maintaining cost efficiency. Our source code and detailed experimental data are available at https://github.com/LeaperOvO/AdaptiveLog-review .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412565290",
    "type": "article"
  },
  {
    "title": "Tracing Optimization for Performance Modeling and Regression Detection",
    "doi": "https://doi.org/10.1145/3749839",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Kaveh Shahedi; Heng Li; Maxime Lamothe; Foutse Khomh",
    "corresponding_authors": "",
    "abstract": "Software performance modeling plays a crucial role in developing and maintaining software systems. A performance model analytically describes the relationship between the performance of a system and its runtime activities. This process typically examines various aspects of a system’s runtime behavior, such as the execution frequency of functions or methods, to forecast performance metrics like program execution time. By using performance models, developers can predict expected performance and thereby effectively identify and address unexpected performance regressions when actual performance deviates from the model’s predictions. One common and precise method for capturing performance behavior is software tracing, which involves instrumenting the execution of a program, either at the kernel level (e.g., system calls) or application level (e.g., function calls). However, due to the nature of tracing, it can be highly resource-intensive, making it impractical for production environments where resources are limited. In this work, we propose statistical approaches to reduce tracing overhead by identifying and excluding performance-insensitive code regions, particularly application-level functions, from tracing while still building accurate performance models that can capture execution time degradations. We develop both dynamic methods that analyze runtime behavior patterns and static methods that examine code structure to identify performance-sensitive functions. Our methodology specifically targets execution time as the primary performance metric, building models that capture the relationship between function call frequencies and overall program latency. By selecting an optimal set of functions to be traced, we can construct optimized performance models that achieve an R 2 score of up to 99% and, in some cases, outperform full tracing models (i.e., models using non-optimized tracing data), while significantly reducing the tracing overhead by more than 80% in most cases. Our optimized performance models can also effectively detect performance regressions in our studied programs, demonstrating their usefulness in distinguishing between normal workload variations and actual performance degradations. Finally, our approach is fully automated, making it ready to be used in production environments with minimal human effort.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412565297",
    "type": "article"
  },
  {
    "title": "Commit Messages Generation Based on Core Changes",
    "doi": "https://doi.org/10.1145/3750039",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Yuan Huang; Zhicao Tang; Xiangping Chen; C Yang; Zibin Zheng; Xian Zhou Xian Zhou",
    "corresponding_authors": "",
    "abstract": "Commits messages play a crucial role in helping developers efficiently comprehend code modifications. Due to the time pressure of project iteration or poor message-writing practices, many commits suffer from missing messages. To address this issue, researchers have explored the automated generation of commit messages. Because of the truncation mechanism of the learning-based model, most of the current studies focus on code changes appearing at the beginning of a commit into the model for commit message generation. This may not be the best strategy for commit message generation because each code change in a commit contributes unequally to its overall purpose. To better generate commit messages, we propose a novel method that identifies the core code change in a commit for commit message generation. Specifically, we employ a method to predict the relative importance of the classes contained in a commit, and the code change of the class with the highest importance score (i.e., core change) is used to generate the commit message. Incorporating core change information can boost the performance of other existing methods (such as NMT, NNGen, CoreGen, etc.). Building on this insight, we develop CCGen - a Core Change-based Generation model that integrates a Transformer architecture with CodeBERT-enhanced encoding to leverage code semantics. The experiment demonstrates that the proposed method for commit message generation outperforms the state-of-the-art by 18.47% on average across 7 metrics including 19.97 on ROUGE-L.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412602623",
    "type": "article"
  },
  {
    "title": "A Review of Learning-based Smart Contract Vulnerability Detection: A Perspective on Code Representation",
    "doi": "https://doi.org/10.1145/3750042",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Ben Wang; Yanxiang Tong; Shunhui Ji; Hai Dong; Xiapu Luo; Pengcheng Zhang",
    "corresponding_authors": "",
    "abstract": "With the rapid development of blockchain technology, smart contract applications have become increasingly widespread. However, vulnerabilities in contracts may be exploited by attackers, causing serious financial losses. In recent years, learning-based approaches have gained prominence for their accuracy and efficiency by automatically extracting explicit syntactic or semantic features from a large number of smart contracts with minimal manual intervention. In this article, we conduct a comprehensive analysis and ultimately select 61 scientific publications to provide researchers, especially beginners, with a comprehensive understanding of the learning-based detection process and guidance on selecting appropriate code representations. We firstly introduce common types of vulnerabilities, detail uncovered vulnerabilities and summarize datasets used in learning-based methods. Then, we elaborate on the general process of learning-based detection and classify existing publications based on code representations, including sequence, tree, graph, and mixed feature. Finally, we summarize the progress of existing work and explore future research directions in this field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412602639",
    "type": "review"
  },
  {
    "title": "Learning Software Bug Reports: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3750040",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Guoming Long; Jingzhi Gong; Hui Fang; Tao Chen",
    "corresponding_authors": "",
    "abstract": "The recent advancement of artificial intelligence, in particular machine learning (ML), has witnessed its significant growth in various software engineering research fields. Among them, bug report analysis is one of such examples as it aims to automatically understand, extract and correlate information from the reports with the help of ML approaches. Despite the importance of ML in automating and enhancing bug report analysis, a comprehensive review that systematically examines the state-of-the-art in this area is still lacking. In this paper, we provide a systematic literature review on this promising research topic. Our review covers 1,825 papers, from which we extract 204 most relevant studies for detailed analysis. Based on the statistics and trends observed in these reviewed studies, we obtained seven key findings summarized as follows: 1) the extensive use of Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM) and \\(k\\) -Nearest Neighbor ( \\(k\\) NN) for bug report analysis, noting the underutilization of more advanced models like BERT due to their complexity and computational demands. 2) Word2Vec and TF-IDF are the most common methods for feature representation, with a notable increase in deep learning-based methods in recent years. 3) Stop word removal is the most common preprocessing method, followed by tokenization and stemming. Structural methods surged post-2020. 4) Eclipse and Mozilla are the most frequently evaluated software projects, reflecting their prominence in the field. 5) Bug categorization is the most popular task, followed by bug localization, assignment, and severity/priority prediction, with a growing interest in bug report summarization driven by advancements in NLP. 6) Most studies focus on general bug types, but there is increasing attention on specific bugs such as non-functional and performance bugs. 7) Common evaluation metrics include F1-score, Recall, Precision, and Accuracy, but bug report related evaluation metrics have not received significant attention. The majority of studies prefer \\(k\\) -fold cross-validation for model evaluation. and 8) many studies lack robust statistical tests or effect size measurements. Finally, based on the key findings, we discover six promising future research directions, by which we hope, together with the findings, can offer useful insights to practitioners of this particular research direction.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412644720",
    "type": "article"
  },
  {
    "title": "It Is Giving Major Satisfaction: Why Fairness Matters for Software Practitioners",
    "doi": "https://doi.org/10.1145/3757060",
    "publication_date": "2025-07-30",
    "publication_year": 2025,
    "authors": "Emeralda Sesari; Federica Sarro; Ayushi Rastogi",
    "corresponding_authors": "",
    "abstract": "Software practitioners often encounter workplace unfairness, such as unequal recognition and gender bias. While the link between fairness and job satisfaction has been established in other fields, its relevance to software professionals remains underexplored. This study examines how fairness perceptions relate to job satisfaction among software practitioners, focusing on both general trends and demographic-specific differences. We conducted an online survey of 108 software practitioners, followed by ordinal logistic regression to analyze the relationship between fairness perceptions and job satisfaction in software engineering contexts, with moderation analysis examining how this relationship varies across demographic groups. Our findings indicate that all four fairness dimensions (namely distributive, procedural, interpersonal, and informational fairness) significantly affect overall job satisfaction and satisfaction with job security. Among these, interpersonal fairness has the biggest impact. The relationship between fairness and job satisfaction is stronger for female, ethnically underrepresented, less experienced practitioners, and those with work limitations. Fairness in authorship emerged as an important factor for job satisfaction collectively, while fairness in policy implementation, high-demand situations, and working hours impacted specific demographic groups. This study highlights the role of fairness among software practitioners, offering strategies for organizations to promote fair practices and targeted approaches for certain demographic groups.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412751134",
    "type": "article"
  },
  {
    "title": "Automated Identification of Sexual Orientation and Gender Identity Discriminatory Texts from Issue Comments",
    "doi": "https://doi.org/10.1145/3757739",
    "publication_date": "2025-08-01",
    "publication_year": 2025,
    "authors": "Sayma Sultana; Jaydeb Sarker; Farzana Israt; Rajshakhar Paul; Amiangshu Bosu",
    "corresponding_authors": "",
    "abstract": "In an industry dominated by straight men, many developers representing other gender identities and sexual orientations often encounter hateful or discriminatory messages. Such communications pose barriers to participation for women and LGBTQ+ persons. Due to sheer volume, manual inspection of all communications for discriminatory communication is infeasible for a large-scale Free Open-Source Software (FLOSS) community. To address this challenge, this study proposes an automated mechanism to identify Sexual Orientation and Gender Identity Discriminatory (SGID) texts in software developers’ communications. On this goal, we trained and evaluated SGID4SE (Sexual orientation and Gender Identity Discriminatory text identification for (4) Software Engineering texts), a supervised learning-based tool. SGID4SE incorporates six preprocessing steps and ten state-of-the-art algorithms. SGID4SE employs six distinct strategies to enhance the performance of the minority class. We empirically evaluated each strategy and identified an optimum configuration for each algorithm. In our ten-fold cross-validation-based evaluations, a BERT-based model achieves the best performance with 85.9% precision, 80.0% recall, and 82.9% F1-score for the SGID class. This model achieves 95.7% accuracy and a Matthews Correlation Coefficient of 80.4%. Our dataset and tool establish a foundation for further research in this direction.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412827313",
    "type": "article"
  },
  {
    "title": "Enhancing Task In-Progress Time Predictions through Affective and Personality Factors",
    "doi": "https://doi.org/10.1145/3749984",
    "publication_date": "2025-08-01",
    "publication_year": 2025,
    "authors": "Leo Silva; Cephas Alves da Silveira Barreto; Margarida Pedroso de Lima; Henrique Madeira",
    "corresponding_authors": "",
    "abstract": "Software developers’ personality traits, emotional states, and stress levels are crucial in their task performance. This study aims to enhance the prediction of task in-progress time by integrating traditional features, such as developers’ experience and task estimates, with affective states and personality traits. This paper reports a long-term empirical study across seven agile projects in four software development companies, applying various machine learning algorithms to assess the predictive power of these combined features, evaluating them primarily through validation accuracy score. Also, we investigated the impact of weighting developers’ affective states based on their personality traits on model performance. Incorporating developers’ affective states and personality traits improved the task in-progress time prediction in 42.59% of classifier, dataset, and scaling combinations, with oversampled combinations achieving up to 8.4% higher validation accuracy than traditional feature models. The innovative weighting strategy improved 31.48% of the combinations. Our best model achieved a validation accuracy of 0.85. These findings suggest that integrating affective and personality data can significantly improve task in-progress time predictions, with significant implications for project planning and task allocation in software development.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412827328",
    "type": "article"
  },
  {
    "title": "AcTracer: Active Testing of Large Language Model via Multi-Stage Sampling",
    "doi": "https://doi.org/10.1145/3744340",
    "publication_date": "2025-08-01",
    "publication_year": 2025,
    "authors": "Yuheng Huang; Jiayang Song; Qiang Hu; Felix Juefei-Xu; Lei Ma",
    "corresponding_authors": "",
    "abstract": "Performance evaluation plays a crucial role in the development life cycle of large language models (LLMs). It estimates the model’s capability, elucidates behavior characteristics, and facilitates the identification of potential issues and limitations, thereby guiding further improvement. Given that LLMs’ diverse task-handling abilities stem from large volumes of training data, a comprehensive evaluation also necessitates abundant, well-annotated, and representative test data to assess LLM performance across various downstream tasks. However, the demand for high-quality test data often entails substantial time, computational resources, and manual efforts, sometimes causing the evaluation to be inefficient or impractical. To address these challenges, researchers propose active testing, which estimates the overall performance by selecting a subset of test data. Nevertheless, the existing active testing methods tend to be inefficient, even inapplicable, given the unique new challenges of LLMs ( e.g. , diverse task types, increased model complexity, and unavailability of training data). To mitigate such limitations and expedite the development cycle of LLMs, in this work, we introduce AcTracer, an active testing framework tailored for LLMs that strategically selects a small subset of test data to achieve a more accurate performance estimation for LLMs. AcTracer utilizes both internal and external information from LLMs to guide the test sampling process, reducing variance through a multi-stage pool-based active selection. Our experiment results demonstrate that AcTracer achieves state-of-the-art performance compared to existing methods across various tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412827352",
    "type": "article"
  },
  {
    "title": "Why Do GitHub Actions Workflows Fail? An Empirical Study",
    "doi": "https://doi.org/10.1145/3749371",
    "publication_date": "2025-07-18",
    "publication_year": 2025,
    "authors": "Lianyu Zheng; Shuang Li; Huang Xi; Jiangnan Huang; Bin Lin; Jinfu Chen; Jifeng Xuan",
    "corresponding_authors": "",
    "abstract": "GitHub Actions (GHA), a built-in continuous integration and continuous delivery (CI/CD) service of GitHub, has been widely adopted by developers, streamlining the automation of software development workflows. Despite its popularity, failures frequently occur during GHA workflow executions. Fixing these failures often requires significant human effort, and unsuccessful workflow executions waste computing resources. Understanding the reasons behind workflow failures could provide valuable insights for troubleshooting the existing issues of CI/CD and further improving the development process. In this paper, we present an empirical study to reveal the reasons behind GHA workflow failures. By manually analyzing 375 failed workflow executions across 260 open-source Java projects, we built a comprehensive taxonomy categorizing the common failure types. The taxonomy was further validated by surveying 151 developers. This study is the first empirical work to analyze GHA workflow failures, bringing valuable knowledge to the field of continuous integration in software engineering. Moreover, our taxonomy and survey results not only underscore the critical need for better tools and practices to mitigate these failures but also indicate the directions to enhance the efficiency and reliability of CI/CD pipelines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413056530",
    "type": "article"
  },
  {
    "title": "An On-the-fly Synthesis Framework for LTL over Finite Traces",
    "doi": "https://doi.org/10.1145/3749101",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "Shengping Xiao; Yongkang Li; Shufang Zhu; Jun Sun; Jianwen Li; Geguang Pu; Moshe Y. Vardi",
    "corresponding_authors": "",
    "abstract": "We present an on-the-fly synthesis framework for Linear Temporal Logic over finite traces ( LTL f ) based on top-down deterministic automata construction. Existing approaches rely on constructing a complete Deterministic Finite Automaton (DFA) corresponding to the LTL f specification, a process with doubly exponential complexity relative to formula size in the worst case. In this case, the synthesis cannot be conducted until the entire DFA is constructed. This inefficiency is the main bottleneck of existing approaches. To address this challenge, we first present a method for converting LTL f into Transition-based DFA (TDFA) by directly leveraging LTL f semantics, incorporating intermediate results as direct components of the final automaton to enable parallelized synthesis and automata construction. We then explore the relationship between LTL f synthesis and TDFA games and subsequently develop an algorithm for performing LTL f synthesis via on-the-fly TDFA game solving. This algorithm traverses the state space in a global forward manner combined with a local backward method, along with detecting strongly connected components. Moreover, we introduce two optimization techniques — model-guided synthesis and state entailment — to enhance the practical efficiency of our approach. Experimental results demonstrate that our on-the-fly approach achieves the best performance on the tested benchmarks and effectively complements existing approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413078449",
    "type": "article"
  },
  {
    "title": "An Empirical Study on Language Models for Generating Log Statements in Test Code",
    "doi": "https://doi.org/10.1145/3759915",
    "publication_date": "2025-08-12",
    "publication_year": 2025,
    "authors": "Honglin Shu; Dong Wang; Antonio Mastropaolo; Gabriele Bavota; Yasutaka Kamei",
    "corresponding_authors": "",
    "abstract": "Log statements play a critical role in modern software development, capturing essential runtime information necessary for software maintenance. Recently, new techniques have been developed to automate logging activities, allowing log statements to be injected into code by identifying specific code locations, selecting the appropriate log level, and generating meaningful log messages that describe the behavior being logged. Although automated logging in production code has attracted significant attention, little focus has been given to the injection of logs in test code. To fill this gap, we conduct an empirical study on 5,206,759 Java test methods collected from 6,405 GitHub projects to explore and disclose the effectiveness and limitations of Pre-trained Language Models (PLMs) and Large Language Models (LLMs) for generating and injecting test log statements. Our findings demonstrate that general-purpose LLMs like GPT-3.5-Turbo, when properly instructed to inject logging statements in test methods, performs comparably to the best-performing PLMs on predicting log level. Additionally, GPT-3.5-Turbo substantially outperforms the best in PLMs on predicting log position with a 33.97% improvement while also achieving superior performance in predicting log messages in terms of BLEU and ROUGE . This work takes the first step toward evaluating the capability of PLMs and LLMs to generate test log statement. This work takes the first step toward evaluating the capability of PLMs and LLMs to generate test log statements. To facilitate future research, we have open-sourced all data and source code used in this work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413105325",
    "type": "article"
  },
  {
    "title": "Security of Language Models for Code: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3735554",
    "publication_date": "2025-08-12",
    "publication_year": 2025,
    "authors": "Yuchen Chen; Weisong Sun; Chunrong Fang; Zhenpeng Chen; Yifei Ge; Tingxu Han; Quanjun Zhang; Yang Liu; Zhenyu Chen; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "Language models for code (CodeLMs) have emerged as powerful tools for code-related tasks, outperforming traditional methods and standard machine learning approaches. However, these models are susceptible to security vulnerabilities, drawing increasing research attention from domains such as software engineering, artificial intelligence, and cybersecurity. Despite the growing body of research focused on the security of CodeLMs, a comprehensive survey in this area remains absent. To address this gap, we systematically review 68 relevant papers, organizing them based on attack and defense strategies. Furthermore, we provide an overview of commonly used language models, datasets, and evaluation metrics, and highlight open-source tools and promising directions for future research in securing CodeLMs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413105341",
    "type": "article"
  },
  {
    "title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models",
    "doi": "https://doi.org/10.1145/3757740",
    "publication_date": "2025-08-11",
    "publication_year": 2025,
    "authors": "Xiaohan Bi; Binhang Qi; Hailong Sun; Xiang Gao; Yue Yu; Xiaojun Liang",
    "corresponding_authors": "",
    "abstract": "With the growing incorporation of deep neural network (DNN) models into modern software systems, the prohibitive construction costs of DNN models have become a significant challenge in software development. To address this challenge, model reuse has been widely applied to reduce model training costs; however, indiscriminately reusing an entire model may incur significant inference overhead. Consequently, DNN modularization — borrowing the idea of modularization in software engineering — has increasingly gained attention, enabling module reuse by decomposing a DNN model into modules. In particular, the emerging modularizing-while-training (MwT) paradigm, which outperforms modularizing-after-training by incorporating modularization into the model's training process, has been demonstrated as a more effective approach for DNN modularization. However, existing MwT approaches focus on small-scale convolutional neural network (CNN) models at the convolutional kernel level. They struggle to handle diverse DNNs and large-scale models, particularly Transformer-based models, which consistently achieve state-of-the-art results across various tasks. To address these limitations, we propose NeMo, a scalable and more generalizable MwT approach. NeMo operates at the neuron level — a fundamental component common to all DNNs — thereby ensuring applicability to Transformers and various DNN architectures. Moreover, we design a contrastive learning-based modular training method, equipped with an effective composite loss function, hence being scalable to large-scale models. Comprehensive experiments on two Transformer-based models and four CNN models across two widely-used classification datasets demonstrate NeMo's superiority over the state-of-the-art MwT method. Results show average performance gains of 1.72% in module classification accuracy and a 58.10% reduction in module size. Our findings demonstrate that NeMo exhibits efficacy across both CNN and large-scale Transformer-based models. Moreover, a case study based on open-source projects demonstrates the potential benefits of NeMo in practical scenarios, offering a promising approach for achieving scalable and generalizable DNN modularization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413214067",
    "type": "article"
  },
  {
    "title": "Leveraging Reviewer Experience in Code Review Comment Generation",
    "doi": "https://doi.org/10.1145/3762183",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Hong Yi Lin; Patanamon Thongtanunam; Christoph Treude; Michael W. Godfrey; Chunhua Liu; Wachiraphan Charoenwet",
    "corresponding_authors": "",
    "abstract": "Modern code review is a ubiquitous software quality assurance process aimed at identifying and resolving potential issues (e.g., functional, evolvability) within newly written code. Despite its effectiveness, the process demands large amounts of effort from the human reviewers involved. To help alleviate this workload, researchers have trained various deep learning based language models to imitate human reviewers in providing natural language code reviews for submitted code. Formally, this automation task is known as code review comment generation. Prior work has demonstrated improvements in code review comment generation by leveraging machine learning techniques and neural models, such as transfer learning and the transformer architecture. However, the quality of the model generated reviews remain sub-optimal due to the quality of the open-source code review data used in model training. This is in part due to the data obtained from open-source projects where code reviews are conducted in a public forum, and reviewers possess varying levels of software development experience, potentially affecting the quality of their feedback. To accommodate for this variation, we propose a suite of experience-aware training methods that utilise the reviewers’ past authoring and reviewing experiences as signals for review quality. Specifically, we propose experience-aware loss functions (ELF), which use the reviewers’ authoring and reviewing ownership of a project as weights in the model's loss function. Through this method, experienced reviewers’ code reviews yield larger influence over the model's behaviour. Compared to the SOTA model, ELF was able to generate higher quality reviews in terms of accuracy (e.g., +29% applicable comments), informativeness (e.g., +56% suggestions), and issue types discussed (e.g., +129% functional issues identified). The key contribution of this work is the demonstration of how traditional software engineering concepts such as reviewer experience can be integrated into the design of AI-based automated code review models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413333491",
    "type": "article"
  },
  {
    "title": "A Multi-Scale Hypergraph-Based Approach for Third-Party Library Recommendation in Mobile App Development",
    "doi": "https://doi.org/10.1145/3764114",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Abhinav Jamwal; Sandeep Kumar",
    "corresponding_authors": "",
    "abstract": "In mobile app development, selecting the right third-party libraries (TPLs) is crucial to enhance functionality, improve code quality, and speed up the development process. However, recommending appropriate TPLs remains challenging due to the complexity of app-library interactions and the need to capture high-order relationships. Existing methods, such as collaborative filtering and graph-based approaches, often fail to adequately address these complexities. To address this challenge, we propose MsRec, a multiscale hypergraph neural network-based approach for TPL recommendation. MsRec uses hypergraphs to model interactions in groups of different sizes, leading to a detailed representation of the relationship between the application and the library. By modeling the strength, category, and functionality of interactions within each category, our framework improves the accuracy and diversity of recommendations. The multiscale hypergraph structure supports fine-grained relational reasoning, making it particularly effective in this context. Extensive experiments on real-world datasets demonstrate that MsRec outperforms current state-of-the-art methods, providing relevant and diverse TPL recommendations. Additionally, our model shows strong performance on various benchmarks, highlighting its ability to effectively handle complex app-library interactions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413635643",
    "type": "article"
  },
  {
    "title": "BePilot: An AI Programming Assistant for Compiler Backend Development",
    "doi": "https://doi.org/10.1145/3764585",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Ming Zhong; Xin Sun; Fang Lv; Lulin Wang; Hongna Geng; Lin Qiu; Huimin Cui; Xiaobing Feng",
    "corresponding_authors": "",
    "abstract": "Compiler backends are tasked with generating executable machine code for various processors. As the diversity of processors continues to grow, it is imperative for programmers to tailor specific compiler backends to accommodate each one. However, compiler backend development remains a labor-intensive and time-consuming process, with limited automation tools available. Although large language models (LLMs) have demonstrated strong abilities in code completion and generation tasks, the lack of appropriate datasets for compiler backend development limits the application of LLMs in this field. this paper, we introduce ComBack++, a multilingual dataset covering C/C++, Machine Description, and TableGen, with 184 backends from GCC and LLVM, four backend-specific tasks. Based on ComBack++, we present BePilot, a compiler backend-specific LLM available in two sizes: BePilot-1.5B and BePilot-7B. We also introduce CB-Retriever, a retriever that constructs few-shot prompts via in-context learning to improve vanilla LLM performance in resource-constrained settings. Experimental results show that BePilot-1.5B and BePilot-7B achieve significantly higher accuracy across four tasks in ComBack++ compared to twelve baseline LLMs (125M – 34B parameters). In addition, CB-Retriever consistently boosts the accuracy of six mainstream LLMs. Both BePilot-1.5B and BePilot-7B, as well as vanilla LLMs augmented with CB-Retriever, outperform the traditional manual compiler backend development approach (Fork-Flow) in efficiency across all four tasks in ComBack++. Furthermore, human evaluation by four experienced compiler backend developers confirms that BePilot not only improves development efficiency over Fork-Flow, but also surpasses commercial AI programming assistants such as GPT-4o-mini and Gemini2-Flash in terms of code quality. These findings confirm that BePilot and CB-Retriever can substantially enhance compiler backend development efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413779626",
    "type": "article"
  },
  {
    "title": "Patch Generation in APR: A Survey from the Perspectives of Utilizing LLMs and Using APR-Specific Information",
    "doi": "https://doi.org/10.1145/3764584",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Yimin Yang; Chuanyi Li; Zhifeng Han; Rui Li; Kui Xu; Qiaoxing Li; Wenkang Zhong; Zongwen Shen; Zhongxiang Fei; Jidong Ge; Bin Luo",
    "corresponding_authors": "",
    "abstract": "Automated Program Repair (APR) is a crucial task in software development and maintenance, aiming to patch software bugs automatically without human intervention. The rise of Large Language Models (LLMs) has significantly advanced APR. However, as researchers delve deeper into APR as a downstream task for LLMs, a key challenge remains: how to effectively integrate APR-specific information to complement LLMs capabilities. This survey revisits 124 existing APR studies, most from 2021 to 2024, from two perspectives: Utilizing LLMs and APR-specific Information. First, we define the concept of APR-specific information. Then, we summarize techniques for utilizing LLMs in patch generation in four dimensions. Next, we explore critical factors influencing the effectiveness of generating patches by LLMs, such as prompting context and fine-tuning configurations. After that, we focus on the evaluation of generated patches, including benchmarks, reasoning cost scaling, etc. Furthermore, we distill all APR-specific information (e.g., bug-fix pairs and error messages), highlighting their unique importance and features. Finally, we comprehensively outline challenges, limitations, and potential future research directions in APR in the LLM era. To conclude, by adopting the two perspectives, we aim to provide valuable insights into mining and leveraging APR-specific information in utilizing LLMs process for the APR community.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413779795",
    "type": "article"
  },
  {
    "title": "The Impact of Class Noise-handling on the Effectiveness of Machine Learning-based Methods for Build Outcome and Code Change Request Predictions",
    "doi": "https://doi.org/10.1145/3764864",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Khaled Al-Sabbagh; Miroslaw Staron; Regina Hebig",
    "corresponding_authors": "",
    "abstract": "Machine learning-based methods for optimizing build processes and code review processes are commonly used to accelerate feature delivery to end-users in modern software engineering. These methods leverage large volumes of historical code changes to train models on predicting and preventing issues in the code-base that could delay code integrations. The objective of this paper is to improve these methods by reducing the impact of noise on their predictive performance. In this paper, we investigate the impact of several class noise-handling techniques on improving the predictive capabilities of machine learning models for build outcome and negative code review prediction tasks. We conduct a series of computational experiments using data from 110 Java open-source projects and examine the effectiveness of two removal-based statistical techniques – Majority Filter (MF) and Consensus Filter (CF) – and two corrective techniques – domain knowledge-based (DB) and CleanLab. Our results show that removal-based techniques significantly improve model predictive performance for both build outcome prediction and negative code review predictions. For build outcome prediction, applying the MF improved the F1-score from 82% to 97%, and MCC from 0.13 to 0.58. In negative code review predictions, MF improved the F1-score from 17% to 53%, and MCC from -0.03 to 0.57. DB proved effective primarily in the context of code review comments, but less so for build outcome predictions. CleanLab demonstrated a more conservative approach to noise detection, retaining complex code constructs that other techniques identified as noisy. While applying CleanLab resulted in more consistent predictions, its overall impact on model predictive performance was more moderate compared to removal-based techniques. Additionally, our findings show that hyperparameter tuning, when applied independently or in combination with CleanLab can improve model performance. However, these improvements did not surpass the gains achieved through applying the removal-based techniques alone. We conclude that applying removal-based techniques to the training data of code changes is necessary to leverage the prediction of build outcomes and negative code review comments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413779807",
    "type": "article"
  },
  {
    "title": "CCIHunter: Enhancing Smart Contract Code-Comment Inconsistencies Detection via Two-Stage Pre-training",
    "doi": "https://doi.org/10.1145/3764867",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Ziwei Li; Jiajing Wu; Zhiying Wu; D. Tan; Weiao Zou; Zigui Jiang; Yi Zhen; Zhang Zheng; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "Smart contracts are self-executing computer programs on blockchains. With the development of blockchain technology, the number of smart contracts has grown rapidly, as has the concern for their security. Regrettably, inconsistencies between the logic implemented in the code and the intentions described in the comments, known as Code-Comment Inconsistencies (CCI), are frequently present in some smart contracts. These inconsistencies can mislead readers in understanding the contract code and, in severe cases, may lead to vulnerabilities and economic losses. Existing learning-based methods are not tailored for smart contract languages, overlook the the issue of insufficient context information caused by comment references and nested intentions, and rely on large-scale labeled data; whereas rule-based methods struggle to accommodate the flexibility with which developers express intentions, often resulting in false positives. To tackle the challenges posed by insufficient context information and the scarcity of labeled data, we introduce CCIHunter, a tool designed to detect CCIs in smart contracts. CCIHunter addresses the issue of insufficient context information during data modeling and incorporates a two-stage pretraining process that does not depend on labeled data to enhance its detection capabilities. Specifically, CCIHunter enhances comments based on templates and models code as a heterogeneous graph based on function calls. It utilizes CodeBERT and UniMp to generate embeddings for comments and code, respectively, and then calculates the similarity between these two embeddings. Consistency is judged by combining code embeddings, comment embeddings, and similarity scores. Notably, CCIHunter undergoes a two-stage pre-training that includes contrastive learning and mutation analysis, aiming to improve its ability to bridge the gap between code and comments and to focus on code elements at different granularities. Experimental results demonstrate that CCIHunter achieves a precision of 0.95, a recall of 0.90, and an F1 score of 0.93, outperforming existing tools.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413788962",
    "type": "article"
  },
  {
    "title": "How Configurable is the Linux Kernel? Analyzing Two Decades of Feature-Model History – RCR Report",
    "doi": "https://doi.org/10.1145/3764666",
    "publication_date": "2025-08-29",
    "publication_year": 2025,
    "authors": "Elias Kuiter; Chico Sundermann; Thomas Thüm; Tobias Heß; Sebastian Krieter; Gunter Saake",
    "corresponding_authors": "",
    "abstract": "This is the RCR report accompanying our TOSEM’25 paper How Configurable is the Linux Kernel? Analyzing Two Decades of Feature-Model History . In this report, we bundle all data relevant to our paper for the purpose of reproducibility and long-term archival. This includes the feature-model extraction tool torte , as well as a comprehensive feature-model dataset and experimental results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413816538",
    "type": "article"
  },
  {
    "title": "WACANA: A Concolic Analyzer for Detecting On-chain Data Vulnerabilities in WASM Smart Contracts",
    "doi": "https://doi.org/10.1145/3757914",
    "publication_date": "2025-09-03",
    "publication_year": 2025,
    "authors": "Wansen Wang; Caichang Tu; Zhaoyi Meng; Wenchao Huang; Yan Xiong",
    "corresponding_authors": "",
    "abstract": "WebAssembly (WASM) has emerged as a crucial technology in smart contract development for several blockchain platforms. Unfortunately, since their introduction, WASM smart contracts have been subject to several security incidents caused by contract vulnerabilities, resulting in substantial economic losses. However, existing tools for detecting WASM contract vulnerabilities have accuracy limitations, one of the main reasons being the coarse-grained emulation of the on-chain data APIs. In this paper, we introduce WACANA, an analyzer for WASM contracts that accurately detects vulnerabilities through fine-grained emulation of on-chain data APIs. WACANA precisely simulates both the structure of on-chain data tables and their corresponding API functions, and integrates concrete and symbolic execution within a coverage-guided loop to balance accuracy and efficiency. Evaluations on a vulnerability dataset of 2,012 contracts show WACANA outperforming state-of-the-art tools in accuracy. Further validation on 5,602 real-world contracts confirms WACANA’s practical effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413943341",
    "type": "article"
  },
  {
    "title": "Detecting and Analyzing Fine-grained Third-party Library Dependencies in Solidity Smart Contracts",
    "doi": "https://doi.org/10.1145/3765755",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Sicheng Hao; Yuhong Nan; Zeqin Liao; Juan Zhai; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "Solidity is the primary programming language for writing smart contracts. As a lightweight language, Solidity does not have a unified way to manage third-party library (TPL) dependencies. Instead, the copy-and-paste pattern and other dependency managers such as NPM and Git submodules have become alternatives. However, these mechanisms significantly increase the complexity of TPL usage with security concerns. Similar to other programming language ecosystems, incorrect TPL usage can influence the reliability of contracts and even introduce vulnerabilities from outdated versions. Therefore, there is an urgent need to understand and comprehend Solidity TPL dependencies. In this work, we conduct a comprehensive study on TPL dependency usage in Solidity. To achieve this, we first present SPADE, which leverages additional metadata (e.g., package and remapping configurations) to infer fine-grained TPL dependencies with version and contract details in various Solidity projects. With SPADE, we investigate a broad spectrum of 5,242 Solidity repositories to understand the TPL dependencies, including their landscape and version-level usage. Our research reveals a set of interesting and important findings that can be beneficial to the Solidity ecosystem. In particular, TPL dependencies are prevalent in Solidity, but the version management remains inadequate. The propagation of vulnerability is severe, affecting 8.87% of the repositories. Finally, we use on-chain contracts to validate the findings and provide suggestions for future research and development on Solidity TPL dependencies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413982174",
    "type": "article"
  },
  {
    "title": "Enhancing Expressivity, Modularity and Rigour of Graphical Data Modelling with Fragmenta",
    "doi": "https://doi.org/10.1145/3765738",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Nuno Amálio",
    "corresponding_authors": "Nuno Amálio",
    "abstract": "Graphical data (or conceptual) modelling, software engineering’s most popular application of modelling, needs an uplift. Graphical data models easily become unwieldy, even for trivial medium-sized systems. Practitioners long for improved modelling primitives and mechanisms that improve modelling, namely, its expressiveness, the way it is experienced and its appeal. These are the drivers of Fragmenta , a formal data- and meta-modelling framework with advanced means to separate concerns. Fragmenta ’s novelties include: (i) multilevel modelling through vertical refinement, (ii) improved horizontal decomposition and composition of fragments through proxies, (iii) virtuals for lighter inheritance, and (iv) graphical model constraints. Fragmenta is the first data modelling framework with a mathematical foundation and a supporting implementation, combining both vertical and horizontal means to separate concerns.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413982588",
    "type": "article"
  },
  {
    "title": "TestLoop: A Process Model Describing Human-in-the-Loop Software Test Suite Generation",
    "doi": "https://doi.org/10.1145/3765754",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Matthew C. Davis; Sangheon Choi; A. Wei; Sam Estep; Brad A. Myers; Joshua Sunshine",
    "corresponding_authors": "",
    "abstract": "There is substantial diversity among testing tools used by software engineers. For example, fuzzers may target crashes and security vulnerabilities while Test sUite Generators (TUGs) may create high-coverage test suites. In the research community, test generation tools are primarily evaluated using metrics like bugs identified or code coverage. However, achieving good values for these metrics does not necessarily imply that these tools help software engineers efficiently develop effective test suites. To understand the test suite generation process, we performed a secondary analysis of recordings from a previously-published user study in which 28 professional software engineers used two tools to generate test suites for three programs with each tool. From these 168 recordings ( \\(28\\ users\\times 2\\ tools\\times 3\\ programs/tool\\) ), we extracted a process model of test suite generation called TestLoop that builds upon prior work and systematizes a user’s test suite generation process for a single function into 7 steps. We then used TestLoop’s steps to describe 8 prior and 10 new recordings of users generating test suites using the Jest, Hypothesis, and NaNofuzz test generation tools. Our results showed that TestLoop can be used to help answer previously hard-to-answer questions about how users interact with test suite generation tools and to identify ways that tools might be improved.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413983082",
    "type": "article"
  },
  {
    "title": "REST API Testing in DevOps: A Study on an Evolving Healthcare IoT Application",
    "doi": "https://doi.org/10.1145/3765744",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Hassan Sartaj; Shaukat Ali; Julie Marie Gjøby",
    "corresponding_authors": "",
    "abstract": "Healthcare Internet of Things (IoT) applications often integrate various third-party healthcare applications and medical devices through REST APIs, resulting in complex and interdependent networks of REST APIs. Oslo City’s healthcare department collaborates with various industry partners to develop these applications, enriched with diverse REST APIs that evolve during the DevOps process to accommodate evolving needs such as new features, services, and devices. Oslo City’s primary goal is to utilize automated solutions for continuous testing of REST APIs at each evolution stage to ensure dependability. Although the literature offers various automated REST API testing tools, their effectiveness in regression testing of the evolving REST APIs of healthcare IoT applications within a DevOps context remains undetermined. This paper evaluates state-of-the-art and well-established REST API testing tools—specifically, RESTest, EvoMaster, Schemathesis, RESTler, and RestTestGen—for the regression testing of a real-world healthcare IoT application, considering failures, faults, coverage, regressions, and cost. We conducted experiments using all accessible REST APIs (17 APIs with 120 endpoints), and 14 releases evolved during DevOps. Overall, all tools generated tests leading to several failures, 18 potential faults, up to 84% coverage, and 23 regressions. Over 70% of tests generated by all tools fail to detect failures, resulting in significant overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413983086",
    "type": "article"
  },
  {
    "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval",
    "doi": "https://doi.org/10.1145/3765748",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Wenchao Gu; Zongyi Lyu; Yanlin Wang; Hongyu Zhang; Cuiyun Gao; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Code retrieval aims to provide users with desired code snippets based on users’ natural language queries. With the development of deep learning technologies , adopting pre-trained models for this task has become mainstream. Considering the retrieval efficiency, most of the previous approaches adopt a dual-encoder for this task, which encodes the description and code snippet into representation vectors, respectively. However, the model structure of the dual-encoder tends to limit the model’s performance, since it lacks the interaction between the code snippet and description at the bottom layer of the model during training. To improve the model’s effectiveness while preserving its efficiency, we propose a framework, which adopts S elf-Ada P tive Model Distillation for E fficient C od E R etrieval, named SPENCER. SPENCER first adopts the dual-encoder to narrow the search space and then adopts the cross-encoder to improve accuracy. To improve the efficiency of SPENCER, we propose a novel model distillation technique, which can greatly reduce the inference time of the dual-encoder while maintaining the overall performance. We also propose a teaching assistant selection strategy for our model distillation, which can adaptively select the suitable teaching assistant models for different pre-trained models during the model distillation to ensure the model performance. Extensive experiments demonstrate that the combination of dual-encoder and cross-encoder improves overall performance compared to solely dual-encoder-based models for code retrieval. Besides, our model distillation technique retains over 98% of the overall performance while reducing the inference time of the dual-encoder by 70%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413983232",
    "type": "article"
  },
  {
    "title": "From Cryptic to Clear - Training on LLM Explanations to Detect Smart Contract Vulnerabilities",
    "doi": "https://doi.org/10.1145/3765753",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Yizhou Chen; Zeyu Sun; Guoqing Wang; Qingyuan Liang; Xiao Yu; Dan Hao",
    "corresponding_authors": "",
    "abstract": "Smart contracts have revolutionized the way transactions are executed, offering decentralized and immutable frameworks. The immutability of smart contracts poses significant risks when vulnerabilities exist in their code, leading to financial losses. Despite advancements in using deep learning for smart contract vulnerability detection (SCVD), existing methods struggle with the complex logic and intricate semantics embedded within smart contract code. Large Language Models (LLMs) have shown promise in providing deeper insights into smart contract logic. However, LLMs, such as GPT, follow a decoder-only architecture and are trained in an unsupervised manner rather than learning specific labels. In the SCVD task, these LLMs have difficulty in capturing information related to vulnerabilities, leading to very low accuracy. Therefore, we propose CodeXplain, a novel SCVD approach that leverages the deep insights into code from LLM and the supervised learning capabilities of deep learning models to set the latest advance and performance. In particular, we deeply analyze 14 types of dangerous and common smart contract vulnerabilities. Based on the rationale of these vulnerabilities, nine perspective prompts are introduced to guide LLMs in generating code explanations that contribute to SCVD. Then, we propose a CodeT5-based semantic fusion module integrating smart contract code and code explanations. Finally, the performance of SCVD is improved by performing supervised learning on trusted labels. Experimental results on 3,544 real-world smart contracts demonstrate that CodeXplain outperforms 16 state-of-the-art SCVD methods, achieving an F1-score of 94.12% and an accuracy of 93.88%, surpassing all baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413983430",
    "type": "article"
  },
  {
    "title": "TypeNFuzz: Dynamic Type-aware Object Dependence Graph Guided Fuzzing for JavaScript Library Bug Discovery",
    "doi": "https://doi.org/10.1145/3765760",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Yishun Zeng; Wu Yue; Chao Zhang",
    "corresponding_authors": "",
    "abstract": "Node.js owes much of its popularity to an enormous library ecosystem. While this abundance speeds development, widely varying code quality complicates efforts to assure robustness. Effective library testing is hard for two reasons: (1) dynamic typing obscures the construction of valid, complex inputs, and (2) crucial internal logic is hidden behind shallow exports, making deep paths difficult to reach. Existing tools handle neither challenge well. To address these challenges and elevate library quality, we propose TypeNFuzz, a novel testing approach for Node.js libraries. TypeNFuzz integrates: 1. Type-driven Input Synthesis: mines TypeScript declaration files to build semantically correct objects, defeating dynamic-typing ambiguities. 2. Deep Reachability Exploration: fuzzing guided by a dynamic and type-aware object dependence graph, systematically triggering execution deep within the internal code paths to uncover deep logic. The evaluation demonstrates the effectiveness of TypeNFuzz: it achieves 1.70 - 6.74 times higher code coverage than state-of-the-art tools, directly attributed to its ability to handle complex types and penetrate deep logic. Critically, it uncovered 77 previously unknown defects in popular built-in and third-party libraries, significantly contributing to improved robustness and stability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413984104",
    "type": "article"
  },
  {
    "title": "Exploring the impact of cloud computing on software architecture for sustainability: A practitioners’ perspective",
    "doi": "https://doi.org/10.1145/3765747",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Sahar Ahmadisakha; Vasilios Andrikopoulos",
    "corresponding_authors": "",
    "abstract": "The ever-increasing use of cloud computing in software systems necessitates considering the sustainability implications of cloud-based software from the perspective of software architecture. Software architecture for these purposes is the set of decisions with lasting, system-wide impact effect on the system’s software elements, the relations among them, and the properties of both. In this context, understanding the implications of cloud adoption for the sustainability of software systems from the perspective of software practitioners is the key objective of this study. Specifically, we aim to determine the type of sustainability impact cloud adoption has on the architecture of these systems across different dimensions and associated quality requirements. To achieve our goal, we employ mining software repository techniques along with qualitative analysis on a constructed dataset on the Software Engineering, Software Quality Assurance and Testing, and DevOps forums of the Stack Exchange Q&amp;A platform. Our results confirm findings from previous work, showing a strong perceived positive effect of cloud computing on quality requirements and sustainability dimensions from the perspective of practitioners. Additionally, we identify 9 new quality requirements frequently employed in cloud-based architectures and their contributions to sustainability dimensions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413984388",
    "type": "article"
  },
  {
    "title": "On-the-fly Generation-Quality Enhancement of Deep Code Models via Model Collaboration",
    "doi": "https://doi.org/10.1145/3765752",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Weifeng Sun; N.A. G.Q.Huang; Meng Yan; Zhongxin Liu; Hongyan Li; Yan Lei; David Lo",
    "corresponding_authors": "",
    "abstract": "The growing prominence of deep code models in automating software engineering tasks is undeniable. However, their deployment encounters significant challenges in on-the-fly performance enhancement , which refers to dynamically improving the performance of deep code models during real-time execution. Conventional techniques, such as retraining or fine-tuning, are effective in controlled pre-deployment scenarios but fall short when adapting to on-the-fly adjustments post-deployment. CodeDenoise, a notable on-the-fly performance enhancement technology, leverages uncertainty-based methods to identify misclassified inputs and applies an input modification strategy to rectify classification errors. While effective for classification tasks, this approach is inapplicable to generative tasks due to two key challenges: ❶ Uncertainty-based methods are unsuitable for identifying challenging inputs , especially in generative tasks with diverse and open-ended outputs. Challenging inputs refers to a class of inputs where, due to the inherent complexity of the task or insufficient context in the input samples, the model struggles to generate high-quality outputs. ❷ Input modification strategies cannot be applied to generative tasks, as modifying the input can unpredictably affect the entire sequence of generated outputs. These limitations highlight the need for novel techniques that can enhance the generation quality of deep code models in real-time. To bridge this gap, we propose CodEn , a framework designed to enhance the generation quality of deployed deep code models through model collaboration and real-time output repair. CodEn employs an ensemble learning approach, integrating multiple generic output quality assessment metrics to identify challenging inputs . By combining these diverse metrics, CodEn overcomes the limitations of uncertainty-based methods, making it effective across various generative tasks. Additionally, we introduce an elaborate on-the-fly repair method for the outputs of challenging inputs , leveraging a large language model (LLM) and a novel dual-prompt strategy. This strategy utilizes both generation and selection-based prompts to provide potential fixes and employs an adaptive mechanism to select the optimal output. Our experiments, conducted on 12 deep code models across three pre-trained code models, three popular code-related generation tasks, and four datasets, demonstrate the effectiveness of CodEn . For example, in the assertion generation task, CodEn enhances the SAM (Semantic Accuracy Match) of baseline models with improvements ranging from 12.14% to 21.65%. In the bug fixing task, CodEn achieves exact match gains ranging from 17.51% to 30.64% on TFix dataset. For the code summarization task, CodEn significantly boosts performance across key metrics: BLEU scores improved by 5.72% \\(\\sim\\) 11.79%, ROUGE-L by 4.41% \\(\\sim\\) 7.70%, METEOR by 7.51% \\(\\sim\\) 12.29%, and CIDEr by 8.09% \\(\\sim\\) 15.80%. Besides, we conduct experiments of CodEn on different open-source LLMs and demonstrate that CodEn can still achieve significant improvements.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414004607",
    "type": "article"
  },
  {
    "title": "Self-monitoring of Developers’ Emotions: the Case of Agile Retrospective Meetings",
    "doi": "https://doi.org/10.1145/3766064",
    "publication_date": "2025-09-09",
    "publication_year": 2025,
    "authors": "Daniela Grassi; Filippo Lanubile; Nicole Novielli; Luigi Quaranta; Alexander Serebrenik",
    "corresponding_authors": "",
    "abstract": "Developers experience a wide range of emotions while creating software. Being able to identify the causes of one’s own and peers’ emotions can equip developers with the ability to regulate their behavior to restore positive moods and productivity. In this paper, we investigate to what extent self-monitoring of emotions can enhance agile retrospective meetings by improving the emotion awareness of participants . To this aim, we conducted a controlled experiment involving three software development teams involving two student teams and one professional developers team . The experiment design involves the collection of biometrics and self-reported information about emotions, which are then visualized before the retrospective meetings to inform discussion using EmoVizPhy, a tool that we designed and implemented for this aim. While students found that self-monitoring helped them recall significant emotional episodes, leading to more meaningful contributions during retrospectives, professional developers perceived limited benefits from this practice . Furthermore, based on the analysis of corrective actions identified by the participants during the study, we hypothesize that self-monitoring of emotions through EmoVizPhy may play a valuable role in facilitating the consolidation of new agile teams for which roles and collaboration dynamics are still being defined.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414145187",
    "type": "article"
  },
  {
    "title": "The Havoc Paradox in Generator-Based Fuzzing — RCR Report",
    "doi": "https://doi.org/10.1145/3767167",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Ao Li; Madonna Huang; Vasudev Vikram; Caroline Lemieux; Rohan Padhye",
    "corresponding_authors": "",
    "abstract": "This artifact is associated with the paper “The Havoc Paradox in Generator-Based Fuzzing.” It contains the implementation of various fuzzing techniques discussed in the paper, along with benchmarks, experimental data, and analysis scripts that support the findings regarding the havoc effect in generator-based fuzzing. The artifact enables the reproduction of all results demonstrating how different mutation strategies affect the performance of generator-based fuzzing techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414153585",
    "type": "article"
  },
  {
    "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3766552",
    "publication_date": "2025-09-09",
    "publication_year": 2025,
    "authors": "Ali Asgari; Milan de Koning; Pouria Derakhshanfar; Annibale Panichella",
    "corresponding_authors": "",
    "abstract": "Large language models and deep learning models designed for code intelligence have revolutionized the software engineering field due to their ability to perform various code-related tasks. These models can process source code and software artifacts with high accuracy in tasks such as code completion, defect detection, and code summarization; therefore, they can potentially become an integral part of modern software engineering practices. Despite these capabilities, robustness remains a critical quality attribute for deep-code models as they may produce different results under varied and adversarial conditions (e.g., variable renaming). Metamorphic testing has become a widely used approach to evaluate models’ robustness by applying semantic-preserving transformations to input programs and analyzing the stability of model outputs. While prior research has explored testing deep learning models, this systematic literature review focuses specifically on metamorphic testing for deep code models. By studying 45 primary papers, we analyze the transformations, techniques, and evaluation methods used to assess robustness. Our review summarizes the current landscape, identifying frequently evaluated models, programming tasks, datasets, target languages, and evaluation metrics, and highlights key challenges and future directions for advancing the field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414155484",
    "type": "article"
  },
  {
    "title": "Interpreting Deep Neural Networks via Relative Activation-Deactivation Abstractions",
    "doi": "https://doi.org/10.1145/3748503",
    "publication_date": "2025-09-15",
    "publication_year": 2025,
    "authors": "Zhen Zhang; Peng Wu; Yuting Yang; Xuran Li",
    "corresponding_authors": "",
    "abstract": "As deep learning models are widely applied in various real-world intelligent systems, the interpretability and trustworthiness of these models have attracted substantial attention. A succinct and effective abstraction that can represent the inference behavior of a deep neural network is significant for explaining its decision logic and ensuring its reliability. We propose in this paper the relative activation and deactivation patterns to redefine the behaviors of deep learning neurons, and the notion of relative selectivity to quantify the output differences of neurons among different prediction categories. Then, we present a relative activation-deactivation abstraction approach to characterize the decision logic of a deep learning model. The relative activation-deactivation abstractions enjoy close intra-class aggregation for each prediction category, as well as diverse inter-class separation between categories. This abstraction approach can be well extended from CNNs to Transformers, demonstrating its excellent scalability. We further propose an anomaly detection algorithm based on the relative activationde-activation abstraction approach, following the principle that the relative activation-deactivation abstraction of a deep learning model under an abnormal input is far away from the one for the predicted category the deep learning model outputs. We evaluate the anomaly detection algorithm with 13 typical benchmark datasets in the computer vision and natural language processing domains. The experimental results on two widely concerned anomaly detection tasks, i.e., out-of-distribution and adversarial detection, show that our algorithm can achieve higher and more stable detection performance than the state-of-the-art algorithms, with significantly more true positives and fewer false positives for anomaly detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414203524",
    "type": "article"
  },
  {
    "title": "Parallel Fuzzing based on Sub-tasks Scheduling",
    "doi": "https://doi.org/10.1145/3766542",
    "publication_date": "2025-09-15",
    "publication_year": 2025,
    "authors": "Taotao Gu; Tong Wang; Xiang Li; Shuaibing Lu; Yuanping Nie; Zhaowei Zhang; Xiaohui Kuang; Gang Zhao",
    "corresponding_authors": "",
    "abstract": "Parallel fuzzing introduces two key steps: task division and task merging to improve efficiency and effectiveness. However, existing works lack effective analysis of task results during task merging; they employ simple differential seed distribution strategies during task division, which cannot prevent task conflicts. Moreover, there is a lack of coordination between task division and task merging phases, and the task division process does not fully utilize feedback information from task merging, reducing the collaboration efficiency between parallel sub-nodes. In this paper, we first introduce a dual-loop multi-phase parallel framework. Under this framework, we combine the “exploration” capability of Structure-Unaware (SU) task division strategy with the “exploitation” capability of Structure-Aware (SA) task division strategy to form a new hybrid task division strategy, aiming to improve the efficiency and effectiveness of parallel fuzzing. Meanwhile, in the task merging phase, we design a comprehensive information collection mechanism that can effectively analyze task results, and in the task division phase, we can further iterate the hybrid task division strategy based on real-time testing feedback. We implement PFuzzer according to the proposed scheme. In our evaluation experiments, PFuzzer successfully discovered vulnerabilities in carefully fuzzed programs that baseline fuzzers failed to detect. Furthermore, on the Fuzzbench test suite, PFuzzer discovered more code branches in most programs. Most importantly, the PFuzzer framework demonstrates good scalability, effectively parallelizing different base fuzzing engines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414203993",
    "type": "article"
  },
  {
    "title": "Requirements-Driven Automated Software Testing: A Systematic Review",
    "doi": "https://doi.org/10.1145/3767739",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Fanyu Wang; Chetan Arora; Chakkrit Tantithamthavorn; Kao Li Huang; Aldeida Aleti",
    "corresponding_authors": "",
    "abstract": "Automated software testing has significant potential to enhance efficiency and reliability within software development processes. However, its broader adoption faces considerable challenges, particularly concerning alignment between test generation methodologies and software requirements. RE quirements- D riven A utomated S oftware T esting (REDAST) addresses this gap by systematically leveraging requirements as the foundation for automated test artifact generation. This systematic literature review (SLR) critically examines the REDAST landscape, analyzing the current state of requirements input formats, transformation techniques, generated test artifacts, evaluation methods, and prevailing limitations. We conducted a thorough analysis of 156 relevant studies selected through a rigorous multi-stage filtering process from an initial collection of 27,333 papers sourced from six major research databases. Our findings highlight the predominance of functional requirements, model-based specifications, and natural language formats. Rule-based techniques are extensively utilized, while machine learning-based approaches remain relatively underexplored. Furthermore, most existing frameworks are sequential and dependent on singular intermediate representations, and while test cases, structured textual formats, and requirements coverage are common, full automation remains rare. We identify significant gaps related to automation completeness, and dependency on input quality. This comprehensive synthesis provides a detailed overview of REDAST research and limitations, offering clear, evidence-based recommendations to guide future advancements in automated software testing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414241772",
    "type": "review"
  },
  {
    "title": "How do Machine Learning Models Change?",
    "doi": "https://doi.org/10.1145/3767157",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Joel Castaño; Rafael Cabañas; Antonio Salmerón; David Lo; Silverio Martínez‐Fernández",
    "corresponding_authors": "",
    "abstract": "The proliferation of Machine Learning (ML) models and their open-source implementations has transformed Artificial Intelligence research and applications. Platforms like Hugging Face (HF) enable this evolving ecosystem, yet a large-scale longitudinal study of how these models change is lacking. This study addresses this gap by analyzing over 680,000 commits from 100,000 models and 2,251 releases from 202 of these models on HF using repository mining and longitudinal methods. We apply an extended ML change taxonomy to classify commits and use Bayesian networks to model temporal patterns in commit and release activities. Our findings show that commit activities align with established data science methodologies, such as the Cross-Industry Standard Process for Data Mining (CRISP-DM), emphasizing iterative refinement. Release patterns tend to consolidate significant updates, particularly in model outputs, sharing, and documentation, distinguishing them from granular commits. Furthermore, projects with higher popularity exhibit distinct evolutionary paths, often starting from a more mature baseline with fewer foundational commits in their public history. In contrast, those with intensive collaboration show unique documentation and technical evolution patterns. These insights enhance the understanding of model changes on community platforms and provide valuable guidance for best practices in model maintenance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414241955",
    "type": "article"
  },
  {
    "title": "More Than Meets the Eye: On Evaluating SBOM Tools In Java",
    "doi": "https://doi.org/10.1145/3766073",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Menghan Wu; Yuliang Zhao; Xing Hu; Xian Zhan; Shanping Li; Xin Xia",
    "corresponding_authors": "",
    "abstract": "Open-source software is widely used in current software development. Unfortunately, this integration introduces a spectrum of challenges and potential threats. Such challenges emerge due to the diversity of import scenarios, which in turn may introduce malicious or vulnerable code in the client software, thereby causing significant security risks. To improve the transparency of software supply chains, Software Bill of Materials (SBOM) tools are proposed to identify the components within software systems. However, there limit investigation of functionality (i.e., tool operational process and data fields) and their practical performance of SBOM tools across various import scenarios. In this paper, we perform a comprehensive empirical study to investigate the impact of different import scenarios on SBOM tools. Specifically, we focus on three distinct component import scenarios: Build Tool Import, Dynamic Loading, and Source Code Import across a new benchmark consisting of 152 projects. We find that (1) The detection capabilities of SBOM tools exhibit considerable variance, especially in identifying dependency relationships; (2) The effectiveness of SBOM tools within the import scenarios of Dynamic Loading and Source Code Import falls short of expectations. Based on our findings, we summarize the lessons learned from different perspectives, including practitioners, tool vendors, and researchers. Our study provides valuable insights into the intricate landscape of software component usage, contributing to enhancing SBOM tools in modern software development.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414246221",
    "type": "article"
  },
  {
    "title": "The Role of Empathy in Software Engineering - A Socio-Technical Grounded Theory",
    "doi": "https://doi.org/10.1145/3768315",
    "publication_date": "2025-09-17",
    "publication_year": 2025,
    "authors": "Hashini Gunatilake; John Grundy; Rashina Hoda; Ingo Mueller",
    "corresponding_authors": "",
    "abstract": "Empathy, defined as the ability to understand and share others’ perspectives and emotions, is essential in software engineering (SE), where developers often collaborate with diverse stakeholders. It is also considered as a vital competency in many professional fields such as medicine, healthcare, nursing, animal science, education, marketing, and project management. Despite its importance, empathy remains under-researched in SE. To further explore this, we conducted a socio-technical grounded theory (STGT) study through in-depth semi-structured interviews with 22 software developers and stakeholders. Our study explored the role of empathy in SE and how SE activities and processes can be improved by considering empathy. Through applying the systematic steps of STGT data analysis and theory development, we developed a theory that explains the role of empathy in SE. Our theory details the contexts in which empathy arises, the conditions that shape it, the causes and consequences of its presence and absence. We also identified contingencies for enhancing empathy or overcoming barriers to its expression. Our findings provide practical implications for SE practitioners and researchers, offering a deeper understanding of how to effectively integrate empathy into SE processes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414277184",
    "type": "article"
  },
  {
    "title": "Laws of Quantum Programming",
    "doi": "https://doi.org/10.1145/3765903",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Mingsheng Ying; Li Zhou; Gilles Barthe",
    "corresponding_authors": "",
    "abstract": "In this paper, we investigate the fundamental laws of quantum programming. We extend a comprehensive set of Hoare et al.'s basic laws of classical programming to the quantum setting. These laws characterise the algebraic properties of quantum programs, such as the distributivity of sequential composition over (quantum) \\({\\mathbf{if}}\\) -statements and the unfolding of nested (quantum) \\({\\mathbf{if}}\\) -statements. At the same time, we clarify some subtle differences between certain laws of classical programming and their quantum counterparts. Additionally, we derive a fixpoint characterisation of quantum \\({\\mathbf{while}}\\) -loops and a loop-based realisation of tail recursion in quantum programming. Furthermore, we establish two normal form theorems: one for quantum circuits and one for finite quantum programs. The theory in which these laws are established is formalised in the Coq proof assistant, and all of these laws are mechanically verified. As an application case of our laws, we present a formal derivation of the principle of deferred measurements in dynamic quantum circuits. We expect that these laws can be utilised in correctness-preserving transformation, compilation, and automatic code optimisation in quantum programming. In particular, because these laws are formally verified in Coq, they can be confidently applied in quantum program development.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414321047",
    "type": "article"
  },
  {
    "title": "PVDetector: Pretrained Vulnerability Detection on Vulnerability-enriched Code Semantic Graph",
    "doi": "https://doi.org/10.1145/3768582",
    "publication_date": "2025-09-19",
    "publication_year": 2025,
    "authors": "Jiachong Li; Lei Cui; Jie Zhang; Lun Li; Rongrong Xi; Hongsong Zhu",
    "corresponding_authors": "",
    "abstract": "Automated vulnerability detection is a critical issue in software security. The advent of deep learning (DL) has led to numerous studies employing DL to detect vulnerabilities in software source code. However, existing approaches still perform poorly, particularly with real-world vulnerabilities, due to the difficulty in accurately capturing their properties. To this end, we introduce PVDetector, a DL-based approach that utilizes rich code semantics, incorporates vulnerability knowledge, and leverages pretrained code representations for precise vulnerability detection. At its core, PVDetector employs a new model called Vulnerability-enriched Code Semantic Graph (VCSG), which accurately characterizes functions by distinguishing the semantics of identical variables and more finely capturing control dependencies, data dependencies, and vulnerability relationships. Additionally, we introduce four pretraining tasks specifically designed to learn the semantics of control, data, vulnerability, and variables from the VCSG model. These pretraining tasks significantly enhance PVDetector's capability to detect vulnerabilities in downstream tasks. Experimental results indicate that PVDetector outperforms SOTAs by 5.0%-12.5% in precision, 0.2%-9.7% in recall, and 3.0%-15.1% in F1-score. Additionally, it supports six programming languages and demonstrates high efficiency (e.g., 10.6 \\(\\times\\) faster than DeepDFA). When applied to seven software products, PVDetector discovered 55 vulnerabilities, including 10 silently patched flaws that had not been previously reported.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414363420",
    "type": "article"
  },
  {
    "title": "YourCoLo: Leveraging One-to-Many Relationships and Inter-Code Connections for User Review-Based Code Localization",
    "doi": "https://doi.org/10.1145/3769010",
    "publication_date": "2025-09-22",
    "publication_year": 2025,
    "authors": "Kuo Chi; Changan Niu; Zhou Yang; Chuanyi Li; Yi Feng; Jidong Ge; Bin Luo; David Lo; Vincent Ng",
    "corresponding_authors": "",
    "abstract": "In an era where mobile devices are ubiquitous, digital distribution platforms such as the Google Play Store have become integral to our daily lives, hosting millions of applications and serving billions of users. Users can leave reviews to provide developers with valuable feedback, including requests for new features and reports of issues. These user reviews play a crucial role in software development, testing, and maintenance by informing developers about user needs and potential problems, which motivates us to revisit a key problem: given user reviews, how can we automatically identify the relevant code snippets from software codebases to assist developers in addressing the reviews? Existing practices to address this problem typically involve calculating the similarity between user reviews and code snippets. However, we identify three key limitations. First, although existing methods show promising results on individual projects, their high performance cannot be generalized across projects. Second, the state-of-the-art approach models the problem as a one-to-one relationship between a user review and code snippets, ignoring the one-to-many relationship that often exists. Third, the state-of-the-art approach focuses solely on the direct relationship between reviews and code snippets, overlooking the interconnections among code snippets themselves, which contain valuable information that can aid in accurately identifying relevant code. To address these limitations and advance the state of the art, we propose YourCoLo , a novel approach that fully leverages contextual information, one-to-many relationships, and inter-code connections. Specifically, YourCoLo is powered by three novel designs: (1) a prompt-enhanced mechanism to incorporate rich project-level context into code localization, (2) a new loss function designed to handle the one-to-many relationships between user reviews and multiple relevant code snippets, and (3) a ranking strategy that considers interconnections among related code snippets. Our experimental evaluation shows that YourCoLo substantially outperforms state-of-the-art models, surpassing CodeBERT, CodeLlama, and GraphCodeBERT by 18.3, 9.3, and 7.7 percentage points at the method level and by 18.4, 7.7, and 7.0 percentage points at the file level (in terms of mean reciprocal rank). In addition, YourCoLo also achieves improvements of 8.8 percentage points and 6.8 percentage points in mean average precision (MAP) at the method and file levels, respectively, compared to the state-of-the-art method. These results underscore YourCoLo ’s effectiveness and its potential to guide developers more accurately toward the code snippets most pertinent to user feedback.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414410403",
    "type": "article"
  },
  {
    "title": "A Taxonomy of System-Level Attacks on Deep Learning Models in Autonomous Vehicles",
    "doi": "https://doi.org/10.1145/3769009",
    "publication_date": "2025-09-22",
    "publication_year": 2025,
    "authors": "Masoud Jamshidiyan Tehrani; Jin-Han Kim; Rosmaël Zidane Lekeufack Foulefack; Alessandro Marchetto; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "The advent of deep learning and its astonishing performance has enabled its usage in complex systems, including autonomous vehicles. On the other hand, deep learning models are susceptible to mis-predictions when small, adversarial changes are introduced into their input. Such mis-predictions can be triggered in the real world and can result in a failure of the entire system. In recent years, a growing number of research works have investigated ways to mount attacks against autonomous vehicles that exploit deep learning components. Such attacks are directed toward elements of the environment where these systems operate and their effectiveness is assessed in terms of system-level failures triggered by them. There has been however no systematic attempt to analyze and categorize such attacks. In this paper, we present the first taxonomy of system-level attacks against autonomous vehicles. We constructed our taxonomy by selecting 21 highly relevant papers, then we tagged them with 12 top-level taxonomy categories and several sub-categories. The taxonomy allowed us to investigate the attack features, the most attacked components and systems, the underlying threat models, and the failure chains from input perturbation to system-level failure. We distilled several lessons for practitioners and identified possible directions for future work for researchers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414410420",
    "type": "article"
  },
  {
    "title": "Addressing OSS Community Managers’ Challenges in Contributor Retention",
    "doi": "https://doi.org/10.1145/3769012",
    "publication_date": "2025-09-22",
    "publication_year": 2025,
    "authors": "Zixuan Feng; Katie Kimura; Bianca Trinkenreich; Igor Steinmacher; Marco Aurélio Gerosa; Anita Sarma",
    "corresponding_authors": "",
    "abstract": "Open-source software (OSS) community managers face significant challenges in retaining contributors, as they must monitor activity and engagement while navigating complex dynamics of collaboration. Current tools designed for managing contributor retention (e.g., dashboards) fall short by providing retrospective rather than predictive insights to identify potential disengagement early. Without understanding how to anticipate and prevent disengagement, new solutions risk burdening community managers rather than supporting retention management. Following the Design Science Research paradigm, we employed a mixed-methods approach for problem identification and solution design to address contributor retention. To identify the challenges hindering retention management in OSS, we conducted semi-structured interviews, a multi-vocal literature review, and community surveys. Then through an iterative build-evaluate cycle, we developed and refined strategies for diagnosing retention risks and informing engagement efforts. We operationalized these strategies into a web-based prototype, incorporating feedback from 100+ OSS practitioners, and conducted an in situ evaluation across two OSS communities. Our study offers (1) empirical insights into the challenges of contributor retention management in OSS, (2) actionable strategies that support OSS community managers’ retention efforts, and (3) a practical framework for future research in developing or validating theories about OSS sustainability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414410436",
    "type": "article"
  },
  {
    "title": "Back to the Roots: Assessing Mining Techniques for Java Vulnerability-Contributing Commits",
    "doi": "https://doi.org/10.1145/3769105",
    "publication_date": "2025-09-24",
    "publication_year": 2025,
    "authors": "Torge Hinrichs; Emanuele Iannone; Tamás Aladics; Péter Hegedűs; Andrea De Lucia; Fabio Palomba; Riccardo Scandariato",
    "corresponding_authors": "",
    "abstract": "Context : Vulnerability-contributing commits (VCCs) are code changes that introduce vulnerabilities. Mining historical VCCs relies on SZZ-based algorithms that trace from known vulnerability-fixing commits. Objective : Although these techniques have been used, e.g., to train just-in-time vulnerability predictors, they lack systematic benchmarking to evaluate their precision, recall, and error sources. Method : We empirically assessed 12 VCC mining techniques in Java repositories using two benchmark datasets (one from the literature and one newly curated). We also explored combinations of techniques, through intersections, voting schemes, and machine learning, to improve performance. Results : Individual techniques achieved at most 0.60 precision but up to 0.89 recall. The precision rose to 0.75 when the outputs were combined with the logical AND, at the expense of recall. Machine learning ensembles reached 0.80 precision with a better precision–recall balance. Performance varied significantly by dataset. Analyzing “fixing commits” showed that certain fix types (e.g., filtering or sanitization) affect retrieval accuracy, and failure patterns highlighted weaknesses when fixes involve external data handling. Conclusion : Such results help software security researchers select the most suitable mining technique for their studies and understand new ways to design more accurate solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414476095",
    "type": "article"
  },
  {
    "title": "FLITSR: Improved Spectrum-Based Localization of Multiple Faults by Iterative Test Suite Reduction – RCR Report",
    "doi": "https://doi.org/10.1145/3768631",
    "publication_date": "2025-09-24",
    "publication_year": 2025,
    "authors": "Dylan Callaghan; Bernd Fischer",
    "corresponding_authors": "",
    "abstract": "This report details the contents of the artifact for the paper “FLITSR: Improved Spectrum-Based Localization of Multiple Faults by Iterative Test Suite Reduction”, as well as detailed instructions for its installation and usage. The artifact contains all the necessary components to replicate the results of the study, including the three datasets used as well as the FLITSR tool which is used as both the implementation of the FLITSR algorithm and the evaluation framework for the experiments in the study.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414476160",
    "type": "article"
  },
  {
    "title": "Large Language Models for Cyber Security: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3769676",
    "publication_date": "2025-09-29",
    "publication_year": 2025,
    "authors": "Haiyang Xu; Shenao Wang; Ningke Li; Kailong Wang; Yanjie Zhao; Kai Chen; Ting Yu; Yang Liu; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in a variety of application domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity (LLM4Security). By comprehensively collecting over 40K relevant papers and systematically analyzing 185 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain. Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to an expanding range of cybersecurity tasks, including vulnerability detection, malware analysis, and network intrusion detection. Second, we analyze application trends of different LLM architectures (such as encoder-only, encoder-decoder, and decoder-only) across security domains. Third, we identify increasingly sophisticated techniques for adapting LLMs to cybersecurity, such as advanced fine-tuning, prompt engineering, and external augmentation strategies. A significant emerging trend is the use of LLM-based autonomous agents, which represent a paradigm shift from single-task execution to orchestrating complex, multi-step security workflows. Furthermore, we find that the datasets used for training and evaluating LLMs are often limited, highlighting the need for more comprehensive datasets and the use of LLMs for data augmentation. Finally, we discuss the main challenges and opportunities for future research, including the need for more interpretable models, addressing the inherent security risks of LLMs, and their potential for proactive defense. Overall, our survey provides a comprehensive overview of the current state-of-the-art in LLM4Security and identifies several promising directions for future research. We believe that the insights and findings presented in this survey will contribute to the growing body of knowledge on the application of LLMs in cybersecurity and provide valuable guidance for researchers and practitioners working in this field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414594130",
    "type": "article"
  },
  {
    "title": "Quantum Neural Network Classifier for Cancer Registry System Testing: A Feasibility Study",
    "doi": "https://doi.org/10.1145/3769302",
    "publication_date": "2025-09-29",
    "publication_year": 2025,
    "authors": "Xinyi Wang; Shaukat Ali; Paolo Arcaini; Narasimha Raghavan Veeraragavan; Jan F. Nygård",
    "corresponding_authors": "",
    "abstract": "With the rapid advancement of quantum computing, research on quantum machine learning (QML) algorithms has grown significantly. Among these, the Quantum Neural Network (QNN) stands out as one of the promising algorithms that integrates the principles of quantum computing with artificial neural networks to process data. Inspired by applications of QNN across fields, we investigate their use in software testing for the Cancer Registry of Norway (CRN), part of the Norwegian Institute of Public Health (NIPH), responsible for cancer statistics among the Norwegian population. CRN develops a complex socio-technical software system, Cancer Registration Support System ( \\(\\mathtt{CaReSS}\\) ), interacting with many entities (e.g., hospitals, medical laboratories, and other patient registries) to achieve its task. For cost-effective testing of \\(\\mathtt{CaReSS}\\) , CRN has employed \\(\\mathtt{EvoMaster}\\) , an AI-based REST API testing tool combined with an integrated classical machine learning model \\(\\mathtt{EvoClass}\\) . Within this context, we propose \\(\\mathtt{EvoQlass}\\) to investigate the feasibility of using, inside \\(\\mathtt{EvoMaster}\\) , a QNN classifier, instead of the existing classical machine learning model. Results indicate that \\(\\mathtt{EvoQlass}\\) can achieve performance comparable to that of \\(\\mathtt{EvoClass}\\) . We further explore the effects of various QNN configurations on performance and offer recommendations for optimal QNN settings for future QNN developers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414594133",
    "type": "article"
  },
  {
    "title": "Weakly Supervised Vulnerability Localization via Multiple Instance Learning",
    "doi": "https://doi.org/10.1145/3768572",
    "publication_date": "2025-09-29",
    "publication_year": 2025,
    "authors": "Wenchao Gu; Yupan Chen; Yanlin Wang; Hongyu Zhang; Cuiyun Gao; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Software vulnerability detection has emerged as a significant concern in the field of software security recently, capturing the attention of numerous researchers and developers. Most previous approaches focus on coarse-grained vulnerability detection, such as at the function or file level. However, the developers would still encounter the challenge of manually inspecting a large volume of code inside the vulnerable function to identify the specific vulnerable statements for modification, indicating the importance of vulnerability localization. Training the model for vulnerability localization usually requires ground-truth labels at the statement-level, and labeling vulnerable statements demands expert knowledge, which incurs high costs. Hence, the demand for an approach that eliminates the need for additional labeling at the statement-level is on the rise. To tackle this problem, we propose a novel approach called WAVES for W e A kly supervised V ulnerability Localization via multipl E in S tance learning, which does not need the additional statement-level labels during the training. WAVES has the capability to determine whether a function is vulnerable (i.e., vulnerability detection) and pinpoint the vulnerable statements (i.e., vulnerability localization). Specifically, inspired by the concept of multiple instance learning, WAVES converts the ground-truth label at the function-level into pseudo labels for individual statements, eliminating the need for additional statement-level labeling. These pseudo labels are utilized to train the classifiers for the function-level representation vectors. Extensive experimentation on three popular benchmark datasets demonstrates that, in comparison to previous baselines, our approach achieves comparable performance in vulnerability detection and state-of-the-art performance in statement-level vulnerability localization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414607177",
    "type": "article"
  },
  {
    "title": "Posing New Challenges to Function Entry Identification Through Fine-Grained CFI Obfuscation",
    "doi": "https://doi.org/10.1145/3770582",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "T. Zhang; Chengbin Pang; Fei Peng; Linzhang Wang; Bing Mao",
    "corresponding_authors": "",
    "abstract": "Function entry identification is a crucial yet challenging task for binary disassemblers that has been the focus of research in the past decades. However, recent researches show that call frame information (CFI) provides accurate and almost complete function entries. With the aid of CFI, disassemblers have significant improvements in function entry detection. CFI is specifically designed for efficient stack unwinding, and every function has corresponding CFI in x64 and aarch64 architectures. Nevertheless, not every function and instruction unwinds the stack at runtime, and this observation has led to the development of techniques such as obfuscation to complicate function detection by disassemblers. We propose a prototype of ocfi to obfuscate CFI based on this observation. The goal of ocfi is to obstruct function detection of popular disassemblers that use CFI as a way to detect function entries. We evaluated ocfi on a large-scale dataset that includes real-world applications and automated generation programs, and found that the obfuscated CFI was able to correctly unwind the stack and made the detection of function entries of popular disassemblers more difficult. Furthermore, on average, ocfi incurs a size overhead of only 5% and nearly zero runtime overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414799460",
    "type": "article"
  },
  {
    "title": "Enhancing Domain-Specific Code Completion via Collaborative Inference with Large and Small Language Models",
    "doi": "https://doi.org/10.1145/3770748",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Jia Yu; Zhipeng Gao; Lingfeng Bao; Zhongxin Liu",
    "corresponding_authors": "",
    "abstract": "Large language model-based code completion has demonstrated excellent performance, but still encounters challenges in capturing domain-specific knowledge for more precise completion within specific domains, i.e., domain-specific code completion. Prior work has studied fine-tuning techniques or retrieval-augmented techniques for this task. Nevertheless, it requires a lot of computational resources to fine-tune large language models (LLMs), and the cost can increase quadratically with the model size. Retrieval-augmented techniques face difficulties in accurately and adaptively retrieving relevant information. Moreover, considering that code completion tools work in real time, how to utilize large language models more efficiently poses challenges. To tackle these challenges, in this paper, we first conduct preliminary experiments and observe that the code completion results of a small model fine-tuned within a specific domain complement those of a large model. Building on this insight, we propose a collaborative framework to effectively combine large and small models for better domain-specific code completion. Specifically, we fine-tune a small code model instead of a large model with the PEFT method, reducing the overhead of fine-tuning. We utilize a well-designed classifier to facilitate the adaptive combination of distinct completion results. The classifier relies on features in various dimensions, such as the similarity between the completed code and the context, and is used to adaptively determine how to combine the tokens predicted by the large and small models for better code completion. Evaluation results show that our approach achieves an average improvement in the exact match of 7.42% and 4.67% over the state-of-the-art baselines in the intra-project and intra-domain code completion scenarios, respectively. Furthermore, compared to the state-of-the-art domain-specific code completion approach FT2Ra, the inference speed of our approach is 1.40 times faster, and the average space requirement drops from 25.98G to 13.69G. These advantages make our approach much more accessible and efficient.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414799837",
    "type": "article"
  },
  {
    "title": "Rethinking Code Similarity: A Logic-Based Framework for Cross-Language Analysis beyond Functional Equivalence",
    "doi": "https://doi.org/10.1145/3769305",
    "publication_date": "2025-10-08",
    "publication_year": 2025,
    "authors": "Jican Zhang; Jiujun Zhang; Xue‐Ning Shen; Lei Xue; Liming Nie; Fengwei Lin; Kefeng Wu; Tao Zhang; Pingxin Du",
    "corresponding_authors": "",
    "abstract": "Cross-language code plagiarism and vulnerability propagation pose significant threats to software integrity, necessitating advanced methods for code similarity analysis. Traditional approaches primarily emphasize functional equivalence, often overlooking deeper structural and algorithmic similarities. Consequently, these methods frequently misclassify code fragments as similar solely based on functional alignment, despite substantial differences in their underlying logic or algorithmic implementation. To address this critical limitation, we propose SimCL, a novel logic-driven cross-language code similarity analysis framework. The core innovation of SimCL lies in employing a unified Control Flow Graph (uCFG) representation, derived from a newly introduced language-agnostic Unified Intermediate Representation (uIR). This unified approach facilitates the precise identification of logical similarities across diverse programming languages, transcending syntactic and superficial functional similarities. Furthermore, we introduce SimGK, a tailored similarity calculation algorithm leveraging graph kernel techniques specifically optimized for accurately comparing code-related uCFGs. To rigorously evaluate SimCL, we constructed two distinctive datasets: the TranDataset, comprising logically and functionally similar cross-language code pairs, and the FuncDataset, consisting of logically similar yet functionally distinct pairs. These datasets address gaps in existing resources by explicitly differentiating between logical and functional dimensions. Comprehensive experimental results indicate that SimCL significantly outperforms existing state-of-the-art approaches in cross-language code similarity detection, demonstrating superior precision, recall, and F1 scores. Thus, SimCL effectively addresses current limitations, substantially enhancing accuracy and reliability in detecting code clones and vulnerability propagation across programming languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414961336",
    "type": "article"
  },
  {
    "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
    "doi": "https://doi.org/10.1145/3771283",
    "publication_date": "2025-10-09",
    "publication_year": 2025,
    "authors": "Fatemeh Hadadi; Qinghua Xu; Domenico Bianculli; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Most log-based anomaly detectors assume logs are stable, though in reality they are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog , a novel hybrid approach for ULAD that combines ML models — decision tree, k-nearest neighbors, and a feedforward neural network — with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog , we configured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points (pp) in F1 score while using much less labeled data (62.87 pp reduction). When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 pp increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications, except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog ’s key components: cache, RAG, and ensemble learning.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414993542",
    "type": "article"
  },
  {
    "title": "Automatic Rule Checking for Microservices: Supporting Security Analysis with Explainability",
    "doi": "https://doi.org/10.1145/3771275",
    "publication_date": "2025-10-09",
    "publication_year": 2025,
    "authors": "Simon Schneider; Pierre-Jean Quéval; Ákos Milánkovich; Nicolás E. Díaz Ferreyra; Uwe Zdun; Riccardo Scandariato",
    "corresponding_authors": "",
    "abstract": "Software security analysis is often done manually, raising performance and correctness issues. Introducing automation is challenging because human verification of the outcomes is often required, especially for security assessment and certification. The distributed nature of microservice applications further increases these concerns. We present an approach for automatically checking architectural security rules on models of microservice applications. It provides explainability for verdicts of rules that are expressed as model queries in our rule specification language. This comprehensible, step-by-step evidence leverages traceability information from the input models to link to artifacts in code. Hence, the complete analysis process from source code via model to rule verdict can be traced and verified. Custom rules can be formulated in addition to a library of 25 best-practice architectural security rules. We evaluated the approach’s correctness by checking the 25 rules on 16 dataflow diagrams of microservice applications with a prototype (called MicroCertiSec ) and observed promising results (precision=0.98; recall=1). Additionally, we performed an evaluation with industry experts and academics to gain initial insights into the approach’s usefulness for real-world security analysis. The nine participants gave highly positive feedback on usefulness and usability and stated they would use such an approach in their daily work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414993546",
    "type": "article"
  },
  {
    "title": "Targeted Deep Learning System Boundary Testing",
    "doi": "https://doi.org/10.1145/3771557",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Oliver Weißl; Ahmed Salama Abdel Latif; Xingcheng Chen; Giorgi Merabishvili; Vincenzo Riccio; Severin Kacianka; Andrea Stocco",
    "corresponding_authors": "",
    "abstract": "Evaluating the behavioral boundaries of deep learning (DL) systems is crucial for understanding their reliability across diverse, unseen inputs. Existing solutions fall short as they rely on untargeted, random perturbations with limited controlled input variations. In this work, we introduce Mimicry , a novel black-box test generator for fine-grained, targeted exploration of DL system boundaries. Mimicry performs boundary testing by leveraging the probabilistic nature of DL outputs to identify promising directions for exploration. By using style-based GANs to disentangle inputs into content and style components, Mimicry generates boundary test inputs by mimicking features from both source and target classes. We evaluated Mimicry ’s effectiveness in generating boundary inputs for five DL image classification systems, comparing it to two baselines from the literature. Our results show that Mimicry consistently identifies inputs up to \\(25\\times\\) closer to the theoretical decision boundary, outperforming the baselines with statistical significance. Moreover, it generates semantically meaningful boundary test cases that reveal new functional misbehaviors, while the baselines mostly produce corrupted or invalid inputs. Thanks to its enhanced control over latent space manipulations, Mimicry remains effective as dataset complexity grows, resulting in a up to \\(36\\%\\) higher validity rate and competitive diversity, as supported by a comprehensive human assessment.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415047076",
    "type": "article"
  },
  {
    "title": "VUI Testing of VPA Apps via Behavior Model-Enhanced LLM Agents",
    "doi": "https://doi.org/10.1145/3771555",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Suwan Li; Lei Bu; Shangqing Liu; Guangdong Bai; Fuman Xie; Kai Chen; Chang Yue",
    "corresponding_authors": "",
    "abstract": "With the increasing adoption of smart speakers, Virtual Personal Assistant (VPA) applications have become integral to daily life, enabling users to access news, entertainment, and smart device control through Voice User Interfaces (VUI). However, many VPA apps suffer from quality issues, such as unexpected terminations and failures to process common user commands, highlighting the urgent need for systematic and efficient VUI testing. Existing chatbot-style and model-based testing approaches lack global and semantic awareness, resulting in ineffective test case generation and inefficient state exploration. To address these challenges, we introduce Elevate, a model-enhanced, LLM-driven VPA testing framework that employs a multi-agent architecture to enhance VUI behavior testing. Elevate comprises three specialized LLM agents—Observer, Generator, and Planner—that collaboratively perform state extraction, test case generation, and guided state exploration. Additionally, a deterministic finite automaton (DFA)-based behavior model is designed to abstract app behavior and provide structured guidance to LLM agents, enhancing testing performance. Elevate also incorporates a feedback mechanism that refines testing strategies based on observed behaviors, ensuring continuous improvement. Implemented using GPT-4-Turbo and DeepSeek-R1, Elevate has been evaluated on problem detection, sentence/semantic coverage, and large-scale testing. Experimental results show that Elevate outperforms state-of-the-art methods (Vitas and LLM-based chatbots), detecting at least 18 and 37 more problems, respectively, and achieving over 10% and 30% higher state coverage. In a large-scale evaluation on 4,000 Alexa skills, Elevate further demonstrated 15% higher coverage than Vitas, confirming its effectiveness, scalability, and potential for widespread application in VUI testing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415047165",
    "type": "article"
  },
  {
    "title": "Requirements-Based Test Generation: A Comprehensive Survey",
    "doi": "https://doi.org/10.1145/3771727",
    "publication_date": "2025-10-13",
    "publication_year": 2025,
    "authors": "Yang Zhenzhen; Rubing Huang; Chenhui Cui; Nan Niu; Dave Towey",
    "corresponding_authors": "",
    "abstract": "As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirements types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415106599",
    "type": "article"
  },
  {
    "title": "Foster the use of Hackathons in Collaborative Research Projects: Methodology, Experience Report and Lesson Learned",
    "doi": "https://doi.org/10.1145/3771552",
    "publication_date": "2025-10-13",
    "publication_year": 2025,
    "authors": "Claudio Di Sipio; Romina Eramo; Vittoriano Muttillo; Riccardo Rubei",
    "corresponding_authors": "",
    "abstract": "In recent years, the growing complexity of ICT projects and the increasing need for effective multi-stakeholder collaboration have prompted the search for innovative methods to enhance project outcomes. This paper proposes a structured methodology for integrating hackathons into the life-cycle of ICT collaborative multi-partner research projects. Drawing on our experience with the ECSEL Joint Undertaking AIDOaRt project, we demonstrate how a series of internally organized hackathons can serve as effective tools for fostering stakeholder engagement, accelerating innovation, and aligning project activities with strategic objectives. Our approach involves a clearly defined process comprising a pre-hackathon phase for challenge definition, intensive hackathon sessions for collaborative solution development, and a post-hackathon phase for iterative refinement. Quantitative analysis of challenge evolution and participant engagement, together with qualitative feedback from a questionnaire survey, reveals that iterative hackathon cycles not only enhance technical outcomes, such as rapid prototyping and tool integration, but also promote sustained collaboration among industry and academic partners. The paper concludes with risks, lessons learned, and practical guidelines to support the adoption of our hackathon-based methodology in multi-partner research projects, highlighting their potential to manage complexity, drive innovation in the ICT ecosystem, and foster continuous and effective collaboration among stakeholders.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415106624",
    "type": "article"
  },
  {
    "title": "SMiR: Minimizing Side-Effects in Repairing Specific Misclassifications for Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3771545",
    "publication_date": "2025-10-14",
    "publication_year": 2025,
    "authors": "Haoran Li; Shihai Wang; Bin Liu; Shurui Fei; Wentao Wu; Yu Liu",
    "corresponding_authors": "",
    "abstract": "Although Deep Neural Networks (DNNs) have achieved significant success across various industries, their erroneous behaviors have raised widespread concerns. In safety-critical domains where DNNs are deployed, certain types of specific misclassifications (i.e., instances of one specific class being misclassified as another) can lead to catastrophic consequences and urgently need to be corrected. However, existing repair approaches often neglect the adverse impact of general feature extraction patterns on distinguishing specific classes, resulting in limited correction of specific misclassifications. Furthermore, most approaches correct errors by directly adjusting the global decision boundary, which inevitably disturbs the well-trained boundaries and leads to significant accuracy loss. In this paper, we propose SMiR, an effective repair framework for addressing specific misclassifications with marginal accuracy loss. Rather than patching the original model, our method localizes and recovers the distorted feature dimensions to mitigate their negative impact on the distinction between specific classes. Moreover, SMiR narrows the scope of repair to the filtered suspect set, protecting other well-trained decision boundaries from disturbance. On average, our approach repairs 27% \\(\\sim\\) 36.2% more specific misclassifications than the baselines, while introducing only 7.8% \\(\\sim\\) 70.9% as many new errors. Besides, SMiR maintains state-of-the-art performance on safety-critical datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415156796",
    "type": "article"
  },
  {
    "title": "Test Oracle Generation for REST APIs - RCR Report",
    "doi": "https://doi.org/10.1145/3771281",
    "publication_date": "2025-10-14",
    "publication_year": 2025,
    "authors": "Juan C. Alonso; Michael D. Ernst; Sergio Segura; Antonio Ruiz–Cortés",
    "corresponding_authors": "",
    "abstract": "This Replicated Computational Results (RCR) Report accompanies our TOSEM paper “Test Oracle Generation for REST APIs”. In this work we introduce AGORA+, a black-box approach for automatically generating domain-specific test oracles for REST APIs by detecting invariants—output properties that should always hold. As part of this RCR, we provide a replication package (available at https://doi.org/10.5281/zenodo.12506791 ) that enables the full reproduction of our results and is designed to pave the way for future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415156813",
    "type": "article"
  },
  {
    "title": "Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners’ Perspectives - RCR Report",
    "doi": "https://doi.org/10.1145/3771771",
    "publication_date": "2025-10-15",
    "publication_year": 2025,
    "authors": "Lidiany Cerqueira; João Pedro Silva Bastos; Danilo Ferreira Neves; Glauco de Figueiredo Carneiro; Rodrigo Spínola; Sávio Freire; José Amâncio Macedo Santos; Manoel Mendonça",
    "corresponding_authors": "",
    "abstract": "This is the Replicated Computational Results (RCR) Report for our TOSEM paper “Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners’ Perspectives” where we present qualitative content analysis conducted on 55 web articles from DEV and Medium, two communities widely used by practitioners, and a follow-up survey with empathy experts. In this paper, we introduced a conceptual framework of empathy in software engineering. As part of this RCR, we provide an empirical package with detailed guidelines to set up and replicate our study, as well as to interpret the results. We hope it can motivate and support future research on empathy and human aspects in software engineering by offering a reusable and transparent example of qualitative analysis involving grey literature and expert feedback. All related data and materials are openly available in the experimental package.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415209571",
    "type": "article"
  },
  {
    "title": "Analysis of EMF meta-model duplication in open-source repositories",
    "doi": "https://doi.org/10.1145/3771765",
    "publication_date": "2025-10-15",
    "publication_year": 2025,
    "authors": "Alfonso de la Vega; José Antonio Hernández López",
    "corresponding_authors": "",
    "abstract": "Context. Model-Driven Software Engineering (MDSE) promotes the use of high-level models to enhance and streamline the development process. These models are often defined using a domain-specific modeling language (DSL). Defining a DSL begins with identifying key domain concepts and the relationships between them. These concepts and relationships are generally represented through a meta-model. In this context, the Eclipse Modeling Framework (EMF) is widely recognized as the de facto standard for meta-modeling, and it is among the most popular choices in the open-source modeling community. EMF provides Ecore, an object-oriented-based language used to define meta-models. Problem statement . Previous studies have observed significant Ecore meta-model duplication in the open-source ecosystem. While it is well known that engineers frequently copy and paste meta-models, the motivations behind this practice and its potential drawbacks remain uncertain. Contribution . This paper explores the phenomenon of meta-model duplication within the open-source ecosystem by addressing the following research question: how and why does meta-model duplication occur? Study design. Using a popular dataset of 30k meta-models extracted from GitHub, we analyze and quantify meta-model duplication at two levels: (1) intra-repository duplication, where a developer replicates the same or similar meta-models multiple times within a single repository, and (2) inter-repository duplication, which occurs when meta-models are reused across different repositories. Additionally, we examine representative samples of repositories exhibiting both types of duplication and apply an open-coding strategy to identify and analyze the underlying reasons behind each. Results. Our findings align with previous research, confirming the widespread occurrence of meta-model duplication on GitHub. Regarding intra-repository duplication, 21.84% of the analyzed repositories exhibit this pattern. Key factors contributing to this duplication include testing and benchmarking of modeling tools, duplication inherent in the underlying modeling technology, tutorials and assignments, coexistence of multiple meta-model versions, and various questionable practices in intra-project dependency and Git version management. Inter-repository duplication is even more prevalent, with 49.04% of repositories containing meta-models sourced from other repositories. The primary reasons for this type of duplication include repository duplication, meta-model reuse within the main tooling, testing and benchmarking, tutorials and assignments, and zoo/dataset repositories. Finally, we observe that when engineers reuse a meta-model, they make only a few modifications, which are typically simple—primarily involving the addition or removal of instances of the core building blocks of the Ecore meta-modeling language ( EAttributes, EReferences , and EClasses ). Based on these findings, we examine certain engineering practices that lead to meta-model duplication, potentially hindering modeling projects, and propose strategies to mitigate these issues. Additionally, we explore cases where duplication does not have negative consequences, as well as instances where it arises due to the limitations of current modeling tools.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415209640",
    "type": "article"
  },
  {
    "title": "CNCFuzzer: Directed Blackbox Fuzzing of Computer Numerical Control System Based on Message Behaviour Guidance",
    "doi": "https://doi.org/10.1145/3771764",
    "publication_date": "2025-10-15",
    "publication_year": 2025,
    "authors": "Zedong Li; Dongliang Fang; Yuqi Chen; Shijie Li; Jiaqian Peng; Zhanwei Song; Shichao Lv; Limin Sun",
    "corresponding_authors": "",
    "abstract": "In the era of the Industrial Internet of Things, Computer Numerical Control (CNC) Systems are confronted with a pervasive threat from attackers. Uncovering their security vulnerabilities before being exploited becomes imperative. Enterprise-level CNC devices often pose significant challenges in obtaining firmware, limiting vulnerability analysis to black-box fuzzing. However, the communication protocols used by CNC devices are characterized by heterogeneity, complexity, and proprietary formats. This inherent complexity not only makes it difficult to build high-quality test cases, but also significantly exceeds the search space of traditional black-box fuzzing methods, thus reducing their efficiency and effectiveness. In this paper, we propose CNCFuzzer, a directed black-box fuzzing approach based on message behaviour guidance. The insights of CNCFuzzer are that if a communication Application Programming Interface (API) is high-risk, the probability that other communication APIs with similar behaviour are high-risk increases. Firstly, we design a black-box directed fuzzing method based on the similarity of the behaviour of machining messages. The key to this solution is that we design graphs of message behaviour as the maps for directed fuzzing. Then, a Transformer-based seq2seq model is employed to train a proprietary protocol traffic generator, which generates high-quality test cases that meet protocol formats. Finally, We design a cyber-physical fusion monitor, which observes the current and voltage of the controllers to monitor the vulnerability of CNC devices and proactive network requests to distinguish the functional command injection and memory corruption. We have implemented CNCFuzzer and evaluated it on 6 enterprise-level CNC devices from 3 brands. Results show that CNCFuzzer outperforms 5 existing black-box fuzzing tools and identified 63 previously unknown vulnerabilities, including 30 memory corruption and 33 functional command injection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415210534",
    "type": "article"
  },
  {
    "title": "Determining Code Proficiency Levels from Python Textbooks",
    "doi": "https://doi.org/10.1145/3769864",
    "publication_date": "2025-10-15",
    "publication_year": 2025,
    "authors": "Ruksit Rojpaisarnkit; Gregório Robles; Jesús M. González-Barahona; Kenichi Matsumoto; Raula Gaikovina Kula",
    "corresponding_authors": "",
    "abstract": "The ability to measure developer proficiency is crucial, as it reflects an individual's capability to understand and interpret efficient, effective, and well-structured code. It is an essential aspect of ensuring software quality. Although prior work has proposed different approaches for measuring proficiency through code, the process of learning various coding concepts remains non-trivial and highly debated. This paper proposes a framework for determining code proficiency through learning aids that serve as ground-truth—namely, textbooks—and two automated methods: Übersequence and Clustering. We conducted an empirical study to identify Python code proficiency levels. We then assessed the effectiveness of the framework by addressing two key research questions. Using a dataset of 22 introductory Python textbooks and code constructs referenced from Python AST , we achieved a high coverage of 85.51% for Python code constructs. Our analysis reveals a significantly high similarity in the sequence of code construct introductions across the textbooks, supporting our methodology of using textbooks to assess code proficiency. The resulting Übersequence demonstrates the feasibility of assigning proficiency levels to individual code constructs, while clustering further enables a structured grouping perspective. To demonstrate practical applicability, we present examples and initiate discussions on future research directions, particularly software maintenance tasks such as bug assignment and code reviews.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415211344",
    "type": "article"
  },
  {
    "title": "M2CVD: Enhancing Vulnerability Understanding through Multi-Model Collaboration for Code Vulnerability Detection",
    "doi": "https://doi.org/10.1145/3771923",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Ziliang Wang; Ge Li; Jia Li; Jia Li; Meng Yan; Yingfei Xiong; Zhi Jin",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have strong capabilities in code comprehension, but fine-tuning costs and semantic alignment issues limit their project-specific optimization; conversely, fine-tuned models such as CodeBERT are easy to fine-tune, but it is often difficult to learn vulnerability semantics from complex code languages. To address these challenges, this paper introduces the Multi-Model Collaborative Vulnerability Detection approach (M2CVD) that leverages the strong capability of analyzing vulnerability semantics from LLMs to improve the detection accuracy of fine-tuned models. M2CVD employs a novel collaborative process: first enhancing the quality of vulnerability description produced by LLMs through the understanding of project code by fine-tuned models, and then using these improved vulnerability descriptions to boost the detection accuracy of fine-tuned models. M2CVD include three main phases: 1) Initial Vulnerability Detection: The initial vulnerability detection is conducted by fine-tuning a detection model (e.g., CodeBERT) and interacting with an LLM (e.g., ChatGPT) respectively. The vulnerability description will be generated by the LLM when the code is detected vulnerable by the LLM. 2) Vulnerability Description Refinement: By informing the LLM of the vulnerability assessment results of the detection model, we refine the vulnerability description by interacting with the LLM. Such refinement can enhance LLM’s vulnerability understanding in specific projects, effectively bridging the previously mentioned alignment gap; 3) Integrated Vulnerability Detection: M2CVD integrates code fragment and the refined vulnerability descriptions inferred to form synthetic data. Then, the synthetic data is used to fine-tune a validation model, optimize the defect feature learning efficiency of the model, and improve the detection accuracy. We demonstrated M2CVD’s effectiveness on two real-world datasets, where M2CVD significantly outperformed the baseline. In addition, we demonstrate that the M2CVD collaborative method can extend to other different LLMs and fine-tuned models to improve their accuracy in vulnerability detection tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261078",
    "type": "article"
  },
  {
    "title": "Do advanced language models eliminate the need for prompt engineering in software engineering?",
    "doi": "https://doi.org/10.1145/3771933",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Guoqing Wang; Zeyu Sun; Sixiang Ye; Zhihao Gong; Yizhou Chen; Yifan Zhao; Qingyuan Liang; Dan Hao",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have significantly advanced software engineering (SE) tasks, with prompt engineering techniques enhancing their performance in code-related areas. However, the rapid development of foundational LLMs such as the non-reasoning models (GPT-4o and Claude 3.5 Sonnet) and the reasoning model o1 raises questions about the continued effectiveness of these prompt engineering techniques. This paper presents an extensive empirical study that reevaluates various prompt engineering techniques within the context of these advanced LLMs. Focusing on five representative SE tasks, i.e., code generation, code translation, program repair, code summarization, and commit message generation, we assess whether prompt engineering techniques still yield improvements with advanced models, the actual effectiveness of reasoning models compared to non-reasoning models, and whether the benefits of using these advanced models justify their increased costs. Our findings reveal that some novel prompt engineering techniques developed for earlier LLMs may provide diminished benefits or even hinder performance when applied to advanced models. In reasoning LLMs, the ability of sophisticated built-in reasoning reduces the impact of complex prompts, sometimes making simple zero-shot prompting more effective in some specific tasks. Prompt strategies that utilize execution feedback or precise task-specific guidance remain effective and are essential for improving performance on complex related-code problems. Furthermore, while reasoning models outperform non-reasoning models in tasks requiring complex reasoning, they offer minimal advantages in tasks that do not need reasoning and may incur unnecessary costs. Based on our study, we provide practical guidance for practitioners on selecting appropriate prompt engineering techniques and foundational LLMs, considering factors such as task requirements, operational costs, and environmental impact. Our work contributes to a deeper understanding of effectively harnessing advanced LLMs in SE tasks, informing future research and application development.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261377",
    "type": "article"
  },
  {
    "title": "Assessing the Latent Automated Program Repair Capabilities of Large Language Models using Round-Trip Translation",
    "doi": "https://doi.org/10.1145/3771922",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Fernando Vallecillos Ruiz; Anastasiia Grishina; Max Hort; Leon Moonen",
    "corresponding_authors": "",
    "abstract": "Research shows that errors in natural language can be corrected by translating texts to another language and back using language models. We explore to what extent this latent correction capability extends to Automated Program Repair (APR) by investigating Round-Trip Translation (RTT): translating code from one programming language into another programming or natural language and back, using Large Language Models (LLMs). We hypothesize that RTT restores patterns most commonly seen in the LLM’s training corpora through regression toward the mean , replacing infrequent bugs with more frequent, natural , bug-free code. To test this hypothesis, we employ nine LLMs and four common APR benchmarks in Java, and perform a detailed quantitative and qualitative analysis of RTT-generated patches. We find that RTT through English generates plausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java benchmark, and 97 are found to be correct in our manual assessment. Moreover, RTT uniquely generates plausible patches for 46 bugs that were missed by LLMs specifically fine-tuned for APR. While this demonstrates the viability of RTT for APR, we also observe limitations, such as a lower overall bug fix rate than the state-of-the-art and diluting the original coding style. We analyze the impact of these limitations and discuss the potential of using RTT as a complementary component in APR frameworks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261590",
    "type": "article"
  },
  {
    "title": "On Developers’ Self-Declaration of AI-Generated Code: An Analysis of Practices",
    "doi": "https://doi.org/10.1145/3771937",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Syed Mohammad Kashif; Peng Liang; Amjed Tahir",
    "corresponding_authors": "",
    "abstract": "AI code generation tools have gained significant popularity among developers, who use them to assist in software development due to their capability to generate code. Existing studies mainly explored the quality, e.g., correctness and security, of AI-generated code, while in real-world software development, the prerequisite is to distinguish AI-generated code from human-written code, which emphasizes the need to explicitly declare AI-generated code by developers. To this end, this study intends to understand the ways developers use to self-declare AI-generated code and explore the reasons why developers choose to self-declare or not. We conducted a mixed-methods study consisting of two phases. In the first phase, we mined GitHub repositories and collected 613 instances of AI-generated code snippets. In the second phase, we conducted a follow-up practitioners’ survey, which received 111 valid responses. Our research revealed the practices followed by developers to self-declare AI-generated code. Most practitioners (76.6%) always or sometimes self-declare AI-generated code. In contrast, other practitioners (23.4%) noted that they never self-declare AI-generated code. The reasons for self-declaring AI-generated code include the need to track and monitor the code for future review and debugging, and ethical considerations. The reasons for not self-declaring AI-generated code include extensive modifications to AI-generated code and the developers’ perception that self-declaration is an unnecessary activity. We finally provided guidelines for practitioners to self-declare AI-generated code, addressing ethical and code quality concerns.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261625",
    "type": "article"
  },
  {
    "title": "A Comparative Study of Android Performance Issues in Real-world Applications and Literature",
    "doi": "https://doi.org/10.1145/3771932",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Dianshu Liao; Shidong Pan; Siyuan Yang; Yanjie Zhao; Zhenchang Xing; Xiaoyu Sun",
    "corresponding_authors": "",
    "abstract": "Performance issues in Android applications significantly undermine users’ experience, engagement, and retention, which is a long-lasting research topic in academia. Unlike functionality issues, performance issues are more difficult to diagnose and resolve due to their complex root causes, which often emerge only under specific conditions or payloads. Although many efforts have attempted to mitigate the impact of performance issues by developing methods to automatically identify and resolve them, it remains unclear if this objective has been fulfilled, and the existing approaches indeed targeted on the most critical performance issues encountered in real-world settings. To this end, we conduct a large-scale comparative study of Android performance issues in real-world applications and literature. Specifically, we started by investigating real-world performance issues, their underlying root causes (i.e., contributing factors), and common code patterns. We then took an additional step to empirically summarize existing approaches and datasets through a literature review, assessing how well academic research reflects the real-world challenges faced by developers and users. Our comparison results show a substantial divergence exists in the primary performance concerns of researchers, developers, and users. Among all the identified factors, 57.14% have not been examined in academic research, while a substantial 63.41% remain unaddressed by existing tools, and 70.73% lack corresponding datasets. This stark contrast underscores a substantial gap in our understanding and management of performance issues. Consequently, it is crucial for our community to intensify efforts to bridge these gaps and achieve comprehensive detection and resolution of performance issues.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261640",
    "type": "article"
  },
  {
    "title": "On the Evaluation of Large Language Models in Multilingual Vulnerability Repair",
    "doi": "https://doi.org/10.1145/3771930",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Dong Wang; Junji Yu; Honglin Shu; Michael C. Fu; Chakkrit Tantithamthavorn; Yasutaka Kamei; Junjie Chen",
    "corresponding_authors": "",
    "abstract": "Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C/C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitation. Although some work has begun to explore LLM’s repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows that Go consistently achieves the highest effectiveness across all model types, while C/C++ performs the worst. Based on findings, we discuss the promising of LLM on multilingual vulnerability repair and reasons behind LLM failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs to multilingual vulnerability repair.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261645",
    "type": "article"
  },
  {
    "title": "DamFlow: Preventing a Flood of Irrelevant Data Flows in Android Apps",
    "doi": "https://doi.org/10.1145/3772002",
    "publication_date": "2025-10-17",
    "publication_year": 2025,
    "authors": "Marco Alecci; Jordan Samhi; Marc Miltenberger; Steven Arzt; Tegawendé F. Bissyandé; Jacques Klein",
    "corresponding_authors": "",
    "abstract": "State-of-the-art tools like FlowDroid have been proposed to detect data leaks in Android apps, but two main challenges persist: ① false alarms and ② undetected data leaks. One contributing factor to these challenges is that a tool such as FlowDroid relies on predefined lists of privacy-sensitive source and sink API methods. Generating such lists is complex; incomplete or inaccurate lists result in both false alarms (i.e., irrelevant data flows) and undetected data leaks. Additionally, data leaks are highly context-dependent. For instance, GPS data flowing from a navigation app is expected, but the same flow in a calculator app is suspicious. Even when FlowDroid identifies a source-to-sink path, it may not be relevant to privacy analysis, further increasing false alarms. To tackle these issues, we propose a novel approach named DamFlow, which, by combining backward taint analysis with context-aware anomaly detection, prevents a “flood” of irrelevant data flows while at the same time finding data leaks missed by existing approaches. Our evaluation demonstrates that DamFlow significantly reduces reported leaks per app while uncovering previously undetected leaks, enhancing FlowDroid's practicality for real-world data leak detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415282048",
    "type": "article"
  },
  {
    "title": "SETS: A Simple yet Effective DNN Test Selection Approach",
    "doi": "https://doi.org/10.1145/3772084",
    "publication_date": "2025-10-18",
    "publication_year": 2025,
    "authors": "Jingling Wang; Huayao Wu; Peng Wang; Xintao Niu; Changhai Nie",
    "corresponding_authors": "",
    "abstract": "To reduce the substantial manual annotation costs in testing deep neural networks (DNNs), various test selection approaches have been proposed, with uncertainty serving as a key objective to trigger more failures. However, focusing solely on uncertainty may expose failures caused by the same underlying fault in the model. Accordingly, recent studies have introduced diversity as another important objective, proposing multi-objective search-based approaches to select test inputs that can reveal distinct faults. Despite their effectiveness, these approaches typically require long execution times, limiting their efficiency in large-scale datasets. In this paper, we propose SETS, a simple yet effective multi-objective DNN test selection approach. Like existing multi-objective approaches, SETS optimizes both uncertainty and diversity to maximize the number of unique faults detected by the selected test inputs. But, SETS prioritizes high-uncertainty test inputs to reduce the candidate set size, and employs an efficient greedy strategy to further reduce the number of fitness evaluations. Thus, SETS differentiates itself by considerably improving the efficiency of selection while preserving effectiveness. We evaluate SETS on eight subjects, demonstrating its significant superiority over baseline approaches. SETS can operate on average 42.98 times faster while still achieving better fault detection and model retraining with its simple implementation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415312752",
    "type": "article"
  },
  {
    "title": "Continuously Learning Bug Locations",
    "doi": "https://doi.org/10.1145/3771929",
    "publication_date": "2025-10-18",
    "publication_year": 2025,
    "authors": "Paulina Stevia Nouwou Mindom; Léuson Da Silva; Amin Nikanjam; Foutse Khomh",
    "corresponding_authors": "",
    "abstract": "Automatically locating buggy changesets associated with bug reports is crucial in the software development process. Deep Learning (DL)-based techniques show promising results by leveraging structural information from the code and learning links between changesets and bug reports. However, since source code associated with changesets evolves, the performance of such models tends to degrade over time due to concept drift. Aiming to address this challenge, in this paper, we evaluate the potential of using Continual Learning (CL) techniques in multiple sub-tasks setting for bug localization (each of which operates on either stationary or non-stationary data), comparing it against a bug localization technique that leverages the BERT model, a deep reinforcement learning-based technique that leverages the A2C algorithm, and a DL-based function-level interaction model for semantic bug localization. Additionally, we enhanced the CL techniques by using logistic regression to identify and integrate the most significant bug-inducing factors. Our empirical evaluation across seven widely used software projects shows that CL techniques perform better than DL-based techniques by up to 61% in terms of Mean Reciprocal Rank (MRR), 44% in terms of Mean Average Precision (MAP), 83% in terms of top@1, 56% in terms of top@5, and 66% in terms of top@10 metrics in non-stationary setting. Further, we show that the CL techniques we studied are effective at localizing changesets relevant to a bug report while being able to mitigate catastrophic forgetting across the studied tasks and require up to 5x less computational effort during training. Our findings demonstrate the potential of adopting CL for bug localization in non-stationary settings, and we hope it helps to improve bug localization activities in Software Engineering using CL techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415312861",
    "type": "article"
  },
  {
    "title": "Do Current Language Models Support Code Intelligence for R Programming Language? RCR Report",
    "doi": "https://doi.org/10.1145/3744902",
    "publication_date": "2025-07-08",
    "publication_year": 2025,
    "authors": "Zixiao Zhao; Fatemeh H. Fard",
    "corresponding_authors": "",
    "abstract": "In this report, we introduce the dataset curated to replicate and extend experiments on R programming tasks, particularly code summarization and method name prediction. The dataset was generated by collecting R repositories from GitHub, parsing the code snippets using the tree-sitter parser, and matching them with natural language descriptions based on Roxygen2 documentation. Building on this dataset, our work conducts an in-depth analysis of the performance of Pre-trained Language Models for Code (Code-PLMs) on R code. We highlight the challenges posed by R’s dual paradigms—Tidyverse and Base R—and demonstrate that current models, including Large Language Models, exhibit varying degrees of performance degradation when applied to R code. As a result, we underscore the complexity of effectively leveraging Code-PLMs for R, given its diverse programming styles and language features.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415330783",
    "type": "article"
  },
  {
    "title": "The use of description logics in KBSE systems",
    "doi": "https://doi.org/10.1145/248233.248253",
    "publication_date": "1997-04-01",
    "publication_year": 1997,
    "authors": "Prémkumar Dévanbu; Mark A. Jones",
    "corresponding_authors": "",
    "abstract": "The increasing size and complexity of many software systems demand a greater emphasis on capturing and maintaining knowledge at many different levels within the software development process. This knowledge includes descriptions of the hardware and software components and their behavior, external and internal design specifications, and support for system testing. The Knowledge-based software engineering (KBSE) research paradigm is concerned with systems that use formally represented knowledge, with associated inference precedures, to support the various subactivities of software development. As they growing scale, KBSE systems must balance expressivity and inferential power with the real demands of knowledge base construction, maintenance, performance, and comprehensibility. Description logics (DLs) possess several features—a terminological orientation, a formal semantics, and efficient reasoning procedures—which offer an effective tradeoff of these factors. We discuss three KBSE systems in which DLs capture some of the requisite knowledge needed to support design, coding, and testing activities. We then survey some alternative approaches (to DLs) in KBSE systems. We close with a discussion of the benefits of DLs and ways to address some of their limitations.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1972551924",
    "type": "article"
  },
  {
    "title": "Modeling mobile IP in mobile UNITY",
    "doi": "https://doi.org/10.1145/304399.304400",
    "publication_date": "1999-04-01",
    "publication_year": 1999,
    "authors": "Peter J. McCann; Gruia-Catalin Roman",
    "corresponding_authors": "",
    "abstract": "With recent advances in wireless communication technology, mobile computing is an increasingly important area of research. A mobile system is one where independently executing components may migrate through some space during the course of the computation, and where the pattern of connectivity among the components changes as they move in and out of proximity. Mobile UNITY is a notation and proof logic for specifying and reasoning about mobile systems. In this article it is argued that Mobile UNITY contributes to the modular development of system specifications because of the declarative fashion in which coordination among components is specified. The packet-fowarding mechanism at the core of the Mobile IP protocol for routing to mobile hosts is taken as an example. A Mobile UNITY model of packet forwarding and the mobile system in which it must operate is developed. Proofs of correctness properties, including important real-time properties, are outlined, and the role of formal verification in the development of protocols such as Mobile IP is discussed.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2069771490",
    "type": "article"
  },
  {
    "title": "Using a coordination language to specify and analyze systems containing mobile components",
    "doi": "https://doi.org/10.1145/350887.350893",
    "publication_date": "2000-04-01",
    "publication_year": 2000,
    "authors": "Paolo Ciancarini; Francesco Franzè; Cecilia Mascolo",
    "corresponding_authors": "",
    "abstract": "New computing paradigms for network-aware applications need specification languages able to deal with the features of mobile code-based systems. A coordination language provides a formal framework in which the interaction of active entities can be expressed. A coordination language deals with the creation and destruction of code or complex agents, their communication activites, as well as their distribution and mobility in space. We show how the coordination language PoliS offers a flexible basis for the description and the automatic analysis of architectures of systems including mobile entities. Polis is based on multiple tuple spaces and offers a basis for defining, studying, and controlling mobility as it allows decoupling mobile entities from their environments both in space and in time. The pattern-matching mechanism adopted for communication helps in abstracting from addressing issues. We have developed a model-checking technique for the automatic analysis of PoliS specifications. In the article we show how this technique can be applied to mobile code-based systems",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2033677542",
    "type": "article"
  },
  {
    "title": "Reasoning about code mobility with mobile UNITY",
    "doi": "https://doi.org/10.1145/383876.383879",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Gian Pietro Picco; Gruia-Catalin Roman; Peter J. McCann",
    "corresponding_authors": "",
    "abstract": "Advancements in network technology have led to the emergence of new computing paradigms that challenge established programming practices by employing weak forms of consistency and dynamic forms of binding. Code mobility, for instance, allows for invocation-time binding between a code fragment and the location where it executes. Similarly, mobile computing allows hosts (and the software they execute) to alter their physical location. Despite apparent similarities, the two paradigms are distinct in their treatment of location and movement. This paper seeks to uncover a common foundation for the two paradigms by exploring the manner in which stereotypical forms of code mobility can be expressed in a programming notation developed for mobile computing. Several solutions to a distributed simulation problem are used to illustrate the modeling strategy and the ability to construct assertional-style proofs for programs that employ code mobility.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2020083587",
    "type": "article"
  },
  {
    "title": "Formal interpreters for diagram notations",
    "doi": "https://doi.org/10.1145/1044834.1044836",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Luciano Baresi; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "The article proposes an approach for defining extensible and flexible formal interpreters for diagram notations with significant dynamic semantics. More precisely, it addresses semi-formal diagram notations that have precisely-defined syntax, but informally defined (dynamic) semantics. These notations are often flexible to fit the different needs and expectations of users. Flexibility comes from the incompleteness or informality of the original definition and results in different interpretations.The approach defines interpreters by means of a mapping onto a semantic domain. Two sets of rules define the correspondences between the elements of the diagram notation and those of the semantic domain, and between events and states of the semantic domain and visual annotations on the elements of the diagram notation.Flexibility also leads to notation families, that is, sets of notations that share core concepts, but present slightly different interpretations. Existing approaches usually interpret these notations in isolation; the approach presented in this article allows the interpretation of a family as a whole. The feasibility of the approach is demonstrated through a prototype generator that allows users to implement special-purpose interpreters by defining relatively small sets of rules.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2016861322",
    "type": "article"
  },
  {
    "title": "Reasoning about static and dynamic properties in alloy",
    "doi": "https://doi.org/10.1145/1101815.1101819",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "Marcelo F. Frias; Carlos G. López Pombo; Gabriel Alfredo Baum; Nazareno Aguirre; T. S. E. Maibaum",
    "corresponding_authors": "",
    "abstract": "We study a number of restrictions associated with the first-order relational specification language Alloy. The main shortcomings we address are:---the lack of a complete calculus for deduction in Alloy's underlying formalism, the so called relational logic,---the inappropriateness of the Alloy language for describing (and analyzing) properties regarding execution traces.The first of these points was not regarded as an important issue during the genesis of Alloy, and therefore has not been taken into account in the design of the relational logic. The second point is a consequence of the static nature of Alloy specifications, and has been partly solved by the developers of Alloy; however, their proposed solution requires a complicated and unstructured characterization of executions.We propose to overcome the first problem by translating relational logic to the equational calculus of fork algebras . Fork algebras provide a purely relational formalism close to Alloy, which possesses a complete equational deductive calculus. Regarding the second problem, we propose to extend Alloy by adding actions . These actions, unlike Alloy functions, do modify the state. Much the same as programs in dynamic logic, actions can be sequentially composed and iterated, allowing them to state properties of execution traces at an appropriate level of abstraction.Since automatic analysis is one of Alloy's main features, and this article aims to provide a deductive calculus for Alloy, we show that:---the extension hereby proposed does not sacrifice the possibility of using SAT solving techniques for automated analysis,---the complete calculus for the relational logic is straightforwardly extended to a complete calculus for the extension of Alloy.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1975286358",
    "type": "article"
  },
  {
    "title": "A Generative Programming Framework for Context-Aware CSCW Applications",
    "doi": "https://doi.org/10.1145/2089116.2089121",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Devdatta Kulkarni; Tanvir Ahmed; Anand Tripathi",
    "corresponding_authors": "",
    "abstract": "We present a programming framework based on the paradigm of generative application development for building context-aware collaborative applications. In this approach, context-aware applications are implemented using a domain-specific design model, and their execution environment is generated and maintained by the middleware. The key features of this design model include support for context-based service discovery and binding, context-based access control, context-based multiuser coordination, and context-triggered automated task executions. The middleware uses the technique of policy-based specialization for generating application-specific middleware components from the generic middleware components. Through a case-study example, we demonstrate this approach and present the evaluations of the design model and the middleware.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2030138726",
    "type": "article"
  },
  {
    "title": "Gaia-PL",
    "doi": "https://doi.org/10.1145/2000799.2000803",
    "publication_date": "2011-09-01",
    "publication_year": 2011,
    "authors": "Josh Dehlinger; Robyn R. Lutz",
    "corresponding_authors": "",
    "abstract": "Agent-oriented software engineering (AOSE) has provided powerful and natural, high-level abstractions in which software developers can understand, model and develop complex, distributed systems. Yet, the realization of AOSE partially depends on whether agent-based software systems can achieve reductions in development time and cost similar to other reuse-conscious development methods. Specifically, AOSE does not adequately address requirements specifications as reusable assets. Software product line engineering is a reuse technology that supports the systematic development of a set of similar software systems through understanding, controlling, and managing their common, core characteristics and their differing variation points. In this article, we present an extension to the Gaia AOSE methodology, named Gaia-PL (Gaia-Product Line), for agent-based distributed software systems that enables requirements specifications to be easily reused. We show how our methodology uses a product line perspective to promote reuse in agent-based software systems early in the development life cycle so that software assets can be reused throughout system development and evolution. We also present results from an application to show how Gaia-PL provided reuse that reduced the design and development effort for a large, multiagent system.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2062412471",
    "type": "article"
  },
  {
    "title": "Domain Analysis and Description Principles, Techniques, and Modelling Languages",
    "doi": "https://doi.org/10.1145/3295738",
    "publication_date": "2019-03-26",
    "publication_year": 2019,
    "authors": "Dines Bjørner",
    "corresponding_authors": "Dines Bjørner",
    "abstract": "Domain science and engineering marks a new area of computing science. Just as we are formalising the syntax and semantics of programming languages, so we are formalising the syntax and semantics of human-assisted domains. Just as physicists are studying the natural physical world, endowing it with mathematical models, so we, computing scientists, are studying these domains, endowing them with mathematical models, A difference between the endeavours of physicists and ours lies in the tools: The physics models are based on classical mathematics, differential equations and integrals, and so on; our models are based on mathematical logic, set theory, and algebra [1]. Where physicists thus classically use a variety of differential and integral calculi to model the physical world, we shall be using the analysis and description calculi presented in this article to model primarily artifactual domains.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2925180974",
    "type": "article"
  },
  {
    "title": "D <scp>ESEN</scp>",
    "doi": "https://doi.org/10.1145/3365664",
    "publication_date": "2019-12-16",
    "publication_year": 2019,
    "authors": "Özgür Kafalı; Nirav Ajmeri; Munindar P. Singh",
    "corresponding_authors": "",
    "abstract": "We address the problem of engineering a sociotechnical system (STS) with respect to its stakeholders’ requirements. We motivate a two-tier STS conception composed of a technical tier that provides control mechanisms and describes what actions are allowed by the software components, and a social tier that characterizes the stakeholders’ expectations of each other in terms of norms. We adopt agents as computational entities, each representing a different stakeholder. Unlike previous approaches, our framework, D ESEN , incorporates the social dimension into the formal verification process. Thus, D ESEN supports agents potentially violating applicable norms—a consequence of their autonomy. In addition to requirements verification, D ESEN supports refinement of STS specifications via design patterns to meet stated requirements. We evaluate D ESEN at three levels. We illustrate how D ESEN carries out refinement via the application of patterns on a hospital emergency scenario. We show via a human-subject study that a design process based on our patterns is helpful for participants who are inexperienced in conceptual modeling and norms. We provide an agent-based environment to simulate the hospital emergency scenario to compare STS specifications (including participant solutions from the human-subject study) with metrics indicating social welfare and norm compliance, and other domain dependent metrics.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2996464519",
    "type": "article"
  },
  {
    "title": "Multi-objective Integer Programming Approaches for Solving the Multi-criteria Test-suite Minimization Problem",
    "doi": "https://doi.org/10.1145/3392031",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Yinxing Xue; Yan‐Fu Li",
    "corresponding_authors": "",
    "abstract": "Test-suite minimization is one key technique for optimizing the software testing process. Due to the need to balance multiple factors, multi-criteria test-suite minimization (MCTSM) becomes a popular research topic in the recent decade. The MCTSM problem is typically modeled as integer linear programming (ILP) problem and solved with weighted-sum single objective approach. However, there is no existing approach that can generate sound (i.e., being Pareto-optimal) and complete (i.e., covering the entire Pareto front) Pareto-optimal solution set, to the knowledge of the authors. In this work, we first prove that the ILP formulation can accurately model the MCTSM problem and then propose the multi-objective integer programming (MOIP) approaches to solve it. We apply our MOIP approaches on three specific MCTSM problems and compare the results with those of the cutting-edge methods, namely, NonlinearFormulation_LinearSolver (NF_LS) and two Multi-Objective Evolutionary Algorithms (MOEAs). The results show that our MOIP approaches can always find sound and complete solutions on five subject programs, using similar or significantly less time than NF_LS and two MOEAs do. The current experimental results are quite promising, and our approaches have the potential to be applied for other similar search-based software engineering problems.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3033058566",
    "type": "article"
  },
  {
    "title": "Using Relative Lines of Code to Guide Automated Test Generation for Python",
    "doi": "https://doi.org/10.1145/3408896",
    "publication_date": "2020-09-26",
    "publication_year": 2020,
    "authors": "Josie Holmes; Iftekhar Ahmed; Caius Brindescu; Rahul Gopinath; He Zhang; Alex Groce",
    "corresponding_authors": "",
    "abstract": "Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation. It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another. However, relative LOC , between components of the same project, turns out to be a highly useful metric for automated testing. In this article, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation. This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead. We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool. In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20%, up to 40% or more) and improve fault detection by an even larger margin (usually more than 75% and up to 400% or more). The LOC heuristic is also easy to combine with other approaches and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3090763333",
    "type": "article"
  },
  {
    "title": "Effective Techniques for Static Race Detection in Java Parallel Loops",
    "doi": "https://doi.org/10.1145/2729975",
    "publication_date": "2015-09-02",
    "publication_year": 2015,
    "authors": "Cosmin Radoi; Danny Dig",
    "corresponding_authors": "",
    "abstract": "Despite significant progress in recent years, the important problem of static race detection remains open. Previous techniques took a general approach and looked for races by analyzing the effects induced by low-level concurrency constructs (e.g., java.lang.Thread). But constructs and libraries for expressing parallelism at a higher level (e.g., fork-join, futures, parallel loops) are becoming available in all major programming languages. We claim that specializing an analysis to take advantage of the extra semantic information provided by the use of these constructs and libraries improves precision and scalability. We present I te R ace , a set of techniques that are specialized to use the intrinsic thread, safety, and dataflow structure of collections and of the new loop parallelism mechanism introduced in Java 8. Our evaluation shows that I te R ace is fast and precise enough to be practical. It scales to programs of hundreds of thousands of lines of code and reports very few race warnings, thus avoiding a common pitfall of static analyses. In five out of the seven case studies, I te R ace reported no false warnings. Also, it revealed six bugs in real-world applications. We reported four of them: one had already been fixed, and three were new and the developers confirmed and fixed them. Furthermore, we evaluate the effect of each specialization technique on the running time and precision of the analysis. For each application, we run the analysis under 32 different configurations. This allows to analyze each technique's effect both alone and in all possible combinations with other techniques.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1993635974",
    "type": "article"
  },
  {
    "title": "Toward a Holistic Approach to Verification and Validation of Autonomous Cognitive Systems",
    "doi": "https://doi.org/10.1145/3447246",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Angelo Ferrando; Louise A. Dennis; Rafael C. Cardoso; Michael Fisher; Davide Ancona; Viviana Mascardi",
    "corresponding_authors": "",
    "abstract": "When applying formal verification to a system that interacts with the real world, we must use a model of the environment. This model represents an abstraction of the actual environment, so it is necessarily incomplete and hence presents an issue for system verification. If the actual environment matches the model, then the verification is correct; however, if the environment falls outside the abstraction captured by the model, then we cannot guarantee that the system is well behaved. A solution to this problem consists in exploiting the model of the environment used for statically verifying the system’s behaviour and, if the verification succeeds, using it also for validating the model against the real environment via runtime verification. The article discusses this approach and demonstrates its feasibility by presenting its implementation on top of a framework integrating the Agent Java PathFinder model checker. A high-level Domain Specific Language is used to model the environment in a user-friendly way; the latter is then compiled to trace expressions for both static formal verification and runtime verification. To evaluate our approach, we apply it to two different case studies: an autonomous cruise control system and a simulation of the Mars Curiosity rover.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3124093587",
    "type": "article"
  },
  {
    "title": "Automated Detection of Client-State Manipulation Vulnerabilities",
    "doi": "https://doi.org/10.1145/2531921",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Anders Møller; Mathias Schwarz",
    "corresponding_authors": "",
    "abstract": "Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For this reason, it is desirable to construct automated tools that can assist the programmers in the application development process by detecting weaknesses. Many vulnerabilities are related to Web application code that stores references to application state in the generated HTML documents to work around the statelessness of the HTTP protocol. In this article, we show that such client-state manipulation vulnerabilities are amenable to tool-supported detection. We present a static analysis for the widely used frameworks Java Servlets, JSP, and Struts. Given a Web application archive as input, the analysis identifies occurrences of client state and infers the information flow between the client state and the shared application state on the server. This makes it possible to check how client-state manipulation performed by malicious users may affect the shared application state and cause leakage or modifications of sensitive information. The warnings produced by the tool help the application programmer identify vulnerabilities before deployment. The inferred information can also be applied to configure a security filter that automatically guards against attacks at runtime. Experiments on a collection of open-source Web applications indicate that the static analysis is able to effectively help the programmer prevent client-state manipulation vulnerabilities. The analysis detects a total of 4,802 client-state parameters in ten applications, whereof 4,437 are classified as safe and 241 reveal exploitable vulnerabilities.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3161167621",
    "type": "article"
  },
  {
    "title": "Model Transformation Development Using Automated Requirements Analysis, Metamodel Matching, and Transformation by Example",
    "doi": "https://doi.org/10.1145/3471907",
    "publication_date": "2021-11-17",
    "publication_year": 2021,
    "authors": "Kevin Lano; Shekoufeh Kolahdouz-Rahimi; S. Fang",
    "corresponding_authors": "",
    "abstract": "In this article, we address how the production of model transformations (MT) can be accelerated by automation of transformation synthesis from requirements, examples, and metamodels. We introduce a synthesis process based on metamodel matching, correspondence patterns between metamodels, and completeness and consistency analysis of matches. We describe how the limitations of metamodel matching can be addressed by combining matching with automated requirements analysis and model transformation by example (MTBE) techniques. We show that in practical examples a large percentage of required transformation functionality can usually be constructed automatically, thus potentially reducing development effort. We also evaluate the efficiency of synthesised transformations. Our novel contributions are: The concept of correspondence patterns between metamodels of a transformation. Requirements analysis of transformations using natural language processing (NLP) and machine learning (ML). Symbolic MTBE using “predictive specification” to infer transformations from examples. Transformation generation in multiple MT languages and in Java, from an abstract intermediate language.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3211635598",
    "type": "article"
  },
  {
    "title": "A Practical Approach for Dynamic Taint Tracking with Control-flow Relationships",
    "doi": "https://doi.org/10.1145/3485464",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Katherine Hough; Jonathan Bell",
    "corresponding_authors": "",
    "abstract": "Dynamic taint tracking, a technique that traces relationships between values as a program executes, has been used to support a variety of software engineering tasks. Some taint tracking systems only consider data flows and ignore control flows. As a result, relationships between some values are not reflected by the analysis. Many applications of taint tracking either benefit from or rely on these relationships being traced, but past works have found that tracking control flows resulted in over-tainting, dramatically reducing the precision of the taint tracking system. In this article, we introduce Conflux , alternative semantics for propagating taint tags along control flows. Conflux aims to reduce over-tainting by decreasing the scope of control flows and providing a heuristic for reducing loop-related over-tainting. We created a Java implementation of Conflux and performed a case study exploring the effect of Conflux on a concrete application of taint tracking, automated debugging. In addition to this case study, we evaluated Conflux ’s accuracy using a novel benchmark consisting of popular, real-world programs. We compared Conflux against existing taint propagation policies, including a state-of-the-art approach for reducing control-flow-related over-tainting, finding that Conflux had the highest F1 score on 43 out of the 48 total tests.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4200282997",
    "type": "article"
  },
  {
    "title": "Automated, Cost-effective, and Update-driven App Testing",
    "doi": "https://doi.org/10.1145/3502297",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Chanh Duc Ngo; Fabrizio Pastore; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Apps' pervasive role in our society led to the definition of test automation approaches to ensure their dependability. However, state-of-the-art approaches tend to generate large numbers of test inputs and are unlikely to achieve more than 50% method coverage. In this paper, we propose a strategy to achieve significantly higher coverage of the code affected by updates with a much smaller number of test inputs, thus alleviating the test oracle problem. More specifically, we present ATUA, a model-based approach that synthesizes App models with static analysis, integrates a dynamically-refined state abstraction function and combines complementary testing strategies, including (1) coverage of the model structure, (2) coverage of the App code, (3) random exploration, and (4) coverage of dependencies identified through information retrieval. Its model-based strategy enables ATUA to generate a small set of inputs that exercise only the code affected by the updates. In turn, this makes common test oracle solutions more cost-effective as they tend to involve human effort. A large empirical evaluation, conducted with 72 App versions belonging to nine popular Android Apps, has shown that ATUA is more effective and less effort intensive than state-of-the-art approaches when testing App updates.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4205529760",
    "type": "article"
  },
  {
    "title": "HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems",
    "doi": "https://doi.org/10.1145/3528100",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Jiezhu Cheng; Cuiyun Gao; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. Besides, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4223516517",
    "type": "article"
  },
  {
    "title": "<scp>LiDetector</scp> : License Incompatibility Detection for Open Source Software",
    "doi": "https://doi.org/10.1145/3518994",
    "publication_date": "2022-05-19",
    "publication_year": 2022,
    "authors": "Sihan Xu; Ya Gao; Lingling Fan; Zheli Liu; Yang Liu; Ji Hua",
    "corresponding_authors": "",
    "abstract": "Open-source software (OSS) licenses dictate the conditions, which should be followed to reuse, distribute, and modify software. Apart from widely-used licenses such as the MIT License, developers are also allowed to customize their own licenses (called custom license), whose descriptions are more flexible. The presence of such various licenses imposes challenges to understand licenses and their compatibility. To avoid financial and legal risks, it is essential to ensure license compatibility when integrating third-party packages or reusing code accompanied with licenses. In this work, we propose LiDetector , an effective tool that extracts and interprets OSS licenses (including both official licenses and custom licenses), and detects license incompatibility among these licenses. Specifically, LiDetector introduces a learning-based method to automatically identify meaningful license terms from an arbitrary license, and employs Probabilistic Context-Free Grammar (PCFG) to infer rights and obligations for incompatibility detection. Experiments demonstrate that LiDetector outperforms existing methods with 93.28% precision for term identification, and 91.09% accuracy for right and obligation inference, and can effectively detect incompatibility with 10.06% FP rate and 2.56% FN rate. Furthermore, with LiDetector , our large-scale empirical study on 1,846 projects reveals that 72.91% of the projects are suffering from license incompatibility, including popular ones such as the MIT License and the Apache License. We highlighted lessons learned from perspectives of different stakeholders and made all related data and the replication package publicly available to facilitate follow-up research.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4224437660",
    "type": "article"
  },
  {
    "title": "There’s no Such Thing as a Free Lunch: Lessons Learned from Exploring the Overhead Introduced by the Greenkeeper Dependency Bot in Npm",
    "doi": "https://doi.org/10.1145/3522587",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Benjamin Rombaut; Filipe R. Cogo; Bram Adams; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "Dependency management bots are increasingly being used to support the software development process, for example, to automatically update a dependency when a new version is available. Yet, human intervention is often required to either accept or reject any action or recommendation the bot creates. In this article, our objective is to study the extent to which dependency management bots create additional, and sometimes unnecessary, work for their users. To accomplish this, we analyze 93,196 issue reports opened by Greenkeeper , a popular dependency management bot used in open source software projects in the npm ecosystem. We find that Greenkeeper is responsible for half of all issues reported in client projects, inducing a significant amount of overhead that must be addressed by clients, since many of these issues were created as a result of Greenkeeper taking incorrect action on a dependency update (i.e., false alarms). Reverting a broken dependency update to an older version, which is a potential solution that requires the least overhead and is automatically attempted by Greenkeeper , turns out to not be an effective mechanism. Finally, we observe that 56% of the commits referenced by Greenkeeper issue reports only change the client’s dependency specification file to resolve the issue. Based on our findings, we argue that dependency management bots should (i) be configurable to allow clients to reduce the amount of generated activity by the bots, (ii) take into consideration more sources of information than only the pass/fail status of the client’s build pipeline to help eliminate false alarms, and (iii) provide more effective incentives to encourage clients to resolve dependency issues.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4225163285",
    "type": "article"
  },
  {
    "title": "Examining Penetration Tester Behavior in the Collegiate Penetration Testing Competition",
    "doi": "https://doi.org/10.1145/3514040",
    "publication_date": "2022-04-09",
    "publication_year": 2022,
    "authors": "Benjamin S. Meyers; Sultan Fahad Almassari; Brandon N. Keller; Andrew Meneely",
    "corresponding_authors": "",
    "abstract": "Penetration testing is a key practice toward engineering secure software. Malicious actors have many tactics at their disposal, and software engineers need to know what tactics attackers will prioritize in the first few hours of an attack. Projects like MITRE ATT&amp;CK™ provide knowledge, but how do people actually deploy this knowledge in real situations? A penetration testing competition provides a realistic, controlled environment with which to measure and compare the efficacy of attackers. In this work, we examine the details of vulnerability discovery and attacker behavior with the goal of improving existing vulnerability assessment processes using data from the 2019 Collegiate Penetration Testing Competition (CPTC). We constructed 98 timelines of vulnerability discovery and exploits for 37 unique vulnerabilities discovered by 10 teams of penetration testers. We grouped related vulnerabilities together by mapping to Common Weakness Enumerations and MITRE ATT&amp;CK™. We found that (1) vulnerabilities related to improper resource control (e.g., session fixation) are discovered faster and more often, as well as exploited faster, than vulnerabilities related to improper access control (e.g., weak password requirements), (2) there is a clear process followed by penetration testers of discovery/collection to lateral movement/pre-attack. Our methodology facilitates quicker analysis of vulnerabilities in future CPTC events.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4225608293",
    "type": "article"
  },
  {
    "title": "The Co-evolution of the WordPress Platform and Its Plugins",
    "doi": "https://doi.org/10.1145/3533700",
    "publication_date": "2022-07-25",
    "publication_year": 2022,
    "authors": "Jiahuei Lin; Mohammed Sayagh; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "One can extend the features of a software system by installing a set of additional components called plugins. WordPress, as a typical example of such plugin-based software ecosystems, is used by millions of websites and has a large number (i.e., 54,777) of available plugins. These plugin-based software ecosystems are different from traditional ecosystems (e.g., NPM dependencies) in the sense that there is high coupling between a platform and its plugins compared to traditional ecosystems for which components might not necessarily depend on each other (e.g., NPM libraries do not depend on a specific version of NPM or a specific version of a client software system). The high coupling between a plugin and its platform and other plugins causes incompatibility issues that occur during the co-evolution of a plugin and its platform as well as other plugins. In fact, incompatibility issues represent a major challenge when upgrading WordPress or its plugins. According to our study of the top 500 most-released WordPress plugins, we observe that incompatibility issues represent the third major cause for bad releases, which are rapidly (within the next 24 hours) fixed via urgent releases. Thirty-two percent of these incompatibilities are between a plugin and WordPress while 19% are between peer plugins. In this article, we study how plugins co-evolve with the underlying platform as well as other plugins, in an effort to understand the practices that are related support such co-evolution and reduce incompatibility issues. In particular, we investigate how plugins support the latest available versions of WordPress, as well as how plugins are related to each other, and how they co-evolve. We observe that a plugin’s support of new versions of WordPress with a large amount of code change is risky, as the releases that declare such support have a higher chance to be followed by an urgent release compared to ordinary releases. Although plugins support the latest WordPress version, plugin developers omit important changes such as deleting the use of removed WordPress APIs, which are removed a median of 873 days after the APIs have been removed from the source code of WordPress. Plugins introduce new releases that are made according to a median of five other plugins, which we refer to as peer-triggered releases. A median of 20% of the peer-triggered releases are urgent releases that fix problems in their previous releases. The most common goal of peer-triggered releases is the fixing of incompatibility issues that a plugin detects as late as after a median of 36 days since the last release of another plugin. Our work sheds light on the co-evolution of WordPress plugins with their platform as well as peer plugins in an effort to uncover the practices of plugin evolution, so WordPress can accordingly design approaches to avoid incompatibility issues.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4287378204",
    "type": "article"
  },
  {
    "title": "<scp>iBiR</scp> : Bug-report-driven Fault Injection",
    "doi": "https://doi.org/10.1145/3542946",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Ahmed Khanfir; Anil Koyuncu; Mike Papadakis; Maxime Cordy; Tegawendé F. Bissyandé; Jacques Klein; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Much research on software engineering relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it “blindly” injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce iBiR , a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by re-targeting a bug-report-driven automated program repair system, i.e., reversing its code transformation templates. iBiR is further appealing in practice since it requires deep knowledge of neither code nor tests, just of the program’s relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess iBiR by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, iBiR injects faults that couple with the real ones in around 36% of the cases, while mutation testing achieves less than 4%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4287554141",
    "type": "article"
  },
  {
    "title": "TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts",
    "doi": "https://doi.org/10.1145/3560263",
    "publication_date": "2022-08-29",
    "publication_year": 2022,
    "authors": "Zheyuan He; Shuwei Song; Yang Bai; Xiapu Luo; Ting Chen; Wensheng Zhang; Peng He; Hongwei Li; Xiaodong Lin; Xiaosong Zhang",
    "corresponding_authors": "",
    "abstract": "Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware , a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4293452506",
    "type": "article"
  },
  {
    "title": "IFDS-based Context Debloating for Object-Sensitive Pointer Analysis",
    "doi": "https://doi.org/10.1145/3579641",
    "publication_date": "2023-01-09",
    "publication_year": 2023,
    "authors": "Dongjie He; Jingbo Lu; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Object-sensitive pointer analysis, which separates the calling contexts of a method by its receiver objects, is known to achieve highly useful precision for object-oriented languages such as Java. Despite recent advances, all object-sensitive pointer analysis algorithms still suffer from the scalability problem due to the combinatorial explosion of contexts in large programs. In this article, we introduce a new approach, Conch , that can be applied to debloat contexts for all object-sensitive pointer analysis algorithms, thereby improving significantly their efficiency while incurring a negligible loss of precision. Our key insight is to approximate a recently proposed set of two necessary conditions for an object in a program to be context-sensitive, i.e., context-dependent (whose precise verification is undecidable) with a set of three linearly verifiable conditions in terms of the number of edges in the pointer assignment graph (PAG) representation of the program. These three linearly verifiable conditions, which turn out to be almost always necessary in practice, are synthesized from three key observations regarding context-dependability for the objects created and used in real-world object-oriented programs. To develop a practical implementation for Conch , we introduce an IFDS-based algorithm for reasoning about object reachability in the PAG of a program, which runs linearly in terms of the number of edges in the PAG. By debloating contexts for three representative object-sensitive pointer analysis algorithms, which are applied to a set of representative Java programs, Conch can speed up these three baseline algorithms substantially at only a negligible loss of precision (less than 0.1%) with respect to several commonly used precision metrics. In addition, Conch also improves their scalability by enabling them to analyze substantially more programs to completion than before (under a time budget of 12 hours). Conch has been open-sourced (http://www.cse.unsw.edu.au/~corg/tools/conch), opening up new opportunities for other researchers and practitioners to further improve this research. To demonstrate this, we introduce one extension of Conch to accelerate further the three baselines without losing any precision, providing further insights on extending Conch to make precision-efficiency tradeoffs in future research.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4313828767",
    "type": "article"
  },
  {
    "title": "Assessing the Early Bird Heuristic (for Predicting Project Quality)",
    "doi": "https://doi.org/10.1145/3583565",
    "publication_date": "2023-02-08",
    "publication_year": 2023,
    "authors": "N. C. Shrikanth; Tim Menzies",
    "corresponding_authors": "",
    "abstract": "Before researchers rush to reason across all available data or try complex methods, perhaps it is prudent to first check for simpler alternatives. Specifically, if the historical data has the most information in some small region, then perhaps a model learned from that region would suffice for the rest of the project. To support this claim, we offer a case study with 240 projects, where we find that the information in those projects “clumps” towards the earliest parts of the project. A quality prediction model learned from just the first 150 commits works as well, or better than state-of-the-art alternatives. Using just this “early bird” data, we can build models very quickly and very early in the project life cycle. Moreover, using this early bird method, we have shown that a simple model (with just a few features) generalizes to hundreds of projects. Based on this experience, we doubt that prior work on generalizing quality models may have needlessly complicated an inherently simple process. Further, prior work that focused on later-life cycle data needs to be revisited, since their conclusions were drawn from relatively uninformative regions. Replication note: All our data and scripts are available here: https://github.com/snaraya7/early-bird.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4319594647",
    "type": "article"
  },
  {
    "title": "Optimization Techniques for Model Checking Leads-to Properties in a Stratified Way",
    "doi": "https://doi.org/10.1145/3604610",
    "publication_date": "2023-06-17",
    "publication_year": 2023,
    "authors": "Canh Minh; Yati Phyo; Adrián Riesco; Kazuhiro Ogata",
    "corresponding_authors": "",
    "abstract": "We devised the L +1-layer divide &amp; conquer approach to leads-to model checking ( L +1-DCA2L2MC) and its parallel version, and developed sequential and parallel tools for L +1-DCA2L2MC. In a temporal logic called UNITY , designed by Chandy and Misra, the leads-to temporal connective plays an important role and many case studies have been conducted in UNITY, demonstrating that many systems requirements can be expressed as leads-to properties. Hence, it is worth dedicating to these properties. Counterexample generation is one of the main tasks in the L +1-DCA2L2MC technique that can be optimized to improve its running performance. This article proposes a technique to find all counterexamples at once in model checking with a new model checker. Furthermore, layer configuration selection is essential to make the best use of the L +1-DCA2L2MC technique. This work also proposes an approach to finding a good layer configuration for the technique with an analysis tool. Some experiments are conducted to demonstrate the power and usefulness of the two optimization techniques, respectively. Moreover, our sequential and parallel tools are compared with SPIN and LTSmin model checkers, showing a promising way to mitigate the state space explosion and improve the running performance of model checking when dealing with large state spaces.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4381050463",
    "type": "article"
  },
  {
    "title": "Exploring the Impact of Code Clones on Deep Learning Software",
    "doi": "https://doi.org/10.1145/3607181",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Ran Mo; Yao Zhang; Yushuo Wang; Siyuan Zhang; Pu Xiong; Zengyang Li; Yang Zhao",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) is a really active topic in recent years. Code cloning is a common code implementation that could negatively impact software maintenance. For DL software, developers rely heavily on frameworks to implement DL features. Meanwhile, to guarantee efficiency, developers often reuse the steps and configuration settings for building DL models. These may bring code copy-pastes or reuses inducing code clones. However, there is little work exploring code clones’ impact on DL software. In this article, we conduct an empirical study and show that: (1) code clones are prevalent in DL projects, about 16.3% of code fragments encounter clones, which is almost twice larger than the traditional projects; (2) 75.6% of DL projects contain co-changed clones, meaning changes are propagated among cloned fragments, which can bring maintenance difficulties; (3) Percentage of the clones and Number of clone lines are associated with the emergence of co-changes; (4) the prevalence of Code clones varies in DL projects with different frameworks, but the difference is not significant; (5) Type 1 co-changed clones often spread over different folders, but Types 2 and 3 co-changed clones mainly occur within the same files or folders; (6) 57.1% of all co-changed clones are involved in bugs.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4382938483",
    "type": "article"
  },
  {
    "title": "What Constitutes the Deployment and Runtime Configuration System? An Empirical Study on OpenStack Projects",
    "doi": "https://doi.org/10.1145/3607186",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Narjes Bessghaier; Mohammed Sayagh; Ali Ouni; Mohamed Wiem Mkaouer",
    "corresponding_authors": "",
    "abstract": "Modern software systems are designed to be deployed in different configured environments (e.g., permissions, virtual resources, network connections) and adapted at runtime to different situations (e.g., memory limits, enabling/disabling features, database credentials). Such a configuration during the deployment and runtime of a software system is implemented via a set of configuration files, which together constitute what we refer to as a “configuration system.” Recent research efforts investigated the evolution and maintenance of configuration files. However, they merely focused on a limited part of the configuration system (e.g., specific infrastructure configuration files or Dockerfiles), and their results do not generalize to the whole configuration system. To cope with such a limitation, we aim to better capture and understand what files constitute a configuration system. To do so, we leverage an open card sort technique to qualitatively study 1,756 configuration files from OpenStack, a large and widely studied open source software ecosystem. Our investigation reveals the existence of nine types of configuration files, which cover the creation of the infrastructure on top of which OpenStack will be deployed, along with other types of configuration files used to customize OpenStack after its deployment. These configuration files are interconnected while being used at different deployment stages. For instance, we observe specific configuration files used during the deployment stage to create other configuration files that are used in the runtime stage. We also observe that identifying and classifying these types of files is not straightforward, as five out of the nine types can be written in similar programming languages (e.g., Python and Bash) as regular source code files. We also found that the same file extensions (e.g., Yaml ) can be used for different configuration types, making it difficult to identify and classify configuration files. Thus, we first leverage a machine learning model to identify configuration from non-configuration files, which achieved a median area under the curve (AUC) of 0.91, a median Brier score of 0.12, a median precision of 0.86, and a median recall of 0.83. Thereafter, we leverage a multi-class classification model to classify configuration files based on the nine configuration types. Our multi-class classification model achieved a median weighted AUC of 0.92, a median Brier score of 0.04, a median weighted precision of 0.84, and a median weighted recall of 0.82. Our analysis also shows that with only 100 labeled configuration and non-configuration files, our model reached a median AUC higher than 0.69. Furthermore, our configuration model requires a minimum of 100 configuration files to reach a median weighted AUC higher than 0.75.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4382987313",
    "type": "article"
  },
  {
    "title": "NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing - RCR Report",
    "doi": "https://doi.org/10.1145/3580599",
    "publication_date": "2023-07-05",
    "publication_year": 2023,
    "authors": "Fan Hu; Shisong Qin; Zheyu Ma; Bodong Zhao; Tingting Yin; Chao Zhang",
    "corresponding_authors": "",
    "abstract": "We provide artifacts to reproduce the evaluation results of our article: “NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing”. The provided artifacts can be downloaded from https://zenodo.org/record/7134490 . It includes 14 docker containers, several scripts for execution and analysis, one additional proof for the crash results, and six related documents for the running of experiments. We claim for all three badges, i.e., Available, Functional, and Reusable. This report gives instructions on how to reproduce the answers which mainly involve basic operations on the Ubuntu operating system.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4383223817",
    "type": "article"
  },
  {
    "title": "<scp>StubCoder</scp> : Automated Generation and Repair of Stub Code for Mock Objects",
    "doi": "https://doi.org/10.1145/3617171",
    "publication_date": "2023-08-21",
    "publication_year": 2023,
    "authors": "Hengcheng Zhu; Lili Wei; Valerio Terragni; Yepang Liu; Shing-Chi Cheung; Jiarong Wu; Qin Sheng; Bing Zhang; Lihong Song",
    "corresponding_authors": "",
    "abstract": "Mocking is an essential unit testing technique for isolating the class under test from its dependencies. Developers often leverage mocking frameworks to develop stub code that specifies the behaviors of mock objects. However, developing and maintaining stub code is labor-intensive and error-prone. In this article, we present StubCoder to automatically generate and repair stub code for regression testing. StubCoder implements a novel evolutionary algorithm that synthesizes test-passing stub code guided by the runtime behavior of test cases. We evaluated our proposed approach on 59 test cases from 13 open source projects. Our evaluation results show that StubCoder can effectively generate stub code for incomplete test cases without stub code and repair obsolete test cases with broken stub code.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386031629",
    "type": "article"
  },
  {
    "title": "A First Look at On-device Models in iOS Apps",
    "doi": "https://doi.org/10.1145/3617177",
    "publication_date": "2023-08-23",
    "publication_year": 2023,
    "authors": "Han Hu; Yujin Huang; Qiuyuan Chen; Terry Yue Zhuo; Chunyang Chen",
    "corresponding_authors": "",
    "abstract": "Powered by the rising popularity of deep learning techniques on smartphones, on-device deep learning models are being used in vital fields such as finance, social media, and driving assistance. Because of the transparency of the Android platform and the on-device models inside, on-device models on Android smartphones have been proven to be extremely vulnerable. However, due to the challenge in accessing and analyzing iOS app files, despite iOS being a mobile platform as popular as Android, there are no relevant works on on-device models in iOS apps. Since the functionalities of the same app on Android and iOS platforms are similar, the same vulnerabilities may exist on both platforms. In this article, we present the first empirical study about on-device models in iOS apps, including their adoption of deep learning frameworks, structure, functionality, and potential security issues. We study why current developers use different on-device models for one app between iOS and Android. We propose a more general attack against white-box models that does not rely on pre-trained models and a new adversarial attack approach based on our findings to target iOS’s gray-box on-device models. Our results show the effectiveness of our approaches. Finally, we successfully exploit the vulnerabilities of on-device models to attack real-world iOS apps.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386099726",
    "type": "article"
  },
  {
    "title": "Testing Abstractions for Cyber-Physical Control Systems",
    "doi": "https://doi.org/10.1145/3617170",
    "publication_date": "2023-08-24",
    "publication_year": 2023,
    "authors": "Claudio Mandrioli; Max Nyberg Carlsson; Martina Maggio",
    "corresponding_authors": "",
    "abstract": "Control systems are ubiquitous and often at the core of Cyber-Physical Systems, like cars and aeroplanes. They are implemented as embedded software that interacts in closed loop with the physical world through sensors and actuators. As a consequence, the software cannot just be tested in isolation. To close the loop in a testing environment and root causing failure generated by different parts of the system, executable models are used to abstract specific components. Different testing setups can be implemented by abstracting different elements: The most common ones are model-in-the-loop, software-in-the-loop, hardware-in-the-loop, and real-physics-in-the-loop. In this article, we discuss the properties of these setups and the types of faults they can expose. We develop a comprehensive case study using the Crazyflie, a drone whose software and hardware are open source. We implement all the most common testing setups and ensure the consistent injection of faults in each of them. We inject faults in the control system and we compare with the nominal performance of the non-faulty software. Our results show the specific capabilities of the different setups in exposing faults. Contrary to intuition and previous literature, we show that the setups do not belong to a strict hierarchy, and they are best designed to maximize the differences across them rather than to be as close as possible to reality.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386136258",
    "type": "article"
  },
  {
    "title": "Probabilistic Safe WCET Estimation for Weakly Hard Real-time Systems at Design Stages",
    "doi": "https://doi.org/10.1145/3617176",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Jaekwon Lee; Seung Yeob Shin; Lionel Briand; Shiva Nejati",
    "corresponding_authors": "",
    "abstract": "Weakly hard real-time systems can, to some degree, tolerate deadline misses, but their schedulability still needs to be analyzed to ensure their quality of service. Such analysis usually occurs at early design stages to provide implementation guidelines to engineers so they can make better design decisions. Estimating worst-case execution times (WCET) is a key input to schedulability analysis. However, early on during system design, estimating WCET values is challenging, and engineers usually determine them as plausible ranges based on their domain knowledge. Our approach aims at finding restricted, safe WCET sub-ranges given a set of ranges initially estimated by experts in the context of weakly hard real-time systems. To this end, we leverage (1) multi-objective search aiming at maximizing the violation of weakly hard constraints to find worst-case scheduling scenarios and (2) polynomial logistic regression to infer safe WCET ranges with a probabilistic interpretation. We evaluated our approach by applying it to an industrial system in the satellite domain and several realistic synthetic systems. The results indicate that our approach significantly outperforms a baseline relying on random search without learning and estimates safe WCET ranges with a high degree of confidence in practical time (&lt; 23 h).",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386191524",
    "type": "article"
  },
  {
    "title": "A Closer Look at the Security Risks in the Rust Ecosystem",
    "doi": "https://doi.org/10.1145/3624738",
    "publication_date": "2023-09-16",
    "publication_year": 2023,
    "authors": "Xiaoye Zheng; Zhiyuan Wan; Yun Zhang; Rui Chang; David Lo",
    "corresponding_authors": "",
    "abstract": "Rust is an emerging programming language designed for the development of systems software. To facilitate the reuse of Rust code, crates.io , as a central package registry of the Rust ecosystem, hosts thousands of third-party Rust packages. The openness of crates.io enables the growth of the Rust ecosystem but comes with security risks by severe security advisories. Although Rust guarantees a software program to be safe via programming language features and strict compile-time checking, the unsafe keyword in Rust allows developers to bypass compiler safety checks for certain regions of code. Prior studies empirically investigate the memory safety and concurrency bugs in the Rust ecosystem, as well as the usage of unsafe keywords in practice. Nonetheless, the literature lacks a systematic investigation of the security risks in the Rust ecosystem. In this article, we perform a comprehensive investigation into the security risks present in the Rust ecosystem, asking “what are the characteristics of the vulnerabilities, what are the characteristics of the vulnerable packages, and how are the vulnerabilities fixed in practice?”. To facilitate the study, we first compile a dataset of 433 vulnerabilities, 300 vulnerable code repositories, and 218 vulnerability fix commits in the Rust ecosystem, spanning over 7 years. With the dataset, we characterize the types, life spans, and evolution of the disclosed vulnerabilities. We then characterize the popularity, categorization, and vulnerability density of the vulnerable Rust packages, as well as their versions and code regions affected by the disclosed vulnerabilities. Finally, we characterize the complexity of vulnerability fixes and localities of corresponding code changes, and inspect how practitioners fix vulnerabilities in Rust packages with various localities. We find that memory safety and concurrency issues account for nearly two thirds of the vulnerabilities in the Rust ecosystem. It takes over 2 years for the vulnerabilities to become publicly disclosed, and one-third of the vulnerabilities have no fixes committed before their disclosure. In terms of vulnerability density, we observe a continuous upward trend at the package level over time, but a decreasing trend at the code level since August 2020. In the vulnerable Rust packages, the vulnerable code tends to be localized at the file level, and contains statistically significantly more unsafe functions and blocks than the rest of the code. More popular packages tend to have more vulnerabilities, while the less popular packages suffer from vulnerabilities for more versions. The vulnerability fix commits tend to be localized to a limited number of lines of code. Developers tend to address vulnerable safe functions by adding safe functions or lines to them, vulnerable unsafe blocks by removing them, and vulnerable unsafe functions by modifying unsafe trait implementations. Based on our findings, we discuss implications, provide recommendations for software practitioners, and outline directions for future research.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386802779",
    "type": "article"
  },
  {
    "title": "Automated Mapping of Adaptive App GUIs from Phones to TVs",
    "doi": "https://doi.org/10.1145/3631968",
    "publication_date": "2023-11-07",
    "publication_year": 2023,
    "authors": "Han Hu; Ruiqi Dong; John Grundy; Thai Minh Nguyen; Huaxiao Liu; Chunyang Chen",
    "corresponding_authors": "",
    "abstract": "With the increasing interconnection of smart devices, users often desire to adopt the same app on quite different devices for identical tasks, such as watching the same movies on both their smartphones and TVs. However, the significant differences in screen size, aspect ratio, and interaction styles make it challenging to adapt Graphical User Interfaces (GUIs) across these devices. Although there are millions of apps available on Google Play, only a few thousand are designed to support smart TV displays. Existing techniques to map a mobile app GUI to a TV either adopt a responsive design, which struggles to bridge the substantial gap between phone and TV, or use mirror apps for improved video display, which requires hardware support and extra engineering efforts. Instead of developing another app for supporting TVs, we propose a semi-automated approach to generate corresponding adaptive TV GUIs, given the phone GUIs as the input. Based on our empirical study of GUI pairs for TVs and phones in existing apps, we synthesize a list of rules for grouping and classifying phone GUIs, converting them to TV GUIs, and generating dynamic TV layouts and source code for the TV display. Our tool is not only beneficial to developers but also to GUI designers, who can further customize the generated GUIs for their TV app development. An evaluation and user study demonstrate the accuracy of our generated GUIs and the usefulness of our tool.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388463775",
    "type": "article"
  },
  {
    "title": "Grammar Mutation for Testing Input Parsers – RCR Report",
    "doi": "https://doi.org/10.1145/3712192",
    "publication_date": "2025-01-21",
    "publication_year": 2025,
    "authors": "Bachir Bendrissou; Cristian Cadar; Alastair F. Donaldson",
    "corresponding_authors": "",
    "abstract": "This document presents the artefact that was used to run experiments and produce results reported in the paper “Grammar Mutation for Testing Input Parsers”. The artefact includes a docker image and a dockerfile. The image can be reconstructed by executing the provided dockerfile. The dockerfile includes all instructions needed to reconstruct the image. Files stored in the image include scripts, systems under test, and grammar files. We also list the steps and instructions required to reproduce the results of the experiment.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406674512",
    "type": "article"
  },
  {
    "title": "JSimpo: Structural Deobfuscation of JavaScript Programs",
    "doi": "https://doi.org/10.1145/3714460",
    "publication_date": "2025-01-22",
    "publication_year": 2025,
    "authors": "Tianyu Chen; Ding Li; Ying Zhang; Tao Xie",
    "corresponding_authors": "",
    "abstract": "JavaScript (JS) obfuscation is now prevalent among popular websites and introduces challenges for malware detection and code review. Given an obfuscated JS program, existing deobfuscation techniques aim to recover the original JS program. However, these techniques overlook structural obfuscation (e.g., control-flow flattening), which causes deobfuscation to have a sub-optimal success rate. To address these challenges, in this article, we propose the first approach of structural deobfuscation named JSimpo for JS programs with two novel techniques: slice symbolic execution and dynamic code execution. We implement our JSimpo approach and evaluate it on 2,000 JS programs from the top 100 JS projects on GitHub. The evaluation results show that JSimpo can effectively conduct structural deobfuscation, boosting the average structural similarity to 78.41% (from 39.33%) between obfuscated programs and their original programs, whereas the best of the state-of-the-art/practice deobfuscators can achieve only 62.64%. The results also show JSimpo's generalization ability over programs obfuscated by various obfuscators. Additionally, JSimpo preserves the semantics of deobfuscated programs by passing all test cases that obfuscated programs have passed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406688111",
    "type": "article"
  },
  {
    "title": "The Good, the Bad, and the Monstrous: Predicting Highly Change-Prone Source Code Methods at Their Inception",
    "doi": "https://doi.org/10.1145/3715006",
    "publication_date": "2025-01-23",
    "publication_year": 2025,
    "authors": "Shaiful Chowdhury",
    "corresponding_authors": "Shaiful Chowdhury",
    "abstract": "The cost of software maintenance often surpasses the initial development expenses, making it a significant concern for the software industry. A key strategy for alleviating future maintenance burdens is the early prediction and identification of change-prone code components, which allows for timely optimizations. While prior research has largely concentrated on predicting change-prone files and classes—an approach less favored by practitioners—this paper shifts focus to predicting highly change-prone methods, aligning with the preferences of both practitioners and researchers. We analyzed 774,051 source code methods from 49 prominent open-source Java projects. Our findings reveal that approximately 80% of changes are concentrated in just 20% of the methods, demonstrating the Pareto 80/20 principle. Moreover, this subset of methods is responsible for the majority of the identified bugs in these projects. After establishing their critical role in mitigating software maintenance costs, our study shows that machine learning models can effectively identify these highly change-prone methods from their inception. Additionally, we conducted a thorough manual analysis to uncover common patterns (or concepts) among the more difficult-to-predict methods. These insights can help future research develop new features and enhance prediction accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406738291",
    "type": "article"
  },
  {
    "title": "Automated Detection and Repair of Floating-point Precision Problems in Convolutional Neural Network Operators",
    "doi": "https://doi.org/10.1145/3715104",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Jiawei Liu; Xufan Zhang; Lurong Xu; Chunrong Fang; Mingzheng Gu; Weisi Luo; Dong Chai; Jiang Wang; Zhihong Zhao; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Network (CNN) operators, mostly based on mathematical linear computations, are of vital importance to developing CNN-based software. Existing studies reveal that these operators are prone to floating-point precision problems (FPPs). In a CNN-based application, such problems can be propagated and result in catastrophic consequences. Thus, it is highly desired to detect and repair the FPPs in CNN operators. Considering the FPPs in CNN operators are mainly caused by accumulated floating-point errors and diverse floating-point tensors instead of wrong codes or bad implementations, it requires much time cost and is difficult to tackle these FPPs. In this paper, we propose the first method for the automated detection and repair of FPPs in CNN operators from the perspective of floating-point tensors. To generate diverse tensors with floating-point numbers, we design two levels of mutation rules, namely computation-level mutation and input-level mutation, containing a total of five mutation methods. To detect the FPPs caused by the accumulated floating-point errors, our method uses a weight matrix to guide the progressive mutation. To repair the detected FPPs, our method transforms the error-prone floating-point tensors based on the mathematical rewriting of the floating-point linear computational properties without destroying the original computation. Experimental results show that our methods can detect and repair FPPs in CNN operators effectively and efficiently and could reduce 93.32% to 100% of the FPPs in CNN operators. We conduct a case study on six different widely-used CNN models and confirm that the proposed FPP method is generalizable and effective across a variety of tasks and architectures. Our detection and repair method offers an intuitive way to handle FPPs during development, allowing users to continue building and fine-tuning their models without being slowed down by numerical precision errors. We believe that our method could open up a new way to enhance the quality of CNN operators and CNN-based software.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406867759",
    "type": "article"
  },
  {
    "title": "WebAssembly for Container Runtime: Are We There Yet?",
    "doi": "https://doi.org/10.1145/3712197",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Mugeng Liu; Haiyang Shen; Yixuan Zhang; Hong Mei; Yun Ma",
    "corresponding_authors": "",
    "abstract": "To pursue more efficient software deployment with containers, WebAssembly (abbreviated as Wasm) has long been regarded as a promising alternative to native container runtime (such as Docker container) due to its features of secure memory sandbox, lightweight isolation, portability, and multi-language support. However, it remains unknown whether and how much Wasm indeed brings benefits for containerized software applications. To fill the knowledge gap, this paper presents the first measurement study on Wasm-based container runtime (i.e., Wasm container) by comparison with the Docker container and native standalone Wasm runtime for execution performance in terms of the startup, computation, system interface access, and resource consumption. Surprisingly, we find that the Wasm container does not achieve better performance versus the Docker container as expected and introduces significant overhead compared to the standalone Wasm runtime. Through comparison, we identify the main causes of performance degradation for Wasm containers. Some stem from the heavy containerization overhead similar to Docker containers, while others are inherently caused by Wasm VMs and the WASI interface. Our findings can help software developers, Wasm container developers and the Wasm community improve the efficiency of utilizing Wasm-based container runtime, ultimately optimizing software performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407241474",
    "type": "article"
  },
  {
    "title": "Obfuscated Clone Search in JavaScript based on Reinforcement Subsequence Learning",
    "doi": "https://doi.org/10.1145/3711903",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Leo Song; Steven H. H. Ding; Yuan Tian; Litao Li; Weihan Ou; Philippe Charland; Andrew Walenstein",
    "corresponding_authors": "",
    "abstract": "Finding similar code is important for software engineering, defense of intellectual property, and security, and one of the increasingly common ways adversaries use to defeat the detection of similar code is through obfuscations such as code transformation and scattering the code they wish to hide amongst long sequences. Moving code far enough apart poses a specific challenge for solutions with localized features (e.g., n-grams) or attention mechanisms as the code parts are distributed beyond the local context window. We introduce a neural network solution pattern called “Cybertron” that addresses this problem by utilizing reinforcement learning to train a code abstraction and summarization function; this converts arbitrarily long code into fixed-length real vectors in a way that is optimized for similarity search. The key to the design is the smart selection of important elements of the code and abstraction to preserve semantic function while minimizing syntactic feature information. We evaluated the approach on a three-challenge benchmark of obfuscated JavaScript, a scripting language that is commonly obfuscated and for which code-mixing is a rising challenge. The evaluation shows our approach identifies obfuscated code within even large scripts with an AUC of 78%, which outperforms current state-of-the-art sequence models by 7-35%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407241478",
    "type": "article"
  },
  {
    "title": "A Comprehensive Study of Governance Issues in Decentralized Finance Applications",
    "doi": "https://doi.org/10.1145/3717062",
    "publication_date": "2025-02-13",
    "publication_year": 2025,
    "authors": "Wei Ma; Chenguang Zhu; Ye Liu; Xiaofei Xie; Yi Li",
    "corresponding_authors": "",
    "abstract": "Decentralized Finance (DeFi) is a prominent application of smart contracts, representing a novel financial paradigm in contrast to centralized finance. While DeFi applications are rapidly emerging on mainstream blockchain platforms, their quality varies greatly, presenting numerous challenges, particularly in terms of their governance mechanisms. In this paper, we present a comprehensive study of governance issues in DeFi applications. Initially, we collected 3,165 academic papers and numerous industry reports. After thorough screening, we selected 44 academic papers and 11 industry reports for detailed analysis. Drawing upon insights from industry reports and academic research articles, we develop a taxonomy to categorize these governance issues. We collect and build a dataset of 4,446 audit reports from seventeen Web3 security companies, categorizing their governance issues according to our constructed taxonomy. We conducted a thorough analysis of governance issues and identified vulnerabilities in the governance design and implementation, e.g., voting sybil attack and proposal front-running. Our statistical analysis indicates that a significant portion (35.48%) of governance-related issues is classified as severe. Within these, ownership-related problems constitute the largest share (65.38%). Despite DeFi governance being essential for the long-term success of DeFi projects, our data shows that both auditors and development teams have not fully grasped its significance. Based on audit reports, we also analyzed common vulnerabilities and issues in the governance domain. Our research identifies two primary categories of DeFi governance issues: technology-centric and human-centric. Technology-centric issues can be addressed through technology updates and iterations, whereas human-centric issues are influenced not only by the development team's technical skills but also by their understanding of DeFi governance. Data analysis reveals that design and implementation issues are frequently overlooked; although not directly associated with vulnerabilities, these issues can impact the equitable distribution of project benefits. Furthermore, our analysis of 104 projects’ tokenomics configurations, including 15 collected from DeFi platforms, uncovered 27 inconsistent configurations, with only two projects exhibiting no issues. This suggests that such issues are relatively common. We therefore advise project teams to ensure consistency between their tokenomics design and the actual code. Our study culminates in providing several key practical implications for various DeFi stakeholders, including developers, users, researchers, and regulators, aiming to deepen the understanding of DeFi governance issues and contribute to the robust growth of DeFi systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407506284",
    "type": "article"
  },
  {
    "title": "<scp>SnapCC</scp> : Effective File System Consistency Testing Using Systematic State Exploration",
    "doi": "https://doi.org/10.1145/3718738",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Jianzhong Liu; Yuheng Shen; Yiru Xu; Hao Sun; Yu Jiang",
    "corresponding_authors": "",
    "abstract": "Modern file systems have become increasingly feature-rich and highly complex, making crash consistency increasingly difficult to perform correctly. Thoroughly testing file systems for crash consistency bugs, however, is difficult to achieve good results due to insufficient state exploration, a lack of guidance for test case generation, and missing support for modern file system features. In this paper, we present a new approach towards testing file system consistency: systematic file system persistent state exploration. In contrast to previous efforts, our design addresses these shortcomings through testing the crash consistency property of file systems systematically using the following procedures. Initially, we use system call generation and execution feedback from fuzzers to generate workloads that stress the file system code. During this process, we systematically explore all possible persistent states of the underlying file system for the given workload, and subsequently use them as file system image inputs for the crash recovery routines to produce a corresponding file system state. After the file system finishes processing an image input, we deploy an efficient file system checker to compare the contents of the image to that of a correct image and determine whether the image is inconsistent, consequently determining whether we has triggered a crash consistency bug in the underlying file system. We implement a prototype tool SnapCC and deployed it for testing multiple mainstream file systems on Linux. We compared its effectiveness along with other relevant tools Hydra and B3, where our results show that SnapCC achieves \\(16\\%\\) to \\(44\\%\\) better coverage over Hydra, and finds \\(15\\) new consistency bugs, whereas B3 and Hydra finds \\(2\\) and \\(6\\) , over a period of 2 weeks, further demonstrating SnapCC ’s effectiveness in discovering file system consistency bugs. To demonstrate our approach’s adaptability, we also tested SnapCC on 5 other file systems, upon which 7 additional bugs were found.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407765830",
    "type": "article"
  },
  {
    "title": "Toward Understanding FPGA Synthesis Tool Bugs",
    "doi": "https://doi.org/10.1145/3718737",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Yi Zhang; He Jiang; Shikai Guo; Xiaochen Li; Hui Liu; Chongyang Shi",
    "corresponding_authors": "",
    "abstract": "FPGA (Field Programmable Gate Array) synthesis tools are crucial for hardware development and AI acceleration, and their bugs could compromise hardware reliability and risk downstream applications. However, it remains unknown in understanding the characteristics of these bugs. What are the root causes that trigger bugs in FPGA synthesis tools? What are the characteristics of these bugs? What are the challenges in detecting and addressing them? This paper takes the first step towards answering these questions by conducting a comprehensive study of FPGA synthesis tool bugs. We analyze 551 confirmed bugs in both commercial and open source FPGA synthesis tools, i.e., Vivado, Quartus Prime, and Yosys, covering root causes, symptoms, bug-prone components, fix characteristics, and achieve 17 valuable findings. We find that, on average, around 46.2% of bugs result from HDL (Hardware Description Language) standard noncompliance across the three tools. However, it is hard for current formal validations to fully test HDL standards compliance. Additionally, on average over 25.8% bugs show domain-specific optimization traits due to inappropriate optimization and mapping. Meanwhile, beyond 28% of bugs trigger unexpected behavior without clear signs, making the formulation of effective test oracles challenging. These findings help addressing FPGA synthesis tool bugs and guide further research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407766179",
    "type": "article"
  },
  {
    "title": "Not All Exceptions Are Created Equal: Triaging Error Logs in Real World Enterprises",
    "doi": "https://doi.org/10.1145/3721126",
    "publication_date": "2025-02-28",
    "publication_year": 2025,
    "authors": "Junlin Liu; Mengyu Yao; Shaofei Li; Dingyu Yang; Zheshun Wu; Xinxin Qu; Ziqi Zhang; Ding Li; Yao Guo; Xiangqun Chen",
    "corresponding_authors": "",
    "abstract": "Error logs like Java exceptions play a crucial role in diagnosing and resolving errors within the industry. Nonetheless, the extensive logging of Java exceptions may result in exception fatigue in large-scale Java systems at an industrial level, where the frequency of Java exceptions being generated surpasses developers’ ability to manage them effectively. Regrettably, there is a lack of research on the seriousness, prevalence, and solutions to this problem. To close this gap, we first make a comprehensive investigation into the exception fatigue problem within a prominent Internet corporation in China, namely Alibaba, confirming its importance in the industry. Consequently, we introduce a novel solution called ABEL, designed to automatically pinpoint the most relevant exceptions associated with software failures. The key challenge lies in the randomness of exceptions, which prevents existing sequence-based techniques from being effective. To address this challenge, ABEL establishes correlations between Java exceptions and the Key Performance Indicator (KPI) of applications, enabling the identification of exceptions leading to irregularities in KPI. Our evaluation of ABEL across four Java applications and five business KPIs within Alibaba illustrates its capability to pinpoint the primary cause of exception logs with a AC@5 (top-5 accuracy) exceeding 90%, effectively mitigating the exception fatigue problem within Alibaba. Furthermore, it can identify the root-cause exceptions in a real software failure within just four minutes, outperforming the manual investigation process by over an hour.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408053971",
    "type": "article"
  },
  {
    "title": "Better Supporting Human Aspects in Mobile eHealth Apps: Development and Validation of Enhanced Guidelines",
    "doi": "https://doi.org/10.1145/3721429",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Md. Shamsujjoha; John Grundy; Qinghua Lu; Hourieh Khalajzadeh; Li Li",
    "corresponding_authors": "",
    "abstract": "eHealth apps are mobile apps that help in self-management of critical illnesses, provide home-based disease management, and assist with personalized care through education, sensing, and interaction. Users of eHealth apps are naturally very diverse in terms of their human aspects, e.g., their emotional reactions to the apps, varying language proficiency, socioeconomic status, educational level, cognitive style, physical and mental challenges, gender, age, personality, etc. Unfortunately, many eHealth apps do not take these user differences sufficiently into account, making them ineffective or even unusable. This paper presents our enhanced and actionable guidelines developed to better support human aspects in mobile eHealth apps. Some of these guidelines are specific, such as collecting minimal personal data or requirements, while others are more generic, applicable specifically to eHealth apps. We discuss how key human aspects, such as usability, accessibility, reliability, and validity, as well as diverse user issues can be addressed in practice with real-life eHealth app examples. We then collected feedback from expert mobile app developers, software engineers, and other relevant eHealth app stakeholders to assess the usefulness and applicability of the proposed guidelines and to identify areas where further refinement and development are needed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408152959",
    "type": "article"
  },
  {
    "title": "Enhancing Log Sentiments: An Exploratory Study of Sentiments and Emotions with Software Logs",
    "doi": "https://doi.org/10.1145/3723355",
    "publication_date": "2025-03-14",
    "publication_year": 2025,
    "authors": "Xiaohui Wang; Youshuai Tan; Zishuo Ding; Jinfu Chen; Jifeng Xuan; Weiyi Shang",
    "corresponding_authors": "",
    "abstract": "Software logs serve as valuable resources for understanding system running and are extensively used in diverse software maintenance tasks. Logs are generated by logging statements in the code, which are written by developers. Therefore, logs may reflect developers’ sentiments about the described situations. Consequently, when developers and system administrators read logs, the sentiments embedded in logs may influence their understanding. Although the sentiments associated with logs can convey valuable information, such information is not leveraged in research and practice. Previous research has primarily relied on verbosity levels of logs to gauge sentiments, which does not really capture the sentiments and emotions perceived by humans. To bridge this gap, in this paper, we first conduct an exploratory study to investigate sentiments and emotions that are communicated within logs. Our study encompasses five anomaly log datasets from LogHub and a dataset involving eight open-source Apache Java projects. We find that 8% of the logs express sentiments and emotions though developers are suggested to write them in an objective way. While most log messages might not explicitly express sentiments and emotions, they can still implicitly evoke sentiments and emotions in those who read them. Therefore, we exploit issue reports referencing logs to capture such sentiments and emotions. In these issue reports, 47.5% exhibit emotions, with 54.7% of those emotions being related to logs and 8.1% directly addressing logs. Furthermore, we demonstrate the potential of leveraging sentiment analysis to complement verbosity levels in logs, showcasing how sentiment information can offer novel insights and enhance log analysis. Specifically, by applying automatic tools, we identify 41 issue reports (9.8% on average) with negative sentiment and 55 reports (13.2% on average) with negative emotions, all referencing INFO or DEBUG logs (i.e., low severity). After manually verifying and filtering exception logs, we uncover three main concerns from 22 critical instances.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408473356",
    "type": "article"
  },
  {
    "title": "An empirical study on vulnerability disclosure management of open source software systems",
    "doi": "https://doi.org/10.1145/3716822",
    "publication_date": "2025-03-24",
    "publication_year": 2025,
    "authors": "S.-S. Liu; Jiayuan Zhou; Xing Hu; Filipe R. Cogo; Xin Xia; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Vulnerability disclosure is critical for ensuring the security and reliability of open source software (OSS). However, in practice, many vulnerabilities are reported and discussed on public platforms before being formally disclosed, posing significant risks to vulnerability management. Inadequate vulnerability disclosure can expose users to security threats and severely impact the stability and reliability of software systems. For example, prior work shows that over 21% of CVEs are publicly discussed before a patch is released. Despite its importance, we still lack clarity on the vulnerability disclosure practices adopted by open source communities and the preferences of practitioners regarding vulnerability management. To fill this gap, we analyzed the vulnerability disclosure practices of 8,073 OSS projects spanning from 2017 to 2023. We then conducted an empirical study by surveying practitioners about their preferences and recommendations in vulnerability disclosure management. Finally, we compared the survey results with the actual vulnerability practice observed within the OSS projects. Our results show that while over 80% of practitioners support Coordinated Vulnerability Disclosure (CVD), only 55% of vulnerabilities conform to CVD in practice. Although only 20% of practitioners advocate discussions before disclosure, 42% of vulnerabilities are discussed in issue reports before their disclosure. This study reveals the vulnerability management practices in OSS, provides valuable guidance to OSS owners, and highlights potential directions to improve the security of OSS platforms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408797770",
    "type": "article"
  },
  {
    "title": "GUI Test Migration via Abstraction and Concretization",
    "doi": "https://doi.org/10.1145/3726525",
    "publication_date": "2025-04-16",
    "publication_year": 2025,
    "authors": "Yakun Zhang; C. Liu; Xiaofei Xie; Yun Lin; Jin Song Dong; Dan Hao; Lu Zhang",
    "corresponding_authors": "",
    "abstract": "GUI test migration aims to produce test cases with events and assertions to test specific functionalities of a target app. Existing migration approaches typically focus on the widget-mapping paradigm that maps widgets from source apps to target apps. However, since different apps may implement the same functionality in different ways, direct mapping may result in incomplete or buggy test cases, thus significantly impacting the effectiveness of testing the target functionality and the practical applicability of migration approaches. In this paper, we propose a new migration paradigm (i.e., the abstraction-concretization paradigm) that first abstracts the test logic for the target functionality and then utilizes this logic to generate the concrete GUI test case. Furthermore, we introduce MACdroid , the first approach that migrates GUI test cases based on this paradigm. Specifically, we propose an abstraction technique that utilizes source test cases from source apps targeting the same functionality to extract a general test logic for that functionality. Then, we propose a concretization technique that utilizes the general test logic to guide an LLM in generating the corresponding GUI test case (including events and assertions) for the target app. We evaluate MACdroid on two widely-used datasets (including 31 apps, 34 functionalities, and 123 test cases). On the FrUITeR dataset, the test cases generated by MACdroid successfully test 64% of the target functionalities, improving the baselines by 191%. On the Lin dataset, MACdroid successfully tests 75% of the target functionalities, outperforming the baselines by 42%. These results underscore the effectiveness of MACdroid in GUI test migration.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409486109",
    "type": "article"
  },
  {
    "title": "Less is More: Feature Engineering for Fairness and Performance of Machine Learning Software",
    "doi": "https://doi.org/10.1145/3730577",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Linghan Meng; Yanhui Li; Lin Chen; Mingliang Ma; Yuming Zhou; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "Machine learning (ML) software employs statistical algorithms to perform high-stake tasks in our daily lives, whose results are usually discriminatory due to protected features (e.g., gender), i.e., one part (called privileged, e.g., male) may be more likely to obtain beneficial decisions than the other part (called unprivileged, e.g., female). In alleviating the unfairness, developers have obtained widely-held beliefs about the trade-off between performance and fairness for ML software. Surprisingly, recent research on feature engineering suggests that enlarging the feature set is the perfect way to kill two birds with one stone, i.e., achieving both higher performance and fairness. However, the experiments used in the prior study did not remove the effect of protected features, which have been suggested to be excluded in both industrial applications and academic studies. As a result, the study did not fully explore the trade-off between performance and fairness. In this paper, we first conduct an empirical study to replicate this prior study after excluding the protected features and observe that there is still a trade-off between performance and fairness with enlarging the features, i.e., more features are not perfect, which would lead to higher performance and lower fairness. Due to more features causing more collection and preprocessing budgets, we aim to search for an effective alternative. Inspired by the “less is more” principle, we propose a novel feature ranking method, H ybrid-importance and E arly-validation based F eature R anking ( HEFR ), to find an efficient subset to replace the full feature set with comparable performance and fairness. Our method, HEFR, employs hybrid feature importances to combine performance and fairness and conducts early validation to check the effectiveness of hybrid importances. We conduct experiments on seven datasets and three classifiers to evaluate our method with five baselines. The results have shown that (a) HEFR is efficient for ML software feature engineering: applying HEFR to choose about 10% of features would construct ML software with better or comparable performance and fairness, and (b) HEFR is actionable with small dataset sizes: applying HEFR with only 10% data size would still help choose the proper feature subset.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409584081",
    "type": "article"
  },
  {
    "title": "Handling Web Service Interactions in Fuzzing with Search-Based Mock-Generation",
    "doi": "https://doi.org/10.1145/3731558",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "Susruthan Seran; Man Zhang; Onur Duman; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "Testing large and complex enterprise software systems can be a challenging task. This is especially the case when the functionality of the system depends on interactions with other external services over a network (e.g., external web services accessed through REST API calls). Although several techniques in the research literature have been shown to be effective at generating test cases in a good number of different software testing contexts, dealing with external services is still a major research challenge. In industry, a common approach is to mock external web services for testing purposes. However, generating and configuring mock web services can be a very time-consuming task, e.g., external services may not be under the control of the same developers of the tested application, making it challenging to identify the external services and simulate various possible responses. In this paper, we present a novel search-based approach aimed at fully automated mocking of external web services as part of white-box, search-based fuzzing. We rely on code instrumentation to detect all interactions with external services, and how their response data is parsed. We then use such information to enhance a search-based approach for fuzzing. The tested application is automatically modified (by manipulating DNS lookups) to rather interact with instances of mock web servers. The search process not only generates inputs to the tested applications but also automatically configures responses in those mock web server instances, aiming at maximizing code coverage and fault-finding. An empirical study on four open-source REST APIs from EMB, and one industrial API from an industry partner, shows the effectiveness of our novel techniques (i.e., in terms of line coverage and fault detection).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409730455",
    "type": "article"
  },
  {
    "title": "WizardMerge - Save Us From Merging Without Any Clues",
    "doi": "https://doi.org/10.1145/3731751",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Qingyu Zhang; Junzhe Li; Jiayi Lin; Jie Ding; Lanteng Lin; Chenxiong Qian",
    "corresponding_authors": "",
    "abstract": "Modern software development necessitates efficient version-oriented collaboration among developers. While Git is the most popular version control system, it generates unsatisfactory version merging results due to textual-based workflow, leading to potentially unexpected results in the merged version of the project. Although numerous merging tools have been proposed for improving merge results, developers remain struggling to resolve the conflicts and fix incorrectly modified code without clues. We present WizardMerge , an auxiliary tool that leverages merging results from Git to retrieve code block dependency on text and LLVM-IR level and provide suggestions for developers to resolve errors introduced by textual merging. Rather than directly resolving these errors, the suggestions provide pigeonholed code blocks with their relevance and prioritized order within each category. To this end, developers can address the specific locations of these issues without manually analyzing their dependencies, thereby reducing the time and effort spent on conflict resolution tasks. Through the evaluation, we subjected WizardMerge to testing on 227 conflicts within five large-scale projects. The outcomes demonstrate that WizardMerge diminishes conflict merging time costs, achieving a 23.85% reduction. Beyond addressing conflicts, WizardMerge provides useful code block classifications and resolving orders for over 70% of the code blocks potentially affected by the conflicts. Notably, WizardMerge exhibits the capability to identify conflict-unrelated code blocks that require manual intervention yet are harmfully applied by Git during the merging.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409797717",
    "type": "article"
  },
  {
    "title": "Learning-Guided Fuzzing for Testing Stateful SDN Controllers",
    "doi": "https://doi.org/10.1145/3733717",
    "publication_date": "2025-05-02",
    "publication_year": 2025,
    "authors": "Raphaël Ollando; Seung Yeob Shin; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Controllers for software-defined networks (SDNs) are centralised software components that enable advanced network functionalities, such as dynamic traffic engineering and network virtualisation. However, these functionalities increase the complexity of SDN controllers, making thorough testing crucial. SDN controllers are stateful, interacting with multiple network devices through sequences of control messages. Identifying stateful failures in an SDN controller is challenging due to the infinite possible sequences of control messages, which result in an unbounded number of stateful interactions between the controller and network devices. In this article, we propose SeqFuzzSDN, a learning-guided fuzzing method for testing stateful SDN controllers. SeqFuzzSDN aims to (1) efficiently explore the state space of the SDN controller under test, (2) generate effective and diverse tests (i.e., control message sequences) to uncover failures, and (3) infer accurate failure-inducing models that characterise the message sequences leading to failures. In addition, we compare SeqFuzzSDN with three extensions of state-of-the-art (SOTA) methods for fuzzing SDNs. Our findings show that, compared to the extended SOTA methods, SeqFuzzSDN (1) generates more diverse message sequences that lead to failures within the same time budget, and (2) produces more accurate failure-inducing models, significantly outperforming the other extended SOTA methods in terms of sensitivity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410028485",
    "type": "article"
  },
  {
    "title": "Advanced Side-Channel Evaluation Using Contextual Deep Learning-Based Leakage Modeling",
    "doi": "https://doi.org/10.1145/3734219",
    "publication_date": "2025-05-05",
    "publication_year": 2025,
    "authors": "Saleh Alabdulwahab; J.S. Kim; Young-Tak Kim; Yunsik Son",
    "corresponding_authors": "",
    "abstract": "Side-channel attacks (SCA) exploit power analysis to extract secret information. Researchers have employed this technique to disassemble software and retrieve cryptographic keys by examining power consumption or electromagnetic emissions. They utilized hardware or Hamming-based fluctuations measurement to profile or model the power leakage. Developers employ power modeling to comprehend software leakage, although manually profiling the power trace across various devices and architectures requires time and effort. This work proposes a custom deep learning (DL) method to model the power trace. The DL model was trained to analyze how each assembly instruction produces leakage based on its context with other instructions. The proposed method can predict the power trace with 0.9963 R² from unseen assembly instructions. This method automates device leakage testing and captures contextual and non-linear relationships to help developers understand the software behavior, significantly reducing the time and effort required for power modeling. The potential impact of this DL model on software security is that it can effectively mitigate the risk of side-channel attacks, thus enhancing the overall security of software systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410088714",
    "type": "article"
  },
  {
    "title": "FuMi: A Runtime Fuzz-based Machine Learning Precision Measurement and Testing Framework",
    "doi": "https://doi.org/10.1145/3734866",
    "publication_date": "2025-05-08",
    "publication_year": 2025,
    "authors": "Peng Zhang; Mike Papadakis; Yuming Zhou",
    "corresponding_authors": "",
    "abstract": "The rapid evolution of machine learning model training has outpaced the development of corresponding measurement and testing tools, leading to two significant challenges. Firstly, developers of deep learning frameworks struggle to identify operators that fail to meet precision criteria, as these issues may only manifest in a few data points. Secondly, model trainers lack effective methods to estimate precision loss caused by operators during training. To address these issues, we introduce a Pythonic framework inspired by common network layer definitions in deep learning. Our framework includes two new layers: the Fuzz Layer and the Check Layer, designed to aid in measurement and testing. The Fuzz Layer introduces minor perturbations to tensor inputs for any deterministic layer under testing (LUT). The Check Layer then measures precision by analyzing the differences before and after the perturbation. This approach estimates a lower bound of the maximal relative error and alerts developers or trainers of potential bugs if the difference exceeds a predefined tolerance. Additionally, Check Layers can be used independently to conduct precision tests for specific layers, ensuring the precision of operators during runtime. Despite the additional memory and time requirements, this runtime testing ensures proper training of the original model. We demonstrate the utility of our framework, FuMi, through two experiments. First, we tested 21 torch operators across 9 popular machine learning models using PyTorch for various tasks, finding that the conv2d and linear operators often fail to meet precision requirements. Second, to showcase the generalizability of our framework, we tested the ATTENTION operator. By comparing different implementations of state-of-the-art ATTENTION operators, we found that the maximum relative error of the ATTENTION operator is not less than 1%, which is 13 times larger than that measured by Predoo (a unit test tool). This framework provides a robust solution for identifying precision issues in deep learning models, ensuring reliable and accurate model training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410192134",
    "type": "article"
  },
  {
    "title": "Not All Paths Are Equal: Multi-path Optimization for Directed Hybrid Fuzzing",
    "doi": "https://doi.org/10.1145/3735555",
    "publication_date": "2025-05-13",
    "publication_year": 2025,
    "authors": "Peihong Lin; Pengfei Wang; Xu Zhou; Wei Xie; Gen Zhang; Kai Lu",
    "corresponding_authors": "",
    "abstract": "Directed grey-box fuzzing (DGF) can improve bug exposure efficiency by stressing bug-prone areas. Recent studies have modeled DGF as the problem of finding and optimizing paths to reach target sites. However, they still face the “multi-path” challenge. When a target site is reachable by multiple paths, it is crucial to comprehensively evaluate and effectively select these paths, as this affects the fuzzer’s choice between reaching target sites via optimal paths and enhancing path diversity toward targets to expose hidden bugs in non-optimal paths. In this paper, we propose MultiGo, a directed hybrid fuzzer designed for multi-path optimization. First, we propose a new fitness metric called path difficulty to comprehensively evaluate the promising paths. This metric uses the Poisson distribution to estimate the probability of exploring basic blocks along execution paths based on statistical block frequency, distinguishing between optimal and challenging paths. With path difficulty as a key factor, a customized Contextual Multi-Armed Bandit (CMAB) model is employed to efficiently optimize path scheduling by comprehensively considering the impact of testing conditions on path scheduling. We introduce the concept of the fuzzing context to represent and evaluate testing conditions, which encompass factors such as path characteristics (e.g., path difficulty), the testing agent (e.g., fuzzing or symbolic execution), and the testing goal (e.g., path exploitation or exploration). Then, the CMAB model predicts the expected rewards for scheduling paths under different testing agents and goals, thereby optimizing path scheduling. By leveraging the CMAB model, MultiGo enhances DGF’s capability to explore easier paths and symbolic execution’s capacity to handle more complex ones, enabling efficient target reaching through optimal paths while ensuring sufficient coverage of non-optimal paths. MultiGo is evaluated on 136 target sites of 41 real-world programs from 3 benchmarks. The experimental results show that MultiGo outperforms the state-of-the-art directed fuzzers (AFLGo, SelectFuzz, Beacon, WindRanger, and DAFL) and hybrid fuzzers (SymCC and SymGo) in reaching target sites and exposing known vulnerabilities. Moreover, MultiGo also discovered 14 undisclosed vulnerabilities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410343894",
    "type": "article"
  },
  {
    "title": "Judge: Effective State Abstraction for Guiding Automated Web GUI Testing",
    "doi": "https://doi.org/10.1145/3736162",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "Chenxu Liu; Junheng Wang; Wei Yang; Ying Zhang; Tao Xie",
    "corresponding_authors": "",
    "abstract": "Automated web GUI testing approaches aim to maximize the code coverage of a web app within a specific time budget. However, due to the highly dynamic characteristics of web apps, testing approaches often get stuck in loops or repeatedly explore the same app areas. To address this issue, existing approaches conduct state abstraction, grouping similar pages into the same state in an effort to approximate the ideal state (i.e., a state that encompasses all-and-only those pages exhibiting the same behavior from a testing perspective) to reduce repetitive explorations. Typically, these approaches rely on the Document Object Model (DOM) or visual similarity, using predefined thresholds or learning-based classifiers to determine which pages should belong to the same state. However, pages within the same ideal state still exhibit discrepancies, caused by factors such as dynamically loaded data and dynamically expanded UI elements. The varying page complexities and design styles among apps bring even more challenges. These phenomena present substantial obstacles to existing approaches in determining desirable classification thresholds or training desirable classifiers, preventing them from conducting satisfactory state abstraction to guide the testing process. To address the preceding challenges, in this article, we propose Judge, a novel approach based on structure merging and contrastive learning for state abstraction. Judge includes a “merge-and-classify” strategy. In the “merge” phase, Judge iterates through the DOM tree of each given page and merges web element siblings that share the same subtree structure into a single one to abstract and simplify the page, while discarding text contents and HTML attributes of web elements in the process. In this way, Judge mitigates the negative effects introduced by dynamically loaded data and dynamically expanded UI elements, substantially reducing discrepancies between pages in the same ideal state. In the “classify” phase, Judge uses a dedicated contrastive learning model to embed simplified page DOMs into vectors and further conducts classification with a Support Vector Machine (SVM), enabling classification in high-dimensional vector space and improving generalizability across diverse web apps. We evaluate Judge against 13 widely used baseline approaches. The results highlight that Judge outperforms these baseline approaches in classifying page pairs, with an average margin ranging from 8.95% to 28.90% in the F1 score across three manually labeled datasets. Additionally, when compared to the five most effective baseline approaches, Judge demonstrates superiority in guiding the exploration of automated web GUI testing in six widely studied apps, with code coverage improved by an average of 2.62% to 14.12%. The code and data of Judge are publicly accessible.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410430711",
    "type": "article"
  },
  {
    "title": "A Survey on Failure Analysis and Fault Injection in AI Systems",
    "doi": "https://doi.org/10.1145/3732777",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "Guangba Yu; G. Tan; Haojia Huang; Zhenyu Zhang; Pengfei Chen; Roberto Natella; Zibin Zheng; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "The rapid advancement of Artificial Intelligence (AI) has led to its integration into various areas, especially with Large Language Models (LLMs) significantly enhancing capabilities in Artificial Intelligence Generated Content (AIGC). However, the complexity of AI systems has also exposed their vulnerabilities, necessitating robust methods for failure analysis (FA) and fault injection (FI) to ensure resilience and reliability. Despite the importance of these techniques, there lacks a comprehensive review of FA and FI methodologies in AI systems. This study fills this gap by presenting a detailed survey of existing FA and FI approaches across six layers of AI systems. We systematically analyze 142 studies to answer three research questions including (1) what are the prevalent failures in AI systems, (2) what types of faults can current FI tools simulate, (3) what gaps exist between the simulated faults and real-world failures. Our findings reveal a taxonomy of AI system failures, assess the capabilities of existing FI tools, and highlight discrepancies between real-world and simulated failures. Moreover, this survey contributes to the field by providing a framework for fault diagnosis, evaluating the state-of-the-art in FI, and identifying areas for improvement in FI techniques to enhance the resilience of AI systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410431126",
    "type": "article"
  },
  {
    "title": "OptRCA: A More Efficient and Accurate Approach for Automated Root Cause Analysis and Explanation",
    "doi": "https://doi.org/10.1145/3736718",
    "publication_date": "2025-05-23",
    "publication_year": 2025,
    "authors": "Jingquan Ge; Yaowen Zheng; Yuekang Li; Wei Ma; Sheikh Mahbub Habib; Praveen Kakkolangara; Gabriel Byman; Yang Liu",
    "corresponding_authors": "",
    "abstract": "With the development of automated software testing technology, software developers can get a large number of crash test cases in a short period of time. However, analyzing these crash test cases and finding their root cause is a time-consuming and labor-intensive task. Techniques based on reverse execution and backward taint analysis are proposed to locate the root cause, but can’t provide context information or explanation of the underlying fault. To address these two limitations, researchers have proposed an automated root cause analysis technique called AURORA. Although this technique provides powerful root cause analysis capabilities, it also have two obvious shortcomings. First, the results of root cause analysis are not accurate enough. Second, the efficiency of root cause analysis is not high enough. In order to improve these two shortcomings, we propose OptRCA, a more efficient and accurate approach for root cause analysis and explanation. Like AURORA's fuzzing strategy, OptRCA is also designed based on AFL's crash mode. The difference between them is mainly reflected in three points. First of all, the goal pursued by OptRCA is different from that of normal fuzzing technology. OptRCA pursues maximum correlation to ensure that as many crash test cases as possible are related to the same root cause. This test case with maximum correlation can greatly improve the accuracy of root cause analysis. Second, OptRCA proposed a more efficient non-crash test case retention strategy, which we named “Hill Climbing Retention”. Using the hill climbing retention method, OptRCA can obtain sufficient root cause information while retaining only a few non-crash test cases. Since the number of test cases is greatly reduced, the efficiency of OptRCA's subsequent root cause analysis process is also greatly improved. In addition, OptRCA also optimizes the analysis formula to obtain more accurate analysis results. In the evaluation experimental results, OptRCA is significantly better than AURORA in terms of accuracy and efficiency. Quantitative analysis shows that OptRCA is 65% more accurate and 61% more efficient than AURORA.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410642543",
    "type": "article"
  },
  {
    "title": "Visualization Task Taxonomy to Understand the Fuzzing Internals",
    "doi": "https://doi.org/10.1145/3718346",
    "publication_date": "2025-05-27",
    "publication_year": 2025,
    "authors": "Sriteja Kummita; Miao Miao; Eric Bodden; Shiyi Wei",
    "corresponding_authors": "",
    "abstract": "Greybox fuzzing is used extensively in research and practice. There are umpteen publications that improve greybox fuzzing. However, to what extent do these improvements affect the internal components or internals of a given fuzzer is not yet understood as the improvements are mostly evaluated using code coverage and bug finding capability. Such an evaluation is insufficient to understand the effect of improvements on the fuzzer internals. Some of the literature visualizes the outcomes of fuzzing to enhance the understanding. However, they only focus on high-level information and no previous research on visualization has been dedicated to understanding fuzzing internals. To close this gap, we propose the first step towards development of a fuzzing-specific visualization framework: a taxonomy of visualization analysis tasks that fuzzing experts desire to help them understand the fuzzing internals. Our approach involves conducting interviews with fuzzing experts and using qualitative data analysis to systematically extract the task taxonomy from the interview data. We also evaluate the support of existing fuzzing visualization tools through the lens of our taxonomy. In our study, we have conducted 33 interviews with fuzzing practitioners and extracted a taxonomy of 120 visualization analysis tasks. Our evaluation shows that the existing fuzzing visualization tools only provide aids to support 10 of them.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410772098",
    "type": "article"
  },
  {
    "title": "S <scp>pectre</scp> : Automated Aliasing Specifications Generation for Library APIs with Fuzzing",
    "doi": "https://doi.org/10.1145/3725811",
    "publication_date": "2025-06-02",
    "publication_year": 2025,
    "authors": "Shuangxiang Kan; Yuekang Li; Weigang He; Zhenchang Xing; Liming Zhu; Yulei Sui",
    "corresponding_authors": "",
    "abstract": "Static program analysis of real-world software that integrates numerous library Application Programming Interfaces (APIs) faces significant challenges due to inaccessible or highly complex source code. A common workaround is to use specifications that summarize the key behaviors of these APIs for analysis. However, manually writing specifications is labor-intensive and requires a deep understanding of API semantics while existing automated specification generation techniques struggle when source code is inaccessible or partially available. This paper introduces Spectre , an automated framework that leverages fuzzing techniques to generate aliasing specifications for library APIs. Spectre operates efficiently and precisely both with and without source code access. When source code is unavailable, Spectre integrates alias-check observers into the driver program after the API call site and performs black-box fuzzing to explore different API behaviors. If a check is satisfied, the corresponding aliasing specification is generated. When source code is available, Spectre incorporates new grey-box fuzzing features specifically tailored for aliasing specification inference, further enhancing its ability to generate aliasing specifications. We conducted extensive experiments to evaluate the performance of Spectre . Without source code access, Spectre demonstrated its specification generation capability across both Musl, a lightweight C standard library, and eight C third-party libraries. For Musl, Spectre recovered 96.7% of correct manually written specifications and identified 40.0% more aliasing specifications than those written by external experts. For C third-party libraries, all Spectre -generated aliasing specifications were validated as correct through static analysis of the API source code. Spectre is also more complete than other specification inference tool generating 16.7% more correct specifications for third-party libraries. The practicality of the generated specifications was confirmed, as they improved aliasing analysis in static pointer analysis of client code while maintaining a balance between accuracy and efficiency. The effectiveness of the tailored grey-box fuzzing features was demonstrated by Spectre identifying 20% more specifications compared to when these features were disabled. These results show that Spectre is an effective tool for inferring aliasing specifications and facilitating static analysis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410953428",
    "type": "article"
  },
  {
    "title": "Programming Smart Playtesting",
    "doi": "https://doi.org/10.1145/3742473",
    "publication_date": "2025-06-03",
    "publication_year": 2025,
    "authors": "I. S. W. B. Prasetya; Mehdi Dastani; Rui Prada; Tanja E. J. Vos; Frank Dignum; Fitsum Meshesha Kifetew; Guido Mintjes; Samira Shirzadehhajimahmood; Saba Gholizadeh Ansari",
    "corresponding_authors": "",
    "abstract": "Until recently the game industry heavily relied on manual playtesting to test the games it produces. Even if the benefits of introducing automated testing are acknowledged, it is rarely done in practice. Some of the main hurdles include the lack of automated testing tools that can target computer games as well as the complexity of automated game plays which are much more difficult to program than typical simple test sequences. This paper presents an agent-based testing framework called aplib that comes with a domain specific language (DSL) that allows complex playtests to be programmed more abstractly. A so-called goal structure is used to abstractly formulate a playtest scenario in terms of main goals and their decomposition into subgoals. Scenarios that are not too complicated can be formulated using static goal structures. More complex scenarios may need a test agent that can dynamically adapt its play according to the situation that evolves during the play. To handle such cases aplib allows dynamic goals to be expressed as well. Invariants and pre-/post-conditions are used to assert the properties that a play is expected to satisfy. They include differential properties that allow constraints on the current state to be related to that of past states. Three case studies are included in the paper. The first one aims to evaluate the performance of playtests programmed with aplib. The second shows that the approach can also be combined with other automated testing approaches, in this case reinforcement learning. The third shows the applicability of such playtests in a 3D setup and for non–functional testing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410993850",
    "type": "article"
  },
  {
    "title": "Enhanced Fairness Testing via Generating Effective Initial Individual Discriminatory Instances",
    "doi": "https://doi.org/10.1145/3737697",
    "publication_date": "2025-06-04",
    "publication_year": 2025,
    "authors": "Zhao Tian; Minghua Ma; Max Hort; Federica Sarro; Hongyu Zhang; Junjie Chen",
    "corresponding_authors": "",
    "abstract": "Fairness testing aims at mitigating unintended discrimination in the decision-making process of data-driven AI systems. Individual discrimination may occur when an AI model makes different decisions for two distinct individuals who are distinguishable solely according to protected attributes, such as age and race. Such instances reveal biased AI behaviour, and are called Individual Discriminatory Instances (IDIs). In this paper, we propose an approach for the selection of the initial seeds to generate IDIs for fairness testing. Previous studies mainly used random initial seeds to this end. However this phase is crucial, as these seeds are the basis of the follow-up IDIs generation. We dubbed our proposed seed selection approach I&amp;D . It generates a large number of initial IDIs exhibiting a great diversity, aiming at improving the overall performance of fairness testing. Our empirical study reveals that I&amp;D is able to produce a larger number of IDIs with respect to four state-of-the-art IDI generation approaches, generating 1.86X more IDIs on average. When using the IDIs generated with I&amp;D for retraining a machine learning model, the percentage of IDIs in the input space \\(\\mathbb{I}\\) is decreased by 24.9% on average, implying that I&amp;D is effective for improving the model’s fairness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411021291",
    "type": "article"
  },
  {
    "title": "Keeper: Automated Testing and Fixing of Machine Learning Software - RCR Report",
    "doi": "https://doi.org/10.1145/3737284",
    "publication_date": "2025-06-05",
    "publication_year": 2025,
    "authors": "Chengcheng Wan; Shicheng Liu; Sophie Xie; Yuhan Liu; Michael Maire; Henry Hoffmann; Shan Lu",
    "corresponding_authors": "",
    "abstract": "This artifact aims to provide source code, benchmark suite, results, and materials used in our study “Keeper: Automated Testing and Fixing of Machine Learning Software” [3]. We developed an automated testing and fixing tool Keeper and its IDE plugin for ML software. It automatically detects software defects and attempts to change how ML APIs are used to alleviate software misbehavior. This artifact provides guidelines to set up and execute Keeper, and also guidelines to interpret our evaluation results. We hope this artifact can motivate and help future research to further tackle ML API misuses. All related data are available online.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411059610",
    "type": "article"
  },
  {
    "title": "<scp>EvoTaint</scp> : Incremental Static Taint Analysis of Evolving Android Apps",
    "doi": "https://doi.org/10.1145/3743132",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Junhua Guo; Haipeng Cai",
    "corresponding_authors": "",
    "abstract": "In the last decade, Android applications have emerged as a primary interface in consumer technology. With approximately 2.5 billion mobile devices running Android globally, security threats to the Android ecosystem due to vulnerabilities in it become increasingly broadly consequential via user applications (i.e., Android apps). This necessitates efficient methods for defending them against those vulnerabilities. Taint analysis, a popular and fundamental security defense technique, assesses the flow of sensitive information within an app between sources (e.g., reading from user inputs) and sinks (e.g., writing to databases). However, traditional taint analysis is notably resource-intensive. Performing a comprehensive analysis on a single app given a complete list of potential sources and sinks can take hours, a situation exacerbated by the frequent updates typical in mobile app development. In this paper, we propose EvoTaint , an incremental taint analysis , tailored to fit and exploit the evolving nature of Android apps. It aims to substantially reduce the time cost of conventional static taint analysis against an evolved version of a given app by narrowing down the analysis scope from the entire app to only the parts that are changed or impacted by the changes in the evolved version. We have implemented EvoTaint as a practical, open-source tool and evaluated it on 100 Android apps each with 2, 3, or even 5 versions considered. Our results demonstrated a significant (51.8—68.9%) reduction in the time cost of static taint analysis of each of the 1—4 evolved versions on average, without compromising the accuracy of the analysis results (i.e., taint flow paths), compared to using the conventional approach treating each version as a separate/standalone app. Our further analysis aimed to clarify why and when EvoTaint performs favorably. It revealed that the time efficiency gains of incremental taint analysis are strongly correlated with the ratio of changed methods and the proportion of sources/sinks affected by these changes during app evolution.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411087843",
    "type": "article"
  },
  {
    "title": "Developer Perspectives on Licensing and Copyright Issues Arising from Generative AI for Software Development",
    "doi": "https://doi.org/10.1145/3743133",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Trevor Stalnaker; Nathan Wintersgill; Oscar Chaparro; Laura A. Heymann; Massimiliano Di Penta; Daniel M. Germán; Denys Poshyvanyk",
    "corresponding_authors": "",
    "abstract": "Despite the utility that Generative AI (GenAI) tools provide for tasks such as writing code, the use of these tools raises important legal questions and potential risks, particularly those associated with copyright law. As lawmakers and regulators respond to these questions, the views of users can offer relevant perspectives. In this paper, we provide: (1) a survey of 574 developers on the licensing and copyright aspects of GenAI for coding, as well as follow-up interviews; (2) a snapshot of developers’ views at a time when GenAI and perceptions of it were rapidly evolving; and (3) an analysis of developers’ perspectives, yielding insights and recommendations that can inform future regulatory decisions in this evolving field. Our results show the benefits developers derive from GenAI, how they view the use of AI-generated code as similar to using other existing code, the varied opinions they have on who should own or be compensated for such code, that they are concerned about data leakage via GenAI, and other findings, providing organizations and policymakers with valuable insights into how the technology is being used and the concerns that stakeholders believe warrant attention.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411087875",
    "type": "article"
  },
  {
    "title": "Simulation-based Safety Assessment of Vehicle Characteristics Variations in Autonomous Driving Systems",
    "doi": "https://doi.org/10.1145/3743673",
    "publication_date": "2025-06-09",
    "publication_year": 2025,
    "authors": "Qi Pan; Tiexin Wang; Jianwei Ma; Paolo Arcaini; Tao Yue",
    "corresponding_authors": "",
    "abstract": "Autonomous driving systems (ADSs) must be sufficiently tested to ensure their safety. Though various ADS testing methods have shown promising results, they are limited to a fixed vehicle characteristics setting (VCS). The impact of variations in vehicle characteristics (e.g., mass, tire friction) on the safety of ADSs has not been sufficiently and systematically studied. Such variations are often due to wear and tear, production errors, etc., which may lead to unexpected driving behaviours of ADSs. To this end, in this paper, we propose a method, named SafeVar , to systematically find minimum variations to the original vehicle characteristics setting, which affect the safety of the ADS deployed on the vehicle. To evaluate the effectiveness of SafeVar , we employed two ADSs and conducted experiments with two driving scenarios. Results show that SafeVar , equipped with NSGA-II, generates more critical settings that put the vehicle into unsafe situations, as compared with the baseline algorithm. We also identified critical vehicle characteristics and reported to which extent varying their settings put the ADS vehicle into unsafe situations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411154125",
    "type": "article"
  },
  {
    "title": "Ain’t No Stopping Us Monitoring Now",
    "doi": "https://doi.org/10.1145/3744241",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Luca Ciccone; Francesco Dagnino; Angelo Ferrando",
    "corresponding_authors": "",
    "abstract": "Not all properties are monitorable. This is a well-known fact, and it means there exist properties that cannot be fully verified at runtime. However, given a non-monitorable property, a monitor can still be synthesised, but it could end up in a state where no verdict will ever be concluded on the satisfaction (resp., violation) of the property. For this reason, non-monitorable properties are usually discarded. In this paper, we carry out an in-depth analysis on monitorability, and how non-monitorable properties can still be partially verified. We present our theoretical results at a semantic level, without focusing on a specific formalism. Then, we show how our theory can be applied to achieve partial runtime verification of linear time properties.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411176699",
    "type": "article"
  },
  {
    "title": "SimADFuzz: Simulation-Feedback Fuzz Testing for Autonomous Driving Systems",
    "doi": "https://doi.org/10.1145/3744242",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Huiwen Yang; Yu Zhou; Taolue Chen",
    "corresponding_authors": "",
    "abstract": "Autonomous driving systems (ADS) have achieved remarkable progress in recent years. However, ensuring their safety and reliability remains a critical challenge due to the complexity and uncertainty of driving scenarios. In this paper, we focus on simulation testing for ADS, where generating diverse and effective testing scenarios is a central task. Existing fuzz testing methods face limitations, such as overlooking the temporal and spatial dynamics of scenarios and failing to leverage simulation feedback ( e.g., speed, acceleration and heading) to guide scenario selection and mutation. To address these issues, we propose SimADFuzz , a novel framework designed to generate high-quality scenarios that reveal violations in ADS behavior. Specifically, SimADFuzz employs violation prediction models, which evaluate the likelihood of ADS violations, to optimize scenario selection. Moreover, SimADFuzz proposes distance-guided mutation strategies to enhance interactions among vehicles in offspring scenarios, thereby triggering more edge-case behaviors of vehicles. Comprehensive experiments demonstrate that SimADFuzz outperforms state-of-the-art fuzzers by identifying 73 more unique violations, including 5 reproducible cases of vehicle-vehicle, vehicle-pedestrian and vehicle-roadside collisions. These results demonstrate SimADFuzz 's effectiveness in enhancing the robustness and safety of autonomous driving systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411177448",
    "type": "article"
  },
  {
    "title": "What Properties Affect Boolean Formula Comprehension in Formal Specifications?",
    "doi": "https://doi.org/10.1145/3744557",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Ilia Shevrin; Shahar Maoz",
    "corresponding_authors": "",
    "abstract": "Writing formal specifications is an important yet challenging aspect of software engineering. Correct specifications facilitate verification efforts and reduce bugs. However, the declarative nature of specifications differs from the imperative approach of most common programming languages, and software engineers often perceive formal methods as difficult. Arguably, guidelines and tools for writing readable specifications should lower the barrier to formal methods adoption. In this work, we focus on Boolean formulas, a fundamental building block of specifications. Analogous to research on code comprehension, we conducted an experiment that attempts to identify what properties affect Boolean formula comprehension by software engineers. To this end, we collected 59 representative Boolean formulas and tested how various syntactic properties, such as negation symbol count and nesting level, affect comprehension task response times and correctness. Our experiment with 181 participants shows that eliminating negation symbols and decreasing operator count are among the most significant factors that improve comprehension. We use these empirical results to derive a reading complexity score and develop a fast regression-based refactoring algorithm for Boolean formulas. Finally, we conducted a follow-up experiment with 57 participants, which provided strong evidence for the algorithm's effectiveness in improving comprehension.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411236081",
    "type": "article"
  },
  {
    "title": "LLM-Cure: LLM-based Competitor User Review Analysis for Feature Enhancement",
    "doi": "https://doi.org/10.1145/3744644",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Maram Assi; Safwat Hassan; Ying Zou",
    "corresponding_authors": "",
    "abstract": "The exponential growth of the mobile app market underscores the importance of constant innovation and rapid response to user demands. As user satisfaction is paramount to the success of a mobile application (app), developers typically rely on user reviews, which represent user feedback that includes ratings and comments to identify areas for improvement. However, the sheer volume of user reviews poses challenges in manual analysis, necessitating automated approaches. Existing automated approaches either analyze only the target app's reviews, neglecting the comparison of similar features to competitors or fail to provide suggestions for feature enhancement. To address these gaps, we propose a Large Language Model (LLM) -based C ompetitive U ser R eview Analysis for Feature E nhancement) ( LLM-Cure ), an approach powered by LLMs to automatically generate suggestions for mobile app feature improvements. More specifically, LLM-Cure identifies and categorizes features within reviews by applying LLMs. When provided with a complaint in a user review, LLM-Cure curates highly rated (4 and 5 stars) reviews in competing apps related to the complaint and proposes potential improvements tailored to the target application. We evaluate LLM-Cure on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates that LLM-Cure significantly outperforms the state-of-the-art approaches in assigning features to reviews by up to 13% in F1-score, up to 16% in recall and up to 11% in precision. Additionally, LLM-Cure demonstrates its capability to provide suggestions for resolving user complaints. We verify the suggestions using the release notes that reflect the changes of features in the target mobile app. LLM-Cure achieves a promising average of 73% of the implementation of the provided suggestions, demonstrating its potential for competitive feature enhancement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411236083",
    "type": "article"
  },
  {
    "title": "On the Influence of the Baseline in Neuroimaging Experiments on Program Comprehension",
    "doi": "https://doi.org/10.1145/3744739",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Annabelle Bergum; Norman Peitek; Maurice Rekrut; Janet Siegmund; Sven Apel",
    "corresponding_authors": "",
    "abstract": "Background: Neuroimaging methods have been proved insightful in program-comprehension research. A key problem is that different baselines have been used in different experiments. A baseline is a task during which the “normal” brain activation is captured as a reference compared to the task of interest. Unfortunately, the influence of the choice of the baseline is still unclear. Aims: We investigate whether and to what extent the selected baseline influences the results of neuroimaging experiments on program comprehension. This helps to understand the trade-offs in baseline selection with the ultimate goal of making the baseline selection informed and transparent. Method: We have conducted a pre-registered program-comprehension study with 20 participants using multiple baselines (i.e., reading, calculations, problem solving, and cross-fixation). We monitored brain activation with a 64-channel electroencephalography (EEG) device. We compared how the different baselines affect the results regarding brain activation of program comprehension. Results and Implications: We found significant differences in mental load across baselines suggesting that selecting a suitable baseline is critical. Our results show that a standard problem-solving task, operationalized by the Raven-Progressive Matrices, is a well-suited default baseline for program-comprehension studies. Our results highlight the need for carefully designing and selecting a baseline in program-comprehension studies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411373993",
    "type": "article"
  },
  {
    "title": "Too Many Issues: Automatically Prioritizing Analyzer Findings by Tracing Security Importance",
    "doi": "https://doi.org/10.1145/3744708",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Sven Peldszus; Katharina Großer; Marco Konersmann; Wasja Brunotte; Maike Ahrens; Kurt Schneider; Jan Jürjens",
    "corresponding_authors": "",
    "abstract": "Code-based analyzers often find too many potentially security-related issues to address them all. Therefore, issues likely to lead to vulnerabilities should be fixed first. Such prioritization requires project-specific knowledge, such as quality requirements, security-related decisions, and design, which is not accessible to code analyzers. We present TraceSEC, an automated technique for prioritizing issues according to their security-related importance to the project. Its core concept is to incorporate available design artifacts and trace links between them, thus considering the project context that the code lacks. We reduce the problem of issue prioritization to a maximum flow problem and quantify the importance of each issue by the flow from user-defined quality aspects to the issue, i.e., quantifying its impact on project-specific security preferences. Our evaluation shows that TraceSEC effectively provides automated prioritization and can be tailored to project-specific quality goals. Its prioritization correlates stronger with manual expert prioritization than SonarQube rule severities, which are commonly used in practice. In particular, TraceSEC has a higher similarity for identifying high-priority issues. TraceSEC scales reasonably well for codebases up to 4 million lines of code, and the initial setup overhead is likely to be recouped after the first automated prioritization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411374011",
    "type": "article"
  },
  {
    "title": "Monitoring data for Anomaly Detection in Cloud-Based Systems: A Systematic Mapping Study",
    "doi": "https://doi.org/10.1145/3744556",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Adha Hrusto; Nauman bin Ali; Emelie Engström; Yuqing Wang",
    "corresponding_authors": "",
    "abstract": "Context: Anomaly detection is crucial for maintaining cloud-based software systems, as it enables early identification and resolution of unexpected failures. Given rapid and significant advances in the anomaly detection domain and the complexity of its industrial implementation, an overview of techniques that utilize real-world operational data is needed. Aim: This study aims to complement existing research with an extensive catalog of the techniques and monitoring data used for detecting anomalies affecting the performance or reliability of cloud-based software systems that have been developed and/or evaluated in a real-world context. Method: We perform a systematic mapping study to examine the literature on anomaly detection in cloud-based systems, particularly focusing on the usage of real-world monitoring data, with the aim of identifying key data categories, tools, data preprocessing, and anomaly detection techniques. Results: Based on a review of 104 papers, we categorize monitoring data by structure, types, and origins and the tools used for data collection and processing. We offer a comprehensive overview of data preprocessing and anomaly detection techniques mapped to different data categories. Our findings highlight practical challenges and considerations in applying these techniques in real-world cloud environments. Conclusion: The findings help practitioners and researchers identify relevant data categories and select appropriate data preprocessing and anomaly detection techniques for their specific operational environments, which is important for improving the reliability and performance of cloud-based systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411374065",
    "type": "article"
  },
  {
    "title": "FAVDisco: Modeling and Discovering File Access Vulnerabilities",
    "doi": "https://doi.org/10.1145/3744901",
    "publication_date": "2025-06-18",
    "publication_year": 2025,
    "authors": "Beibei Zhao; Wenjie Feng; Qingli Guo; Yingli Sun; Fangming Gu; Bolun Zhang; Xiaorui Gong; Hong Li",
    "corresponding_authors": "",
    "abstract": "File access vulnerabilities (FAVs) are one type of security weakness arising from adversary manipulations of file access inputs, posing significant threats to system integrity. Despite their prevalence, FAVs remain underexplored due to limited understanding, complex triggering scenarios, and stealthy and diverse manifestations; these challenges render current detection approaches incomplete and inaccurate. To this end, we conducted an in-depth empirical study across 204 file-related CVEs, uncovering the root cause and trigger mechanisms of FAVs. Based on these findings, we propose an exhaustive accessing model and a specialized threat model that define the Adversary and Attack Surface for FAVs, enabling systematic attribution and analysis of file operations. Furthermore, we propose FAVDisco , a novel framework for discovering FAVs by mutating, triggering, and analyzing file operations. It employs a File Mutator to simulate diverse execution scenarios and a FAV Checker that integrates a model-based adversary controllable checker with pattern-based detection rules to identify FAVs. Implemented on Windows, FAVDisco achieves remarkable performance with 92.1% precision and 83.3% recall on the disclosed FAV detection task, outperforming state-of-the-art methods. Moreover, it uncovers 13 zero-day FAVs in 10 widely-used services, with 6 assigned new CVEs and earning a reward of $29,000 from Microsoft Security Response Center.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411403434",
    "type": "article"
  },
  {
    "title": "Software Architecture Matters: Challenges and Opportunities for Android Upgrade Conflicts in Practice",
    "doi": "https://doi.org/10.1145/3744921",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Wuxia Jin; Mengjie Sun; Junhui Zhou; Jiaowei Shang; Zhenyu Huang; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Ever since its initial release in 2008, the Android operating system has rapidly grown to become the world’s most widely used mobile operating system. Mobile vendors extend the Android Open Source Project (AOSP) led by Google to customize their own Android variants. With the AOSP releasing new versions frequently, vendors need to periodically carry out Android upgrades that integrate the latest code changes from AOSP into their Android variants. Both the AOSP and Android variants independently undergo complex modifications. Consequently, Android upgrades often lead to merge conflicts caused by competing changes to the same code line. Vendors have devoted significant effort to understanding and resolving these problems. Despite extensive research on Android upgrades and merge conflicts, there is little understanding of the conflict-related activities performed in Android upgrade practice, and the corresponding challenges faced by practitioners . In this study, we employed a qualitative research methodology involving questionnaires with 120 practitioners and interviews with a leading Android vendor to explore the challenges and improvement opportunities. Our investigation demonstrates that Android upgrade process is fundamentally an exercise in architectural evolution, necessitating the adoption of architectural thinking rather than relying on mere code-level patches to systematically address upgrade-induced challenges. We have identified challenges at different stages of Android upgrade implementation, including baseline analysis, conflict reason analysis, conflict resolution, and conflict impact analysis. Our findings indicate opportunities for enhancing the Android upgrade practice, particularly in documentation, management, refactoring activities, and team collaboration. Additionally, we outline future research directions from an architectural perspective. We envision that our study can benefit software ecosystems where customized downstream derivatives need to maintain co-evolution with their upstream core.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411488278",
    "type": "article"
  },
  {
    "title": "POS Tagging on Code Identifiers: How Far Are We?",
    "doi": "https://doi.org/10.1145/3744919",
    "publication_date": "2025-06-23",
    "publication_year": 2025,
    "authors": "Hanlin Tang; Yanjie Jiang; Yuxia Zhang; Nan Niu; Hui Liu",
    "corresponding_authors": "",
    "abstract": "Part-of-speech (POS) tags are natural attributes of words in natural languages, and they are fundamental for natural language analysis. Many automated approaches have been proposed to tag natural language texts. Identifiers in source code have POS tags as well, which are useful for various source code analysis tasks, like code search, code comment generation, and code completion. Currently, state-of-the-art POS taggers originally designed for natural languages are often employed to tag source code identifiers. However, identifiers in source code are significantly different from natural languages. Consequently, POS taggers designed for natural languages could be less accurate in source code identifiers. Recently, several identifier-specific taggers have been proposed within the field of software engineering, but their adoption in practical software engineering tasks remains limited. This raises the question of why these taggers have not been more widely utilized in such tasks. In this paper, we investigate the performance of natural language POS taggers on source code identifiers, specifically method names, parameter names, and class names. To do so, we manually annotated identifiers from open-source projects in Java, C, and Python, creating a large dataset IDData for evaluation. We then evaluated six widely-used natural language POS taggers: NLTK, CoreNLP, OpenNLP, spaCy, Flair, and Stanza, alongside three identifier-specific taggers: SWUM, POSSE, and Ensemble Tagger. Our evaluation reveals that while natural language oriented POS taggers outperform identifier-specific taggers, their performance on identifiers is still significantly lower compared to their performance on natural language sentences. To understand the underlying reasons for this, we conducted an in-depth analysis, examining factors such as identifier length, POS distribution, syntactic structures, and special tags, which differentiate identifiers from natural language sentences. To further improve POS tagging performance on identifiers, we created a large-scale method name dataset MNTrain with manually labeled tags and retrained the natural language taggers on this new dataset. The results show substantial improvements in method name POS tagging performance, with taggers achieving performance comparable to their results on natural language sentences. Finally, we discuss the significance and practical implications of our findings, offering insights for future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411563192",
    "type": "article"
  },
  {
    "title": "Estimating Uncertainty in Line-Level Defect Prediction via Perceptual Borderline Oversampling",
    "doi": "https://doi.org/10.1145/3746225",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Chen Wu; Shikai Guo; Hui Li; Chenchen Li; Rong Chen",
    "corresponding_authors": "",
    "abstract": "Software defect prediction aims to identify potentially defective software modules using various techniques, while fine-grained line-level defect prediction can pinpoint defective lines of code. This helps developers promptly discover and fix errors, thereby enhancing the efficiency of testing and code review. However, previous studies often overlook the impact of characterizing noise and the skewed distribution of defect knowledge in software projects, making it difficult for current methods to achieve satisfactory accuracy and cost-effectiveness in software defect prediction. To address these challenges, we propose a model named EU-LLDP, which effectively resolves the issue of low cost-effectiveness in line-level defect prediction models. Specifically, the EU-LLDP model consists of two main components: the defect mining component mines the most valuable defect knowledge from numerous software defects using prediction probability matrices, noise labels, and the borderline information of code vectorizations. The adaptive resampling component samples valuable defect knowledge through the density distribution of defect knowledge, thereby making full use of existing defect knowledge and improving the cost-effectiveness of line-level software defect prediction models. Seven comprehensive experiments were conducted on 32 defect datasets from nine Java open source systems using file-level prediction models and line-level defect prediction models to evaluate the effectiveness of the EU-LLDP model. The EU-LLDP model improves the state-of-the-art file-level defect prediction model in terms of Balanced Accuracy by 9.87%, the MCC by 38.09%, and enhances the state-of-the-art line-level defect prediction method in terms of Recall@Top20%LOC by 44.16%, and Effort@Top20%Recall by 17.62%. These results fully demonstrate the effectiveness of EU-LLDP in improving the accuracy and cost-effectiveness of Software defect prediction.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411673854",
    "type": "article"
  },
  {
    "title": "Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey",
    "doi": "https://doi.org/10.1145/3748647",
    "publication_date": "2025-07-15",
    "publication_year": 2025,
    "authors": "Xinyu She; Yue Liu; Yanjie Zhao; Yiling He; Li Li; Chakkrit Tantithamthavorn; Zhan Qin; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "Modern language models (LMs) have been successfully employed in source code generation and understanding, leading to a significant increase in research focused on learning-based code intelligence, such as automated bug repair, and test case generation. Despite their great potential, language models for code intelligence (LM4Code) are susceptible to potential pitfalls , which hinder realistic performance and further impact their reliability and applicability in real-world deployment . Such challenges drive the need for a comprehensive understanding - not just identifying these issues but delving into their possible implications and existing solutions to build more reliable language models tailored to code intelligence. Based on a well-defined systematic research approach, we conducted an extensive literature review to uncover the pitfalls inherent in LM4Code. Finally, 121 primary studies from top-tier venues have been identified. After carefully examining these studies, we designed a taxonomy of pitfalls in LM4Code research and conducted a systematic study to summarize the issues, current solutions, implications, and challenges of different pitfalls for LM4Code systems. We developed a comprehensive classification scheme that dissects pitfalls across four crucial aspects: data collection and labeling, system design and learning, performance evaluation, and deployment and maintenance. Through this study, we aim to provide a roadmap for researchers and practitioners, facilitating their understanding and utilization of LM4Code in reliable and trustworthy ways.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412420766",
    "type": "article"
  },
  {
    "title": "Learning Program Behavioral Models from Synthesized Input-Output Pairs",
    "doi": "https://doi.org/10.1145/3748720",
    "publication_date": "2025-07-16",
    "publication_year": 2025,
    "authors": "Tural Mammadov; Dietrich Klakow; Alexander Koller; Andreas Zeller",
    "corresponding_authors": "",
    "abstract": "We introduce Modelizer —a novel framework that, given a black-box program, learns a model from its input/output behavior using neural machine translation algorithms. The resulting model mocks the original program: Given an input, the model predicts the output that would have been produced by the program. However, the model is also reversible —that is, the model can predict the input that would have produced a given output. Finally, the model is differentiable and can be efficiently restricted to predict only a certain aspect of the program behavior. Modelizer uses grammars to synthesize and inputs and unsupervised tokenizers to decompose the resulting outputs, allowing it to learn sequence-to-sequence associations between token streams. Other than input grammars, Modelizer only requires the ability to execute the program. The resulting models are small, requiring fewer than 6.3 million parameters for languages such as Markdown or HTML; and they are accurate, achieving up to 95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking real-world applications. As it learns from and predicts executions rather than code, Modelizer departs from the LLM-centric research trend, opening new opportunities for program-specific models that are fully tuned towards individual programs. Indeed, we foresee several applications of these models, especially as the output of the program can be any aspect of program behavior. Beyond mocking and predicting program behavior, the models can also synthesize inputs that are likely to produce a particular behavior, such as failures or coverage, thus assisting in program understanding and maintenance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412477643",
    "type": "article"
  },
  {
    "title": "AntiCopyPaster 3.0: Just-in-Time Clone Refactoring",
    "doi": "https://doi.org/10.1145/3749100",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Eman Abdullah AlOmar; Jacob Ashkenas; Robert Feliciano; Matthew Angelakos; Dimitrios Haralamppopoulos; Qian Xing; Mohamed Wiem Mkaouer; Ali Ouni",
    "corresponding_authors": "",
    "abstract": "Refactoring is a crucial practice in software maintenance that aims at improving design and coding practices while addressing design flaws. The Extract Method refactoring is particularly popular for consolidating duplicate code fragments into a single method. Various studies have explored ways to recommend Extract Method refactoring opportunities using techniques such as program slicing, program dependency graph analysis, change history analysis, structural similarity and feature extraction. Despite their effectiveness, these approaches often disrupt the developer’s workflow, requiring them to pause their coding and assess the refactoring opportunities suggested throughout the project, without considering the specific development context. To enhance the adoption of Extract Method refactoring, our previous work proposed A nti C opy P aster 2.0, and investigated the effectiveness of detecting and extracting code clones without disrupting the developer’s workflow. To address these limitations, we develop a new approach in this paper that supports the detection of Type-1 and Type-2 clones using the Program Structure Interface (PSI) and includes a custom-built Extract Method refactoring tool. We implement our approach using an IntelliJ IDEA extension plugin. Additionally, we integrated name recommendation models, including IntelliJ’s built-in recommender and Code2Vec, to enhance the quality of method names and improve developer productivity. To evaluate the accuracy and usefulness of our approach, we conduct a qualitative study involving 13 developers. The results indicate that (1) developers appreciate the approach and are satisfied with various aspects of the plugin’s functionality, (2) PSI effectively identifies clones by analyzing the structural and semantic aspects of the code, (3) IntelliJ’s naming recommender often provides default generic names, while code2vec produces descriptive and relevant names based on the code context, (4) the performance of A nti C opy P aster remains stable regardless of the file size and the number of clones present, (5) despite different detection and correction mechanisms, JDeodorant and A nti C opy P aster were able to perform method extraction, and A nti C opy P aster features just-in-time detection and correction, and (6) our results show an improvement in code quality after performing Extract Method refactoring with both refactoring tools. We envision that our A nti C opy P aster solution can streamline the Extract Method refactoring process, enhancing both developer efficiency and code quality by seamlessly integrating Type-2 clone detection and name recommendation capabilities in the development workflow.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412602677",
    "type": "article"
  },
  {
    "title": "Just-in-Time Detection of Silent Security Patches",
    "doi": "https://doi.org/10.1145/3749370",
    "publication_date": "2025-07-29",
    "publication_year": 2025,
    "authors": "Xunzhu Tang; Kisub Kim; Saad Ezzini; Yewei Song; Haoye Tian; Jacques Klein; Tegawendé F. Bissyandé",
    "corresponding_authors": "",
    "abstract": "Open-source code is pervasive. In this setting, embedded vulnerabilities are spreading to downstream software at an alarming rate. Although such vulnerabilities are generally identified and addressed rapidly, inconsistent maintenance policies can cause security patches to go unnoticed. Indeed, security patches can be silent, i.e., they do not always come with comprehensive advisories such as CVEs. This lack of transparency leaves users oblivious to available security updates, providing ample opportunity for attackers to exploit unpatched vulnerabilities. Consequently, identifying silent security patches just in time when they are released is essential for preventing n-day attacks and for ensuring robust and secure maintenance practices. With llmda we propose to (1) leverage large language models (LLMs) to augment patch information with generated code change explanations, (2) design a representation learning approach that explores code-text alignment methodologies for feature combination, (3) implement a label-wise training with labeled instructions for guiding the embedding based on security relevance, and (4) rely on a probabilistic batch contrastive learning mechanism for building a high-precision identifier of security patches. We evaluate llmda on the PatchDB and SPI-DB literature datasets and show that our approach substantially improves over the state-of-the-art, notably GraphSPD by 20% in terms of F-Measure on the SPI-DB benchmark.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412898693",
    "type": "article"
  },
  {
    "title": "Model Driven Engineering, Artificial Intelligence, and DevOps for Software and Systems Engineering: A Systematic Mapping Study of Synergies and Challenges",
    "doi": "https://doi.org/10.1145/3759454",
    "publication_date": "2025-08-08",
    "publication_year": 2025,
    "authors": "Luca Berardinelli; Vittoriano Muttillo; Romina Eramo; Hugo Brunelière; Abbas Rahimi; Antonio Cicchetti; Joan Giner-Miguelez; Abel Gómez; Pasqualina Potena; Mehrdad Saadatmand",
    "corresponding_authors": "",
    "abstract": "This paper presents a systematic mapping study classifying existing scientific contributions on synergies of Model Driven Engineering (MDE), Artificial Intelligence/Machine Learning (AI/ML), and DevOps, with the overall objective of supporting the continuous development of Cyber-Physical Systems (CPSs). We collected papers from bibliographic sources and selected primary studies to analyse. Then, we characterised and classified the current state of the art, focusing on 1) main aspects already tackled at the intersection of at least two of the three studied areas, and 2) findings emerging from the analysis as a framework for potential future research, notably regarding the integration of the three studied areas. The results reveal that few approaches combine MDE, AI/ML, and DevOps for software and systems engineering. In contrast, several approaches have combined two of them, specifically MDE and DevOps. Approaches combining AI/ML with MDE or DevOps are also becoming more frequent and will most likely continue to progress in the future. These synergies cover a range of engineering activities, from requirements and design to monitoring, maintenance, and evolution. Open research challenges include advancing AI/ML, MDE, and DevOps integration, supporting scalable, data-oriented solutions, proposing new continuous engineering methods, and adapting DevOps practices to diverse systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413243756",
    "type": "article"
  },
  {
    "title": "VulScribeR: Exploring RAG-based Vulnerability Augmentation with LLMs",
    "doi": "https://doi.org/10.1145/3760775",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Seyed Shayan Daneshvar; Yu Nong; Xu Yang; Shaowei Wang; Haipeng Cai",
    "corresponding_authors": "",
    "abstract": "Detecting vulnerabilities is vital for software security, yet deep learning-based vulnerability detectors (DLVD) face a data shortage, which limits their effectiveness. Data augmentation can potentially alleviate the data shortage, but augmenting vulnerable code is challenging and requires a generative solution that maintains vulnerability. Previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities. Recently, large language models (LLMs) have been used to solve various code generation and comprehension tasks with inspiring results, especially when fused with retrieval augmented generation (RAG). Therefore, we propose VulScribeR , a novel LLM-based solution that leverages carefully curated prompt templates to augment vulnerable datasets. More specifically, we explore three strategies to augment both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension. Our extensive evaluation across four vulnerability datasets and DLVD models, using three LLMs, show that our approach beats two SOTA methods Vulgen and VGX, and Random Oversampling (ROS) by 27.48%, 27.93%, and 15.41% in f1-score with 5K generated vulnerable samples on average, and 53.84%, 54.10%, 69.90%, and 40.93% with 15K generated vulnerable samples. Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413279387",
    "type": "article"
  },
  {
    "title": "Understanding the potentially confounding effect of test suite size in test effectiveness evaluation",
    "doi": "https://doi.org/10.1145/3748504",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Zeyu Lu; Yang Wang; Yi Rong; Yifan Huang; Xuan Wang; Peng Zhang; Shan Gao; Maolin Sun; Yibiao Yang; Yanhui Li; Lin Chen; Yuming Zhou",
    "corresponding_authors": "",
    "abstract": "Background: Code coverage and mutation score serve as pivotal test effectiveness metrics used to assess a test suite's ability to uncover actual defects. However, prior research has produced inconsistent or even conflicting findings regarding their correlation with defect detection capability, particularly concerning the impact of test suite size. Problem: The extent of the potentially confounding effect of test suite size in test effectiveness evaluation context is not clear, nor is the method to remove the potentially confounding effect, or the influence of this removal on the performance of test suite optimization. Objective: Our goal is to deeply understand how test suite size affects the true relationship between test effectiveness metrics and a test suite's ability to detect actual defects. Method: We first employ statistical methods to examine the extent of the potentially confounding effect of test suite size in the context of test effectiveness evaluation. After that, we propose a linear regression-based method to remove the potentially confounding effect of test suite size. Finally, we empirically explore the impact of this removal method on test suite optimization. Result: Our experimental results, based on the Defects4J defect dataset, uncovers that: (1) the confounding effect of test suite size on the associations between test effectiveness metrics and defect detection capability in general exists; (2) the proposed linear regression-based method can effectively remove the confounding effect; and (3) after removing the confounding effect, mutation score demonstrates superior effectiveness in predicting test suite effectiveness, while statement coverage is the least effective metric. Furthermore, both coverage-based and mutation-based test suite reduction exhibit enhanced cost-effectiveness in defect detection, and there is a marginal improvement in the speed of defect detection for coverage-based test case prioritization. Conclusion: When using test effectiveness metrics to assess test suite effectiveness, it is crucial to eliminate the influence of test suite size.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413788980",
    "type": "article"
  },
  {
    "title": "Vision to Specification: Automating the Transition from Conceptual Features to Functional Requirements",
    "doi": "https://doi.org/10.1145/3754450",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Xiaoli Lian; Wu Jian; Xiaoyun Gao; Shuaisong Wang; Li Zhang",
    "corresponding_authors": "",
    "abstract": "The translation of high-level abstract features into clear, and testable functional requirements (FRs) is a crucial step in software development, bridging the gap between user needs and technical specifications. In engineering practice, significant expert effort is needed for this translation. Our approach, EasyFR , streamlines the process by recommending Semantic Role Labeling (SRL) sequences for the given abstract features to guide Pre-trained Language Models (PLMs) in producing cohesive FR statements. By analyzing ten diverse datasets, we induce two variable SRL templates, each including two configurable parts. For concrete features, our proposed Key2Temp model can construct the appropriate variant of the SRL template by identifying a variable SRL template and placing the feature tokens in the appropriate slots. In this way, our approach reframes the process of requirement generation into a structured slot-filling activity. Experimental validation on four open datasets demonstrates that EasyFR outperforms three advanced Natural language generation (NLG) approaches, including GPT-4, particularly when existing FRs are available for training. The positive influence of our SRL template variant recommendations is further confirmed through an ablation study. We believe that our results indicate a notable step forward in the realm of automated requirements synthesis, holding potential to improve the process of requirements specification in future software projects.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413982506",
    "type": "article"
  },
  {
    "title": "LLM-based Crowdsourced Test Report Clustering",
    "doi": "https://doi.org/10.1145/3765756",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Yuchen Ling; Shengcheng Yu; Chunrong Fang; Quan Zhou; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "The openness of crowdsourced testing introduces diversity in testing results. However, it also leads to a large volume of test reports, many of which highlight the same recurring issues. While these reports provide valuable feedback, their redundancy makes it inefficient for developers to review the reports and identify bugs. Crowdsourced test report clustering has been proposed to mitigate this problem, allowing developers to focus only on the representative reports from each cluster. However, existing methods primarily rely on embedding features extracted from reports for clustering, which limits their ability to generate accurate and interpretable clusters due to a lack of deeper semantic understanding of the reports. To address the aforementioned challenge, we propose LLMCluster , a novel method for crowdsourced test report clustering based on large language models (LLMs). LLMCluster employs an iterative clustering strategy. In each iteration, LLMCluster processes a subset of reports by instructing the LLM to disregard surface-level variations in expression, analyze the core issue in each report, and group reports addressing the same issue into new or existing clusters. After the iterative clustering process, LLMCluster applies correction algorithms to ensure the completeness and validity of the clustering result. Finally, LLMCluster utilizes the LLM to generate concise summaries for each cluster, making the results more intuitive and interpretable. Experimental results show that LLMCluster outperforms state-of-the-art methods across six commonly used clustering evaluation metrics. Additionally, the cluster summaries generated by LLMCluster semantically align well with manually written summaries.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414004598",
    "type": "article"
  },
  {
    "title": "FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3766890",
    "publication_date": "2025-09-10",
    "publication_year": 2025,
    "authors": "Moses Openja; Paolo Arcaini; Foutse Khomh; Fuyuki Ishikawa",
    "corresponding_authors": "",
    "abstract": "Deep neural networks (DNNs) are being utilized in various aspects of our daily lives, including high-stakes decision-making applications that impact individuals. However, these systems reflect and amplify bias from the data used during training and testing, potentially resulting in biased behavior and inaccurate decisions. For instance, having different misclassification rates between white and black sub-populations. However, effectively and efficiently identifying and correcting biased behavior in DNNs is a challenge. This paper introduces FairFLRep , an automated fairness-aware fault localization and repair technique that identifies and corrects potentially bias-inducing neurons in DNN classifiers. FairFLRep focuses on adjusting neuron weights associated with sensitive attributes, such as race or gender, that contribute to unfair decisions. By analyzing the input-output relationships within the network, FairFLRep corrects neurons responsible for disparities in predictive quality parity. We evaluate FairFLRep on four image classification datasets using two DNN classifiers, and four tabular datasets with a DNN model. The results show that FairFLRep consistently outperforms existing methods in improving fairness while preserving accuracy. An ablation study confirms the importance of considering fairness during both fault localization and repair stages. Our findings also show that FairFLRep is more efficient than the baseline approaches in repairing the network.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414091836",
    "type": "article"
  },
  {
    "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks",
    "doi": "https://doi.org/10.1145/3766895",
    "publication_date": "2025-09-10",
    "publication_year": 2025,
    "authors": "Andreas Happe; Jürgen Cito",
    "corresponding_authors": "",
    "abstract": "Traditional enterprise penetration-testing, while critical for validating defenses and uncovering vulnerabilities, is often limited by high operational costs and the scarcity of human expertise. This paper investigates the feasibility and effectiveness of using Large Language Model (LLM)-driven autonomous systems to address these challenges in real-world Active Directory (AD) enterprise networks. We introduce a novel prototype, cochise , designed to employ LLMs to autonomously perform Assumed Breach penetration-testing against enterprise networks. Our system represents the first demonstration of a fully autonomous, LLM-driven framework capable of compromising accounts within a real-life Microsoft Active Directory testbed, the Game of Active Directory (GOAD). The evaluation deliberately utilizes GOAD to capture the intricate interactions and sometimes nondeterministic outcomes of live network penetration-testing, moving beyond the limitations of synthetic benchmarks. We perform our empirical evaluation using five LLMs, comparing reasoning to non-reasoning models as well as including open-weight models. Through comprehensive quantitative and qualitative analysis, incorporating insights from cybersecurity experts, we demonstrate that autonomous LLMs can effectively conduct Assumed Breach simulations. Key findings highlight their ability to dynamically adapt attack strategies, perform inter-context attacks (e.g., web application audits, social engineering, and unstructured data analysis for credentials), and generate scenario-specific attack parameters like realistic password candidates. The prototype also exhibits robust self-correction mechanisms, automatically installing missing tools and rectifying invalid command generations. Critically, we find that the associated costs are competitive with, and often significantly lower than, those incurred by professional human penetration testers, suggesting a path toward democratizing access to essential security testing for organizations with budgetary constraints. However, our research also illuminates existing limitations, including instances of LLM “going down rabbit holes”, challenges in comprehensive information transfer between planning and execution modules, and critical safety concerns that necessitate human oversight. Our findings lay foundational groundwork for future software engineering research into LLM-driven cybersecurity automation, emphasizing that the prototype's underlying LLM-driven architecture and techniques are domain-agnostic and hold promise for improving autonomous LLM usage in broader software engineering domains. The source code, traces, and analyzed logs are open-sourced to foster collective cybersecurity and future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414091921",
    "type": "article"
  },
  {
    "title": "Applications and Challenges of Fairness APIs in Machine Learning Software",
    "doi": "https://doi.org/10.1145/3765735",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Ajoy Das; Gias Uddin; Shaiful Chowdhury; Mostafijur Rahman Akhond; Hadi Hemmati",
    "corresponding_authors": "",
    "abstract": "Machine Learning software systems are frequently used in our day-to-day lives. Some of these systems are used in various sensitive environments to make life-changing decisions. Therefore, it is crucial to ensure that these AI/ML systems do not make any discriminatory decisions for any specific groups or populations. In that vein, different bias detection and mitigation open-source software libraries (aka API libraries) are being developed and used. In this paper, we conduct a qualitative study to understand in what scenarios these open-source fairness APIs are used in the wild, how they are used, and what challenges the developers of these APIs face while developing and adopting these libraries. We have analyzed 204 GitHub repositories (from a list of 1885 candidate repositories) which used 13 APIs that are developed to address bias in ML software. We found that these APIs are used for two primary purposes (i.e., learning and solving real-world problems), targeting 17 unique use-cases. Our study suggests that developers are not well-versed in bias detection and mitigation; they face lots of troubleshooting issues, and frequently ask for opinions and resources. Our findings can be instrumental for future bias-related software engineering research, and for guiding educators in developing more state-of-the-art curricula.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414115194",
    "type": "article"
  },
  {
    "title": "ReachCheck: Compositional Library-Aware Call Graph Reachability Analysis in the IDEs",
    "doi": "https://doi.org/10.1145/3767166",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Chao Wang; Li Lin; Chengpeng Wang; Jiafeng Huang; ChaoMin Wu; Rongxin Wu",
    "corresponding_authors": "",
    "abstract": "Call graph reachability analysis is essential for vulnerability detection, dependency conflict analysis, and compatibility checks. However, modern software systems, particularly those developed within integrated development environments (IDEs), often rely on third-party libraries (TPLs), which significantly increase the analysis cost. This paper introduces ReachCheck, a compositional library-aware analysis for method pair reachability in the IDEs. Specifically, ReachCheck summarizes TPL reachability via offline transitive closure and integrates the summaries with application code on demand, eliminating redundant analysis. Additionally, we use matrix representations for call graphs and employ fast matrix multiplication for transitive closure, further improving efficiency. We have implemented our approach as a prototype and evaluated it upon real-world projects. Compared to online traversal, function summary approaches and three state-of-the-art graph reachability approaches (Ferrari, BL and BFL), ReachCheck achieves 237.75×, 78.55×, 84.86×, 4369.09× and 80.91× speedup respectively. For downstream clients like dependency conflict detection and CVE risk detection, ReachCheck completes analysis in 0.61 and 0.35 seconds, yielding 537.59× and 519.03× speedup over existing techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414118658",
    "type": "article"
  },
  {
    "title": "Exploring Development Methods for Reactive Synthesis Specifications",
    "doi": "https://doi.org/10.1145/3767159",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Dor Ma’ayan; Shahar Maoz; Jan Oliver Ringert",
    "corresponding_authors": "",
    "abstract": "Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Despite significant research progress in the past decades, reactive synthesis is still in early-stage of use. Previous studies found that the lack of development methods for reactive synthesis specifications is one barrier to its wider adoption. In this paper, we adapt two development methods, an incremental method and a modular method, to the context of reactive synthesis specifications. The methods are based on existing software development methods on the one hand and studies about reactive synthesis on the other hand. Then, we report on an exploratory case study in which participants developed specifications using the two methods. We evaluated the methods using a mixed-method analysis that combines grounded theory analysis of Slack communication with participants, quantitative exploratory data analysis of the synthesis IDE usage logs, and qualitative independent expert review of the final specifications. Our findings show clear benefits of modular specification development in terms of ease of planning, synthesis time, fewer unrealizability issues, and faster debugging. However, the incremental development method was more natural and easy to use, and specifications developed incrementally were also easier to validate during the development process.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414118682",
    "type": "article"
  },
  {
    "title": "ACTesting: Automated Cross-modal Testing Method of Text-to-Image Software",
    "doi": "https://doi.org/10.1145/3768581",
    "publication_date": "2025-09-19",
    "publication_year": 2025,
    "authors": "Siqi Gu; Chunrong Fang; Quanjun Zhang; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Recently, creative generative artificial intelligence software has emerged as a pivotal assistant, enabling users to generate content and seek inspiration rapidly. Text-to-Image (T2I) software, one of the most widely used, synthesizes images with text input by engaging in a cross-modal process. However, despite substantial advancements in the T2I engine, T2I software still encounters errors when generating complex or non-realistic scenes, including omitting focal entities, low image realism, and mismatched text-image information. The cross-modal nature of T2I software complicates error detection for traditional testing methods, and the absence of test oracles further exacerbates the complexity of the testing process. To fill this gap, we propose ACTesting , an A utomated C ross-modal Testing Method of Text-to-Image Software, the first testing method explicitly designed for T2I software. ACTesting utilizes the metamorphic testing principle to address the oracle problem and identifies cross-modal semantic consistency as its fundamental Metamorphic relation (MR) by employing the Entity-relationship (ER) triples. We design three kinds of mutation operators under the guidance of MR and the adaptability density constraint to construct the new input text. After generating the images based on the text, ACTesting verifies whether MR is satisfied by detecting the ER triples across two modalities to detect the errors of T2I software. In our experiments across five popular T2I software, ACTesting effectively generates error-revealing tests, resulting in a decrease in text-image consistency by up to 20% when compared to the baseline. Additionally, an ablation study demonstrates the efficacy of the proposed mutation operators. The experimental results validate that ACTesting can reliably identify errors within T2I software.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414363239",
    "type": "article"
  },
  {
    "title": "CodeS: Natural Language to Code Repository via Multi-Layer Sketch",
    "doi": "https://doi.org/10.1145/3768577",
    "publication_date": "2025-09-19",
    "publication_year": 2025,
    "authors": "Daoguang Zan; Ailun Yu; Wei Liu; Dong Chen; Bo Shen; Yafen Yao; Wei Li; X. Chen; Yongshun Gong; Bei Guan; Zhiguang Yang; Y.Q. Wang; Lizhen Cui; Qianxiang Wang",
    "corresponding_authors": "",
    "abstract": "The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository’s directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414363390",
    "type": "article"
  },
  {
    "title": "<i>PLocator</i> : Fine-Grained Patch Presence Test in Binaries via Patch Code Localization",
    "doi": "https://doi.org/10.1145/3770079",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Chaopeng Dong; Jingdong Guo; Shouguo Yang; Yang Xiao; Yi Li; Hong Li; Zhi Li; Limin Sun",
    "corresponding_authors": "",
    "abstract": "1-day vulnerabilities in binaries have become a major threat to software security. Patch presence test is one of the effective ways to detect the vulnerability. However, existing patch presence test works do not perform well in practical scenarios due to the interference from the various compilers and optimizations, patch-similar code blocks, and irrelevant functions in stripped binaries. In this paper, we propose a novel approach named PLocator , which leverages constants from both the patch code and its context, extracted from the control flow graph, to form the anchors and accurately locate the real patch code in the target function, offering a practical solution for real-world vulnerability detection scenarios. To evaluate the effectiveness of PLocator , we collected 73 CVEs and constructed two datasets with and without the irrelevant functions, comprising 1,090 and 27,250 functions, respectively. Moreover, we set three different experiments, i.e., Same, XO (cross-optimizations), and XC (cross-compilers), to evaluate the performance of existing patch presence test approaches and PLocator . The results demonstrate that PLocator outperforms the second state-of-the-art approach on accuracy by 44.3% (without irrelevant functions) and 74.9% (with irrelevant functions), indicating that PLocator is more practical for the patch presence task.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414688249",
    "type": "article"
  },
  {
    "title": "Refinement calculus of quantum programs with projective assertions",
    "doi": "https://doi.org/10.1145/3770083",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Yuan Feng; Li Zhou; Yingte Xu; Xiaoquan Xu",
    "corresponding_authors": "",
    "abstract": "This paper presents a refinement calculus designed specifically for quantum programs, offering a structured approach to their progressive and modular development while ensuring correctness throughout the refinement process. We begin by investigating the partial correctness of nondeterministic programs within a quantum while language featuring prescription statements. Orthogonal projectors, which are in one-to-one correspondence to subspaces of the state Hilbert space, are taken as assertions for quantum states. We provide the weakest liberal precondition and strongest postcondition semantics of quantum programs, and based on these dual semantics, introduce refinement rules that facilitate the incremental development of quantum programs. All these rules, as well as their soundness and completeness, are formalized and proven in the Coq proof assistant by utilizing CoqQ, a general-purpose framework for quantum computing and program verification. To demonstrate the utility of the refinement calculus, we examine examples including the implementation of a Z -rotation gate, the repetition error-correction code, and the quantum-to-quantum Bernoulli factory. Moreover, we present QRefine, a Python-based interactive prototype designed as a proof of concept to demonstrate the potential of our approach in the stepwise development of correct quantum programs",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414688375",
    "type": "article"
  },
  {
    "title": "The Factors Influencing Well-Being in Software Engineers: A Mixed-Method Study",
    "doi": "https://doi.org/10.1145/3770074",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Cristina Martinez Montes; Birgit Penzenstadler; Robert Feldt",
    "corresponding_authors": "",
    "abstract": "The well-being of software engineers is increasingly under strain due to the high-stress nature of their roles, which involve complex problem-solving, tight deadlines, and the pressures of rapidly evolving technologies. Despite increasing recognition of mental health challenges in software engineering, few studies focus on the factors that sustain or undermine well-being. Existing research often overlooks the interaction between personal, collaborative, and organisational influences on this unique population. This study fills this gap by investigating the specific factors affecting the well-being of software engineers. We conducted 15 qualitative interviews and complemented them with a survey with participants from multiple countries to validate and extend our findings to a broader population. Our mixed-methods approach provides a robust framework to identify key factors influencing well-being, including personal perceptions of well-being, interpersonal and collaborative dynamics, workplace support and recognition, organisational culture, and specific stressors inherent to software engineering. By offering a detailed, context-specific exploration of these factors, our study builds on existing literature and provides actionable insights for improving well-being in software engineering. We conclude with policy recommendations to inform organisational strategies and develop targeted interventions that address the specific challenges of this field, contributing to more sustainable and supportive work environments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414688387",
    "type": "article"
  },
  {
    "title": "A Scalable Vulnerability Detection System with Multi-View Graph Representations",
    "doi": "https://doi.org/10.1145/3770075",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Shihan Dou; Huiyuan Zheng; Junjie Shan; Yueming Wu; Deqing Zou; Xuanjing Huang; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) has been extensively utilized in source code vulnerability detection due to its robust automatic feature extraction capabilities. To achieve scalable vulnerability scanning, some prior studies intend to process the source code directly by treating them as text. However, these text-based methods can not achieve the best performance due to the lack utilizing program semantics. In contrast, other approaches aim to enhance accuracy by distilling program semantics into graph representations for vulnerability detection. Despite their accuracy, these graph-based methods are not scalable, as graph analysis is generally time-intensive. In this paper, we aim to achieve both scalability and accuracy in scanning large-scale source code vulnerabilities. Inspired by existing DL-based image classification which has the ability to analyze millions of images accurately, we prefer to use these techniques to accomplish our purpose. Specifically, we propose an innovative approach that efficiently converts the source code of a function into multi-view images, while preserving the diverse program information from different perspectives. Based on this approach, we introduce a vulnerability detection system, VulCNN , which can leverage the function images and simple convolutional neural networks (CNN) to effectively and efficiently detect vulnerabilities. We implement VulCNN and evaluate it on three popular and widely used datasets ( ie., the Devign , Big-Vul , and ReVeal datasets). Experimental results report that VulCNN can achieve better accuracy than nine state-of-the-art vulnerability detectors ( ie., TokenCNN , VulDeePecker , SySeVR , VulDeeLocator , Devign , EPVD , VulDecgre , UniXcoder , and PDBERT ). We also compare VulCNN with three widely used Large Language Models ( ie., Llama-2-Instruct , DeepSeek-Coder , Qwen2.5-Coder-Instruct ) and one LLM-based detection method ( ie., GRACE ), and the experimental results demonstrate that our proposed method can perform better than most of vulnerability detection methods. As for scalability, VulCNN is about five times faster than VulDeeLocator , and about two times faster than Devign . VulCNN achieves vulnerability detection effectiveness comparable to the state-of-the-art method GRACE , but with a 31.52% reduction in time overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414688389",
    "type": "article"
  },
  {
    "title": "Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair",
    "doi": "https://doi.org/10.1145/3770581",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Qiong Feng; Xiaotian Ma; Jiayi Sheng; Ziyuan Feng; Wei Song; Peng Liang",
    "corresponding_authors": "",
    "abstract": "LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code using an infilling-style technique or directly generate patches when provided with buggy methods, aiming for plausible patches to pass all tests. However, most of LLM-based APR methods rely on a single type of software information, such as issue descriptions or error stack traces, without fully leveraging a combination of diverse software artifacts. Human developers, in contrast, often use a range of information — such as debugging data, issue discussions, and error stack traces — to diagnose and fix bugs. Despite this, many LLM-based approaches do not explore which specific types of software information best assist in localizing and repairing software bugs. Addressing this gap is crucial for advancing LLM-based APR techniques. To investigate this and mimic the way human developers fix bugs, we propose DEVLoRe (short for DEV eloper Lo calization and Re pair). In this framework, LLMs first use issue content (description and discussion) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate valid patches. We evaluated the effectiveness of issue content, error stack traces, and debugging information in bug localization and automatic program repair. Our results show that while issue content and error stack is particularly effective in assisting LLMs with fault localization and program repair respectively, different types of software artifacts complement each other in addressing various bugs. By incorporating these three types of artifacts and using the Defects4J v2.0 dataset for evaluation, DEVLoRe successfully localizes 49.3% of single-method bugs and generates 56.0% plausible patches. Additionally, DEVLoRe can localize 47.6% of non-single-method bugs and generates 14.5% plausible patches. Moreover, our framework streamlines the end-to-end process from buggy source code to a complete repair, and achieves a 39.7% and 17.1% of single-method and non-single-method bug repair rate, outperforming current state-of-the-art APR methods. Furthermore, we re-implemented and evaluated our framework, demonstrating its effectiveness in resolving 9 unique issues compared to other state-of-the-art frameworks using the same or more advanced models on SWE-bench Lite. We also discussed whether a leading framework for Python code can be directly applied to Java code, or vice versa. The source code and experimental results of this work for replication are available at https://github.com/XYZboom/DEVLoRe .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414799519",
    "type": "article"
  },
  {
    "title": "Mapping the Trust Terrain: LLMs in Software Engineering - Insights and Perspectives",
    "doi": "https://doi.org/10.1145/3771282",
    "publication_date": "2025-10-09",
    "publication_year": 2025,
    "authors": "Dipin Khati; Yijin Liu; David N. Palacio; Yixuan Zhang; Denys Poshyvanyk",
    "corresponding_authors": "",
    "abstract": "The application of Large Language Models (LLMs) in Software Engineering (SE) continues to grow rapidly across both industry and academia. As these models become integral to critical SE processes, ensuring their reliability and trustworthiness becomes essential. Achieving this requires a balanced approach to trust: excessive trust can introduce security vulnerabilities, while insufficient trust may hinder innovation. However, the conceptual landscape of trust in LLMs for SE(LLM4SE) remains unclear. Key concepts such as trust, distrust, and trustworthiness lack precise definitions, factors that shape trust formation remain underexplored, and metrics for trust in LLMs remain undeveloped. To clarify the current research landscape and identify future directions, we conducted a comprehensive review of \\(88\\) articles: a systematic review of \\(18\\) studies on LLMs in SE, supplemented by an analysis of \\(70\\) articles from the broader trust literature. Furthermore, we surveyed \\(25\\) domain experts to gather practitioners’ perspectives on trust and identify gaps between their experiences and the existing literature. Our findings provide a structured overview of trust-related concepts in LLM4SE, outlining key areas for future research. This study contributes to building more trustworthy LLM-assisted software engineering processes, ultimately supporting safer and more effective adoption of LLMs in SE.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414993524",
    "type": "article"
  },
  {
    "title": "RunTyper: Enhancing Deep Type Inference Using Dynamic Analysis for Python",
    "doi": "https://doi.org/10.1145/3771544",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Guoqing Yang; Shilin He; Fu Song; Yuqi Chen",
    "corresponding_authors": "",
    "abstract": "Dynamic languages like Python are highly valued for the flexibility they offer, primarily due to their dynamic typing system. However, this flexibility often comes at a cost: type-related errors typically manifest only at runtime, which can result in elusive bugs. This scenario highlights the urgent need for effective automatic type inference strategies. Recently, there has been significant progress in the development of deep learning-based methods targeting the issues. Despite their potential, these methods face considerable limitations in the context of rare types (predominantly user-defined types in Python), which are often underrepresented in training datasets. To tackle the inherent unpredictability of dynamic characteristics in Python, we present an innovative framework named RunTyper. RunTyper uniquely integrates dynamic analysis with a sophisticated framework that combines dynamic instrumentation and static rules. This hybrid approach is utilized to validate and refine predictions made by deep neural networks. When compared with Type4Py, a deep learning-based type inference tool, the experiment results indicate that RunTyper achieves a significantly higher accuracy rate of 53.7%, whereas Type4Py attains only 41.9%. Additionally, a series of experiments demonstrate that RunTyper significantly enhances performance in user-defined types. When compared to HiTyper, which similarly seeks to refine deep type inference but is limited to static rules, RunTyper exhibits a notable improvement, increasing accuracy from 17.0% to 22.9%, an enhancement rate of 35%. Moreover, the experiments indicate that RunTyper improves the accuracy of GPT-4 in processing user-defined types by a remarkable 17%. These findings emphasize RunTyper’s proficiency in refining and enhancing the results produced by deep prediction models, as well as its ability to accurately infer rare types that are traditionally difficult to predict.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415046925",
    "type": "article"
  },
  {
    "title": "A Survey of Learning-based Method Name Prediction",
    "doi": "https://doi.org/10.1145/3771999",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Hanwei Qian; Wei Cheng Liu; Tingyang Xu; Jie Yin; Xia Feng; Weisong Sun; Ziqi Ding; Yuchen Chen; Yun Miao; Jianlong Li; Jianhua Zhao; Chunrong Fang",
    "corresponding_authors": "",
    "abstract": "The choice of method names significantly influences code comprehension and maintenance, posing a considerable challenge, especially for novice developers. Automating the prediction of appropriate method names based on the method code body has emerged as a promising approach to address this challenge. In recent years, numerous machine/deep learning (ML/DL)-based method name prediction (MNP) techniques have been proposed. However, a systematic review of these techniques is currently lacking, hindering future researchers from understanding the research status, development trends, challenges, and opportunities in this field. To fill this gap, in this paper, we conduct a systematic literature review on learning-based MNP studies. Specifically, we first perform a thorough review of the literature concerning publication venue, publication year, and contribution types. This analysis enables us to discern trends in studies related to MNP. Second, we depict the general workflow of learning-based MNP techniques, which involves three consecutive subprocesses: context extraction, context preprocessing, and context-based prediction. Subsequently, we investigate contemporary techniques/solutions applied in the three subprocesses. Third, we scrutinize the widely used experimental databases, evaluation metrics, and replication packages utilized in MNP studies. Moreover, we summarize existing empirical studies on MNP to facilitate a quick understanding of their focus and findings for subsequent researchers. Finally, based on a systematic review and summary of existing work, we outline several open challenges and opportunities in MNP that remain to be addressed in future work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435602",
    "type": "article"
  },
  {
    "title": "Facilitating Trust in AI-assisted Software Tools",
    "doi": "https://doi.org/10.1145/3772370",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Brittany Johnson; Christian Bird; Denae Ford; Ebtesam Al Haque; Nicole Forsgren; Thomas Zimmermann",
    "corresponding_authors": "",
    "abstract": "The day to day of a software engineer involves a variety of tasks. While many of these tasks are collaborative, it is not always possible or feasible to engage with other engineers for task completion. Software tools, such as code generators and static analysis tools, aim to fill this gap by providing additional support for developers to effectively complete their tasks. With a steady stream of new tools emerging to support software engineers, including a new breed of tools that rely on artificial intelligence (AI), there are important questions to answer regarding the trust engineers can, and should, put into their software tools and what it means to build a trustworthy tool. To this end, this paper presents findings from a mixed methods investigation of the factors that contribute to trust in traditional and AI-assisted software tools. First, we introduce the PICSE (pronounced “pixie”) framework for trust in software tools that we developed based on a set of 18 interviews with software practitioners internal and external to Microsoft. We then discuss insights from a survey with 368 internal Microsoft responses on the relevance and importance of the factors in our framework with respect to traditional and AI-assisted tools.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435606",
    "type": "article"
  },
  {
    "title": "Fast, Fine-Grained Equivalence Checking for Neural Decompilers",
    "doi": "https://doi.org/10.1145/3772368",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Luke Dramko; Claire Le Goues; Edward J. Schwartz",
    "corresponding_authors": "",
    "abstract": "Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models are generally inadequate, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign , 1 a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides—which parts of the functions are equivalent and how well the variable names match—is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435609",
    "type": "article"
  },
  {
    "title": "An Automated Approach to Constructing STRIDE Threat Rule Model and Updating Rule Base",
    "doi": "https://doi.org/10.1145/3772283",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Changlan Fu; He Zhang; Xiaofeng Guan",
    "corresponding_authors": "",
    "abstract": "Threat modeling is a structured method for identifying and responding to threats, and the STRIDE method has become the de facto mainstream threat identification technology in practice. At present, the analysis of STRIDE threats and the construction of the rules for threat identification largely rely on human expertise, resulting in incomplete rules for threat identification and data volume of threat modeling as well as insufficient analysis accuracy and efficiency. Along with the rapid emergence of new threats to Internet software every year, there is an urgent need to automatically construct and update a relatively complete rule base to leverage the effectiveness and automation of threat analysis. In this article, we propose a threat identification rule model based on the STRIDE method, providing comprehensive rule base data. Then, we propose an automated approach for classifying STRIDE threats, and further propose an automated approach for constructing the rule base. In addition, we design an automatic update mechanism for the rule base to ensure its effectiveness. The proposed approach is evaluated by conducting comparative experiments, and the results show that the precision of the STRIDE threat automatic classification on the CNNVD dataset reaches 92.5%, the recall at 87.6%, and the F1-score at 89.3%. Compared with the baseline method, our classification approach significantly improve precision, recall, and F1-score by 11.2%, 8.2%, and 9.2% respectively. The accuracy of the automatically constructed rules reached 89.5%. Compared with manual construction approach, our approach improves the automation level and efficiency of rule construction.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435618",
    "type": "article"
  },
  {
    "title": "OBSERV—a prototyping language and environment",
    "doi": "https://doi.org/10.1145/131736.131751",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Shmuel Tyszberowicz; Amiram Yehudai",
    "corresponding_authors": "",
    "abstract": "The OBSERV methodology for software development is based on rapid construction of an executable specification, or prototype, of a systems, which may be examined and modified repeatedly to achieve the desired functionality. The objectives of OBSERV also include facilitating a smooth transition to a target system, and providing means for reusing specification, design, and code of systems and subsystems. We are particularly interested in handling embedded systems, which are likely to have concurrency and have some real-time requirements. The OBSERV prototyping language combines several paradigms to express the behavior of a system. The object-oriented approach provides the basic mechanism for building a system from a collection of objects, with well-defined interfaces between them. We use finite-state machines to model the behavior of individual objects. At a lower level, activities that occur within objects, either upon entry to a state or in transition between thus allowing a nonprocedural description. The environment provided to a prototype builder is as important as the language. We have made an attempt to provide flexible tools for executing or simulating the prototype being built, as well as for browsing and static checking. The first implementation of the tools was window based but not graphic. A graphic front end, name CRUISE, was developed afterwards. A simulation sequence focuses on a single object, which can be as complex as necessary, possibly the entire system, and expects all the interactions between it and the outside world to be achieved by communication between the simulator and the user. The simulator allows the user to easily switch back and forth from one object to another, simulating each object in isolation. To enable testing the behavior of a prototype in a realistic environment, it is possible to construct objects that imitate the environment objects. We also allow simulation of systems with missing pieces, by calling upon the user to simulate any such missing piece by himself.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1991204389",
    "type": "article"
  },
  {
    "title": "Automatic high-quality reengineering of database programs by abstraction, transformation and reimplementation",
    "doi": "https://doi.org/10.1145/958961.958962",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Yossi Cohen; Yishai Feldman",
    "corresponding_authors": "",
    "abstract": "Old-generation database models, such as the indexed-sequential, hierarchical, or network models, provide record-level access to their data, with all application logic residing in the hosting program. In contrast, relational databases can perform complex operations, such as filter, aggregation, and join, on multiple records without an external specification of the record-access logic. Programs written for relational databases attempt to move as much of the application logic as possible into the database, in order to make the most of the optimizations performed internally by the database.This conceptual gap between the programming styles makes automatic high-quality translation of programs written for the older database models to the relational model difficult. It is not enough to convert just the database-access operations, since this would result in unacceptably inefficient programs. It is necessary to convert parts of the application logic from the procedural style of the hosting program (which is almost always Cobol) to the declarative style of SQL.This article describes an automatic system, called MIDAS, that performs high-quality reengineering of legacy database programs in this way. MIDAS is based on the paradigm of translation by abstraction, transformation, and reimplementation. The abstract representation is based on the Plan Calculus, with the addition of Query Graphs, introduced in this article in order to abstract the temporal behavior of database access patterns.The results of MIDAS's translation were found to be superior to those of the naive translation that only converts database-access operations in terms of readability, size of code, speed, and network data traffic. Initial industrial experience with MIDAS also demonstrates the high quality of its translations on large-scale programs.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2010687797",
    "type": "article"
  },
  {
    "title": "HOTTest",
    "doi": "https://doi.org/10.1145/1151695.1151697",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Avik Sinha; Carol Smidts",
    "corresponding_authors": "",
    "abstract": "Model-based testing is an effective black-box test generation technique for applications. Existing model-based testing techniques, however, fail to capture implicit domain-specific properties, as they overtly rely on software artifacts such as design documents, requirement specifications, etc., for completeness of the test model. This article presents a technique, HOTTest, which uses a strongly typed domain-specific language to model the system under test. This allows extraction of type-related system invariants, which can be related to various domain-specific properties of the application. Thus, using HOTTest, it is possible to automatically extract and embed domain-specific requirements into the test models. In this article we describe HOTTest, its principles and methodology, and how it is possible to relate domain-specific properties to specific type constraints. HOTTest is described using the example of HaskellDB, which is a Haskell-based embedded domain-specific language for relational databases. We present an example application of the technique and compare the results to some other commonly used Model-based test automation techniques like ASML-based testing, UML-based testing, and EFSM-based testing.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2023691318",
    "type": "article"
  },
  {
    "title": "Symbolic Message Sequence Charts",
    "doi": "https://doi.org/10.1145/2089116.2089122",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Abhik Roychoudhury; Ankit Goel; Bikram Sengupta",
    "corresponding_authors": "",
    "abstract": "Message sequence charts (MSCs) are a widely used visual formalism for scenario-based specifications of distributed reactive systems. In its conventional usage, an MSC captures an interaction snippet between concrete objects in the system. This leads to voluminous specifications when the system contains several objects that are behaviorally similar. MSCs also play an important role in the model-based testing of reactive systems, where they may be used for specifying (partial) system behaviors, describing test generation criteria, or representing test cases. However, since the number of processes in a MSC specification are fixed, model-based testing of systems consisting of process classes may involve a significant amount of rework: for example, reconstructing system models, or regenerating test cases for systems differing only in the number of processes of various types. In this article we propose a scenario-based notation, called symbolic message sequence charts (SMSCs), for modeling, simulation, and testing of process classes. SMSCs are a lightweight syntactic and semantic extension of MSCs where, unlike MSCs, a SMSC lifeline can denote some/all objects from a collection. Our extensions give us substantially more modeling power. Moreover, we present an abstract execution semantics for (structured collections of) SMSCs. This allows us to validate MSC-based system models capturing interactions between large, or even unbounded, number of objects. Finally, we describe a SMSC-based testing methodology for process classes, which allows generation of test cases for new object configurations with minimal rework. Since our SMSC extensions are only concerned with MSC lifelines, we believe that they can be integrated into existing standards such as UML 2.0. We illustrate our SMSC-based framework for modeling, simulation, and testing of process classes using a weather-update controller case-study from NASA.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2013702204",
    "type": "article"
  },
  {
    "title": "An Approach for Modeling Architectural Design Rules in UML and its Application to Embedded Software",
    "doi": "https://doi.org/10.1145/2089116.2089120",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Anders Mattsson; Brian Fitzgerald; Björn Lundell; Brian Lings",
    "corresponding_authors": "",
    "abstract": "Current techniques for modeling software architecture do not provide sufficient support for modeling architectural design rules. This is a problem in the context of model-driven development in which it is assumed that major design artifacts are represented as formal or semi-formal models. This article addresses this problem by presenting an approach to modeling architectural design rules in UML at the abstraction level of the meaning of the rules. The high abstraction level and the use of UML makes the rules both amenable to automation and easy to understand for both architects and developers, which is crucial to deployment in an organization. To provide a proof-of-concept, a tool was developed that validates a system model against the architectural rules in a separate UML model. To demonstrate the feasibility of the approach, the architectural design rules of an existing live industrial-strength system were modeled according to the approach.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2018220973",
    "type": "article"
  },
  {
    "title": "A formal model for automated software modularity and evolvability analysis",
    "doi": "https://doi.org/10.1145/2377656.2377658",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Yuanfang Cai; Kevin Sullivan",
    "corresponding_authors": "",
    "abstract": "Neither the nature of modularity in software design, characterized as a property of the structure of dependencies among design decisions, or its economic value are adequately well understood. One basic problem is that we do not even have a sufficiently clear definition of what it means for one design decision to depend on another. The main contribution of this work is one possible mathematically precise definition of dependency based on an augmented constraint network model. The model provides an end-to-end account of the connection between modularity and its value in terms of options to make adaptive changes in uncertain and changing design spaces. We demonstrate the validity and theoretical utility of the model, showing that it is consistent with, and provides new insights into, several previously published results in design theory.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2072143588",
    "type": "article"
  },
  {
    "title": "Runtime Fault Detection in Programmed Molecular Systems",
    "doi": "https://doi.org/10.1145/3295740",
    "publication_date": "2019-03-13",
    "publication_year": 2019,
    "authors": "Samuel Ellis; Titus H. Klinge; James I. Lathrop; Jack H. Lutz; Robyn R. Lutz; Andrew S. Miner; Hugh Potter",
    "corresponding_authors": "",
    "abstract": "Watchdog timers are devices that are commonly used to monitor the health of safety-critical hardware and software systems. Their primary function is to raise an alarm if the monitored systems fail to emit periodic “heartbeats” that signal their well-being. In this article, we design and verify a molecular watchdog timer for monitoring the health of programmed molecular nanosystems. This raises new challenges, because our molecular watchdog timer and the system that it monitors both operate in the probabilistic environment of chemical kinetics, where many failures are certain to occur and it is especially hard to detect the absence of a signal. Our molecular watchdog timer is the result of an incremental design process that uses goal-oriented requirements engineering, simulation, stochastic analysis, and software verification tools. We demonstrate the molecular watchdog’s functionality by having it monitor a molecular oscillator. Both the molecular watchdog timer and the oscillator are implemented as chemical reaction networks, which are the current programming language of choice for many molecular programming applications.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2964289871",
    "type": "article"
  },
  {
    "title": "Generating Test Cases for Programs that Are Coded against Interfaces and Annotations",
    "doi": "https://doi.org/10.1145/2544135",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Mainul Islam; Christoph Csallner",
    "corresponding_authors": "",
    "abstract": "Automatic test case generation for software programs is very powerful but suffers from a key limitation. That is, most current test case generation techniques fail to cover testee code when covering that code requires additional pieces of code not yet part of the program under test. To address some of these cases, the Pex state-of-the-art test case generator can generate basic mock code. However, current test case generators cannot handle cases in which the code under test uses multiple interfaces, annotations, or reflection. To cover such code in an object-oriented setting, we describe a novel technique for generating test cases and mock classes. The technique consists of collecting constraints on interfaces, annotations, and reflection, combining them with program constraints collected during dynamic symbolic execution, encoding them in a constraint system, solving them with an off-the-shelf constraint solver, and mapping constraint solutions to test cases and custom mock classes. We demonstrate the value of this technique on open-source applications. Our approach covered such third-party code with generated mock classes, while competing approaches failed to cover the code and sometimes produced unintended side-effects such as filling the screen with dialog boxes and writing into the file system.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1975474596",
    "type": "article"
  },
  {
    "title": "Discovering Multidimensional Correlations among Regulatory Requirements to Understand Risk",
    "doi": "https://doi.org/10.1145/2000799.2000802",
    "publication_date": "2011-09-01",
    "publication_year": 2011,
    "authors": "Robin Gandhi; S. W. Lee",
    "corresponding_authors": "",
    "abstract": "Security breaches most often occur due to a cascading effect of failure among security constraints that collectively contribute to overall secure system behavior in a socio-technical environment. Therefore, during security certification activities, analysts must systematically take into account the nexus of causal chains that exist among security constraints imposed by regulatory requirements. Numerous regulatory requirements specified in natural language documents or listed in spreadsheets/databases do not facilitate such analysis. The work presented in this article outlines a stepwise methodology to discover and understand the multidimensional correlations among regulatory requirements for the purpose of understanding the potential for risk due to noncompliance during system operation. Our lattice algebraic computational model helps estimate the collective adequacy of diverse security constraints imposed by regulatory requirements and their interdependencies with each other in a bounded scenario of investigation. Abstractions and visual metaphors combine human intuition with metrics available from the methodology to improve the understanding of risk based on the level of compliance with regulatory requirements. In addition, a problem domain ontology that classifies and categorizes regulatory requirements from multiple dimensions of a socio-technical environment promotes a common understanding among stakeholders during certification and accreditation activities. A preliminary empirical investigation of our theoretical propositions has been conducted in the domain of The United States Department of Defense Information Technology Security Certification and Accreditation Process (DITSCAP). This work contributes a novel approach to understand the level of compliance with regulatory requirements in terms of the potential for risk during system operation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2027127753",
    "type": "article"
  },
  {
    "title": "Developing and Evaluating Objective Termination Criteria for Random Testing",
    "doi": "https://doi.org/10.1145/3339836",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Porfirio Tramontana; Domenico Amalfitano; Nicola Amatucci; Atif M. Memon; Anna Rita Fasolino",
    "corresponding_authors": "",
    "abstract": "Random testing is a software testing technique through which programs are tested by generating and executing random inputs. Because of its unstructured nature, it is difficult to determine when to stop a random testing process. Faults may be missed if the process is stopped prematurely, and resources may be wasted if the process is run too long. In this article, we propose two promising termination criteria, “All Equivalent” (AEQ) and “All Included in One” (AIO), applicable to random testing. These criteria stop random testing once the process has reached a code-coverage-based saturation point after which additional testing effort is unlikely to provide additional effectiveness. We model and implement them in the context of a general random testing process composed of independent random testing sessions. Thirty-six experiments involving GUI testing and unit testing of Java applications have demonstrated that the AEQ criteria is generally able to stop the process when a code coverage equal or very near to the saturation level is reached, while AIO is able to stop the process earlier in cases it reaches the saturation level of coverage. In addition, the performance of the two criteria has been compared against other termination criteria adopted in the literature.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2963670055",
    "type": "article"
  },
  {
    "title": "Theoretical and Practical Aspects of Linking Operational and Algebraic Semantics for MDESL",
    "doi": "https://doi.org/10.1145/3295699",
    "publication_date": "2019-07-29",
    "publication_year": 2019,
    "authors": "Feng Sheng; Huibiao Zhu; Jifeng He; Zongyuan Yang; Jonathan P. Bowen",
    "corresponding_authors": "",
    "abstract": "Verilog is a hardware description language (HDL) that has been standardized and widely used in industry. Multithreaded discrete event simulation language (MDESL) is a Verilog-like language. It contains interesting features such as event-driven computation and shared-variable concurrency. This article considers how the algebraic semantics links with the operational semantics for MDESL. Our approach is from both the theoretical and practical aspects. The link is proceeded by deriving the operational semantics from the algebraic semantics. First, we present the algebraic semantics for MDESL. We introduce the concept of head normal form. Second, we present the strategy of deriving operational semantics from algebraic semantics. We also investigate the soundness and completeness of the derived operational semantics with respect to the derivation strategy. Our theoretical approach is complemented by a practical one, and we use the theorem proof assistant Coq to formalize the algebraic laws and the derived operational semantics. Meanwhile, the soundness and completeness of the derived operational semantics is also verified via the mechanical approach in Coq. Our approach is a novel way to formalize and verify the correctness and equivalence of different semantics for MDESL in both a theoretical approach and a practical approach.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2964513622",
    "type": "article"
  },
  {
    "title": "Testing Relative to Usage Scope",
    "doi": "https://doi.org/10.1145/3389126",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Breno Miranda; Antonia Bertolino",
    "corresponding_authors": "",
    "abstract": "Coverage criteria provide a useful and widely used means to guide software testing; however, indiscriminately pursuing full coverage may not always be convenient or meaningful, as not all entities are of interest in any usage context. We aim at introducing a more meaningful notion of coverage that takes into account how the software is going to be used. Entities that are not going to be exercised by the user should not contribute to the coverage ratio. We revisit the definition of coverage measures, introducing a notion of relative coverage. According to this notion, we provide a definition and a theoretical framework of relative coverage, within which we discuss implications on testing theory and practice. Through the evaluation of three different instances of relative coverage, we could observe that relative coverage measures provide a more effective strategy than traditional ones: we could reach higher coverage measures, and test cases selected by relative coverage could achieve higher reliability. We hint at several other useful implications of relative coverage notion on different aspects of software testing.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3033180469",
    "type": "article"
  },
  {
    "title": "Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes",
    "doi": "https://doi.org/10.1145/3433928",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "Bailey Vandehei; Daniel Alencar da Costa; Davide Falessi",
    "corresponding_authors": "",
    "abstract": "Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach . However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation &lt;2)—across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3129254119",
    "type": "article"
  },
  {
    "title": "Dynamite",
    "doi": "https://doi.org/10.1145/2544136",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Mariano M. Moscato; Carlos G. López Pombo; Marcelo F. Frias",
    "corresponding_authors": "",
    "abstract": "Automatic analysis of Alloy models is supported by the Alloy Analyzer, a tool that translates an Alloy model to a propositional formula that is then analyzed using off-the-shelf SAT solvers. The translation requires user-provided bounds on the sizes of data domains. The analysis is limited by the bounds and is therefore partial. Thus, the Alloy Analyzer may not be appropriate for the analysis of critical applications where more conclusive results are necessary. Dynamite is an extension of PVS that embeds a complete calculus for Alloy. It also includes extensions to PVS that allow one to improve the proof effort by, for instance, automatically analyzing new hypotheses with the aid of the Alloy Analyzer. Since PVS sequents may get cluttered with unnecessary formulas, we use the Alloy unsat-core extraction feature in order to refine proof sequents. An internalization of Alloy's syntax as an Alloy specification allows us to use the Alloy Analyzer for producing witnesses for proving existentially quantified formulas. Dynamite complements the partial automatic analysis offered by the Alloy Analyzer with semi-automatic verification through theorem proving. It also improves the theorem proving experience by using the Alloy Analyzer for early error detection, sequent refinement, and witness generation.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1972619218",
    "type": "article"
  },
  {
    "title": "Stochastic Performance Analysis of Global Software Development Teams",
    "doi": "https://doi.org/10.1145/2955093",
    "publication_date": "2016-08-22",
    "publication_year": 2016,
    "authors": "Ricardo M. Czekster; Paulo Fernandes; Lucelene Lopes; Afonso Sales; Alan R. Santos; Thais Webber",
    "corresponding_authors": "",
    "abstract": "Measuring productivity in globally distributed projects is crucial to improve team performance. These measures often display information on whether a given project is moving forward or starts to demonstrate undesired behaviors. In this paper we are interested in showing how analytical models could deliver insights for the behavior of specific distributed software collaboration projects. We present a model for distributed projects using stochastic automata networks (SAN) formalism to estimate, for instance, the required level of coordination for specific project configurations. We focus our attention on the level of interaction among project participants and its close relation with team’s productivity. The models are parameterized for different scenarios and solved using numerical methods to obtain exact solutions. We vary the team’s expertise and support levels to measure the impact on the overall project performance. As results, we present our derived productivity index for all scenarios and we state implications found in order to analyze popular preconceptions in GSD area, confirming some, and refusing others. Finally, we foresee ways to extend the models to represent more intricate behaviors and communication patterns that are usually present in globally distributed software projects.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2517666118",
    "type": "article"
  },
  {
    "title": "Machine Translation Testing via Syntactic Tree Pruning",
    "doi": "https://doi.org/10.1145/3640329",
    "publication_date": "2024-01-10",
    "publication_year": 2024,
    "authors": "Quanjun Zhang; Juan Zhai; Chunrong Fang; Jiawei Liu; Weisong Sun; Haichuan Hu; Qingyu Wang",
    "corresponding_authors": "",
    "abstract": "Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses. This requires to improve the accuracy and the reliability of machine translation systems. However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models. To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems. Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence. Specifically, STP (1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation, (2) generates source sentence pairs based on the metamorphic relation, and (3) reports suspicious issues whose translations break the consistency property by a bag-of-words model. We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs. The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400% more than state-of-the-art techniques), with 64.5% and 65.4% precision, respectively. The reported erroneous translations vary in types and more than 90% of them are not found by state-of-the-art techniques. There are 9,393 erroneous translations unique to STP, which is 711.9% more than state-of-the-art techniques. Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0%, improving state-of-the-art techniques by 55.1% on average.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390722253",
    "type": "article"
  },
  {
    "title": "RAPID: Zero-Shot Domain Adaptation for Code Search with Pre-Trained Models",
    "doi": "https://doi.org/10.1145/3641542",
    "publication_date": "2024-01-18",
    "publication_year": 2024,
    "authors": "Guodong Fan; Shizhan Chen; Cuiyun Gao; Jianmao Xiao; Tao Zhang; Zhiyong Feng",
    "corresponding_authors": "",
    "abstract": "Code search, which refers to the process of identifying the most relevant code snippets for a given natural language query, plays a crucial role in software maintenance. However, current approaches heavily rely on labeled data for training, which results in performance decreases when confronted with cross-domain scenarios including domain- or project-specific situations. This decline can be attributed to their limited ability to effectively capture the semantics associated with such scenarios. To tackle the aforementioned problem, we propose a ze R o-shot dom A in ada P tion with pre-tra I ned mo D els framework for code search named RAPID. The framework first generates synthetic data by pseudo labeling, then trains the CodeBERT with sampled synthetic data. To avoid the influence of noisy synthetic data and enhance the model performance, we propose a mixture sampling strategy to obtain hard negative samples during training. Specifically, the mixture sampling strategy considers both relevancy and diversity to select the data that are hard to be distinguished by the models. To validate the effectiveness of our approach in zero-shot settings, we conduct extensive experiments and find that RAPID outperforms the CoCoSoDa and UniXcoder model by an average of 15.7% and 10%, respectively, as measured by the MRR metric. When trained on full data, our approach results in an average improvement of 7.5% under the MRR metric using CodeBERT. We observe that as the model’s performance in zero-shot tasks improves, the impact of hard negatives diminishes. Our observation also indicates that fine-tuning CodeT5 for generating pseudo labels can enhance the performance of the code search model, and using only 100-shot samples can yield comparable results to the supervised baseline. Furthermore, we evaluate the effectiveness of RAPID in real-world code search tasks in three GitHub projects through both human and automated assessments. Our findings reveal RAPID exhibits superior performance, e.g., an average improvement of 18% under the MRR metric over the top-performing model.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390975090",
    "type": "article"
  },
  {
    "title": "Mapping APIs in Dynamic-typed Programs by Leveraging Transfer Learning",
    "doi": "https://doi.org/10.1145/3641848",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Zhenfei Huang; Junjie Chen; Jiajun Jiang; Yihua Liang; Hanmo You; Fengjie Li",
    "corresponding_authors": "",
    "abstract": "Application Programming Interface (API) migration is a common task for adapting software across different programming languages and platforms, where manually constructing the mapping relations between APIs is indeed time-consuming and error-prone. To facilitate this process, many automated API mapping approaches have been proposed. However, existing approaches were mainly designed and evaluated for mapping APIs of statically-typed languages, while their performance on dynamically-typed languages remains unexplored. In this article, we conduct the first extensive study to explore existing API mapping approaches’ performance for mapping APIs in dynamically-typed languages, for which we have manually constructed a high-quality dataset. According to the empirical results, we have summarized several insights. In particular, the source code implementations of APIs can significantly improve the effectiveness of API mapping. However, due to the confidentiality policy, they may not be available in practice. To overcome this, we propose a novel API mapping approach, named Matl , which leverages the transfer learning technique to learn the semantic embeddings of source code implementations from large-scale open-source repositories and then transfers the learned model to facilitate the mapping of APIs. In this way, Matl can produce more accurate API embedding of its functionality for more effective mapping without knowing the source code of the APIs. To evaluate the performance of Matl , we have conducted an extensive study by comparing Matl with state-of-the-art approaches. The results demonstrate that Matl is indeed effective as it improves the state-of-the-art approach by at least 18.36% for mapping APIs of dynamically-typed language and by 30.77% for mapping APIs of the statically-typed language.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391110910",
    "type": "article"
  },
  {
    "title": "An Empirical Analysis of Issue Templates Usage in Large-Scale Projects on GitHub",
    "doi": "https://doi.org/10.1145/3643673",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "Emre Sülün; Metehan Saçakçı; Eray Tüzün",
    "corresponding_authors": "",
    "abstract": "GitHub Issues is a widely used issue tracking tool in open-source software projects. Originally designed with broad flexibility, its lack of standardization led to incomplete issue reports, impeding software development and maintenance efficiency. To counteract this, GitHub introduced issue templates in 2016, which rapidly became popular. Our study assesses the current use and evolution of these templates in large-scale open-source projects and their impact on issue tracking metrics, including resolution time, number of reopens, and number of issue comments. Employing a comprehensive analysis of 350 templates from 100 projects, we also evaluated over 1.9 million issues for template conformity and impact. Additionally, we solicited insights from open-source software maintainers through a survey. Our findings highlight issue templates’ extensive usage in 99 of the 100 surveyed projects, with a growing preference for YAML-based templates, a more structured template variant. Projects with a template exhibited markedly reduced resolution time (381.02 days to 103.18 days) and reduced issue comment count (4.95 to 4.32) compared to those without. The use of YAML-based templates further significantly decreased resolution time, the number of reopenings, and the discussion extent. Thus, our research underscores issue templates’ positive impact on large-scale open-source projects, offering recommendations for improved effectiveness.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391402420",
    "type": "article"
  },
  {
    "title": "Precisely Extracting Complex Variable Values from Android Apps",
    "doi": "https://doi.org/10.1145/3649591",
    "publication_date": "2024-02-27",
    "publication_year": 2024,
    "authors": "Marc Miltenberger; Steven Arzt",
    "corresponding_authors": "",
    "abstract": "Millions of users nowadays rely on their smartphones to process sensitive data through apps from various vendors and sources. Therefore, it is vital to assess these apps for security vulnerabilities and privacy violations. Information such as to which server an app connects through which protocol, and which algorithm it applies for encryption, are usually encoded as variable values and arguments of API calls. However, extracting these values from an app is not trivial. The source code of an app is usually not available, and manual reverse engineering is cumbersome with binary sizes in the tens of megabytes. Current automated tools, however, cannot retrieve values that are computed at runtime through complex transformations. In this article, we present ValDroid , a novel static analysis tool for automatically extracting the set of possible values for a given variable at a given statement in the Dalvik byte code of an Android app. We evaluate ValDroid against existing approaches (JSA, Violist, DroidRA, Harvester, BlueSeal, StringHound, IC3, and COAL) on benchmarks and 794 real-world apps. ValDroid greatly outperforms existing tools. It provides an average F 1 score of more than 90%, while only requiring 0.1 s per value on average. For many data types including Network Connections and Dynamic Code Loading, its recall is more than twice the recall of the best existing approaches.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392199326",
    "type": "article"
  },
  {
    "title": "On the Impact of Lower Recall and Precision in Defect Prediction for Guiding Search-Based Software Testing",
    "doi": "https://doi.org/10.1145/3655022",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Anjana Perera; Burak Turhan; Aldeida Aleti; Marcel Böhme",
    "corresponding_authors": "",
    "abstract": "Defect predictors, static bug detectors, and humans inspecting the code can propose locations in the program that are more likely to be buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs in those locations. Often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this article, we study the impact of imprecision in defect prediction on the bug detection effectiveness of SBST. Our study finds that the recall of the defect predictor, i.e., the proportion of correctly identified buggy code, has a significant impact on bug detection effectiveness of SBST with a large effect size. More precisely, the SBST technique detects 7.5 fewer bugs on average (out of 420 bugs) for every 5% decrements of the recall. However, the effect of precision, a measure for false alarms, is not of meaningful practical significance, as indicated by a very small effect size. In the context of combining defect prediction and SBST, our recommendation is to increase the recall of defect predictors as a primary objective and precision as a secondary objective. In our experiments, we find that 75% precision is as good as 100% precision. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393934430",
    "type": "article"
  },
  {
    "title": "Deep Domain Adaptation With Max-Margin Principle for Cross-Project Imbalanced Software Vulnerability Detection",
    "doi": "https://doi.org/10.1145/3664602",
    "publication_date": "2024-05-09",
    "publication_year": 2024,
    "authors": "Van Nguyen; Trung Le; Chakkrit Tantithamthavorn; John Grundy; Dinh Phung",
    "corresponding_authors": "",
    "abstract": "Software vulnerabilities (SVs) have become a common, serious, and crucial concern due to the ubiquity of computer software. Many AI-based approaches have been proposed to solve the software vulnerability detection (SVD) problem to ensure the security and integrity of software applications (in both the development and testing phases). However, there are still two open and significant issues for SVD in terms of (i) learning automatic representations to improve the predictive performance of SVD, and (ii) tackling the scarcity of labeled vulnerability datasets that conventionally need laborious labeling effort by experts. In this paper, we propose a novel approach to tackle these two crucial issues. We first exploit the automatic representation learning with deep domain adaptation for SVD. We then propose a novel cross-domain kernel classifier leveraging the max-margin principle to significantly improve the transfer learning process of SVs from imbalanced labeled into imbalanced unlabeled projects. Our approach is the first work that leverages solid body theories of the max-margin principle, kernel methods, and bridging the gap between source and target domains for imbalanced domain adaptation (DA) applied in cross-project SVD . The experimental results on real-world software datasets show the superiority of our proposed method over state-of-the-art baselines. In short, our method obtains a higher performance on F1-measure, one of the most important measures in SVD, from 1.83% to 6.25% compared to the second highest method in the used datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396773564",
    "type": "article"
  },
  {
    "title": "What Makes a Good TODO Comment?",
    "doi": "https://doi.org/10.1145/3664811",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Haoye Wang; Zhipeng Gao; Tingting Bi; John Grundy; Xinyu Wang; Minghui Wu; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7\\% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396861028",
    "type": "article"
  },
  {
    "title": "Enhancing GUI Exploration Coverage of Android Apps with Deep Link-Integrated Monkey",
    "doi": "https://doi.org/10.1145/3664810",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Han Hu; Han Wang; Ruiqi Dong; Xiao Chen; Chunyang Chen",
    "corresponding_authors": "",
    "abstract": "Mobile apps are ubiquitous in our daily lives for supporting different tasks such as reading and chatting. Despite the availability of many GUI testing tools, app testers still struggle with low testing code coverage due to tools frequently getting stuck in loops or overlooking activities with concealed entries. This results in a significant amount of testing time being spent on redundant and repetitive exploration of a few GUI pages. To address this, we utilize Android’s deep links, which assist in triggering Android intents to lead users to specific pages and introduce a deep link-enhanced exploration method. This approach, integrated into the testing tool Monkey, gives rise to Delm (Deep Link-enhanced Monkey). Delm oversees the dynamic exploration process, guiding the tool out of meaningless testing loops to unexplored GUI pages. We provide a rigorous activity context mock-up approach for triggering existing Android intents to discover more activities with hidden entrances. We conduct experiments to evaluate Delm’s effectiveness on activity context mock-up, activity coverage, method coverage, and crash detection. The findings reveal that Delm can mock up more complex activity contexts and significantly outperform state-of-the-art baselines with 27.2% activity coverage, 21.13% method coverage, and 23.81% crash detection.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396871652",
    "type": "article"
  },
  {
    "title": "A Formal Explainer for Just-In-Time Defect Predictions",
    "doi": "https://doi.org/10.1145/3664809",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Jinqiang Yu; Michael C. Fu; Alexey Ignatiev; Chakkrit Tantithamthavorn; Peter J. Stuckey",
    "corresponding_authors": "",
    "abstract": "Just-in-Tim e (JIT) defect prediction has been proposed to help teams prioritize the limited resources on the most risky commits (or pull requests), yet it remains largely a black box, whose predictions are not explainable or actionable to practitioners. Thus, prior studies have applied various model-agnostic techniques to explain the predictions of JIT models. Yet, explanations generated from existing model-agnostic techniques are still not formally sound, robust, and actionable. In this article, we propose FoX , a Fo rmal e X plainer for JIT Defect Prediction, which builds on formal reasoning about the behavior of JIT defect prediction models and hence is able to provide provably correct explanations, which are additionally guaranteed to be minimal. Our experimental results show that FoX is able to efficiently generate provably correct, robust, and actionable explanations, while existing model-agnostic techniques cannot. Our survey study with 54 software practitioners provides valuable insights into the usefulness and trustworthiness of our FoX approach; 86% of participants agreed that our approach is useful, while 74% of participants found it trustworthy. Thus, this article serves as an important stepping stone towards trustable explanations for JIT models to help domain experts and practitioners better understand why a commit is predicted as defective and what to do to mitigate the risk.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396894041",
    "type": "article"
  },
  {
    "title": "A Tale of Two Comprehensions? Analyzing Student Programmer Attention during Code Summarization",
    "doi": "https://doi.org/10.1145/3664808",
    "publication_date": "2024-05-15",
    "publication_year": 2024,
    "authors": "Zachary Karas; Aakash Bansal; Yifan Zhang; Toby Jia-Jun Li; Collin McMillan; Yu Huang",
    "corresponding_authors": "",
    "abstract": "Code summarization is the task of creating short, natural language descriptions of source code. It is an important part of code comprehension and a powerful method of documentation. Previous work has made progress in identifying where programmers focus in code as they write their own summaries (i.e., Writing). However, there is currently a gap in studying programmers’ attention as they read code with pre-written summaries (i.e., Reading). As a result, it is currently unknown how these two forms of code comprehension compare: Reading and Writing. Also, there is a limited understanding of programmer attention with respect to program semantics. We address these shortcomings with a human eye-tracking study ( n = 27) comparing Reading and Writing. We examined programmers’ attention with respect to fine-grained program semantics, including their attention sequences (i.e., scan paths). We find distinctions in programmer attention across the comprehension tasks, similarities in reading patterns between them, and differences mediated by demographic factors. This can help guide code comprehension in both computer science education and automated code summarization. Furthermore, we mapped programmers’ gaze data onto the Abstract Syntax Tree to explore another representation of human attention. We find that visual behavior on this structure is not always consistent with that on source code.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396927785",
    "type": "article"
  },
  {
    "title": "Neuron Sensitivity Guided Test Case Selection",
    "doi": "https://doi.org/10.1145/3672454",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Dong Huang; Qingwen Bu; Yichao Fu; Yuhao Qing; Xiaofei Xie; Junjie Chen; Heming Cui",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) have been widely deployed in software to address various tasks (e.g., autonomous driving, medical diagnosis). However, they can also produce incorrect behaviors that result in financial losses and even threaten human safety. To reveal and repair incorrect behaviors in DNNs, developers often collect rich, unlabeled datasets from the natural world and label them to test DNN models. However, properly labeling a large number of datasets is a highly expensive and time-consuming task. To address the above-mentioned problem, we propose NSS, Neuron Sensitivity Guided Test Case Selection, which can reduce the labeling time by selecting valuable test cases from unlabeled datasets. NSS leverages the information of the internal neuron induced by the test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly. We evaluated NSS with four widely used datasets and four well-designed DNN models compared to the state-of-the-art (SOTA) baseline methods. The results show that NSS performs well in assessing the probability of failure triggering in test cases and in the improvement capabilities of the model. Specifically, compared to the baseline approaches, NSS achieves a higher fault detection rate (e.g., when selecting 5% of the test cases from the unlabeled dataset in the MNIST&amp;LeNet1 experiment, NSS can obtain an 81.8% fault detection rate, which is a 20% increase compared with SOTA baseline strategies).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399597085",
    "type": "article"
  },
  {
    "title": "Understanding Vulnerability Inducing Commits of the Linux Kernel",
    "doi": "https://doi.org/10.1145/3672452",
    "publication_date": "2024-06-14",
    "publication_year": 2024,
    "authors": "Muhui Jiang; Jinan Jiang; T.Y. Wu; Zuchao Ma; Xiapu Luo; Yajin Zhou",
    "corresponding_authors": "",
    "abstract": "The Linux kernel is popular and well-maintained. Over the past decade, around 860 thousand commits were merged with hundreds of vulnerabilities (i.e., 223 on average) disclosed every year, taking the total lines of code to 35.1 million in 2022. Many algorithms have been proposed to detect the vulnerabilities, but few studied how they were induced. To fill this gap, we conduct the first empirical study on the Kernel Vulnerability Inducing Commits (KVIC), the commits that induced vulnerabilities in the Linux kernel. We utilized 6 different methods on identifying the Kernel Vulnerability Fixing Commits (KVFCs), the commits that fix vulnerabilities in the Linux kernel, and proposed the other 4 different methods for identifying KVICs by using the identified KVFCs as a bridge. In total, we constructed the first dataset of KVICs with 1,240 KVICs for 1,335 CVEs. We conducted a thorough analysis on the characteristics, purposes, and involved human factors of the KVICs and obtained many interesting findings and insights. For example, KVICs usually have limited reviewers and can still be induced by experienced authors or maintainers. Based on these insights, we proposed several suggestions to the Linux community to help mitigate the induction of KVICs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399668891",
    "type": "article"
  },
  {
    "title": "A Systematic Mapping Study Exploring Quantification Approaches to Code, Design, and Architecture Technical Debt",
    "doi": "https://doi.org/10.1145/3675393",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Judith Perera; Ewan Tempero; Yu‐Cheng Tu; Kelly Blincoe",
    "corresponding_authors": "",
    "abstract": "To effectively manage Technical Debt (TD), we need reliable means to quantify it. We conducted a Systematic Mapping Study (SMS) where we identified 39 quantification approaches for Code, Design, and Architecture TD. We analyzed concepts and metrics discussed in these quantification approaches by classifying the quantification approaches based on a set of abstract TD Quantification (TDQ) concepts and their high-level themes, process/time, cost, benefit, probability, and priority, which we developed during our SMS. This helped identify gaps in the literature and to propose future research directions. Among the abstract TDQ concepts discussed in the different quantification approaches, TD item, TD remediation cost, TD interest, and Benefit of remediating TD were the most frequently discussed concepts. They were also supported by some form of measurement. However, some TDQ concepts were poorly examined, for example, the benefit of taking TD. It was evident that cost concepts were more frequently quantified among the approaches, while benefit concepts were not. Most of the approaches focused on remediating TD in retrospect rather than quantifying TD to strategically use it during software development. This raises the question of whether existing approaches reliably quantify TD and suggests the need to further explore TDQ.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400223329",
    "type": "article"
  },
  {
    "title": "Efficient Management of Containers for Software Defined Vehicles",
    "doi": "https://doi.org/10.1145/3672461",
    "publication_date": "2024-07-09",
    "publication_year": 2024,
    "authors": "Anwar Ghammam; Rania Khalsi; Marouane Kessentini; Foyzul Hassan",
    "corresponding_authors": "",
    "abstract": "Containerization technology, such as Docker, is gaining in popularity in newly established software-defined vehicle architectures (SDVA). However, executing those containers can quickly become computationally expensive in constrained environments, given the limited CPU, memory, and energy resources in the Electric Control Units (ECU) of SDVA. Consequently, the efficient management of these containers is crucial for enabling the on-demand usage of the applications in the vehicle based on the available resources while considering several constraints and priorities, including failure tolerance, security, safety, and comfort. In this article, we propose a dynamic software container management approach for constrained environments such as embedded devices/ECUs in SDVA within smart cars. To address the conflicting objectives and constraints within the vehicle, we design a novel search-based approach based on multi-objective optimization. This approach facilitates the allocation, movement, or suspension of containers between ECUs in the cluster. Collaborating with our industry partner, Ford Motor Company, we evaluate our approach using different real-world software-defined scenarios. These scenarios involve using heterogeneous clusters of ECU devices in vehicles based on real-world software containers and use-case studies from the automotive industry. The experimental results demonstrate that our scheduler outperforms existing scheduling algorithms, including the default Docker scheduler -Spread- commonly used in automotive applications. Our proposed scheduler exhibits superior performance in terms of energy and resource cost efficiency. Specifically, it achieves a 35% reduction in energy consumption in power-saving mode compared to the scheduler employed by Ford Motor Company. Additionally, our scheduler effectively distributes workload among the ECUs in the cluster, minimizing resource usage, and dynamically adjusts to the real-time requirements and constraints of the car environment. This work will serve as a fundamental building block in the automotive industry to efficiently manage software containers in smart vehicles, considering constraints and priorities in the real world.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400453092",
    "type": "article"
  },
  {
    "title": "Studying the Impact of TensorFlow and PyTorch Bindings on Machine Learning Software Quality",
    "doi": "https://doi.org/10.1145/3678168",
    "publication_date": "2024-07-13",
    "publication_year": 2024,
    "authors": "Hao Li; Gopi Krishnan Rajbahadur; Cor‐Paul Bezemer",
    "corresponding_authors": "Cor‐Paul Bezemer",
    "abstract": "Bindings for machine learning frameworks (such as TensorFlow and PyTorch) allow developers to integrate a framework’s functionality using a programming language different from the framework’s default language (usually Python). In this paper, we study the impact of using TensorFlow and PyTorch bindings in C#, Rust, Python and JavaScript on the software quality in terms of correctness (training and test accuracy) and time cost (training and inference time) when training and performing inference on five widely used deep learning models. Our experiments show that a model can be trained in one binding and used for inference in another binding for the same framework without losing accuracy. Our study is the first to show that using a non-default binding can help improve machine learning software quality from the time cost perspective compared to the default Python binding while still achieving the same level of correctness.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400613154",
    "type": "article"
  },
  {
    "title": "MarMot: Metamorphic Runtime Monitoring of Autonomous Driving Systems",
    "doi": "https://doi.org/10.1145/3678171",
    "publication_date": "2024-07-15",
    "publication_year": 2024,
    "authors": "Jon Ayerdi; Asier Iriarte; Pablo Valle; Ibai Roman; Miren Illarramendi; Aitor Arrieta",
    "corresponding_authors": "",
    "abstract": "Autonomous Driving Systems (ADSs) are complex Cyber-Physical Systems (CPSs) that must ensure safety even in uncertain conditions. Modern ADSs often employ Deep Neural Networks (DNNs), which may not produce correct results in every possible driving scenario. Thus, an approach to estimate the confidence of an ADS at runtime is necessary to prevent potentially dangerous situations. In this paper we propose MarMot, an online monitoring approach for ADSs based on Metamorphic Relations (MRs), which are properties of a system that hold among multiple inputs and the corresponding outputs. Using domain-specific MRs, MarMot estimates the uncertainty of the ADS at runtime, allowing the identification of anomalous situations that are likely to cause a faulty behavior of the ADS, such as driving off the road. We perform an empirical assessment of MarMot with five different MRs, using two different subject ADSs, including a small-scale physical ADS and a simulated ADS. Our evaluation encompasses the identification of both external anomalies, e.g., fog, as well as internal anomalies, e.g., faulty DNNs due to mislabeled training data. Our results show that MarMot can identify up to 65% of the external anomalies and 100% of the internal anomalies in the physical ADS, and up to 54% of the external anomalies and 88% of the internal anomalies in the simulated ADS. With these results, MarMot outperforms or is comparable to other state-of-the-art approaches, including SelfOracle, Ensemble, and MC Dropout-based ADS monitors.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400651912",
    "type": "article"
  },
  {
    "title": "Timing Side-Channel Mitigation via Automated Program Repair",
    "doi": "https://doi.org/10.1145/3678169",
    "publication_date": "2024-07-16",
    "publication_year": 2024,
    "authors": "Haifeng Ruan; Yannic Noller; Saeid Tizpaz-Niari; Sudipta Chattopadhyay; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Side-channel vulnerability detection has gained prominence recently due to Spectre and Meltdown attacks. Techniques for side-channel detection range from fuzz testing to program analysis and program composition. Existing side-channel mitigation techniques repair the vulnerability at the IR/binary level or use runtime monitoring solutions. In both cases, the source code itself is not modified, can evolve while keeping the vulnerability, and the developer would get no feedback on how to develop secure applications in the first place. Thus, these solutions do not help the developer understand the side-channel risks in her code and do not provide guidance to avoid code patterns with side-channel risks. In this article, we present Pendulum , the first approach for automatically locating and repairing side-channel vulnerabilities in the source code, specifically for timing side channels. Our approach uses a quantitative estimation of found vulnerabilities to guide the fix localization, which goes hand-in-hand with a pattern-guided repair. Our evaluation shows that Pendulum can repair a large number of side-channel vulnerabilities in real-world applications. Overall, our approach integrates vulnerability detection, quantization, localization, and repair into one unified process. This also enhances the possibility of our side-channel mitigation approach being adopted into programmingenvironments.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400685605",
    "type": "article"
  },
  {
    "title": "MULTICR: Predicting Merged and Abandoned Code Changes in Modern Code Review Using Multi-Objective Search",
    "doi": "https://doi.org/10.1145/3680472",
    "publication_date": "2024-07-30",
    "publication_year": 2024,
    "authors": "Moataz Chouchen; Ali Ouni; Mohamed Wiem Mkaouer",
    "corresponding_authors": "",
    "abstract": "Modern Code Review (MCR) is an essential process in software development to ensure high-quality code. However, developers often spend considerable time reviewing code changes before being merged into the main code base. Previous studies attempted to predict whether a code change was going to be merged or abandoned soon after it was submitted to improve the code review process. However, these approaches require complex cost-sensitive learning, which makes their adoption challenging since it is difficult for developers to understand the main factors behind the models’ predictions. To address this issue, we introduce in this article, MULTICR , a multi-objective search-based approach that uses Multi-Objective Genetic Programming (MOGP) to learn early code review prediction models as IF-THEN rules. MULTICR evolves predictive models while maximizing the accuracy of both merged and abandoned classes, eliminating the need for misclassification cost estimation. To evaluate MULTICR, we conducted an empirical study on 146,612 code reviews from Eclipse, LibreOffice, and Gerrithub. The obtained results show that MULTICR outperforms existing baselines in terms of Matthew Correlation Coefficient (MCC) and F1 scores while learning less complex models compared to decision trees. Our experiments also showed how MULTICR allows identifying the main factors related to abandoned code reviews as well as their associated thresholds, making it a promising approach for early code review prediction with notable performance and inter-operability. Additionally, we qualitatively evaluate MULTICR by conducting a user study through semi-structured interviews involving 10 practitioners from different organizations. The obtained results indicate that 90% of the participants find that MULTICR is useful and can help them to improve the code review process. Additionally, the learned IF-THEN rules of MULTICR are transparent.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401123839",
    "type": "article"
  },
  {
    "title": "CARL: Unsupervised Code-Based Adversarial Attacks for Programming Language Models via Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3688839",
    "publication_date": "2024-08-14",
    "publication_year": 2024,
    "authors": "Kaichun Yao; Hao Wang; Chuan Qin; Hengshu Zhu; Yanjun Wu; Libo Zhang",
    "corresponding_authors": "",
    "abstract": "Code based adversarial attacks play a crucial role in revealing vulnerabilities of software system. Recently, pre-trained programming language models (PLMs) have demonstrated remarkable success in various significant software engineering tasks, progressively transforming the paradigm of software development. Despite their impressive capabilities, these powerful models are vulnerable to adversarial attacks. Therefore, it is necessary to carefully investigate the robustness and vulnerabilities of the PLMs by means of adversarial attacks. Adversarial attacks entail imperceptible input modifications that cause target models to make incorrect predictions. Existing approaches for attacking PLMs often employ either identifier renaming or the greedy algorithm, which may yield sub-optimal performance or lead to high inference times. In response to these limitations, we propose CARL, an unsupervised black-box attack model that leverages reinforcement learning to generate imperceptible adversarial examples. Specifically, CARL comprises a programming language encoder and a perturbation prediction layer. In order to achieve more effective and efficient attack, we cast the task as a sequence decision-making process, optimizing through policy gradient with a suite of reward functions. We conduct extensive experiments to validate the effectiveness of CARL on code summarization, code translation, and code refinement tasks, covering various programming languages and PLMs. The experimental results demonstrate that CARL surpasses state-of-the-art code attack models, achieving the highest attack success rate across multiple tasks and PLMs while maintaining high attack efficiency, imperceptibility, consistency, and fluency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401577050",
    "type": "article"
  },
  {
    "title": "An Exploratory Study on Machine Learning Model Management",
    "doi": "https://doi.org/10.1145/3688841",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Jasmine Latendresse; Samuel Abedu; Ahmad Abdellatif; Emad Shihab",
    "corresponding_authors": "",
    "abstract": "Effective model management is crucial for ensuring performance and reliability in Machine Learning (ML) systems, given the dynamic nature of data and operational environments. However, standard practices are lacking, often resulting in ad hoc approaches. To address this, our research provides a clear definition of ML model management activities, processes, and techniques. Analyzing 227 ML repositories, we propose a taxonomy of 16 model management activities and identify 12 unique challenges. We find that 57.9% of the identified activities belong to the maintenance category, with activities like refactoring (20.5%) and documentation (18.3%) dominating. Our findings also reveal significant challenges in documentation maintenance (15.3%) and bug management (14.9%), emphasizing the need for robust versioning tools and practices in the ML pipeline. Additionally, we conducted a survey that underscores a shift towards automation, particularly in data, model, and documentation versioning, as key to managing ML models effectively. Our contributions include a detailed taxonomy of model management activities, a mapping of challenges to these activities, practitioner-informed solutions for challenge mitigation, and a publicly available dataset of model management activities and challenges. This work aims to equip ML developers with knowledge and best practices essential for the robust management of ML models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401635770",
    "type": "article"
  },
  {
    "title": "Predicting Attrition among Software Professionals: Antecedents and Consequences of Burnout and Engagement",
    "doi": "https://doi.org/10.1145/3691629",
    "publication_date": "2024-09-02",
    "publication_year": 2024,
    "authors": "Bianca Trinkenreich; Fabio Santos; Klaas-Jan Stol",
    "corresponding_authors": "",
    "abstract": "In this study of burnout and engagement, we address three major themes. First, we offer a review of prior studies of burnout among IT professionals and link these studies to the Job Demands-Resources (JD-R) model. Informed by the JD-R model, we identify three factors that are organizational job resources, and posit that these (a) increase engagement, and (b) decrease burnout. Second, we extend the JD-R by considering software professionals’ intention to stay as a consequence of these two affective states, burnout and engagement. Third, we focus on the importance of factors for intention to stay, and actual retention behavior. We use a unique dataset of over 13,000 respondents at one global IT organization, enriched with employment status 90 days after the initial survey. Leveraging partial least squares structural equation modeling and machine learning, we find that the data mostly support our theoretical model, with some variation across different subgroups of respondents. An importance-performance map analysis suggests that managers may wish to focus on interventions regarding burnout as a predictor of intention to leave. The Machine Learning model suggests that engagement and opportunities to learn are the top two most important factors that explain whether software professionals leave an organization.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402129395",
    "type": "article"
  },
  {
    "title": "A Large-Scale Study of IoT Security Weaknesses and Vulnerabilites in the Wild",
    "doi": "https://doi.org/10.1145/3691628",
    "publication_date": "2024-09-04",
    "publication_year": 2024,
    "authors": "Madhu Selvaraj; Gias Uddin",
    "corresponding_authors": "",
    "abstract": "Internet of Things (IoT) is defined as the connection between places and physical objects (i.e., things) over the internet/network via smart computing devices. IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As these devices differ from traditional computing, it is important to understand the challenges IoT developers face while implementing proper security measures in their IoT devices. We observed that IoT software developers share solutions to programming questions as code examples on three Stack Exchange Q&amp;A sites: Stack Overflow (SO), Arduino, and Raspberry Pi. Previous research studies found vulnerabilities/weaknesses in C/C++ code examples shared in Stack Overflow. However, the studies did not investigate C/C++ code examples related to IoT. The studies investigated SO code examples only. In this paper, we conduct a large-scale empirical study of all IoT C/C++ code examples shared in the three Stack Exchange sites, i.e., SO, Arduino, and Raspberry Pi. From the 11,329 obtained code snippets from the three sites, we identify 29 distinct CWE (Common Weakness Enumeration) types in 609 snippets. These CWE types can be categorized into 8 general weakness categories, and we observe that evaluation, memory, and initialization related weaknesses are the most common to be introduced by users when posting programming solutions. Furthermore, we find that 39.58% of the vulnerable code snippets contain instances of CWE types that can be mapped to real-world occurrences of those CWE types (i.e CVE instances). The most number vulnerable IoT code examples was found in Arduino, followed by SO, and Raspberry Pi. Memory type vulnerabilities are on the rise in the sites. For example, from the 3595 mapped CVE instances, we find that 28.99% result in Denial of Service (DoS) errors, which is particularly harmful for network reliant IoT devices such as smart cars. Our study results can guide various IoT stakeholders to be aware of such vulnerable IoT code examples and to inform IoT researchers during their development of tools that can help prevent developers the sharing of such vulnerable code examples in the sites.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402215615",
    "type": "article"
  },
  {
    "title": "On the Understandability of Design-Level Security Practices in Infrastructure-as-Code Scripts and Deployment Architectures",
    "doi": "https://doi.org/10.1145/3691630",
    "publication_date": "2024-09-11",
    "publication_year": 2024,
    "authors": "Evangelos Ntentos; Nicole Elisabeth Lueger; Georg Simhandl; Uwe Zdun; Simon Schneider; Riccardo Scandariato; Nicolás E. Díaz Ferreyra",
    "corresponding_authors": "",
    "abstract": "Infrastructure as Code (IaC) automates IT infrastructure deployment, which is particularly beneficial for continuous releases, for instance, in the context of microservices and cloud systems. Despite its flexibility in application architecture, neglecting security can lead to vulnerabilities. The lack of comprehensive architectural security guidelines for IaC poses challenges in adhering to best practices. We studied how developers interpret IaC scripts (source code) in two IaC technologies, Ansible and Terraform, compared to semi-formal IaC deployment architecture models and metrics regarding design-level security understanding. In a controlled experiment involving ninety-four participants, we assessed the understandability of IaC-based deployment architectures through source code inspection compared to semi-formal representations in models and metrics. We hypothesized that providing semi-formal IaC deployment architecture models and metrics as supplementary material would significantly improve the comprehension of IaC security-related practices, as measured by task correctness . Our findings suggest that semi-formal IaC deployment architecture models and metrics as supplementary material enhance the understandability of IaC security-related practices without significantly increasing duration . We also observed a significant correlation between task correctness and duration when models and metrics were provided.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402456068",
    "type": "article"
  },
  {
    "title": "Divide-and-Conquer: Automating Code Revisions via Localization-and-Revision",
    "doi": "https://doi.org/10.1145/3697013",
    "publication_date": "2024-09-24",
    "publication_year": 2024,
    "authors": "Shangwen Wang; Bo Lin; Liqian Chen; Xiaoguang Mao",
    "corresponding_authors": "",
    "abstract": "Despite its effectiveness in ensuring software quality, code review remains a labor-intensive and time-consuming task. In order to alleviate this burden on developers, researchers have proposed the automation of code review activities, particularly focusing on automating code revisions. This automation can benefit both code authors, as they are relieved from the manual task of code revision, and code reviewers, as they are spared from addressing minor code flaws through manual comments. While current code revision approaches have shown promising results, they typically operate within a single phase, in which the code requiring revision is treated as the input of a deep learning model, and the revised code is directly generated through a sequence-to-sequence transformation. Consequently, these approaches tackle both the challenges of localization (i.e., where to revise) and revision (i.e., how to revise) simultaneously. Attempting to handle the entire complex process with a single model goes against the principle of “Divide-and-Conquer”, which encourages breaking down complex problems into smaller sub-problems and addressing them individually. In fact, we have observed that existing code revision approaches often yield inaccurate results in both the localization and revision phases. In this paper, we present a two-phase code revision approach that aims to overcome the aforementioned limitations by adhering to the “Divide-and-Conquer” principle. Our approach comprises two key components: a localizer, responsible for identifying the specific parts of the input code that require revisions, and a reviser, tasked with generating the revised code based on the localization result. Extensive experiments conducted on two widely-used datasets demonstrate the substantial superiority of our approach over existing code revision approaches. For instance, when revising code based on the code reviewer’s comments, our approach achieves a success rate of over 20% in implementing the ground-truth code revisions. In comparison, the widely-used pre-trained model CodeT5 achieves a success rate of less than 16% on the same test set, which contains 16K+ cases.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402759180",
    "type": "article"
  },
  {
    "title": "<u>Tra</u> ined <u>Wi</u> thout My <u>C</u> onsent: Detecting Code Inclusion In Language Models Trained on Code",
    "doi": "https://doi.org/10.1145/3702980",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Vahid Majdinasab; Amin Nikanjam; Foutse Khomh",
    "corresponding_authors": "",
    "abstract": "Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers’ codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM’s training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404002909",
    "type": "article"
  },
  {
    "title": "Temporal-Incremental Learning for Android Malware Detection",
    "doi": "https://doi.org/10.1145/3702990",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Tiezhu Sun; Nadia Daoudi; Weiguo Pian; Kisub Kim; Kevin Allix; Tegawendé F. Bissyandé; Jacques Klein",
    "corresponding_authors": "",
    "abstract": "Malware classification is a specific and refined task within the broader malware detection problem. Effective classification aids in understanding attack techniques and developing robust defenses, ensuring application security and timely mitigation of software vulnerabilities. The dynamic nature of malware demands adaptive classification techniques that can handle the continuous emergence of new families. Traditionally, this is done by retraining models on all historical samples, which requires significant resources in terms of time and storage. An alternative approach is Class-Incremental Learning (CIL), which focuses on progressively learning new classes (malware families) while preserving knowledge from previous training steps. However, CIL assumes that each class appears only once in training and is not revisited, an assumption that does not hold for malware families, which often persist across multiple time intervals. This leads to shifts in the data distribution for the same family over time, a challenge that is not addressed by traditional CIL methods. We formulate this problem as Temporal-Incremental Malware Learning (TIML), which adapts to these shifts and effectively classifies new variants. To support this, we organize the MalNet dataset, consisting of over a million entries of Android malware data collected over a decade, in chronological order. We first adapt state-of-the-art CIL approaches to meet TIML's requirements, serving as baseline methods. Then, we propose a novel multimodal TIML approach that leverages multiple malware modalities for improved performance. Extensive evaluations show that our TIML approaches outperform traditional CIL methods and demonstrate the feasibility of periodically updating malware classifiers at a low cost. This process is efficient and requires minimal storage and computational resources, with only a slight dip in performance compared to full retraining with historical data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404060117",
    "type": "article"
  },
  {
    "title": "Leveraging Modular Architecture for Bug Characterization and Analysis in Automated Driving Software",
    "doi": "https://doi.org/10.1145/3707455",
    "publication_date": "2024-12-06",
    "publication_year": 2024,
    "authors": "Yingjie Jiang; Ran Mo; Wenjing Zhan; Dongyu Wang; Zengyang Li; Yutao Ma",
    "corresponding_authors": "",
    "abstract": "With the rapid advancement of automated driving technology, numerous manufacturers deploy vehicles with auto-driving features. This highlights the importance of ensuring the quality of automated driving software. To achieve this, characterizing bugs in automated driving software is important, as it can facilitate bug detection and bug fixes, thereby ensuring software quality. Automated driving software typically has a modular architecture, where software is divided into multiple modules, each designed for its own functionality for automated driving. This may lead to varying bug characteristics. Additionally, our recent study has shown a correlation between bugs caused by code clones and the functionalities of modules in automated driving software. Hence, we consider the modular structure when analyzing bug characteristics. In this paper, we analyze 3,078 bugs from two representative open-source Level-4 automated driving systems, Apollo and Autoware. By analyzing the bug report description, title, and developers’ discussions, we have identified 20 bug symptoms and 17 bug-fixing strategies, and analyzed their relationships with the respective modules. Our analysis achieves 12 main findings offering a comprehensive view of bug characteristics in automated driving software. We believe our findings can help developers better understand and manage bugs in automated driving software, thereby improving software quality and reliability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405109287",
    "type": "article"
  },
  {
    "title": "Prioritizing Speech Test Cases",
    "doi": "https://doi.org/10.1145/3707450",
    "publication_date": "2024-12-09",
    "publication_year": 2024,
    "authors": "Zhou Yang; Jieke Shi; Muhammad Hilmi Asyrofi; Bowen Xu; Xin Zhou; Dong‐Gyun Han; David Lo",
    "corresponding_authors": "",
    "abstract": "As automated speech recognition (ASR) systems gain widespread acceptance, there is a pressing need to rigorously test and enhance their performance. Nonetheless, the process of collecting and executing speech test cases is typically both costly and time-consuming. This presents a compelling case for the strategic prioritization of speech test cases, which consist of a piece of audio and the corresponding reference text . The central question we address is: In what sequence should speech test cases be collected and executed to identify the maximum number of errors at the earliest stage? In this study, we introduce Prophet ( PR i O ritising s P eec H t E s T cases), a tool designed to predict the likelihood that speech test cases will identify errors. Consequently, Prophet can assess and prioritize these test cases without having to run the ASR system, facilitating large-scale analysis. Our evaluation encompasses \\(6\\) distinct prioritization techniques across \\(3\\) ASR systems and \\(12\\) datasets. When constrained by the same test budget, our approach identified \\(15.44\\%\\) more misrecognized words than the leading the state-of-the-art method. We select top-ranked speech test cases from the prioritized list to fine-tune ASR systems and analyze how our approach can improve the ASR system performance. Statistical evaluations show that our method delivers a considerably higher performance boost for ASR systems compared to established baseline techniques. Moreover, our correlation analysis confirms that fine-tuning an ASR system with a dataset where the model initially underperforms tends to yield greater performance improvements.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405185384",
    "type": "article"
  },
  {
    "title": "MeDeT: Medical Device Digital Twins Creation with Few-shot Meta-learning",
    "doi": "https://doi.org/10.1145/3708534",
    "publication_date": "2024-12-14",
    "publication_year": 2024,
    "authors": "Hassan Sartaj; Shaukat Ali; Julie Marie Gjøby",
    "corresponding_authors": "",
    "abstract": "Testing healthcare Internet of Things (IoT) applications at system and integration levels necessitates integrating numerous medical devices. Challenges of incorporating medical devices are: (i) their continuous evolution, making it infeasible to include all device variants, and (ii) rigorous testing at scale requires multiple devices and their variants, which is time-intensive, costly, and impractical. Our collaborator, Oslo City’s health department, faced these challenges in developing automated test infrastructure, which our research aims to address. In this context, we propose a meta-learning-based approach (MeDeT) to generate digital twins (DTs) of medical devices and adapt DTs to evolving devices. We evaluate MeDeT in Oslo City’s context using five widely-used medical devices integrated with a real-world healthcare IoT application. Our evaluation assesses MeDeT’s ability to generate and adapt DTs across various devices and versions using different few-shot methods, the fidelity of these DTs, the scalability of operating 1000 DTs concurrently, and the associated time costs. Results show that MeDeT can generate DTs with over 96% fidelity, adapt DTs to different devices and newer versions with reduced time cost (around one minute), and operate 1000 DTs in a scalable manner while maintaining the fidelity level, thus serving in place of physical devices for testing.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405396699",
    "type": "article"
  },
  {
    "title": "Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning",
    "doi": "https://doi.org/10.1145/3708529",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Zhihao Lin; Wei Ma; Tao Lin; Yaowen Zheng; Jingquan Ge; Jun Wang; Jacques Klein; Tegawendé F. Bissyandé; Yang Liu; Li Li",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. AI code models has demonstrated their value not only in code generating but also in defect detection, enhancing security measures, and improving overall software quality. They are emerging as crucial tools for both software development and maintaining software quality. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organizations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on federated learning, our research examines the effect of code data heterogeneity on federated learning performance. We consider 6 different scenarios of data distributions and include 4 code models. We also include 4 most common federated learning algorithms. Our experimental findings highlight the potential for employing federated learning in the collaborative development and maintenance of AI-based software engineering models. We also discuss the key issues to be addressed in the co-construction process and future research directions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405544059",
    "type": "article"
  },
  {
    "title": "TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments When Attackers have Pre-Trained Models",
    "doi": "https://doi.org/10.1145/3707453",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Ding Li; Ziqi Zhang; Mengyu Yao; Yifeng Cai; Yao Guo; Xiangqun Chen",
    "corresponding_authors": "",
    "abstract": "Trusted Execution Environments (TEE) are used to safeguard on-device models. However, directly employing TEEs to secure the entire DNN model is challenging due to the limited computational speed. Utilizing GPU can accelerate DNN's computation speed but commercial widely-available GPUs usually lack security protection. To this end, scholars introduce TEE-shielded DNN partition (TSDP), a method that protects privacy-sensitive weights within TEEs and offloads insensitive weights to GPUs. Nevertheless, current methods do not consider the presence of a knowledgeable adversary who can access abundant publicly available pre-trained models and datasets. This paper investigates the security of existing methods against such a knowledgeable adversary and reveals their inability to fulfill their security promises. Consequently, we introduce a novel partition before training strategy, which effectively separates privacy-sensitive weights from other components of the model. Our evaluation demonstrates that our approach can offer full model protection with a computational cost reduced by a factor of 10. In addition to traditional CNN models, we also demonstrate the scalability to large language models. Our approach can compress the private functionalities of the large language model to lightweight slices and achieve the same level of protection as the shielding-whole-model baseline.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405544354",
    "type": "article"
  },
  {
    "title": "Software Security Analysis in 2030 and Beyond: A Research Roadmap",
    "doi": "https://doi.org/10.1145/3708533",
    "publication_date": "2024-12-19",
    "publication_year": 2024,
    "authors": "Marcel Böhme; Eric Bodden; Tevfik Bultan; Cristian Cadar; Yang Liu; Giuseppe Scanniello",
    "corresponding_authors": "",
    "abstract": "As our lives, our businesses, and indeed our world economy become increasingly reliant on the secure operation of many interconnected software systems, the software engineering research community is faced with unprecedented research challenges, but also with exciting new opportunities. In this roadmap paper, we outline our vision of Software Security Analysis for the systems of the future. Given the recent advances in generative AI, we need new methods to assess and maximize the security of code co-written by machines. As our systems become increasingly heterogeneous, we need practical approaches that work even if some functions are automatically generated, e.g., by deep neural networks. As software systems depend evermore on the software supply chain, we need tools that scale to an entire ecosystem. What kind of vulnerabilities exist in future systems and how do we detect them? When all the shallow bugs are found, how do we discover vulnerabilities hidden deeply in the system? Assuming we cannot find all security flaws, how can we nevertheless protect our system? To answer these questions, we start our roadmap with a survey of recent advances in software security, then discuss open challenges and opportunities, and conclude with a long-term perspective for the field.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405569873",
    "type": "article"
  },
  {
    "title": "Sustainable Software Engineering: Concepts, Challenges, and Vision",
    "doi": "https://doi.org/10.1145/3709352",
    "publication_date": "2024-12-21",
    "publication_year": 2024,
    "authors": "Christoph König; Daniel J. Lang; Ina Schaefer",
    "corresponding_authors": "",
    "abstract": "Information and communication technology (ICT) offers promising opportunities to address global sustainability challenges such as climate change and social inequality by enabling energy savings and social innovations. At the same time, ICT threatens to exacerbate these crises, as evident in the increasing consumption of resources and widening digital inequalities. As one of the enablers of ICT, software engineering plays a key role to tackle the problems and explore the potentials of ICT for sustainability. However, sustainability in software engineering is still a niche topic, with little structure, a limited understanding of sustainability and few comprehensive strategies. In this paper, we introduce the main concepts of sustainable software engineering, critically review the state of research and identify seven future research challenges across all research areas. We further present our research vision – sustainability-driven software engineering and transdisciplinary research formats – and outline a research roadmap with the key steps to be achieved by 2030.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405675722",
    "type": "article"
  },
  {
    "title": "A knowledge-based method for inferring semantic concepts from visual models of system behavior",
    "doi": "https://doi.org/10.1145/352591.352594",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Kevin L. Mills; Hassan Gomaa",
    "corresponding_authors": "",
    "abstract": "Software designers use visual models, such as data flow/control flow diagrams or object collaboration diagrams, to express system behavior in a form that can be understood easily by users and by pogrammers, and from which designers can generate a software architecture. The research described in this paper is motivated by a desire to provide an automated designer's assistant that can generate software architectures for concurrent systems directly from behavioral models expressed visually as flow diagrams. To achieve this goal, an automated designer's assistant must be capable of interpreting flow diagrams in semantic, rather than syntactic, terms. While semantic concepts can be attached manually to diagrams using labels, such as stereotypes in the Unified Model Language (UML), this paper considers the possibility of providing autmated assistance to infer appropriate tags for symbols on a flow diagram. The approach relies upon constructing an underlying metamodel that defines semantic concepts based upon (1) syntactic relationships among visual symbols and (2) inheritance relationships among semantic concepts. Given such a metamodel, a rule-based inference engine can, in many situations, infer the presence of semantic concepts on flow diagram, and can tag symbols accordingly. Futher, an object-oriented query system can compare semantic tags on digram instances for conformance with their definition in the metamodel. To illustrate the approach, the paper describes a metamodel for data flow/control flow diagrams used in the context of a specific software modeling method, Concurrent Object-Based Real-time Analysis (COBRA). The metamodel is implemented using an expert-system shell, CLIPS V6.0, which integrates an object-oriented language with a rule-based inference engine. The paper applies the implemented metamodel to design software for an automobile cruise-control system and provides an evaluation of the approach based upon results from four case studies. For the case studies, the implemented metamodel recognized, automatically and correctly, the existence of 86% of all COBRA semantic concepts within the flow diagrams. Varying degrees of human assistance were used to correctly identify the remaining semantic concepts within the diagrams: in two percent of the cases the implemented metamodel reached tentative classifications that a designer was asked to confirm or override; in four percent of the cases a designer was asked to provide additional information before a concept was classified; in the remaining eight percent of the cases the designer was asked to identify the concept.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2006877631",
    "type": "article"
  },
  {
    "title": "Semantic self-assessment of query results in dynamic environments",
    "doi": "https://doi.org/10.1145/1734229.1734231",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Jamie Payton; Christine Julien; Gruia-Catalin Roman; Vasanth Rajamani",
    "corresponding_authors": "",
    "abstract": "Queries are convenient abstractions for the discovery of information and services, as they offer content-based information access. In distributed settings, query semantics are well-defined, for example, queries are often designed to satisfy ACID transactional properties. When query processing is introduced in a dynamic network setting, achieving transactional semantics becomes complex due to the open and unpredictable environment. In this article, we propose a query processing model for mobile ad hoc and sensor networks that is suitable for expressing a wide range of query semantics; the semantics differ in the degree of consistency with which query results reflect the state of the environment during query execution. We introduce several distinct notions of consistency and formally express them in our model. A practical and significant contribution of this article is a protocol for query processing that automatically assesses and adaptively provides an achievable degree of consistency given the operational environment throughout its execution. The protocol attaches an assessment of the achieved guarantee to returned query results, allowing precise reasoning about a query with a range of possible semantics. We evaluate the performance of this protocol and demonstrate the benefits accrued to applications through examples drawn from an industrial application.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2079210504",
    "type": "article"
  },
  {
    "title": "Test-and-adapt",
    "doi": "https://doi.org/10.1145/2522920.2522921",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Giovanni Denaro; Mauro Pezzè; Davide Tosi",
    "corresponding_authors": "",
    "abstract": "Service-oriented applications do not fully benefit from standard APIs yet, and many applications fail to use interchangeably all the services that implement a standard service API. This article presents an approach to develop adaptation strategies that improve service interchangeability for service-oriented applications based on standard APIs. In our approach, an adaptation strategy consists of sets of parametric adaptation plans (called test-and-adapt plans), which execute test cases to reveal the occurrence of interchangeability problems, and activate runtime adaptors according to the test results. Throughout this article, we formalize the structure of the parametric test-and-adapt plans and of their execution semantics, present an algorithm for identifying correct execution orders through sets of test-and-adapt plans, provide empirical evidence of the occurrence of interchangeability problems for sample applications and services, and discuss the effectiveness of the approach in terms of avoided failures, runtime overheads and development costs.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2014179439",
    "type": "article"
  },
  {
    "title": "Using a functional size measurement procedure to evaluate the quality of models in MDD environments",
    "doi": "https://doi.org/10.1145/2491509.2491520",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Beatriz Marín; Giovanni Giachetti; Óscar Pastor; Tanja E. J. Vos; Alain Abran",
    "corresponding_authors": "",
    "abstract": "Models are key artifacts in Model-Driven Development (MDD) methods. To produce high-quality software by using MDD methods, quality assurance of models is of paramount importance. To evaluate the quality of models, defect detection is considered a suitable approach and is usually applied using reading techniques. However, these reading techniques have limitations and constraints, and new techniques are required to improve the efficiency at finding as many defects as possible. This article presents a case study that has been carried out to evaluate the use of a Functional Size Measurement (FSM) procedure in the detection of defects in models of an MDD environment. To do this, we compare the defects and the defect types found by an inspection group with the defects and the defect types found by the FSM procedure. The results indicate that the FSM is useful since it finds all the defects related to a specific defect type, it finds different defect types than an inspection group, and it finds defects related to the correctness and the consistency of the models.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2056899516",
    "type": "article"
  },
  {
    "title": "Recommending Who to Follow in the Software Engineering Twitter Space",
    "doi": "https://doi.org/10.1145/3266426",
    "publication_date": "2018-10-22",
    "publication_year": 2018,
    "authors": "Abhishek Sharma; Yuan Tian; Agus Sulistya; Dinusha Wijedasa; David Lo",
    "corresponding_authors": "",
    "abstract": "With the advent of social media, developers are increasingly using it in their software development activities. Twitter is one of the popular social mediums used by developers. A recent study by Singer et al. found that software developers use Twitter to “keep up with the fast-paced development landscape.” Unfortunately, due to the general-purpose nature of Twitter, it’s challenging for developers to use Twitter for their development activities. Our survey with 36 developers who use Twitter in their development activities highlights that developers are interested in following specialized software gurus who share relevant technical tweets. To help developers perform this task, in this work we propose a recommendation system to identify specialized software gurus. Our approach first extracts different kinds of features that characterize a Twitter user and then employs a two-stage classification approach to generate a discriminative model, which can differentiate specialized software gurus in a particular domain from other Twitter users that generate domain-related tweets (aka domain-related Twitter users). We have investigated the effectiveness of our approach in finding specialized software gurus for four different domains (JavaScript, Android, Python, and Linux) on a dataset of 86,824 Twitter users who generate 5,517,878 tweets over 1 month. Our approach can differentiate specialized software experts from other domain-related Twitter users with an F-Measure of up to 0.820. Compared with existing Twitter domain expert recommendation approaches, our proposed approach can outperform their F-Measure by at least 7.63%.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2897671622",
    "type": "article"
  },
  {
    "title": "Adversarial Specification Mining",
    "doi": "https://doi.org/10.1145/3424307",
    "publication_date": "2021-01-03",
    "publication_year": 2021,
    "authors": "Hong Jin Kang; David Lo",
    "corresponding_authors": "",
    "abstract": "There have been numerous studies on mining temporal specifications from execution traces. These approaches learn finite-state automata (FSA) from execution traces when running tests. To learn accurate specifications of a software system, many tests are required. Existing approaches generalize from a limited number of traces or use simple test generation strategies. Unfortunately, these strategies may not exercise uncommon usage patterns of a software system. To address this problem, we propose a new approach, adversarial specification mining, and develop a prototype, Diversity through Counter-examples (DICE). DICE has two components: DICE-Tester and DICE-Miner. After mining Linear Temporal Logic specifications from an input test suite, DICE-Tester adversarially guides test generation, searching for counterexamples to these specifications to invalidate spurious properties. These counterexamples represent gaps in the diversity of the input test suite. This process produces execution traces of usage patterns that were unrepresented in the input test suite. Next, we propose a new specification inference algorithm, DICE-Miner, to infer FSAs using the traces, guided by the temporal specifications. We find that the inferred specifications are of higher quality than those produced by existing state-of-the-art specification miners. Finally, we use the FSAs in a fuzzer for servers of stateful protocols, increasing its coverage.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3120742815",
    "type": "article"
  },
  {
    "title": "Diversifying Focused Testing for Unit Testing",
    "doi": "https://doi.org/10.1145/3447265",
    "publication_date": "2021-04-19",
    "publication_year": 2021,
    "authors": "Héctor D. Menéndez; Gunel Jahangirova; Federica Sarro; Paolo Tonella; David Clark",
    "corresponding_authors": "",
    "abstract": "Software changes constantly, because developers add new features or modifications. This directly affects the effectiveness of the test suite associated with that software, especially when these new modifications are in a specific area that no test case covers. This article tackles the problem of generating a high-quality test suite to cover repeatedly a given point in a program, with the ultimate goal of exposing faults possibly affecting the given program point. Both search-based software testing and constraint solving offer ready, but low-quality, solutions to this: Ideally, a maximally diverse covering test set is required, whereas search and constraint solving tend to generate test sets with biased distributions. Our approach, Diversified Focused Testing (DFT), uses a search strategy inspired by GödelTest. We artificially inject parameters into the code branching conditions and use a bi-objective search algorithm to find diverse inputs by perturbing the injected parameters, while keeping the path conditions still satisfiable. Our results demonstrate that our technique, DFT, is able to cover a desired point in the code at least 90% of the time. Moreover, adding diversity improves the bug detection and the mutation killing abilities of the test suites. We show that DFT achieves better results than focused testing, symbolic execution, and random testing by achieving from 3% to 70% improvement in mutation score and up to 100% improvement in fault detection across 105 software subjects.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3156195940",
    "type": "article"
  },
  {
    "title": "Bug Localization in Model-Based Systems in the Wild",
    "doi": "https://doi.org/10.1145/3472616",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Lorena Arcega; Jaime Font; Øystein Haugen; Carlos Cetina",
    "corresponding_authors": "",
    "abstract": "The companies that have adopted the Model-Driven Engineering (MDE) paradigm have the advantage of working at a high level of abstraction. Nevertheless, they have the disadvantage of the lack of tools available to perform bug localization at the model level. In addition, in an MDE context, a bug can be related to different MDE artefacts, such as design-time models, model transformations, or run-time models. Starting the bug localization in the wrong place or with the wrong tool can lead to a result that is unsatisfactory. We evaluate how to apply the existing model-based approaches in order to mitigate the effect of starting the localization in the wrong place. We also take into account that software engineers can refine the results at different stages. In our evaluation, we compare different combinations of the application of bug localization approaches and human refinement. The combination of our approaches plus manual refinement obtains the best results. We performed a statistical analysis to provide evidence of the significance of the results. The conclusions obtained from this evaluation are: humans have to be involved at the right time in the process (or results can even get worse), and artefact-independence can be achieved without worsening the results.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3209090657",
    "type": "article"
  },
  {
    "title": "Measuring and Modeling Group Dynamics in Open-Source Software Development: A Tensor Decomposition Approach",
    "doi": "https://doi.org/10.1145/3473139",
    "publication_date": "2021-11-17",
    "publication_year": 2021,
    "authors": "Thomas Böck; A. Schmid; Sven Apel",
    "corresponding_authors": "",
    "abstract": "Many open-source software projects depend on a few core developers, who take over both the bulk of coordination and programming tasks. They are supported by peripheral developers, who contribute either via discussions or programming tasks, often for a limited time. It is unclear what role these peripheral developers play in the programming and communication efforts, as well as the temporary task-related sub-groups in the projects. We mine code-repository data and mailing-list discussions to model the relationships and contributions of developers in a social network and devise a method to analyze the temporal collaboration structures in communication and programming, learning about the strength and stability of social sub-groups in open-source software projects. Our method uses multi-modal social networks on a series of time windows. Previous work has reduced the network structure representing developer collaboration to networks with only one type of interaction, which impedes the simultaneous analysis of more than one type of interaction. We use both communication and version-control data of open-source software projects and model different types of interaction over time. To demonstrate the practicability of our measurement and analysis method, we investigate 10 substantial and popular open-source software projects and show that, if sub-groups evolve, modeling these sub-groups helps predict the future evolution of interaction levels of programmers and groups of developers. Our method allows maintainers and other stakeholders of open-source software projects to assess instabilities and organizational changes in developer interaction and can be applied to different use cases in organizational analysis, such as understanding the dynamics of a specific incident or discussion.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3214057859",
    "type": "article"
  },
  {
    "title": "A Continuous ASM Modelling Approach to Pacemaker Sensing",
    "doi": "https://doi.org/10.1145/2610375",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Richard Banach; Huibiao Zhu; Wen Su; Xiaofeng Wu",
    "corresponding_authors": "",
    "abstract": "The cardiac pacemaker system, proposed as a problem topic in the Verification Grand Challenge, offers a range of difficulties to address for formal specification, development, and verification technologies. We focus on the sensing problem, the question of whether the heart has produced a spontaneous heartbeat or not. This question is plagued by uncertainties arising from the often unpredictable environment that a real pacemaker finds itself in. We develop a time domain tracking approach to this problem, as a complement to the usual frequency domain approach most frequently used. We develop our case study in the continuous ASM (Abstract State Machine) formalism, which is briefly summarised, through a series of refinement and retrenchment steps, each adding new levels of complexity to the model.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2046102833",
    "type": "article"
  },
  {
    "title": "Do Developers Really Know How to Use Git Commands? A Large-scale Study Using Stack Overflow",
    "doi": "https://doi.org/10.1145/3494518",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Wenhua Yang; Chong Zhang; Minxue Pan; Chang Xu; Yu Zhou; Zhiqiu Huang",
    "corresponding_authors": "",
    "abstract": "Git, a cross-platform and open source distributed version control tool, provides strong support for non-linear development and is capable of handling everything from small to large projects with speed and efficiency. It has become an indispensable tool for millions of software developers and is the de facto standard of version control in software development nowadays. However, despite its widespread use, developers still frequently face difficulties when using various Git commands to manage projects and collaborate. To better help developers use Git, it is necessary to understand the issues and difficulties that they may encounter when using Git. Unfortunately, this problem has not yet been comprehensively studied. To fill this knowledge gap, in this article, we conduct a large-scale study on Stack Overflow, a popular Q&amp;A forum for developers. We extracted and analyzed 80,370 relevant questions from Stack Overflow, and reported the increasing popularity of the Git command questions. By analyzing the questions, we identified the Git commands that are frequently asked and those that are associated with difficult questions on Stack Overflow to help understand the difficulties developers may encounter when using Git commands. In addition, we conducted a survey to understand how developers learn Git commands in practice, showing that self-learning is the primary learning approach. These findings provide a range of actionable implications for researchers, educators, and developers.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4210531528",
    "type": "article"
  },
  {
    "title": "Buddy Stacks: Protecting Return Addresses with Efficient Thread-Local Storage and Runtime Re-Randomization",
    "doi": "https://doi.org/10.1145/3494516",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Changwei Zou; Xudong Wang; Yaoqing Gao; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Shadow stacks play an important role in protecting return addresses to mitigate ROP attacks. Parallel shadow stacks, which shadow the call stack of each thread at the same constant offset for all threads, are known not to support multi-threading well. On the other hand, compact shadow stacks must maintain a separate shadow stack pointer in thread-local storage (TLS) , which can be implemented in terms of a register or the per-thread Thread-Control-Block (TCB) , suffering from poor compatibility in the former or high performance overhead in the latter. In addition, shadow stacks are vulnerable to information disclosure attacks. In this paper, we propose to mitigate ROP attacks for single- and multi-threaded server programs running on general-purpose computing systems by using a novel stack layout, called a buddy stack (referred to as Bustk ), that is highly performant, compatible with existing code, and provides meaningful security. These goals are met due to three novel design aspects in Bustk . First, Bustk places a parallel shadow stack just below a thread’s call stack (as each other’s buddies allocated together), avoiding the need to maintain a separate shadow stack pointer and making it now well-suited for multi-threading. Second, Bustk uses an efficient stack-based thread-local storage mechanism, denoted STK-TLS , to store thread-specific metadata in two TLS sections just below the shadow stack in dual redundancy (as each other’s buddies), so that both can be accessed and updated in a lightweight manner from the call stack pointer rsp alone. Finally, Bustk re-randomizes continuously (on the order of milliseconds) the return addresses on the shadow stack by using a new microsecond-level runtime re-randomization technique, denoted STK-MSR . This mechanism aims to obsolete leaked information, making it extremely unlikely for the attacker to hijack return addresses, particularly against a server program that sits often tens of milliseconds away from the attacker. Our evaluation using web servers, Nginx and Apache Httpd , shows that Bustk works well in terms of performance, compatibility, and security provided, with its parallel shadow stacks incurring acceptable memory overhead for real-world applications and its STK-TLS mechanism costing only two pages per thread. In particular, Bustk can protect the Nginx and Apache servers with an adaptive 1-ms re-randomization policy (without observable overheads when IO is intensive, with about 17,000 requests per second). In addition, we have also evaluated Bustk using other non-server applications, Firefox , Python , LLVM , JDK and SPEC CPU2006 , to demonstrate further the same degree of performance and compatibility provided, but the protection provided for, say, browsers, is weaker (since network-access delays can no longer be assumed).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4214849268",
    "type": "article"
  },
  {
    "title": "Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph Convolutional Networks",
    "doi": "https://doi.org/10.1145/3542944",
    "publication_date": "2022-06-08",
    "publication_year": 2022,
    "authors": "Zishuo Ding; Heng Li; Weiyi Shang; Tse-Hsun Chen",
    "corresponding_authors": "",
    "abstract": "Code embeddings have seen increasing applications in software engineering (SE) research and practice recently. Despite the advances in embedding techniques applied in SE research, one of the main challenges is their generalizability. A recent study finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not particularly trained for. Therefore, in this article, we propose GraphCodeVec , which represents the source code as graphs and leverages the Graph Convolutional Networks to learn more generalizable code embeddings in a task-agnostic manner. The edges in the graph representation are automatically constructed from the paths in the abstract syntax trees, and the nodes from the tokens in the source code. To evaluate the effectiveness of GraphCodeVec , we consider three downstream benchmark tasks (i.e., code comment generation, code authorship identification, and code clones detection) that are used in a prior benchmarking of code embeddings and add three new downstream tasks (i.e., source code classification, logging statements prediction, and software defect prediction), resulting in a total of six downstream tasks that are considered in our evaluation. For each downstream task, we apply the embeddings learned by GraphCodeVec and the embeddings learned from four baseline approaches and compare their respective performance. We find that GraphCodeVec outperforms all the baselines in five out of the six downstream tasks, and its performance is relatively stable across different tasks and datasets. In addition, we perform ablation experiments to understand the impacts of the training context (i.e., the graph context extracted from the abstract syntax trees) and the training model (i.e., the Graph Convolutional Networks) on the effectiveness of the generated embeddings. The results show that both the graph context and the Graph Convolutional Networks can benefit GraphCodeVec in producing high-quality embeddings for the downstream tasks, while the improvement by Graph Convolutional Networks is more robust across different downstream tasks and datasets. Our findings suggest that future research and practice may consider using graph-based deep learning methods to capture the structural information of the source code for SE tasks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4282033849",
    "type": "article"
  },
  {
    "title": "A Characterization Study of Merge Conflicts in Java Projects",
    "doi": "https://doi.org/10.1145/3546944",
    "publication_date": "2022-07-06",
    "publication_year": 2022,
    "authors": "Bowen Shen; Muhammad Ali Gulzar; Fei He; Na Meng",
    "corresponding_authors": "",
    "abstract": "In collaborative software development, programmers create software branches to add features and fix bugs tentatively, and then merge branches to integrate edits. When edits from different branches textually overlap (i.e., textual conflicts ) or lead to compilation and runtime errors (i.e., build and test conflicts ), it is challenging for developers to remove such conflicts. Prior work proposed tools to detect and solve conflicts. They investigate how conflicts relate to code smells and the software development process. However, many questions are still not fully investigated, such as what types of conflicts exist in real-world applications and how developers or tools handle them. For this article, we used automated textual merge, compilation, and testing to reveal three types of conflicts in 208 open-source repositories: textual conflicts, build conflicts (i.e., conflicts causing build errors), and test conflicts (i.e., conflicts triggering test failures). We manually inspected 538 conflicts and their resolutions to characterize merge conflicts from different angles. Our analysis revealed three interesting phenomena. First, higher-order conflicts (i.e., build and test conflicts) are harder to detect and resolve, while existing tools mainly focus on textual conflicts. Second, developers manually resolved most higher-order conflicts by applying similar edits to multiple program locations; their conflict resolutions share common editing patterns implying great opportunities for future tool design. Third, developers resolved 64% of true textual conflicts by keeping complete edits from either a left or right branch. Unlike prior studies, our research for the first time thoroughly characterizes three types of conflicts, with a special focus on higher-order conflicts and limitations of existing tool design. Our work will shed light on future research of software merge.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4283831104",
    "type": "article"
  },
  {
    "title": "Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices",
    "doi": "https://doi.org/10.1145/3546949",
    "publication_date": "2022-07-08",
    "publication_year": 2022,
    "authors": "Chao Wang; Hao He; Uma Pal; Darko Marinov; Minghui Zhou",
    "corresponding_authors": "",
    "abstract": "High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1) independent comment changes (ICCs) —comment changes committed independently of code changes—which likely address suboptimal comments, (2) commenting guidelines, and (3) comment-checking tools and comment-generating tools, which are often employed to help commenting practice—especially to prevent suboptimal comments. We collect 24M+ comment changes from 4,392 open-source GitHub Java repositories and find that ICCs widely exist. The ICC ratio —proportion of ICCs among all comment changes—is ~15.5%, with 98.7% of the repositories having ICC. Our thematic analysis of 3,533 randomly sampled ICCs provides a three-dimensional taxonomy for what is changed (four comment categories and 13 subcategories), how it changed (six commenting activity categories), and what factors are associated with the change (three factors). We investigate 600 repositories to understand the prevalence, content, impact, and violations of commenting guidelines. We find that only 15.5% of the 600 sampled repositories have any commenting guidelines. We provide the first taxonomy for elements in commenting guidelines: where and what to comment are particularly important. The repositories without such guidelines have a statistically significantly higher ICC ratio, indicating the negative impact of the lack of commenting guidelines. However, commenting guidelines are not strictly followed: 85.5% of checked repositories have violations. We also systematically study how developers use two kinds of tools, comment-checking tools and comment-generating tools, in the 4,392 repositories. We find that the use of Javadoc tool is negatively correlated with the ICC ratio, while the use of Checkstyle has no statistically significant correlation; the use of comment-generating tools leads to a higher ICC ratio. To conclude, we reveal issues and challenges in current commenting practice, which help understand how suboptimal comments are introduced. We propose potential research directions on comment location prediction, comment generation, and comment quality assessment; suggest how developers can formulate commenting guidelines and enforce rules with tools; and recommend how to enhance current comment-checking and comment-generating tools.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4284889817",
    "type": "article"
  },
  {
    "title": "Estimating Probabilistic Safe WCET Ranges of Real-Time Systems at Design Stages",
    "doi": "https://doi.org/10.1145/3546941",
    "publication_date": "2022-07-09",
    "publication_year": 2022,
    "authors": "Jaekwon Lee; Seung Yeob Shin; Shiva Nejati; Lionel Briand; Yago Isasi Parache",
    "corresponding_authors": "",
    "abstract": "Estimating worst-case execution times (WCET) is an important activity at early design stages of real-time systems. Based on WCET estimates, engineers make design and implementation decisions to ensure that task executions always complete before their specified deadlines. However, in practice, engineers often cannot provide precise point WCET estimates and prefer to provide plausible WCET ranges. Given a set of real-time tasks with such ranges, we provide an automated technique to determine for what WCET values the system is likely to meet its deadlines, and hence operate safely with a probabilistic guarantee. Our approach combines a search algorithm for generating worst-case scheduling scenarios with polynomial logistic regression for inferring probabilistic safe WCET ranges. We evaluated our approach by applying it to three industrial systems from different domains and several synthetic systems. Our approach efficiently and accurately estimates probabilistic safe WCET ranges within which deadlines are likely to be satisfied with a high degree of confidence.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4284962442",
    "type": "article"
  },
  {
    "title": "Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization",
    "doi": "https://doi.org/10.1145/3517036",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Zijing Yin; Yiwen Xu; Fuchen Ma; Haohao Gao; Lei Qiao; Yu Jiang",
    "corresponding_authors": "",
    "abstract": "Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scanners’ request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scanners’ detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance. For evaluation, we implemented and integrated Scanner++ together with four widely used scanners, BurpSuite, AWVS, Arachni, and ZAP, testing it on ten benchmark web applications and three well-tested real-world web applications of a critical financial platform from our industry partner. Working under the Scanner++ framework helps BurpSuite, AWVS, Arachni, and ZAP cover 15.26%, 37.14%, 59.21%, 68.54% more pages, construct 12.95×, 1.13×, 15.03×, 52.66× more attack packets, and discover 77, 55, 77, 176 more bugs, respectively. Furthermore, Scanner++ detected eight serious previously unknown vulnerabilities on real-world applications, while the base scanners only found three of them.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4293235803",
    "type": "article"
  },
  {
    "title": "Precise Quantitative Analysis of Binarized Neural Networks: A BDD-based Approach",
    "doi": "https://doi.org/10.1145/3563212",
    "publication_date": "2022-09-10",
    "publication_year": 2022,
    "authors": "Yedi Zhang; Zhe Zhao; Guangke Chen; Fu Song; Taolue Chen",
    "corresponding_authors": "",
    "abstract": "As a new programming paradigm, neural-network-based machine learning has expanded its application to many real-world problems. Due to the black-box nature of neural networks, verifying and explaining their behavior are becoming increasingly important, especially when they are deployed in safety-critical applications. Existing verification work mostly focuses on qualitative verification, which asks whether there exists an input (in a specified region) for a neural network such that a property (e.g., local robustness) is violated. However, in many practical applications, such an (adversarial) input almost surely exists, which makes a qualitative answer less meaningful. In this work, we study a more interesting yet more challenging problem, i.e., quantitative verification of neural networks, which asks how often a property is satisfied or violated. We target binarized neural networks (BNNs), the 1-bit quantization of general neural networks. BNNs have attracted increasing attention in deep learning recently, as they can drastically reduce memory storage and execution time with bit-wise operations, which is crucial in recourse-constrained scenarios, e.g., embedded devices for Internet of Things. Toward quantitative verification of BNNs, we propose a novel algorithmic approach for encoding BNNs as Binary Decision Diagrams (BDDs), a widely studied model in formal verification and knowledge representation. By exploiting the internal structure of the BNNs, our encoding translates the input-output relation of blocks in BNNs to cardinality constraints, which are then encoded by BDDs. Based on the new BDD encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. To improve the scalability of BDD encoding, we also investigate parallelization strategies at various levels. We demonstrate applications of our framework by providing quantitative robustness verification and interpretability for BNNs. An extensive experimental evaluation confirms the effectiveness and efficiency of our approach.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4296270209",
    "type": "article"
  },
  {
    "title": "Consent Verification Monitoring",
    "doi": "https://doi.org/10.1145/3490754",
    "publication_date": "2022-06-29",
    "publication_year": 2022,
    "authors": "Marco Robol; Travis D. Breaux; Elda Paja; Paolo Giorgini",
    "corresponding_authors": "",
    "abstract": "Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include: (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios; (ii) a scripting language that implements the consent framework for encoding and executing different scenarios; (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework; and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real-time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access, and practice and policy evolution.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4300765175",
    "type": "article"
  },
  {
    "title": "Demystifying Hidden Sensitive Operations in Android Apps",
    "doi": "https://doi.org/10.1145/3574158",
    "publication_date": "2022-12-05",
    "publication_year": 2022,
    "authors": "Xiaoyu Sun; Xiao Chen; Li Li; Haipeng Cai; John G. Grundy; Jordan Samhi; Tegawendé F. Bissyandé; Jacques Klein",
    "corresponding_authors": "",
    "abstract": "Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4311596986",
    "type": "article"
  },
  {
    "title": "Toward Interpretable Graph Tensor Convolution Neural Network for Code Semantics Embedding",
    "doi": "https://doi.org/10.1145/3582574",
    "publication_date": "2023-02-20",
    "publication_year": 2023,
    "authors": "Jia Yang; Cai Fu; Fengyang Deng; Ming Wen; Xiaowei Guo; Chuanhao Wan",
    "corresponding_authors": "",
    "abstract": "Intelligent deep learning-based models have made significant progress for automated source code semantics embedding, and current research works mainly leverage natural language-based methods and graph-based methods. However, natural language-based methods do not capture the rich semantic structural information of source code, and graph-based methods do not utilize rich distant information of source code due to the high cost of message-passing steps. In this article, we propose a novel interpretable model, called graph tensor convolution neural network (GTCN), to generate accurate code embedding, which is capable of comprehensively capturing the distant information of code sequences and rich code semantics structural information. First, we propose to utilize a high-dimensional tensor to integrate various heterogeneous code graphs with node sequence features, such as control flow, data flow. Second, inspired by the current advantages of graph-based deep learning and efficient tensor computations, we propose a novel interpretable graph tensor convolution neural network for learning accurate code semantic embedding from the code graph tensor. Finally, we evaluate three popular applications on the GTCN model: variable misuse detection, source code prediction, and vulnerability detection. Compared with current state-of-the-art methods, our model achieves higher scores with respect to the top-1 accuracy while costing less training time.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4321372624",
    "type": "article"
  },
  {
    "title": "<scp>DatAFLow</scp> : Toward a Data-Flow-Guided Fuzzer",
    "doi": "https://doi.org/10.1145/3587156",
    "publication_date": "2023-03-10",
    "publication_year": 2023,
    "authors": "Adrian Herrera; Mathias Payer; Antony L. Hosking",
    "corresponding_authors": "",
    "abstract": "Coverage-guided greybox fuzzers rely on control-flow coverage feedback to explore a target program and uncover bugs. Compared to control-flow coverage, data-flow coverage offers a more fine-grained approximation of program behavior. Data-flow coverage captures behaviors not visible as control flow and should intuitively discover more (or different) bugs. Despite this advantage, fuzzers guided by data-flow coverage have received relatively little attention, appearing mainly in combination with heavyweight program analyses (e.g., taint analysis, symbolic execution). Unfortunately, these more accurate analyses incur a high run-time penalty, impeding fuzzer throughput. Lightweight data-flow alternatives to control-flow fuzzing remain unexplored. We present datAFLow , a greybox fuzzer guided by lightweight data-flow profiling. We also establish a framework for reasoning about data-flow coverage, allowing the computational cost of exploration to be balanced with precision. Using this framework, we extensively evaluate datAFLow across different precisions, comparing it against state-of-the-art fuzzers guided by control flow, taint analysis, and data flow. Our results suggest that the ubiquity of control-flow-guided fuzzers is well-founded. The high run-time costs of data-flow-guided fuzzing (~10 × higher than control-flow-guided fuzzing) significantly reduces fuzzer iteration rates, adversely affecting bug discovery and coverage expansion. Despite this, datAFLow uncovered bugs that state-of-the-art control-flow-guided fuzzers (notably, AFL++) failed to find. This was because data-flow coverage revealed states in the target not visible under control-flow coverage. Thus, we encourage the community to continue exploring lightweight data-flow profiling; specifically, to lower run-time costs and to combine this profiling with control-flow coverage to maximize bug-finding potential.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4323864011",
    "type": "article"
  },
  {
    "title": "Predicting the Change Impact of Resolving Defects by Leveraging the Topics of Issue Reports in Open Source Software Systems",
    "doi": "https://doi.org/10.1145/3593802",
    "publication_date": "2023-04-24",
    "publication_year": 2023,
    "authors": "Maram Assi; Safwat Hassan; Stefanos Georgiou; Ying Zou",
    "corresponding_authors": "",
    "abstract": "Upon receiving a new issue report, practitioners start by investigating the defect type, the potential fixing effort needed to resolve the defect and the change impact. Moreover, issue reports contain valuable information, such as, the title, description and severity, and researchers leverage the topics of issue reports as a collective metric portraying similar characteristics of a defect. Nonetheless, none of the existing studies leverage the defect topic, i.e., a semantic cluster of defects of the same nature, such as Performance, GUI, and Database , to estimate the change impact that represents the amount of change needed in terms of code churn and the number of files changed. To this end, in this article, we conduct an empirical study on 298,548 issue reports belonging to three large-scale open-source systems, i.e., Mozilla, Apache, and Eclipse, to estimate the change impact in terms of code churn or the number of files changed while leveraging the topics of issue reports. First, we adopt the Embedded Topic Model (ETM), a state-of-the-art topic modelling algorithm, to identify the topics. Second, we investigate the feasibility of predicting the change impact using the identified topics and other information extracted from the issue reports by building eight prediction models that classify issue reports requiring small or large change impact along two dimensions, i.e., the code churn size and the number of files changed. Our results suggest that XGBoost is the best-performing algorithm for predicting the change impact, with an AUC of 0.84, 0.76, and 0.73 for the code churn and 0.82, 0.71, and 0.73 for the number of files changed metric for Mozilla, Apache, and Eclipse, respectively. Our results also demonstrate that the topics of issue reports improve the recall of the prediction model by up to 45%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4366825753",
    "type": "article"
  },
  {
    "title": "JavaScript SBST Heuristics to Enable Effective Fuzzing of NodeJS Web APIs",
    "doi": "https://doi.org/10.1145/3593801",
    "publication_date": "2023-04-24",
    "publication_year": 2023,
    "authors": "Man Zhang; Asma Belhadi; Andrea Arcuri",
    "corresponding_authors": "",
    "abstract": "JavaScript is one of the most popular programming languages. However, its dynamic nature poses several challenges to automated testing techniques. In this paper, we propose an approach and open-source tool support to enable white-box testing of JavaScript applications using Search-Based Software Testing (SBST) techniques. We provide an automated approach to collect search-based heuristics like the common Branch Distance and to enable Testability Transformations . To empirically evaluate our results, we integrated our technique into the EvoMaster test generation tool, and carried out analyses on the automated system testing of RESTful and GraphQL APIs. Experiments on eight Web APIs running on NodeJS show that our technique leads to significantly better results than existing black-box and grey-box testing tools, in terms of code coverage and fault detection.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4366825970",
    "type": "article"
  },
  {
    "title": "<scp>DatAFLow</scp> : Toward a Data-flow-guided Fuzzer",
    "doi": "https://doi.org/10.1145/3587159",
    "publication_date": "2023-04-24",
    "publication_year": 2023,
    "authors": "Adrian Herrera; Mathias Payer; Antony L. Hosking",
    "corresponding_authors": "",
    "abstract": "This Replicating Computational Report (RCR) describes (a) our datAFLow fuzzer and (b) how to replicate the results in “ datAFLow : Toward a Data-Flow-Guided Fuzzer.” Our primary artifact is the datAFLow fuzzer. Unlike traditional coverage-guided greybox fuzzers—which use control-flow coverage to drive program exploration— datAFLow uses data-flow coverage to drive exploration. This is achieved through a set of LLVM-based analyses and transformations. In addition to datAFLow , we also provide a set of tools, scripts, and patches for (a) statically analyzing data flows in a target program, (b) compiling a target program with the datAFLow instrumentation, (c) evaluating datAFLow on the Magma benchmark suite, and (d) evaluating datAFLow on the DDFuzz dataset. datAFLow is available at https://github.com/HexHive/datAFLow.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4366830184",
    "type": "article"
  },
  {
    "title": "Pre-implementation Method Name Prediction for Object-oriented Programming",
    "doi": "https://doi.org/10.1145/3597203",
    "publication_date": "2023-05-13",
    "publication_year": 2023,
    "authors": "Shangwen Wang; Ming Wen; Bo Lin; Yepang Liu; Tegawendé F. Bissyandé; Xiaoguang Mao",
    "corresponding_authors": "",
    "abstract": "Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario , to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4376503941",
    "type": "article"
  },
  {
    "title": "What Quality Aspects Influence the Adoption of Docker Images?",
    "doi": "https://doi.org/10.1145/3603111",
    "publication_date": "2023-05-31",
    "publication_year": 2023,
    "authors": "Giovanni Rosa; Simone Scalabrino; Gabriele Bavota; Rocco Oliveto",
    "corresponding_authors": "",
    "abstract": "Docker is a containerization technology that allows developers to ship software applications along with their dependencies in Docker images. Developers can extend existing images using them as base images when writing Dockerfiles. However, a lot of alternative functionally equivalent base images are available. Although many studies define and evaluate quality features that can be extracted from Docker artifacts, the criteria on which developers choose a base image over another remain unclear. In this article, we aim to fill this gap. First, we conduct a literature review through which we define a taxonomy of quality features, identifying two main groups: configuration-related features (i.e., mainly related to the Dockerfile and image build process), and externally observable features (i.e., what the Docker image users can observe). Second, we ran an empirical study considering the developers’ preference for 2,441 Docker images in 1,911 open source software projects. We want to understand how the externally observable features influence the developers’ preferences, and how they are related to the configuration-related features. Our results pave the way to the definition of a reliable quality measure for Docker artifacts, along with tools that support developers for a quality-aware development of them.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4378830801",
    "type": "article"
  },
  {
    "title": "DeepPatch: Maintaining Deep Learning Model Programs to Retain Standard Accuracy with Substantial Robustness Improvement",
    "doi": "https://doi.org/10.1145/3604609",
    "publication_date": "2023-06-14",
    "publication_year": 2023,
    "authors": "Zhengyuan Wei; Haipeng Wang; Imran Ashraf; W. K. Chan",
    "corresponding_authors": "",
    "abstract": "Maintaining a deep learning (DL) model by making the model substantially more robust through retraining with plenty of adversarial examples of non-trivial perturbation strength often reduces the model’s standard accuracy. Many existing model repair or maintenance techniques sacrifice standard accuracy to produce a large gain in robustness or vice versa. This article proposes DeepPatch, a novel technique to maintain filter-intensive DL models. To the best of our knowledge, DeepPatch is the first work to address the challenge of standard accuracy retention while substantially improving the robustness of DL models with plenty of adversarial examples of non-trivial and diverse perturbation strengths. Rather than following the conventional wisdom to generalize all the components of a DL model over the union set of clean and adversarial samples, DeepPatch formulates a novel division of labor method to adaptively activate a subset of its inserted processing units to process individual samples. Its produced model can generate the original or replacement feature maps in each forward pass of the patched model, making the patched model carry an intrinsic property of behaving like the model under maintenance on demand. The overall experimental results show that DeepPatch successfully retains the standard accuracy of all pretrained models while improving the robustness accuracy substantially. However, the models produced by the peer techniques suffer from either large standard accuracy loss or small robustness improvement compared with the models under maintenance, rendering them unsuitable in general to replace the latter.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4380609744",
    "type": "article"
  },
  {
    "title": "Snippet Comment Generation Based on Code Context Expansion",
    "doi": "https://doi.org/10.1145/3611664",
    "publication_date": "2023-07-31",
    "publication_year": 2023,
    "authors": "Hanyang Guo; Xiangping Chen; Yuan Huang; Yanlin Wang; Xi Ding; Zibin Zheng; Xiaocong Zhou; Hong‐Ning Dai",
    "corresponding_authors": "",
    "abstract": "Code commenting plays an important role in program comprehension. Automatic comment generation helps improve software maintenance efficiency. The code comments to annotate a method mainly include header comments and snippet comments. The header comment aims to describe the functionality of the entire method, thereby providing a general comment at the beginning of the method. The snippet comment appears at multiple code segments in the body of a method, where a code segment is called a code snippet. Both of them help developers quickly understand code semantics, thereby improving code readability and code maintainability. However, existing automatic comment generation models mainly focus more on header comments, because there are public datasets to validate the performance. By contrast, it is challenging to collect datasets for snippet comments, because it is difficult to determine their scope. Even worse, code snippets are often too short to capture complete syntax and semantic information. To address this challenge, we propose a novel S nippet C omment Gen eration approach called SCGen . First, we utilize the context of the code snippet to expand the syntax and semantic information. Specifically, 600,243 snippet code-comment pairs are collected from 959 Java projects. Then, we capture variables from code snippets and extract variable-related statements from the context. After that, we devise an algorithm to parse and traverse abstract syntax tree (AST) information of code snippets and corresponding context. Finally, SCGen generates snippet comments after inputting the source code snippet and corresponding AST information into a sequence-to-sequence-based model. We conducted extensive experiments on the dataset we collected to evaluate our SCGen . Our approach obtains 18.23 in BLEU-4 metrics, 18.83 in METEOR, and 23.65 in ROUGE-L, which outperforms state-of-the-art comment generation models.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385417895",
    "type": "article"
  },
  {
    "title": "Differentiable Quantum Programming with Unbounded Loops",
    "doi": "https://doi.org/10.1145/3617178",
    "publication_date": "2023-08-24",
    "publication_year": 2023,
    "authors": "Wang Fang; Mingsheng Ying; Xiaodi Wu",
    "corresponding_authors": "",
    "abstract": "The emergence of variational quantum applications has led to the development of automatic differentiation techniques in quantum computing. Existing work has formulated differentiable quantum programming with bounded loops, providing a framework for scalable gradient calculation by quantum means for training quantum variational applications. However, promising parameterized quantum applications, e.g., quantum walk and unitary implementation, cannot be trained in the existing framework due to the natural involvement of unbounded loops. To fill in the gap, we provide the first differentiable quantum programming framework with unbounded loops, including a newly designed differentiation rule, code transformation, and their correctness proof. Technically, we introduce a randomized estimator for derivatives to deal with the infinite sum in the differentiation of unbounded loops, whose applicability in classical and probabilistic programming is also discussed. We implement our framework with Python and Q# and demonstrate a reasonable sample efficiency. Through extensive case studies, we showcase an exciting application of our framework in automatically identifying close-to-optimal parameters for several parameterized quantum applications.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386142146",
    "type": "article"
  },
  {
    "title": "<i>LoGenText-Plus</i> : Improving Neural Machine Translation Based Logging Texts Generation with Syntactic Templates",
    "doi": "https://doi.org/10.1145/3624740",
    "publication_date": "2023-09-18",
    "publication_year": 2023,
    "authors": "Zishuo Ding; Yiming Tang; Xiaoyu Cheng; Heng Li; Weiyi Shang",
    "corresponding_authors": "",
    "abstract": "Developers insert logging statements in the source code to collect important runtime information about software systems. The textual descriptions in logging statements (i.e., logging texts) are printed during system executions and exposed to multiple stakeholders including developers, operators, users, and regulatory authorities. Writing proper logging texts is an important but often challenging task for developers. Prior studies find that developers spend significant efforts modifying their logging texts. However, despite extensive research on automated logging suggestions, research on suggesting logging texts rarely exists. To fill this knowledge gap, we first propose LoGenText (initially reported in our conference paper), an automated approach that uses neural machine translation (NMT) models to generate logging texts by translating the related source code into short textual descriptions. LoGenText takes the preceding source code of a logging text as the input and considers other context information, such as the location of the logging statement, to automatically generate the logging text. LoGenText ’s evaluation on 10 open source projects indicates that the approach is promising for automatic logging text generation and significantly outperforms the state-of-the-art approach. Furthermore, we extend LoGenText to LoGenText-Plus by incorporating the syntactic templates of the logging texts. Different from LoGenText , LoGenText-Plus decomposes the logging text generation process into two stages. LoGenText-Plus first adopts an NMT model to generate the syntactic template of the target logging text. Then LoGenText-Plus feeds the source code and the generated template as the input to another NMT model for logging text generation. We also evaluate LoGenText-Plus on the same 10 projects and observe that it outperforms LoGenText on 9 of them. According to a human evaluation from developers’ perspectives, the logging texts generated by LoGenText-Plus have a higher quality than those generated by LoGenText and the prior baseline approach. By manually examining the generated logging texts, we then identify five aspects that can serve as guidance for writing or generating good logging texts. Our work is an important step toward the automated generation of logging statements, which can potentially save developers’ efforts and improve the quality of software logging. Our findings shed light on research opportunities that leverage advances in NMT techniques for automated generation and suggestion of logging statements.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386830495",
    "type": "article"
  },
  {
    "title": "The Good, the Bad, and the Missing: Neural Code Generation for Machine Learning Tasks",
    "doi": "https://doi.org/10.1145/3630009",
    "publication_date": "2023-10-23",
    "publication_year": 2023,
    "authors": "Jiho Shin; Moshi Wei; Junjie Wang; Lin Shi; Song Wang",
    "corresponding_authors": "",
    "abstract": "Machine learning (ML) has been increasingly used in a variety of domains, while solving ML programming tasks poses unique challenges due to the fundamental difference in the nature and the construct of general programming tasks, especially for developers who do not have ML backgrounds. Automatic code generation that produces a code snippet from a natural language description can be a promising technique to accelerate ML programming tasks. In recent years, although many deep learning-based neural code generation models have been proposed with high accuracy, the fact that most of them are mainly evaluated on general programming tasks calls into question their effectiveness and usefulness in ML programming tasks. In this article, we set out to investigate the effectiveness of existing neural code generation models on ML programming tasks. For our analysis, we select six state-of-the-art neural code generation models and evaluate their performance on four widely used ML libraries, with newly created 83K pairs of natural-language described ML programming tasks. Our empirical study reveals some good, bad, and missing aspects of neural code generation models on ML tasks, with a few major ones listed below. ( Good ) Neural code generation models perform significantly better on ML tasks than on non-ML tasks with an average difference of 10.6 points in BLEU-4 scores. ( Bad ) More than 80% of the generated code is semantically incorrect. ( Bad ) Code generation models do not have significance in improving developers’ completion time. ( Good ) The generated code can help developers write correct code by providing developers with clues for using correct APIs. ( Missing ) The observation from our user study reveals the missing aspects of code generation for ML tasks, e.g., decomposing code generation for divide-and-conquer into API sequence identification and API usage generation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387869001",
    "type": "article"
  },
  {
    "title": "Reusing Convolutional Neural Network Models through Modularization and Composition",
    "doi": "https://doi.org/10.1145/3632744",
    "publication_date": "2023-11-13",
    "publication_year": 2023,
    "authors": "Binhang Qi; Hailong Sun; Hongyu Zhang; Xiang Gao",
    "corresponding_authors": "",
    "abstract": "With the widespread success of deep learning technologies, many trained deep neural network (DNN) models are now publicly available. However, directly reusing the public DNN models for new tasks often fails due to mismatching functionality or performance. Inspired by the notion of modularization and composition in software reuse, we investigate the possibility of improving the reusability of DNN models in a more fine-grained manner. Specifically, we propose two modularization approaches named CNNSplitter and GradSplitter, which can decompose a trained convolutional neural network (CNN) model for N -class classification into N small reusable modules. Each module recognizes one of the N classes and contains a part of the convolution kernels of the trained CNN model. Then, the resulting modules can be reused to patch existing CNN models or build new CNN models through composition. The main difference between CNNSplitter and GradSplitter lies in their search methods: the former relies on a genetic algorithm to explore search space, while the latter utilizes a gradient-based search method. Our experiments with three representative CNNs on three widely used public datasets demonstrate the effectiveness of the proposed approaches. Compared with CNNSplitter, GradSplitter incurs less accuracy loss, produces much smaller modules (19.88% fewer kernels), and achieves better results on patching weak models. In particular, experiments on GradSplitter show that (1) by patching weak models, the average improvement in terms of precision, recall, and F1-score is 17.13%, 4.95%, and 11.47%, respectively, and (2) for a new task, compared with the models trained from scratch, reusing modules achieves similar accuracy (the average loss of accuracy is only 2.46%) without a costly training process. Our approaches provide a viable solution to the rapid development and improvement of CNN models.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4388632182",
    "type": "article"
  },
  {
    "title": "A Smart Status Based Monitoring Algorithm for the Dynamic Analysis of Memory Safety",
    "doi": "https://doi.org/10.1145/3637227",
    "publication_date": "2023-12-11",
    "publication_year": 2023,
    "authors": "Zhe Chen; Rui Yan; Yingzi Ma; Yulei Sui; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "C is a dominant programming language for implementing system and low-level embedded software. Unfortunately, the unsafe nature of its low-level control of memory often leads to memory errors. Dynamic analysis has been widely used to detect memory errors at runtime. However, existing monitoring algorithms for dynamic analysis are not yet satisfactory, as they cannot deterministically and completely detect some types of errors, such as segment confusion errors, sub-object overflows, use-after-frees and memory leaks. We propose a new monitoring algorithm, namely Smatus , short for smart status , that improves memory safety by performing comprehensive dynamic analysis. The key innovation is to maintain at runtime a small status node for each memory object. A status node records the status value and reference count of an object, where the status value denotes the liveness and segment type of this object, and the reference count tracks the number of pointer variables pointing to this object. Smatus maintains at runtime a pointer metadata for each pointer variable, to record not only the base and bound of a pointer’s referent but also the address of the referent’s status node. All the pointers pointing to the same referent share the same status node in their pointer metadata. A status node is smart in the sense that it is automatically deleted when it becomes useless (indicated by its reference count reaching zero). To the best of our knowledge, Smatus represents the most comprehensive approach of its kind. We have evaluated Smatus by using a large set of programs including the NIST Software Assurance Reference Dataset, MSBench, MiBench, SPEC and stress testing benchmarks. In terms of effectiveness (detecting different types of memory errors), Smatus outperforms state-of-the-art tools, Google’s AddressSanitizer, SoftBoundCETS and Valgrind, as it is capable of detecting more errors. In terms of performance (the time and memory overheads), Smatus outperforms SoftBoundCETS and Valgrind in terms of both lower time and memory overheads incurred, and is on par with AddressSanitizer in terms of the time and memory overhead tradeoff made (with much lower memory overheads incurred).",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4389571996",
    "type": "article"
  },
  {
    "title": "Test Generation Strategies for Building Failure Models and Explaining Spurious Failures",
    "doi": "https://doi.org/10.1145/3638246",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Baharin A. Jodat; Abhishek Chandar; Shiva Nejati; Mehrdad Sabetzadeh",
    "corresponding_authors": "",
    "abstract": "Test inputs fail not only when the system under test is faulty but also when the inputs are invalid or unrealistic. Failures resulting from invalid or unrealistic test inputs are spurious. Avoiding spurious failures improves the effectiveness of testing in exercising the main functions of a system, particularly for compute-intensive (CI) systems where a single test execution takes significant time. In this article, we propose to build failure models for inferring interpretable rules on test inputs that cause spurious failures. We examine two alternative strategies for building failure models: (1) machine learning (ML)-guided test generation and (2) surrogate-assisted test generation. ML-guided test generation infers boundary regions that separate passing and failing test inputs and samples test inputs from those regions. Surrogate-assisted test generation relies on surrogate models to predict labels for test inputs instead of exercising all the inputs. We propose a novel surrogate-assisted algorithm that uses multiple surrogate models simultaneously, and dynamically selects the prediction from the most accurate model. We empirically evaluate the accuracy of failure models inferred based on surrogate-assisted and ML-guided test generation algorithms. Using case studies from the domains of cyber-physical systems and networks, we show that our proposed surrogate-assisted approach generates failure models with an average accuracy of 83%, significantly outperforming ML-guided test generation and two baselines. Further, our approach learns failure-inducing rules that identify genuine spurious failures as validated against domain knowledge.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390051453",
    "type": "article"
  },
  {
    "title": "Building Domain-Specific Machine Learning Workflows: A Conceptual Framework for the State-of-the-Practice",
    "doi": "https://doi.org/10.1145/3638243",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Bentley James Oakes; Michalis Famelis; Houari Sahraoui",
    "corresponding_authors": "",
    "abstract": "Domain experts are increasingly employing machine learning to solve their domain-specific problems. This article presents to software engineering researchers the six key challenges that a domain expert faces in addressing their problem with a computational workflow, and the underlying executable implementation. These challenges arise out of our conceptual framework which presents the “route” of transformations that a domain expert may choose to take while developing their solution. To ground our conceptual framework in the state of the practice, this article discusses a selection of available textual and graphical workflow systems and their support for the transformations described in our framework. Example studies from the literature in various domains are also examined to highlight the tools used by the domain experts as well as a classification of the domain specificity and machine learning usage of their problem, workflow, and implementation. The state of the practice informs our discussion of the six key challenges, where we identify which challenges and transformations are not sufficiently addressed by available tools. We also suggest possible research directions for software engineering researchers to increase the automation of these tools and disseminate best-practice techniques between software engineering and various scientific domains.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390051486",
    "type": "article"
  },
  {
    "title": "Deceiving Humans and Machines Alike: Search-based Test Input Generation for DNNs Using Variational Autoencoders",
    "doi": "https://doi.org/10.1145/3635706",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Sungmin Kang; Robert Feldt; Shin Yoo",
    "corresponding_authors": "",
    "abstract": "Due to the rapid adoption of Deep Neural Networks (DNNs) into larger software systems, testing of DNN-based systems has received much attention recently. While many different test adequacy criteria have been suggested, we lack effective test input generation techniques. Inputs such as images of real-world objects and scenes are not only expensive to collect but also difficult to randomly sample. Consequently, current testing techniques for DNNs tend to apply small local perturbations to existing inputs to generate new inputs. We propose SINVAD (Search-based Input space Navigation using Variational AutoencoDers), a way to sample from, and navigate over, a space of realistic inputs that resembles the true distribution in the training data. Our input space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. Our analysis shows that the VAE-based input space is well-aligned with human perception of what constitutes realistic inputs. Further, we show that this space can be effectively searched to achieve various testing scenarios, such as boundary testing of two different DNNs or analyzing class labels that are difficult for the given DNN to distinguish. Guidelines on how to design VAE architectures are presented as well. Our results have the potential to open the field to meaningful exploration through the space of highly structured images.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390051511",
    "type": "article"
  },
  {
    "title": "Corrigenda: a hierarchy-aware approach to faceted classification of object-oriented components",
    "doi": "https://doi.org/10.1145/322993.322997",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "Ernesto Damiani; Mariagrazia Fugini; Carlo Bellettini",
    "corresponding_authors": "",
    "abstract": "This article presents a hierarchy-aware classification schema for object-oriented code, where software components are classified according to their behavioral characteristics , such as provided services, employed algorithms, and needed data. In the case of reusable application frameworks, these characteristics are constructed from their model , i.e., from the description of the abstract classes specifying both the framework structure and purpose. In conventional object libraries, the characteristics are extracted semiautomatically from class interfaces. Characteristics are term pairs, weighted to represent \"how well\" they describe component behavior. The set of characteristics associated with a given component forms its software descriptor . A descriptor base is presented where descriptors are organized on the basis of structured relationships, such as similarity and composition. The classification is supported by a thesaurus acting as a language-independent unified lexicon. The descriptor base is conceived for developers who, besides conventionally browsing the descriptors hierarchy, can query the system, specifying a set of desired functionalities and getting a ranked set of adaptable candidates. User feedback is taken into account in order to progressively ameliorate the quality of the descriptors according to the views of the user community. Feedback is made dependent of the user typology through a user profile . Experimental results in terms of recall and precision of the retrieval mechanism against a sample code base are reported.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1981454921",
    "type": "article"
  },
  {
    "title": "A time-sensitive object model for real-time systems",
    "doi": "https://doi.org/10.1145/214013.214021",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "H. Rebecca Callison",
    "corresponding_authors": "H. Rebecca Callison",
    "abstract": "Process-oriented models for real-time systems focus on the timing constraints of processes , a focus that can adversely affect resulting designs. Data dependencies between processes create scheduling interactions that limit the times at which processes may execute. Processes are then designed to fit available windows in the overall system schedule. “Fitting in” frequently involves fragmenting processes to fit scheduling windows and/or designing program and data structures for speed rather than for program comprehension. The result is often a system with very sensitive timing that is hard to understand and maintain. As an alternative to process-oriented design, we present time-sensitive objects: a data-oriented model for real-time systems. The time-sensitive object (TSO) model structures systems as time-constrained data, rather than time constrained processing. Object values are extended to object histories in which a sequence of time constrained values describe the evolution of the object over time. Systems comprise a set of objects and their dependencies. The TSO model describes the effects of object operations and the propagation of change among related objects. Periodic objects, a class of objects within the TSO model, are described in detail in this article and compared with traditional periodic processes. Advantages of time-sensitive objects are identified, including greater scheduling independence when processes have data dependencies, more opportunity for concurrency, and greater inherent capability for detection of and tolerance to timing errors.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1965680579",
    "type": "article"
  },
  {
    "title": "The automated production control documentation system",
    "doi": "https://doi.org/10.1145/125489.122826",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Carmen J. Trammell; Leon H. Binder; Cathrine E. Snyder",
    "corresponding_authors": "",
    "abstract": "A prototype software system was developed for the U.S. Naval Underwater Systems Center(NUSC) as a demonstration of the Cleanroom Software Engineering methodology. The Cleanroom method is a team approach to the incremental development of software under statistical quality control. Cleanroom's formal methods of Box Structure specification and design, functional verification, and statistical testing were used by a four-person team to develop the Automated Production Control Documentation(APCODOC) system, a relational database application. As is typical in Cleanroom developments, correctness of design and code were ensured through team reviews. Eighteen errors were found during functional verification of the design, and nineteen errors were found during walkthrough of the 1820 lines of FOXBASE code. The software was not executed by developers prior to independent testing (i.e., there was no debugging). There were no errors in compilation, no failures during statistical certification testing, and the software was certified at the target levels of reliability and confidence. Team members attribute the ultimate error-free compilation and failure-free execution of the software to the rigor of the methodology and the intellectual control afforded by the team approach.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2019063197",
    "type": "article"
  },
  {
    "title": "A formal approach for designing CORBA-based applications",
    "doi": "https://doi.org/10.1145/941566.941567",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Alberto Coen‐Porisini; Matteo Pradella; Matteo Rossi; Dino Mandrioli",
    "corresponding_authors": "",
    "abstract": "The design of distributed applications in a CORBA-based environment can be carried out by means of an incremental approach, which starts from the specification and leads to the high-level architectural design. This article discusses a methodology to transform a formal specification written in TRIO into a high-level design document written in an extension of TRIO, named TRIO/CORBA (TC). The TC language is suited to formally describe the high-level architecture of a CORBA-based application. As a result, designers are offered high-level concepts that precisely define the architectural elements of an application. Furthermore, TC offers mechanisms to extend its base semantics, and can be adapted to future developments and enhancements in the CORBA standard. The methodology and the associated language are presented through a case study derived from a real Supervision and Control System.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2106260927",
    "type": "article"
  },
  {
    "title": "A formal design notation for real-time systems",
    "doi": "https://doi.org/10.1145/505145.505146",
    "publication_date": "2002-04-01",
    "publication_year": 2002,
    "authors": "Miguel Felder; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "The development of real-time systems is based on a variety of different methods and notations. Despite the purported benefits of formal methods, informal techniques still play a predominant role in current industrial practice. Formal and informal methods have been combined in various ways to smoothly introduce formal methods in industrial practice. The combination of real-time structured analysis (SA-RT) with Petri nets is among the most popular approaches, but has been applied only to requirements specifications. This paper extends SA-RT to specifications of the detailed design of embedded real-time systems, and combines the proposed notation with Petri nets.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2046305137",
    "type": "article"
  },
  {
    "title": "Supporting dynamic aspect-oriented features",
    "doi": "https://doi.org/10.1145/1824760.1824764",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Robert Dyer; Hridesh Rajan",
    "corresponding_authors": "",
    "abstract": "Dynamic aspect-oriented (AO) features have important software engineering benefits such as allowing unanticipated software evolution and maintenance. It is thus important to efficiently support these features in language implementations. Current implementations incur unnecessary design-time and runtime overhead due to the lack of support in underlying intermediate language (IL) models. To address this problem, we present a flexible and dynamic IL model that we call Nu . The Nu model provides a higher level of abstraction compared to traditional object-oriented ILs, making it easier to efficiently support dynamic AO features. We demonstrate these benefits by providing an industrial-strength VM implementation for Nu , by showing translation strategies from dynamic source-level constructs to Nu and by analyzing the performance of the resulting IL code. Nu 's VM extends the Sun Hotspot VM interpreter and uses a novel caching mechanism to significantly reduce the amortized costs of join point dispatch. Our evaluation using standard benchmarks shows that the overhead of supporting a dynamic deployment model can be reduced to as little as ˜1.5%. Nu provides an improved compilation target for dynamic deployment features, which makes it easier to support such features with corresponding software engineering benefits in software evolution and maintenance and in runtime verification.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1982766681",
    "type": "article"
  },
  {
    "title": "Marple",
    "doi": "https://doi.org/10.1145/2491509.2491512",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Wei Le; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "Generally, a fault is a property violation at a program point along some execution path. To obtain the path where a fault occurs, we can either run the program or manually identify the execution paths through code inspection. In both of the cases, only a very limited number of execution paths can be examined for a program. This article presents a static framework, Marple, that automatically detects path segments where a fault occurs at a whole program scale. An important contribution of the work is the design of a demand-driven analysis that effectively addresses scalability challenges faced by traditional path-sensitive fault detection. The techniques are made general via a specification language and an algorithm that automatically generates path-based analyses from specifications. The generality is achieved in handling both data- and control-centric faults as well as both liveness and safety properties, enabling the exploitation of fault interactions for diagnosis and efficiency. Our experimental results demonstrate the effectiveness of our techniques in detecting path segments of buffer overflows, integer violations, null-pointer dereferences, and memory leaks. Because we applied an interprocedural, path-sensitive analysis, our static fault detectors generally report better precision than the tools available for comparison. Our demand-driven analyses are shown scalable to deployed applications such as apache , putty , and ffmpeg .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1979096626",
    "type": "article"
  },
  {
    "title": "Evaluating a query framework for software evolution data",
    "doi": "https://doi.org/10.1145/2522920.2522931",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Michael Würsch; Emanuel Giger; Harald C. Gall",
    "corresponding_authors": "",
    "abstract": "With the steady advances in tooling to support software engineering, mastering all the features of modern IDEs, version control systems, and project trackers is becoming increasingly difficult. Answering even the most common developer questions can be surprisingly tedious and difficult. In this article we present a user study with 35 subjects to evaluate our quasi-natural language interface that provides access to various facets of the evolution of a software system but requires almost zero learning effort. Our approach is tightly woven into the Eclipse IDE and allows developers to answer questions related to source code, development history, or bug and issue management. The results of our evaluation show that our query interface can outperform classical software engineering tools in terms of correctness, while yielding significant time savings to its users and greatly advancing the state of the art in terms of usability and learnability.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1991203009",
    "type": "article"
  },
  {
    "title": "A web-centred approach to end-user software engineering",
    "doi": "https://doi.org/10.1145/2522920.2522929",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "David Lizcano; Fernando Alonso; Javier Soriano; Genoveva López",
    "corresponding_authors": "",
    "abstract": "This article addresses one of the major end-user software engineering (EUSE) challenges, namely, how to motivate end users to apply unfamiliar software engineering techniques and activities to achieve their goal: translate requirements into software that meets their needs. EUSE activities are secondary to the goal that the program is helping to achieve and end-user programming is opportunistic. The challenge is then to find ways to incorporate EUSE activities into the existing workflow without users having to make substantial changes to the type of work they do or their priorities. In this article, we set out an approach to EUSE for web-based applications. We also propose a software lifecycle that is consistent with the conditions and priorities of end users without programming skills and is well-aligned with EUSE's characteristic informality, ambiguity and opportunisticness. Users applying this lifecycle manage to find solutions that they would otherwise be unable to identify. They also develop quality products. Users of this approach will not have to be acquainted with software engineering, as a framework will take them through the web-centred EUSE lifecycle step-by-step. We also report a statistical experiment in which users develop web software with and without a framework to guide them through the lifecycle. Its aim is to validate the applicability of our framework-driven lifecycle.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2093265570",
    "type": "article"
  },
  {
    "title": "Understanding and Combating Memory Bloat in Managed Data-Intensive Systems",
    "doi": "https://doi.org/10.1145/3162626",
    "publication_date": "2017-10-31",
    "publication_year": 2017,
    "authors": "Khanh Nguyen; Kai Wang; Yingyi Bu; Lu Fang; Guoqing Xu",
    "corresponding_authors": "",
    "abstract": "The past decade has witnessed increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer’s choice for implementing such applications, due to its quick development cycle and rich suite of libraries and frameworks. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets large volumes of input data, memory bloat is significantly magnified and becomes a scalability-prohibiting bottleneck. This article first studies, analytically and empirically, the impact of bloat on the performance and scalability of large-scale, real-world data-intensive systems. To combat bloat, we design a novel compiler framework, called F acade , that can generate highly efficient data manipulation code by automatically transforming the data path of an existing data-intensive application. The key treatment is that in the generated code, the number of runtime heap objects created for data classes in each thread is (almost) statically bounded , leading to significantly reduced memory management cost and improved scalability. We have implemented F acade and used it to transform seven common applications on three real-world, already well-optimized data processing frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: the generated programs have (1) achieved a 3% to 48% execution time reduction and an up to 88× GC time reduction, (2) consumed up to 50% less memory, and (3) scaled to much larger datasets.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2782032628",
    "type": "article"
  },
  {
    "title": "Efficient Verification of Concurrent Systems Using Synchronisation Analysis and SAT/SMT Solving",
    "doi": "https://doi.org/10.1145/3335149",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Pedro Antonino; Thomas Gibson−Robinson; A. W. Roscoe",
    "corresponding_authors": "",
    "abstract": "This article investigates how the use of approximations can make the formal verification of concurrent systems scalable. We propose the idea of synchronisation analysis to automatically capture global invariants and approximate reachability. We calculate invariants on how components participate on global system synchronisations and use a notion of consistency between these invariants to establish whether components can effectively communicate to reach some system state. Our synchronisation-analysis techniques try to show either that a system state is unreachable by demonstrating that components cannot agree on the order they participate in system rules or that a system state is unreachable by demonstrating components cannot agree on the number of times they participate on system rules. These fully automatic techniques are applied to check deadlock and local-deadlock freedom in the PairStatic framework. It extends Pair (a recent framework where we use pure pairwise analysis of components and SAT checkers to check deadlock and local-deadlock freedom) with techniques to carry out synchronisation analysis. So, not only can it compute the same local invariants that Pair does, it can leverage global invariants found by synchronisation analysis, thereby improving the reachability approximation and tightening our verifications. We implement PairStatic in our DeadlOx tool using SAT/SMT and demonstrate the improvements they create in checking (local) deadlock freedom.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2963048143",
    "type": "article"
  },
  {
    "title": "Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software",
    "doi": "https://doi.org/10.1145/3313789",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Dennis Reuling; Udo Kelter; Johannes Bürdek; Malte Lochau",
    "corresponding_authors": "",
    "abstract": "Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called S i MPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of S i MPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying S i MPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that S i MPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2963786844",
    "type": "article"
  },
  {
    "title": "The Virtual Developer",
    "doi": "https://doi.org/10.1145/3340545",
    "publication_date": "2019-09-02",
    "publication_year": 2019,
    "authors": "Carlo Bernaschina; Emanuele Falzone; Piero Fraternali; Sergio Luis Herrera González",
    "corresponding_authors": "",
    "abstract": "Model Driven Development (MDD) requires proper tools to derive the implementation code from the application models. However, the integration of handwritten and generated code is a long-standing issue that affects the adoption of MDD in the industry. This article presents a model and code co-evolution approach that addresses such a problem a posteriori , using the standard collision detection capabilities of Version Control Systems to support the semi-automatic merge of the two types of code. We assess the proposed approach by contrasting it with the more traditional template-based, forward-engineering process, adopted by most MDD tools.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2971549319",
    "type": "article"
  },
  {
    "title": "iSENSE2.0",
    "doi": "https://doi.org/10.1145/3394602",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Junjie Wang; Ye Yang; Tim Menzies; Qing Wang",
    "corresponding_authors": "",
    "abstract": "Software engineers get questions of “how much testing is enough” on a regular basis. Existing approaches in software testing management employ experience-, risk-, or value-based analysis to prioritize and manage testing processes. However, very few is applicable to the emerging crowdtesting paradigm to cope with extremely limited information and control over unknown, online crowdworkers. In practice, deciding when to close a crowdtesting task is largely done by experience-based guesswork and frequently results in ineffective crowdtesting. More specifically, it is found that an average of 32% testing cost was wasteful spending in current crowdtesting practice. This article intends to address this challenge by introducing automated decision support for monitoring and determining appropriate time to close crowdtesting tasks. To that end, it first investigates the necessity and feasibility of close prediction of crowdtesting tasks based on an industrial dataset. Next, it proposes a close prediction approach named iSENSE2.0, which applies incremental sampling technique to process crowdtesting reports arriving in chronological order and organizes them into fixed-sized groups as dynamic inputs. Then, a duplicate tagger analyzes the duplicate status of received crowd reports, and a CRC-based (Capture-ReCapture) close estimator generates the close decision based on the dynamic bug arrival status. In addition, a coverage-based sanity checker is designed to reinforce the stability and performance of close prediction. Finally, the evaluation of iSENSE2.0 is conducted on 56,920 reports of 306 crowdtesting tasks from one of the largest crowdtesting platforms. The results show that a median of 100% bugs can be detected with 30% saved cost. The performance of iSENSE2.0 does not demonstrate significant difference with the state-of-the-art approach iSENSE , while the later one relies on the duplicate tag, which is generally considered as time-consuming and tedious to obtain.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3039889685",
    "type": "article"
  },
  {
    "title": "Practical Constraint Solving for Generating System Test Data",
    "doi": "https://doi.org/10.1145/3381032",
    "publication_date": "2020-04-29",
    "publication_year": 2020,
    "authors": "Ghanem Soltana; Mehrdad Sabetzadeh; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "The ability to generate test data is often a necessary prerequisite for automated software testing. For the generated data to be fit for their intended purpose, the data usually have to satisfy various logical constraints. When testing is performed at a system level, these constraints tend to be complex and are typically captured in expressive formalisms based on first-order logic. Motivated by improving the feasibility and scalability of data generation for system testing, we present a novel approach, whereby we employ a combination of metaheuristic search and Satisfiability Modulo Theories (SMT) for constraint solving. Our approach delegates constraint solving tasks to metaheuristic search and SMT in such a way as to take advantage of the complementary strengths of the two techniques. We ground our work on test data models specified in UML, with OCL used as the constraint language. We present tool support and an evaluation of our approach over three industrial case studies. The results indicate that, for complex system test data generation problems, our approach presents substantial benefits over the state-of-the-art in terms of applicability and scalability.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3122914370",
    "type": "article"
  },
  {
    "title": "A Hybrid Approach to Formal Verification of Higher-Order Masked Arithmetic Programs",
    "doi": "https://doi.org/10.1145/3428015",
    "publication_date": "2021-02-11",
    "publication_year": 2021,
    "authors": "Pengfei Gao; Hongyi Xie; Fu Song; Taolue Chen",
    "corresponding_authors": "",
    "abstract": "Side-channel attacks, which are capable of breaking secrecy via side-channel information, pose a growing threat to the implementation of cryptographic algorithms. Masking is an effective countermeasure against side-channel attacks by removing the statistical dependence between secrecy and power consumption via randomization. However, designing efficient and effective masked implementations turns out to be an error-prone task. Current techniques for verifying whether masked programs are secure are limited in their applicability and accuracy, especially when they are applied. To bridge this gap, in this article, we first propose a sound type system, equipped with an efficient type inference algorithm, for verifying masked arithmetic programs against higher-order attacks. We then give novel model-counting-based and pattern-matching-based methods that are able to precisely determine whether the potential leaky observable sets detected by the type system are genuine or simply spurious. We evaluate our approach on various implementations of arithmetic cryptographic programs. The experiments confirm that our approach outperforms the state-of-the-art baselines in terms of applicability, accuracy, and efficiency.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3132102707",
    "type": "article"
  },
  {
    "title": "An Empirical Study on Type Annotations",
    "doi": "https://doi.org/10.1145/3439775",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "John‐Paul Ore; Carrick Detweiler; Sebastian Elbaum",
    "corresponding_authors": "",
    "abstract": "Type annotations connect variables to domain-specific types. They enable the power of type checking and can detect faults early. In practice, type annotations have a reputation of being burdensome to developers. We lack, however, an empirical understanding of how and why they are burdensome. Hence, we seek to measure the baseline accuracy and speed for developers making type annotations to previously unseen code. We also study the impact of one or more type suggestions. We conduct an empirical study of 97 developers using 20 randomly selected code artifacts from the robotics domain containing physical unit types. We find that subjects select the correct physical type with just 51% accuracy, and a single correct annotation takes about 2 minutes on average. Showing subjects a single suggestion has a strong and significant impact on accuracy both when correct and incorrect, while showing three suggestions retains the significant benefits without the negative effects. We also find that suggestions do not come with a time penalty. We require subjects to explain their annotation choices, and we qualitatively analyze their explanations. We find that identifier names and reasoning about code operations are the primary clues for selecting a type. We also examine two state-of-the-art automated type annotation systems and find opportunities for their improvement.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3133303669",
    "type": "article"
  },
  {
    "title": "A Formal Framework of Software Product Line Analyses",
    "doi": "https://doi.org/10.1145/3442389",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Thiago Castro; Leopoldo Teixeira; Vander Alves; Sven Apel; Maxime Cordy; Rohit Gheyi",
    "corresponding_authors": "",
    "abstract": "A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3161677868",
    "type": "article"
  },
  {
    "title": "Parallel Test Prioritization",
    "doi": "https://doi.org/10.1145/3471906",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Jianyi Zhou; Junjie Chen; Dan Hao",
    "corresponding_authors": "",
    "abstract": "Although regression testing is important to guarantee the software quality in software evolution, it suffers from the widely known cost problem. To address this problem, existing researchers made dedicated efforts on test prioritization, which optimizes the execution order of tests to detect faults earlier; while practitioners in industry leveraged more computing resources to save the time cost of regression testing. By combining these two orthogonal solutions, in this article, we define the problem of parallel test prioritization, which is to conduct test prioritization in the scenario of parallel test execution to reduce the cost of regression testing. Different from traditional sequential test prioritization, parallel test prioritization aims at generating a set of test sequences, each of which is allocated in an individual computing resource and executed in parallel. In particular, we propose eight parallel test prioritization techniques by adapting the existing four sequential test prioritization techniques, by including and excluding testing time in prioritization. To investigate the performance of the eight parallel test prioritization techniques, we conducted an extensive study on 54 open-source projects and a case study on 16 commercial projects from Baidu , a famous search service provider with 600M monthly active users. According to the two studies, parallel test prioritization does improve the efficiency of regression testing, and cost-aware additional parallel test prioritization technique significantly outperforms the other techniques, indicating that this technique is a good choice for practical parallel testing. Besides, we also investigated the influence of two external factors, the number of computing resources and time allowed for parallel testing, and find that more computing resources indeed improve the performance of parallel test prioritization. In addition, we investigated the influence of two more factors, test granularity and coverage criterion, and find that parallel test prioritization can still accelerate regression testing in parallel scenario. Moreover, we investigated the benefit of parallel test prioritization on the regression testing process of continuous integration, considering both the cumulative acceleration performance and the overhead of prioritization techniques, and the results demonstrate the superiority of parallel test prioritization.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3201977969",
    "type": "article"
  },
  {
    "title": "MICOSE4aPS: Industrially Applicable Maturity Metric to Improve Systematic Reuse of Control Software",
    "doi": "https://doi.org/10.1145/3467896",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Birgit Vogel‐Heuser; Eva-Maria Neumann; Juliane Fischer",
    "corresponding_authors": "",
    "abstract": "automated Production Systems (aPS) are highly complex, mechatronic systems that usually have to operate reliably for many decades. Standardization and reuse of control software modules is a core prerequisite to achieve the required system quality in increasingly shorter development cycles. However, industrial case studies in the field of aPS show that many aPS companies still struggle with strategically reusing software. This paper proposes a metric-based approach to objectively measure the maturity of industrial IEC 61131-based control software in aPS (MICOSE4aPS) to identify potential weaknesses and quality issues hampering systematic reuse. Module developers in the machine and plant manufacturing industry can directly benefit as the metric calculation is integrated into the software engineering workflow. An in-depth industrial evaluation in a top-ranked machine manufacturing company in food packaging and an expert evaluation with different companies confirmed the benefit to efficiently manage the quality of control software.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3202349310",
    "type": "article"
  },
  {
    "title": "Mining Unit Tests for Discovery and Migration of Math APIs",
    "doi": "https://doi.org/10.1145/2629506",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Anirudh Santhiar; Omesh Pandita; Aditya Kanade",
    "corresponding_authors": "",
    "abstract": "Today's programming languages are supported by powerful third-party APIs. For a given application domain, it is common to have many competing APIs that provide similar functionality. Programmer productivity therefore depends heavily on the programmer's ability to discover suitable APIs both during an initial coding phase, as well as during software maintenance. The aim of this work is to support the discovery and migration of math APIs. Math APIs are at the heart of many application domains ranging from machine learning to scientific computations. Our approach, called M ath F inder , combines executable specifications of mathematical computations with unit tests (operational specifications) of API methods. Given a math expression, M ath F inder synthesizes pseudo-code comprised of API methods to compute the expression by mining unit tests of the API methods. We present a sequential version of our unit test mining algorithm and also design a more scalable data-parallel version. We perform extensive evaluation of M ath F inder (1) for API discovery , where math algorithms are to be implemented from scratch and (2) for API migration , where client programs utilizing a math API are to be migrated to another API. We evaluated the precision and recall of M ath F inder on a diverse collection of math expressions, culled from algorithms used in a wide range of application areas such as control systems and structural dynamics. In a user study to evaluate the productivity gains obtained by using M ath F inder for API discovery, the programmers who used M ath F inder finished their programming tasks twice as fast as their counterparts who used the usual techniques like web and code search, IDE code completion, and manual inspection of library documentation. For the problem of API migration, as a case study, we used M ath F inder to migrate Weka, a popular machine learning library. Overall, our evaluation shows that M ath F inder is easy to use, provides highly precise results across several math APIs and application domains even with a small number of unit tests per method, and scales to large collections of unit tests.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2124722342",
    "type": "article"
  },
  {
    "title": "Software Change Contracts",
    "doi": "https://doi.org/10.1145/2729973",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Jooyong Yi; Dawei Qi; Shin Hwei Tan; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Software errors often originate from incorrect changes, including incorrect program fixes, incorrect feature updates, and so on. Capturing the intended program behavior explicitly via contracts is thus an attractive proposition. In our recent work, we had espoused the notion of “change contracts” to express the intended program behavior changes across program versions. Change contracts differ from program contracts in that they do not require the programmer to describe the intended behavior of those program features which are unchanged across program versions. In this work, we present the formal semantics of our change contract language built on top of the Java modeling language (JML). Our change contract language can describe behavioral as well as structural changes. We evaluate the expressivity of the change contract language via a survey given to final-year undergraduate students. The survey results enable to understand the usability of our change contract language for purposes of writing contracts, comprehending written contracts, and modifying programs according to given change contracts. Finally, we develop both dynamic and static checkers for change contracts, and show how they can be used in maintaining software changes. We use our dynamic checker to automatically suggest tests that manifest violations of change contracts. Meanwhile, we use our static checker to verify that a program is changed as specified in its change contract. Apart from verification, our static checker also performs various other software engineering tasks, such as localizing the buggy method, detecting/debugging regression errors, and classifying the cause for a test failure as either error in production code or error in test code.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2210410193",
    "type": "article"
  },
  {
    "title": "Understanding JavaScript Event-Based Interactions with Clematis",
    "doi": "https://doi.org/10.1145/2876441",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "Saba Alimadadi; Sheldon Sequeira; Ali Mesbah; Karthik Pattabiraman",
    "corresponding_authors": "",
    "abstract": "Web applications have become one of the fastest-growing types of software systems today. Despite their popularity, understanding the behavior of modern web applications is still a challenging endeavor for developers during development and maintenance tasks. The challenges mainly stem from the dynamic, event-driven, and asynchronous nature of the JavaScript language. We propose a generic technique for capturing low-level event-based interactions in a web application and mapping those to a higher-level behavioral model. This model is then transformed into an interactive visualization, representing episodes of triggered causal and temporal events, related JavaScript code executions, and their impact on the dynamic DOM state. Our approach, implemented in a tool called C lematis , allows developers to easily understand the complex dynamic behavior of their application at three different semantic levels of granularity. Furthermore, C lematis helps developers bridge the gap between test cases and program code by localizing the fault related to a test assertion. The results of our industrial controlled experiment show that C lematis is capable of improving the comprehension task accuracy by 157% while reducing the task completion time by 47%. A follow-up experiment reveals that C lematis improves the fault localization accuracy of developers by a factor of two.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2395760792",
    "type": "article"
  },
  {
    "title": "Hierarchical Program Paths",
    "doi": "https://doi.org/10.1145/2963094",
    "publication_date": "2016-08-22",
    "publication_year": 2016,
    "authors": "Chunbai Yang; Shangru Wu; W. K. Chan",
    "corresponding_authors": "",
    "abstract": "Complete dynamic control flow is a fundamental kind of execution profile about program executions with a wide range of applications. Tracing the dynamic control flow of program executions for a brief period easily generates a trace consisting of billions of control flow events. The number of events in such a trace is large, making both path tracing and path querying to incur significant slowdowns. A major class of path tracing techniques is to design novel trace representations that can be generated efficiently, and encode the inputted sequences of such events so that the inputted sequences are still derivable from the encoded and smaller representations. The control flow semantics in such representations have, however, become obscure, which makes implementing path queries on such a representation inefficient and the design of such queries complicated. We propose a novel two-phase path tracing framework— Hierarchical Program Path (HPP)—to model the complete dynamic control flow of an arbitrary number of executions of a program. In Phase 1, HPP monitors each execution, and efficiently generates a stream of events, namely HPPTree, representing a novel tree-based representation of control flow for each thread of control in the execution. In Phase 2, given a set of such event streams, HPP identifies all the equivalent instances of the same exercised interprocedural path in all the corresponding HPPTree instances, and represents each such equivalent set of paths with a single subgraph, resulting in our compositional graph-based trace representation, namely, HPPDAG. Thus, an HPPDAG instance has the potential to be significantly smaller in size than the corresponding HPPTree instances, and still completely preserves the control flow semantics of the traced executions. Control flow queries over all the traced executions can also be directly performed on the single HPPDAG instance instead of separately processing the trace representation of each execution followed by aggregating their results. We validate HPP using the SPLASH2 and SPECint 2006 benchmarks. Compared to the existing technique, named BLPT (Ball-Larus-based Path Tracing), HPP generates significantly smaller trace representations and incurs fewer slowdowns to the native executions in online tracing of Phase 1. The HPPDAG instances generated in Phase 2 are significantly smaller than their corresponding BLPT and HPPTree traces. We show that HPPDAG supports efficient backtrace querying, which is a representative path query based on complete control flow trace. Finally, we illustrate the ease of use of HPPDAG by building a novel and highly efficient path profiling technique to demonstrate the applicability of HPPDAG.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2512141937",
    "type": "article"
  },
  {
    "title": "All in One: Design, Verification, and Implementation of SNOW-optimal Read Atomic Transactions",
    "doi": "https://doi.org/10.1145/3494517",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Si Liu",
    "corresponding_authors": "Si Liu",
    "abstract": "Distributed read atomic transactions are important building blocks of modern cloud databases that magnificently bridge the gap between data availability and strong data consistency. The performance of their transactional reads is particularly critical to the overall system performance, as many real-world database workloads are dominated by reads. Following the SNOW design principle for optimal reads, we develop LORA, a novel SNOW-optimal algorithm for distributed read atomic transactions. LORA completes its reads in exactly one round trip, even in the presence of conflicting writes, without imposing additional overhead to the communication, and it outperforms the state-of-the-art read atomic algorithms. To guide LORA’s development, we present a rewriting-logic-based framework and toolkit for design, verification, implementation, and evaluation of distributed databases. Within the framework, we formalize LORA and mathematically prove its data consistency guarantees. We also apply automatic model checking and statistical verification to validate our proofs and to estimate LORA’s performance. We additionally generate from the formal model a correct-by-construction distributed implementation for testing and performance evaluation under realistic deployments. Our design-level and implementation-based experimental results are consistent, which together demonstrate LORA’s promising data consistency and performance achievement.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3210892990",
    "type": "article"
  },
  {
    "title": "Continuous and Proactive Software Architecture Evaluation: An IoT Case",
    "doi": "https://doi.org/10.1145/3492762",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Dalia Sobhy; Leandro L. Minku; Rami Bahsoon; Rick Kazman",
    "corresponding_authors": "",
    "abstract": "Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3216687398",
    "type": "article"
  },
  {
    "title": "A Common Terminology for Software Risk Management",
    "doi": "https://doi.org/10.1145/3498539",
    "publication_date": "2022-02-12",
    "publication_year": 2022,
    "authors": "Jhon Masso; Félix García; César Pardo; Francisco J. Pino; Mario Piattini",
    "corresponding_authors": "",
    "abstract": "In order to improve and sustain their competitiveness over time, organisations nowadays need to undertake different initiatives to adopt frameworks, models and standards that will allow them to align and improve their business processes. In spite of these efforts, organisations may still encounter governance and management problems. This is where Risk Management (RM) can play a major role, since its purpose is to contribute to the creation and preservation of value in the context of the organisation's processes. RM is a complex and subjective activity that requires experience and a high level of knowledge about risks, and it is for this reason that standardisation institutions and researchers have made great efforts to define initiatives to overcome these challenges. However, the RM field nevertheless presents a lack of uniformity in its terms and concepts, due to the different contexts and scopes of application, a situation that can generate ambiguities and misunderstandings. To address these issues, this paper aims to present an ontology called SRMO (Software Risk Management Ontology) , which seeks to unify the terms and concepts associated with RM and provide an integrated and holistic view of risk. In doing so, the Pipeline framework has been applied in order to assure and verify the quality of the proposed ontology, and it has been implemented in Protégé and validated by means of competency questions. Three application scenarios of this ontology demonstrating their usefulness in the software engineering field are presented in this paper. We believe that this ontology can be useful for organisations that are interested in: (i) establishing an RM strategy from an integrated approach, (ii) defining the elements that help to identify risks and the criteria that support decision-making in risk assessment, and (iii) helping the involved stakeholders during the process of risk management.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4211210755",
    "type": "article"
  },
  {
    "title": "Parametric Timed Pattern Matching",
    "doi": "https://doi.org/10.1145/3517194",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Masaki Waga; Étienne André; Ichiro Hasuo",
    "corresponding_authors": "",
    "abstract": "Given a log and a specification, timed pattern matching aims at exhibiting for which start and end dates a specification holds on that log. For example, “a given action is always followed by another action before a given deadline”. This problem has strong connections with monitoring real-time systems. We address here timed pattern matching in the presence of an uncertain specification, i.e., that may contain timing parameters (e.g., the deadline can be uncertain or unknown). We want to know for which start and end dates, and for what values of the timing parameters, a property holds. For instance, we look for the minimum or maximum deadline (together with the corresponding start and end dates) for which the property holds. We propose two frameworks for parametric timed pattern matching. The first one is based on parametric timed model checking. In contrast to most parametric timed problems, the solution is effectively computable. The second one is a dedicated method; not only we largely improve the efficiency compared to the first method, but we further propose optimizations with skipping. Our experiment results suggest that our algorithms, especially the second one, are efficient and practically relevant.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4224717535",
    "type": "article"
  },
  {
    "title": "Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning",
    "doi": "https://doi.org/10.1145/3502852",
    "publication_date": "2022-06-25",
    "publication_year": 2022,
    "authors": "Hao Yu; Xing Hu; Ge Li; Ying Li; Qianxiang Wang; Tao Xie",
    "corresponding_authors": "",
    "abstract": "In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4283525684",
    "type": "article"
  },
  {
    "title": "Bash in the Wild: Language Usage, Code Smells, and Bugs",
    "doi": "https://doi.org/10.1145/3517193",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Yiwen Dong; Zheyang Li; Yongqiang Tian; C. P. Sun; Michael W. Godfrey; Meiyappan Nagappan",
    "corresponding_authors": "",
    "abstract": "The Bourne-again shell (Bash) is a prevalent scripting language for orchestrating shell commands and managing resources in Unix-like environments. It is one of the mainstream shell dialects that is available on most GNU Linux systems. However, the unique syntax and semantics of Bash could easily lead to unintended behaviors if carelessly used. Prior studies primarily focused on improving the reliability of Bash scripts or facilitating writing Bash scripts; there is yet no empirical study on the characteristics of Bash programs written in reality, e.g., frequently used language features, common code smells, and bugs. In this article, we perform a large-scale empirical study of Bash usage, based on analyses over one million open source Bash scripts found in Github repositories. We identify and discuss which features and utilities of Bash are most often used. Using static analysis, we find that Bash scripts are often error-prone, and the error-proneness has a moderately positive correlation with the size of the scripts. We also find that the most common problem areas concern quoting, resource management, command options, permissions, and error handling. We envision that these findings can be beneficial for learning Bash and future research that aims to improve shell and command-line productivity and reliability.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4293235851",
    "type": "article"
  },
  {
    "title": "<scp>Route</scp> : Roads Not Taken in UI Testing",
    "doi": "https://doi.org/10.1145/3571851",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Jun-Wei Lin; Navid Salehnamadi; Sam Malek",
    "corresponding_authors": "",
    "abstract": "Core features (functionalities) of an app can often be accessed and invoked in several ways, i.e., through alternative sequences of user-interface (UI) interactions. Given the manual effort of writing tests, developers often only consider the typical way of invoking features when creating the tests (i.e., the “sunny day scenario”). However, the alternative ways of invoking a feature are as likely to be faulty. These faults would go undetected without proper tests. To reduce the manual effort of creating UI tests and help developers more thoroughly examine the features of apps, we present Route , an automated tool for feature-based UI test augmentation for Android apps. Route first takes a UI test and the app under test as input. It then applies novel heuristics to find additional high-quality UI tests, consisting of both inputs and assertions, that verify the same feature as the original test in alternative ways. Application of Route on several dozen tests for popular apps on Google Play shows that for 96% of the existing tests, Route was able to generate at least one alternative test. Moreover, the fault detection effectiveness of augmented test suites in our experiments showed substantial improvements of up to 39% over the original test suites.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4309561822",
    "type": "article"
  },
  {
    "title": "Hierarchical and Hybrid Organizational Structures in Open-source Software Projects: A Longitudinal Study",
    "doi": "https://doi.org/10.1145/3569949",
    "publication_date": "2022-12-02",
    "publication_year": 2022,
    "authors": "Mitchell Joblin; Barbara Eckl; Thomas Böck; A. Schmid; Janet Siegmund; Sven Apel",
    "corresponding_authors": "",
    "abstract": "Despite the absence of a formal process and a central command-and-control structure, developer organization in open-source software (OSS) projects are far from being a purely random process. Prior work indicates that, over time, highly successful OSS projects develop a hybrid organizational structure that comprises a hierarchical part and a non-hierarchical part. This suggests that hierarchical organization is not necessarily a global organizing principle and that a fundamentally different principle is at play below the lowest positions in the hierarchy. Given the vast proportion of developers are in the non-hierarchical part, we seek to understand the interplay between these two fundamentally differently organized groups, how this hybrid structure evolves, and the trajectory individual developers take through these structures over the course of their participation. We conducted a longitudinal study of the full histories of 20 popular OSS projects, modeling their organizational structures as networks of developers connected by communication ties and characterizing developers’ positions in terms of hierarchical (sub)structures in these networks. We observed a number of notable trends and patterns in the subject projects: (1) hierarchy is a pervasive structural feature of developer networks of OSS projects; (2) OSS projects tend to form hybrid organizational structures, consisting of a hierarchical and a non-hierarchical part; and (3) the positional trajectory of a developer starts loosely connected in the non-hierarchical part and then tightly integrate into the hierarchical part, which is associated with the acquisition of experience (tenure), in addition to coordination and coding activities. Our study (a) provides a methodological basis for further investigations of hierarchy formation, (b) suggests a number of hypotheses on prevalent organizational patterns and trends in OSS projects to be addressed in further work, and (c) may ultimately guide the governance of organizational structures.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4311184977",
    "type": "article"
  },
  {
    "title": "Software trustability analysis",
    "doi": "https://doi.org/10.1145/201055.201058",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "William E. Howden; Yudong Huang",
    "corresponding_authors": "",
    "abstract": "A measure of software dependability called trustability is described. A program p has trustability T if we are at least T confident that p is free of faults. Trustability measurement depends on detectability. The detectability of a method is the probability that it will detect faults, when there are faults present. Detectability research can be used to characterize conditions under which one testing and analysis method is more effective than another. Several detectability results that were only previously described informally, and illustrated by example, are proved. Several new detectability results are also proved. The trustability model characterizes the kind of information that is needed to justify a given level of trustability. When the required information is available, the trustability approach can be used to determine strategies in which methods are combined for maximum effectiveness. It can be used to determine the minimum amount of resources needed to guarantee a required degree of trustability, and the maximum trustability that is achievable with a given amount of resources. Theorems proving several optimization results are given. Applications of the trustability model are discussed. Methods for the derivation of detectability factors, the relationship between trustability and operational reliability, and the relationship between the software development process and trustability are described.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2061181647",
    "type": "article"
  },
  {
    "title": "An algebraic theory of class specification",
    "doi": "https://doi.org/10.1145/192218.192235",
    "publication_date": "1994-04-01",
    "publication_year": 1994,
    "authors": "Francesco Parisi-Presicce; Alfonso Pierantonio",
    "corresponding_authors": "",
    "abstract": "The notion of class (or object pattern) as defined in most object-oriented languages is formalized using known techniques from algebraic specifications. Inheritance can be viewed as a relation betweeen classes, which suggests how classes can be arranged in hierarchies. The hierarchies contain two kinds of information: on the one hand, they indicate how programs are structured and how code is shared among classes; on the other hand, they give information about compatible assignment rules, which are based on subtyping. In order to distinguish between code sharing, which is related to implementational aspects, and functional specialization, which is connected to the external behavior of objects, we introduce an algebraic specification-based formalism, by which one can specify the behavior of a class and state when a class inherits another one. It is shown that reusing inheritance can be reduced to specialization inheritance with respect to a virtual class. The class model and the two distinct aspects of inheritance allow the definition of clean interconnection mechanisms between classes leading to new classes which inherit from old classes their correctness and their semantics.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1963971880",
    "type": "article"
  },
  {
    "title": "Proof linking",
    "doi": "https://doi.org/10.1145/363516.363523",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Philip Fong; Robert D. Cameron",
    "corresponding_authors": "",
    "abstract": "Although mobile code systems typically employ link-time code verifiers to protect host computers from potentially malicious code, implementation flaws in the verifiers may still leave the host system vulnerable to attack. Compounding the inherent complexity of the verification algorithms themselves, the need to support lazy, dynamic linking in mobile code systems typically leads to architectures that exhibit strong interdependencies between the loader, the verifier, and the linker. To simplify verifier construction and provide improved assurances of verifier integrity, we propose a modular architecture based on the concept of proof linking. This architecture encapsulates the verification process and removes dependencies between the loader, the verifier, and the linker. We also formally model the process of proof linking and establish properties to which correct implementations must conform. As an example, we instantiate our architecture for the problem of Java bytecode verification and assess the correctness of this instantiation. Finally, we briefly discuss alternative mobile code verification architectures enabled by the proof-linking concept.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2095443555",
    "type": "article"
  },
  {
    "title": "Address translation in telecommunication features",
    "doi": "https://doi.org/10.1145/1005561.1005562",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Pamela Zave",
    "corresponding_authors": "Pamela Zave",
    "abstract": "Address translation causes a wide variety of interactions among telecommunication features. This article begins with a formal model of address translation and its effects, and with principles for understanding how features should interact in the presence of address translation. There is a simple and intuitive set of constraints on feature behavior so that features will interact according to the principles. This scheme (called \"ideal address translation\") has provable properties, is modular (explicit cooperation among features is not required), and supports extensibility (adding new features does not require changing old features). The article also covers reasoning in the presence of exceptions to the constraints, limitations of the theory, relation to real networks and protocols, and relation to other research.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2000534502",
    "type": "article"
  },
  {
    "title": "Post-release reliability growth in software products",
    "doi": "https://doi.org/10.1145/13487689.13487690",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Pankaj Jalote; Thomas Brendan Murphy; Vibhu Saujanya Sharma",
    "corresponding_authors": "",
    "abstract": "Most software reliability growth models work under the assumption that reliability of software grows due to the removal of bugs that cause failures. However, another phenomenon has often been observed—the failure rate of a software product following its release decreases with time even if no bugs are corrected. In this article we present a simple model to represent this phenomenon. We introduce the concept of initial transient failure rate of the product and assume that it decays with a factor α per unit time thereby increasing the product reliability with time. When the transient failure rate decays away, the product displays a steady state failure rate. We discuss how the parameters in this model—initial transient failure rate, decay factor, and steady state failure rate—can be determined from the failure and sales data of a product. We also describe how, using the model, we can determine the product stabilization time—a product quality metric that describes how long it takes a product to reach close to its stable failure rate. We provide many examples where this model has been applied to data from released products.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1964508730",
    "type": "article"
  },
  {
    "title": "A verification system for interval-based specification languages",
    "doi": "https://doi.org/10.1145/1734229.1734232",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Chunqing Chen; Jin Song Dong; Jun Sun; Andrew Martin",
    "corresponding_authors": "",
    "abstract": "Interval-based specification languages have been used to formally model and rigorously reason about real-time computing systems. This usually involves logical reasoning and mathematical computation with respect to continuous or discrete time. When these systems are complex, analyzing their models by hand becomes error-prone and difficult. In this article, we develop a verification system to facilitate the formal analysis of interval-based specification languages with machine-assisted proof support. The verification system is developed using a generic theorem prover, Prototype Verification System (PVS). Our system elaborately encodes a highly expressive set-based notation, Timed Interval Calculus (TIC), and can rigorously carry out the verification of TIC models at an interval level. We validated all TIC reasoning rules and discovered subtle flaws in the original rules. We also apply TIC to model Duration Calculus (DC), which is a popular interval-based specification language, and thus expand the capacity of the verification system. We can check the correctness of DC axioms, and execute DC proofs in a manner similar to the corresponding pencil-and-paper DC arguments.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2075262062",
    "type": "article"
  },
  {
    "title": "Augmenting Field Data for Testing Systems Subject to Incremental Requirements Changes",
    "doi": "https://doi.org/10.1145/3053430",
    "publication_date": "2017-01-31",
    "publication_year": 2017,
    "authors": "Daniel Di Nardo; Fabrizio Pastore; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "When testing data processing systems, software engineers often use real-world data to perform system-level testing. However, in the presence of new data requirements, software engineers may no longer benefit from having real-world data with which to perform testing. Typically, new test inputs complying with the new requirements have to be manually written. We propose an automated model-based approach that combines data modelling and constraint solving to modify existing field data to generate test inputs for testing new data requirements. The approach scales in the presence of complex and structured data, thanks to both the reuse of existing field data and the adoption of an innovative input generation algorithm based on slicing the model into parts. We validated the scalability and effectiveness of the proposed approach using an industrial case study. The empirical study shows that the approach scales in the presence of large amounts of structured and complex data. The approach can produce, within a reasonable time, test input data that is over ten times larger in size than the data generated with constraint solving only. We also demonstrate that the generated test inputs achieve more code coverage than the test cases implemented by experienced software engineers.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2613553457",
    "type": "article"
  },
  {
    "title": "Maintaining Architecture-Implementation Conformance to Support Architecture Centrality",
    "doi": "https://doi.org/10.1145/3229048",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Yongjie Zheng; Cuong Cu; Richard N. Taylor",
    "corresponding_authors": "",
    "abstract": "Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2811109404",
    "type": "article"
  },
  {
    "title": "Methodology Matters: How We Study Socio-Technical Aspects in Software Engineering",
    "doi": null,
    "publication_date": "2019-08-29",
    "publication_year": 2019,
    "authors": "Courtney Williams; Margaret-Anne D. Storey; Neil Ernst; Alexey Zagalsky; Eirini Kalliamvakou",
    "corresponding_authors": "",
    "abstract": "Modern software engineering involves both human and technical aspects, the importance of which is widely accepted by practitioners and researchers alike. At a community level, software engineering researchers may be expected to choose a balance of research strategies that capture both social and technical characteristics of software development. In this paper, we consider if the research strategies we use do in fact provide this balance. We first developed a research strategy framework to help distinguish research strategies that directly study human and social aspects, from strategies that rely on data such as trace, archival or simulated data, and those that may focus more on technical or system aspects. We utilized this framework to categorize the research strategies used by 253 technical track papers from the International Conference on Software Engineering (ICSE). Using a design science lens, we further identified the types of research contributions provided in these papers---either descriptive knowledge, or the design and evaluation of technical solutions. We mapped the contribution types to the research strategies identified. We found that, at the community level, the papers we analyzed strongly favour data strategies over strategies that directly study human and social aspects, and most research contributions consist of the design or evaluation of technical solutions. We conclude by proposing that our community should diversify our use of research strategies so that we may have a deeper understanding of human and social aspects of software development practice, while balancing the design and evaluation of innovations on the technical side.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3144297024",
    "type": "article"
  },
  {
    "title": "Residual Investigation",
    "doi": "https://doi.org/10.1145/2656201",
    "publication_date": "2014-12-23",
    "publication_year": 2014,
    "authors": "Kaituo Li; Christoph Reichenbach; Christoph Csallner; Yannis Smaragdakis",
    "corresponding_authors": "",
    "abstract": "We introduce the concept of residual investigation for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted program fault is likely to be realizable and relevant. The key feature of a residual investigation is that it has to be much more precise (i.e., with fewer false warnings) than the static analysis alone, yet significantly more general (i.e., reporting more errors) than the dynamic tests in the program's test suite that are pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when considered in conjunction with the static error report, increase confidence in the existence or severity of an error without needing to directly observe a fault resulting from the error. We enhance the static analyzer FindBugs with several residual investigations appropriately tuned to the static error patterns in FindBugs, and apply it to nine large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (false positives) while reporting several actual errors that would not have been detected by mere execution of a program's test suite.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2089891494",
    "type": "article"
  },
  {
    "title": "Type-Based Call Graph Construction Algorithms for Scala",
    "doi": "https://doi.org/10.1145/2824234",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "Karim Ali; Marianna Rapoport; Ondřej Lhoták; Julian Dolby; Frank Tip",
    "corresponding_authors": "",
    "abstract": "Call graphs have many applications in software engineering. For example, they serve as the basis for code navigation features in integrated development environments and are at the foundation of static analyses performed in verification tools. While many call graph construction algorithms have been presented in the literature, we are not aware of any that handle Scala features such as traits and abstract type members. Applying existing algorithms to the JVM bytecodes generated by the Scala compiler produces very imprecise results because type information is lost during compilation. We adapt existing type-based call graph construction algorithms to Scala and present a formalization based on Featherweight Scala. An experimental evaluation shows that our most precise algorithm generates call graphs with 1.1--3.7 times fewer nodes and 1.5--17.3 times fewer edges than a bytecode-based RTA analysis.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2291690114",
    "type": "article"
  },
  {
    "title": "Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization",
    "doi": "https://doi.org/10.1145/3628158",
    "publication_date": "2023-10-16",
    "publication_year": 2023,
    "authors": "Yi Xiang; Han Huang; Si‐Zhe Li; Miqing Li; Chuan Luo; Xiaowei Yang",
    "corresponding_authors": "",
    "abstract": "A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t -wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387664638",
    "type": "article"
  },
  {
    "title": "A Post-training Framework for Improving the Performance of Deep Learning Models via Model Transformation",
    "doi": "https://doi.org/10.1145/3630011",
    "publication_date": "2023-10-23",
    "publication_year": 2023,
    "authors": "Jiajun Jiang; Junjie Yang; Yingyi Zhang; Zan Wang; Hanmo You; Junjie Chen",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) techniques have attracted much attention in recent years and have been applied to many application scenarios. To improve the performance of DL models regarding different properties, many approaches have been proposed in the past decades, such as improving the robustness and fairness of DL models to meet the requirements for practical use. Among existing approaches, post-training is an effective method that has been widely adopted in practice due to its high efficiency and good performance. Nevertheless, its performance is still limited due to the incompleteness of training data. Additionally, existing approaches are always specifically designed for certain tasks, such as improving model robustness, which cannot be used for other purposes. In this article, we aim to fill this gap and propose an effective and general post-training framework, which can be adapted to improve the model performance from different aspects. Specifically, it incorporates a novel model transformation technique that transforms a classification model into an isomorphic regression model for fine-tuning, which can effectively overcome the problem of incomplete training data by forcing the model to strengthen the memory of crucial input features and thus improve the model performance eventually. To evaluate the performance of our framework, we have adapted it to two emerging tasks for improving DL models, i.e., robustness and fairness improvement, and conducted extensive studies by comparing it with state-of-the-art approaches. The experimental results demonstrate that our framework is indeed general, as it is effective in both tasks. Specifically, in the task of robustness improvement, our approach Dare has achieved the best results on 61.1% cases (vs. 11.1% cases achieved by baselines). In the task of fairness improvement, our approach FMT can effectively improve the fairness without sacrificing the accuracy of the models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387868939",
    "type": "article"
  },
  {
    "title": "Simulating reactive systems by deduction",
    "doi": "https://doi.org/10.1145/151257.151259",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Yishai Feldman; Haim Schneider",
    "corresponding_authors": "",
    "abstract": "Debugging is one of the main uses of simulation. Localizing bugs or finding the reasons for unclear behavior involves going backwards in time, whereas simulation goes forward in time. Therefore, identifying causes with the aid of most existing simulation tools usually requires repeating the simulation several times, each time with reduced holes in the sieve. An alternative is simulation by deduction, a technique in which the steps in the dynamic behavior of the simulated model are deduced by a reasoning system. A simulation system that uses simulation by deduction can give direct answers to questions about the reasons for the simulation results. By recording the support for its deductions, such a system can answer “why” and “why not” questions about the scenario. Another benefit of simulation by deduction is that it enables symbolic simulation, that is, simulating a scenario given only a partial description of the environment and the simulated model. This allows verifying properties of an evolving design at any stage of the design process, and thus checking the consequences of the design decisions made so far. In order to allow deducing as much as possible from partial information, the axiom system has to be minimalistic, i.e., axioms have to require the minimum amount of knowledge of simulation inputs. These ideas were implemented in a system called SIP, which simulates the behavior of reactive systems. SIP is capable of answering “why,” “why not,” and “what if” questions. It also has a limited capability of dealing with partial knowledge. SIP is based on a reasoning system that is responsible for deducing the effects of the external inputs on the state of the simulated model, and recording the support for its deductions. The logical basis for the deduction of a step in SIP is provided by a minimalistic axiom system for statecharts. Although SIP simulates reactive systems described as statecharts, the principle of simulation by deduction is applicable to other types of systems and descriptions, provided only that they have a well-defined formal semantics.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1967730132",
    "type": "article"
  },
  {
    "title": "Toward formalizing structured analysis",
    "doi": "https://doi.org/10.1145/268411.268429",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Luciano Baresi; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "Real-time extensions to structured analysis (SA/RT) are popular in industrial practice. Despite the large industrial experience and the attempts to formalize the various “dialects,” SA/RT notations are still imprecise and ambiguous. This article tries to identify the semantic problems of the requirements definition notation defined by Hatley and Pirbhai, one of the popular SA/RT “dialects,” and discusses possible solutions. As opposed to other articles that give their own interpretation, this article does not propose a specific semantics for the notation. This article identifies imprecisions, i.e., missing or partial information about features of the notation; it discusses ambiguities, i.e., elements of the definition that allow at least two different (“reasonable”) interpretations of features of the notation; and it lists extensions, i.e., features not belonging to the notation, but required by many industrial users and often supported by CASE tools. This article contributes by clarifying whether specific interpretations can be given unique semantics or retain ambiguities of the original definition. The article allows for the evaluation of formal definitions by indicating alternatives and consequences of the specific choices.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1989185354",
    "type": "article"
  },
  {
    "title": "Equivalence analysis and its application in improving the efficiency of program slicing",
    "doi": "https://doi.org/10.1145/567793.567796",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Donglin Liang; Mary Jean Harrold",
    "corresponding_authors": "",
    "abstract": "Existing methods for handling pointer variables during dataflow analyses can make such analyses inefficient in both time and space because the data-flow analyses must store and propagate large sets of data facts that are introduced by dereferences of pointer variables. This article presents equivalence analysis , a general technique to improve the efficiency of data-flow analyses in the presence of pointer variables. The technique identifies equivalence relations among the memory locations accessed by a procedure, and ensures that two equivalent memory locations share the same set of data facts in a procedure and in the procedures that are called by that procedure. Thus, a data-flow analysis needs to compute the data-flow information for only a representative memory location in an equivalence class. The data-flow information for other memory locations in the equivalence class can be derived from that of the representative memory location. The article also shows the extension to an interprocedural slicing algorithm that uses equivalence analysis to improve the efficiency of the algorithm. Our empirical studies suggest that equivalence analysis may effectively improve the efficiency of many data-flow analyses.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1985899097",
    "type": "article"
  },
  {
    "title": "Model checking the Java metalocking algorithm",
    "doi": "https://doi.org/10.1145/1243987.1243990",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "Samik Basu; Scott A. Smolka",
    "corresponding_authors": "",
    "abstract": "We report on our efforts to use the XMC model checker to model and verify the Java metalocking algorithm. XMC [Ramakrishna et al. 1997] is a versatile and efficient model checker for systems specified in XL, a highly expressive value-passing language. Metalocking [Agesen et al. 1999] is a highly-optimized technique for ensuring mutually exclusive access by threads to object monitor queues and, therefore; plays an essential role in allowing Java to offer concurrent access to objects. Metalocking can be viewed as a two-tiered scheme. At the upper level, the metalock level, a thread waits until it can enqueue itself on an object's monitor queue in a mutually exclusive manner. At the lower level, the monitor-lock level, enqueued threads race to obtain exclusive access to the object. Our abstract XL specification of the metalocking algorithm is fully parameterized, both on the number of threads M , and the number of objects N . It also captures a sophisticated optimization of the basic metalocking algorithm known as extra-fast locking and unlocking of uncontended objects. Using XMC, we show that for a variety of values of M and N , the algorithm indeed provides mutual exclusion and freedom from deadlock and lockout at the metalock level. We also show that, while the monitor-lock level of the protocol preserves mutual exclusion and deadlock-freedom, it is not lockout-free because the protocol's designers chose to give equal preference to awaiting threads and newly arrived threads.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1983202289",
    "type": "article"
  },
  {
    "title": "Feature Matching-based Approaches to Improve the Robustness of Android Visual GUI Testing",
    "doi": "https://doi.org/10.1145/3477427",
    "publication_date": "2021-11-17",
    "publication_year": 2021,
    "authors": "Luca Ardito; A. Bottino; Riccardo Coppola; Fabrizio Lamberti; Francesco Manigrasso; Lia Morra; Marco Torchiano",
    "corresponding_authors": "",
    "abstract": "In automated Visual GUI Testing (VGT) for Android devices, the available tools often suffer from low robustness to mobile fragmentation, leading to incorrect results when running the same tests on different devices. To soften these issues, we evaluate two feature matching-based approaches for widget detection in VGT scripts, which use, respectively, the complete full-screen snapshot of the application ( Fullscreen ) and the cropped images of its widgets ( Cropped ) as visual locators to match on emulated devices. Our analysis includes validating the portability of different feature-based visual locators over various apps and devices and evaluating their robustness in terms of cross-device portability and correctly executed interactions. We assessed our results through a comparison with two state-of-the-art tools, EyeAutomate and Sikuli. Despite a limited increase in the computational burden, our Fullscreen approach outperformed state-of-the-art tools in terms of correctly identified locators across a wide range of devices and led to a 30% increase in passing tests. Our work shows that VGT tools’ dependability can be improved by bridging the testing and computer vision communities. This connection enables the design of algorithms targeted to domain-specific needs and thus inherently more usable and robust.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3212533339",
    "type": "article"
  },
  {
    "title": "Why Do Developers Reject Refactorings in Open-Source Projects?",
    "doi": "https://doi.org/10.1145/3487062",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Jevgenija Pantiuchina; Bin Lin; Fiorella Zampetti; Massimiliano Di Penta; Michele Lanza; Gabriele Bavota",
    "corresponding_authors": "",
    "abstract": "Refactoring operations are behavior-preserving changes aimed at improving source code quality. While refactoring is largely considered a good practice, refactoring proposals in pull requests are often rejected after the code review. Understanding the reasons behind the rejection of refactoring contributions can shed light on how such contributions can be improved, essentially benefiting software quality. This article reports a study in which we manually coded rejection reasons inferred from 330 refactoring-related pull requests from 207 open-source Java projects. We surveyed 267 developers to assess their perceived prevalence of these identified rejection reasons, further complementing the reasons. Our study resulted in a comprehensive taxonomy consisting of 26 refactoring-related rejection reasons and 21 process-related rejection reasons. The taxonomy, accompanied with representative examples and highlighted implications, provides developers with valuable insights on how to ponder and polish their refactoring contributions, and indicates a number of directions researchers can pursue toward better refactoring recommenders.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4200533183",
    "type": "article"
  },
  {
    "title": "Use case and task models",
    "doi": "https://doi.org/10.1145/2491509.2491521",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Daniel Sinnig; Patrice Chalin; Ferhat Khendek",
    "corresponding_authors": "",
    "abstract": "User Interface (UI) development methods are poorly integrated with standard software engineering practice. The differences in terms of artifacts involved, development philosophies, and lifecycles can often result in inconsistent system and UI specifications leading to duplication of effort and increased maintenance costs. To address such shortcomings, we propose an integrated development methodology for use case and task models. Use cases are generally used to capture functional requirements whereas task models specify the detailed user interactions with the UI. Our methodology can assist practitioners in developing software processes which allow these two kinds of artifacts to be developed in a codependent and integrated manner. We present our methodology, describe its semantic foundations along with a set of formal conformance relations, and introduce an automated verification tool.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2014744822",
    "type": "article"
  },
  {
    "title": "Design and implementation of Sator",
    "doi": "https://doi.org/10.1145/1656250.1656254",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Antonio Brogi; Răzvan Popescu; Matteo Tanca",
    "corresponding_authors": "",
    "abstract": "Our long-term objective is to develop a general methodology for deploying (Web) service aggregation and adaptation middleware, capable of suitably overcoming syntactic and behavioral mismatches in view of application integration within and across organizational boundaries. This article focuses on describing the core aggregation process, which generates the workflow of a composite service from a set of service workflows to be aggregated and a data-flow mapping linking service parameters.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2086823525",
    "type": "article"
  },
  {
    "title": "Trading obliviousness for modularity with cooperative aspect-oriented programming",
    "doi": "https://doi.org/10.1145/2491509.2491516",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Kevin Hoffman; Patrick Eugster",
    "corresponding_authors": "",
    "abstract": "The potential of aspect-oriented programming to adequately capture crosscutting concerns has yet to be fully realized. For example, authors have detailed significant challenges in creating reusable aspect component libraries. One proposed solution is to introduce Explicit Join Points (EJPs) to increase modularity by reducing obliviousness, enabling a Cooperative Aspect-Oriented Programming (Co-AOP) methodology where base code and aspects synergistically collaborate. This article explores the trade-offs between obliviousness and modularity. We briefly introduce EJPs and Co-AOP, and hypothesize how to balance obliviousness and modularity using Co-AOP. We build upon a prior empirical study to refactor three real-life Java applications to implement the exception handling concern using three distinct strategies: (1) using fully oblivious aspects in AspectJ, (2) using EJPs in a fully explicit fashion, and (3) using EJPs while following the Co-AOP methodology. We study other crosscutting concerns by refactoring a fourth application, JHotDraw. The differences in terms of common code metrics are analyzed, and the impact on modularity is assessed using design structure matrices. Results indicate that the Co-AOP methodology can in many cases significantly improve code quality attributes versus fully oblivious or fully explicit approaches. We conclude with guiding principles on the proper use of EJPs within the Co-AOP methodology.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2172284542",
    "type": "article"
  },
  {
    "title": "A Logic-Based Approach for the Verification of UML Timed Models",
    "doi": "https://doi.org/10.1145/3106411",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Luciano Baresi; Angelo Morzenti; Alfredo Motta; Mohammad Mehdi Pourhashem Kallehbasti; Matteo Rossi",
    "corresponding_authors": "",
    "abstract": "This article presents a novel technique to formally verify models of real-time systems captured through a set of heterogeneous UML diagrams. The technique is based on the following key elements: (i) a subset of Unified Modeling Language (UML) diagrams, called Coretto UML (C-UML), which allows designers to describe the components of the system and their behavior through several kinds of diagrams (e.g., state machine diagrams, sequence diagrams, activity diagrams, interaction overview diagrams), and stereotypes taken from the UML Profile for Modeling and Analysis of Real-Time and Embedded Systems; (ii) a formal semantics of C-UML diagrams, defined through formulae of the metric temporal logic Tempo Reale ImplicitO (TRIO); and (iii) a tool, called Corretto, which implements the aforementioned semantics and allows users to carry out formal verification tasks on modeled systems. We validate the feasibility of our approach through a set of different case studies, taken from both the academic and the industrial domain.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2756108008",
    "type": "article"
  },
  {
    "title": "Scaling Up Symbolic Analysis by Removing Z-Equivalent States",
    "doi": "https://doi.org/10.1145/2652484",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Yueqi Li; Shing-Chi Cheung; Xiangyu Zhang; Yepang Liu",
    "corresponding_authors": "",
    "abstract": "Path explosion is a major issue in applying path-sensitive symbolic analysis to large programs. We observe that many symbolic states generated by the symbolic analysis of a procedure are indistinguishable to its callers. It is, therefore, possible to keep only one state from each set of equivalent symbolic states without affecting the analysis result. Based on this observation, we propose an equivalence relation called z-equivalence, which is weaker than logical equivalence, to relate a large number of z-equivalent states. We prove that z-equivalence is strong enough to guarantee that paths to be traversed by the symbolic analysis of two z-equivalent states are identical, giving the same solutions to satisfiability and validity queries. We propose a sound linear algorithm to detect z-equivalence. Our experiments show that the symbolic analysis that leverages z-equivalence is able to achieve more than ten orders of magnitude reduction in terms of search space. The reduction significantly alleviates the path explosion problem, enabling us to apply symbolic analysis in large programs such as Hadoop and Linux Kernel.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1973494035",
    "type": "article"
  },
  {
    "title": "Less is More",
    "doi": "https://doi.org/10.1145/2890494",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Esteban Pavese; Vı́ctor Braberman; Sebastián Uchitel",
    "corresponding_authors": "",
    "abstract": "Model-based reliability estimation of systems can provide useful insights early in the development process. However, computational complexity of estimating metrics such as mean time to first failure (MTTFF), turnaround time (TAT), or other domain-based quantitative measures can be prohibitive both in time, space, and precision. In this article, we present an alternative to exhaustive model exploration, as in probabilistic model checking, and partial random exploration, as in statistical model checking. Our hypothesis is that a (carefully crafted) partial systematic exploration of a system model can provide better bounds for these quantitative model metrics at lower computation cost. We present a novel automated technique for metric estimation that combines simulation, invariant inference, and probabilistic model checking. Simulation produces a probabilistically relevant set of traces from which a state invariant is inferred. The invariant characterises a partial model, which is then exhaustively explored using probabilistic model checking. We report on experiments that suggest that metric estimation using this technique (for both fully probabilistic models and those exhibiting nondeterminism) can be more effective than (full-model) probabilistic and statistical model checking, especially for system models for which the events of interest are rare.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2327602015",
    "type": "article"
  },
  {
    "title": "Mutant Reduction Evaluation: What is There and What is Missing?",
    "doi": "https://doi.org/10.1145/3522578",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Peng Zhang; Yang Wang; Xutong Liu; Yanhui Li; Yibiao Yang; Ziyuan Wang; Xiaoyu Zhou; Lin Chen; Yuming Zhou",
    "corresponding_authors": "",
    "abstract": "Background. Mutation testing is a commonly used defect injection technique for evaluating the effectiveness of a test suite. However, it is usually computationally expensive. Therefore, many mutation reduction strategies, which aim to reduce the number of mutants, have been proposed. Problem. It is important to measure the ability of a mutation reduction strategy to maintain test suite effectiveness evaluation. However, existing evaluation indicators are unable to measure the “order-preserving ability”, i.e., to what extent the mutation score order among test suites is maintained before and after mutation reduction. As a result, misleading conclusions can be achieved when using existing indicators to evaluate the reduction effectiveness. Objective. We aim to propose evaluation indicators to measure the “order-preserving ability” of a mutation reduction strategy, which is important but missing in our community. Method. Given a test suite on a Software Under Test (SUT) with a set of original mutants, we leverage the test suite to generate a group of test suites that have a partial order relationship in defect detecting ability. When evaluating a reduction strategy, we first construct two partial order relationships among the generated test suites in terms of mutation score, one with the original mutants and another with the reduced mutants. Then, we measure the extent to which the partial order under the original mutants remains unchanged in the partial order under the reduced mutants. The more partial order is unchanged, the stronger the Order Preservation ( OP ) of the mutation reduction strategy is, and the more effective the reduction strategy is. Furthermore, we propose Effort-aware Relative Order Preservation ( EROP ) to measure how much gain a mutation reduction strategy can provide compared with a random reduction strategy. Result. The experimental results show that OP and EROP are able to efficiently measure the “order-preserving ability” of a mutation reduction strategy. As a result, they have a better ability to distinguish various mutation reduction strategies compared with the existing evaluation indicators. In addition, we find that Subsuming Mutant Selection (SMS) and Clustering Mutant Selection (CMS) are more effective than the other strategies under OP and EROP. Conclusion. We suggest, for the researchers, that OP and EROP should be used to measure the effectiveness of a mutant reduction strategy, and for the practitioners, that SMS and CMS should be given priority in practice.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3127336347",
    "type": "article"
  },
  {
    "title": "<scp>Hippodrome</scp>: Data Race Repair Using Static Analysis Summaries",
    "doi": "https://doi.org/10.1145/3546942",
    "publication_date": "2022-07-07",
    "publication_year": 2022,
    "authors": "Andreea Costea; Abhishek Tiwari; Sigmund Chianasta; Rajiv Kishore; Abhik Roychoudhury; Ilya Sergey",
    "corresponding_authors": "",
    "abstract": "Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyze and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study, conducted on popular open-source projects, has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3189860654",
    "type": "article"
  },
  {
    "title": "OSS Effort Estimation Using Software Features Similarity and Developer Activity-Based Metrics",
    "doi": "https://doi.org/10.1145/3485819",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Ritu Kapur; Balwinder Sodhi",
    "corresponding_authors": "",
    "abstract": "Software development effort estimation (SDEE) generally involves leveraging the information about the effort spent in developing similar software in the past. Most organizations do not have access to sufficient and reliable forms of such data from past projects. As such, the existing SDEE methods suffer from low usage and accuracy. We propose an efficient SDEE method for open source software, which provides accurate and fast effort estimates. The significant contributions of our paper are i) Novel SDEE software metrics derived from developer activity information of various software repositories, ii) SDEE dataset comprising the SDEE metrics' values derived from $\\approx13,000$ GitHub repositories from 150 different software categories, iii) an effort estimation tool based on SDEE metrics and a software description similarity model. Our software description similarity model is basically a machine learning model trained using the Paragraph Vectors algorithm on the software product descriptions of GitHub repositories. Given the software description of a newly-envisioned software, our tool yields an effort estimate for developing it. Our method achieves the highest Standard Accuracy score of 87.26% (with cliff's $\\delta$=0.88 at 99.999% confidence level) and 42.7% with the Automatic Transformed Linear Baseline model. Our software artifacts are available at https://doi.org/10.5281/zenodo.5095723.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3199078364",
    "type": "article"
  },
  {
    "title": "A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning",
    "doi": "https://doi.org/10.1145/3511805",
    "publication_date": "2022-04-19",
    "publication_year": 2022,
    "authors": "Aurora Ramírez; Robert Feldt; José Raúl Romero",
    "corresponding_authors": "",
    "abstract": "Most software companies have extensive test suites and re-run parts of them continuously to ensure recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test (SUT) and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This paper analyses two decades of TCP research, and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to SUT code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4224245566",
    "type": "article"
  },
  {
    "title": "Feedback-Directed Metamorphic Testing",
    "doi": "https://doi.org/10.1145/3533314",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Chang‐ai Sun; Hepeng Dai; Huai Liu; Tsong Yueh Chen",
    "corresponding_authors": "",
    "abstract": "Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify “good” metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4281393087",
    "type": "article"
  },
  {
    "title": "DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class",
    "doi": "https://doi.org/10.1145/3546946",
    "publication_date": "2022-07-22",
    "publication_year": 2022,
    "authors": "Luke Dramko; Jeremy Lacomis; Pengcheng Yin; Ed Schwartz; Miltiadis Allamanis; Graham Neubig; Bogdan Vasilescu; Claire Le Goues",
    "corresponding_authors": "",
    "abstract": "The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE . We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE ’s overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4286488921",
    "type": "article"
  },
  {
    "title": "Semantics Foundation for Cyber-physical Systems Using Higher-order UTP",
    "doi": "https://doi.org/10.1145/3517192",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Xiong Xu; Jean-Pierre Talpin; Shuling Wang; Bohua Zhan; Naijun Zhan",
    "corresponding_authors": "",
    "abstract": "Model-based design has become the predominant approach to the design of hybrid and cyber-physical systems (CPSs). It advocates the use of mathematically founded models to capture heterogeneous digital and analog behaviours from domain-specific formalisms, allowing all engineering tasks of verification, code synthesis, and validation to be performed within a single semantic body. Guaranteeing the consistency among the different views and heterogeneous models of a system at different levels of abstraction, however, poses significant challenges. To address these issues, Hoare and He’s Unifying Theories of Programming (UTP) proposes a calculus to capture domain-specific programming and modelling paradigms into a unified semantic framework. Our goal is to extend UTP to form a semantic foundation for CPS design. Higher-order UTP (HUTP) is a conservative extension to Hoare and He’s theory that supports the specification of discrete, real-time, and continuous dynamics, concurrency and communication, and higher-order quantification. Within HUTP, we define a calculus of normal hybrid designs to model, analyse, compose, refine, and verify heterogeneous hybrid system models. In addition, we define respective formal semantics for Hybrid Communicating Sequential Processes and Simulink using HUTP.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4293235771",
    "type": "article"
  },
  {
    "title": "Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications",
    "doi": "https://doi.org/10.1145/3583564",
    "publication_date": "2023-02-08",
    "publication_year": 2023,
    "authors": "Yongqiang Tian; Wuqi Zhang; Ming Wen; Shing-Chi Cheung; C. P. Sun; Shiqing Ma; Yu Jiang",
    "corresponding_authors": "",
    "abstract": "Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available. To this end, we propose Dflare , a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models’ outputs or trigger previously unobserved models’ probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84×–446.06× as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186–1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4319451761",
    "type": "article"
  },
  {
    "title": "Stress Testing Control Loops in Cyber-physical Systems",
    "doi": "https://doi.org/10.1145/3624742",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Claudio Mandrioli; Seung Yeob Shin; Martina Maggio; Domenico Bianculli; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Cyber-Physical Systems (CPSs) are often safety-critical and deployed in uncertain environments. Identifying scenarios where CPSs do not comply with requirements is fundamental but difficult due to the multidisciplinary nature of CPSs. We investigate the testing of control-based CPSs, where control and software engineers develop the software collaboratively. Control engineers make design assumptions during system development to leverage control theory and obtain guarantees on CPS behaviour. In the implemented system, however, such assumptions are not always satisfied, and their falsification can lead to loss of guarantees. We define stress testing of control-based CPSs as generating tests to falsify such design assumptions. We highlight different types of assumptions, focusing on the use of linearised physics models. To generate stress tests falsifying such assumptions, we leverage control theory to qualitatively characterise the input space of a control-based CPS. We propose a novel test parametrisation for control-based CPSs and use it with the input space characterisation to develop a stress testing approach. We evaluate our approach on three case study systems, including a drone, a continuous-current motor (in five configurations), and an aircraft.Our results show the effectiveness of the proposed testing approach in falsifying the design assumptions and highlighting the causes of assumption violations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4322717938",
    "type": "article"
  },
  {
    "title": "Structured Theorem for Quantum Programs and its Applications",
    "doi": "https://doi.org/10.1145/3587154",
    "publication_date": "2023-03-10",
    "publication_year": 2023,
    "authors": "Nengkun Yu",
    "corresponding_authors": "Nengkun Yu",
    "abstract": "This article proves a structured program theorem for flowchart quantum programs. The theorem states that any flowchart quantum program is equivalent to a single quantum program that repeatedly executes a quantum measurement and a subprogram, so long as the measurement outcome is true. Moreover, their expected runtime, variance, and general moments are the same. This theorem simplifies the quantum program’s verification significantly. – We derive an analytical characterization of the termination problem for quantum programs in polynomial time. Our procedure is more efficient and accurate with much simpler techniques than the analysis of this problem, as described in [ 29 ]. – We compute the expected runtime analytically and exactly for quantum programs in polynomial time. This result improves the methods based on the weakest precondition calculus for the question recently developed in [ 31 , 34 ]. – We show that a single loop rule is a relatively complete Hoare logic for quantum programs after applying our structured theorem. Although using fewer rules, our method verifies a broader class of quantum programs, compared with the results in [ 45 ] and [ 56 ].",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4323864072",
    "type": "article"
  },
  {
    "title": "UniLoc: Unified Fault Localization of Continuous Integration Failures",
    "doi": "https://doi.org/10.1145/3593799",
    "publication_date": "2023-05-08",
    "publication_year": 2023,
    "authors": "Foyzul Hassan; Na Meng; Xiaoyin Wang",
    "corresponding_authors": "",
    "abstract": "Continuous integration (CI) practices encourage developers to frequently integrate code into a shared repository. Each integration is validated by automatic build and testing such that errors are revealed as early as possible. When CI failures or integration errors are reported, existing techniques are insufficient to automatically locate the root causes for two reasons. First, a CI failure may be triggered by faults in source code and/or build scripts, whereas current approaches consider only source code. Second, a tentative integration can fail because of build failures and/or test failures, whereas existing tools focus on test failures only. This article presents UniLoc, the first unified technique to localize faults in both source code and build scripts given a CI failure log, without assuming the failure’s location (source code or build scripts) and nature (a test failure or not). Adopting the information retrieval (IR) strategy, UniLoc locates buggy files by treating source code and build scripts as documents to search and by considering build logs as search queries. However, instead of naïvely applying an off-the-shelf IR technique to these software artifacts, for more accurate fault localization, UniLoc applies various domain-specific heuristics to optimize the search queries, search space, and ranking formulas. To evaluate UniLoc, we gathered 700 CI failure fixes in 72 open source projects that are built with Gradle. UniLoc could effectively locate bugs with the average mean reciprocal rank value as 0.49, mean average precision value as 0.36, and normalized discounted cumulative gain value as 0.54. UniLoc outperformed the state-of-the-art IR-based tool BLUiR and Locus. UniLoc has the potential to help developers diagnose root causes for CI failures more accurately and efficiently.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4375861705",
    "type": "article"
  },
  {
    "title": "API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model",
    "doi": "https://doi.org/10.1145/3607188",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Qing Huang; Yanbang Sun; Zhenchang Xing; Min Yu; Xiwei Xu; Qinghua Lu",
    "corresponding_authors": "",
    "abstract": "Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4382938482",
    "type": "article"
  },
  {
    "title": "LaF: Labeling-free Model Selection for Automated Deep Neural Network Reusing",
    "doi": "https://doi.org/10.1145/3611666",
    "publication_date": "2023-07-31",
    "publication_year": 2023,
    "authors": "Qiang Hu; Yuejun Guo; Xiaofei Xie; Maxime Cordy; Mike Papadakis; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Applying deep learning (DL) to science is a new trend in recent years, which leads DL engineering to become an important problem. Although training data preparation, model architecture design, and model training are the normal processes to build DL models, all of them are complex and costly. Therefore, reusing the open-sourced pre-trained model is a practical way to bypass this hurdle for developers. Given a specific task, developers can collect massive pre-trained deep neural networks from public sources for reusing. However, testing the performance (e.g., accuracy and robustness) of multiple deep neural networks (DNNs) and recommending which model should be used is challenging regarding the scarcity of labeled data and the demand for domain expertise. In this article, we propose a labeling-free (LaF) model selection approach to overcome the limitations of labeling efforts for automated model reusing. The main idea is to statistically learn a Bayesian model to infer the models’ specialty only based on predicted labels. We evaluate LaF using nine benchmark datasets, including image, text, and source code, and 165 DNNs, considering both the accuracy and robustness of models. The experimental results demonstrate that LaF outperforms the baseline methods by up to 0.74 and 0.53 on Spearman’s correlation and Kendall’s τ, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385428276",
    "type": "article"
  },
  {
    "title": "<scp>Horus</scp> : Accelerating Kernel Fuzzing through Efficient Host-VM Memory Access Procedures",
    "doi": "https://doi.org/10.1145/3611665",
    "publication_date": "2023-08-08",
    "publication_year": 2023,
    "authors": "Jianzhong Liu; Yuheng Shen; Yiru Xu; Hao Sun; Yu Jiang",
    "corresponding_authors": "",
    "abstract": "Kernel fuzzing is an effective technique in operating system vulnerability detection. Fuzzers such as Syzkaller and Moonshine frequently pass highly structured data between fuzzer processes in guest virtual machines and manager processes in the host operating system to synchronize fuzzing-relevant data and information. Since the guest virtual machines’ and the host operating system’s memory spaces are mutually isolated, fuzzers conduct synchronization operations using mechanisms such as Remote Procedure Calls over TCP/IP networks, incurring significant overheads that negatively impact the fuzzer’s efficiency and effectiveness in increasing code coverage and finding vulnerabilities. In this paper, we propose Horus , a kernel fuzzing data transfer mechanism that mitigates the aforementioned data transfer overheads. Horus removes host-VM memory isolation and performs data transfers through copying to and from target memory locations in the guest virtual machine. Horus facilitates such efficient transfers through using fixed stub structures in the guest’s memory space, whose addresses, along with the guest’s RAM contents, are exposed to the host during the fuzzer’s initialization process. When conducting transfers, Horus passes highly-structured non-trivial data between the host and guest instances through copying the data directly to and from the stub structures, reducing the overall overhead significantly compared to that of using a network-based approach. We implemented Horus upon state-of-the-art kernel fuzzers Syzkaller , Moonshine and kAFL and evaluated its effectiveness. For Syzkaller and Moonshine , Horus increased their transfer speeds by 84.5% and 85.8% for non-trivial workloads on average and improved their fuzzing throughputs by 31.07% and 30.62%, respectively. Syzkaller and Moonshine both achieved a coverage speedup of 1.6× through using Horus . For kAFL, Horus improved specifically its Redqueen component’s execution speeds by 19.4%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385664955",
    "type": "article"
  },
  {
    "title": "Understanding the Helpfulness of Stale Bot for Pull-Based Development: An Empirical Study of 20 Large Open-Source Projects",
    "doi": "https://doi.org/10.1145/3624739",
    "publication_date": "2023-09-19",
    "publication_year": 2023,
    "authors": "SayedHassan Khatoonabadi; Diego Elias Costa; Suhaib Mujahid; Emad Shihab",
    "corresponding_authors": "",
    "abstract": "Pull Requests (PRs) that are neither progressed nor resolved clutter the list of PRs, making it difficult for the maintainers to manage and prioritize unresolved PRs. To automatically track, follow up, and close such inactive PRs, Stale bot was introduced by GitHub. Despite its increasing adoption, there are ongoing debates on whether using Stale bot alleviates or exacerbates the problem of inactive PRs. To better understand if and how Stale bot helps projects in their pull-based development workflow, we perform an empirical study of 20 large and popular open source projects. We find that Stale bot can help deal with a backlog of unresolved PRs, as the projects closed more PRs within the first few months of adoption. Moreover, Stale bot can help improve the efficiency of the PR review process as the projects reviewed PRs that ended up merged and resolved PRs that ended up closed faster after the adoption. However, Stale bot can also negatively affect the contributors, as the projects experienced a considerable decrease in their number of active contributors after the adoption. Therefore, relying solely on Stale bot to deal with inactive PRs may lead to decreased community engagement and an increased probability of contributor abandonment.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386845625",
    "type": "article"
  },
  {
    "title": "Measurement of Embedding Choices on Cryptographic API Completion Tasks",
    "doi": "https://doi.org/10.1145/3625291",
    "publication_date": "2023-10-17",
    "publication_year": 2023,
    "authors": "Ya Xiao; Wenjia Song; Salman Ahmed; Xinyang Ge; Bimal Viswanath; Na Meng; Danfeng Yao",
    "corresponding_authors": "",
    "abstract": "In this article, we conduct a measurement study to comprehensively compare the accuracy impacts of multiple embedding options in cryptographic API completion tasks. Embedding is the process of automatically learning vector representations of program elements. Our measurement focuses on design choices of three important aspects, program analysis preprocessing , token-level embedding , and sequence-level embedding . Our findings show that program analysis is necessary even under advanced embedding. The results show 36.20% accuracy improvement, on average, when program analysis preprocessing is applied to transfer bytecode sequences into API dependence paths. With program analysis and the token-level embedding training, the embedding dep2vec improves the task accuracy from 55.80% to 92.04%. Moreover, only a slight accuracy advantage (0.55%, on average) is observed by training the expensive sequence-level embedding compared with the token-level embedding. Our experiments also suggest the differences made by the data. In the cross-app learning setup and a data scarcity scenario, sequence-level embedding is more necessary and results in a more obvious accuracy improvement (5.10%).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387708155",
    "type": "article"
  },
  {
    "title": "EASE: An effort-aware extension of unsupervised key class identification approaches",
    "doi": "https://doi.org/10.1145/3635714",
    "publication_date": "2023-12-02",
    "publication_year": 2023,
    "authors": "Weifeng Pan; Marouane Kessentini; Hua Ming; Zijiang Yang",
    "corresponding_authors": "",
    "abstract": "Key class identification approaches aim at identifying the most important classes to help developers, especially newcomers, start the software comprehension process. So far, many supervised and unsupervised approaches have been proposed; however, they have not considered the effort to comprehend classes. In this article, we identify the challenge of “ effort-aware key class identification ”; to partially tackle it, we propose an approach, EASE , which is implemented through a modification to existing unsupervised key class identification approaches to take into consideration the effort to comprehend classes. First, EASE chooses a set of network metrics that has a wide range of applications in the existing unsupervised approaches and also possesses good discriminatory power . Second, EASE normalizes the network metric values of classes to quantify the probability of any class to be a key class and utilizes Cognitive Complexity to estimate the effort required to comprehend classes. Third, EASE proposes a metric, RKCP , to measure the relative key-class proneness of classes and further uses it to sort classes in descending order. Finally, an effort threshold is utilized, and the top-ranked classes within the threshold are identified as the cost-effective key classes. Empirical results on a set of 18 software systems show that (i) the proposed effort-aware variants perform significantly better in almost all (≈98.33%) the cases, (ii) they are superior to most of the baseline approaches with only several exceptions, and (iii) they are scalable to large-scale software systems. Based on these findings, we suggest that (i) we should resort to effort-aware key class identification techniques in budget-limited scenarios; and (ii) when using different techniques, we should carefully choose the weighting mechanism to obtain the best performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4389268852",
    "type": "article"
  },
  {
    "title": "Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews",
    "doi": "https://doi.org/10.1145/3635712",
    "publication_date": "2023-12-06",
    "publication_year": 2023,
    "authors": "Alessio Ferrari; Thaide Huichapa; Paola Spoletini; Nicole Novielli; Davide Fucci; Daniela Girardi",
    "corresponding_authors": "",
    "abstract": "Capturing users’ engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users’ feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this article, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback ) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users’ engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1 ∼ 70% in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4389386412",
    "type": "article"
  },
  {
    "title": "Parallel and distributed incremental attribute evaluation algorithms for multiuser software development environments",
    "doi": "https://doi.org/10.1145/151299.151312",
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Gail E. Kaiser; Simon M. Kaplan",
    "corresponding_authors": "",
    "abstract": "The problem of change propagation in multiuser software development environments distributed across a local-area network is addressed. The program is modeled as an attributed parse tree segmented among multiple user processes and changes are modeled as subtree replacements requested asynchronously by individual users. Change propagation is then implemented using decentralized incremental evaluation of an attribute grammar that defines the static semantic properties of the programming language. Building up to our primary result, we first present algorithms that support parallel evaluation on a centralized tree in response to single edits using a singe editing cursor and multiple edits with multiple editing cursors. Then we present our algorithm for parallel evaluation on a decentralized tree. We also present a protocol to guarantee reliability of the evaluation algorithm as components of the decentralized tree become unavailable due to failures and return to availability.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1966683870",
    "type": "article"
  },
  {
    "title": "Formal specification and design of a message router",
    "doi": "https://doi.org/10.1145/201024.201026",
    "publication_date": "1994-10-01",
    "publication_year": 1994,
    "authors": "Christian Créveuil; Gruia-Catalin Roman",
    "corresponding_authors": "",
    "abstract": "Formal derivation refers to a family of design techniques that entail the development of programs which are guaranteed to be correct by construction. Only limited industrial use of such techniques (e.g., UNITY-style specification refinement) has been reported in the literature, and there is a great need for methodological developments aimed at facilitating their application to complex problems. This article examines the formal specification and design of a message router in an attempt to identify those methodological elements that are likely to contribute to successful industrial uses of program derivation. Although the message router cannot be characterized as being industrial grade, it is a sophisticated problem that poses significant specification and design challenges—its apparent simplicity is rather deceiving. The main body of the article consists of a complete formal specification of the router and a series of successive refinements that eventually lead to an immediate construction of a correct UNITY program. Each refinement is accompanied by its design rationale and is explained in a manner accessible to a broad audience. We use this example to make the case that program derivation provides a good basis for introducing rigor in the design strategy, regardless of the degrees of formality one is willing to consider.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2169731867",
    "type": "article"
  },
  {
    "title": "Modeling statecharts and activitycharts as signal equations",
    "doi": "https://doi.org/10.1145/384189.384191",
    "publication_date": "2001-10-01",
    "publication_year": 2001,
    "authors": "Jean-René Beauvais; Éric Rutten; Thierry Gautier; R. Houdebine; Paul Le Guernic; Youning Tang",
    "corresponding_authors": "",
    "abstract": "The languages for modeling reactive systems are of different styles, like the imperative, state-based ones and the declarative, data-flow ones. They are adapted to different application domains. This paper, through the example of the languages Statecharts and Signal, shows a way to give a model of an imperative specification (Statecharts) in a declarative, equational one (Signal). This model constitutes a formal model of the Statemate semantics of Statecharts, upon which formal analysis techniques can be applied. Being a transformation from an imperative to a declarative structure, it involves the definition of generic models for the explicit management of state (in the case of control as well as of data). In order to obtain a structural construction of the model, a hierarchical and modular organization is proposed, including proper management and propagation of control along the hierarchy. The results presented here cover the essential features of Statecharts as well as of another language of Statemate: Activitycharts. As a translation, it makes multiformalism specification possible, and provides support for the integrated operation of the languages. The motivation lies also in the perspective of gaining access to the various formal analysis and implementation tools of the synchronous technology, using the DC1 exchange format, as in the Sacres programming environment.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1973699168",
    "type": "article"
  },
  {
    "title": "Impact of classes of development coordination tools on software development performance",
    "doi": "https://doi.org/10.1145/1348250.1348257",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Amrit Tiwana",
    "corresponding_authors": "Amrit Tiwana",
    "abstract": "Although a diverse variety of software development coordination tools are widely used in practice, considerable debate surrounds their impact on software development performance. No large-scale field research has systematically examined their impact on software development performance. This paper reports the results of a multinational field study of software projects in 209 software development organizations to empirically examine the influence of six key classes of development coordination tools on the efficiency (reduction of development rework, budget compliance) and effectiveness (defect reduction) of software development performance. Based on an in-depth field study, the article conceptualizes six holistic classes of development coordination tools. The results provide nuanced insights—some counter to prevailing beliefs—into the relationships between the use of various classes of development coordination tools and software development performance. The overarching finding is that the performance benefits of development coordination tools are contingent on the salient types of novelty in a project. The dimension of development performance—efficiency or effectiveness—that each class of tools is associated with varies systematically with whether a project involves conceptual novelty, process novelty, multidimensional novelty (both process and conceptual novelty), or neither. Another noteworthy insight is that the use of some classes of tools introduces an efficiency-effectiveness tradeoff. Collectively, the findings are among the first to offer empirical support for the varied performance impacts of various classes of development coordination tools and have important implications for software development practice. The paper also identifies several promising areas for future research.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2062790964",
    "type": "article"
  },
  {
    "title": "How C++ Templates Are Used for Generic Programming",
    "doi": "https://doi.org/10.1145/3356579",
    "publication_date": "2020-01-30",
    "publication_year": 2020,
    "authors": "Lin Chen; Di Wu; Wanwangying Ma; Yuming Zhou; Baowen Xu; Hareton Leung",
    "corresponding_authors": "",
    "abstract": "Generic programming is a key paradigm for developing reusable software components. The inherent support for generic constructs is therefore important in programming languages. As for C++, the generic construct, templates, has been supported since the language was first released. However, little is currently known about how C++ templates are actually used in developing real software. In this study, we conduct an experiment to investigate the use of templates in practice. We analyze 1,267 historical revisions of 50 open source systems, consisting of 566 million lines of C++ code, to collect the data of the practical use of templates. We perform statistical analyses on the collected data and produce many interesting results. We uncover the following important findings: (1) templates are practically used to prevent code duplication, but this benefit is largely confined to a few highly used templates; (2) function templates do not effectively replace C-style generics, and developers with a C background do not show significant preference between the two language constructs; (3) developers seldom convert dynamic polymorphism to static polymorphism by using CRTP (Curiously Recursive Template Pattern); (4) the use of templates follows a power-law distribution in most cases, and C++ developers who prefer using templates are those without other language background; (5) C developer background seems to override C++ project guidelines. These findings are helpful not only for researchers to understand the tendency of template use but also for tool builders to implement better tools to support generic programming.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3008119296",
    "type": "article"
  },
  {
    "title": "RegionTrack",
    "doi": "https://doi.org/10.1145/3412377",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Xiaoxue Ma; Shangru Wu; Ernest Pobee; Xiupei Mei; Hao Zhang; Bo Jiang; W.K. Chan",
    "corresponding_authors": "",
    "abstract": "Atomicity is a correctness criterion to reason about isolated code regions in a multithreaded program when they are executed concurrently. However, dynamic instances of these code regions, called transactions , may fail to behave atomically, resulting in transactional atomicity violations. Existing dynamic online atomicity checkers incur either false positives or false negatives in detecting transactions experiencing transactional atomicity violations. This article proposes &lt;monospace&gt;RegionTrack&lt;/monospace&gt;. &lt;monospace&gt;RegionTrack&lt;/monospace&gt; tracks cross-thread dependences at the event, dynamic subregion, and transaction levels. It maintains both dynamic subregions within selected transactions and transactional happens-before relations through its novel timestamp propagation approach. We prove that &lt;monospace&gt;RegionTrack&lt;/monospace&gt; is sound and complete in detecting both transactional atomicity violations and non-serializable traces. To the best of our knowledge, it is the first online technique that precisely captures the transitively closed set of happens-before relations over all conflicting events with respect to every running transaction for the above two kinds of issues. We have evaluated &lt;monospace&gt;RegionTrack&lt;/monospace&gt; on 19 subjects of the DaCapo and the Java Grande Forum benchmarks. The empirical results confirm that &lt;monospace&gt;RegionTrack&lt;/monospace&gt; precisely detected all those transactions which experienced transactional atomicity violations and identified all non-serializable traces. The overall results also show that &lt;monospace&gt;RegionTrack&lt;/monospace&gt; incurred 1.10x and 1.08x lower memory and runtime overheads than &lt;monospace&gt;Velodrome&lt;/monospace&gt; and 2.10x and 1.21x lower than &lt;monospace&gt;Aerodrome&lt;/monospace&gt;, respectively. Moreover, it incurred 2.89x lower memory overhead than &lt;monospace&gt;DoubleChecker&lt;/monospace&gt;. On average, &lt;monospace&gt;Velodrome&lt;/monospace&gt; detected about 55% fewer violations than &lt;monospace&gt;RegionTrack&lt;/monospace&gt;, which in turn reported about 3%–70% fewer violations than &lt;monospace&gt;DoubleChecker&lt;/monospace&gt;.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3116389844",
    "type": "article"
  },
  {
    "title": "An Adaptive Search Budget Allocation Approach for Search-Based Test Case Generation",
    "doi": "https://doi.org/10.1145/3446199",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Simone Scalabrino; Antonio Mastropaolo; Gabriele Bavota; Rocco Oliveto",
    "corresponding_authors": "",
    "abstract": "Search-based techniques have been successfully used to automate test case generation. Such approaches allocate a fixed search budget to generate test cases aiming at maximizing code coverage. The search budget plays a crucial role; due to the hugeness of the search space, the higher the assigned budget, the higher the expected coverage. Code components have different structural properties that may affect the ability of search-based techniques to achieve a high coverage level. Thus, allocating a fixed search budget for all the components is not recommended and a component-specific search budget should be preferred. However, deciding the budget to assign to a given component is not a trivial task. In this article, we introduce Budget Optimization for Testing (BOT), an approach to adaptively allocate the search budget to the classes under test. BOT requires information about the branch coverage that will be achieved on each class with a given search budget. Therefore, we also introduce BRANCHOS, an approach that predicts coverage in a budget-aware way. The results of our experiments show that (i) BRANCHOS can approximate the branch coverage in time with a low error, and (ii) BOT can significantly increase the coverage achieved by a test generation tool and the effectiveness of generated tests.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3161262227",
    "type": "article"
  },
  {
    "title": "Guided Feature Identification and Removal for Resource-constrained Firmware",
    "doi": "https://doi.org/10.1145/3487568",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Ryan Williams; Tongwei Ren; Lorenzo De Carli; Long Lu; Gillian Smith",
    "corresponding_authors": "",
    "abstract": "IoT firmware oftentimes incorporates third-party components, such as network-oriented middleware and media encoders/decoders. These components consist of large and mature codebases, shipping with a variety of non-critical features. Feature bloat increases code size, complicates auditing/debugging, and reduces stability. This is problematic for IoT devices, which are severely resource-constrained and must remain operational in the field for years. Unfortunately, identification and complete removal of code related to unwanted features requires familiarity with codebases of interest, cumbersome manual effort, and may introduce bugs. We address these difficulties by introducing PRAT, a system that takes as input the codebase of software of interest, identifies and maps features to code, presents this information to a human analyst, and removes all code belonging to unwanted features. PRAT solves the challenge of identifying feature-related code through a novel form of differential dynamic analysis and visualizes results as user-friendly feature graphs . Evaluation on diverse codebases shows superior code removal compared to both manual feature deactivation and state-of-art debloating tools, and generality across programming languages. Furthermore, a user study comparing PRAT to manual code analysis shows that it can significantly simplify the feature identification workflow.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4200595125",
    "type": "article"
  },
  {
    "title": "Verification across Intellectual Property Boundaries",
    "doi": "https://doi.org/10.1145/2430545.2430550",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Sagar Chaki; Christian Schallhart; Helmut Veith",
    "corresponding_authors": "",
    "abstract": "In many industries, the importance of software components provided by third-party suppliers is steadily increasing. As the suppliers seek to secure their intellectual property (IP) rights, the customer usually has no direct access to the suppliers’ source code, and is able to enforce the use of verification tools only by legal requirements. In turn, the supplier has no means to convince the customer about successful verification without revealing the source code. This article presents an approach to resolve the conflict between the IP interests of the supplier and the quality interests of the customer. We introduce a protocol in which a dedicated server (called the “amanat”) is controlled by both parties: the customer controls the verification task performed by the amanat, while the supplier controls the communication channels of the amanat to ensure that the amanat does not leak information about the source code. We argue that the protocol is both practically useful and mathematically sound. As the protocol is based on well-known (and relatively lightweight) cryptographic primitives, it allows a straightforward implementation on top of existing verification tool chains. To substantiate our security claims, we establish the correctness of the protocol by cryptographic reduction proofs.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2007741236",
    "type": "article"
  },
  {
    "title": "Dynamic Dependence Summaries",
    "doi": "https://doi.org/10.1145/2968444",
    "publication_date": "2017-01-09",
    "publication_year": 2017,
    "authors": "Vijay Krishna Palepu; Guoqing Xu; James A. Jones",
    "corresponding_authors": "",
    "abstract": "Software engineers construct modern-day software applications by building on existing software libraries and components that they necessarily do not author themselves. Thus, contemporary software applications rely heavily on existing standard and third-party libraries for their execution and behavior. As such, effective runtime analysis of such a software application’s behavior is met with new challenges. To perform dynamic analysis of a software application, all transitively dependent external libraries must also be monitored and analyzed at each layer of the software application’s call stack. However, monitoring and analyzing large and often numerous external libraries may prove to be prohibitively expensive. Moreover, an overabundance of library-level analyses may obfuscate the details of the actual software application’s dynamic behavior. In other words, the extensive use of existing libraries by a software application renders the results of its dynamic analysis both expensive to compute and difficult to understand. We model software component behavior as dynamically observed data- and control dependencies between inputs and outputs of a software component. Such data- and control dependencies are monitored at a fine-grain instruction-level and are collected as dynamic execution traces for software runs. As an approach to address the complexities and expenses associated with analyzing dynamically observable behavior of software components, we summarize and reuse the data- and control dependencies between the inputs and outputs of software components. Dynamically monitored data- and control dependencies, between the inputs and outputs of software components, upon summarization are called dynamic dependence summaries . Software components, equipped with dynamic dependence summaries, afford the omission of their exhaustive runtime analysis. Nonetheless, the reuse of dependence summaries would necessitate the abstraction of any concrete runtime information enclosed within the summary, thus potentially causing a loss in the information modeled by the dependence summary. Therefore, benefits to the efficiency of dynamic analyses that use such summarization may be afforded with losses of accuracy. As such, we evaluate the potential accuracy loss and the potential performance gain with the use of dynamic dependence summaries. Our results show, on average, a 13× speedup with the use of dynamic dependence summaries, with an accuracy of 90% in a real-world software engineering task.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2570435792",
    "type": "article"
  },
  {
    "title": "Automated Reuse of Model Transformations through Typing Requirements Models",
    "doi": "https://doi.org/10.1145/3340108",
    "publication_date": "2019-09-02",
    "publication_year": 2019,
    "authors": "Juan de Lara; Esther Guerra; Davide Di Ruscio; Juri Di Rocco; Jesús Sánchez Cuadrado; Ludovico Iovino; Alfonso Pierantonio",
    "corresponding_authors": "",
    "abstract": "Model transformations are key elements of model-driven engineering, where they are used to automate the manipulation of models. However, they are typed with respect to concrete source and target meta-models, making their reuse for other (even similar) meta-models challenging. To improve this situation, we propose capturing the typing requirements for reusing a transformation with other meta-models by the notion of a typing requirements model (TRM). A TRM describes the prerequisites that a model transformation imposes on the source and target meta-models to obtain a correct typing. The key observation is that any meta-model pair that satisfies the TRM is a valid reuse context for the transformation at hand. A TRM is made of two domain requirement models (DRMs) describing the requirements for the source and target meta-models, and a compatibility model expressing dependencies between them. We define a notion of refinement between DRMs and see meta-models as a special case of DRM. We provide a catalogue of valid refinements and describe how to automatically extract a TRM from an ATL transformation. The approach is supported by our tool TOTEM. We report on two experiments—based on transformations developed by third parties and meta-model mutation techniques—validating the correctness and completeness of our TRM extraction procedure and confirming the power of TRMs to encode variability and support flexible reuse.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2971888983",
    "type": "article"
  },
  {
    "title": "Measuring Task Conflict and Person Conflict in Software Testing",
    "doi": "https://doi.org/10.1145/3395029",
    "publication_date": "2020-07-06",
    "publication_year": 2020,
    "authors": "Xihui Zhang; Thomas F. Stafford; Tao Hu; Hua Dai",
    "corresponding_authors": "",
    "abstract": "Task-related conflict and person-related conflict in software testing are inevitable and can impact the effectiveness and efficiency of the software development process. The dimensionality of conflict in software testing is reasonably well understood, although in past research both types of conflict have frequently been modeled as reflective constructs that can obstruct the effectiveness of their use as organizational assessment and training tools. One contribution of this study is an empirical model of conflict sources in software engineering; such sources of conflict threaten to derail efficient software development outcomes in firms. A second contribution of this research is the development of a formative measurement model for purposes of development of assessing task conflict and person conflict in software teams. These validated measures can be utilized as training and development instruments for on-the-job remediation of development team conflict. As is indicated in the organizational behavior and software engineering literature, deploying valid measures of workplace stressors such as conflict can lead to the managerial application of effective strategies and tactics to improve workplace morale and satisfaction, to the great benefit of productivity and retention.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3039687657",
    "type": "article"
  },
  {
    "title": "Automated Support for Reproducing and Debugging Field Failures",
    "doi": "https://doi.org/10.1145/2774218",
    "publication_date": "2015-09-02",
    "publication_year": 2015,
    "authors": "Wei Jin; Alessandro Orso",
    "corresponding_authors": "",
    "abstract": "As confirmed by a recent survey conducted among developers of the Apache, Eclipse, and Mozilla projects, two extremely challenging tasks during maintenance are reproducing and debugging field failures—failures that occur on user machines after release. To help developers with these tasks, in this article we present an overall approach that comprises two different techniques: B ug R edux and F 3 . B ug R edux is a general technique for reproducing field failures that collects dynamic data about failing executions in the field and uses this data to synthesize executions that mimic the observed field failures. F 3 leverages the executions generated by B ug R edux to perform automated debugging using a set of suitably optimized fault-localization techniques. To assess the usefulness of our approach, we performed an empirical evaluation of the approach on a set of real-world programs and field failures. The results of our evaluation are promising in that, for all the failures considered, our approach was able to (1) synthesize failing executions that mimicked the observed field failures, (2) synthesize passing executions similar to the failing ones, and (3) use the synthesized executions to successfully perform fault localization with accurate results.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2054213836",
    "type": "article"
  },
  {
    "title": "A Methodology for Exposing Risk in Achieving Emergent System Properties",
    "doi": "https://doi.org/10.1145/2560048",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Lucas Layman; Victor R. Basili; Marvin V. Zelkowitz",
    "corresponding_authors": "",
    "abstract": "Determining whether systems achieve desired emergent properties, such as safety or reliability, requires an analysis of the system as a whole, often in later development stages when changes are difficult and costly to implement. In this article we propose the Process Risk Indicator (PRI) methodology for analyzing and evaluating emergent properties early in the development cycle. A fundamental assumption of system engineering is that risk mitigation processes reduce system risks, yet these processes may also be a source of risk: (1) processes may not be appropriate for achieving the desired emergent property; or (2) processes may not be followed appropriately. PRI analyzes development process artifacts (e.g., designs pertaining to reliability or safety analysis reports) to quantify process risks that may lead to higher system risk. We applied PRI to the hazard analysis processes of a network-centric, Department of Defense system-of-systems and two NASA spaceflight projects to assess the risk of not achieving one such emergent property, software safety, during the early stages of the development lifecycle. The PRI methodology was used to create measurement baselines for process indicators of software safety risk, to identify risks in the hazard analysis process, and to provide feedback to projects for reducing these risks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2163122573",
    "type": "article"
  },
  {
    "title": "Preference-wise Testing of Android Apps via Test Amplification",
    "doi": "https://doi.org/10.1145/3511804",
    "publication_date": "2022-04-19",
    "publication_year": 2022,
    "authors": "Minxue Pan; Yifei Lu; Yu Pei; Tian Zhang; Xuandong Li",
    "corresponding_authors": "",
    "abstract": "Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore their impacts need to be considered when testing the apps. Unfortunately, few test cases explicitly specify the assignments of valid values to the preferences, or configurations , under which they should be executed, and few existing mobile testing tools take the impact of preferences into account or provide help to testers in identifying and setting up the configurations for running the tests. This article presents the Prefest approach to effective testing of Android apps with preferences. Given an Android app and a set of test cases for the app, Prefest amplifies the test cases with a small number of configurations to exercise more behaviors and detect more bugs that are related to preferences. In an experimental evaluation conducted on real-world Android apps, amplified test cases produced by Prefest from automatically generated test cases covered significantly more code of the apps and detected seven real bugs, and the tool’s test amplification time was at the same order of magnitude as the running time of the input test cases. Prefest ’s effectiveness and efficiency in amplifying programmer-written test cases was comparable with that in amplifying automatically generated test cases.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4224286019",
    "type": "article"
  },
  {
    "title": "Assessing the Alignment between the Information Needs of Developers and the Documentation of Programming Languages: A Case Study on Rust",
    "doi": "https://doi.org/10.1145/3546945",
    "publication_date": "2022-07-13",
    "publication_year": 2022,
    "authors": "Filipe R. Cogo; Xin Xia; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "Programming language documentation refers to the set of technical documents that provide application developers with a description of the high-level concepts of a language. Such documentation is essential to support application developers in the effective use of a programming language. One of the challenges faced by documenters (i.e., personnel that produce documentation) is to ensure that documentation has relevant information that aligns with the concrete needs of developers. In this paper, we present an automated approach to support documenters in evaluating the differences and similarities between the concrete information need of developers and the current state of documentation (a problem that we refer to as the topical alignment of a programming language documentation). Our approach leverages semi-supervised topic modelling to assess the similarities and differences between the topics of Q&A posts and the official documentation. To demonstrate the application of our approach, we perform a case study on the documentation of Rust. Our results show that there is a relatively high level of topical alignment in Rust documentation. Still, information about specific topics is scarce in both the Q&A websites and the documentation, particularly related topics with programming niches such as network, game, and database development. For other topics (e.g., related topics with language features such as structs, patterns and matchings, and foreign function interface), information is only available on Q&A websites while lacking in the official documentation. Finally, we discuss implications for programming language documenters, particularly how to leverage our approach to prioritize topics that should be added to the documentation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4285394558",
    "type": "article"
  },
  {
    "title": "On Proving the Correctness of Refactoring Class Diagrams of MDE Metamodels",
    "doi": "https://doi.org/10.1145/3549541",
    "publication_date": "2022-07-26",
    "publication_year": 2022,
    "authors": "Najd Altoyan; Don Batory",
    "corresponding_authors": "",
    "abstract": "Model Driven Engineering ( MDE ) is a general-purpose engineering methodology to elevate system design, maintenance, and analysis to corresponding activities on models. Models (graphical and/or textual) of a target application are automatically transformed into source code, performance models, Promela files (for model checking), and so on for system analysis and construction. Models are instances of metamodels . One form an MDE metamodel can take is a [class diagram, constraints] pair: the class diagram defines all object diagrams that could be metamodel instances; object constraint language ( OCL ) constraints eliminate semantically undesirable instances. A metamodel refactoring is an invertible semantics-preserving co-transformation, i.e., it transforms both a metamodel and its models without losing data. This article addresses a subproblem of metamodel refactoring: how to prove the correctness of refactorings of class diagrams without OCL constraints using the Coq Proof Assistant.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4287981760",
    "type": "article"
  },
  {
    "title": "Refactoring in Computational Notebooks",
    "doi": "https://doi.org/10.1145/3576036",
    "publication_date": "2022-12-13",
    "publication_year": 2022,
    "authors": "Eric S. Liu; Dylan A. Lukes; William G. Griswold",
    "corresponding_authors": "",
    "abstract": "Due to the exploratory nature of computational notebook development, a notebook can be extensively evolved even though it is small, potentially incurring substantial technical debt. Indeed, in interview studies notebook authors have attested to performing ongoing tidying and big cleanups. However, many notebook authors are not trained as software developers, and environments like JupyterLab possess few features to aid notebook maintenance. As software refactoring is traditionally a critical tool for reducing technical debt, we sought to better understand the unique and growing ecology of computational notebooks by investigating the refactoring of public Jupyter notebooks. We randomly selected 15,000 Jupyter notebooks hosted on GitHub and studied 200 with meaningful commit histories. We found that notebook authors do refactor, favoring a few basic classic refactorings as well as those involving the notebook cell construct. Those with a computing background refactored differently than others, but not more so. Exploration-focused notebooks had a unique refactoring profile compared to more exposition-focused notebooks. Authors more often refactored their code as they went along, rather than deferring maintenance to big cleanups. These findings point to refactoring being intrinsic to notebook development.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4311349022",
    "type": "article"
  },
  {
    "title": "Battling against Protocol Fuzzing: Protecting Networked Embedded Devices from Dynamic Fuzzers",
    "doi": "https://doi.org/10.1145/3641847",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Puzhuo Liu; Yaowen Zheng; C. P. Sun; Hong Li; Zhi Li; Limin Sun",
    "corresponding_authors": "",
    "abstract": "N etworked E mbedded D evices (NEDs) are increasingly targeted by cyberattacks, mainly due to their widespread use in our daily lives. Vulnerabilities in NEDs are the root causes of these cyberattacks. Although deployed NEDs go through thorough code audits, there can still be considerable exploitable vulnerabilities. Existing mitigation measures like code encryption and obfuscation adopted by vendors can resist static analysis on deployed NEDs, but are ineffective against protocol fuzzing. Attackers can easily apply protocol fuzzing to discover vulnerabilities and compromise deployed NEDs. Unfortunately, prior anti-fuzzing techniques are impractical as they significantly slow down NEDs, hampering NED availability. To address this issue, we propose Armor—the first anti-fuzzing technique specifically designed for NEDs. First, we design three adversarial primitives–delay, fake coverage, and forged exception–to break the fundamental mechanisms on which fuzzing relies to effectively find vulnerabilities. Second, based on our observation that inputs from normal users consistent with the protocol specification and certain program paths are rarely executed with normal inputs, we design static and dynamic strategies to decide whether to activate the adversarial primitives. Extensive evaluations show that Armor incurs negligible time overhead and effectively reduces the code coverage (e.g., line coverage by 22%-61%) for fuzzing, significantly outperforming the state of the art.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391110914",
    "type": "article"
  },
  {
    "title": "Learning Failure-Inducing Models for Testing Software-Defined Networks",
    "doi": "https://doi.org/10.1145/3641541",
    "publication_date": "2024-01-23",
    "publication_year": 2024,
    "authors": "Raphaël Ollando; Seung Yeob Shin; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating effective test data leading to failures in SDN-based systems and (2) learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, no existing work simultaneously addresses these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Furthermore, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines for learning failure-inducing models. Our results show that (1) compared to the state-of-the-art methods, FuzzSDN generates at least 12 times more failures, within the same time budget, with a controller that is fairly robust to fuzzing and (2) our failure-inducing models have, on average, a precision of 98% and a recall of 86%, significantly outperforming the baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391136370",
    "type": "article"
  },
  {
    "title": "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks",
    "doi": "https://doi.org/10.1145/3649596",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Xiaohui Wan; Zheng Zheng; Fangyun Qin; Xuhui Lu",
    "corresponding_authors": "",
    "abstract": "Defect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this article, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4) integrating data complexity information into the learning process can enhance an algorithm’s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392157755",
    "type": "article"
  },
  {
    "title": "Risky Dynamic Typing-related Practices in Python: An Empirical Study",
    "doi": "https://doi.org/10.1145/3649593",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Zhifei Chen; Lin Chen; Yibiao Yang; Qiong Feng; Xuansong Li; Wei Song",
    "corresponding_authors": "",
    "abstract": "Python’s dynamic typing nature provides developers with powerful programming abstractions. However, many type-related bugs are accumulated in code bases of Python due to the misuse of dynamic typing. The goal of this article is to aid in the understanding of developers’ high-risk practices toward dynamic typing and the early detection of type-related bugs. We first formulate the rules of six types of risky dynamic typing-related practices (type smells for short) in Python. We then develop a rule-based tool named RUPOR, which builds an accurate type base to detect type smells. Our evaluation shows that RUPOR outperforms the existing type smell detection techniques (including the Large Language Models–based approaches, Mypy, and PYDYPE) on a benchmark of 900 Python methods. Based on RUPOR, we conduct an empirical study on 25 real-world projects. We find that type smells are significantly related to the occurrence of post-release faults. The fault-proneness prediction model built with type smell features slightly outperforms the model built without them. We also summarize the common patterns, including inserting type check to fix type smell bugs. These findings provide valuable insights for preventing and fixing type-related bugs in the programs written in dynamic-typed languages.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392166154",
    "type": "article"
  },
  {
    "title": "Fine-grained Coverage-based Fuzzing - RCR Report",
    "doi": "https://doi.org/10.1145/3649592",
    "publication_date": "2024-02-27",
    "publication_year": 2024,
    "authors": "Wei‐Cheng Wu; Bernard Nongpoh; Marwan Nour; Michaël Marcozzi; Sébastien Bardin; Christophe Hauser",
    "corresponding_authors": "",
    "abstract": "This is the RCR report of the artifact for the article “Fine-grained Coverage-based Fuzzing.” This report contains scripts and pre-build binary programs to reproduce the results presented in the main article. The artifact is released on Zenodo with DOI: 10.5281/zenodo.7275184. We claim the artifact to be available, functional, and reusable. The technology skills needed to review the artifact are knowing how to use Linux/Unix terminal and a basic understanding of Docker.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392192191",
    "type": "article"
  },
  {
    "title": "MTL-TRANSFER: Leveraging Multi-task Learning and Transferred Knowledge for Improving Fault Localization and Program Repair",
    "doi": "https://doi.org/10.1145/3654441",
    "publication_date": "2024-03-27",
    "publication_year": 2024,
    "authors": "Xu Wang; Hongwei Yu; Xiangxin Meng; Hongliang Cao; Hongyu Zhang; Hailong Sun; Xudong Liu; Chunming Hu",
    "corresponding_authors": "",
    "abstract": "Fault localization (FL) and automated program repair (APR) are two main tasks of automatic software debugging. Compared with traditional methods, deep learning-based approaches have been demonstrated to achieve better performance in FL and APR tasks. However, the existing deep learning-based FL methods ignore the deep semantic features or only consider simple code representations. And for APR tasks, existing template-based APR methods are weak in selecting the correct fix templates for more effective program repair, which are also not able to synthesize patches via the embedded end-to-end code modification knowledge obtained by training models on large-scale bug-fix code pairs. Moreover, in most of FL and APR methods, the model designs and training phases are performed separately, leading to ineffective sharing of updated parameters and extracted knowledge during the training process. This limitation hinders the further improvement in the performance of FL and APR tasks. To solve the above problems, we propose a novel approach called MTL-TRANSFER, which leverages a multi-task learning strategy to extract deep semantic features and transferred knowledge from different perspectives. First, we construct a large-scale open-source bug datasets and implement 11 multi-task learning models for bug detection and patch generation sub-tasks on 11 commonly used bug types, as well as one multi-classifier to learn the relevant semantics for the subsequent fix template selection task. Second, an MLP-based ranking model is leveraged to fuse spectrum-based, mutation-based and semantic-based features to generate a sorted list of suspicious statements. Third, we combine the patches generated by the neural patch generation sub-task from the multi-task learning strategy with the optimized fix template selecting order gained from the multi-classifier mentioned above. Finally, the more accurate FL results, the optimized fix template selecting order, and the expanded patch candidates are combined together to further enhance the overall performance of APR tasks. Our extensive experiments on widely-used benchmark Defects4J show that MTL-TRANSFER outperforms all baselines in FL and APR tasks, proving the effectiveness of our approach. Compared with our previously proposed FL method TRANSFER-FL (which is also the state-of-the-art statement-level FL method), MTL-TRANSFER increases the faults hit by 8/11/12 on Top-1/3/5 metrics (92/159/183 in total). And on APR tasks, the number of successfully repaired bugs of MTL-TRANSFER under the perfect localization setting reaches 75, which is 8 more than our previous APR method TRANSFER-PR. Furthermore, another experiment to simulate the actual repair scenarios shows that MTL-TRANSFER can successfully repair 15 and 9 more bugs (56 in total) compared with TBar and TRANSFER, which demonstrates the effectiveness of the combination of our optimized FL and APR components.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393233520",
    "type": "article"
  },
  {
    "title": "Help Them Understand: Testing and Improving Voice User Interfaces",
    "doi": "https://doi.org/10.1145/3654438",
    "publication_date": "2024-04-05",
    "publication_year": 2024,
    "authors": "Emanuela Guglielmi; Giovanni Rosa; Simone Scalabrino; Gabriele Bavota; Rocco Oliveto",
    "corresponding_authors": "",
    "abstract": "Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this article, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393992813",
    "type": "article"
  },
  {
    "title": "On the Model Update Strategies for Supervised Learning in AIOps Solutions",
    "doi": "https://doi.org/10.1145/3664599",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Yingzhe Lyu; Heng Li; Zhen Ming Jiang; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "AIOps (Artificial Intelligence for IT Operations) solutions leverage the massive data produced during the operation of large-scale systems and machine learning models to assist software engineers in their system operations. As operation data produced in the field are constantly evolving due to factors such as the changing operational environment and user base, the models in AIOps solutions need to be constantly maintained after deployment. While prior works focus on innovative modeling techniques to improve the performance of AIOps models before releasing them into the field, when and how to update AIOps models remain an under-investigated topic. In this work, we performed a case study on three large-scale public operation data: two trace datasets from the cloud computing platforms of Google and Alibaba and one disk stats dataset from the BackBlaze cloud storage data center. We empirically assessed five different types of model update strategies for supervised learning regarding their performance, updating cost, and stability. We observed that active model update strategies (e.g., periodical retraining, concept drift guided retraining, time-based model ensembles, and online learning) achieve better and more stable performance than a stationary model. Particularly, applying sophisticated model update strategies (e.g., concept drift detection, time-based ensembles, and online learning) could provide better performance, efficiency, and stability than simply retraining AIOps models periodically. In addition, we observed that, although some update strategies (e.g., time-based ensemble and online learning) can save model training time, they significantly sacrifice model testing time, which could hinder their applications in AIOps solutions where the operation data arrive at high pace and volume and where immediate inferences are required. Our findings highlight that practitioners should consider the evolution of operation data and actively maintain AIOps models over time. Our observations can also guide researchers and practitioners in investigating more efficient and effective model update strategies that fit in the context of AIOps.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396871622",
    "type": "article"
  },
  {
    "title": "Testing Updated Apps by Adapting Learned Models",
    "doi": "https://doi.org/10.1145/3664601",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Chanh Duc Ngo; Fabrizio Pastore; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Although App updates are frequent and software engineers would like to verify updated features only, automated testing techniques verify entire Apps and are thus wasting resources. We present Continuous Adaptation of Learned Models (CALM) , an automated App testing approach that efficiently test App updates by adapting App models learned when automatically testing previous App versions. CALM focuses on functional testing. Since functional correctness can be mainly verified through the visual inspection of App screens, CALM minimizes the number of App screens to be visualized by software testers while maximizing the percentage of updated methods and instructions exercised. Our empirical evaluation shows that CALM exercises a significantly higher proportion of updated methods and instructions than six state-of-the-art approaches, for the same maximum number of App screens to be visually inspected. Further, in common update scenarios, where only a small fraction of methods are updated, CALM is even quicker to outperform all competing approaches in a more significant way.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396871671",
    "type": "article"
  },
  {
    "title": "Mobile Application Online Cross-Project Just-in-Time Software Defect Prediction Framework",
    "doi": "https://doi.org/10.1145/3664607",
    "publication_date": "2024-05-14",
    "publication_year": 2024,
    "authors": "Siyu Jiang; Zhenhang He; Yuwen Chen; Zhang Ming-rong; Le Ma",
    "corresponding_authors": "",
    "abstract": "As mobile applications evolve rapidly, their fast iterative update nature leads to an increase in software defects. Just-In-Time Software Defect Prediction (JIT-SDP) offers immediate feedback on code changes. For new applications without historical data, researchers have proposed Cross-Project JIT-SDP (CP JIT-SDP). Existing CP JIT-SDP approaches are designed for offline scenarios where target data is available in advance. However, target data in real-world applications usually arrives online in a streaming manner, making online CP JIT-SDP face cross-project distribution differences and target project data concept drift challenges in online scenarios. These challenges often co-exist during application development, and their interactions cause model performance to degrade. To address these issues, we propose an online CP JIT-SDP framework called COTL. Specifically, COTL consists of two stages: offline and online. In the offline stage, the cross-domain structure preserving projection algorithm is used to reduce the cross-project distribution differences. In the online stage, target data arrives sequentially over time. By reducing the differences in marginal and conditional distributions between offline and online data for target project, concept drift is mitigated and classifier weights are updated online. Experimental results on 15 mobile application benchmark datasets show that COTL outperforms 13 benchmark methods on four performance metrics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396904199",
    "type": "article"
  },
  {
    "title": "Can Coverage Criteria Guide Failure Discovery for Image Classifiers? An Empirical Study",
    "doi": "https://doi.org/10.1145/3672446",
    "publication_date": "2024-06-13",
    "publication_year": 2024,
    "authors": "Zhiyu Wang; Sihan Xu; Lingling Fan; Xiangrui Cai; Linyu Li; Zheli Liu",
    "corresponding_authors": "",
    "abstract": "Quality assurance of deep neural networks (DNNs) is crucial for the deployment of DNN-based software, especially in mission- and safety-critical tasks. Inspired by structural white-box testing in traditional software, many test criteria have been proposed to test DNNs, i.e., to exhibit erroneous behaviors by activating new test units that have not been covered, such as new neurons, values, and decision paths. Many studies have been done to evaluate the effectiveness of DNN test coverage criteria. However, existing empirical studies mainly focused on measuring the effectiveness of DNN test criteria for improving the adversarial robustness of DNNs, while ignoring the correctness property when testing DNNs. To fill in this gap, we conduct a comprehensive study on 11 structural coverage criteria, 6 widely-used image datasets, and 9 popular DNNs. We investigate the effectiveness of DNN coverage criteria over natural inputs from 4 aspects: (1) the correlation between test coverage and test diversity; (2) the effects of criteria parameters and target DNNs; (3) the effectiveness to prioritize in-distribution natural inputs that lead to erroneous behaviors; (4) the capability to detect out-of-distribution natural samples. Our findings include: (1) For measuring the diversity, coverage criteria considering the relationship between different neurons are more effective than coverage criteria that treat each neuron independently. For instance, the neuron-path criteria (i.e., SNPC and ANPC) show high correlation with test diversity, which is promising to measure test diversity for DNNs. (2) The hyper-parameters have a big influence on the effectiveness of criteria, especially those relevant to the granularity of test criteria. Meanwhile, the computational complexity is one of the important issues to be considered when designing deep learning test coverage criteria, especially for large-scale models. (3) Test criteria related to data distribution (i.e., LSA and DSA, SNAC, and NBC) can be used to prioritize both in-distribution natural faults and out-of-distribution inputs. Furthermore, for OOD detection, the boundary metrics (i.e., SNAC and NBC) are also effective indicators with lower computational costs and higher detection efficiency compared with LSA and DSA. These findings motivate follow-up research on scalable test coverage criteria that improve the correctness of DNNs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399601788",
    "type": "article"
  },
  {
    "title": "<i>GIST</i> : Generated Inputs Sets Transferability in Deep Learning",
    "doi": "https://doi.org/10.1145/3672457",
    "publication_date": "2024-06-13",
    "publication_year": 2024,
    "authors": "Florian Tambon; Foutse Khomh; Giuliano Antoniol",
    "corresponding_authors": "",
    "abstract": "To foster the verifiability and testability of Deep Neural Networks (DNN), an increasing number of methods for test case generation techniques are being developed. When confronted with testing DNN models, the user can apply any existing test generation technique. However, it needs to do so for each technique and each DNN model under test, which can be expensive. Therefore, a paradigm shift could benefit this testing process: rather than regenerating the test set independently for each DNN model under test, we could transfer from existing DNN models. This paper introduces GIST (Generated Inputs Sets Transferability), a novel approach for the efficient transfer of test sets. Given a property selected by a user (e.g., neurons covered, faults), GIST enables the selection of good test sets from the point of view of this property among available test sets. This allows the user to recover similar properties on the transferred test sets as he would have obtained by generating the test set from scratch with a test cases generation technique. Experimental results show that GIST can select effective test sets for the given property to transfer. Moreover, GIST scales better than reapplying test case generation techniques from scratch on DNN models under test.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399601886",
    "type": "article"
  },
  {
    "title": "Keeper: Automated Testing and Fixing of Machine Learning Software",
    "doi": "https://doi.org/10.1145/3672451",
    "publication_date": "2024-06-13",
    "publication_year": 2024,
    "authors": "Chengcheng Wan; Shicheng Liu; Sophie Xie; Yuhan Liu; Henry Hoffmann; Michael Maire; Shan Lu",
    "corresponding_authors": "",
    "abstract": "The increasing number of software applications incorporating machine learning (ML) solutions has led to the need for testing techniques. However, testing ML software requires tremendous human effort to design realistic and relevant test inputs, and to judge software output correctness according to human common sense. Even when misbehavior is exposed, it is often unclear whether the defect is inside ML API or the surrounding code, and how to fix the implementation. This article tackles these challenges by proposing Keeper, an automated testing and fixing tool for ML software. The core idea of Keeper is designing pseudo-inverse functions that semantically reverse the corresponding ML task in an empirical way and proxy common human judgment of real-world data. It incorporates these functions into a symbolic execution engine to generate tests. Keeper also detects code smells that degrade software performance. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used to alleviate the misbehavior. Our evaluation on a variety of applications shows that Keeper greatly improves branch coverage, while identifying 74 previously unknown failures and 19 code smells from 56 out of 104 applications. Our user studies show that 78% of end-users and 95% of developers agree with Keeper's detection and fixing results.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399601922",
    "type": "article"
  },
  {
    "title": "Paving a Path for a Combined Family of Feature Toggle and Configuration Option Research",
    "doi": "https://doi.org/10.1145/3672555",
    "publication_date": "2024-06-14",
    "publication_year": 2024,
    "authors": "Rezvan Mahdavi-Hezaveh; Sameeha Fatima; Laurie Williams",
    "corresponding_authors": "",
    "abstract": "Feature toggles and configuration options are techniques to include or exclude functionality in software. The research contributions to these two techniques have most often been focused on either one of them. However, focusing on the similarities of these two techniques and the use of a common terminology may enable a combined family of research on software configuration (a term we use to encompass both techniques) and prevent duplication of effort. The goal of this study is to aid researchers in conducting a family of research on software configuration by extending an existing model of software configuration that provides a common terminology for feature toggles and configuration options in research studies. We started with Siegmund et al.’s Model of Software Configuration (MSC), which was developed based on configuration option-related resources. We extend the MSC by qualitative analysis of feature toggle-related resources. From our analysis, we proposed MSCv2 and evaluated it through its application on publications and an industrial system. Our results indicate researchers studying the same system may provide different definitions of software configuration in publications, similar research questions may be answered repeatedly because of a lack of a clear definition of software configuration, and having a model of software configuration may enable generalized research on this family of research. 1",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399669087",
    "type": "article"
  },
  {
    "title": "IDE <sub>4</sub> ICDS: A human-centric and model-driven proposal to improve the digitization of clinical practice guideline",
    "doi": "https://doi.org/10.1145/3674732",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Carlos Luís Parra-Calderón; J. A. García-García; Juan Manuel Ramos-Cueli; Celia Álvarez-Romero; Esther Román-Villarán; Alicia Martínez-García; María José Escalona",
    "corresponding_authors": "",
    "abstract": "Clinical practice guidelines (CPGs) are a formalization of specific clinical knowledge that states the best evidence-based clinical practices for treating pathologies. However, CPGs are limited because they are usually expressed as text. This gives rise to a certain level of ambiguity, subjective interpretation of the actions to be performed, and variability in clinical practice by different health professionals facing the same circumstances. The inherent complexity of CPGs is also a challenge for software engineers designing, developing, and maintaining software systems and clinical decision support system to manage and digitize them. This challenge stems from the need to evolve CPGs and design software systems capable of allowing their evolution. This paper proposes a model-driven, human-centric and tool-supported framework (called IDE 4 ICDS) for improving digitisation of CPG in practical environments. This framework is designed from a human-centric perspective to be used by mixed teams of clinicians and software engineers. It was also validated with the type 2 diabetes mellitus CPG in the Andalusian Public Health System (Spain) involving 89 patients and obtaining a kappa-based analysis. The recommendations were acceptable (0.61–0.80) with a total kappa index of 0.701, leading to the conclusion that the proposal provided appropriate recommendations for each patient.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400117333",
    "type": "article"
  },
  {
    "title": "Word Closure-Based Metamorphic Testing for Machine Translation",
    "doi": "https://doi.org/10.1145/3675396",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Xiaoyuan Xie; Shuo Jin; Songqiang Chen; Shing-Chi Cheung",
    "corresponding_authors": "",
    "abstract": "With the wide application of machine translation, the testing of Machine Translation Systems (MTSs) has attracted much attention. Recent works apply Metamorphic Testing (MT) to address the oracle problem in MTS testing. Existing MT methods for MTS generally follow the workflow of input transformation and output relation comparison, which generates a follow-up input sentence by mutating the source input and compares the source and follow-up output translations to detect translation errors, respectively. These methods use various input transformations to generate the test case pairs and have successfully triggered numerous translation errors. However, they have limitations in performing fine-grained and rigorous output relation comparison and thus may report many false alarms and miss many true errors. In this article, we propose a word closure-based output comparison method to address the limitations of the existing MTS MT methods. We first propose word closure as a new comparison unit, where each closure includes a group of correlated input and output words in the test case pair. Word closures suggest the linkages between the appropriate fragment in the source output translation and its counterpart in the follow-up output for comparison. Next, we compare the semantics on the level of word closure to identify the translation errors. In this way, we perform a fine-grained and rigorous semantic comparison for the outputs and thus realize more effective violation identification. We evaluate our method with the test cases generated by five existing input transformations and the translation outputs from three popular MTSs. Results show that our method significantly outperforms the existing works in violation identification by improving the precision and recall and achieving an average increase of 29.9% in F1 score. It also helps to increase the F1 score of translation error localization by 35.9%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400190921",
    "type": "article"
  },
  {
    "title": "Neural Solving Uninterpreted Predicates with Abstract Gradient Descent",
    "doi": "https://doi.org/10.1145/3675394",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Shiwen Yu; Zengyu Liu; Ting Wang; Ji Wang",
    "corresponding_authors": "",
    "abstract": "Uninterpreted predicate solving is a fundamental problem in formal verification, including loop invariant and Constrained Horn Clauses predicate solving. Existing approaches have been mostly in symbolic ways. While achieving sustainable progress, they still suffer from inefficiency and seem unable to leverage the ever-increasing computility such as GPU. Recently, Neural Relaxation has been proposed to tackle this problem. They treat the uninterpreted predicate-solving task as an optimization problem by relaxing the discrete search process into a learning process of neural networks. However, two bottlenecks keep them from being valid. First, relaxed neural networks cannot match the original semantics rigorously; second, the neural networks are difficult to train to reach global optimization. Therefore, this paper presents a novel discrete neural architecture with the Abstract Gradient Decent (AGD) algorithm to directly solve uninterpreted predicates in the discrete hypothesis space. The abstract gradient is for discrete neurons whose calculation rules are designed in an abstract domain. Our approach conforms to the original semantics, and the proposed AGD algorithm can achieve global optimization satisfactorily. We implement Dasp in the Boxes Abstract Domain to solve uninterpreted predicates in the QF-NIA SMT theory. In the experiments, Dasp has outperformed 7 state-of-the-art tools across three predicate synthesis tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400223001",
    "type": "article"
  },
  {
    "title": "Evolution-aware Constraint Derivation Approach for Software Remodularization",
    "doi": "https://doi.org/10.1145/3676960",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Fanyi Meng; Ying Wang; Chun Yong Chong; Hai Yu; Zhiliang Zhu",
    "corresponding_authors": "",
    "abstract": "Existing software clustering techniques tend to ignore prior knowledge from domain experts, leading to results (suggested big-bang remodularization actions) that cannot be acceptable to developers. Incorporating domain experts knowledge or constraints during clustering ensures the obtained modularization aligns with developers’ perspectives, enhancing software quality. However, manual review by knowledgeable domain experts for constraint generation is time-consuming and labor-intensive. In this article, we propose an evolution-aware constraint derivation approach, Escort , which automatically derives clustering constraints based on the evolutionary history from the analyzed software. Specifically, Escort can serve as an alternative approach to derive implicit and explicit constraints in situations where domain experts are absent. In the subsequent constrained clustering process, Escort can be considered as a framework to help supplement and enhance various unconstrained clustering techniques to improve their accuracy and reliability. We evaluate Escort based on both quantitative and qualitative analysis. In quantitative validation, Escort , using generated clustering constraints, outperforms seven classic unconstrained clustering techniques. Qualitatively, a survey with developers from five IT companies indicates that 89% agree with Escort ’s clustering constraints. We also evaluate the utility of refactoring suggestions from our constrained clustering approach, with 54% acknowledged by project developers, either implemented or planned for future releases.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400416314",
    "type": "article"
  },
  {
    "title": "SimClone: Detecting Tabular Data Clones using Value Similarity",
    "doi": "https://doi.org/10.1145/3676961",
    "publication_date": "2024-07-16",
    "publication_year": 2024,
    "authors": "Xu Yang; Gopi Krishnan Rajbahadur; Dayi Lin; Shaowei Wang; Zhen Ming Jiang",
    "corresponding_authors": "",
    "abstract": "Data clones are defined as multiple copies of the same data among datasets. The presence of data clones between datasets can cause issues such as difficulties in managing data assets and data license violations when using datasets with clones to build AI software. However, detecting data clones is not trivial. The majority of the prior studies in this area rely on structural information to detect data clones (e.g., font size, column header). However, tabular datasets used to build AI software are typically stored without any structural information. In this article, we propose a novel method called SimClone for data clone detection in tabular datasets without relying on structural information. SimClone method utilizes value similarities for data clone detection. We also propose a visualization approach as a part of our SimClone method to help locate the exact position of the cloned data between a dataset pair. Our results show that our SimClone outperforms the current state-of-the-art method by at least 20% in terms of both F1-score and AUC. In addition, SimClone’s visualization component helps identify the exact location of the data clone in a dataset with a Precision@10 value of 0.80 in the top 20 true positive predictions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400685466",
    "type": "article"
  },
  {
    "title": "Can GitHub Issues Help in App Review Classifications?",
    "doi": "https://doi.org/10.1145/3678170",
    "publication_date": "2024-07-18",
    "publication_year": 2024,
    "authors": "Yasaman Abedini; Abbas Heydarnoori",
    "corresponding_authors": "",
    "abstract": "App reviews reflect various user requirements that can aid in planning maintenance tasks. Recently, proposed approaches for automatically classifying user reviews rely on machine learning algorithms. A previous study demonstrated that models trained on existing labeled datasets exhibit poor performance when predicting new ones. Therefore, a comprehensive labeled dataset is essential to train a more precise model. In this paper, we propose a novel approach that assists in augmenting labeled datasets by utilizing information extracted from an additional source, GitHub issues, that contains valuable information about user requirements. First, we identify issues concerning review intentions (bug reports, feature requests, and others) by examining the issue labels. Then, we analyze issue bodies and define 19 language patterns for extracting targeted information. Finally, we augment the manually labeled review dataset with a subset of processed issues through the Within-App , Within-Context , and Between-App Analysis methods. We conducted several experiments to evaluate the proposed approach. Our results demonstrate that using labeled issues for data augmentation can improve the F1-score to 6.3 in bug reports and 7.2 in feature requests. Furthermore, we identify an effective range of 0.3 to 0.7 for the auxiliary volume, which provides better performance improvements.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400777898",
    "type": "article"
  },
  {
    "title": "Speeding up Genetic Improvement via Regression Test Selection",
    "doi": "https://doi.org/10.1145/3680466",
    "publication_date": "2024-07-23",
    "publication_year": 2024,
    "authors": "Giovani Guizzo; D. A. Williams; Mark Harman; Justyna Petke; Federica Sarro",
    "corresponding_authors": "",
    "abstract": "Genetic Improvement (GI) uses search-based optimisation algorithms to automatically improve software with respect to both functional and non-functional properties. Our previous work showed that Regression Test Selection (RTS) can help speed up the use of GI and enhance the overall results while not affecting the software system’s validity. This article expands upon our investigation by answering further questions about safety and applying a GI algorithm based on Local Search (LS) in addition to the previously explored Genetic Programming (GP) approach. Further, we extend the number of subjects to 12 by analysing five larger real-world open-source programs. We empirically compare two state-of-the-art RTS techniques combined with GP and LS for these 12 programs. The results show that both RTS techniques are safe to use and can reduce the cost of GI by up to 80% and by 31% on average across programs. We also observe that both search-based algorithms impact the effectiveness gains of GI differently, and that various RTS strategies achieve differing gains in terms of efficiency. These results serve as further evidence that RTS must be used as a core component of the GI search process to maximise its effectiveness and efficiency.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400914506",
    "type": "article"
  },
  {
    "title": "An empirical study of testing machine learning in the wild",
    "doi": "https://doi.org/10.1145/3680463",
    "publication_date": "2024-07-24",
    "publication_year": 2024,
    "authors": "Moses Openja; Foutse Khomh; Armstrong Foundjem; Zhen Ming Jiang; Mouna Abidi; Ahmed E. Hassan",
    "corresponding_authors": "",
    "abstract": "Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively). Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies. Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow. Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems. Results: Our findings reveal several key insights: 1.) The most common testing strategies, accounting for less than 40%, are Grey-box and White-box methods, such as Negative Testing , Oracle Approximation , and Statistical Testing . 2.) A wide range of \\(17\\) ML properties are tested, out of which only 20% to 30% are frequently tested, including Consistency , Correctness , and Efficiency . 3.) Bias and Fairness is more tested in Recommendation (6%) and CV (3.9%) systems, while Security &amp; Privacy is tested in CV (2%), Application Platforms (0.9%), and NLP (0.5%). 4.) We identified 13 types of testing methods, such as Unit Testing , Input Testing , and Model Testing . Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400934506",
    "type": "article"
  },
  {
    "title": "Reinforcement Learning Informed Evolutionary Search for Autonomous Systems Testing",
    "doi": "https://doi.org/10.1145/3680468",
    "publication_date": "2024-07-27",
    "publication_year": 2024,
    "authors": "Dmytro Humeniuk; Foutse Khomh; Giuliano Antoniol",
    "corresponding_authors": "",
    "abstract": "Evolutionary search-based techniques are commonly used for testing autonomous robotic systems. However, these approaches often rely on computationally expensive simulator-based models for test scenario evaluation. To improve the computational efficiency of the search-based testing, we propose augmenting the evolutionary search (ES) with a reinforcement learning (RL) agent trained using surrogate rewards derived from domain knowledge. In our approach, known as RIGAA (Reinforcement learning Informed Genetic Algorithm for Autonomous systems testing), we first train an RL agent to learn useful constraints of the problem and then use it to produce a certain part of the initial population of the search algorithm. By incorporating an RL agent into the search process, we aim to guide the algorithm towards promising regions of the search space from the start, enabling more efficient exploration of the solution space. We evaluate RIGAA on two case studies: maze generation for an autonomous ‘Ant’ robot and road topology generation for an autonomous vehicle lane keeping assist system. In both case studies, RIGAA reveals more failures of a high level of diversity, than the compared baselines. RIGAA also outperforms the state-of-the-art tools for vehicle lane keeping assist system testing, such as AmbieGen, CRAG, WOGAN and Frenetic in terms of number of revealed failures in a two-hour budget.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401047823",
    "type": "article"
  },
  {
    "title": "Anatomizing Deep Learning Inference in Web Browsers",
    "doi": "https://doi.org/10.1145/3688843",
    "publication_date": "2024-08-14",
    "publication_year": 2024,
    "authors": "Qipeng Wang; Shiqi Jiang; Zhenpeng Chen; Xu Cao; Yuanchun Li; Aoyu Li; Yun Ma; Ting Cao; Xuanzhe Liu",
    "corresponding_authors": "",
    "abstract": "Web applications have increasingly adopted Deep Learning (DL) through in-browser inference , wherein DL inference performs directly within Web browsers. The actual performance of in-browser inference and its impacts on the quality of experience ( QoE ) remain unexplored, and urgently require new QoE measurements beyond traditional ones, e.g., mainly focusing on page load time. To bridge this gap, we make the first comprehensive performance measurement of in-browser inference to date. Our approach proposes new metrics to measure in-browser inference: responsiveness, smoothness, and inference accuracy. Our extensive analysis involves 9 representative DL models across Web browsers of 50 popular PC devices and 20 mobile devices. The results reveal that in-browser inference exhibits a substantial latency gap, averaging 16.9 times slower on CPU and 4.9 times slower on GPU compared to native inference on PC devices. The gap on mobile CPU and mobile GPU is 15.8 times and 7.8 times, respectively. Furthermore, we identify contributing factors to such latency gap, including underutilized hardware instruction sets, inherent overhead in the runtime environment, resource contention within the browser, and inefficiencies in software libraries and GPU abstractions. Additionally, in-browser inference imposes significant memory demands, at times exceeding 334.6 times the size of the DL models themselves, partly attributable to suboptimal memory management. We also observe that in-browser inference leads to a significant 67.2% increase in the time it takes for GUI components to render within Web browsers, significantly affecting the overall user QoE of Web applications reliant on this technology.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401576757",
    "type": "article"
  },
  {
    "title": "Neuron Semantic-Guided Test Generation for Deep Neural Networks Fuzzing",
    "doi": "https://doi.org/10.1145/3688835",
    "publication_date": "2024-08-14",
    "publication_year": 2024,
    "authors": "Li Huang; Weifeng Sun; Meng Yan; Zhongxin Liu; Yan Lei; David Lo",
    "corresponding_authors": "",
    "abstract": "In recent years, significant progress has been made in testing methods for deep neural networks (DNNs) to ensure their correctness and robustness. Coverage-guided criteria, such as neuron-wise, layer-wise, and path-/trace-wise, have been proposed for DNN fuzzing. However, existing coverage-based criteria encounter performance bottlenecks for several reasons: ❶ Testing Adequacy : Partial neural coverage criteria have been observed to achieve full coverage using only a small number of test inputs. In this case, increasing the number of test inputs does not consistently improve the quality of models. ❷ Interpretability : The current coverage criteria lack interpretability. Consequently, testers are unable to identify and understand which incorrect attributes or patterns of the model are triggered by the test inputs. This lack of interpretability hampers the subsequent debugging and fixing process. Therefore, there is an urgent need for a novel fuzzing criterion that offers improved testing adequacy, better interpretability, and more effective failure detection capabilities for DNNs. To alleviate these limitations, we propose NSGen, an approach for DNN fuzzing that utilizes neuron semantics as guidance during test generation. NSGen identifies critical neurons, translates their high-level semantic features into natural language descriptions, and then assembles them into human-readable DNN decision paths (representing the internal decision of the DNN). With these decision paths, we can generate more fault-revealing test inputs by quantifying the similarity between original test inputs and mutated test inputs for fuzzing. We evaluate NSGen on popular DNN models (VGG16_BN, ResNet50, and MobileNet_v2) using CIFAR10, CIFAR100, Oxford 102 Flower, and ImageNet datasets. Compared to 12 existing coverage-guided fuzzing criteria, NSGen outperforms all baselines, increasing the number of triggered faults by 21.4% to 61.2% compared to the state-of-the-art coverage-guided fuzzing criterion. This demonstrates NSGen’s effectiveness in generating fault-revealing test inputs through guided input mutation, highlighting its potential to enhance DNN testing and interpretability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401576964",
    "type": "article"
  },
  {
    "title": "Don’t Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems",
    "doi": "https://doi.org/10.1145/3688831",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Zhensu Sun; Xiaoning Du; Fu Song; Shangwen Wang; Mingze Ni; Li Li; David Lo",
    "corresponding_authors": "",
    "abstract": "Currently, large pre-trained language models are widely applied in neural code completion systems. Though large code models significantly outperform their smaller counterparts, around 70% of displayed code completions from Github Copilot are not accepted by developers. Being reviewed but not accepted, their help to developer productivity is considerably limited and may conversely aggravate the workload of developers, as the code completions are automatically and actively generated in state-of-the-art code completion systems as developers type out once the service is enabled. Even worse, considering the high cost of the large code models, it is a huge waste of computing resources and energy, which severely goes against the sustainable development principle of AI technologies. However, such waste has never been realized, not to mention effectively addressed, in the research community for neural code completion. Hence, preventing such unhelpful code completions from happening in a cost-friendly way is of urgent need. To fill this significant gap, we first investigate the prompts of unhelpful code completions, called “low-return prompts”. We empirically identify four observable patterns in low-return prompts, each lacking necessary information, making it difficult to address through enhancements to the model’s accuracy alone. This demonstrates the feasibility of identifying such low-return prompts based on the prompts themselves. Motivated by this finding, we propose an early-rejection mechanism to turn down low-return prompts by foretelling the code completion qualities. The prompts that are estimated to receive unhelpful code completions will not be sent to the model. Furthermore, we investigated five types of estimators to demonstrate the feasibility of the mechanism. The experimental results show that the estimator can reject 20% of code completion requests with a 97.4% Precision. To the best of our knowledge, it is the first systemic approach to address the problem of unhelpful code completions and this work also sheds light on an important research direction of large code models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401635661",
    "type": "article"
  },
  {
    "title": "Solving the <i>t</i> -wise Coverage Maximum Problem via Effective and Efficient Local Search-based Sampling",
    "doi": "https://doi.org/10.1145/3688836",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Chuan Luo; Jianping Song; Qiyuan Zhao; Binqi Sun; Junjie Chen; Hongyu Zhang; Jinkun Lin; Chunming Hu",
    "corresponding_authors": "",
    "abstract": "To meet the increasing demand for customized software, highly configurable systems become essential in practice. Such systems offer many options to configure, and ensuring the reliability of these systems is critical. A widely-used evaluation metric for testing these systems is \\(t\\) -wise coverage, where \\(t\\) represents testing strength, and its value typically ranges from 2 to 6. It is crucial to design effective and efficient methods for generating test suites that achieve high \\(t\\) -wise coverage. However, current state-of-the-art methods need to generate large test suites for achieving high \\(t\\) -wise coverage. In this work, we propose a novel method called LS-Sampling-Plus that can efficiently generate test suites with high \\(t\\) -wise coverage for \\(2\\leq t\\leq 6\\) while being smaller in size compared to existing state-of-the-art methods. LS-Sampling-Plus incorporates many core algorithmic techniques, including two novel scoring functions, a dynamic mechanism for updating sampling probabilities, and a validity-guaranteed systematic search method. Our experiments on various practical benchmarks show that LS-Sampling-Plus can achieve higher \\(t\\) -wise coverage than current state-of-the-art methods, through building a test suite of the same size. Moreover, our evaluations indicate the effectiveness of all core algorithmic techniques of LS-Sampling-Plus . Further, LS-Sampling-Plus exhibits better scalability and fault detection capability than existing state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401635748",
    "type": "article"
  },
  {
    "title": "Towards the Fractal Dimension of Classes",
    "doi": "https://doi.org/10.1145/3688844",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Weifeng Pan; Wei Wu; Hua Ming; Dae‐Kyoo Kim; Zijiang Yang; Yutao Ma",
    "corresponding_authors": "",
    "abstract": "The fractal property has been regarded as a fundamental property of complex networks, characterizing the self-similarity of a network. Such a property is usually numerically characterized by the fractal dimension metric, and it not only helps the understanding of the relationship between the structure and function of complex networks, but also finds a wide range of applications in complex systems. The existing literature shows that class-level software networks (i.e., class dependency networks) are complex networks with the fractal property. However, the fractal property at the feature (i.e., methods and fields) level has never been investigated, although it is useful for measuring class complexity and predicting bugs in classes. Furthermore, existing studies on the fractal property of software systems were all performed on un-weighted software networks and have not been used in any practical quality assurance tasks such as bug prediction. Generally, considering the weights on edges can give us more accurate representations of the software structure and thus help us obtain more accurate results. The illustration of an approach’s practical use can promote its adoption in practice. In this paper, we examine the fractal property of classes by proposing a new metric. Specifically, we build an FLSN ( F eature L evel S oftware N etwork) for each class to represent the methods/fields and their couplings (including coupling frequencies) within the class, and propose a new metric, ( F ractal D imension for C lasses), to numerically describe the fractal property of classes using FLSNs, which captures class complexity. We evaluate theoretically against Weyuker’s nine properties, and the results show that adheres to eight of the nine properties. Empirical experiments performed on a set of twelve large open-source Java systems show that i) for most classes (larger than \\(96\\%\\) ), there exists the fractal property in their FLSNs, ii) is capable of capturing additional aspects of class complexity that have not been addressed by existing complexity metrics, iii) significantly correlates with both the existing class-level complexity metrics and the number of bugs in classes, and iv), when used together with existing class-level complexity metrics, can significantly improve bug prediction in classes in three scenarios (i.e., bug-count , bug-classification , and effort-aware ) of the cross-project context, but in the within-project context, it cannot.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401635882",
    "type": "article"
  },
  {
    "title": "QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems",
    "doi": "https://doi.org/10.1145/3688840",
    "publication_date": "2024-08-19",
    "publication_year": 2024,
    "authors": "Jinjing Shi; Zimeng Xiao; Heyuan Shi; Yu Jiang; Xuelong Li",
    "corresponding_authors": "",
    "abstract": "Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods to QNN systems due to differences in programming paradigms and decision logic representations, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs. Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement adequacy and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples. Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48%-96.05% more high-quality test samples than the random noise under the same perturbation size constraints). The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401694091",
    "type": "article"
  },
  {
    "title": "Benchmarking and Categorizing the Performance of Neural Program Repair Systems for Java",
    "doi": "https://doi.org/10.1145/3688834",
    "publication_date": "2024-08-19",
    "publication_year": 2024,
    "authors": "Wenkang Zhong; Chuanyi Li; Kui Liu; Jidong Ge; Bin Luo; Tegawendé F. Bissyandé; Vincent Ng",
    "corresponding_authors": "",
    "abstract": "Recent years have seen a rise in neural program repair systems in the software engineering community, which adopt advanced deep learning techniques to automatically fix bugs. Having a comprehensive understanding of existing systems can facilitate new improvements in this area and provide practical instructions for users. However, we observe two potential weaknesses in the current evaluation of NPR systems: ① published systems are trained with varying data, and ② NPR systems are roughly evaluated through the number of totally fixed bugs. Questions such as “what types of bugs are repairable for current systems” cannot be answered yet. Consequently, researchers can not make target improvements in this area and users have no idea of the real affair of existing systems. In this paper, we perform a systematic evaluation of the existing nine state-of-the-art NPR systems. To perform a fair and detailed comparison, we (1) build a new benchmark and framework that supports training and validating the nine systems with unified data, and (2) evaluate retrained systems with detailed performance analysis, especially on the effectiveness and the efficiency. We believe our benchmark tool and evaluation results could offer practitioners the real affairs of current NPR systems and the implications of further facilitating the improvements of NPR.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401694099",
    "type": "article"
  },
  {
    "title": "AutoRIC: Automated Neural Network Repairing Based on Constrained Optimization",
    "doi": "https://doi.org/10.1145/3690634",
    "publication_date": "2024-09-04",
    "publication_year": 2024,
    "authors": "Xinyu Sun; Wanwei Liu; Shangwen Wang; Taiping Chen; Ye Tao; Xiaoguang Mao",
    "corresponding_authors": "",
    "abstract": "Neural networks are important computational models used in the domains of artificial intelligence and software engineering. Parameters of a neural network are obtained via training it against a specific dataset with a standard process, which guarantees each sample within that set is mapped to the correct class. In general, for a trained neural network, there is no warranty of high-level properties, such as fairness, robustness, etc. In this case, one need to tune the parameters in an alternative manner, and it is called repairing. In this paper, we present AutoRIC ( Auto mated R epair w I th C onstraints), an analytical-approach-based white-box repairing framework against general properties that could be quantitatively measured. Our approach is mainly based on constrained optimization, namely, we treat the properties of neural network as the optimized objective described by a quadratic formula about the faulty parameters. To ensure the classification accuracy of the repaired neural network, we impose linear inequality constraints to the inputs that obtain incorrect outputs from the neural network. In general, this may generate a huge amount of constraints, resulting in the prohibitively high cost in the problem solving, or even making the problem unable to be solved by the constraint solver. To circumvent this, we present a selection strategy to diminish the restrictions, i.e., we always select the most ‘strict’ ones into the constraint set each time. Experimental results show that repairing with constraints performs efficiently and effectively. AutoRIC tends to achieve a satisfactory repairing result whereas brings in a negligible accuracy drop. AutoRIC enjoys a notable time advantage and this advantage becomes increasingly evident as the network complexity rises. Moreover, experiment results also demonstrate that repairing based on unconstrained optimizations are not stable, which embodies the necessity of constraints.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402215748",
    "type": "article"
  },
  {
    "title": "Demo2Test: Transfer Testing of Agent in Competitive Environment with Failure Demonstrations",
    "doi": "https://doi.org/10.1145/3696001",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Chen Jian-ming; Yawen Wang; Junjie Wang; Xiaofei Xie; Dandan Wang; Qing Wang; Fanjiang Xu",
    "corresponding_authors": "",
    "abstract": "The competitive game between agents exists in many critical applications, such as military unmanned aerial vehicles. It is urgent to test these agents to reduce the significant losses caused by their failures. Existing studies mainly are to construct a testing agent that competes with the target agent to induce its failures. These approaches usually focus on a single task, requiring much more time for multi-task testing. However, if the previously tested tasks (source tasks) and the task to be tested (target task) share similar agents or task objectives, the transferable knowledge in source tasks can potentially increase the effectiveness of testing in the target task. We propose Demo2Test for conducting transfer testing of agents in the competitive environment, i.e., leveraging the demonstrations of failure scenarios from the source task to boost the testing effectiveness in the target task. It trains a testing agent with demonstrations and incorporates the action perturbation at key states to balance the number of revealed failures and their diversity. We conduct experiments in the simulated robotics competitive environments of MuJoCo. The results indicate that Demo2Test outperforms the best-performing baseline with improvements ranging from 22.38% to 87.98%, and 12.69% to 60.98%, in terms of the number and diversity of discovered failure scenarios, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402515992",
    "type": "article"
  },
  {
    "title": "KBX: Verified Model Synchronization via Formal Bidirectional Transformation",
    "doi": "https://doi.org/10.1145/3696000",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Jianhong Zhao; Yongwang Zhao; Peisen Yao; Fanlang Zeng; Bohua Zhan; Kui Ren",
    "corresponding_authors": "",
    "abstract": "Complex safety-critical systems require multiple models for a comprehensive description, resulting in error-prone development and laborious verification. Bidirectional transformation (BX) is an approach to automatically synchronizing these models. However, existing BX frameworks lack formal verification to enforce these models’ consistency rigorously. This paper introduces KBX, a formal bidirectional transformation framework for verified model synchronization. First, we present a matching logic-based BX model, providing a logical foundation for constructing BX definitions within the \\(\\mathbb{K}\\) framework. Second, we propose algorithms to synthesize formal BX definitions from unidirectional ones, which allows developers to focus on crafting the unidirectional definitions while disregarding the reverse direction and missing information recovery for synchronization. Afterward, we harness \\(\\mathbb{K}\\) to generate a formal synchronizer from the synthesized definitions for consistency maintenance and verification. To evaluate the effectiveness of KBX, we conduct a comparative analysis against existing BX frameworks. Furthermore, we demonstrate the application of KBX in constructing a BX between UML and HCSP for real-world scenarios, showcasing an 72% reduction in BX development effort compared to manual specification writing in \\(\\mathbb{K}\\) .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402516022",
    "type": "article"
  },
  {
    "title": "Deep API Sequence Generation via Golden Solution Samples and API Seeds",
    "doi": "https://doi.org/10.1145/3695995",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Yue-Kai Huang; Junjie Wang; Song Wang; Moshi Wei; Lin Shi; Zhe Liu; Qing Wang",
    "corresponding_authors": "",
    "abstract": "Automatic API recommendation can accelerate developers’ programming, and has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information retrieval-based (IR-based) approaches and sequence to sequence (seq2seq) model based approaches. Although these approaches were reported to have remarkable performance, our observation finds two major drawbacks, i.e., IR-based approaches lack the consideration of relations among the recommended APIs, and seq2seq models do not model the API’s semantic meaning. To alleviate the above two problems, we propose APIGens, which is a retrieval-enhanced large language model (LLM) based API recommendation approach to recommend an API sequence for a natural language query. The approach first retrieves similar programming questions in history based on the input natural language query, and then scores the results based on API documents via a scorer model. Finally, these results are used as samples for few-shot learning of LLM. To reduce the risk of encountering local optima, we also extract API seeds from the retrieved results to increase the search scope during the LLM generation process. The results show that our approach can achieve 48.41% ROUGE@10 on API sequence recommendation and the 82.61% MAP on API set recommendation, largely outperforming the state-of-the-art baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402516515",
    "type": "article"
  },
  {
    "title": "Software Product Line Engineering via Software Transplantation",
    "doi": "https://doi.org/10.1145/3695987",
    "publication_date": "2024-09-20",
    "publication_year": 2024,
    "authors": "Leandro Oliveria de Souza; Eduardo Santana de Almeida; Paulo Anselmo da Mota Silveira Neto; Earl T. Barr; Justyna Petke",
    "corresponding_authors": "",
    "abstract": "Software Product Lines (SPLs) improve time-to-market, enhance software quality, and reduce maintenance costs. Current SPL re-engineering practices are largely manual and require domain knowledge. Thus, adopting and, to a lesser extent, maintaining SPLs are expensive tasks, preventing many companies from enjoying their benefits. To address these challenges, we introduce Foundry , an approach utilizing software transplantation to reduce the manual effort of SPL adoption and maintenance. Foundry enables integrating features across different codebases, even codebases that are unaware that they are contributing features to a software product line. Each product produced by Foundry is pure code, without variability annotation, unlike feature flags, which eases variability management and reduces code bloat. We realise Foundry in prodScalpel , a tool that transplants multiple organs ( i.e. , a set of interesting features) from donor systems into an emergent product line for codebases written in C. Given tests and lightweight annotations identifying features and implantation points, prodScalpel automates feature extraction and integration. To evaluate its effectiveness, our evaluation compares feature transplantation using prodScalpel to the current state of practice: on our dataset, prodScalpel’s use speeds up feature migration by an average of 4.8 times when compared to current practice.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402665761",
    "type": "article"
  },
  {
    "title": "Automating Comment Generation for Smart Contract from Bytecode",
    "doi": "https://doi.org/10.1145/3699597",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Jianhang Xiang; Zhipeng Gao; Lingfeng Bao; Xing Hu; Jiayuan Chen; Xin Xia",
    "corresponding_authors": "",
    "abstract": "Recently, smart contracts have played a vital role in automatic financial and business transactions. To help end users without programming background to better understand the logic of smart contracts, previous studies have proposed models for automatically translating smart contract source code into their corresponding code summaries. However, in practice, only 13% of smart contracts deployed on the Ethereum blockchain are associated with source code. The practical usage of these existing tools is significantly restricted. Considering that bytecode is always necessary when deploying smart contracts, in this paper, we first introduce the task of automatically generating smart contract code summaries from bytecode. We propose a novel approach, named S mart BT ( Smart contract B ytecode T ranslator) for automatically translating smart contract bytecode into fine-grained natural language description directly. Two key challenges are posed for this task: structural code logic hidden in bytecode and the huge semantic gap between bytecode and natural language descriptions. To address the first challenge, we transform bytecode into CFG (Control-Flow Graph) to learn code structural and logic details. Regarding the second challenge, we introduce an information retrieval component to fetch similar comments for filling the semantic gap. Then the structural input and semantic input are used to build an attentional sequence-to-sequence neural network model. The copy mechanism is employed to copy rare words directly from similar comments and the coverage mechanism is employed to eliminate repetitive outputs. The automatic evaluation results show that SmartBT outperforms a set of baselines by a large margin, and the human evaluation results show the effectiveness and potential of SmartBT in producing meaningful and accurate comments for smart contract code from bytecode directly.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403185844",
    "type": "article"
  },
  {
    "title": "Instance Space Analysis of Testing of Autonomous Vehicles in Critical Scenarios",
    "doi": "https://doi.org/10.1145/3699596",
    "publication_date": "2024-10-08",
    "publication_year": 2024,
    "authors": "Victor Crespo-Rodriguez; Neelofar Neelofar; Aldeida Aleti; Burak Turhan",
    "corresponding_authors": "",
    "abstract": "Before being deployed on roads, Autonomous Vehicles (AVs) must undergo comprehensive testing. Safety-critical situations, however, are infrequent in usual driving conditions, so simulated scenarios are used to create them. A test scenario comprises static and dynamic features related to the AV and the test environment; the representation of these features is complex and makes testing a heavy process. A test scenario is effective if it identifies incorrect behaviors of the AV. In this article, we present a technique for identifying key features of test scenarios associated with their effectiveness using Instance Space Analysis (ISA). ISA generates a ( \\(2D\\) ) representation of test scenarios and their features. This visualization helps to identify combinations of features that make a test scenario effective. We present a graphical representation of each feature that helps identify how well each testing technique explores the search space. While identifying key features is a primary goal, this study specifically seeks to determine the critical features that differentiate the performance of algorithms. Finally, we present metrics to assess the robustness of testing algorithms and the scenarios generated. Collecting essential features in combination with their values associated with effectiveness can be used for selection and prioritization of effective test cases.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403222332",
    "type": "article"
  },
  {
    "title": "An Empirical Study on Governance in Bitcoin's Consensus Evolution",
    "doi": "https://doi.org/10.1145/3699600",
    "publication_date": "2024-10-10",
    "publication_year": 2024,
    "authors": "Jakob Svennevik Notland; Mariusz Nowostawski; Jingyue Li",
    "corresponding_authors": "",
    "abstract": "Consensus rule changes in public permissionless blockchains are challenging. Changes can be contentious, and getting all participants to agree could be tedious. Notably, Bitcoin has seen centralisation tendencies in mining and development. However, how these tendencies influence governance processes of consensus evolution has received minimal attention. We explore how the evolution of blockchain systems and the governance of consensus intertwine from socio-technical aspects. Our study analyses the governmental structures in blockchain by looking into Bitcoin. We investigate consensus change processes through grounded theory, comprising quantitative and qualitative data from 34 consensus forks in two different blockchains, Bitcoin Core and Bitcoin Cash. We explore how decentralisation and governance unfold in practice. In contrast to existing studies, we revealed that centralisation tendencies among miners and developers have no direct control over consensus rules in a blockchain. Furthermore, centralisation tendencies do not affect decision-making for consensus evolution governance in the same way as they facilitate consensus attacks, such as 51% attacks. We also discovered that consensus governance is constrained by the technicalities of change and deployment techniques. Consequently, even though miners have the authority to make consensus changes and propose new blocks, they are restricted by deployment techniques and dependence on user adoption.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403297672",
    "type": "article"
  },
  {
    "title": "Effective Hard Negative Mining for Contrastive Learning-based Code Search",
    "doi": "https://doi.org/10.1145/3695994",
    "publication_date": "2024-10-11",
    "publication_year": 2024,
    "authors": "Fan Ye; Chuanyi Li; Jidong Ge; LiGuo Huang; Bin Luo",
    "corresponding_authors": "",
    "abstract": "Background. Code search aims to find the most relevant code snippet in a large codebase based on a given natural language query. An accurate code search engine can increase code reuse and improve programming efficiency. The focus of code search is how to represent the semantic similarity of code and query. With the development of code pre-trained models, the pattern of using numeric feature vectors (embeddings) to represent code semantics and using vector distance to represent semantic similarity has replaced traditional string matching methods. The quality of semantic representations is critical to the effectiveness of downstream tasks such as code search. Currently, the state-of-the-art (SOTA) learning method uses the contrastive learning paradigm. The objective of contrastive learning is to maximize the similarity between matching code and query (positive samples) and minimize the similarity between mismatched pairs (negative samples). To increase the reusing of negative samples, prior contrastive learning approaches use a large queue (memory bank) to store embeddings. Problem. However, there is still a lot of room for improvement in using negative examples for code search: ① Due to the random selection of negative samples, semantic representations learned by existing models cannot distinguish similar codes well. ② Since semantic vectors in the memory bank are reused from previous inference results and then directly used for loss function calculation without gradient descent, the model cannot effectively learn the negative sample semantic information. Method. To solve the above problems, we propose a contrastive learning code search model with hard negative mining called CoCoHaNeRe: ❶ To enable the model to distinguish similar codes, we introduce hard negative examples into contrastive training, which are negative examples in the codebase that are most similar to positive examples. As a result, hard negative examples are most likely to make the model make mistakes. ❷ To improve the learning efficiency of negative samples during training, we add all hard negative examples to the model's gradient descent process. Result. To verify the effectiveness of CoCoHaNeRe, we conducted experiments on large code search datasets with six programming languages, as well as similar retrieval tasks code clone detection and code question answering. Experimental results show that our model achieves SOTA performance. In the code search task, the average MRR score of CoCoHaNeRe exceeds CodeBERT, GraphCodeBERT, and UniXcoder by 11.25%, 8.13%, and 7.38%, respectively. It has also made great progress in code clone detection and code question answering. In addition, our method performs well in different programming languages and code pre-training models. Furthermore, qualitative analysis shows that our model effectively distinguishes high-order semantic differences between similar codes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403336452",
    "type": "article"
  },
  {
    "title": "Understanding Test Convention Consistency as a Dimension of Test Quality",
    "doi": "https://doi.org/10.1145/3672448",
    "publication_date": "2024-10-21",
    "publication_year": 2024,
    "authors": "Martin P. Robillard; Mathieu Nassif; Muhammad Sohail",
    "corresponding_authors": "",
    "abstract": "Unit tests must be readable to help developers understand and evolve production code. Most existing test quality metrics assess test code's ability to detect bugs. Few metrics focus on test code's readability. One standard approach to improve readability is the consistent application of conventions. We investigated test convention consistency as a dimension of test quality. We formalized test suite consistency as the extent to which alternatives are used within a code base and introduce two complementary metrics to capture this extent. We elaborated a catalog of over 30 test conventions for the Java language organized in 10 convention classes that group mutual alternatives. We developed tool support to detect occurrences of conventions, compute consistency metrics over a test suite, and view occurrences of conventions in the corresponding code. We applied our tools to study the consistency of the test suites of 20 large open-source Java projects. The study validates the design of the test convention classes, provides descriptive statistics on the range of consistency values for ten different convention classes, and enables us to link observed changes in consistency values to specific events in the change history of our target systems, thus providing evidence of the construct validity of the metrics. We conclude that analyzing test suite consistency via static analysis shows promise as a practical approach to help improve test suite quality.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403604523",
    "type": "article"
  },
  {
    "title": "Adaptive modelling languages: Abstract syntax and model migration",
    "doi": "https://doi.org/10.1145/3702975",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Juan de Lara; Esther Guerra",
    "corresponding_authors": "",
    "abstract": "Modelling languages are heavily used in many disciplines, including software engineering. However, current languages are rigid , since they do not get adapted to fit the users’ expertise, the modelling task, or the usage platform. This may turn some languages unsuitable for a range of users (from unexperienced to experts), goals (from informal discussion to precise specification) and platforms (from desktops to mobile phones). We claim that making languages adaptive to the modelling scenario would alleviate these issues and help simplifying recurring tasks such as language evolution or interoperability between the languages of a family. In this paper, we propose the new notion of adaptive modelling language . This concept combines meta-modelling and product lines to support variants of a given language, and encompasses contextual conditions triggering language reconfigurations, and mechanisms for model migration across the language variants. The paper presents a theory and its realisation atop the Eclipse Modeling Framework. Our tool includes an Eclipse workbench to specify adaptive languages and produce Eclipse modelling editors with adaptation support. We report on an evaluation demonstrating the advantages of using our framework to express migrations across the variants of adaptive languages, which moreover have generally fast execution times.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403997028",
    "type": "article"
  },
  {
    "title": "Real-time Rectifying Flight Control Misconfiguration Using Intelligent Agent",
    "doi": "https://doi.org/10.1145/3702994",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Ruidong Han; Shangzhi Xu; Juanru Li; Elisa Bertino; David Lo; Jianfeng Ma; Siqi Ma",
    "corresponding_authors": "",
    "abstract": "Configurations are supported by most flight control systems, allowing users to control a flying drone adapted to complexities such as environmental changes or mission alterations. Such an advanced functionality also introduces a significant problem - misconfiguration settings. It may cause drone instability, threaten drone safety, and potentially lead to substantial financial loss. However, detecting and rectifying misconfigurations across different flight control systems is challenging because 1) (mis)configuration-related code snippets might be syntactically correct and thus hard to identify through traditional code analysis; 2) the response to each configuration varies under different flying scenarios. In this paper, we propose and implement a novel rectification approach, Nyctea , to detect instability caused by misconfigurations and conduct an on-the-fly rectification. Nyctea first continuously inspects state changes over consecutive time intervals and calculates the overall deviations to determine whether a drone is in a transition of instability to control loss. When a potential instability is reported, Nyctea instantly invokes a pre-trained intelligent agent to automatically generate proper configurations and then re-configure the drone against entering a state of loss of control. This process of reconfiguration is conducted iteratively until the instability is eliminated. We integrated Nyctea with the widely used flight control system, Ardupilot and PX4 . The simulated and practical experiment results showed that Nyctea successfully eliminates instabilities caused by 85% of misconfigurations. For each misconfiguration, Nyctea averagely generated 4 to 5 configurations to achieve a successful rectification.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404002937",
    "type": "article"
  },
  {
    "title": "<scp>FunFuzz</scp> : Greybox Fuzzing with Function Significance",
    "doi": "https://doi.org/10.1145/3702974",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Ruixiang Qian; Quanjun Zhang; Chunrong Fang; Lihua Guo; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Greybox fuzzing is dedicated to revealing software bugs by maximizing code coverage. Concentrating on code coverage, greybox fuzzing effectively exposes bugs in real-world programs by continuously executing the program under test (PUT) with the test inputs generated from initial seeds, making it a popular software testing technique. Although powerful, the effectiveness of greybox fuzzing can be restricted in some cases. Ignoring the significant degrees of executed functions, traditional greybox fuzzing usually fails to identify significant seeds that execute more significant functions, and thus may assign similar energy to significant and trivial seeds when conducting power scheduling. As a result, the effectiveness of greybox fuzzing can be degraded due to wasting too much energy on trivial seeds. In this paper, we introduce function significance (FS) to measure the significant degrees of functions. Our key insight is that the influential functions that connect to many other functions are significant to greybox fuzzing as they provide more probabilities to reach previously unexplored code regions. To quantify FS, we conduct influence analysis upon the call graphs extracted from the PUTs to obtain the centrality values of function nodes. With FS as the significance measurement, we further propose FunFuzz , an FS-aware greybox fuzzing technique, to optimize significant seeds and tackle the aforementioned restriction. To this end, FunFuzz dynamically tracks the functions executed by a seed during fuzzing, and computes the significance score for the seed by accumulating the FS values of the functions executed by it. Based on the computed FS values, FunFuzz then takes an estimation-based power scheduling to assign more (or less) energy to seeds that achieve over-estimated (or under-estimated) significance scores. Specifically, the seed energy is adjusted by multiplying with a scale factor computed regarding the ratio of the actual significance score achieved by executing the seed and the estimated significance score predicted by a linear model constructed on-the-fly. To evaluate FunFuzz , we prototype it on top of AFL++ and conduct experiments with 15 programs, of which 10 are from common real-world projects and five are from Magma, and compare it to seven popular fuzzers. The experimental results obtained through fuzzing exceeding 40,800 CPU hours show that: (1) In terms of covering code, FunFuzz outperforms AFL++ by achieving 0.1% \\(\\sim\\) 18.4% more region coverage on 13 out of 15 targets. (2) In terms of finding bugs, FunFuzz unveils 114 unique crashes and 25 Magma bugs (which are derived from CVEs) in 20 trials of 24-hour fuzzing, which are the most compared to the competitor fuzzers and include 32 crashes and 1 Magma bug that the other fuzzers fail to discover. Besides the experiments focusing on code coverage and bug finding, we evaluate the key components of FunFuzz , namely the FS-centered estimation-based power scheduling and the lazy FS computation mechanism. The extensive evaluation not only suggests FunFuzz ’s superiority in code coverage and bug finding, but also demonstrates the effectiveness of the two components.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404002945",
    "type": "article"
  },
  {
    "title": "llasm: Naming Functions in Binaries by Fusing Encoder-only and Decoder-only LLMs",
    "doi": "https://doi.org/10.1145/3702988",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Zihan Sha; Hao Wang; Zeyu Gao; Hui Shu; Bolun Zhang; Ziqing Wang; Chao Zhang",
    "corresponding_authors": "",
    "abstract": "Predicting function names in stripped binaries, which requires succinctly summarizing semantics of binary code in natural languages, is a crucial but challenging task. Recently, many machine learning based solutions have been proposed. However, they have poor generalizability, i.e., fail to handle unseen binaries. To advance the state of the art, we present llasm ( L arge AS sembly L anguage M odel), a novel framework which fuses encoder-only and decoder-only LLMs for function name prediction. It refines encoder-only models to preserve more binary information and learn better binary representations. Then it adopts a novel architecture to project the encoding to the input space of a decoder-only natural language model, which enables it to have better capability of inferring general knowledge and better generalizability. We have evaluated llasm in the BinaryCorp and Debin datasets. llasm outperforms the state-of-the-art function name prediction tools by up to 19.9%, 40.7%, and 36.5% in precision, recall, and F1 score, with significantly better generalizability in unseen binaries. Our case studies further demonstrate the practical use cases of llasm in analyzing real-world malware, showing the usefulness of function name prediction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404060073",
    "type": "article"
  },
  {
    "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using LLMs",
    "doi": "https://doi.org/10.1145/3702979",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Azmain Kabir; Shaowei Wang; Yuan Tian; Tse-Hsun Chen; Muhammad Asaduzzaman; Wenbin Zhang",
    "corresponding_authors": "",
    "abstract": "Technical Q&amp;A sites are valuable for software developers seeking knowledge, but the code snippets they provide are often uncompilable and incomplete due to unresolved types and missing libraries. This poses a challenge for users who wish to reuse or analyze these snippets. Existing methods either do not focus on creating compilable code or have low success rates. To address this, we propose ZS4C, a lightweight approach for zero-shot synthesis of compilable code from incomplete snippets using Large Language Models (LLMs). ZS4C operates in two stages: first, it uses an LLM, like GPT-3.5, to identify missing import statements in a snippet; second, it collaborates with a validator (e.g., compiler) to fix compilation errors caused by incorrect imports and syntax issues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset, Python-SO, which includes 539 Python snippets from Stack Overflow across the 20 most popular Python libraries. ZS4C significantly outperforms existing methods, improving the compilation rate from 63% to 95.1% compared to the state-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer more accurate import statements (with an F1 score of 0.98) than SnR, with an improvement of 8.5% in the F1.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404060074",
    "type": "article"
  },
  {
    "title": "Non-Linear Software Documentation with Interactive Code Examples",
    "doi": "https://doi.org/10.1145/3702976",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Mathieu Nassif; Martin P. Robillard",
    "corresponding_authors": "",
    "abstract": "Documentation enables sharing knowledge between the developers of a technology and its users. Creating quality documents, however, is challenging: Documents must satisfy the needs of a large audience without being overwhelming for individuals. We address this challenge with a new document format, named Casdoc. Casdoc documents are interactive resources centered around code examples for programmers. Explanations of the code elements are presented as annotations that the readers reveal based on their needs. We evaluated Casdoc in a field study with over 300 participants who used 126 documents as part of a software design course. During the study, the majority of participants adopted Casdoc instead of a baseline format and used interactive annotations to reveal additional information about the code example. Although participants collectively viewed the majority of the documents’ content, they individually revealed a minority of the annotations they saw. We gathered insights into five aspects of Casdoc that can be applied to other formats, and highlighted five lessons learned to improve navigability in online documents.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404060176",
    "type": "article"
  },
  {
    "title": "On Process Discovery Experimentation",
    "doi": "https://doi.org/10.1145/3672447",
    "publication_date": "2024-11-13",
    "publication_year": 2024,
    "authors": "Jana-Rebecca Rehse; Sander J. J. Leemans; Peter Fettke; Jan Martijn E. M. van der Werf",
    "corresponding_authors": "",
    "abstract": "Process mining aims to derive insights into business processes from event logs recorded from information systems. Process discovery algorithms construct process models that describe the executed process. With the increasing availability of large-scale event logs, process discovery has shifted towards a data-oriented research discipline, aiming to design algorithms that are applicable and useful in practice. This shift has revealed a fundamental problem in process discovery research: Currently, contributions can only be considered in isolation. Researchers conduct experiments to show that they move the field forward, but due to a lack of reliability and validity, the individual contributions are hard to generalise. In this paper, we argue that one reason for these problems is the lack of conventions or standards for experimental design in process discovery. Hence, we propose “process discovery engineering”: a research methodology for process discovery, consisting of a shared terminology and a checklist for conducting experiments. We demonstrate its applicability by means of an example experimental evaluation of process discovery algorithms and discuss the implications of the methodology on the field. This paper is not meant to be prescriptive, but to invite and encourage the community to contribute to this discussion to advance the field as a whole.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404320458",
    "type": "article"
  },
  {
    "title": "Context-based Transfer Learning for Structuring Fault Localization and Program Repair Automation",
    "doi": "https://doi.org/10.1145/3705302",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Lehuan Zhang; Shikai Guo; Yi Guo; Hui Li; Yu Chai; Rong Chen; Xiaochen Li; He Jiang",
    "corresponding_authors": "",
    "abstract": "Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4%, 2.7%, and 1.6% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8% and 2.1%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404584824",
    "type": "article"
  },
  {
    "title": "<scp>SpectAcle</scp> : Fault Localisation of AI-Enabled CPS by Exploiting Sequences of DNN Controller Inferences",
    "doi": "https://doi.org/10.1145/3705307",
    "publication_date": "2024-11-22",
    "publication_year": 2024,
    "authors": "Deyun Lyu; Zhenya Zhang; Paolo Arcaini; Xiao-Yi Zhang; Fuyuki Ishikawa; Jianjun Zhao",
    "corresponding_authors": "",
    "abstract": "Cyber-Physical Systems (CPSs) are increasingly adopting deep neural networks (DNNs) as controllers, giving birth to AI-enabled CPSs . Despite their advantages, many concerns arise about the safety of DNN controllers. Numerous efforts have been made to detect system executions that violate safety specifications; however, once a violation is detected, to fix the issue, it is necessary to localise the parameters of the DNN controller responsible for the wrong decisions leading to the violation. This is particularly challenging, as it requires to consider a sequence of control decisions, rather than a single one, preceding the violation. To tackle this problem, we propose SpectAcle , that can localise the faulty parameters in DNN controllers. SpectAcle considers the DNN inferences preceding the specification violation and uses forward impact to determine the DNN parameters that are more relevant to the DNN outputs. Then, it identifies which of these parameters are responsible for the specification violation, by adapting classic suspiciousness metrics. Moreover, we propose two versions of SpectAcle , that consider differently the timestamps that precede the specification violation. We experimentally evaluate the effectiveness of SpectAcle on 6067 faulty benchmarks, spanning over different application domains. The results show that SpectAcle can detect most of the faults.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404635163",
    "type": "article"
  },
  {
    "title": "IATT: Interpretation Analysis based Transferable Test Generation for Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3705301",
    "publication_date": "2024-11-26",
    "publication_year": 2024,
    "authors": "Ruilin Xie; Xiang Chen; Qifan He; Bixin Li; Zhanqi Cui",
    "corresponding_authors": "",
    "abstract": "Convolutional neural networks (CNNs) have been widely used in various fields. However, it is essential to perform sufficient testing to detect internal defects before deploying CNNs, especially in security-sensitive scenarios. Generating error-inducing inputs to trigger erroneous behavior is the primary way to detect CNN model defects. However, in practice, when the model under test is a black-box CNN model without accessible internal information, in some scenarios it is still necessary to generate high-quality test inputs within a limited testing budget. In such a new scenario, a potential approach is to generate transferable test inputs by analyzing the internal knowledge of other white-box CNN models similar to the model under test, and then use transferable test inputs to test the black-box CNN model. The main challenge in generating transferable test inputs is how to improve their error-inducing capability for different CNN models without changing the test oracle. We found that different CNN models make predictions based on features of similar important regions in images. Adding targeted perturbations to important regions will generate transferable test inputs with high realism. Therefore, we propose the Interpretable Analysis based Transferable Test Generation method for CNNs (IATT), which employs interpretation methods of CNN models to explain and localize important regions in test inputs, using backpropagation optimizer and perturbation mask process to add targeted perturbations to these important regions, thereby generating transferable test inputs. This process is repeated to iteratively optimize the transferability and realism of the test inputs. To verify the effectiveness of IATT, we perform experimental studies on nine deep learning models, including ResNet-50 and Vit-B/16, and commercial computer vision system Google Cloud Vision , and compared our method with four state-of-the-art baseline methods. Experimental results show that transferable test inputs generated by IATT can effectively cause black-box target models to output incorrect results. Compared to existing testing and adversarial attack methods, the average error-inducing success rate (ESR) in different testing scenarios is 18.1% \\(\\sim\\) 52.7% greater than the baseline methods. Additionally, the test inputs generated by IATT achieve high ESR while maintaining high realism.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404711463",
    "type": "article"
  },
  {
    "title": "Vulnerability Repair via Concolic Execution and Code Mutations",
    "doi": "https://doi.org/10.1145/3707454",
    "publication_date": "2024-12-06",
    "publication_year": 2024,
    "authors": "Ridwan Shariffdeen; Christopher S. Timperley; Yannic Noller; Claire Le Goues; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "Security vulnerabilities detected via techniques like greybox fuzzing are often fixed with a significant time lag. This increases the exposure of the software to vulnerabilities. Automated fixing of vulnerabilities where a tool can generate fix suggestions is thus of value. In this work, we present such a tool, called CrashRepair , to automatically generate fix suggestions using concolic execution, specification inference, and search techniques. Our approach avoids generating fix suggestions merely at the crash location because such fixes often disable the manifestation of the error instead of fixing the error. Instead, based on sanitizer-guided concolic execution, we infer desired constraints at specific program locations and then opportunistically search for code mutations that help respect those constraints. Our technique only requires a single detected vulnerability or exploit as input; it does not require any user-provided properties. Evaluation results on a wide variety of CVEs in the VulnLoc benchmark, show CrashRepair achieves greater efficacy than state-of-the-art vulnerability repair tools like Senx. The repairs suggested come in the form of a ranked set of patches at different locations, and we show that on most occasions, the desired fix is among the top-3 fixes reported by CrashRepair .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405109142",
    "type": "article"
  },
  {
    "title": "A Catalog of Data Smells for Coding Tasks",
    "doi": "https://doi.org/10.1145/3707457",
    "publication_date": "2024-12-10",
    "publication_year": 2024,
    "authors": "Antonio Vitale; Rocco Oliveto; Simone Scalabrino",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) are increasingly becoming fundamental in supporting software developers in coding tasks. The massive datasets used for training LLMs are often collected automatically, leading to the introduction of data smells. Previous work addressed this issue by using quality filters to handle some specific smells. Still, the literature lacks a systematic catalog of the data smells for coding tasks currently known. This paper presents a Systematic Literature Review (SLR) focused on articles that introduce LLMs for coding tasks. We first extracted the quality filters adopted for training and testing such LLMs, inferred the root problem behind their adoption (data smells for coding tasks), and defined a taxonomy of such smells. Our results highlight discrepancies in the adoption of quality filters between pre-training and fine-tuning stages and across different coding tasks, shedding light on areas for improvement in LLM-based software development support.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405239633",
    "type": "article"
  },
  {
    "title": "<scp>DistMeasure</scp> : A Framework for Run-Time Characterization and Quality Assessment of Distributed Software via Interprocess Communications",
    "doi": "https://doi.org/10.1145/3708476",
    "publication_date": "2024-12-13",
    "publication_year": 2024,
    "authors": "Xiaoqin Fu; Asif Zaman; Haipeng Cai",
    "corresponding_authors": "",
    "abstract": "A defining, unique aspect of distributed systems lies in interprocess communication (IPC) through which distributed components interact and collaborate toward the holistic system behaviors. This highly decoupled construction intuitively contributes to the scalability, performance, and resiliency advantages of distributed software, but also adds largely to their greater complexity, compared to centralized software. Yet despite the importance of IPC in distributed systems, little is known about how to quantify IPC-induced behaviors in these systems through IPC measurement and how such behaviors may be related to the quality of distributed software . To answer these questions, in this paper, we present DistMeasure , a framework for measuring distributed software systems via the lens of IPC hence enabling the study of its correlation with distributed system quality. Underlying DistMeasure is a novel set of IPC metrics that focus on gauging the coupling and cohesion of distributed processes. Through these metrics, DistMeasure quantifies relevant run-time characteristics of distributed systems and their quality relevance, covering a range of quality aspects each via respective direct quality metrics. Further, DistMeasure enables predictive assessment of distributed system quality in those aspects via learning-based anomaly detection with respect to the corresponding quality metrics based on their significant correlations with related IPC metrics. Using DistMeasure , we demonstrated the practicality and usefulness of IPC measurement against 11 real-world distributed systems and their diverse execution scenarios. Among other findings, our results revealed that IPC has a strong correlation with distributed system complexity, performance efficiency, and security. Higher IPC coupling between distributed processes tended to be negatively indicative of distributed software quality, while more cohesive processes have positive quality implications. Yet overall IPC-induced behaviors are largely independent of the system scale, and higher (lower) process coupling does not necessarily come with lower (higher) process cohesion. We also show promising merits (with 98% precision/recall/F1) of IPC measurement (e.g., class-level coupling and process-level cohesion) for predictive anomaly assessment of various aspects (e.g., attack surface and performance efficiency) of distributed system quality.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405365805",
    "type": "article"
  },
  {
    "title": "A road-map to Readily Available Early Validation &amp; Verification of System Behaviour in Model-Based Systems Engineering using Software Engineering Best Practices",
    "doi": "https://doi.org/10.1145/3708520",
    "publication_date": "2024-12-13",
    "publication_year": 2024,
    "authors": "Johan Cederbladh; Antonio Cicchetti; Robbert Jongeling",
    "corresponding_authors": "",
    "abstract": "In this article we discuss how we can facilitate the growing need for early validation and verification (V&amp;V) of system behaviour in Model-Based Systems Engineering (MBSyE). Several aspects, such as reducing cost and time to market, push companies towards integration of V&amp;V methods earlier in development to support effective decision-making. One foundational methodology seeing increased attention in industry is the use of MBSyE, which brings benefits of models with well-defined syntax and semantics to support V&amp;V activities, rather than relying on natural language text documentation. Despite their promise, industrial adoption of these practices is still challenging. This article presents a vision for readily available early V&amp;V . We present a summary of the literature on early V&amp;V in MBSyE and position existing challenges regarding potential solutions and future investigations towards this vision. We elaborate our vision by means of challenges with a specific emphasis on early V&amp;V of system behaviour . We identify three specific challenge areas: Creating and managing Models , Organisational systems engineering aspects, and early V&amp;V Methods . Finally, we outline a road-map to address these categories of challenges, in which we propose the transfer of established best practices from the software engineering domain to support emerging technologies in the systems engineering domain.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405365953",
    "type": "article"
  },
  {
    "title": "LLM App Store Analysis: A Vision and Roadmap",
    "doi": "https://doi.org/10.1145/3708530",
    "publication_date": "2024-12-14",
    "publication_year": 2024,
    "authors": "Yanjie Zhao; Xinyi Hou; Shenao Wang; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "The rapid growth and popularity of large language model (LLM) app stores have created new opportunities and challenges for researchers, developers, users, and app store managers. As the LLM app ecosystem continues to evolve, it is crucial to understand the current landscape and identify potential areas for future research and development. This paper presents a forward-looking analysis of LLM app stores, focusing on key aspects such as data mining, security risk identification, development assistance, and market dynamics. Our comprehensive examination extends to the intricate relationships between various stakeholders and the technological advancements driving the ecosystem’s growth. We explore the ethical considerations and potential societal impacts of widespread LLM app adoption, highlighting the need for responsible innovation and governance frameworks. By examining these aspects, we aim to provide a vision for future research directions and highlight the importance of collaboration among stakeholders to address the challenges and opportunities within the LLM app ecosystem. The insights and recommendations provided in this paper serve as a foundation for driving innovation, ensuring responsible development, and creating a thriving, user-centric LLM app landscape.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405396166",
    "type": "article"
  },
  {
    "title": "Contemporary Software Modernization: Strategies, Driving Forces, and Research Opportunities",
    "doi": "https://doi.org/10.1145/3708527",
    "publication_date": "2024-12-16",
    "publication_year": 2024,
    "authors": "Wesley K. G. Assunção; Luciano Marchezan; Lawrence Arkoh; Alexander Egyed; Rudolf Ramler",
    "corresponding_authors": "",
    "abstract": "Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 127. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405444642",
    "type": "article"
  },
  {
    "title": "Explaining Explanations: An Empirical Study of Explanations in Code Reviews",
    "doi": "https://doi.org/10.1145/3708518",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Ratnadira Widyasari; Ting Zhang; Abir Bouraffa; Walid Maalej; David Lo",
    "corresponding_authors": "",
    "abstract": "Code reviews are central for software quality assurance. Ideally, reviewers should explain their feedback to enable authors of code changes to understand the feedback and act accordingly. Different developers might need different explanations in different contexts. Therefore, assisting this process first requires understanding the types of explanations reviewers usually provide. The goal of this paper is to study the types of explanations used in code reviews and explore the potential of Large Language Models (LLMs), specifically ChatGPT, in generating these specific types. We extracted 793 code review comments from Gerrit and manually labeled them based on whether they contained a suggestion, an explanation, or both. Our analysis shows that 42% of comments only include suggestions without explanations. We categorized the explanations into seven distinct types including rule or principle, similar examples, and future implications. When measuring their prevalence, we observed that some explanations are used differently by novice and experienced reviewers. Our manual evaluation shows that, when the explanation type is specified, ChatGPT can correctly generate the explanation in 88 out of 90 cases. This foundational work highlights the potential for future automation in code reviews, which can assist developers in sharing and obtaining different types of explanations as needed, thereby reducing back-and-forth communication.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405543754",
    "type": "article"
  },
  {
    "title": "Metamorphic Relation Generation: State of the Art and Research Directions",
    "doi": "https://doi.org/10.1145/3708521",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Rui Li; Huai Liu; Pak‐Lok Poon; Dave Towey; Chang‐ai Sun; Zheng Zheng; Zhi Quan Zhou; Tsong Yueh Chen",
    "corresponding_authors": "",
    "abstract": "Metamorphic testing has become one mainstream technique to address the notorious oracle problem in software testing, thanks to its great successes in revealing real-life bugs in a wide variety of software systems. Metamorphic relations, the core component of metamorphic testing, have continuously attracted research interests from both academia and industry. In the last decade, a rapidly increasing number of studies have been conducted to systematically generate metamorphic relations from various sources and for different application domains. In this article, based on the systematic review on the state of the art for metamorphic relations’ generation, we summarize and highlight visions for further advancing the theory and techniques for identifying and constructing metamorphic relations, and discuss promising research directions in related areas.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405543795",
    "type": "article"
  },
  {
    "title": "LLM for Mobile: An Initial Roadmap",
    "doi": "https://doi.org/10.1145/3708528",
    "publication_date": "2024-12-20",
    "publication_year": 2024,
    "authors": "D. Chen; Yonghui Liu; Mingyi Zhou; Yanjie Zhao; Haoyu Wang; Shuai Wang; Xiao Chen; Tegawendé F. Bissyandé; Jacques Klein; Li Li",
    "corresponding_authors": "",
    "abstract": "When mobile meets LLMs, mobile app users deserve to have more intelligent usage experiences. For this to happen, we argue that there is a strong need to apply LLMs for the mobile ecosystem. We therefore provide a research roadmap for guiding our fellow researchers to achieve that as a whole. In this roadmap, we sum up six directions that we believe are urgently required for research to enable native intelligence in mobile devices. In each direction, we further summarize the current research progress and the gaps that still need to be filled by our fellow researchers.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405640587",
    "type": "article"
  },
  {
    "title": "Grammar Mutation for Testing Input Parsers",
    "doi": "https://doi.org/10.1145/3708517",
    "publication_date": "2024-12-20",
    "publication_year": 2024,
    "authors": "Bachir Bendrissou; Cristian Cadar; Alastair F. Donaldson",
    "corresponding_authors": "",
    "abstract": "Grammar-based fuzzing is an effective method for testing programs that consume structured inputs, particularly input parsers. However, if the available grammar does not accurately represent the input format, or if the system under test (SUT) does not conform strictly to the grammar, there may be an impedance mismatch between inputs generated via grammars and inputs accepted by the SUT. Even if the SUT has been designed to strictly conform to the grammar, the SUT parser may exhibit vulnerabilities that would only be triggered by slightly invalid inputs. Grammar-based generation, by construction, will not yield such edge case inputs. To overcome these limitations, we present two mutational-based approaches: Gmutator and G+M. Both approaches are built upon Grammarinator , a grammar-based generator. Gmutator applies mutations to the grammar input of Grammarinator , while G+M directly applies byte-level mutations to Grammarinator -generated inputs. To evaluate the effectiveness of these techniques ( Grammarinator , Gmutator , G+M) in testing programs that parse various input formats, we conducted an experimental evaluation over four different input formats and twelve SUTs (three per input format). Our findings suggest that both Gmutator and G+M excel in generating edge case inputs, facilitating the detection of disparities between input specifications and parser implementations.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405640723",
    "type": "article"
  },
  {
    "title": "From Triumph to Uncertainty: The Journey of Software Engineering in the AI Era",
    "doi": "https://doi.org/10.1145/3709360",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Antonio Mastropaolo; Camilo Escobar‐Velásquez; Mario Linares‐Vásquez",
    "corresponding_authors": "",
    "abstract": "Over the last ten years, the realm of Artificial Intelligence (AI) has experienced an explosion of revolutionary breakthroughs, transforming what seemed like a far-off dream into a reality that is now deeply embedded in our everyday lives. AI’s widespread impact is revolutionizing virtually all aspects of human life, and software engineering (SE) is no exception. As we explore this changing landscape, we are faced with questions about what the future holds for SE and how AI will reshape the roles, duties, and methodologies within the field. The introduction of these groundbreaking technologies highlights the inevitable shift towards a new paradigm, suggesting a future where AI’s capabilities may redefine the boundaries of SE, potentially even more than human input. In this paper, we aim at outlining the key elements that, based on our expertise, are vital for the smooth integration of AI into SE, all while preserving the intrinsic human creativity that has been the driving force behind the field. First, we provide a brief description of SE and AI evolution. Afterward, we delve into the intricate interplay between AI-driven automation and human innovation, exploring how these two components can work together to advance SE practices to new methods and standards.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405703830",
    "type": "article"
  },
  {
    "title": "<scp>Scuzer</scp> : A Scheduling Optimization Fuzzer for TVM",
    "doi": "https://doi.org/10.1145/3705308",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Xiangxiang Chen; Xingwei Lin; Jingyi Wang; Jun Sun; Jiashui Wang; Wenhai Wang",
    "corresponding_authors": "",
    "abstract": "The concept of deep learning (DL) compiler was proposed to deploy DL models more efficiently on diverse hardware through optimization techniques. As one of the most popular DL compilers, TVM incorporates three levels (high-level, schedule, and low-level) of optimizations, which can inadvertently introduce code logic bugs and build failure bugs. Among these optimizations, scheduling optimization is the core component of DL compilers, which ensures the acceleration of models on all devices. However, the existing works only focus on the testing of high-level and low-level optimizations in TVM, fail to take the most important and challenging intermediate scheduling optimization layer into consideration. To fill the gap, we propose a Sc heduling optimization oriented f u z zer for TVM, named Scuzer , which is specially designed to effectively detect bugs introduced by the scheduling optimization. In particular, Scuzer first proposes a set of schedule-triggering mutators to actively trigger many scheduling optimizations. Meanwhile, observing that scheduling optimization is closely coupled with program data flow and operator type, Scuzer additionally proposes a set of structure-enriching mutators to enrich the structure of data flows and operators. Based on these carefully designed mutators, Scuzer then devises a multi-objective algorithm that can adaptively select different combinations of objectives at each period to guide the selection of seeds and mutators during fuzzing. We conduct extensive experiments comparing with three state-of-the-art fuzzers that can be applied in testing scheduling optimization to evaluate the effectiveness of Scuzer . The experimental results demonstrate that Scuzer outperforms the 2nd-best state-of-the-art fuzzer by 7.4% in edge coverage and achieves 7 \\(\\times\\) improvement in rule-operator coverage. Scuzer has successfully detected 17 previously unknown bugs (9 are inconsistent results and 5 are inconsistent compilations) in TVM, out of which 10 have been confirmed and 5 been fixed.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405703944",
    "type": "article"
  },
  {
    "title": "A Roadmap for Software Testing in Open-Collaborative and AI-Powered Era",
    "doi": "https://doi.org/10.1145/3709355",
    "publication_date": "2024-12-24",
    "publication_year": 2024,
    "authors": "Qing Wang; Junjie Wang; Mingyang Li; Yawen Wang; Zhe Liu",
    "corresponding_authors": "",
    "abstract": "Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in artificial intelligence (AI) have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this paper explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as large language models (LLMs) and the AI model-centric development paradigm.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405758136",
    "type": "article"
  },
  {
    "title": "An inheritance-based technique for building simulation proofs incrementally",
    "doi": "https://doi.org/10.1145/504087.504090",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Idit Keidar; Roger Khazan; Nancy Lynch; Alex A. Shvartsman",
    "corresponding_authors": "",
    "abstract": "This paper presents a formal technique for incremental construction of system specifications, algorithm descriptions, and simulation proofs showing that algorithms meet their specifications.The technique for building specifications and algorithms incrementally allows a child specification or algorithm to inherit from its parent by two forms of incremental modification: (a) signature extension , where new actions are added to the parent, and (b) specialization (subtyping), where the child's behavior is a specialization (restriction) of the parent's behavior. The combination of signature extension and specialization provides a powerful and expressive incremental modification mechanism for introducing new types of behavior without overriding behavior of the parent; this mechanism corresponds to the subclassing for extension form of inheritance.In the case when incremental modifications are applied to both a parent specification S and a parent algorithm A, the technique allows a simulation proof showing that the child algorithm A′ implements the child specification S′ to be constructed incrementally by extending a simulation proof that algorithm A implements specification S. The new proof involves reasoning about the modifications only, without repeating the reasoning done in the original simulation proof.The paper presents the technique mathematically, in terms of automata. The technique has been used to model and verify a complex middleware system; the methodology and results of that experiment are summarized in this paper.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2127063571",
    "type": "article"
  },
  {
    "title": "StreamGen",
    "doi": "https://doi.org/10.1145/3408895",
    "publication_date": "2021-01-20",
    "publication_year": 2021,
    "authors": "Michele Guerriero; Damian A. Tamburri; Elisabetta Di Nitto",
    "corresponding_authors": "",
    "abstract": "Distributed streaming applications, i.e., applications that process massive streams of data in a distributed fashion, are becoming increasingly popular to tame the velocity and the volume of Big Data . Nevertheless, the widespread adoption of data-intensive processing is still limited by the non-trivial design paradigms involved, which deal with the unboundedness and volume of involved data streams and by the many distributed streaming platforms, each with its own characteristics and APIs. In this article, we present StreamGen, a Model-Driven Engineering tool to simplify the design of such streaming applications and automatically generate the corresponding code. StreamGen is able to automatically generate fully working and processing-ready code for different target platforms (e.g., Apache Spark, Apache Flink). Evaluation shows that (i) StreamGen is general enough to model and generate the code, offering comparable performance against a preexisting similar and well-known application; (ii) the tool is fully compliant with streaming concepts defined as part of the Google Dataflow Model; and (iii) users with little computer science background and limited experience with big data have been able to work with StreamGen and create/refactor an application in a matter of minutes.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3122095170",
    "type": "article"
  },
  {
    "title": "Facet-oriented Modelling",
    "doi": "https://doi.org/10.1145/3428076",
    "publication_date": "2021-02-11",
    "publication_year": 2021,
    "authors": "Juan de Lara; Esther Guerra; Jörg Kienzle",
    "corresponding_authors": "",
    "abstract": "Models are the central assets in model-driven engineering (MDE), as they are actively used in all phases of software development. Models are built using metamodel-based languages, and so objects in models are typed by a metamodel class. This typing is static, established at creation time, and cannot be changed later. Therefore, objects in MDE are closed and fixed with respect to the class they conform to, the fields they have, and the well-formedness constraints they must comply with. This hampers many MDE activities, like the reuse of model-related artefacts such as transformations, the opportunistic or dynamic combination of metamodels, or the dynamic reconfiguration of models. To alleviate this rigidity, we propose making model objects open so that they can acquire or drop so-called facets . These contribute with a type, fields and constraints to the objects holding them. Facets are defined by regular metamodels, hence being a lightweight extension of standard metamodelling. Facet metamodels may declare usage interfaces , as well as laws that govern the assignment of facets to objects (or classes). This article describes our proposal, reporting on a theory, analysis techniques, and an implementation. The benefits of the approach are validated on the basis of five case studies dealing with annotation models, transformation reuse, multi-view modelling, multi-level modelling, and language product lines.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3133170989",
    "type": "article"
  },
  {
    "title": "Recommending Faulty Configurations for Interacting Systems Under Test Using Multi-objective Search",
    "doi": "https://doi.org/10.1145/3464939",
    "publication_date": "2021-08-03",
    "publication_year": 2021,
    "authors": "Safdar Aqeel Safdar; Tao Yue; Shaukat Ali",
    "corresponding_authors": "",
    "abstract": "Modern systems, such as cyber-physical systems, often consist of multiple products within/across product lines communicating with each other through information networks. Consequently, their runtime behaviors are influenced by product configurations and networks. Such systems play a vital role in our daily life; thus, ensuring their correctness by thorough testing becomes essential. However, testing these systems is particularly challenging due to a large number of possible configurations and limited available resources. Therefore, it is important and practically useful to test these systems with specific configurations under which products will most likely fail to communicate with each other. Motivated by this, we present a search-based configuration recommendation ( SBCR ) approach to recommend faulty configurations for the system under test (SUT) based on cross-product line (CPL) rules. CPL rules are soft constraints, constraining product configurations while indicating the most probable system states with a certain degree of confidence. In SBCR , we defined four search objectives based on CPL rules and combined them with six commonly applied search algorithms. To evaluate SBCR (i.e., SBCR NSGA-II , SBCR IBEA , SBCR MoCell , SBCR SPEA2 , SBCR PAES , and SBCR SMPSO ), we performed two case studies (Cisco and Jitsi) and conducted difference analyses. Results show that for both of the case studies, SBCR significantly outperformed random search-based configuration recommendation ( RBCR ) for 86% of the total comparisons based on six quality indicators, and 100% of the total comparisons based on the percentage of faulty configurations (PFC). Among the six variants of SBCR, SBCR SPEA2 outperformed the others in 85% of the total comparisons based on six quality indicators and 100% of the total comparisons based on PFC.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3189490885",
    "type": "article"
  },
  {
    "title": "Global and Local Deadlock Freedom in BIP",
    "doi": "https://doi.org/10.1145/3152910",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Paul C. Attie; Saddek Bensalem; Marius Bozga; Mohamad Jaber; Joseph Sifakis; Fadi A. Zaraket",
    "corresponding_authors": "",
    "abstract": "We present a criterion for checking local and global deadlock freedom of finite state systems expressed in BIP: a component-based framework for constructing complex distributed systems. Our criterion is evaluated by model-checking a set of subsystems of the overall large system. If satisfied in small subsystems, it implies deadlock-freedom of the overall system. If not satisfied, then we re-evaluate over larger subsystems, which improves the accuracy of the check. When the subsystem being checked becomes the entire system, our criterion becomes complete for deadlock-freedom. Hence our criterion only fails to decide deadlock freedom because of computational limitations: state-space explosion sets in when the subsystems become too large. Our method thus combines the possibility of fast response together with theoretical completeness. Other criteria for deadlock freedom, in contrast, are incomplete in principle, and so may fail to decide deadlock freedom even if unlimited computational resources are available. Also, our criterion certifies freedom from local deadlock, in which a subsystem is deadlocked while the rest of the system executes. Other criteria only certify freedom from global deadlock. We present experimental results for dining philosophers and for a multi-token-based resource allocation system, which subsumes several data arbiters and schedulers, including Milner’s token-based scheduler.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2782532010",
    "type": "article"
  },
  {
    "title": "ADAM",
    "doi": "https://doi.org/10.1145/2529998",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Dharmalingam Ganesan; Mikael Lindvall",
    "corresponding_authors": "",
    "abstract": "This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAM supports the discovery of module and runtime views as well as the analysis of quality attributes, such as testability, performance, and maintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2014757669",
    "type": "article"
  },
  {
    "title": "Concurrency Debugging with Differential Schedule Projections",
    "doi": "https://doi.org/10.1145/2885495",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Nuno Machado; Daniel Quinta; Brandon Lucia; Luı́s Rodrigues",
    "corresponding_authors": "",
    "abstract": "We present Symbiosis: a concurrency debugging technique based on novel differential schedule projections (DSPs). A DSP shows the small set of memory operations and dataflows responsible for a failure, as well as a reordering of those elements that avoids the failure. To build a DSP, Symbiosis first generates a full, failing, multithreaded schedule via thread path profiling and symbolic constraint solving. Symbiosis selectively reorders events in the failing schedule to produce a nonfailing, alternate schedule . A DSP reports the ordering and dataflow differences between the failing and nonfailing schedules. Our evaluation on buggy real-world software and benchmarks shows that, in practical time, Symbiosis generates DSPs that both isolate the small fraction of event orders and dataflows responsible for the failure and report which event reorderings prevent failing. In our experiments, DSPs contain 90% fewer events and 96% fewer dataflows than the full failure-inducing schedules. We also conducted a user study that shows that, by allowing developers to focus on only a few events, DSPs reduce the amount of time required to understand the bug’s root cause and find a valid fix.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2317931072",
    "type": "article"
  },
  {
    "title": "<scp>FormatFuzzer</scp> : Effective Fuzzing of Binary File Formats",
    "doi": "https://doi.org/10.1145/3628157",
    "publication_date": "2023-10-17",
    "publication_year": 2023,
    "authors": "Rafael Dutra; Rahul Gopinath; Andreas Zeller",
    "corresponding_authors": "",
    "abstract": "Effective fuzzing of programs that process structured binary inputs, such as multimedia files, is a challenging task, since those programs expect a very specific input format. Existing fuzzers, however, are mostly format-agnostic, which makes them versatile, but also ineffective when a specific format is required. We present FormatFuzzer , a generator for format-specific fuzzers . FormatFuzzer takes as input a binary template (a format specification used by the 010 Editor) and compiles it into C++ code that acts as parser, mutator, and highly efficient generator of inputs conforming to the rules of the language. The resulting format-specific fuzzer can be used as a standalone producer or mutator in black-box settings, where no guidance from the program is available. In addition, by providing mutable decision seeds, it can be easily integrated with arbitrary format-agnostic fuzzers such as AFL to make them format-aware. In our evaluation on complex formats such as MP4 or ZIP, FormatFuzzer showed to be a highly effective producer of valid inputs that also detected previously unknown memory errors in ffmpeg and timidity .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3200756997",
    "type": "article"
  },
  {
    "title": "SEAL: Integrating Program Analysis and Repository Mining",
    "doi": "https://doi.org/10.1145/3585008",
    "publication_date": "2023-02-25",
    "publication_year": 2023,
    "authors": "Florian Sattler; Sebastian Böhm; Philipp Dominik Schubert; Norbert Siegmund; Sven Apel",
    "corresponding_authors": "",
    "abstract": "Software projects are complex technical and organizational systems involving large numbers of artifacts and developers. To understand and tame software complexity, a wide variety of program analysis techniques have been developed for bug detection, program comprehension, verification, and more. At the same time, repository mining techniques aim at obtaining insights into the inner socio-technical workings of software projects at a larger scale. While both program analysis and repository mining have been successful on their own, they are largely isolated, which leaves considerable potential for synergies untapped. We present SEAL, the first integrated approach that combines low-level program analysis with high-level repository information. SEAL maps repository information, mined from the development history of a project, onto a low-level intermediate program representation, making it available for state-of-the-art program analysis. SEAL’s integrated approach allows us to efficiently address software engineering problems that span multiple levels of abstraction, from low-level data flow to high-level organizational information. To demonstrate its merits and practicality, we use SEAL to determine which code changes modify central parts of a given software project, how authors interact (indirectly) with each other through code, and we demonstrate that putting static analysis’ results into a socio-technical context improves their expressiveness and interpretability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4322007924",
    "type": "article"
  },
  {
    "title": "Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection",
    "doi": "https://doi.org/10.1145/3597202",
    "publication_date": "2023-05-16",
    "publication_year": 2023,
    "authors": "Sahil Suneja; Yufan Zhuang; Yunhui Zheng; Jim Laredo; Alessandro Morari; Udayan Khurana",
    "corresponding_authors": "",
    "abstract": "AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness , i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models’ signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models’ signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model’s performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8× improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4376643746",
    "type": "article"
  },
  {
    "title": "Programming by Example Made Easy",
    "doi": "https://doi.org/10.1145/3607185",
    "publication_date": "2023-07-07",
    "publication_year": 2023,
    "authors": "Jiarong Wu; Lili Wei; Yanyan Jiang; Shing-Chi Cheung; Luyao Ren; Chang Xu",
    "corresponding_authors": "",
    "abstract": "Programming by example (PBE) is an emerging programming paradigm that automatically synthesizes programs specified by user-provided input-output examples. Despite the convenience for end-users, implementing PBE tools often requires strong expertise in programming language and synthesis algorithms. Such a level of knowledge is uncommon among software developers. It greatly limits the broad adoption of PBE by the industry. To facilitate the adoption of PBE techniques, we propose a PBE framework called Bee, which leverages an \"entity-action\" model based on relational tables to ease PBE development for a wide but restrained range of domains. Implementing PBE tools with Bee only requires adapting domain-specific data entities and user actions to tables, with no need to design a domain-specific language or an efficient synthesis algorithm. The synthesis algorithm of Bee exploits bidirectional searching and constraint-solving techniques to address the challenge of value computation nested in table transformation. We evaluated Bee's effectiveness on 64 PBE tasks from three different domains and usability with a human study of 12 participants. Evaluation results show that Bee is easier to learn and use than the state-of-the-art PBE framework, and the bidirectional algorithm achieves comparable performance to domain-specifically optimized synthesizers.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4383555717",
    "type": "article"
  },
  {
    "title": "TopicAns: Topic-informed Architecture for Answer Recommendation on Technical Q&amp;A Site",
    "doi": "https://doi.org/10.1145/3607189",
    "publication_date": "2023-07-11",
    "publication_year": 2023,
    "authors": "Yuanhang Yang; Wei He; Cuiyun Gao; Zenglin Xu; Xin Xia; Chuanyi Liu",
    "corresponding_authors": "",
    "abstract": "Technical Q&amp;A sites, such as Stack Overflow and Ask Ubuntu, have been widely utilized by software engineers to seek support for development challenges. However, not all the raised questions get instant feedback, and the retrieved answers can vary in quality. The users can hardly avoid spending much time before solving their problems. Prior studies propose approaches to automatically recommend answers for the question posts on technical Q&amp;A sites. However, the lengthiness and the lack of background knowledge issues limit the performance of answer recommendation on these sites. The irrelevant sentences in the posts may introduce noise to the semantics learning and prevent neural models from capturing the gist of texts. The lexical gap between question and answer posts further misleads current models to make failure recommendations. From this end, we propose a novel neural network named TopicAns for answer selection on technical Q&amp;A sites. TopicAns aims at learning high-quality representations for the posts in Q&amp;A sites with a neural topic model and a pre-trained model. This involves three main steps: (1) generating topic-aware representations of Q&amp;A posts with the neural topic model, (2) incorporating the corpus-level knowledge from the neural topic model to enhance the deep representations generated by the pre-trained language model, and (3) determining the most suitable answer for a given query based on the topic-aware representation and the deep representation. Moreover, we propose a two-stage training technique to improve the stability of our model. We conduct comprehensive experiments on four benchmark datasets to verify our proposed TopicAns’s effectiveness. Experiment results suggest that TopicAns consistently outperforms state-of-the-art techniques by over 30% in terms of Precision@1.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4383906838",
    "type": "article"
  },
  {
    "title": "DRIVE: Dockerfile Rule Mining and Violation Detection",
    "doi": "https://doi.org/10.1145/3617173",
    "publication_date": "2023-08-21",
    "publication_year": 2023,
    "authors": "Yu Zhou; Weilin Zhan; Zi Li; Tingting Han; Taolue Chen; Harald C. Gall",
    "corresponding_authors": "",
    "abstract": "A Dockerfile defines a set of instructions to build Docker images, which can then be instantiated to support containerized applications. Recent studies have revealed a considerable amount of quality issues with Dockerfiles. In this article, we propose a novel approach, Dockerfiles Rule mIning and Violation dEtection ( DRIVE ), to mine implicit rules and detect potential violations of such rules in Dockerfiles. DRIVE first parses Dockerfiles and transforms them to an intermediate representation. It then leverages an efficient sequential pattern mining algorithm to extract potential patterns. With heuristic-based reduction and moderate human intervention, potential rules are identified, which can then be utilized to detect potential violations of Dockerfiles. DRIVE identifies 34 semantic rules and 19 syntactic rules including 9 new semantic rules that have not been reported elsewhere. Extensive experiments on real-world Dockerfiles demonstrate the efficacy of our approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386027301",
    "type": "article"
  },
  {
    "title": "KAPE: <i>k</i> NN-based Performance Testing for Deep Code Search",
    "doi": "https://doi.org/10.1145/3624735",
    "publication_date": "2023-09-16",
    "publication_year": 2023,
    "authors": "Yuejun Guo; Qiang Hu; Xiaofei Xie; Maxime Cordy; Mike Papadakis; Yves Le Traon",
    "corresponding_authors": "",
    "abstract": "Code search is a common yet important activity of software developers. An efficient code search model can largely facilitate the development process and improve the programming quality. Given the superb performance of learning the contextual representations, deep learning models, especially pre-trained language models, have been widely explored for the code search task. However, studies mainly focus on proposing new architectures for ever-better performance on designed test sets but ignore the performance on unseen test data where only natural language queries are available. The same problem in other domains, e.g., CV and NLP, is usually solved by test input selection that uses a subset of the unseen set to reduce the labeling effort. However, approaches from other domains are not directly applicable and still require labeling effort. In this article, we propose the k NN-b a sed p erformance t e sting ( KAPE ) to efficiently solve the problem without manually matching code snippets to test queries. The main idea is to use semantically similar training data to perform the evaluation. Extensive experiments on six programming language datasets, three state-of-the-art pre-trained models, and seven baseline methods demonstrate that KAPE can effectively assess the model performance (e.g., CodeBERT achieves MRR 0.5795 on JavaScript) with a slight difference (e.g., 0.0261).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386802874",
    "type": "article"
  },
  {
    "title": "Automated and Efficient Test-Generation for Grid-Based Multiagent Systems: Comparing Random Input Filtering versus Constraint Solving",
    "doi": "https://doi.org/10.1145/3624736",
    "publication_date": "2023-09-30",
    "publication_year": 2023,
    "authors": "Sina Entekhabi; Wojciech Mostowski; Mohammad Reza Mousavi",
    "corresponding_authors": "",
    "abstract": "Automatic generation of random test inputs is an approach that can alleviate the challenges of manual test case design. However, random test cases may be ineffective in fault detection and increase testing cost, especially in systems where test execution is resource- and time-consuming. To remedy this, the domain knowledge of test engineers can be exploited to select potentially effective test cases. To this end, test selection constraints suggested by domain experts can be utilized either for filtering randomly generated test inputs or for direct generation of inputs using constraint solvers. In this article, we propose a domain specific language (DSL) for formalizing locality-based test selection constraints of autonomous agents and discuss the impact of test selection filters, specified in our DSL, on randomly generated test cases. We study and compare the performance of filtering and constraint solving approaches in generating selective test cases for different test scenario parameters and discuss the role of these parameters in test generation performance. Through our study, we provide criteria for suitability of the random data filtering approach versus the constraint solving one under the varying size and complexity of our testing problem. We formulate the corresponding research questions and answer them by designing and conducting experiments using QuickCheck for random test data generation with filtering and Z3 for constraint solving. Our observations and statistical analysis indicate that applying filters can significantly improve test efficiency of randomly generated test cases. Furthermore, we observe that test scenario parameters affect the performance of the filtering and constraint solving approaches differently. In particular, our results indicate that the two approaches have complementary strengths: random generation and filtering works best for large agent numbers and long paths, while its performance degrades in the larger grid sizes and more strict constraints. On the contrary, constraint solving has a robust performance for large grid sizes and strict constraints, while its performance degrades with more agents and long paths.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387219713",
    "type": "article"
  },
  {
    "title": "Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health",
    "doi": "https://doi.org/10.1145/3630252",
    "publication_date": "2023-11-09",
    "publication_year": 2023,
    "authors": "Andre Lustosa; Tim Menzies",
    "corresponding_authors": "",
    "abstract": "When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors. Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a) clusters the data to find the general landscape of the hyperparameters, then (b) explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA). The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK ’s 12-month prediction errors are {I=0%, R=33% C=47%}, whereas other methods have far larger errors of {I=61%,R=119% C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options. Based on the preceding, we recommend landscape analytics (e.g., niSNEAK ) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems. To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388539339",
    "type": "article"
  },
  {
    "title": "SourcererJBF: A Java Build Framework For Large-Scale Compilation",
    "doi": "https://doi.org/10.1145/3635710",
    "publication_date": "2023-12-02",
    "publication_year": 2023,
    "authors": "Md Rakib Hossain Misu; Rohan Achar; Cristina Videira Lopes",
    "corresponding_authors": "",
    "abstract": "Researchers and tool developers working on dynamic analysis, software testing, automated program repair, verification, and validation, need large compiled, compilable, and executable code corpora to test their ideas. The publicly available corpora are relatively small, and/or non-compilable, and/or non-executable. Developing a compiled code corpus is a laborious activity demanding significant manual effort and human intervention. To facilitate large-scale program analysis research, we develop SourcererJBF , a J ava B uild F ramework that can automatically build a large Java code corpus without project-specific instructions and human intervention. To generate a compiled code corpus, SourcererJBF creates an offline knowledge base by collecting external dependencies from the project directories and existing build scripts (if available). It constructs indices of those collected external dependencies that enable a fast search for resolving dependencies during the project compilation. As the output of the large-scale compilation, it produces JAigantic, a compilable Java corpus containing compiled projects, their bytecode, dependencies, normalized build script, and build command. We evaluated SourcererJBF’s effectiveness, correctness, performance, and scalability in a large collection of Java projects. Our experimental results demonstrate that SourcererJBF is significantly effective and scalable in building large Java code corpus. Besides, it substantiates reasonable performance and correctness similar to projects’ existing build systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389268893",
    "type": "article"
  },
  {
    "title": "Algorithm Selection for Software Verification Using Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3637225",
    "publication_date": "2023-12-11",
    "publication_year": 2023,
    "authors": "Will Leeson; Matthew B. Dwyer",
    "corresponding_authors": "",
    "abstract": "The field of software verification has produced a wide array of algorithmic techniques that can prove a variety of properties of a given program. It has been demonstrated that the performance of these techniques can vary up to 4 orders of magnitude on the same verification problem. Even for verification experts, it is difficult to decide which tool will perform best on a given problem. For general users, deciding the best tool for their verification problem is effectively impossible. In this work, we present Graves , a selection strategy based on graph neural networks (GNNs). Graves generates a graph representation of a program from which a GNN predicts a score for a verifier that indicates its performance on the program. We evaluate Graves on a set of 10 verification tools and over 8,000 verification problems and find that it improves the state-of-the-art in verification algorithm selection by 12%, or 8 percentage points. Further, it is able to verify 9% more problems than any existing verifier on our test set. Through a qualitative study on model interpretability, we find strong evidence that the Graves model learns to base its predictions on factors that relate to the unique features of the algorithmic techniques.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389560258",
    "type": "article"
  },
  {
    "title": "Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects",
    "doi": "https://doi.org/10.1145/3638245",
    "publication_date": "2023-12-20",
    "publication_year": 2023,
    "authors": "Han Wang; Sijia Yu; Chunyang Chen; Burak Turhan; Xiaodong Zhu",
    "corresponding_authors": "",
    "abstract": "Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: (1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests; (2) 68% of the sampled DL projects are not unit tested at all; (3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability. The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390002420",
    "type": "article"
  },
  {
    "title": "Reifying variants in configuration management",
    "doi": "https://doi.org/10.1145/310663.310668",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Jean‐Marc Jezéquél",
    "corresponding_authors": "Jean‐Marc Jezéquél",
    "abstract": "Using a solid software configuration management (SCM) is mandatory to establish and maintain the integrity of the products of a software project throughout the project's software life cycle. Even with the help of sophisticated tools, handling the various dimensions of SCM can be a daunting (and costly) task for many projects. The contribution of this article is to (1)propose a method (based on the use creational design patterns) to simplify SCM by reifying the variants of an object-oriented software system into language-level objects and (2)show that newly available compilation technology makes this proposal attractive with respect to performance (memory footprint and execution time) by inferring which classes are needed for a specific configuration and optimizing the generated code accordingly.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2060210353",
    "type": "article"
  },
  {
    "title": "Mastering Variation in Human Studies",
    "doi": "https://doi.org/10.1145/3406544",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Janet Siegmund; Norman Peitek; Sven Apel; Norbert Siegmund",
    "corresponding_authors": "",
    "abstract": "The human factor is prevalent in empirical software engineering research. However, human studies often do not use the full potential of analysis methods by combining analysis of individual tasks and participants with an analysis that aggregates results over tasks and/or participants. This may hide interesting insights of tasks and participants and may lead to false conclusions by overrating or underrating single-task or participant performance. We show that studying multiple levels of aggregation of individual tasks and participants allows researchers to have both insights from individual variations as well as generalized, reliable conclusions based on aggregated data. Our literature survey revealed that most human studies perform either a fully aggregated analysis or an analysis of individual tasks. To show that there is important, non-trivial variation when including human participants, we reanalyze 12 published empirical studies, thereby changing the conclusions or making them more nuanced. Moreover, we demonstrate the effects of different aggregation levels by answering a novel research question on published sets of fMRI data. We show that when more data are aggregated, the results become more accurate. This proposed technique can help researchers to find a sweet spot in the tradeoff between cost of a study and reliability of conclusions.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3117894873",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1189748.1189749",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4242584075",
    "type": "editorial"
  },
  {
    "title": "A two-phase approximation for model checking probabilistic unbounded until properties of probabilistic systems",
    "doi": "https://doi.org/10.1145/2211616.2211621",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Paul C. Jennings; Arka P. Ghosh; Samik Basu",
    "corresponding_authors": "",
    "abstract": "We have developed a new approximate probabilistic model-checking method for untimed properties in probabilistic systems, expressed in a probabilistic temporal logic (PCTL, CSL). This method, in contrast to the existing ones, does not require the untimed until properties to be bounded a priori, where the bound refers to the number of discrete steps in the system required to verify the until property. The method consists of two phases. In the first phase, a suitable system- and property-dependent bound k 0 is obtained automatically. In the second phase, the probability of satisfying the k 0 -bounded until property is computed as the estimate of the probability of satisfying the original unbounded until property. Both phases require only verification of bounded until properties, which can be effectively performed by simulation-based methods. We prove the correctness of the proposed two-phase method and present its optimized implementation in the widely used PRISM model-checking engine. We compare this implementation with sampling-based model-checking techniques implemented in two tools: PRISM and MRMC. We show that for several models these existing tools fail to compute the result, while the two-phase method successfully computes the result efficiently with respect to time and space.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2016582864",
    "type": "article"
  },
  {
    "title": "Views",
    "doi": "https://doi.org/10.1145/2430536.2430538",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Brian Demsky; Patrick Lam",
    "corresponding_authors": "",
    "abstract": "Fine-grained locking is often necessary to increase concurrency. Correctly implementing fine-grained locking with today's concurrency primitives can be challenging—race conditions often plague programs with sophisticated locking schemes. We present views, a new approach to concurrency control. Views ease the task of implementing sophisticated locking schemes and provide static checks to automatically detect many data races. A view of an object declares a partial interface, consisting of fields and methods, to the object that the view protects. A view also contains an incompatibility declaration, which lists views that may not be simultaneously held by other threads. A set of view annotations specify which code regions hold a view of an object. Our view compiler performs simple static checks that identify many data races. We pair the basic approach with an inference algorithm that can infer view incompatibility specifications for many applications. We have ported four benchmark applications to use views: portions of Vuze, a BitTorrent client; Mailpuccino, a graphical email client; jphonelite, a VoIP softphone implementation; and TupleSoup, a database. Our experience indicates that views are easy to use, make implementing sophisticated locking schemes simple, and can help eliminate concurrency bugs. We have evaluated the performance of a view implementation of a red-black tree and found that views can significantly improve performance over that of the lock-based implementation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2055098550",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1101815.1101816",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "Leon J. Osterweil; Carlo Ghezzi; Jeff Kramer; Alexander L. Wolf",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4251279729",
    "type": "editorial"
  },
  {
    "title": "Architecting Internet of Things Systems with Blockchain",
    "doi": "https://doi.org/10.1145/3442412",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Wendy Yánez-Pazmiño; Rami Bahsoon; Yuqun Zhang; Rick Kazman",
    "corresponding_authors": "",
    "abstract": "Blockchain offers a distributed ledger to record data collected from Internet of Thing (IoT) devices as immutable and tamper-proof transactions and securely shared among authorized participants in a Peer-to-Peer (P2P) network. Despite the growing interest in using blockchain for securing IoT systems, there is a general lack of systematic research and comprehensive review of the design issues on the integration of blockchain and IoT from the software architecture perspective. This article presents a catalog of architectural tactics for the design of IoT systems supported by blockchain as a result of a Systematic Literature Review (SLR) on IoT and blockchain to extract the commonly reported quality attributes, design decisions, and relevant architectural tactics for the architectural design of this category of systems. Our findings are threefold:&lt;?brk?&gt; (i) identification of security, scalability, performance, and interoperability as the commonly reported quality attributes; (ii) a catalog of twelve architectural tactics for the design of IoT systems supported by blockchain; and (iii) gaps in research that include tradeoffs among quality attributes and identified tactics. These tactics might provide architects and designers with different options when searching for an optimal architectural design that meets the quality attributes of interest and constraints of a system.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3161434522",
    "type": "article"
  },
  {
    "title": "Specifying with Interface and Trait Abstractions in Abstract State Machines: A Controlled Experiment",
    "doi": "https://doi.org/10.1145/3450968",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Philipp Paulweber; Georg Simhandl; Uwe Zdun",
    "corresponding_authors": "",
    "abstract": "Abstract State Machine (ASM) theory is a well-known state-based formal method. As in other state-based formal methods, the proposed specification languages for ASMs still lack easy-to-comprehend abstractions to express structural and behavioral aspects of specifications. Our goal is to investigate object-oriented abstractions such as interfaces and traits for ASM-based specification languages. We report on a controlled experiment with 98 participants to study the specification efficiency and effectiveness in which participants needed to comprehend an informal specification as problem (stimulus) in form of a textual description and express a corresponding solution in form of a textual ASM specification using either interface or trait syntax extensions. The study was carried out with a completely randomized design and one alternative (interface or trait) per experimental group. The results indicate that specification effectiveness of the traits experiment group shows a better performance compared to the interfaces experiment group, but specification efficiency shows no statistically significant differences. To the best of our knowledge, this is the first empirical study studying the specification effectiveness and efficiency of object-oriented abstractions in the context of formal methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3184084070",
    "type": "article"
  },
  {
    "title": "ConE: A Concurrent Edit Detection Tool for Large-scale Software Development",
    "doi": "https://doi.org/10.1145/3478019",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Chandra Maddila; Nachiappan Nagappan; Christian Bird; Georgios Gousios; Arie van Deursen",
    "corresponding_authors": "",
    "abstract": "Modern, complex software systems are being continuously extended and adjusted. The developers responsible for this may come from different teams or organizations, and may be distributed over the world. This may make it difficult to keep track of what other developers are doing, which may result in multiple developers concurrently editing the same code areas. This, in turn, may lead to hard-to-merge changes or even merge conflicts, logical bugs that are difficult to detect, duplication of work, and wasted developer productivity. To address this, we explore the extent of this problem in the pull request based software development model. We study half a year of changes made to six large repositories in Microsoft in which at least 1,000 pull requests are created each month. We find that files concurrently edited in different pull requests are more likely to introduce bugs. Motivated by these findings, we design, implement, and deploy a service named ConE (Concurrent Edit Detector) that proactively detects pull requests containing concurrent edits, to help mitigate the problems caused by them. ConE has been designed to scale, and to minimize false alarms while still flagging relevant concurrently edited files. Key concepts of ConE include the detection of the Extent of Overlap between pull requests, and the identification of Rarely Concurrently Edited Files. To evaluate ConE, we report on its operational deployment on 234 repositories inside Microsoft. ConE assessed 26,000 pull requests and made 775 recommendations about conflicting changes, which were rated as useful in over 70% (554) of the cases. From interviews with 48 users we learned that they believed ConE would save time in conflict resolution and avoiding duplicate work, and that over 90% intend to keep using the service on a daily basis.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4221084810",
    "type": "article"
  },
  {
    "title": "Interacting process classes",
    "doi": "https://doi.org/10.1145/1538942.1538943",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "Ankit Goel; Abhik Roychoudhury; P. S. Thiagarajan",
    "corresponding_authors": "",
    "abstract": "Many reactive control systems consist of classes of active objects involving both intraclass interactions (i.e., objects belonging to the same class interacting with each other) and interclass interactions. Such reactive control systems appear in domains such as telecommunication, transportation and avionics. In this article, we propose a modeling and simulation technique for interacting process classes. Our modeling style uses standard notations to capture behavior. In particular, the control flow of a process class is captured by a labeled transition system, unit interactions between process objects are described as transactions , and the structural relations are captured via class diagrams. The key feature of our approach is that our execution semantics leads to an abstract simulation technique which involves (i) grouping together active objects into equivalence classes according their potential futures, and (ii) keeping track of the number of objects in an equivalence class rather than their identities. Our simulation strategy is both time and memory efficient and we demonstrate this on well-studied nontrivial examples of reactive systems. We also present a case study involving a weather-update controller from NASA to demonstrate the use of our simulator for debugging realistic designs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2140669545",
    "type": "article"
  },
  {
    "title": "Ensuring the Consistency of Adaptation through Inter- and Intra-Component Dependency Analysis",
    "doi": "https://doi.org/10.1145/3063385",
    "publication_date": "2017-01-31",
    "publication_year": 2017,
    "authors": "Alireza Sadeghi; Naeem Esfahani; Sam Malek",
    "corresponding_authors": "",
    "abstract": "Dynamic adaptation should not leave a software system in an inconsistent state, as it could lead to failure. Prior research has used inter-component dependency models of a system to determine a safe interval for the adaptation of its components, where the most important tradeoff is between disruption in the operations of the system and reachability of safe intervals. This article presents Savasana, which automatically analyzes a software system’s code to extract both inter- and intra-component dependencies. In this way, Savasana is able to obtain more fine-grained models compared to previous approaches. Savasana then uses the detailed models to find safe adaptation intervals that cannot be determined using techniques from prior research. This allows Savasana to achieve a better tradeoff between disruption and reachability. The article demonstrates how Savasana infers safe adaptation intervals for components of a software system under various use cases and conditions.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2613174189",
    "type": "article"
  },
  {
    "title": "Automated Identification of Uniqueness in JUnit Tests",
    "doi": "https://doi.org/10.1145/3533313",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Jianwei Wu; James Clause",
    "corresponding_authors": "",
    "abstract": "In the context of testing, descriptive test names are desirable because they document the purpose of tests and facilitate comprehension tasks during maintenance. Unfortunately, prior work has shown that tests often do not have descriptive names. To address this limitation, techniques have been developed to automatically generate descriptive names. However, they often generated names that are invalid or do not meet developer approval. To help address these limitations, we present a novel approach to extract the attributes of a given test that make it unique among its siblings. Because such attributes often serve as the basis for descriptive names, identifying them is an important first step towards improving test name generation approaches. To evaluate the approach, we created a prototype implementation for JUnit tests and compared its output with human judgment. The results of the evaluation demonstrate that the attributes identified by the approach are consistent with human judgment and are likely to be useful for future name generation techniques.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293066133",
    "type": "article"
  },
  {
    "title": "SLR: From Saltzer and Schroeder to 2021…47 Years of Research on the Development and Validation of Security API Recommendations",
    "doi": "https://doi.org/10.1145/3561383",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Nikhil Patnaik; Andrew C. Dwyer; Joseph Hallett; Awais Rashid",
    "corresponding_authors": "",
    "abstract": "Producing secure software is challenging. The poor usability of security Application Programming Interfaces (APIs) makes this even harder. Many recommendations have been proposed to support developers by improving the usability of cryptography libraries—rooted in wider best practice guidance in software engineering and API design. In this SLR, we systematize knowledge regarding these recommendations. We identify and analyze 65 papers, offering 883 recommendations. Through thematic analysis, we identify seven core ways to improve usability of APIs. Most of the recommendations focus on helping API developers to construct and structure their code and make it more usable and easier for programmers to understand . There is less focus, however, on documentation , writing requirements , code quality assessment , and the impact of organizational software development practices . By tracing and analyzing paper ancestry, we map how this knowledge becomes validated and translated over time. We find that very few API usability recommendations are empirically validated, and that recommendations specific to usable security APIs lag even further behind.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4296131189",
    "type": "article"
  },
  {
    "title": "DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data",
    "doi": "https://doi.org/10.1145/3569936",
    "publication_date": "2022-10-29",
    "publication_year": 2022,
    "authors": "Xueling Zhang; John Heaps; Rocky Slavin; Jianwei Niu; Travis D. Breaux; Xiaoyin Wang",
    "corresponding_authors": "",
    "abstract": "Mobile apps are widely used and often process users’ sensitive data. Many taint analysis tools have been applied to analyze sensitive information flows and report data leaks in apps. These tools require a list of sources (where sensitive data is accessed) as input, and researchers have constructed such lists within the Android platform by identifying Android API methods that allow access to sensitive data. However, app developers may also define methods or use third-party library’s methods for accessing data. It is difficult to collect such source methods, because they are unique to the apps, and there are a large number of third-party libraries available on the market that evolve over time. To address this problem, we propose DAISY, a Dynamic-Analysis-Induced Source discoverY approach for identifying methods that return sensitive information from apps and third-party libraries. Trained on an automatically labeled dataset of methods and their calling context, DAISY identifies sensitive methods in unseen apps. We evaluated DAISY on real-world apps, and the results show that DAISY can achieve an overall precision of 77.9% when reporting the most confident results. Most of the identified sources and leaks cannot be detected by existing technologies.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4307811449",
    "type": "article"
  },
  {
    "title": "Software reuse for scientific computing through program generation",
    "doi": "https://doi.org/10.1145/1061254.1061257",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "Martin Erwig; Zhe Fu",
    "corresponding_authors": "",
    "abstract": "We present a program-generation approach to address a software-reuse challenge in the area of scientific computing. More specifically, we describe the design of a program generator for the specification of subroutines that can be generic in the dimensions of arrays, parameter lists, and called subroutines. We describe the application of that approach to a real-world problem in scientific computing which requires the generic description of inverse ocean modeling tools. In addition to a compiler that can transform generic specifications into efficient Fortran code for models, we have also developed a type system that can identify possible errors already in the specifications. This type system is important for the acceptance of the program generator among scientists because it prevents a large class of errors in the generated code.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2029514141",
    "type": "article"
  },
  {
    "title": "Data Model Property Inference, Verification, and Repair for Web Applications",
    "doi": "https://doi.org/10.1145/2699691",
    "publication_date": "2015-09-02",
    "publication_year": 2015,
    "authors": "Jaideep Nijjar; Ivan Bocić; Tevfik Bultan",
    "corresponding_authors": "",
    "abstract": "Most software systems nowadays are Web-based applications that are deployed over compute clouds using a three-tier architecture, where the persistent data for the application is stored in a backend datastore and is accessed and modified by the server-side code based on the user interactions at the client-side. The data model forms the foundation of these three tiers, and identifies the sets of objects (object classes) and the relations among them (associations among object classes) stored by the application. In this article, we present a set of property patterns to specify properties of a data model, as well as several heuristics for automatically inferring them. We show that the specified or inferred data model properties can be automatically verified using bounded and unbounded verification techniques. For the properties that fail, we present techniques that generate fixes to the data model that establish the failing properties. We implemented this approach for Web applications built using the Ruby on Rails framework and applied it to ten open source applications. Our experimental results demonstrate that our approach is effective in automatically identifying and fixing errors in data models of real-world web applications.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2003334194",
    "type": "article"
  },
  {
    "title": "Control Explicit--Data Symbolic Model Checking",
    "doi": "https://doi.org/10.1145/2888393",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Petr Bauch; Vojtěch Havel; Jǐŕı Barnat",
    "corresponding_authors": "",
    "abstract": "Automatic verification of programs and computer systems with data nondeterminism (e.g., reading from user input) represents a significant and well-motivated challenge. The case of parallel programs is especially difficult, because then also the control flow nontrivially complicates the verification process. We apply the techniques of explicit-state model checking to account for the control aspects of a program to be verified and use set-based reduction of the data flow, thus handling the two sources of nondeterminism separately. We build the theory of set-based reduction using first-order formulae in the bit-vector theory to encode the sets of variable evaluations representing program data. These representations are tested for emptiness and equality (state matching) during the verification, and we harness modern satisfiability modulo theory solvers to implement these tests. We design two methods of implementing the state matching, one using quantifiers and one that is quantifier-free, and we provide both analytical and experimental comparisons. Further experiments evaluate the efficiency of the set-based reduction method, showing the classical, explicit approach to fail to scale with the size of data domains. Finally, we propose and evaluate two heuristics to decrease the number of expensive satisfiability queries, together yielding a 10-fold speedup.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2318396582",
    "type": "article"
  },
  {
    "title": "A Stack Memory Abstraction and Symbolic Analysis Framework for Executables",
    "doi": "https://doi.org/10.1145/2897511",
    "publication_date": "2016-04-27",
    "publication_year": 2016,
    "authors": "Kapil Anand; Khaled ElWazeer; Aparna Kotha; Matthew Smithson; Rajeev Barua; Angelos D. Keromytis",
    "corresponding_authors": "",
    "abstract": "This article makes three contributions regarding reverse-engineering of executables. First, techniques are presented for recovering a precise and correct stack-memory model in executables while addressing executable-specific challenges such as indirect control transfers. Next, the enhanced memory model is employed to define a novel symbolic analysis framework for executables that can perform the same types of program analyses as source-level tools. Third, a demand-driven framework is presented to enhance the scalability of the symbolic analysis framework. Existing symbolic analysis frameworks for executables fail to simultaneously maintain the properties of correct representation, a precise stack-memory model, and scalability. Furthermore, they ignore memory-allocated variables when defining symbolic analysis mechanisms. Our methods do not use symbolic, relocation or debug information, which are usually absent in deployed binaries. We describe our framework, highlighting the novel intellectual contributions of our approach and demonstrating its efficacy and robustness. Our techniques improve the precision of existing stack-memory models by 25%, enhance scalability of our basic symbolic analysis mechanism by 10×, and successfully uncovers five previously undiscovered information-flow vulnerabilities in several widely used programs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2342473256",
    "type": "article"
  },
  {
    "title": "A visual execution model for Ada tasking",
    "doi": "https://doi.org/10.1145/158431.158432",
    "publication_date": "1993-10-01",
    "publication_year": 1993,
    "authors": "Laura K. Dillon",
    "corresponding_authors": "Laura K. Dillon",
    "abstract": "A visual execution model for Ada tasking can help programmers attain a deeper understanding of the tasking semantics. It can illustrate subtleties in semantic definitions that are not apparent in natural language design. We describe a contour model of Ada tasking that depicts asynchronous tasks (threads of control), relationships between the environments in which tasks execute, and the manner in which tasks interact. The use of this high-level execution model makes it possible to see what happens during execution of a program. The paper provides an introduction to the contour model of Ada tasking and demonstrates its use.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2081007799",
    "type": "article"
  },
  {
    "title": "Double iterative framework for flow-sensitive interprocedural data flow analysis",
    "doi": "https://doi.org/10.1145/174634.174635",
    "publication_date": "1994-01-02",
    "publication_year": 1994,
    "authors": "István Forgács",
    "corresponding_authors": "István Forgács",
    "abstract": "Compiler optimization, parallel processing, data flow testing, and symbolic debugging can benefit from interprocedural data flow analysis. However, the live, reaching definition, and most summary data flow problems are theoretically intractable in the interprocedural case. A method is presented that reduces the exponential time bound with the help of an algorithm that solves the problem in polynomial time. Either the resulting sets contain precise results or the missing (or additional) results do not cause any problems during their use. We also introduce the double iterative framework, where one procedure is processed at a time. The results of the intraprocedural analysis of procedures then propagates along the edges of the call multi-graph. In this way the intra and interprocedural analyses are executed alternately until there is no change in any result set. This method can be applied to any known interprocedural data flow problem. Here the algorithms for the kill, live variables, and reaching definitions problems are presented. Besides for precision, the algorithms can be used for very large programs, and since inter and intraprocedural analyses can be optimized separately, the method is fast as well.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1991882640",
    "type": "article"
  },
  {
    "title": "Software Architectural Migration",
    "doi": "https://doi.org/10.1145/3461011",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Nacha Chondamrongkul; Jing Sun; Ian Warren",
    "corresponding_authors": "",
    "abstract": "Software architectural designs are usually changed over time to support emerging technologies and to adhere to new principles. Architectural migration is an important activity that helps to transform the architectural styles applied during a system’s design with the result of modernising the system. If not performed correctly, this process could lead to potential system failures. This article presents an automated approach to refactoring architectural design and to planning the evolution process. With our solution, the architectural design can be refactored, ensuring that system functionality is preserved. Furthermore, the architectural migration process allows the system to be safely and incrementally transformed. We have evaluated our approach with five real-world software applications. The results prove the effectiveness of our approach and identify factors that impact the performance of architectural verification and migration planning. An interesting finding is that planning algorithms generate migration plans that differ in term of their relative efficiency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3185076973",
    "type": "article"
  },
  {
    "title": "Speeding Up Data Manipulation Tasks with Alternative Implementations",
    "doi": "https://doi.org/10.1145/3456873",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Yida Tao; Shan Tang; Yepang Liu; Zhiwu Xu; Shengchao Qin",
    "corresponding_authors": "",
    "abstract": "As data volume and complexity grow at an unprecedented rate, the performance of data manipulation programs is becoming a major concern for developers. In this article, we study how alternative API choices could improve data manipulation performance while preserving task-specific input/output equivalence. We propose a lightweight approach that leverages the comparative structures in Q&amp;A sites to extracting alternative implementations. On a large dataset of Stack Overflow posts, our approach extracts 5,080 pairs of alternative implementations that invoke different data manipulation APIs to solve the same tasks, with an accuracy of 86%. Experiments show that for 15% of the extracted pairs, the faster implementation achieved &gt;10x speedup over its slower alternative. We also characterize 68 recurring alternative API pairs from the extraction results to understand the type of APIs that can be used alternatively. To put these findings into practice, we implement a tool, AlterApi7 , to automatically optimize real-world data manipulation programs. In the 1,267 optimization attempts on the Kaggle dataset, 76% achieved desirable performance improvements with up to orders-of-magnitude speedup. Finally, we discuss notable challenges of using alternative APIs for optimizing data manipulation programs. We hope that our study offers a new perspective on API recommendation and automatic performance optimization.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3186728919",
    "type": "article"
  },
  {
    "title": "From business process models to process-oriented software",
    "doi": null,
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Chun Ouyang; Wil M. P. van der Aalst; Marlon Dumas; Arthur H. M. ter Hofstede",
    "corresponding_authors": "",
    "abstract": "Several methods for enterprise systems analysis rely on flow-oriented representations of business operations, otherwise known as business process models. The Business Process Modeling Notation (BPMN) is a standard for capturing such models. BPMN models facilitate communication between domain experts and analysts and provide input to software development projects. Meanwhile, there is an emergence of methods for enterprise software development that rely on detailed process definitions that are executed by process engines. These process definitions refine their counterpart BPMN models by introducing data manipulation, application binding, and other implementation details. The de facto standard for defining executable processes is the Business Process Execution Language (BPEL). Accordingly, a standards-based method for developing process-oriented systems is to start with BPMN models and to translate these models into BPEL definitions for subsequent refinement. However, instrumenting this method is challenging because BPMN models and BPEL definitions are structurally very different. Existing techniques for translating BPMN to BPEL only work for limited classes of BPMN models. This article proposes a translation technique that does not impose structural restrictions on the source BPMN model. At the same time, the technique emphasizes the generation of readable (block-structured) BPEL code. An empirical evaluation conducted over a large collection of process models shows that the resulting BPEL definitions are largely block-structured. Beyond its direct relevance in the context of BPMN and BPEL, the technique presented in this article addresses issues that arise when translating from graph-oriented to block-structure flow definition languages.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1505426680",
    "type": "article"
  },
  {
    "title": "Task dependence and termination in Ada",
    "doi": "https://doi.org/10.1145/237432.237459",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Laura K. Dillon",
    "corresponding_authors": "Laura K. Dillon",
    "abstract": "This article analyzes the semantics of task dependence and termination in Ada. We use a contour model of Ada tasking in examining the implications of and possible motivation for the rules that determine when procedures and tasks terminate during execution of an Ada program. The termination rules prevent the data that belong to run-time instances of scope units from being deallocated prematurely, but they are unnecessarily conservative in this regard. For task instances that are created by invoking a storage allocator, we show that the conservative termination policy allows heap storage to be managed more efficiently than a less conservative policy. The article also examines the manner in which the termination rules affect the synchronization of concurrent tasks. Master-slave and client-server applications are considered. We show that the rules for distributed termination of concurrent tasks guarantee that a task terminates only if it can no longer affect the outcome of an execution. The article is meant to give programmers a better understanding of Ada tasking and to help language designers assess the strengths and weaknesses of the termination model.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2033595278",
    "type": "article"
  },
  {
    "title": "Temporal abstract classes and virtual temporal specifications for real-time systems",
    "doi": "https://doi.org/10.1145/567793.567794",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Alexander Pons",
    "corresponding_authors": "Alexander Pons",
    "abstract": "The design and development of real-time systems is often a difficult and time-consuming task. System realization has become increasingly difficult due to the proliferation of larger and more complex applications. To offset some of these difficulties, real-time developers have turned to object-oriented methodology. The success of object-oriented concepts in the development of non-real-time programs motivates the relevance of these concepts to achieve similar gains from encapsulation and code reuse in the real-time domain. This article presents an approach of integrating real-time constraint specifications within the constructs of an object-oriented language, affording these constraints a status equivalent to other language elements. This has led to the definition of such novel concepts as temporal abstract classes, virtual temporal constraints, and temporal specification inheritance, which extends inheritance mechanisms to accommodate real-time constraint specifications. These extensions provide real-time developers with the ability to manage and maintain the temporal behavior of a real-time program in a comparable manner to its functional behavior.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1963727635",
    "type": "article"
  },
  {
    "title": "Isolation Modeling and Analysis Based on Mobility",
    "doi": "https://doi.org/10.1145/3306606",
    "publication_date": "2019-02-26",
    "publication_year": 2019,
    "authors": "Jianmin Jiang; Huibiao Zhu; Qin Li; Yongxin Zhao; Hong Zhong; Shi Zhang; Ping Gong",
    "corresponding_authors": "",
    "abstract": "In a mobile system, mobility refers to a change in position of a mobile object with respect to time and its reference point, whereas isolation means the isolation relationship between mobile objects under some scheduling policies. Inspired by event-based formal models and the ambient calculus, we first propose the two types of special events, entering and exiting an ambient, as movement events to model and analyze mobility. Based on mobility, we then introduce the notion of the isolation of mobile objects for ambients. To ensure the isolation, a priority policy needs to be used to schedule the movement of mobile objects. However, traditional scheduling policies focus on task scheduling and depend on the strong hypothesis: The scheduled tasks are independent—that is, the scheduled tasks do not affect each other. In a practical mobile system, mobile objects and ambients interact with each other. It is difficult to separate a mobile system into independent tasks. We finally present an automatic approach for generating a priority scheduling policy without considering the preceding assumption. The approach can guarantee the isolation of the mobile objects for ambients in a mobile system. Experiments demonstrate these results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2917528208",
    "type": "article"
  },
  {
    "title": "Editorial from the Incoming Editor-in-Chief",
    "doi": "https://doi.org/10.1145/3301290",
    "publication_date": "2019-01-31",
    "publication_year": 2019,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2941334815",
    "type": "article"
  },
  {
    "title": "BiRD: Race Detection in Software Binaries under Relaxed Memory Models",
    "doi": "https://doi.org/10.1145/3498538",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Ridhi Jain; Rahul Purandare; Subodh Sharma",
    "corresponding_authors": "",
    "abstract": "Instruction reordering and interleavings in program execution under relaxed memory semantics result in non-intuitive behaviors, making it difficult to provide assurances about program correctness. Studies have shown that up to 90% of the concurrency bugs reported by state-of-the-art static analyzers are false alarms. As a result, filtering false alarms and detecting real concurrency bugs is a challenging problem. Unsurprisingly, this problem has attracted the interest of the research community over the past few decades. Nonetheless, many of the existing techniques rely on analyzing source code, rarely consider the effects introduced by compilers, and assume a sequentially consistent memory model. In a practical setting, however, developers often do not have access to the source code, and even commodity architectures such as x86 and ARM are not sequentially consistent. In this work, we present B i rd , a prototype tool, to dynamically detect harmful data races in x86 binaries under relaxed memory models, TSO and PSO. B i rd employs source-DPOR to explore all distinct feasible interleavings for a multithreaded application. Our evaluation of B i rd on 42 publicly available benchmarks and its comparison with the state-of-the-art tools indicate B i rd ’s potential in effectively detecting data races in software binaries.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4210626567",
    "type": "article"
  },
  {
    "title": "Combatting Energy Issues for Mobile Applications",
    "doi": "https://doi.org/10.1145/3527851",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Xueliang Li; Junyang Chen; Yepang Liu; Kaishun Wu; John P. Gallagher",
    "corresponding_authors": "",
    "abstract": "Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g. pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4225163840",
    "type": "article"
  },
  {
    "title": "Toward More Efficient Statistical Debugging with Abstraction Refinement",
    "doi": "https://doi.org/10.1145/3544790",
    "publication_date": "2022-06-23",
    "publication_year": 2022,
    "authors": "Zhiqiang Zuo; Xintao Niu; Siyi Zhang; Lu Fang; Siau Cheng Khoo; Shan Lu; C. P. Sun; Guoqing Xu",
    "corresponding_authors": "",
    "abstract": "Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. This article presents a systematic abstraction refinement-based pruning technique for statistical debugging. Our technique only needs to instrument and analyze the code partially. While guided by a mathematically rigorous analysis, our technique is guaranteed to produce the same debugging results as an exhaustive analysis in deterministic settings. With the help of the effective and safe pruning, our technique greatly saves the cost of failure diagnosis without sacrificing any debugging capability. We apply this technique to two different statistical debugging scenarios: in-house and production-run statistical debugging. The comprehensive evaluations validate that our technique can significantly improve the efficiency of statistical debugging in both scenarios, while without jeopardizing the debugging capability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4283323653",
    "type": "article"
  },
  {
    "title": "Patching Locking Bugs Statically with Crayons",
    "doi": "https://doi.org/10.1145/3548684",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Juan A. Cruz-Carlón; Mahsa Varshosaz; Claire Le Goues; Andrzej Wąsowski",
    "corresponding_authors": "",
    "abstract": "The Linux Kernel is a world-class operating system controlling most of our computing infrastructure: mobile devices, Internet routers and services, and most of the supercomputers. Linux is also an example of low-level software with no comprehensive regression test suite (for good reasons). The kernel's tremendous societal importance imposes strict stability and correctness requirements. These properties make Linux a challenging and relevant target for static automated program repair (APR). Over the last decade, a significant progress has been made in dynamic APR. However, dynamic APR techniques do not translate naturally to systems without tests. We present a static APR technique addressing sequential locking API misuse bugs in the Linux Kernel. We attack the key challenge of static APR, namely the lack of detailed program specification, by combining static analysis with machine learning to complement the information presented by the static analyzer. In experiments on historical real-world bugs in the kernel, we were able to automatically re-produce or propose equivalent patches in 85% of the human made patches, and automatically rank them among the top three candidates for 64% of the cases and among the top five for 74%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4292595668",
    "type": "article"
  },
  {
    "title": "How the Quality of Maintenance Tasks is Affected by Criteria for Selecting Engineers for Collaboration",
    "doi": "https://doi.org/10.1145/3561384",
    "publication_date": "2022-09-04",
    "publication_year": 2022,
    "authors": "Francisca Pérez; Raúl Lapeña; Ana C. Marcén; Carlos Cetina",
    "corresponding_authors": "",
    "abstract": "In industry, software projects might span over decades, with many engineers joining or leaving the company over time. In these circumstances, no single engineer has all of the knowledge when maintenance tasks such as Traceability Link Recovery (TLR), Bug Localization (BL), and Feature Location (FL) are performed. Thus, collaboration has the potential to boost the quality of maintenance tasks since the solution advanced by one engineer might be enhanced with contributions from other engineers. However, assembling a team of software engineers to collaborate may not be as intuitive as we might think. In the context of a worldwide industrial supplier of railway solutions, this work evaluates how the quality of TLR, BL, and FL is affected by the criteria for selecting engineers for collaboration. The criteria for collaboration are based on engineers’ profile information to select the set of search queries that are involved in the maintenance task. Collaboration is achieved by applying automatic query reformulation, and the location relies on an evolutionary algorithm. Our work uncovers how software engineers who might be seen as not being relevant in the collaboration can lead to significantly better results. A focus group confirmed the relevance of the findings.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4294558874",
    "type": "article"
  },
  {
    "title": "Seeing the Whole Elephant: Systematically Understanding and Uncovering Evaluation Biases in Automated Program Repair",
    "doi": "https://doi.org/10.1145/3561382",
    "publication_date": "2022-09-04",
    "publication_year": 2022,
    "authors": "Deheng Yang; Yan Lei; Xiaoguang Mao; Yuhua Qi; Xin Yi",
    "corresponding_authors": "",
    "abstract": "Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques. In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig , based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4294558964",
    "type": "article"
  },
  {
    "title": "Fuzzing Configurations of Program Options - RCR Report",
    "doi": "https://doi.org/10.1145/3580601",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "Zenong Zhang; George Klees; Eric Wang; Michael Hicks; Shiyi Wei",
    "corresponding_authors": "",
    "abstract": "This artifact contains the source code and instructions to reproduce the evaluation results of the article “Fuzzing Configurations of Program Options.” The source code includes the configuration grammars for six target programs, the scripts to generate configuration stubs, and the scripts to post-process fuzzing results. The README of the artifact includes the steps to prepare the experimental environment on a clean Ubuntu machine and step-by-step commands to reproduce the evaluation experiments. A VirtualBox image with ConfigFuzz properly set up is also included.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4318618535",
    "type": "article"
  },
  {
    "title": "Dissecting American Fuzzy Lop – A FuzzBench Evaluation - RCR Report",
    "doi": "https://doi.org/10.1145/3580600",
    "publication_date": "2023-02-27",
    "publication_year": 2023,
    "authors": "Andrea Fioraldi; Alessandro Mantovani; Dominik Maier; Davide Balzarotti",
    "corresponding_authors": "",
    "abstract": "This report describes the artifacts of the “Dissecting American Fuzzy Lop – A FuzzBench Evaluation” paper. The artifacts are available online at https://github.com/eurecom-s3/dissecting_afl and archived at https://doi.org/10.6084/m9.figshare.21401280 . American Fuzzy Lop (AFL) consists of the produced code, the setup to run the experiments in FuzzBench, and the generated reports. We claim the Functional badge as the patches to AFL are easy to enable and the experiments are easy to run thanks to the FuzzBench service, but the evaluations are self-contained and the modifications to AFL are as is. For the purpose of reproducing the experiments, no particular skills are needed as the process is straightforward and described in https://google.github.io/fuzzbench/getting-started/adding-a-new-fuzzer/#requesting-an-experiment .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4322504467",
    "type": "article"
  },
  {
    "title": "FQN Inference in Partial Code by Prompt-tuned Language Model of Code",
    "doi": "https://doi.org/10.1145/3617174",
    "publication_date": "2023-08-24",
    "publication_year": 2023,
    "authors": "Qing Huang; Zhiqiang Yuan; Zhenchang Xing; Xin Peng; Xiwei Xu; Qinghua Lu",
    "corresponding_authors": "",
    "abstract": "Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this article, we propose using a p rompt-tuned c o de m asked language mod e l (MLM) as a neural knowledge base for type inference, called POME, which is lightweight and has minimal requirements on code compilation. Unlike the existing symbol name and context matching for type inference, POME infers the FQNs syntax and usage knowledge encapsulated in prompt-tuned code MLM through a colze-style fill-in-blank strategy. POME is integrated as a plug-in into web and integrated development environments (IDE) to assist developers in inferring FQNs in the real world. We systematically evaluate POME on a large amount of source code from GitHub and Stack Overflow, and explore its generalization and hybrid capability. The results validate the effectiveness of the POME design and its applicability for partial code type inference, and they can be easily extended to different programming languages (PL). POME can also be used to generate a PL-hybrid type inference model for providing a one-for-all solution. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386136163",
    "type": "article"
  },
  {
    "title": "Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks - RCR Report",
    "doi": "https://doi.org/10.1145/3617594",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Michael Weiß; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "This is the Replicated Computational Results (RCR) Report for our TOSEM paper “Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks”, where we propose a novel client-server architecture allowing to leverage the high accuracy of huge neural networks running on remote servers while reducing the economical and latency costs typically coming from using such models. As part of this RCR, we provide a replication package, which allows the full replication of all our results and is specifically designed to facilitate reuse.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386195734",
    "type": "article"
  },
  {
    "title": "Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3617593",
    "publication_date": "2023-08-28",
    "publication_year": 2023,
    "authors": "Michael Weiß; Paolo Tonella",
    "corresponding_authors": "",
    "abstract": "Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of AI tasks. Often consisting of hundreds of million, if not hundreds of billion, parameters, these DNNs are too large to be deployed to or efficiently run on resource-constrained devices such as mobile phones or Internet of Things microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this article, we propose BiSupervised , a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs while only marginally impacting the overall system accuracy. Our architecture furthermore foresees a second supervisor to monitor the remote predictions and identify inputs for which not even these can be trusted, allowing to raise an exception or run a fallback strategy instead. We evaluate the cost savings and the ability to detect incorrectly predicted inputs on four diverse case studies: IMDb movie review sentiment classification, GitHub issue triaging, ImageNet image classification, and SQuADv2 free-text question answering. In all four case studies, we find that BiSupervised allows to reduce cost by at least 30% while maintaining similar system-level prediction performance. In two case studies (IMDb and SQuADv2), we find that BiSupervised even achieves a higher system-level accuracy, at reduced cost, compared to a remote-only model. Furthermore, measurements taken on our setup indicate a large potential of BiSupervised to reduce average prediction latency.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386224022",
    "type": "article"
  },
  {
    "title": "Compositional Verification of First-Order Masking Countermeasures against Power Side-Channel Attacks",
    "doi": "https://doi.org/10.1145/3635707",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Pengfei Gao; Fu Song; Taolue Chen",
    "corresponding_authors": "",
    "abstract": "Power side-channel attacks allow an adversary to efficiently and effectively steal secret information (e.g., keys) by exploiting the correlation between secret data and runtime power consumption, hence posing a serious threat to software security, particularly cryptographic implementations. Masking is a commonly used countermeasure against such attacks, which breaks the statistical dependence between secret data and side-channel leaks via randomization. In a nutshell, a variable is represented by a vector of shares armed with random variables, called masking encoding , on which cryptographic computations are performed. While compositional verification for the security of masked cryptographic implementations has received much attention because of its high efficiency, existing compositional approaches either use implicitly fixed pre-conditions that may not be fulfilled by state-of-the-art efficient implementations, or require user-provided hard-coded pre-conditions that are time consuming and highly non-trivial, even for an expert. In this article, we tackle the compositional verification problem of first-order masking countermeasures, where first-order means that the adversary is allowed to access only one intermediate computation result. Following the literature, we consider countermeasures given as gadgets, which are special procedures whose inputs are masking encodings of variables. We introduce a new security notion parameterized by an explicit pre-condition for each gadget, as well as composition rules for reasoning about masking countermeasures against power side-channel attacks. We propose accompanying efficient algorithms to automatically infer proper pre-conditions, based on which our new compositional approach can efficiently and automatically prove security for masked implementations. We implement our approaches as a tool MaskCV and conduct experiments on publicly available masked cryptographic implementations including 10 different full AES implementations. The experimental results confirm the effectiveness and efficiency of our approach.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389337860",
    "type": "article"
  },
  {
    "title": "Estimating Uncertainty in Labeled Changes by SZZ Tools on Just-In-Time Defect Prediction",
    "doi": "https://doi.org/10.1145/3637226",
    "publication_date": "2023-12-11",
    "publication_year": 2023,
    "authors": "Shikai Guo; Dongmin Li; Lin Huang; Sijia Lv; Rong Chen; Hui Li; Xiaochen Li; He Jiang",
    "corresponding_authors": "",
    "abstract": "The aim of Just-In-Time (JIT) defect prediction is to predict software changes that are prone to defects in a project in a timely manner, thereby improving the efficiency of software development and ensuring software quality. Identifying changes that introduce bugs is a critical task in just-in-time defect prediction, and researchers have introduced the SZZ approach and its variants to label these changes. However, it has been shown that different SZZ algorithms introduce noise to the dataset to a certain extent, which may reduce the predictive performance of the model. To address this limitation, we propose the Confident Learning Imbalance (CLI) model. The model identifies and excludes samples whose labels may be corrupted by estimating the joint distribution of noisy labels and true labels, and mitigates the impact of noisy data on the performance of the prediction model. The CLI consists of two components: identifying noisy data (Confident Learning Component) and generating a predicted probability matrix for imbalanced data (Imbalanced Data Probabilistic Prediction Component). The IDPP component generates precise predicted probabilities for each instance in the training set, while the CL component uses the generated predicted probability matrix and noise labels to clean up the noise and build a classification model. We evaluate the performance of our model through extensive experiments on a total of 126,526 changes from ten Apache open source projects, and the results show that our model outperforms the baseline methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389572555",
    "type": "article"
  },
  {
    "title": "PACE: A Program Analysis Framework for Continuous Performance Prediction",
    "doi": "https://doi.org/10.1145/3637230",
    "publication_date": "2023-12-14",
    "publication_year": 2023,
    "authors": "Chidera Biringa; Gökhan Kul",
    "corresponding_authors": "",
    "abstract": "Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This article presents PACE , a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75% on neural-represented code stylometry features.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389735314",
    "type": "article"
  },
  {
    "title": "Expressive and Extensible Parameter Passing for Distributed Object Systems",
    "doi": "https://doi.org/10.1145/2063239.2063242",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Eli Tilevich; Sriram Gopal",
    "corresponding_authors": "",
    "abstract": "In modern distributed object systems, reference parameters to a remote method are passed according to their runtime type. This design choice limits the expressiveness, readability, and maintainability of distributed applications. Further, to extend the built-in set of parameter passing semantics of a distributed object system, the programmer has to understand and modify the underlying middleware implementation. To address these design shortcomings, this article presents (i) a declarative and extensible approach to remote parameter passing that decouples parameter passing semantics from parameter types, and (ii) a plugin-based framework, DeXteR , which enables the programmer to extend the built-in set of remote parameter passing semantics, without having to understand or modify the underlying middleware implementation. DeXteR treats remote parameter passing as a distributed cross-cutting concern and uses aspect-oriented and generative techniques. DeXteR enables the implementation of different parameter passing semantics as reusable application-level plugins, applicable to application, system, and third-party library classes. The expressiveness, flexibility, and extensibility of the approach is validated by adding several nontrivial remote parameter passing semantics (i.e., copy-restore, lazy, streaming) to Java Remote Method Invocation (RMI) as DeXteR plugins.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2028908992",
    "type": "article"
  },
  {
    "title": "Editorial—looking forward",
    "doi": "https://doi.org/10.1145/2430536.2431202",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "David S. Rosenblum",
    "corresponding_authors": "David S. Rosenblum",
    "abstract": "editorial Free AccessEditorial—looking forward Editor: David S. Rosenblum View Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 22Issue 1February 2013 Article No.: 2pp 1–3https://doi.org/10.1145/2430536.2431202Published:04 March 2013Publication History 0citation273DownloadsMetricsTotal Citations0Total Downloads273Last 12 Months14Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2064748737",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2430545",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Multicore technology is making concurrent programs increasingly pervasive. Unfortunately, it is difficult to deliver reliable concurrent programs, because of the huge and nondeterministic interleaving space. In reality, without the resources to ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4241955020",
    "type": "paratext"
  },
  {
    "title": "Early Evaluation of Implementation Alternatives of Composite Data Structures Toward Maintainability",
    "doi": "https://doi.org/10.1145/3132731",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Chris Karanikolas; Grigoris Dimitroulakos; Κωνσταντίνος Μασσέλος",
    "corresponding_authors": "",
    "abstract": "Selecting between different design options is a crucial decision for object-oriented software developers that affects code quality characteristics. Conventionally developers use their experience to make such decisions, which leads to suboptimal results regarding code quality. In this article, a formal model for providing early estimates of quality metrics of object-oriented software implementation alternatives is proposed. The model supports software developers in making fast decisions in a systematic way early during the design phase to achieve improved code characteristics. The approach employs a comparison model related to the application of the Visitor design pattern and inheritance-based implementation on structures following the Composite design pattern. The model captures maintainability as a metric of software quality and provides precise assessments of the quality of each implementation alternative. Furthermore, the model introduces the structural maintenance cost metric based on which the progressive analysis of the maintenance process is introduced. The proposed approach has been applied to several test cases for different relevant quality metrics. The results prove that the proposed model delivers accurate estimations. Thus, the proposed methodology can be used for comparing different implementation alternatives against various measures and quality factors before code development, leading to reduced effort and cost for software maintenance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2762682750",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1767751",
    "publication_date": "2010-06-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4229978614",
    "type": "paratext"
  },
  {
    "title": "Enabledness-based Testing of Object Protocols",
    "doi": "https://doi.org/10.1145/3415153",
    "publication_date": "2021-01-03",
    "publication_year": 2021,
    "authors": "Javier Godoy; Juan Pablo Galeotti; Diego Garbervetsky; Sebastián Uchitel",
    "corresponding_authors": "",
    "abstract": "A significant proportion of classes in modern software introduce or use object protocols, prescriptions on the temporal orderings of method calls on objects. This article studies search-based test generation techniques that aim to exploit a particular abstraction of object protocols (enabledness preserving abstractions (EPAs)) to find failures. We define coverage criteria over an extension of EPAs that includes abnormal method termination and define a search-based test case generation technique aimed at achieving high coverage. Results suggest that the proposed case generation technique with a fitness function that aims at combined structural and extended EPA coverage can provide better failure-detection capabilities not only for protocol failures but also for general failures when compared to random testing and search-based test generation for standard structural coverage.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3120014478",
    "type": "article"
  },
  {
    "title": "Analyzing Uncertainty in Release Planning: A Method and Experiment for Fixed-Date Release Cycles",
    "doi": "https://doi.org/10.1145/3490487",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Olawole Oni; Emmanuel Letier",
    "corresponding_authors": "",
    "abstract": "Release planning—deciding what features to implement in upcoming releases of a software system—is a critical activity in iterative software development. Many release planning methods exist, but most ignore the inevitable uncertainty in estimating software development effort and business value. The article’s objective is to study whether analyzing uncertainty during release planning generates better release plans than if uncertainty is ignored. To study this question, we have developed a novel release planning method under uncertainty, called BEARS, that models uncertainty using Bayesian probability distributions and recommends release plans that maximize expected net present value and expected punctuality. We then compare release plans recommended by BEARS to those recommended by methods that ignore uncertainty on 32 release planning problems. The experiment shows that BEARS recommends release plans with higher expected net present value and expected punctuality than methods that ignore uncertainty, thereby indicating the harmful effects of ignoring uncertainty during release planning. These results highlight the importance of eliciting and analyzing uncertainty in software effort and value estimations and call for increased research in these areas.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200111838",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1243987",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Fault-based testing of software checks the software implementation for a set of faults. Two previous papers on fault-based testing [Kuhn 1999; Tsuchiya and Kikuno 2002] represent the required behavior of the software as a Boolean specification ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4250546046",
    "type": "paratext"
  },
  {
    "title": "Understanding Developers Well-being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic—RCR Report",
    "doi": "https://doi.org/10.1145/3640338",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Daniel Russo; Paul H. P. Hanel; Niels van Berkel",
    "corresponding_authors": "",
    "abstract": "The artifact accompanying the paper \"Understanding Developers Well-Being and Productivity: A 2-year Longitudinal Analysis during the COVID-19 Pandemic\"provides a comprehensive set of tools, data, and scripts that were utilized in the longitudinal study. Spanning 24 months, from April 2020 to April 2022, the study delves into the shifts in well-being, productivity, social contacts, needs, and several other variables of software engineers during the COVID-19 pandemic. The artifact facilitates the reproduction of the study's findings, offering a deeper insight into the systematic changes observed in various variables, such as well-being, quality of social contacts, and emotional loneliness. By providing access to the evidence-generating mechanisms and the generated data, the artifact ensures transparency and reproducibility and allows researchers to use our rich dataset to test their own research question. This Replicated Computational Results report aims to detail the contents of the artifact, its relevance to the main paper, and guidelines for its effective utilization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390751617",
    "type": "article"
  },
  {
    "title": "Rigorous Assessment of Model Inference Accuracy using Language Cardinality",
    "doi": "https://doi.org/10.1145/3640332",
    "publication_date": "2024-01-16",
    "publication_year": 2024,
    "authors": "Donato Clun; Donghwan Shin; Antonio Filieri; Domenico Bianculli",
    "corresponding_authors": "",
    "abstract": "Models such as finite state automata are widely used to abstract the behavior of software systems by capturing the sequences of events observable during their execution. Nevertheless, models rarely exist in practice and, when they do, get easily outdated; moreover, manually building and maintaining models is costly and error-prone. As a result, a variety of model inference methods that automatically construct models from execution traces have been proposed to address these issues. However, performing a systematic and reliable accuracy assessment of inferred models remains an open problem. Even when a reference model is given, most existing model accuracy assessment methods may return misleading and biased results. This is mainly due to their reliance on statistical estimators over a finite number of randomly generated traces, introducing avoidable uncertainty about the estimation and being sensitive to the parameters of the random trace generative process. This article addresses this problem by developing a systematic approach based on analytic combinatorics that minimizes bias and uncertainty in model accuracy assessment by replacing statistical estimation with deterministic accuracy measures. We experimentally demonstrate the consistency and applicability of our approach by assessing the accuracy of models inferred by state-of-the-art inference tools against reference models from established specification mining benchmarks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390918560",
    "type": "article"
  },
  {
    "title": "Editorial: Toward the Future with Eight Issues Per Year",
    "doi": "https://doi.org/10.1145/3637444",
    "publication_date": "2024-01-18",
    "publication_year": 2024,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "Toward the Future with Eight Issues Per YearI am pleased to make several announcements with this issue of ACM Transactions on Software Engineering and Methodology (TOSEM), including the publication of eight issues per year, the implementation of the human-centric software engineering continuous special section, the forthcoming 2030 Roadmap for software engineering special issue, and the introduction of a \"new frontiers\" track.The landscape of software engineering is dramatically changing: New technologies challenge software engineering with new problems and solutions, and software engineering grows and diversifies.The research on applications of artificial intelligence (AI) to software engineering is a hot topic.The articles in the TOSEM continuous special section on AI and software engineering now represent almost one-fifth of the accepted articles, while they were a tiny fraction only 5 years ago.Original submissions to ACM TOSEM more than tripled in the past 5 years, with a relevant geographic shift from a dominant role of Europe and North America, with 84% of accepted articles 5 years ago, to a balanced distribution and the excellent presence of Asia and Australia, with 50% of accepted articles from the Asia-Pacific region in the past 12 months.ACM TOSEM has addressed the new challenges with new initiatives and a timely evolution.We introduced the fast track, continuous special sections, RCR reports, and registered articles.The fast-impact track offers a timebound review turnaround time, with over 50% of the overall submissions reviewed within 60 days and 80% reviewed within 90 days.The continuous special sections on AI and software engineering and on security and software engineering offer teams of expert editors to thoroughly review articles in fast-growing areas of software engineering.The fast track and the continuous special sections host articles that present completely new research results, and they now represent 60% of articles published in TOSEM.The RCR report offers a stable forum to share artifacts, tools, and data in the long term.The registered reports provide early feedback on new ideas that require expensive experimental validation and motivate investing in long-term expensive experiments.In 2024 we will move on with eight issues per year, a new human-centric software engineering continuous special section, a new technical communication track, and a 2030 Roadmap for a software engineering special issue.ACM TOSEM has published four issues per year in the first 30 years of its existence (from 1992 to 2022), it has published six issues in 2023, and it will regularly publish eight issues per year starting in 2024.The publication of eight issues per year will dramatically reduce the time to publication, thus giving the early visibility that high-quality articles deserve in a fast-changing landscape.The new continuous special section on human-centric software engineering offers an expert board to review contributions in the extremely relevant and fast-emerging field of human factors",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390984859",
    "type": "editorial"
  },
  {
    "title": "Analyzing and Detecting Information Types of Developer Live Chat Threads",
    "doi": "https://doi.org/10.1145/3643677",
    "publication_date": "2024-01-29",
    "publication_year": 2024,
    "authors": "Xiuwei Shang; Shuai Zhang; Yitong Zhang; Shikai Guo; Yulong Li; Rong Chen; Hui Li; Xiaochen Li; He Jiang",
    "corresponding_authors": "",
    "abstract": "Online chatrooms serve as vital platforms for information exchange among software developers. With multiple developers engaged in rapid communication and diverse conversation topics, the resulting chat messages often manifest complexity and lack structure. To enhance the efficiency of extracting information from chat threads , automatic mining techniques are introduced for thread classification. However, previous approaches still grapple with unsatisfactory classification accuracy due to two primary challenges that they struggle to adequately capture long-distance dependencies within chat threads and address the issue of category imbalance in labeled datasets. To surmount these challenges, we present a topic classification approach for chat information types named EAEChat. Specifically, EAEChat comprises three core components: the text feature encoding component captures contextual text features using a multi-head self-attention mechanism-based text feature encoder, and a siamese network is employed to mitigate overfitting caused by limited data; the data augmentation component expands a small number of categories in the training dataset using a technique tailored to developer chat messages, effectively tackling the challenge of imbalanced category distribution; the non-text feature encoding component employs a feature fusion model to integrate deep text features with manually extracted non-text features. Evaluation across three real-world projects demonstrates that EAEChat, respectively, achieves an average precision, recall, and F1-score of 0.653, 0.651, and 0.644, and it marks a significant 7.60% improvement over the state-of-the-art approaches. These findings confirm the effectiveness of our method in proficiently classifying developer chat messages in online chatrooms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391323061",
    "type": "article"
  },
  {
    "title": "Enumerating Valid Non-Alpha-Equivalent Programs for Interpreter Testing",
    "doi": "https://doi.org/10.1145/3647994",
    "publication_date": "2024-02-12",
    "publication_year": 2024,
    "authors": "Xinmeng Xia; Yang Feng; Qingkai Shi; James A. Jones; Xiangyu Zhang; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "Skeletal program enumeration (SPE) can generate a great number of test programs for validating the correctness of compilers or interpreters. The classic SPE generates programs by exhaustively enumerating all possible variable usage patterns into a given syntactic structure. Even though it is capable of producing many test programs, the exhaustive enumeration strategy generates a large number of invalid programs, which may waste plenty of testing time and resources. To address the problem, this article proposes a tree-based SPE technique. Compared to the state-of-the-art, the key merit of the tree-based approach is that it allows us to take the dependency information into consideration when producing test programs and, thus, make it possible to (1) directly generate non-equivalent programs and (2) apply dominance relations to eliminate invalid test programs that have undefined variables. Hence, our approach significantly saves the cost of the naïve SPE approach. We have implemented our approach into an automated testing tool, IFuzzer , and applied it to test eight different implementations of Python interpreters, including CPython, PyPy, IronPython, Jython, RustPython, GPython, Pyston, and Codon. In three months of fuzzing, IFuzzer detected 142 bugs, of which 87 have been confirmed to be previously unknown bugs, of which 34 have been fixed. Compared to the state-of-the-art SPE techniques, IFuzzer takes only 61.0% of the time cost given the same number of testing seeds and improves 5.3% source code function coverage in the same time budget of testing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391753607",
    "type": "article"
  },
  {
    "title": "Reducing the Impact of Time Evolution on Source Code Authorship Attribution via Domain Adaptation",
    "doi": "https://doi.org/10.1145/3652151",
    "publication_date": "2024-03-11",
    "publication_year": 2024,
    "authors": "Zhen Li; Shasha Zhao; Chen Chen; Qian Chen",
    "corresponding_authors": "",
    "abstract": "Source code authorship attribution is an important problem in practical applications such as plagiarism detection, software forensics, and copyright disputes. Recent studies show that existing methods for source code authorship attribution can be significantly affected by time evolution, leading to a decrease in attribution accuracy year by year. To alleviate the problem of Deep Learning (DL)-based source code authorship attribution degrading in accuracy due to time evolution, we propose a new framework called Time D omain A daptation (TimeDA) by adding new feature extractors to the original DL-based code attribution framework that enhances the learning ability of the original model on source domain features without requiring new or more source data. Moreover, we employ a centroid-based pseudo-labeling strategy using neighborhood clustering entropy for adaptive learning to improve the robustness of DL-based code authorship attribution. Experimental results show that TimeDA can significantly enhance the robustness of DL-based source code authorship attribution to time evolution, with an average improvement of 8.7% on the Java dataset and 5.2% on the C++ dataset. In addition, our TimeDA benefits from employing the centroid-based pseudo-labeling strategy, which significantly reduced the model training time by 87.3% compared to traditional unsupervised domain adaptive methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392658280",
    "type": "article"
  },
  {
    "title": "On Estimating the Feasible Solution Space of Multi-Objective Testing Resource Allocation",
    "doi": "https://doi.org/10.1145/3654444",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Guofu Zhang; Lei Li; Zhaopin Su; Feng Yue; Yang Chen; Miqing Li; Xin Yao",
    "corresponding_authors": "",
    "abstract": "The multi-objective testing resource allocation problem (MOTRAP) is concerned on how to reasonably plan the testing time of software testers to save the cost and improve the reliability as much as possible. The feasible solution space of a MOTRAP is determined by its variables (i.e., the time invested in each component) and constraints (e.g., the pre-specified reliability, cost, or time). Although a variety of state-of-the-art constrained multi-objective optimisers can be used to find individual solutions in this space, their search remains inefficient and expensive due to the fact that this space is very tiny compared to the large search space. The decision maker may often suffer a prolonged but unsuccessful search that fails to return a feasible solution. In this work, we first formulate a heavily constrained MOTRAP on the basis of an architecture-based model, in which reliability, cost, and time are optimised under the pre-specified multiple constraints on reliability, cost, and time. Then, to estimate the feasible solution space of this specific MOTRAP, we develop theoretical and algorithmic approaches to deduce new tighter lower and upper bounds on variables from constraints. Importantly, our approach can help the decision maker identify whether their constraint settings are practicable, and meanwhile, the derived bounds can just enclose the tiny feasible solution space and help off-the-shelf constrained multi-objective optimisers make the search within the feasible solution space as much as possible. Additionally, to further make good use of these bounds, we propose a generalised bound constraint handling method that can be readily employed by constrained multi-objective optimisers to pull infeasible solutions back into the estimated space with theoretical guarantee. Finally, we evaluate our approach on application and empirical cases. Experimental results reveal that our approach significantly enhances the efficiency, effectiveness, and robustness of off-the-shelf constrained multi-objective optimisers and state-of-the-art bound constraint handling methods at finding high-quality solutions for the decision maker. These improvements may help the decision maker take the stress out of setting constraints and selecting constrained multi-objective optimisers and facilitate the testing planning more efficiently and effectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393192514",
    "type": "article"
  },
  {
    "title": "Technical Debt Monitoring Decision Making with Skin in the Game",
    "doi": "https://doi.org/10.1145/3664805",
    "publication_date": "2024-06-01",
    "publication_year": 2024,
    "authors": "Suwichak Fungprasertkul; Rami Bahsoon; Rick Kazman",
    "corresponding_authors": "",
    "abstract": "Technical Debt Management (TDM) can suffer from unpredictability, communication gaps and the inaccessibility of relevant information, which hamper the effectiveness of its decision making. These issues can stem from division among decision-makers which takes root in unfair consequences of decisions among different decision-makers. One mitigation route is Skin in the Game thinking, which enforces transparency, fairness and shared responsibility during collective decision-making under uncertainty. This article illustrates characteristics which require Skin in the Game thinking in Technical Debt (TD) identification, measurement, prioritisation and monitoring. We point out crucial problems in TD monitoring rooted in asymmetric information and asymmetric payoff between different factions of decision-makers. A systematic TD monitoring method is presented to mitigate the said problems. The method leverages Replicator Dynamics and Behavioural Learning. The method supports decision-makers with automated TD monitoring decisions; it informs decision-makers when human interventions are required. Two publicly available industrial projects with a non-trivial number of TD and timestamps are utilised to evaluate the application of our method. Mann–Whitney U hypothesis tests are conducted on samples of decisions from our method and the baseline. The statistical evidence indicates that our method can produce cost-effective and contextual TD monitoring decisions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399262410",
    "type": "article"
  },
  {
    "title": "Revealing the Unseen: AI Chain on LLMs for Predicting Implicit Data Flows to Generate Data Flow Graphs in Dynamically-Typed Code",
    "doi": "https://doi.org/10.1145/3672458",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Qing Huang; Zhiwen Luo; Zhenchang Xing; Jinshan Zeng; Jieshan Chen; Xiwei Xu; Yong Chen",
    "corresponding_authors": "",
    "abstract": "Data flow graphs (DFGs) capture definitions (defs) and uses across program blocks, which is a fundamental program representation for program analysis, testing and maintenance. However, dynamically-typed programming languages like Python present implicit data flow issues that make it challenging to determine def-use flow information at compile time. Static analysis methods like Soot and WALA are inadequate for handling these issues, and manually enumerating comprehensive heuristic rules is impractical. Large pre-trained language models (LLMs) offer a potential solution, as they have powerful language understanding and pattern matching abilities, allowing them to predict implicit data flow by analyzing code context and relationships between variables, functions, and statements in code. We propose leveraging LLMs’ in-context learning ability to learn implicit rules and patterns from code representation and contextual information to solve implicit data flow problems. To further enhance the accuracy of LLMs, we design a five-step Chain of Thought (CoT) and break it down into an AI chain, with each step corresponding to a separate AI unit to generate accurate DFGs for Python code. Our approach’s performance is thoroughly assessed, demonstrating the effectiveness of each AI unit in the AI Chain. Compared to static analysis, our method achieves 82% higher def coverage and 58% higher use coverage in DFG generation on implicit data flow. We also prove the indispensability of each unit in the AI Chain. Overall, our approach offers a promising direction for building software engineering tools by utilizing foundation models, eliminating significant engineering and maintenance effort, but focusing on identifying problems for AI to solve.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399565297",
    "type": "article"
  },
  {
    "title": "When Automated Program Repair Meets Regression Testing – An Extensive Study on 2 Million Patches",
    "doi": "https://doi.org/10.1145/3672450",
    "publication_date": "2024-06-13",
    "publication_year": 2024,
    "authors": "Yiling Lou; Jun Yang; Samuel Benton; Dan Hao; Lin Tan; Zhenpeng Chen; L. H. Zhang; Lingming Zhang",
    "corresponding_authors": "",
    "abstract": "In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399601924",
    "type": "article"
  },
  {
    "title": "Automated Testing Linguistic Capabilities of NLP Models",
    "doi": "https://doi.org/10.1145/3672455",
    "publication_date": "2024-06-14",
    "publication_year": 2024,
    "authors": "Jaeseong Lee; S. H. Chen; Austin Mordahl; Cong Liu; Wei Yang; Shiyi Wei",
    "corresponding_authors": "",
    "abstract": "Natural language processing (NLP) has gained widespread adoption in the development of real-world applications. However, the black-box nature of neural networks in NLP applications poses a challenge when evaluating their performance, let alone ensuring it. Recent research has proposed testing techniques to enhance the trustworthiness of NLP-based applications. However, most existing works use a single, aggregated metric ( i.e ., accuracy) which is difficult for users to assess NLP model performance on fine-grained aspects such as linguistic capabilities. To address this limitation, we present ALiCT, an automated testing technique for validating NLP applications based on their linguistic capabilities. ALiCT takes user-specified linguistic capabilities as inputs and produce diverse test suite with test oracles for each of given linguistic capability. We evaluate ALiCT on two widely adopted NLP tasks, sentiment analysis and hate speech detection, in terms of diversity, effectiveness, and consistency. Using Self-BLEU and syntactic diversity metrics, our findings reveal that ALiCT generates test cases that are 190% and 2213% more diverse in semantics and syntax, respectively, compared to those produced by state-of-the-art techniques. In addition, ALiCT is capable of producing a larger number of NLP model failures in 22 out of 25 linguistic capabilities over the two NLP applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399665376",
    "type": "article"
  },
  {
    "title": "Bitmap-Based Security Monitoring for Deeply Embedded Systems",
    "doi": "https://doi.org/10.1145/3672460",
    "publication_date": "2024-06-19",
    "publication_year": 2024,
    "authors": "Anni Peng; Dongliang Fang; Le Guan; Erik van der Kouwe; Yin Li; Wenwen Wang; Limin Sun; Yuqing Zhang",
    "corresponding_authors": "",
    "abstract": "Deeply embedded systems powered by microcontrollers are becoming popular with the emergence of Internet-of-Things (IoT) technology. However, these devices primarily run C/C \\({+}{+}\\) code and are susceptible to memory bugs, which can potentially lead to both control data attacks and non-control data attacks. Existing defense mechanisms (such as control-flow integrity (CFI), dataflow integrity (DFI) and write integrity testing (WIT), etc.) consume a massive amount of resources, making them less practical in real products. To make it lightweight, we design a bitmap-based allowlist mechanism to unify the storage of the runtime data for protecting both control data and non-control data. The memory requirements are constant and small, regardless of the number of deployed defense mechanisms. We store the allowlist in the TrustZone to ensure its integrity and confidentiality. Meanwhile, we perform an offline analysis to detect potential collisions and make corresponding adjustments when it happens. We have implemented our idea on an ARM Cortex-M-based development board. Our evaluation results show a substantial reduction in memory consumption when deploying the proposed CFI and DFI mechanisms, without compromising runtime performance. Specifically, our prototype enforces CFI and DFI at a cost of just 2.09% performance overhead and 32.56% memory overhead on average.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399817439",
    "type": "article"
  },
  {
    "title": "<i>DSHGT</i> : Dual-Supervisors Heterogeneous Graph Transformer - A pioneer study of using heterogeneous graph learning for detecting software vulnerabilities",
    "doi": "https://doi.org/10.1145/3674729",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Tiehua Zhang; Rui Xu; J. S. Zhang; Yuze Liu; Xin Chen; Jun Yin; Xi Zheng",
    "corresponding_authors": "",
    "abstract": "Vulnerability detection is a critical problem in software security and attracts growing attention both from academia and industry. Traditionally, software security is safeguarded by designated rule-based detectors that heavily rely on empirical expertise, requiring tremendous effort from software experts to generate rule repositories for large code corpus. Recent advances in deep learning, especially Graph Neural Networks (GNN), have uncovered the feasibility of automatic detection of a wide range of software vulnerabilities. However, prior learning-based works only break programs down into a sequence of word tokens for extracting contextual features of codes, or apply GNN largely on homogeneous graph representation (e.g., AST) without discerning complex types of underlying program entities (e.g., methods, variables). In this work, we are one of the first to explore heterogeneous graph representation in the form of Code Property Graph and adapt a well-known heterogeneous graph network with a dual-supervisor structure for the corresponding graph learning task. Using the prototype built, we have conducted extensive experiments on both synthetic datasets and real-world projects. Compared with the state-of-the-art baselines, the results demonstrate superior performance in vulnerability detection (average F1 improvements over 10% in real-world projects) and language-agnostic transferability from C/C \\({+}{+}\\) to other programming languages (average F1 improvements over 11%).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400117359",
    "type": "article"
  },
  {
    "title": "Shortening Overlong Method Names with Abbreviations",
    "doi": "https://doi.org/10.1145/3676959",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Yanjie Jiang; Hui Liu; Shing-Chi Cheung; Lu Zhang",
    "corresponding_authors": "",
    "abstract": "Methods should be named to summarize their responsibilities meaningfully. When a method has a non-trivial responsibility, it may require a naming using multiple words. However, overlong method names are susceptible to typos and reduced readability (e.g., displaying a statement partially in standard screen width or splitting it into multiple lines). Programming naming conventions commonly adopt a maximal length (in characters) for identifiers. In practice, developers may not necessarily find a meaningful name that follows such naming conventions when coding a non-trivial method. This article presents the first automated technique (called NameCompressor ) to shorten overlong method names. Our inspiration is that many lengthy words/phrases in an overlong method name have known and unambiguous abbreviations. The use of these abbreviations for method names is common. To shorten an overlong method name, NameCompressor employs three compression techniques, i.e., context-aware compression, probability-based compression, and machine learning-based compression, to find appropriate abbreviations for the words/phrases in the method name. We evaluate NameCompressor on a dataset of 700 overlong method names. It correctly generates 613 short names identical to those specified by the developers of these methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400416415",
    "type": "article"
  },
  {
    "title": "SURE: A Visualized Failure Indexing Approach using Program Memory Spectrum",
    "doi": "https://doi.org/10.1145/3676958",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Yi Song; Xihao Zhang; Xiaoyuan Xie; Songqiang Chen; Quanming Liu; Ruizhi Gao",
    "corresponding_authors": "",
    "abstract": "Failure indexing is a longstanding crux in software debugging, the goal of which is to automatically divide failures (e.g., failed test cases) into distinct groups according to the culprit root causes, as such multiple faults residing in a faulty program can be handled independently and simultaneously. The community of failure indexing has long been plagued by two challenges: 1) The effectiveness of division is still far from promising. Specifically, existing failure indexing techniques only employ a limited source of software run-time data, for example, code coverage, to be failure proximity and further divide them, which typically delivers unsatisfactory results. 2) The outcome can be hardly comprehensible. Specifically, a developer who receives the division result is just aware of how all failures are divided, without knowing why they should be divided the way they are. This leads to difficulties for developers to be convinced by the division result, which in turn affects the adoption of the results. To tackle these two problems, in this paper, we propose SURE, a vi SU alized failu R e ind E xing approach using the program memory spectrum. We first collect the run-time memory information (i.e., variables’ names and values, as well as the depth of the stack frame) at several preset breakpoints during the execution of a failed test case, and transform the gathered memory information into a human-friendly image (called program memory spectrum, PMS). Then, any pair of PMS images that serve as proxies for two failures is fed to a trained Siamese convolutional neural network, to predict the likelihood of them being triggered by the same fault. Last, a clustering algorithm is adopted to divide all failures based on the mentioned likelihood. In the experiments, we use 30% of the simulated faults to train the neural network, and use 70% of the simulated faults as well as real-world faults to test. Results demonstrate the effectiveness of SURE: It achieves 101.20% and 41.38% improvements in faults number estimation, as well as 105.20% and 35.53% improvements in clustering, compared with the state-of-the-art technique in this field, in simulated and real-world environments, respectively. Moreover, we carry out a human study to quantitatively evaluate the comprehensibility of PMS, revealing that this novel type of representation can help developers better comprehend failure indexing results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400416954",
    "type": "article"
  },
  {
    "title": "Context-Aware Fuzzing for Robustness Enhancement of Deep Learning Models",
    "doi": "https://doi.org/10.1145/3680464",
    "publication_date": "2024-07-24",
    "publication_year": 2024,
    "authors": "Haipeng Wang; Zhengyuan Wei; Qilin Zhou; W. K. Chan",
    "corresponding_authors": "",
    "abstract": "In the testing-retraining pipeline for enhancing the robustness property of deep learning (DL) models, many state-of-the-art robustness-oriented fuzzing techniques are metric-oriented. The pipeline generates adversarial examples as test cases via such a DL testing technique and retrains the DL model under test with test suites that contain these test cases. On the one hand, the strategies of these fuzzing techniques tightly integrate the key characteristics of their testing metrics. On the other hand, they are often unaware of whether their generated test cases are different from the samples surrounding these test cases and whether there are relevant test cases of other seeds when generating the current one. We propose a novel testing metric called Contextual Confidence (CC). CC measures a test case through the surrounding samples of a test case in terms of their mean probability predicted to the prediction label of the test case. Based on this metric, we further propose a novel fuzzing technique Clover as a DL testing technique for the pipeline. In each fuzzing round, Clover first finds a set of seeds whose labels are the same as the label of the seed under fuzzing. At the same time, it locates the corresponding test case that achieves the highest CC values among the existing test cases of each seed in this set of seeds and shares the same prediction label as the existing test case of the seed under fuzzing that achieves the highest CC value. Clover computes the piece of difference between each such pair of a seed and a test case. It incrementally applies these pieces of differences to perturb the current test case of the seed under fuzzing that achieves the highest CC value and to perturb the resulting samples along the gradient to generate new test cases for the seed under fuzzing. Clover finally selects test cases among the generated test cases of all seeds as even as possible and with a preference to select test cases with higher CC values for improving model robustness. The experiments show that Clover outperforms the state-of-the-art coverage-based technique Adapt and loss-based fuzzing technique RobOT by 67%–129% and 48%–100% in terms of robustness improvement ratio, respectively, delivered through the same testing-retraining pipeline. For test case generation, in terms of numbers of unique adversarial labels and unique categories for the constructed test suites, Clover outperforms Adapt by \\(2.0\\times\\) and \\(3.5{\\times}\\) and RobOT by \\(1.6\\times\\) and \\(1.7\\times\\) on fuzzing clean models, and also outperforms Adapt by \\(3.4\\times\\) and \\(4.5\\times\\) and RobOT by \\(9.8\\times\\) and \\(11.0\\times\\) on fuzzing adversarially trained models, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400949861",
    "type": "article"
  },
  {
    "title": "Measuring and Mining Community Evolution in Developer Social Networks with Entropy-Based Indices",
    "doi": "https://doi.org/10.1145/3688832",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Jierui Zhang; Liang Wang; Ying Li; Jing Jiang; Tao Wang; Xianping Tao",
    "corresponding_authors": "",
    "abstract": "This work presents four novel entropy-based indices for measuring the community evolution of developer social networks (DSNs) in open source software (OSS) projects. The proposed indices offer a quantitative measure of community split, shrink, merge, and expand events. The indices have proven properties like monotonicity, and they have defined maximum and minimum values that signify meaningful scenarios. These indices can be combined to describe complex community evolution events such as emergence and extinction. Expanding upon these indices, this research proposes a novel machine learning approach, leveraging shapelet mining, to unearth representative patterns of community evolution. The results from real-world OSS projects show that these indices effectively capture various community evolution behaviors with a 94.1% accuracy compared to existing work. They also predict OSS team productivity with a 0.718 accuracy. With the shapelet mining and learning framework, the indices can identify patterns of community evolution and predict the survival of OSS projects with 93% accuracy three months before the projects’ last observed commits. The findings highlight the potential of these entropy-based indices for understanding OSS project status and predicting future trends, which are valuable for supporting future research on DSNs and OSS communities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401635853",
    "type": "article"
  },
  {
    "title": "History-Driven Fuzzing For Deep Learning Libraries",
    "doi": "https://doi.org/10.1145/3688838",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Nima Shiri Harzevili; Mohammad Mahdi Mohajer; Moshi Wei; Hung Viet Pham; Song Wang",
    "corresponding_authors": "",
    "abstract": "Recently, many Deep Learning (DL) fuzzers have been proposed for API-level testing of DL libraries. However, they either perform unguided input generation (e.g., not considering the relationship between API arguments when generating inputs) or only support a limited set of corner-case test inputs. Furthermore, many developer APIs crucial for library development remain untested, as they are typically not well documented and lack clear usage guidelines, unlike end-user APIs. This makes them a more challenging target for automated testing. To fill this gap, we propose a novel fuzzer named Orion, which combines guided test input generation and corner-case test input generation based on a set of fuzzing heuristic rules constructed from historical data known to trigger critical issues in the underlying implementation of DL APIs. To extract the fuzzing heuristic rules, we first conduct an empirical study on the root cause analysis of 376 vulnerabilities in two of the most popular DL libraries, PyTorch and TensorFlow. We then construct the fuzzing heuristic rules based on the root causes of the extracted historical vulnerabilities. Using these fuzzing heuristic rules, Orion generates corner-case test inputs for API-level fuzzing. In addition, we extend the seed collection of existing studies to include test inputs for developer APIs. Our evaluation shows that Orion reports 135 vulnerabilities in the latest releases of TensorFlow and PyTorch, 76 of which were confirmed by the library developers. Among the 76 confirmed vulnerabilities, 69 were previously unknown, and 7 have already been fixed. The rest are awaiting further confirmation. For end-user APIs, Orion detected 45.58% and 90% more vulnerabilities in TensorFlow and PyTorch, respectively, compared to the state-of-the-art conventional fuzzer, DeepRel. When compared to the state-of-the-art LLM-based DL fuzzer, AtlasFuz, and Orion detected 13.63% more vulnerabilities in TensorFlow and 18.42% more vulnerabilities in PyTorch. Regarding developer APIs, Orion stands out by detecting 117% more vulnerabilities in TensorFlow and 100% more vulnerabilities in PyTorch compared to the most relevant fuzzer designed for developer APIs, such as FreeFuzz.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401635890",
    "type": "article"
  },
  {
    "title": "Relevant information in TDD experiment reporting",
    "doi": "https://doi.org/10.1145/3688837",
    "publication_date": "2024-08-22",
    "publication_year": 2024,
    "authors": "Fernando Uyaguari; Silvia T. Acuña; John W. Castro; Davide Fucci; Óscar Dieste; Sira Vegas",
    "corresponding_authors": "",
    "abstract": "Experiments are a commonly used method of research in software engineering (SE). Researchers report their experiments following detailed guidelines. However, researchers do not, in the field of test-driven development (TDD) at least, specify how they operationalized the response variables and, particularly, the measurement process. This article has three aims: (i) identify the response variable operationalization components in TDD experiments that study external quality; (ii) study their influence on the experimental results; (iii) determine if the experiment reports describe the measurement process components that have an impact on the results. We used two-part sequential mixed methods research. The first part of the research adopts a quantitative approach applying a statistical analysis of the impact of the operationalization components on the experimental results. The second part follows on with a qualitative approach applying a systematic mapping study (SMS). The test suites, intervention types and measurers have an influence on the measurements and results of the statistical analysis of TDD experiments in SE. The test suites have a major impact on both the measurements and the results of the experiments. The intervention type has less impact on the results than on the measurements. While the measurers have an impact on the measurements, this is not transferred to the experimental results. On the other hand, the results of our SMS confirm that TDD experiments do not usually report either the test suites, the test case generation method, or the details of how external quality was measured. A measurement protocol should be used to assure that the measurements made by different measurers are similar. It is necessary to report the test cases, the experimental task and the intervention type in order to be able to reproduce the measurements and statistical analyses, as well as to replicate experiments and build dependable families of experiments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401794768",
    "type": "article"
  },
  {
    "title": "MalSensor: Fast and Robust Windows Malware Classification",
    "doi": "https://doi.org/10.1145/3688833",
    "publication_date": "2024-08-24",
    "publication_year": 2024,
    "authors": "Haojun Zhao; Yueming Wu; Deqing Zou; Yang Liu; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Driven by the substantial profits, the evolution of Portable Executable (PE) malware has posed persistent threats. PE malware classification has been an important research field, and numerous classification methods have been proposed. With the development of machine learning, learning-based static classification methods achieve excellent performance. However, most existing methods cannot meet the requirements of industrial applications due to the limited resource consumption and concept drift. In this paper, we propose a fast, high-accuracy, and robust FCG-based PE malware classification method. We first extract precise function call relationships through code and data cross-referencing analysis. Then we normalize function names to construct a concise and accurate function call graph. Furthermore, we perform topological analysis of the function call graph using social network analysis techniques, thereby enhancing the program function call features. Finally, we use a series of machine learning algorithms for classification. We implement a prototype system named MalSensor and compare it with nine state-of-the-art static PE malware classification methods. The experimental results show that MalSensor is capable of classifying a malicious file in 0.7 seconds on average with up to 98.35% accuracy, which represents a significant advantage over existing methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401847340",
    "type": "article"
  },
  {
    "title": "Termination and Universal Termination Problems for Nondeterministic Quantum Programs",
    "doi": "https://doi.org/10.1145/3691632",
    "publication_date": "2024-09-02",
    "publication_year": 2024,
    "authors": "Ming Xu; Jianling Fu; Hui Jiang; Yuxin Deng; Z.Q. Li",
    "corresponding_authors": "",
    "abstract": "Verifying quantum programs has attracted a lot of interest in recent years. In this paper, we consider the following two categories of termination problems of quantum programs with nondeterminism, namely: (1) (termination) Is an input of a program terminating with probability one under all schedulers? If not, how can a scheduler be synthesized to evidence the nontermination? (2) (universal termination) Are all inputs terminating with probability one under their respective schedulers? If yes, a further question asks whether there is a scheduler that forces all inputs to be terminating with probability one together with how to synthesize it; otherwise, how can an input be provided to refute the universal termination? For the effective verification of the first category, we over-approximate the reachable set of quantum program states by the reachable subspace, whose algebraic structure is a linear space. On the other hand, we study the set of divergent states from which the program terminates with probability zero under some scheduler. The divergent set also has an explicit algebraic structure. Exploiting these explicit algebraic structures, we address the decision problem by a necessary and sufficient condition, i. e. the disjointness of the reachable subspace and the divergent set. Furthermore, the scheduler synthesis is completed in exponential time, whose bottleneck lies in computing the divergent set, reported for the first time. For the second category, we reduce the decision problem to the existence of invariant subspace, from which the program terminates with probability zero under all schedulers. The invariant subspace is characterized by linear equations and thus can be efficiently computed. The states on that invariant subspace are evidence of the nontermination. Furthermore, the scheduler synthesis is completed by seeking a pattern of finite schedulers that forces all inputs to be terminating with positive probability. The repetition of that pattern yields the desired universal scheduler that forces all inputs to be terminating with probability one. All the problems in the second category are shown, also for the first time, to be solved in polynomial time. Finally, we demonstrate the aforementioned methods via a running example — the quantum Bernoulli factory protocol.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402129906",
    "type": "article"
  },
  {
    "title": "Reputation Gaming in Crowd Technical Knowledge Sharing",
    "doi": "https://doi.org/10.1145/3691627",
    "publication_date": "2024-09-04",
    "publication_year": 2024,
    "authors": "Iren Mazloomzadeh; Gias Uddin; Foutse Khomh; Ashkan Sami",
    "corresponding_authors": "",
    "abstract": "Stack Overflow incentive system awards users with reputation scores to ensure quality. The decentralized nature of the forum may make the incentive system prone to manipulation. This paper offers, for the first time, a comprehensive study of the reported types of reputation manipulation scenarios that might be exercised in Stack Overflow and the prevalence of such reputation gamers by a qualitative study of 1,697 posts from meta Stack Exchange sites. We found four different types of reputation fraud scenarios, such as voting rings where communities form to upvote each other repeatedly on similar posts. We developed algorithms that enable platform managers to automatically identify these suspicious reputation gaming scenarios for review. The first algorithm identifies isolated/semi-isolated communities where probable reputation frauds may occur mostly by collaborating with each other. The second algorithm looks for sudden unusual big jumps in the reputation scores of users. We evaluated the performance of our algorithms by examining the reputation history dashboard of Stack Overflow users from the Stack Overflow website. We observed that around 60-80% of users flagged as suspicious by our algorithms experienced reductions in their reputation scores by Stack Overflow.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402216867",
    "type": "article"
  },
  {
    "title": "Non-Flaky and Nearly-Optimal Time-based Treatment of Asynchronous Wait Web Tests",
    "doi": "https://doi.org/10.1145/3695989",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Yu Pei; Jeongju Sohn; Sarra Habchi; Mike Papadakis",
    "corresponding_authors": "",
    "abstract": "Asynchronous waits are a common root cause of flaky tests and a major time-influential factor of web application testing. We build a dataset of 49 reproducible asynchronous wait flaky tests and their fixes from 26 open-source projects to study their characteristics in web testing. Our study reveals that developers adjusted wait time to address asynchronous wait flakiness in about 63% of cases (31 out of 49), even when the underlying causes lie elsewhere. From this, we introduce TRaf , an automated time-based repair for asynchronous wait flakiness in web applications. TRaf determines appropriate wait times for asynchronous calls in web applications by analyzing code similarity and past change history. Its key insight is that efficient wait times can be inferred from the current or past codebase since developers tend to repeat similar mistakes. Our analysis shows that TRaf can statically suggest a shorter wait time to alleviate async wait flakiness immediately upon the detection, reducing test execution time by 11.1% compared to the timeout values initially chosen by developers. With optional dynamic tuning, TRaf can reduce the execution time by 16.8% in its initial refinement compared to developer-written patches and by 6.2% compared to the post-refinements of these original patches. Overall, we sent 16 pull requests from our dataset, each fixing one test, to the developers. So far, three have been accepted by the developers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402516053",
    "type": "article"
  },
  {
    "title": "Automatic Identification of Game Stuttering via Gameplay Videos Analysis",
    "doi": "https://doi.org/10.1145/3695992",
    "publication_date": "2024-09-18",
    "publication_year": 2024,
    "authors": "Emanuela Guglielmi; Gabriele Bavota; Rocco Oliveto; Simone Scalabrino",
    "corresponding_authors": "",
    "abstract": "Modern video games are extremely complex software systems and, as such, they might suffer from several types of post-release issues. A particularly insidious issue is constituted by drops in the frame rate ( i.e. , stuttering events), which might have a negative impact on the user experience. Stuttering events are frequently documented in the million of hours of gameplay videos shared by players on platforms such as Twitch or YouTube. From the developers’ perspective, these videos represent a free source of documented “testing activities”. However, especially for popular games, the quantity and length of these videos make impractical their manual inspection. We introduce HASTE, an approach for the automatic detection of stuttering events in gameplay videos that can be exploited to generate candidate bug reports. HASTE firstly splits a given video into visually coherent slices, with the goal of filtering-out those that not representing actual gameplay ( e.g ., navigating the game settings). Then, it identifies the subset of pixels in the video frames which actually show the game in action excluding additional elements on screen such as the logo of the YouTube channel, on-screen chats etc. In this way, HASTE can exploit state-of-the-art image similarity metrics to identify candidate stuttering events, namely subsequent frames being almost identical in the pixels depicting the game. We evaluate the different steps behind HASTE on a total of 105 videos showing that it can correctly extract video slices with a 76% precision, and can correctly identify the slices related to gameplay with a recall and precision higher than 77%. Overall, HASTE achieves 71% recall and 89% precision for the identification of stuttering events in gameplay videos.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402599094",
    "type": "article"
  },
  {
    "title": "Is It Hard to Generate Holistic Commit Message?",
    "doi": "https://doi.org/10.1145/3695996",
    "publication_date": "2024-09-16",
    "publication_year": 2024,
    "authors": "Guoqing Wang; Zeyu Sun; Jinhao Dong; Yuxia Zhang; Mingxuan Zhu; Qingyuan Liang; Dan Hao",
    "corresponding_authors": "",
    "abstract": "Commit messages are important for developers to understand the content and the reason for code changes. However, poor and even empty commit messages widely exist. To improve the quality of commit messages and development efficiency, many commit message generation methods have been proposed. Nevertheless, previous methods mainly focus on a brief generation problem, where both the input code change and the output commit messages are restricted to short. This may initiate a debate on the performance of these methods in practice. In this paper, we attempt to remove the restrictions and move the needle forward to a holistic commit message generation problem. In particular, we conduct experiments to evaluate the performance of existing commit message generation methods in holistic commit message generation. In the experiments, we choose seven state-of-the-art commit generation methods and focus on two important scenarios in commit message generation (i.e., the within-project scenario and the cross-project scenario). To conduct our experiments, we publish a holistic commit message dataset HORDA with test data manually labeled. In our evaluations, we find that in generating holistic commit messages, the IR-based method has a better performance than non-pre-trained generation-based methods in the within-project scenario, contradicting previous research findings. Further, while the pre-trained generation-based methods are better than non-pre-trained generation-based methods, they are still constrained by the limitations of generation models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402785921",
    "type": "article"
  },
  {
    "title": "Improving Fault Localization with External Oracle by using Counterfactual Execution",
    "doi": "https://doi.org/10.1145/3695997",
    "publication_date": "2024-09-26",
    "publication_year": 2024,
    "authors": "Jongchan Park; Tae Eun Kim; Dongsun Kim; Kihong Heo",
    "corresponding_authors": "",
    "abstract": "We present Flex , a new approach to improve fault localization with external oracles. Spectrum-based fault localization techniques estimate suspicious statements based on the execution trace of the test suite. State-of-the-art techniques rely on test oracles that internally exist in the program. However, programs often have external oracles that observe their behavior from outside. This in turn hinders fine-grained and accurate estimation of suspicious statements in practice because the correctness of each execution can only be observed at termination. In this paper, we aim to address this problem by observing counterfactual execution traces, which enable fine-grained estimation even without precise internal oracles. We observe two types of counterfactual scenarios related to different types of test cases: When the branch condition is set to a boolean constant, (1) if most of the passing test cases still pass, we consider the newly executed statements in the branch statement as unrelated to the failure; (2) if failing test case still fails, we also consider the originally executed statements as unrelated to the failure. We evaluated the performance on widely used C and Java programs. Flex improves the accuracy of state-of-the-art SBFL techniques on C and Java programs by 24% and 22% on average, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402860113",
    "type": "article"
  },
  {
    "title": "ZigZagFuzz: Interleaved Fuzzing of Program Options and Files",
    "doi": "https://doi.org/10.1145/3697014",
    "publication_date": "2024-09-26",
    "publication_year": 2024,
    "authors": "Ahcheong Lee; Y.K. Choi; Shin Hong; Yunho Kim; Kyutae Cho; Moonzoo Kim",
    "corresponding_authors": "",
    "abstract": "Command-line options (e.g., -l , -F , -R for ls ) given to a command-line program can significantly alternate the behaviors of the program. Thus, fuzzing not only file input but also program options can improve test coverage and bug detection. In this paper, we propose ZigZagFuzz which achieves higher test coverage and detects more bugs than the state-of-the-art fuzzers by separately mutating program options and file inputs in an iterative/interleaving manner. ZigZagFuzz applies the following three core ideas. First, to utilize different characteristics of the program option domain and the file input domain, ZigZagFuzz separates phases of mutating program options from ones of mutating file inputs and performs two distinct mutation strategies on the two different domains. Second, to reach deep segments of a target program that are accessed through an interleaving sequence of program option checks and file inputs checks, ZigZagFuzz continuously interleaves phases of mutating program options with phases of mutating file inputs. Finally, to improve fuzzing performance further, ZigZagFuzz periodically shrinks input corpus by removing similar test inputs based on their function coverage. The experiment results on the 20 real-world programs show that ZigZagFuzz improves test coverage and detects 1.9 to 10.6 times more bugs than the state-of-the-art fuzzers that mutate program options such as AFL++-argv, AFL++-all, Eclipser, CarpetFuzz, ConfigFuzz, and POWER. We have reported the new bugs detected by ZigZagFuzz, and the original developers confirmed our bug reports.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402860132",
    "type": "article"
  },
  {
    "title": "PROV-IDEA: Supporting Interoperable Schema and Data Provenance within Database Evolution",
    "doi": "https://doi.org/10.1145/3697008",
    "publication_date": "2024-09-27",
    "publication_year": 2024,
    "authors": "Beatriz Pérez; Ángel L. Rubio; María A. Zapata",
    "corresponding_authors": "",
    "abstract": "Database evolution and data provenance are two closely related research fields. On the one hand, the registry (via provenance) of the schema evolution allows the maintenance of its version record. On the other hand, the origin of the data (i.e. its provenance) will always be affected by modifications (i.e. the evolution) in the schema on which they are based. Despite these interrelationships, there are few works in the literature that have proposed advances in that direction. In particular, to the best of our knowledge, there is no research that has resulted in a general and interoperable solution to the problem of managing database evolution using provenance. In this paper we present PROV-IDEA: a PROV-Interoperable Database Evolution Approach. This is a proposal that allows the simultaneous management of the provenance of schemas (of relational databases) and data, using the PROV standard as a way to guarantee interoperability. Furthermore, it is an adaptable and expandable approach (by using PROV templates), which allows a non-intrusive and seamless integration with existing applications, as well as different aspects of provenance information generation. These properties are demonstrated in the article by presenting a proof of concept built on top of a third-party relational database evolution tool.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402922825",
    "type": "article"
  },
  {
    "title": "NLPLego: Assembling Test Generation for Natural Language Processing Applications",
    "doi": "https://doi.org/10.1145/3691631",
    "publication_date": "2024-10-05",
    "publication_year": 2024,
    "authors": "Pin Ji; Yang Feng; Ruohao Zhang; Ruichen Xue; Yichi Zhang; Weitao Huang; Jia Liu; Zhihong Zhao",
    "corresponding_authors": "",
    "abstract": "With the development of Deep Learning, Natural Language Processing (NLP) applications have reached or even exceeded human-level capabilities in certain tasks. Although NLP applications have shown good performance, they can still have bugs like traditional software and even lead to serious consequences. Inspired by Lego blocks and syntax structure analysis, we propose an assembling test generation method for NLP applications or models and implement it in NLPLego . The key idea of NLPLego is to assemble the sentence skeleton and adjuncts in order by simulating the building of Lego blocks to generate multiple grammatically and semantically correct sentences based on one seed sentence. The sentences generated by NLPLego have derivation relations and different degrees of variation. These characteristics make it well-suited for integration with metamorphic testing theory, addressing the challenge of test oracle absence in NLP application testing. To validate NLPLego , we conduct experiments on three commonly used NLP tasks (i.e., machine reading comprehension, sentiment analysis, and semantic similarity measures), focusing on the efficiency of test generation and the quality and effectiveness of generated tests. We select five advanced NLP models and one popular industrial NLP software as the tested subjects. Given seed tests from SQuAD 2.0, SST, and QQP, NLPLego successfully detects 1,732, 3,140, and 261,879 incorrect behaviors with around 93.1% precision in three tasks, respectively. The experiment results show that NLPLego can efficiently generate high-quality tests for multiple NLP tasks to detect erroneous behaviors effectively. In the case study, we analyze the testing results provided by NLPLego to obtain intuitive representations of the different NLP capabilities of the tested subjects. The case study confirms that NLPLego can provide developers with clarity on the direction to improve NLP models or applications, laying the foundation for enhancing performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403155081",
    "type": "article"
  },
  {
    "title": "Improving Source Code Pre-training via Type-Specific Masking",
    "doi": "https://doi.org/10.1145/3699599",
    "publication_date": "2024-10-11",
    "publication_year": 2024,
    "authors": "Wentao Zou; Qi Li; Chuanyi Li; Jidong Ge; Xiang Chen; LiGuo Huang; Bin Luo",
    "corresponding_authors": "",
    "abstract": "The masked language modeling (MLM) task is widely recognized as one of the most effective pre-training tasks and currently derives many variants in the software engineering (SE) field. However, most of these variants mainly focus on code representation without distinguishing between different code token types, while some focus on a specific type, such as code identifiers. Indeed, various code token types exist, and there is no evidence that only identifiers can improve PTMs. Thus, to improve PTMs through different types, we conducted an extensive study to evaluate how different type-specific masking tasks can affect PTMs. First, we extract five code token types, convert them into type-specific masking tasks, and generate their combinations. Second, we pre-train CodeBERT and PLBART using combinations and fine-tuned them on four SE downstream tasks. Experimental results show that type-specific masking tasks can enhance CodeBERT and PLBART on all downstream tasks. Furthermore, we discuss topics related to low-resource datasets, conflicting PTMs that original pre-training tasks conflict with our methods, the cost and performance of our methods, factors that impact the performance of our methods, and applying our methods on state-of-the-art PTMs. These discussions comprehensively analyze the strengths and weaknesses of different type-specific masking tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403336760",
    "type": "article"
  },
  {
    "title": "<scp>DiPri</scp> : Distance-based Seed Prioritization for Greybox Fuzzing — RCR Report",
    "doi": "https://doi.org/10.1145/3701298",
    "publication_date": "2024-10-19",
    "publication_year": 2024,
    "authors": "Ruixiang Qian; Quanjun Zhang; Chunrong Fang; D.Z. Yang; Shun Li; Bohao Li; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "This replicated computational results (RCR) report describes how to (1) set up DiPri and (2) replicate the experimental results. The primary artifact is the C/C++ prototype of DiPri , which is essentially an extension of the state-of-the-art greybox fuzzer AFL++ (version 4.06). Other artifacts include the Java implementation of DiPri on Zest, the materials for integrating DiPri into FuzzBench and Magma, and the scripts for running docker and processing data. All artifacts can be found at our GitHub repository 1 and Zenodo archive. 2",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403565155",
    "type": "article"
  },
  {
    "title": "Twenty years later: Evaluating the Adoption of Control Flow Integrity",
    "doi": "https://doi.org/10.1145/3702982",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Sabine Houy; Alexandre Bartel",
    "corresponding_authors": "",
    "abstract": "Memory corruption vulnerabilities still allow compromising computers through software written in a memory-unsafe language such as C/C++. This highlights that mitigation techniques to prevent such exploitations are not all widely deployed. In this paper, we introduce SeeCFI , a tool to detect the presence of a memory corruption mitigation technique called control flow integrity (CFI). We leverage SeeCFI to investigate to what extent the mitigation has been deployed in complex software systems such as Android and specific Linux distributions (Ubuntu and Debian). Our results indicate that the overall adoption of CFI (forward- and backward-edge) is increasing across Android versions ( \\(\\sim\\) 30% in Android 13) but remains the same low ( \\(\\lt\\) 1%) throughout different Linux versions. Our tool, SeeCFI , offers the possibility to identify which binaries in a system were compiled using the CFI option. This can be deployed by external security researchers to efficiently decide which binaries to prioritize when fixing vulnerabilities and how to fix them. Therefore, SeeCFI can help to make software systems more secure.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404002942",
    "type": "article"
  },
  {
    "title": "Efficient Multivariate Time Series Anomaly Detection Through Transfer Learning for Large-Scale Software Systems",
    "doi": "https://doi.org/10.1145/3702984",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Yongqian Sun; Minghan Liang; Shenglin Zhang; Zeyu Che; Zhiyao Luo; Dongwen Li; Yuzhi Zhang; Dan Pei; Lemeng Pan; Liping Hou",
    "corresponding_authors": "",
    "abstract": "Timely anomaly detection of multivariate time series (MTS) is of vital importance for managing large-scale software systems. However, many deep learning-based MTS anomaly detection models require long-term MTS training data to achieve optimal performance, which often conflicts with the frequent pattern changes observed in software systems. Moreover, the training overhead of vast MTS in large-scale software systems is unacceptably high. To address these issues, we design OmniTransfer , a model-agnostic framework that combines weighted hierarchical agglomerative clustering with an adaptive transfer learning strategy, making many state-of-the-art (SOTA) MTS anomaly detection models efficient and effective. Extensive experiments using real-world data from a large web content service provider and a network operator show that OmniTransfer significantly reduces the model initialization time by 46.49% and the training cost by 74.51%, while maintaining high accuracy in detecting anomalies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404041379",
    "type": "article"
  },
  {
    "title": "Repairs and Breaks Prediction for Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3702983",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Yuta Ishimoto; Masanari Kondo; Lei Ma; Naoyasu Ubayashi; Yasutaka Kamei",
    "corresponding_authors": "",
    "abstract": "With the increasing prevalence of software incorporating deep neural networks (DNNs), quality assurance for these software systems has become a crucial concern. To this end, various methods have been proposed to repair the misbehavior of DNNs by modifying their weights. However, these repair methods may not meet the developer's needs for a given dataset and model. In this study, we build prediction models for repair outcomes (i.e., repairs and breaks) to help determine whether the repair method is likely to work. By using our prediction models, developers and operators of DNNs can decide whether or not to apply a repair method, and if so, which method to use. Our prediction models utilize four metrics as explanatory metrics that represent the confidence or ambiguity in the DNN predictions. We experimented with four repair methods and 10 datasets. The experimental results demonstrate that our prediction models successfully select a repair method that meets developers’ needs in 16 out of 24 cases, resulting in an average time saving of 16.29% compared to the naive method. Based on these results, our prediction models can reduce costs for developers and operators when deciding whether to employ repair methods for real-world applications of DNNs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404060023",
    "type": "article"
  },
  {
    "title": "Identifying Performance Issues in Cloud Service Systems Based on Relational-Temporal Features",
    "doi": "https://doi.org/10.1145/3702978",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Wenwei Gu; Jinyang Liu; Zhuangbin Chen; Jianping Zhang; Yuxin Su; Jiazhen Gu; C. Q. Feng; Zengyin Yang; Yongqiang Yang; Michael R. Lyu",
    "corresponding_authors": "",
    "abstract": "Cloud systems, typically comprised of various components ( e.g. , microservices), are susceptible to performance issues, which may cause service-level agreement violations and financial losses. Identifying performance issues is thus of paramount importance for cloud vendors. In current practice, crucial metrics, i.e. , key performance indicators (KPIs), are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account, however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE ), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags pseudo labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3. The ablation study also proves the effectiveness of the relational-temporal features and the PU-learning strategy. Furthermore, we share the success stories of leveraging ISOLATE to identify performance issues in Huawei Cloud, which demonstrates its superiority in practice.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404060138",
    "type": "article"
  },
  {
    "title": "DREAM: Debugging and Repairing AutoML Pipelines",
    "doi": "https://doi.org/10.1145/3702992",
    "publication_date": "2024-11-06",
    "publication_year": 2024,
    "authors": "Xiaoyu Zhang; Juan Zhai; Shiqing Ma; Xiaohong Guan; Chao Shen",
    "corresponding_authors": "",
    "abstract": "Deep Learning models have become an integrated component of modern software systems. In response to the challenge of model design, researchers proposed Automated Machine Learning (AutoML) systems, which automatically search for model architecture and hyperparameters for a given task. Like other software systems, existing AutoML systems have shortcomings in their design. We identify two common and severe shortcomings in AutoML, performance issue (i.e., searching for the desired model takes an unreasonably long time) and ineffective search issue (i.e., AutoML systems are not able to find an accurate enough model). After analyzing the workflow of AutoML, we observe that existing AutoML systems overlook potential opportunities in search space, search method, and search feedback, which results in performance and ineffective search issues. Based on our analysis, we design and implement DREAM, an automatic and general-purpose tool to alleviate and repair the shortcomings of AutoML pipelines and conduct effective model searches for diverse tasks. It monitors the process of AutoML to collect detailed feedback and automatically repairs shortcomings by expanding search space and leveraging a feedback-driven search strategy. Our evaluation results show that DREAM can be applied on two state-of-the-art AutoML pipelines and effectively and efficiently repair their shortcomings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404107888",
    "type": "article"
  },
  {
    "title": "Unifying Model Execution and Deductive Verification with Interaction Trees in Isabelle/HOL",
    "doi": "https://doi.org/10.1145/3702981",
    "publication_date": "2024-11-06",
    "publication_year": 2024,
    "authors": "Simon Foster; Chung-Kil Hur; Jim Woodcock",
    "corresponding_authors": "",
    "abstract": "Model execution allows us to prototype and analyse software engineering models by stepping through their possible behaviours, using techniques like animation and simulation. On the other hand, deductive verification allows us to construct formal proofs demonstrating satisfaction of certain critical properties in support of high-assurance software engineering. To ensure coherent results between execution and proof, we need unifying semantics and automation. In this paper, we mechanise Interaction Trees (ITrees) in Isabelle/HOL to produce an execution and verification framework. ITrees are coinductive structures that allow us to encode infinite labelled transition systems, yet they are inherently executable. We use ITrees to create verification tools for stateful imperative programs, concurrent programs with message passing in the form of the CSP and Circus languages, and abstract system models in the style of the Z and B methods. We demonstrate how ITrees can account for diverse semantic presentations, such as structural operational semantics, a relational program model, and CSP's failures-divergences trace model. Finally, we demonstrate how ITrees can be executed using the Isabelle code generator to support the animation of models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404107949",
    "type": "article"
  },
  {
    "title": "Automating TODO-missed Methods Detection and Patching",
    "doi": "https://doi.org/10.1145/3700793",
    "publication_date": "2024-11-06",
    "publication_year": 2024,
    "authors": "Zhipeng Gao; Yanqi Su; Xing Hu; Xin Xia",
    "corresponding_authors": "",
    "abstract": "TODO comments are widely used by developers to remind themselves or others about incomplete tasks. In other words, TODO comments are usually associated with temporary or suboptimal solutions. In practice, all the equivalent suboptimal implementations should be updated (e.g., adding TODOs) simultaneously. However, due to various reasons (e.g., time constraints or carelessness), developers may forget or even are unaware of adding TODO comments to all necessary places, which results in the TODO-missed methods . These “hidden” suboptimal implementations in TODO-missed methods may hurt the software quality and maintainability in the long-term. Therefore, in this paper, we propose the novel task of TODO-missed methods detection and patching, and develop a novel model, namely TDPatcher ( T O D O-comment Patcher ), to automatically patch TODO comments to the TODO-missed methods in software projects. Our model has two main stages: offline learning and online inference. During the offline learning stage, TDPatcher employs the GraphCodeBERT and contrastive learning for encoding the TODO comment (natural language) and its suboptimal implementation (code fragment) into vector representations. For the online inference stage, we can identify the TODO-missed methods and further determine their patching position by leveraging the offline trained model. We built our dataset by collecting TODO-introduced methods from the top-10,000 Python GitHub repositories and evaluated TDPatcher on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We further conduct an in-the-wild evaluation which successfully detects 26 TODO-missed methods from 50 GitHub repositories.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404112638",
    "type": "article"
  },
  {
    "title": "An Empirical Study on the Suitability of Test-based Patch Acceptance Criteria",
    "doi": "https://doi.org/10.1145/3702971",
    "publication_date": "2024-11-12",
    "publication_year": 2024,
    "authors": "Luciano Zemín; Simón Gutiérrez Brida; Ariel Godio; César Cornejo; Renzo Degiovanni; Germán Regis; Nazareno Aguirre; Marcelo F. Frias",
    "corresponding_authors": "",
    "abstract": "In this article, we empirically study the suitability of tests as acceptance criteria for automated program fixes, by checking patches produced by automated repair tools using a bug-finding tool, as opposed to previous works that used tests or manual inspections. We develop a number of experiments in which faulty programs from IntroClass , a known benchmark for program repair techniques, are fed to the program repair tools GenProg, Angelix, AutoFix and Nopol, using test suites of varying quality, including those accompanying the benchmark. We then check the produced patches against formal specifications using a bug-finding tool. Our results show that, in the studied scenarios, automated program repair tools are significantly more likely to accept a spurious program fix than producing an actual one. Using bounded-exhaustive suites larger than the originally given ones (with about 100 and 1,000 tests) we verify that overfitting is reduced but a) few new correct repairs are generated and b) some tools see their performance reduced by the larger suites and fewer correct repairs are produced. Finally, by comparing with previous work, we show that overfitting is underestimated in semantics-based tools and that patches not discarded using held-out tests may be discarded using a bug-finding tool.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404299213",
    "type": "article"
  },
  {
    "title": "Demystifying React Native Android Apps for Static Analysis",
    "doi": "https://doi.org/10.1145/3702977",
    "publication_date": "2024-11-12",
    "publication_year": 2024,
    "authors": "Yonghui Liu; Xiao Chen; Pei Liu; Jordan Samhi; John Grundy; Chunyang Chen; Li Li",
    "corresponding_authors": "",
    "abstract": "React Native, an open-source framework, simplifies cross-platform app development by allowing JavaScript-side code to interact with native-side code. Previous studies disregarded React Native, resulting in insufficient static analysis of React Native app code. This study initiates the investigation of challenges when statically analyzing React Native apps. We propose ReuNify to improve Soot-based static analysis coverage for JavaScript-side and native-side code. ReuNify converts Hermes bytecode to Soot’s intermediate representation. Hermes bytecode, compiled from JavaScript code and integrated into React Native apps, possesses a unique syntax that eludes current JavaScript analyzers. Additionally, we investigate opcode distribution and conduct in-depth analyses of the usage of opcode between popular apps and malware. We also propose a benchmark consisting of 97 control-flow-related cases to validate the control-flow recovery of the generated intermediate representation. Furthermore, we model the cross-language communication mechanisms of React Native to expand the static analysis coverage for native-side code. Our evaluation demonstrates that ReuNify enables an average increase of 84% in reached nodes within the call graph and further identifies an average of two additional privacy leaks in taint analysis. In summary, this paper demonstrates that ReuNify significantly improves the static analysis for the React Native Android apps",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404299423",
    "type": "article"
  },
  {
    "title": "A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms - RCR Report",
    "doi": "https://doi.org/10.1145/3702985",
    "publication_date": "2024-11-14",
    "publication_year": 2024,
    "authors": "Hichem Belgacem; Xiaochen Li; Domenico Bianculli; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "This paper represents the Replicated Computational Results (RCR) related to our TOSEM paper “A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms”, where we proposed LAFF, an approach to automatically suggest possible values of categorical fields in data entry forms, which is a common user interface feature in many software systems. In this RCR report, we provide details about our replication package. We make available the different scripts needed to fully replicate the results obtained in our paper.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404373849",
    "type": "article"
  },
  {
    "title": "Certified Cost Bounds for Abstract Programs",
    "doi": "https://doi.org/10.1145/3705298",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Elvira Albert; Reiner Hähnle; Alicia Merayo Corcoba; Dominic Steinhöfel",
    "corresponding_authors": "",
    "abstract": "A program containing placeholders for unspecified statements or expressions is called an abstract (or schematic) program. Placeholder symbols occur naturally in program transformation rules, as used in refactoring, compilation or optimization. Static cost analysis derives the precise cost –or upper and lower bounds for it– of executing programs, as functions in terms of the program’s input data size. We present a generalization of automated cost analysis that can handle abstract programs and, hence, can analyze the impact on the cost effect of program transformations . This kind of relational property requires provably precise cost bounds which are not always produced by cost analysis. Therefore, we certify by deductive verification that the inferred abstract cost bounds are correct and sufficiently precise. It is the first approach solving this problem. Both, abstract cost analysis and certification, are based on quantitative abstract execution (QAE) which in turn is a variation of abstract execution, a recently developed symbolic execution technique for abstract programs. To realize QAE the new concept of a cost invariant is introduced. QAE is implemented and runs fully automatically on a benchmark set consisting of representative optimization rules.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404584034",
    "type": "article"
  },
  {
    "title": "EffFix: Efficient and Effective Repair of Pointer Manipulating Programs",
    "doi": "https://doi.org/10.1145/3705310",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Yuntong Zhang; Andreea Costea; Ridwan Shariffdeen; Davin McCall; Abhik Roychoudhury",
    "corresponding_authors": "",
    "abstract": "This work introduces EffFix, a tool that applies a novel static analysis-driven Automated Program Repair (APR) technique for fixing memory errors. APR tools typically rely on a given test-suite to guide the repair process. Apart from the need to provide test oracles, this reliance is also one of the main contributors to the over-fitting problem. Static analysis based APR techniques bypass these issues only to introduce new ones, such as soundness, scalability, and generalizability. This work demonstrates how we can overcome these challenges and achieve sound memory bug repair at scale by leveraging static analysis (specifically Incorrectness Separation Logic – ISL) to guide repair. This is the first repair approach to use ISL. Our key insight is that the abstract domain used by static analysis to detect the bugs also contains key information to derive correct patches. Our proposed approach learns what a desirable patch is by inspecting how close a patch is to fixing the bug based on the feedback from ISL based static analysis (specifically the Pulse analyzer), and turning this information into a distribution of probabilities over context free grammars. This approach to repair is generic in that its learning strategy allows for finding patches without relying on the commonly used patch templates. Furthermore, to achieve efficient program repair, instead of focusing on heuristics for reducing the search space of patches, we make repair scalable by creating classes of equivalent patches according to the effect they have on the symbolic heap. We then conduct candidate patch validation only once per patch equivalence class. This allows EffFix to efficiently discover quality repairs even in the presence of a large pool of patch candidates. Experimental evaluation of fixing real world memory errors in medium to large scale subjects like OpenSSL, Linux Kernel, swoole, shows the efficiency and effectiveness of EffFix — in terms of automatically producing repairs from large search spaces. In particular, EffFix has a fix ratio of 66% for memory leak bugs and 83% for Null Pointer Dereferences for the considered dataset.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404584343",
    "type": "article"
  },
  {
    "title": "ROSE: An IDE-Based Interactive Repair Framework for Debugging",
    "doi": "https://doi.org/10.1145/3705306",
    "publication_date": "2024-11-22",
    "publication_year": 2024,
    "authors": "Steven P. Reiss; Xuan Wei; J P Yuan; Qi Xin",
    "corresponding_authors": "",
    "abstract": "Debugging is costly. Automated program repair (APR) holds the promise of reducing its cost by automatically fixing errors. However, current techniques are not easily applicable in a realistic debugging scenario because they assume a high-quality test suite and frequent program re-execution, have low repair efficiency, and only handle a limited set of errors. To improve the practicality of APR for debugging, we propose ROSE, an interactive repair framework that is able to suggest quick and effective repairs of semantic errors while debugging in an Integrated Development Environment (IDE). ROSE allows an easy integration of existing APR patch generators and can do program repair without assuming the existence of a test suite and without requiring program re-execution. It works in conjunction with an IDE debugger and assumes a debugger stopping point where a problem symptom is observed. ROSE asks the developer to quickly describe the symptom. Then it uses the stopping point, the identified symptom, and the current environment to identify potentially faulty lines, uses a variety of APR techniques to suggest repairs at those lines, and validates those repairs without re-executing the program. Finally, it presents the results so the developer can examine, select, and make the appropriate repair. ROSE uses novel approaches to achieve effective fault localization and patch validation without a test suite or program re-execution. For fault localization, ROSE builds on a fast abstract-interpretation-based flow analysis to compute a static backward slice approximating the real dynamic slice while taking into account the symptom and the current execution. For patch validation without re-running the program, ROSE generates simulated traces based on a live-programming system for both the original and repaired executions and compares the traces with respect to the problem symptoms to infer patch correctness. We implemented a prototype of ROSE that works in an Eclipse-based IDE and evaluated its potency and utility with an effectiveness study and a user study. We found that ROSE's fault localization and validation are highly effective and a ROSE-based tool using existing APR patch generators generated correct repair suggestions for many errors in only seconds. Moreover, the user study demonstrated that ROSE was helpful for debugging and developers liked to use it.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404635185",
    "type": "article"
  },
  {
    "title": "Understanding the OSS Communities of Deep Learning Frameworks: A Comparative Case Study of <scp>PyTorch</scp> and <scp>TensorFlow</scp>",
    "doi": "https://doi.org/10.1145/3705303",
    "publication_date": "2024-11-23",
    "publication_year": 2024,
    "authors": "Ye Chen; Zhiyuan Wan; Zhuang Yue; Ning Liu; David Lo; Xiaohu Yang",
    "corresponding_authors": "",
    "abstract": "Over the past two decades, deep learning has received tremendous success in developing software systems across various domains. Deep learning frameworks have been proposed to facilitate the development of such software systems, among which, PyTorch and TensorFlow stand out as notable examples. Considerable attention focuses on exploring software engineering practices and addressing diverse technical aspects in developing and deploying deep learning frameworks and software systems. Despite these efforts, little is known about the open-source software communities involved in the development of deep learning frameworks. In this paper, we perform a comparative investigation into the open-source software communities of the two representative deep learning frameworks, PyTorch and TensorFlow. To facilitate the investigation, we compile a dataset of 2,792 and 3,288 code commit authors, along with 9,826 and 19,750 participants engaged in issue events on GitHub, from the two communities, respectively. With the dataset, we first characterize the structures of the two communities by employing four operationalizations to classify contributors into various roles and inspect the contributions made by common contributors across the two communities. We then conduct a longitudinal analysis to characterize the evolution of the two communities across various releases, in terms of the numbers of contributors with various roles and role transitions among contributors. Finally, we explore the causal effects between community characteristics and the popularity of the two frameworks. We find that the TensorFlow community harbors a larger base of contributors, encompassing a higher proportion of core developers and a more extensive cohort of active users compared to the PyTorch community. In terms of the technical background of the developers, 64.4% and 56.1% developers in the PyTorch and TensorFlow communities are employed by the leading companies of the corresponding open-source software projects, Meta and Google, respectively. 25.9% and 21.9% core developers in the PyTorch and TensorFlow communities possess Ph.D. degrees, while 77.2% and 77.7% contribute to other machine learning or deep learning open-source projects, respectively. Developers contributing to both communities demonstrate spatial and temporal similarities to some extent in their pull requests across the respective projects. The evolution of contributors with various roles exhibits a consistent upward trend over time in the PyTorch community. Conversely, a noticeable turning point in the growth of contributors characterizes the evolution of the TensorFlow community. Both communities show a statistically significant decreasing trend in the inflow rates of core developers. Furthermore, we observe statistically significant causal effects between the expansion of communities and retention of core developers and the popularity of deep learning frameworks. Based on our findings, we discuss implications, provide recommendations for sustaining open-source software communities of deep learning frameworks, and outline directions for future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404645070",
    "type": "article"
  },
  {
    "title": "Editorial: The end of the journey",
    "doi": "https://doi.org/10.1145/3705711",
    "publication_date": "2024-12-02",
    "publication_year": 2024,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "Dear readers, in almost all editorials I talked about volunteers dropping out of our Editorial Board and new ones coming in. It may sound like a lot of fluctuation among the members of the Editorial Board, but it is not. Associate Editors start with a two-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404927164",
    "type": "editorial"
  },
  {
    "title": "Beyond Cohesion and Coupling: Integrating Control Flow in Software Modularization Process for Better Code Comprehensibility",
    "doi": "https://doi.org/10.1145/3707452",
    "publication_date": "2024-12-06",
    "publication_year": 2024,
    "authors": "Babak Pourasghar; Habib Izadkhah; Maryam Akhtari",
    "corresponding_authors": "",
    "abstract": "As software systems evolve to meet the changing needs of users, understanding the source code becomes a critical step in the process. Clustering techniques, also known as modularization techniques, offer a solution to breaking down complex source code into smaller, more manageable parts. This facilitates improved analysis and understanding of the software’s structure. However, the effectiveness of clustering algorithms in code understanding heavily relies on the chosen criteria. While existing methods typically consider cohesion, coupling, and balance between clusters, we argue that these criteria alone may not fully satisfy one of the primary objectives of clustering, which is to enhance understanding. This is because spaghetti-like structures can be created even when these criteria are satisfied. To address this issue, we introduce two new criteria incorporating program control flow to regulate cluster dependencies. By controlling the uniformity of input and output directions, as well as the distribution of inputs and outputs, clustering algorithms can generate clusters that are more developer-friendly and easier to comprehend. We provide intuitive explanations and real-world projects to demonstrate the effectiveness of our approach, and also incorporate feedback from academics and expert programmers. This paper reveals that integrating these new criteria into existing clustering algorithms enables developers to gain deeper insights into the structure of software systems. This, in turn, leads to better design decisions and improved developer understanding of the source code.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405109386",
    "type": "article"
  },
  {
    "title": "Refining code-line-level bugginess identification: Getting the best of both worlds by fusing syntactic and semantic features",
    "doi": "https://doi.org/10.1145/3707456",
    "publication_date": "2024-12-07",
    "publication_year": 2024,
    "authors": "Yufei Zhou; Haihua Tang; Longtao Zhu; Hao Ding; Junyan Qian",
    "corresponding_authors": "",
    "abstract": "Background: Code-line-level bugginess identification (CLBI) is an important area within software quality assurance, aiming to pinpoint potential buggy source code lines in a given software product. Recently, two concurrent approaches, GLANCE and DeepLineDP, have showcased impressive performance by respectively leveraging syntactic and semantic features compared with existing state-of-the-art (SOTA) approaches in this field. Problem: Yet, the literature lacks a thorough investigation that fuses these two types of features to enhance CLBI. Such fusion holds the promise of significantly improving the efficacy of identifying defective lines. Objective: We aim to advance CLBI by fusing syntactic and semantic features, thereby harnessing their respective strengths. Method: We propose to build a CLBI approach, SPLICE (boo S ting dee P Linedp w I th synta C tic f E atures), by fusing syntactic features from GLANCE and semantic features from DeepLineDP. SPLICE comprises three variants—SPLICE-S, SPLICE-G, and SPLICE-F—each utilizing a unique line-level sorting approach. We make a comprehensive comparison with existing SOTA approaches using six performance metrics. Result: Through an analysis of nine open-source projects, our experimental results reveal that SPLICE is competitive with current SOTA CLBI approaches. Notably, SPLICE-F demonstrates superiority over all SOTA CLBI approaches, including GLANCE and DeepLineDP, across all six metrics, indicating a substantial improvement. Conclusion: This discovery underscores the critical importance of future CLBI research in fusing syntactic and semantic features to construct more effective bugginess identification approaches. It is worth noting that the analysis was conducted within the context of Java programs, which highlights the potential for exploring similar methods in other programming languages in future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405138042",
    "type": "article"
  },
  {
    "title": "What Could Possibly Go Wrong: Undesirable Patterns in Collective Development",
    "doi": "https://doi.org/10.1145/3707451",
    "publication_date": "2024-12-09",
    "publication_year": 2024,
    "authors": "Mikhail Evtikhiev; Ekaterina Koshchenko; Vladimir Kovalenko",
    "corresponding_authors": "",
    "abstract": "Software development, often perceived as a technical endeavour, is fundamentally a social activity requiring collaboration among team members. Acknowledging this, the software development community has devised strategies to address possible collaboration-related shortcomings. Various studies have attempted to capture the social dynamics within software engineering. These studies developed methods to identify numerous teamwork issues and proposed various approaches to address them. However, there is a need for a comprehensive bottom-up exploration from practitioner’s perceptions to common patterns. This paper introduces the concept of undesirable patterns in collective development, referring to potential teamwork problems that may escalate if unaddressed. Through 38 in-depth exploratory interviews, we identify and classify 42 patterns, revealing their origins and consequences. To the best of our knowledge, some patterns, like Teamwork pipeline bottleneck , were never reported before. Subsequent surveys, 436 and 968 participants each, explore the significance and frequency of the undesirable patterns, and evaluate potential tools and features to manage these patterns. The study contributes a nuanced understanding of undesirable patterns, evaluating their impact and proposing pragmatic tools and features for industrial application. The findings provide a valuable foundation for further in-depth studies and the development of tools to enhance collaborative software engineering practices.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405185588",
    "type": "article"
  },
  {
    "title": "TG-CUP: A Transformer and GNN Based Multi-Modal Comment Updating Method",
    "doi": "https://doi.org/10.1145/3708474",
    "publication_date": "2024-12-12",
    "publication_year": 2024,
    "authors": "Y. Chen; Yuan Huang; Xiangping Chen; Zibin Zheng",
    "corresponding_authors": "",
    "abstract": "Comments play a crucial role in code comprehension and maintenance. This is particularly vital when the code is changed, as comments should be promptly updated to maintain consistency between the code and the comments. Existing comment update methods usually treat code as natural language text, ignore the information of code structure, and often fail when code changes are not associated with comment updates (called a non-code-indicative update, i.e., NCIU). Therefore, we propose a Transformer and graph neural network based comment update method (TG-CUP). The model integrates the information of old comments, code edit sequences, and AST-Difference Graph to update outdated comments. The experimental results show that TG-CUP increased by 5.16% and 2.23% compared with the most advanced methods on Accuracy and Recall@5, and the performance on NCIUs is improved as well.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405316411",
    "type": "article"
  },
  {
    "title": "A Systematic Literature Review of Multi-Label Learning in Software Engineering",
    "doi": "https://doi.org/10.1145/3708532",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Joonas Hämäläinen; Teerath Das; Tommi Mikkonen",
    "corresponding_authors": "",
    "abstract": "In this paper, we provide the first systematic literature review of the intersection of two research areas, Multi-Label Learning (MLL) and Software Engineering (SE). We refer to this intersection as MLL4SE. In recent years, MLL problems have increased in many applications and research areas because real-world datasets often have a multi-label nature. For multi-label data, simplifying the assumption of traditional classification approaches that an instance can only be associated with one class only leads to worse accuracy. Thus, a better match of methods and assumptions about the data is required. We identified 50 primary studies in our systematic literature review in the MLL4SE domain. Based on this review, we identified six main SE application domains where MLL has been applied. These domains include Software Requirement Engineering, Issue Tracking and Management, Community and Knowledge Management, API Usage and Management, Code Quality and Maintenance, and Mobile Application Development . We summarized the methods used and the data nature of the MLL4SE applications. Moreover, we separately provide taxonomies of future work directions from machine learning and software engineering perspectives. In general, we highlight current trends, research gaps, and shortcomings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405544346",
    "type": "article"
  },
  {
    "title": "<scp>MiniScope</scp> : Automated UI Exploration and Privacy Inconsistency Detection of MiniApps via Two-phase Iterative Hybrid Analysis",
    "doi": "https://doi.org/10.1145/3709351",
    "publication_date": "2024-12-21",
    "publication_year": 2024,
    "authors": "Shenao Wang; Yuekang Li; Kailong Wang; Yi Liu; Hui Li; Yang Liu; Haoyu Wang",
    "corresponding_authors": "",
    "abstract": "The advent of MiniApps, operating within larger SuperApps, has revolutionized user experiences by offering a wide range of services without the need for individual app downloads. However, this convenience has raised significant privacy concerns, as these MiniApps often require access to sensitive data, potentially leading to privacy violations. Despite existing privacy regulations and platform guidelines, there is a lack of effective mechanisms to safeguard user privacy fully. To address this critical gap, we introduce MiniScope , a novel two-phase hybrid analysis approach, specifically designed for the MiniApp environment. This approach overcomes the limitations of existing static analysis techniques by incorporating UI transition states analysis, cross-package callback control flow resolution, and automated iterative UI exploration. This allows for a comprehensive understanding of MiniApps’ privacy practices, addressing the unique challenges of sub-package loading and event-driven callbacks. Our empirical evaluation of over 120K MiniApps using MiniScope demonstrates its effectiveness in identifying privacy inconsistencies. The results reveal significant issues, with 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data collection. We have responsibly disclosed our findings to 2,282 developers, receiving 44 acknowledgments. These findings emphasize the urgent need for more precise privacy monitoring systems and highlight the responsibility of SuperApp operators to enforce stricter privacy measures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405673538",
    "type": "article"
  },
  {
    "title": "An Empirical Study on Common Sense-Violating Bugs in Mobile Apps",
    "doi": "https://doi.org/10.1145/3709356",
    "publication_date": "2024-12-21",
    "publication_year": 2024,
    "authors": "Fu Fan; Yanjie Jiang; T Chen; H. B. Zhang; Yuxia Zhang; Nan Niu; Hui Liu",
    "corresponding_authors": "",
    "abstract": "Mobile applications are widely used by billions of users in their daily work and life. Such GUI software is prone to bugs, potentially degrading user experience. Notably, many bugs in mobile apps are reported by end-users who cannot access the requirements of the app or test cases accompanied by explicitly specified test oracles. It may suggest that such bugs are not identified in the traditional way, i.e., by comparing the actual behaviors of the apps against their expected behaviors explicitly specified in the requirements or test cases. Instead, such bugs are often identified by comparing the actual behaviors against users’ common knowledge of apps, noted as common sense. We refer to such bugs as common sense-violating bugs. Although it is well-known that common sense-violating bugs are common in mobile apps, it remains unclear how popular they are and what kind of common sense principles are violated by them, let alone the relationship among the violated common sense principles. To this end, in this paper, we conduct the first large-scale empirical study on common sense-violating bugs in open-source mobile apps. We manually analyzed 2,808 real-world bug reports across 948 open-sourced mobile apps on GitHub. Our analysis results suggest that 1,006 (35.8%) out of the 2,808 bugs pertain to common sense-violating bugs. From those common sense-violating bugs, we identified a set of common sense principles violated by the buggy behaviors, and built a taxonomy for the common sense principles. Such principles fall into three categories: UI content-related common sense principles, UI layout-related common sense principles, and interaction-related common sense principles. By analyzing the frequency of the common sense principles being violated, we observed that a small set of common sense principles were frequently violated by the majority of common sense-violating bugs: 18 common sense principles, accounting for only 5% of the violated common sense principles, were violated by more than half of the common sense-violating bugs. These findings suggest that identifying the most frequent common sense-violating bugs could be achieved by using a small set of critical common sense principles, which may significantly reduce the cost of common sense-based bug detection. Finally, to demonstrate the feasibility of automated bug detection with common sense-based test oracles, we propose an automated approach to validating whether a given test run violates the most frequently violated common sense principle: No raw error message. Our evaluation results suggest that the automated approach is accurate, whose precision and recall are 91.3% and 91.6%, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405679694",
    "type": "article"
  },
  {
    "title": "RuMono: Fuzz Driver Synthesis for Rust Generic APIs",
    "doi": "https://doi.org/10.1145/3709359",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Yehong Zhang; Jun Wu; Hui Xu",
    "corresponding_authors": "",
    "abstract": "Fuzzing is a popular technique for detecting bugs, which can be extended to libraries by constructing executables that call library APIs, known as fuzz drivers. Automated fuzz driver synthesis has been an important research topic in recent years since it can facilitate the library fuzzing process. Nevertheless, existing approaches generally ignore generic APIs or simply treat them as non-generic APIs. As a result, they cannot generate effective fuzz drivers for generic APIs. This paper explores the challenge of automating fuzz driver synthesis for Rust libraries with generic APIs. The problem is essential because Rust prioritizes security and generic APIs are widely employed in Rust libraries. We propose a novel approach and develop a prototype, RuMono, to tackle the problem. Our approach initially infers the API reachability from the generic API dependency graph, discovering the reachable and valid monomorphic APIs within the library. Further, we apply a similarity-based filter to eliminate redundant monomorphic APIs. Experimental results from 29 popular open-source libraries demonstrate that RuMono can achieve promising generic API coverage with a low rate of invalid fuzz drivers. Besides, we have identified 23 previously unknown bugs in these libraries, with 18 related to generic APIs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405703843",
    "type": "article"
  },
  {
    "title": "Validity-Preserving Delta Debugging via Generator Trace Reduction",
    "doi": "https://doi.org/10.1145/3705305",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Luyao Ren; Xing Zhang; Ziyue Hua; Yanyan Jiang; Xiao He; Yingfei Xiong; Tao Xie",
    "corresponding_authors": "",
    "abstract": "Reducing test inputs that trigger bugs is crucial for efficient debugging. Delta debugging is the most popular approach for this purpose. When test inputs need to conform to certain specifications, existing delta debugging practice encounters a validity problem: it blindly applies reduction rules, producing a large number of invalid test inputs that do not satisfy the required specifications. This overall diminishing effectiveness and efficiency becomes even more pronounced when the specifications extend beyond syntactical structures. Our key insight is that we should leverage input generators, which are aware of these specifications, to generate valid reduced inputs, rather than straightforwardly performing reduction on test inputs. In this paper, we propose a generator-based delta debugging method, namely GReduce, which derives validity-preserving reducers. Specifically, given a generator and its execution, demonstrating how the bug-inducing test input is generated, GReduce searches for other executions on the generator that yield reduced, valid test inputs. The evaluation results on five benchmarks (i.e., graphs, DL models, JavaScript programs, SymPy, and algebraic data types) show that GReduce substantially outperforms state-of-the-art syntax-based reducers including Perses and T-PDD, and also outperforms QuickCheck, SmartCheck, as well as the state-of-the-art choice-sequence-based reducer Hypothesis, demonstrating the effectiveness, efficiency, and versatility of GReduce.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405706648",
    "type": "article"
  },
  {
    "title": "A Practical Approach to Verification of Floating-Point C/C++ Programs with math.h/cmath Functions",
    "doi": "https://doi.org/10.1145/3410875",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Roberto Bagnara; Michele Chiari; Roberta Gori; Abramo Bagnara",
    "corresponding_authors": "",
    "abstract": "Verification of C++ programs has seen considerable progress in several areas, but not for programs that use these languages' mathematical libraries. The reason is that all libraries in widespread use come with no guarantees about the computed results. This would seem to prevent any attempt at formal verification of programs that use them: without a specification for the functions, no conclusion can be drawn statically about the behavior of the program. We propose an alternative to surrender. We introduce a pragmatic approach that leverages the fact that most math.h/cmath functions are almost piecewise monotonic: as we discovered through exhaustive testing, they may have glitches, often of very small size and in small numbers. We develop interval refinement techniques for such functions based on a modified dichotomic search, that enable verification via symbolic execution based model checking, abstract interpretation, and test data generation. Our refinement algorithms are the first in the literature to be able to handle non-correctly rounded function implementations, enabling verification in the presence of the most common implementations. We experimentally evaluate our approach on real-world code, showing its ability to detect or rule out anomalous behaviors.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3115076626",
    "type": "article"
  },
  {
    "title": "Coverage-directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations",
    "doi": "https://doi.org/10.1145/3510416",
    "publication_date": "2022-04-19",
    "publication_year": 2022,
    "authors": "Pengbo Nie; Chengcheng Wan; Jiayu Zhu; Ziyi Lin; Yuting Chen; Zhendong Su",
    "corresponding_authors": "",
    "abstract": "Secure Sockets Layer (SSL) and Transport Security (TLS) are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert , have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. This article tackles this challenge by proposing transcert , a coverage-directed technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates and (2) use code coverage to direct certificate mutation toward generating a set of diverse certificates. The generated certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We implement transcert and evaluate it against frankencert , NEZHA , and RFCcert (three advanced fuzzing techniques) on five widely used SSL/TLS implementations. The evaluation results clearly show the strengths of transcert : During 10,000 iterations, transcert reveals 71 unique validation differences, 12×, 1.4×, and 7× as many as those revealed by frankencert , NEZHA , and RFCcert , respectively; it also supplements RFCcert in conformance testing of the SSL/TLS implementations against 120 validation rules, 85 of which are exclusively covered by transcert -generated certificates. We identify 17 root causes of validation differences, all of which have been confirmed and 11 have never been reported previously. The transcert -generated X.509 certificates also reveal that the primary goal of certificate chain validation is stated ambiguously in the widely adopted public key infrastructure standard RFC 5280.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4224270324",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1125808",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Two translations from activity diagrams to the input language of NuSMV, a symbolic model verifier, are presented. Both translations map an activity diagram into a finite state machine and are inspired by existing statechart semantics. The requirements-...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4256561689",
    "type": "paratext"
  },
  {
    "title": "Aide-mémoire: Improving a Project’s Collective Memory via Pull Request–Issue Links",
    "doi": "https://doi.org/10.1145/3542937",
    "publication_date": "2022-06-06",
    "publication_year": 2022,
    "authors": "Profir-Petru Pârţachi; David R. White; Earl T. Barr",
    "corresponding_authors": "",
    "abstract": "Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-mémoire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-mémoire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project’s lifetime and continuously improve traceability. Aide-mémoire is tailored for two specific instances of the general traceability problem—namely, commit to issue and pull request to issue links, with a focus on the latter—and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-mémoire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4281700386",
    "type": "article"
  },
  {
    "title": "Verification of Programs Sensitive to Heap Layout",
    "doi": "https://doi.org/10.1145/3508363",
    "publication_date": "2022-06-27",
    "publication_year": 2022,
    "authors": "Henrich Lauko; Lukáš Korenčik; Petr Ročkai",
    "corresponding_authors": "",
    "abstract": "Most C and C++ programs use dynamically allocated memory (often known as a heap) to store and organize their data. In practice, it can be useful to compare addresses of different heap objects, for instance, to store them in a binary search tree or a sorted array. However, comparisons of pointers to distinct objects are inherently ambiguous: The address order of two objects can be reversed in different executions of the same program, due to the nature of the allocation algorithm and other external factors. This poses a significant challenge to program verification, since a sound verifier must consider all possible behaviors of a program, including an arbitrary reordering of the heap. A naive verification of all possibilities, of course, leads to a combinatorial explosion of the state space: For this reason, we propose an under-approximating abstract domain that can be soundly refined to consider all relevant heap orderings. We have implemented the proposed abstract domain and evaluated it against several existing software verification tools on a collection of pointer-manipulating programs. In many cases, existing tools only consider a single fixed heap order, which is a source of unsoundness. We demonstrate that using our abstract domain, this unsoundness can be repaired at only a very modest performance cost. Additionally, we show that, even though many verifiers ignore it, ambiguous behavior is present in a considerable fraction of programs from software verification competition ( sv-comp ).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4283582377",
    "type": "article"
  },
  {
    "title": "Graded Refinement, Retrenchment, and Simulation",
    "doi": "https://doi.org/10.1145/3534116",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Richard Banach",
    "corresponding_authors": "Richard Banach",
    "abstract": "Refinement of formal system models towards implementation has been a mainstay of system development since the inception of formal and Correct by Construction approaches to system development. However, pure refinement approaches do not always deal fluently with all desirable system requirements. This prompted the development of alternatives and generalizations, such as retrenchment. The crucial concept of simulation is key to judging the quality of the conformance between abstract and more concrete system models. Reformulations of these theoretical approaches are reprised and are embedded in a graded framework. The added flexibility this offers is intended to deal more effectively with the needs of applications in which the relationship between different levels of abstraction is not straightforward, and in which behavior can oscillate between conforming quite closely to an idealized abstraction and deviating quite far from it. The framework developed is confronted with an intentionally demanding case study: a model active control system for the protection of buildings during earthquakes. This offers many challenges: it is hybrid/cyber-physical; it has to respond to rather unpredictable inputs; and it has to straddle the gap between continuous behavior and discretized/quantized/numerical implementation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4293065287",
    "type": "article"
  },
  {
    "title": "<scp>DeltaDroid</scp> : Dynamic Delivery Testing in Android",
    "doi": "https://doi.org/10.1145/3563213",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Negar Ghorbani; Reyhaneh Jabbarvand; Navid Salehnamadi; Joshua Garcia; Sam Malek",
    "corresponding_authors": "",
    "abstract": "Android is a highly fragmented platform with a diverse set of devices and users. To support the deployment of apps in such a heterogeneous setting, Android has introduced dynamic delivery —a new model of software deployment in which optional, device- or user-specific functionalities of an app, called Dynamic Feature Modules (DFMs) , can be installed, as needed, after the app’s initial installation. This model of app deployment, however, has exacerbated the challenges of properly testing Android apps. In this article, we first describe the results of an extensive study in which we formalized a defect model representing the various conditions under which DFM installations may fail. We then present DeltaDroid —a tool aimed at assisting the developers with validating dynamic delivery behavior in their apps by augmenting their existing test suite. Our experimental evaluation using real-world apps corroborates DeltaDroid ’s ability to detect many crashes and unexpected behaviors that the existing automated testing tools cannot reveal.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4296131098",
    "type": "article"
  },
  {
    "title": "Simulating Software Evolution to Evaluate the Reliability of Early Decision-making among Design Alternatives toward Maintainability",
    "doi": "https://doi.org/10.1145/3569931",
    "publication_date": "2022-10-31",
    "publication_year": 2022,
    "authors": "Chris Karanikolas; Grigoris Dimitroulakos; Κωνσταντίνος Μασσέλος",
    "corresponding_authors": "",
    "abstract": "Critical decisions among design altern seventh atives with regards to maintainability arise early in the software design cycle. Existing comparison models relayed on the structural evolution of the used design patterns are suitable to support such decisions. However, their effectiveness on predicting maintenance effort is usually verified on a limited number of case studies under heterogeneous metrics. In this article, a multi-variable simulation model for validating the decision-making reliability of the derived formal comparison models for the significant designing problem of recursive hierarchies of part-whole aggregations, proposed in our prior work, is introduced. In the absence of a strict validation, the simulation model has been thoroughly calibrated concerning its decision-making precision based on empirical distributions from time-series analysis, approximating the highly uncertain nature of actual maintenance process. The decision reliability of the formal models has been statistically validated on a sample of 1,000 instances of design attributes representing the entire design space of the problem. Despite the limited accuracy of measurements, the results show that the models demonstrate an increasing reliability in a long-term perspective, even under assumptions of high variability. Thus, the modeling theory discussed in our prior work delivers reliable models that significantly reduce decision-risk and relevant maintenance cost.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4307886466",
    "type": "article"
  },
  {
    "title": "Mitigating Debugger-based Attacks to Java Applications with Self-Debugging",
    "doi": "https://doi.org/10.1145/3631971",
    "publication_date": "2024-01-25",
    "publication_year": 2024,
    "authors": "Davide Pizzolotto; Stefano Berlato; Mariano Ceccato",
    "corresponding_authors": "",
    "abstract": "Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode. In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them. We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391222181",
    "type": "article"
  },
  {
    "title": "Dynamic Transitive Closure-Based Static Analysis through the Lens of Quantum Search",
    "doi": "https://doi.org/10.1145/3644389",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Jiawei Ren; Yulei Sui; Xiao Cheng; Yuan Feng; Jianjun Zhao",
    "corresponding_authors": "",
    "abstract": "Many existing static analysis algorithms suffer from cubic bottlenecks because of the need to compute a dynamic transitive closure (DTC). For the first time, this article studies the quantum speedups on searching subtasks in DTC-based static analysis algorithms using quantum search (e.g., Grover’s algorithm). We first introduce our oracle implementation in Grover’s algorithm for DTC-based static analysis and illustrate our quantum search subroutine. Then, we take two typical DTC-based analysis algorithms: context-free-language reachability and set constraint-based analysis, and show that our quantum approach can reduce the time complexity of these two algorithms to truly subcubic ( \\(O(N^2\\sqrt {N}{\\it polylog}(N))\\) ), yielding better results than the upper bound ( O ( N 3 /log N )) of existing classical algorithms. Finally, we conducted a classical simulation of Grover’s search to validate our theoretical approach, due to the current quantum hardware limitation of lacking a practical, large-scale, noise-free quantum machine. We evaluated the correctness and efficiency of our approach using IBM Qiskit on nine open-source projects and randomly generated edge-labeled graphs/constraints. The results demonstrate the effectiveness of our approach and shed light on the promising direction of applying quantum algorithms to address the general challenges in static analysis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391568804",
    "type": "article"
  },
  {
    "title": "BatFix: Repairing language model-based transpilation",
    "doi": "https://doi.org/10.1145/3658668",
    "publication_date": "2024-04-12",
    "publication_year": 2024,
    "authors": "Daniel Ramos; Inês Lynce; Vasco Manquinho; Ruben Martins; Claire Le Goues",
    "corresponding_authors": "",
    "abstract": "To keep up with changes in requirements, frameworks, and coding practices, software organizations might need to migrate code from one language to another. Source-to-source migration, or transpilation, is often a complex, manual process. Transpilation requires expertise both in the source and target language, making it highly laborious and costly. Languages models for code generation and transpilation are becoming increasingly popular. However, despite capturing code-structure well, code generated by language models is often spurious and contains subtle problems. We propose BatFix , a novel approach that augments language models for transpilation by leveraging program repair and synthesis to fix the code generated by these models. BatFix takes as input both the original program, the target program generated by the machine translation model, and a set of test cases and outputs a repaired program that passes all test cases. Experimental results show that our approach is agnostic to language models and programming languages. BatFix can locate bugs spawning multiple lines and synthesize patches for syntax and semantic bugs for programs migrated from Java to C++ and Python to C++ from multiple language models, including, OpenAI’s Codex .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394790510",
    "type": "article"
  },
  {
    "title": "Editorial: ICSE and the Incredible Contradictions of Software Engineering",
    "doi": "https://doi.org/10.1145/3656301",
    "publication_date": "2024-04-18",
    "publication_year": 2024,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "This report gives an overview of the 6th ICSE Workshop on Component-Based Software Engineering held at 25th International Conference on Software Engineering. The workshop brought together researchers and practitioners from three communities: component ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394918619",
    "type": "editorial"
  },
  {
    "title": "Graphuzz: Data-driven Seed Scheduling for Coverage-guided Greybox Fuzzing",
    "doi": "https://doi.org/10.1145/3664603",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Hang Xu; Liheng Chen; Shuitao Gan; Chao Zhang; Z.J. Li; Jiangan Ji; Baojian Chen; Fan Hu",
    "corresponding_authors": "",
    "abstract": "Seed scheduling is a critical step of greybox fuzzing, which assigns different weights to seed test cases during seed selection, and significantly impacts the efficiency of fuzzing. Existing seed scheduling strategies rely on manually designed models to estimate the potentials of seeds and determine their weights, which fails to capture the rich information of a seed and its execution and thus the estimation of seeds’ potentials is not optimal. In this article, we introduce a new seed scheduling solution, Graphuzz, for coverage-guided greybox fuzzing, which utilizes deep learning models to estimate the potentials of seeds and works in a data-driven way. Specifically, we propose an extended control flow graph called e-CFG to represent the control-flow and data-flow features of a seed's execution, which is suitable for graph neural networks (GNN) to process and estimate seeds’ potential. We evaluate each seed's code coverage increment and use it as the label to train the GNN model. Further, we propose a self-attention mechanism to enhance the GNN model so that it can capture overlooked features. We have implemented a prototype of Graphuzz based on the baseline fuzzer AFLplusplus. The evaluation results show that our model can estimate the potential of seeds and has the robust capability to generalize to different targets. Furthermore, the evaluation using 12 benchmarks from FuzzBench shows that Graphuzz outperforms AFLplusplus and the state-of-the-art seed scheduling solution K-Scheduler and other coverage-guided fuzzers in terms of code coverage, and the evaluation using 8 benchmarks from Magma shows that Graphuzz outperforms the baseline fuzzer AFLplusplus and SOTA solutions in terms of bug detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4398183498",
    "type": "article"
  },
  {
    "title": "Code to Qed, the Project Manager's Guide to Proof Engineering",
    "doi": "https://doi.org/10.1145/3664807",
    "publication_date": "2024-06-04",
    "publication_year": 2024,
    "authors": "Nicolas Dejon; Chrystel Gaber; Gilles Grimaud; Narjes Jomaa",
    "corresponding_authors": "",
    "abstract": "Despite growing efforts and encouraging successes in recent decades, fully formally verified projects are still rare in the industrial landscape. The industry often lacks the tools and methodologies to efficiently scale the proof development process. In this work, we give a comprehensible overview of the proof development process for proof developers and project managers. The goal is to support proof developers by rationalizing the proof development process, which currently relies heavily on their intuition and expertise, and by facilitating communication with the management line. To this end, we concentrate on the aspect of proof manufacturing and highlight the most significant sources of proof effort. We propose means to mitigate the latter through proof practices (proof structuring, proof strategies, and proof planning), proof metrics, and tools. Our approach is project-agnostic, independent of specific proof expertise, and computed estimations do not assume prior similar developments. We evaluate our guidelines using a separation kernel undergoing formal verification, driving the proof process in an optimised way. Feedback from a project manager unfamiliar with proof development confirms the benefits of detailed planning of the proof development steps, clear progress communication to the hierarchy line, and alignment with established practices in the software industry.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399320743",
    "type": "article"
  },
  {
    "title": "An Empirical Study on the Characteristics of Database Access Bugs in Java Applications",
    "doi": "https://doi.org/10.1145/3672449",
    "publication_date": "2024-06-13",
    "publication_year": 2024,
    "authors": "Wei Liu; Shouvick Mondal; Tse-Hsun Chen",
    "corresponding_authors": "",
    "abstract": "Database-backed applications rely on the database access code to interact with the underlying database management systems (DBMSs). Although many prior studies aim at database access issues like SQL anti-patterns or SQL code smells, there is a lack of study of database access bugs during the maintenance of database-backed applications. In this paper, we empirically investigate 423 database access bugs collected from seven large-scale Java open source applications that use relational database management systems (e.g., MySQL or PostgreSQL). We study the characteristics (e.g., occurrence and root causes) of the bugs by manually examining the bug reports and commit histories. We find that the number of reported database and non-database access bugs share a similar trend but their modified files in bug fixing commits are different. Additionally, we generalize categories of the root causes of database access bugs, containing five main categories (SQL queries, Schema, API, Configuration, SQL query result) and 25 unique root causes. We find that the bugs pertaining to SQL queries, Schema, and API cover 84.2% of database access bugs across all studied applications. In particular, SQL queries bug (54%) and API bug (38.7%) are the most frequent issues when using JDBC and Hibernate, respectively. Finally, we provide a discussion on the implications of our findings for developers and researchers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399619591",
    "type": "article"
  },
  {
    "title": "Fairness Concerns in App Reviews: A Study on AI-based Mobile Apps",
    "doi": "https://doi.org/10.1145/3690633",
    "publication_date": "2024-08-29",
    "publication_year": 2024,
    "authors": "Ali Rezaei Nasab; Maedeh Dashti; Mojtaba Shahin; Mansooreh Zahedi; Hourieh Khalajzadeh; Chetan Arora; Peng Liang",
    "corresponding_authors": "",
    "abstract": "Fairness is one of the socio-technical concerns that must be addressed in software systems. Considering the popularity of mobile software applications (apps) among a wide range of individuals worldwide, mobile apps with unfair behaviors and outcomes can affect a significant proportion of the global population, potentially more than any other type of software system. Users express a wide range of socio-technical concerns in mobile app reviews. This research aims to investigate fairness concerns raised in mobile app reviews. Our research focuses on AI-based mobile app reviews as the chance of unfair behaviors and outcomes in AI-based mobile apps may be higher than in non-AI-based apps. To this end, we first manually constructed a ground-truth dataset, including 1,132 fairness and 1,473 non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning models that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing model can detect fairness reviews with a precision of 94%. We then applied the best-performing model on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness reviews, followed by manual analysis, led to the identification of six distinct types of fairness concerns (e.g., ‘ receiving different quality of features and services in different platforms and devices ’ and ‘ lack of transparency and fairness in dealing with user-generated content ’). Finally, the manual analysis of 2,248 app owners’ responses to the fairness reviews identified six root causes (e.g., ‘copyright issues’) that app owners report to justify fairness concerns.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401993539",
    "type": "article"
  },
  {
    "title": "Bounded Verification of Atomicity Violations for Interrupt-Driven Programs via Lazy Sequentialization",
    "doi": "https://doi.org/10.1145/3705311",
    "publication_date": "2024-11-22",
    "publication_year": 2024,
    "authors": "Yuan Zhang; Lei Qu; Yifei Wu; Leihuan Wu; Tingting Yu; Rui Chen; Weiqiang Kong",
    "corresponding_authors": "",
    "abstract": "Detecting atomicity violations effectively in interrupt-driven programs is difficult due to the asymmetric concurrency interleaving of interrupts. Current approaches face two main challenges: (1) A large number of false positives are generated by efficient static analysis techniques. (2) Loops with large or unknown bounds in these programs limit the scalability of the bounded verification techniques. To address these challenges, we present NIChecker, a new bounded verification tool designed to detect atomicity violations in interrupt-driven programs. The key ideas are: (1) Transforming an interrupt-driven program into a bounded sequential C program through lazy sequentialization technique. This sequential program accurately models interrupt masking and nested interrupt execution. (2) Combining a refined loop abstraction technique with our sequentialization to enhance the efficiency of detecting programs with intractable loops. (3) Integrating slicing and an interleaving path reduction technique known as preemption point reduction in NIChecker to shrink the explored state space. We prove the bounded correctness of our translation and discuss the impact of our optimizations. We evaluate NIChecker on 31 academic benchmark programs and 18 real-world interrupt-driven programs. Our results show that NIChecker achieves better precision, a lower false positive rate, and a significant verification speed-up than related state-of-the-art tools.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404635223",
    "type": "article"
  },
  {
    "title": "Automated Quantum Protocol Verification Based on Concurrent Dynamic Quantum Logic",
    "doi": "https://doi.org/10.1145/3708475",
    "publication_date": "2024-12-12",
    "publication_year": 2024,
    "authors": "Canh Minh; Tsubasa Takagi; Kazuhiro Ogata",
    "corresponding_authors": "",
    "abstract": "While constructing practical quantum computers by big companies remains a challenge, the application of quantum communication and cryptography has made remarkable progress. Therefore, it is crucial to verify quantum protocols before they can be trusted in safety and security-critical applications. We have proposed Basic Dynamic Quantum Logic (BDQL) to formalize and verify sequential models of quantum protocols with a support tool developed in Maude. However, BDQL does not support concurrency in its formalization. This paper introduces Concurrent Dynamic Quantum Logic (CDQL), an extension of BDQL, to formalize and verify concurrent models of quantum protocols. CDQL is more expressive than BDQL by considering concurrent behavior and communication among participants in quantum protocols. Since CDQL is a conservative extension of BDQL, we extend the syntax of BDQL to CDQL and make a transformation from CDQL to BDQL without interrupting the semantics of BDQL. We also extend the implementation of BDQL to support CDQL, making a new support tool in Maude. The new support tool is equipped with a lazy rewriting strategy to make the verification process significantly faster. Several quantum communication protocols are successfully formalized and verified in BDQL/CDQL, demonstrating the effectiveness of our automated approach and tool in verifying quantum protocols.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405316245",
    "type": "article"
  },
  {
    "title": "Editorial: TOSEM journal in 2025 and beyond",
    "doi": "https://doi.org/10.1145/3708477",
    "publication_date": "2024-12-13",
    "publication_year": 2024,
    "authors": "Abhik Roychoudhury",
    "corresponding_authors": "Abhik Roychoudhury",
    "abstract": "TOSEM is ACM’s flagship journal for publishing software engineering research. TOSEM stays true to the foundations of the discipline while meaningfully engaging with the wave of disruptive innovations in the field. In this light, we discuss the plans for TOSEM in 2025. We discuss how we plan to continue to engage broadly with authors as well as the community as a whole.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405366061",
    "type": "editorial"
  },
  {
    "title": "Leveraging Data Characteristics for Bug Localization in Deep Learning Programs",
    "doi": "https://doi.org/10.1145/3708473",
    "publication_date": "2024-12-16",
    "publication_year": 2024,
    "authors": "Ruchira Manke; Mohammad Wardat; Foutse Khomh; Hridesh Rajan",
    "corresponding_authors": "",
    "abstract": "Deep Learning (DL) is a class of machine learning algorithms that are used in a wide variety of applications. Like any software system, DL programs can have bugs. To support bug localization in DL programs, several tools have been proposed in the past. As most of the bugs that occur due to improper model structure known as structural bugs lead to inadequate performance during training, it is challenging for developers to identify the root cause and address these bugs. To support bug detection and localization in DL programs, in this paper, we propose Theia, which detects and localizes structural bugs in DL programs. Unlike the previous works, Theia considers the training dataset characteristics to automatically detect bugs in DL programs developed using two deep learning libraries, Keras and PyTorch . Since training the DL models is a time-consuming process, Theia detects these bugs at the beginning of the training process and alerts the developer with informative messages containing the bug's location and actionable fixes which will help them to improve the structure of the model. We evaluated Theia on a benchmark of 40 real-world buggy DL programs obtained from Stack Overflow . Our results show that Theia successfully localizes 57/75 structural bugs in 40 buggy programs, whereas NeuraLint, a state-of-the-art approach capable of localizing structural bugs before training localizes 17/75 bugs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405444248",
    "type": "article"
  },
  {
    "title": "Errata",
    "doi": "https://doi.org/10.1145/287000.287031",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Robert J. Allen; David Garlan",
    "corresponding_authors": "",
    "abstract": "We present corrections to a previously published article which appeared in ACM Transaction on Software Engineering and Methodology 6, 3 (July 1997), pp. 213–249",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1982365866",
    "type": "article"
  },
  {
    "title": "Addendum to “Delta algorithms: an empirical analysis”",
    "doi": "https://doi.org/10.1145/292182.292200",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "James J. Hunt; Walter F. Tichy",
    "corresponding_authors": "",
    "abstract": "The authors supply machine configurations for experiments reported in “Delta Algorithms: An Empirical Analysis,” by Hunt et al. (ACM Trans. Softw. Eng. Methodol. 7, 2 (Apr. 1998), pp. 192-214).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2020391899",
    "type": "article"
  },
  {
    "title": "Deciding Type-Based Partial-Order Constraints for Path-Sensitive Analysis",
    "doi": "https://doi.org/10.1145/2755971",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "Elena Sherman; Brady J. Garvin; Matthew B. Dwyer",
    "corresponding_authors": "",
    "abstract": "The precision and scalability of path-sensitive program analyses depend on their ability to distinguish feasible and infeasible program paths. Analyses express path feasibility as the satisfiability of conjoined branch conditions, which is then decided by cooperating decision procedures such as those in satisfiability modulo theory (SMT) solvers. Consequently, efficient underlying decision procedures are key to precise, scalable program analyses. When we investigate the branch conditions accumulated by inter-procedural path-sensitive analyses of object-oriented programs, we find that many relate to an object's dynamic type. These conditions arise from explicit type tests and the branching implicit in dynamic dispatch and type casting. These conditions share a common form that comprises a fragment of the theory of partial orders, which we refer to as type-based partial orders (TPO) . State-of-the-art SMT solvers can heuristically instantiate the quantified formulae that axiomatize partial orders, and thereby support TPO constraints. We present two custom decision procedures with significantly better performance. On benchmarks that reflect inter-procedural path-sensitive analyses applied to significant Java systems, the custom procedures run three orders of magnitude faster. The performance of the two decision procedures varies across benchmarks, which suggests that a portfolio approach may be beneficial for solving constraints generated by program analyses.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1163856976",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on ISSTA 2013",
    "doi": "https://doi.org/10.1145/2809789",
    "publication_date": "2015-09-02",
    "publication_year": 2015,
    "authors": "Mark Harman; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "editorial Free AccessIntroduction to the Special Issue on ISSTA 2013 Editors: Mark Harman University College London, UK University College London, UKView Profile , Mauro Pezzé University of Milano Bicocca and University of Lugano University of Milano Bicocca and University of LuganoView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 24Issue 4August 2015 Article No.: 21pp 1–3https://doi.org/10.1145/2809789Published:02 September 2015Publication History 0citation219DownloadsMetricsTotal Citations0Total Downloads219Last 12 Months10Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1968927610",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue International Conference on Software Engineering (ICSE 2012)",
    "doi": "https://doi.org/10.1145/2658849",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Gail C. Murphy; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2055746044",
    "type": "article"
  },
  {
    "title": "External Dependencies-Driven Architecture Discovery and Analysis of Implemented Systems",
    "doi": null,
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Dharmalingam Ganesan; Mikael Lindvall; Monica Ron",
    "corresponding_authors": "",
    "abstract": "A method for architecture discovery and analysis of implemented systems (AIS) is disclosed. The premise of the method is that architecture decisions are inspired and influenced by the external entities that the software system makes use of. Examples of such external entities are COTS components, frameworks, and ultimately even the programming language itself and its libraries. Traces of these architecture decisions can thus be found in the implemented software and is manifested in the way software systems use such external entities. While this fact is often ignored in contemporary reverse engineering methods, the AIS method actively leverages and makes use of the dependencies to external entities as a starting point for the architecture discovery. The AIS method is demonstrated using the NASA's Space Network Access System (SNAS). The results show that, with abundant evidence, the method offers reusable and repeatable guidelines for discovering the architecture and locating potential risks (e.g. low testability, decreased performance) that are hidden deep in the implementation. The analysis is conducted by using external dependencies to identify, classify and review a minimal set of key source code files. Given the benefits of analyzing external dependencies as a way to discover architectures, it is argued that external dependencies deserve to be treated as first-class citizens during reverse engineering. The current structure of a knowledge base of external entities and analysis questions with strategies for getting answers is also discussed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2788575229",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2582050",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In today's volatile business environments, collaboration between information systems, both within and across company borders, has become essential to success. An efficient supply chain, for example, requires the collaboration of distributed and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229625208",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2943790",
    "publication_date": "2016-08-22",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Model-driven engineering promotes models as main development artifacts. As several models may be manipulated during the software-development life cycle, model transformations ensure their consistency by automating model generation and update tasks. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231695459",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2702120",
    "publication_date": "2014-12-23",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We introduce the concept of residual investigation for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233213706",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2668018",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Margaret‐Anne Storey; David S. Rosenblum",
    "corresponding_authors": "Margaret‐Anne Storey",
    "abstract": "Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234512441",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2820114",
    "publication_date": "2015-09-02",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques that produce those test suites. Researchers frequently compare test suites by measuring their coverage. A ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234815336",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2904898",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "David S. Rosenblum",
    "corresponding_authors": "David S. Rosenblum",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235539170",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2676679",
    "publication_date": "2014-10-14",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Over the last few years, the software engineering community has proposed a number of modeling methods to represent functional requirements. Among them, use cases are recognized as an easy to use and intuitive way to capture and define such requirements. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236879808",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2656368",
    "publication_date": "2014-09-05",
    "publication_year": 2014,
    "authors": "Margaret‐Anne Storey; David S. Rosenblum",
    "corresponding_authors": "Margaret‐Anne Storey",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242414656",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2600788",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Testing database applications typically requires the generation of tests consisting of both program inputs and database states. Recently, a testing technique called Dynamic Symbolic Execution (DSE) has been proposed to reduce manual effort in test ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242912106",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2776776",
    "publication_date": "2015-05-13",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246188544",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2852270",
    "publication_date": "2015-12-02",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Integer overflow bugs in C and C++ programs are difficult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for finding these bugs exist, the situation is complicated because not all overflows are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247914074",
    "type": "paratext"
  },
  {
    "title": "EDITORIAL",
    "doi": "https://doi.org/10.1145/2559939",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "David Rosenblum",
    "corresponding_authors": "David Rosenblum",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249544413",
    "type": "editorial"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2581373",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "André van der Hoek; Alex Orso; Corina Ǎs Ǎreanu; Lori Pollock; Antonia Bertolino; Vittorio Cortellessa; Jin Dong; Sarfraz Khurshid; Neno Medvidović; Nasko Rountev; David S. Rosenblum",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Editorial Editor: David S. Rosenblum View Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 23Issue 2Article No.: 11pp 1–4https://doi.org/10.1145/2581373Published:04 April 2014Publication History 0citation0DownloadsMetricsTotal Citations0Total Downloads0Last 12 Months0Last 6 weeks0 Get Citation Alerts Save to binder temporarily unavailable - maintenance in progressSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250530338",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2628068",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Automatic test case generation for software programs is very powerful but suffers from a key limitation. That is, most current test case generation techniques fail to cover testee code when covering that code requires additional pieces of code not yet ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252797514",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2913009",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Web applications have become one of the fastest-growing types of software systems today. Despite their popularity, understanding the behavior of modern web applications is still a challenging endeavor for developers during development and maintenance ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254073138",
    "type": "paratext"
  },
  {
    "title": "Verification of Program Transformations with Inductive Refinement Types",
    "doi": "https://doi.org/10.1145/3409805",
    "publication_date": "2021-01-20",
    "publication_year": 2021,
    "authors": "Ahmad Salim Al-Sibahi; Thomas Jensen; Aleksandar S. Dimovski; Andrzej Wąsowski",
    "corresponding_authors": "",
    "abstract": "High-level transformation languages like Rascal include expressive features for manipulating large abstract syntax trees: first-class traversals, expressive pattern matching, backtracking, and generalized iterators. We present the design and implementation of an abstract interpretation tool,Rabit, for verifying inductive type and shape properties for transformations written in such languages. We describe how to perform abstract interpretation based on operational semantics, specifically focusing on the challenges arising when analyzing the expressive traversals and pattern matching.Finally, we evaluate Rabit on a series of transformations (normalization, desugaring, refactoring, code generators, type inference, etc.) showing that we can effectively verify stated properties.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3125267634",
    "type": "article"
  },
  {
    "title": "Lattice-Based Sampling for Path Property Monitoring",
    "doi": "https://doi.org/10.1145/2063239.2063244",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Madeline Diep; Matthew B. Dwyer; Sebastian Elbaum",
    "corresponding_authors": "",
    "abstract": "Runtime monitoring can provide important insights about a program’s behavior and, for simple properties, it can be done efficiently. Monitoring properties describing sequences of program states and events, however, can result in significant runtime overhead. This is particularly critical when monitoring programs deployed at user sites that have low tolerance for overhead. In this paper we present a novel approach to reducing the cost of runtime monitoring of path properties. A set of original properties are composed to form a single integrated property that is then systematically decomposed into a set of properties that encode necessary conditions for property violations. The resulting set of properties forms a lattice whose structure is exploited to select a sample of properties that can lower monitoring cost, while preserving violation detection power relative to the original properties. The lattice is then complemented with a weighting scheme that assigns each property a different priority that can be adjusted continuously to better drive the property sampling process. Our evaluation using the Hibernate API reveals that our approach produces a rich, structured set of properties that enables control of monitoring overhead, while detecting more violations more quickly than alternative techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2046485856",
    "type": "article"
  },
  {
    "title": "In memoriam",
    "doi": "https://doi.org/10.1145/2491509.2491510",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "David S. Rosenblum",
    "corresponding_authors": "David S. Rosenblum",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2131730270",
    "type": "article"
  },
  {
    "title": "A Combinatorial Approach to Detecting Buffer Overflow Vulnerabilities | NIST",
    "doi": null,
    "publication_date": "2011-06-14",
    "publication_year": 2011,
    "authors": "Raghu N. Kacker; Yu Lei; David R. Kuhn; Wenhua Wang",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2736809307",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2000799",
    "publication_date": "2011-09-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article studies runtime verification of properties expressed either in lineartime temporal logic (LTL) or timed lineartime temporal logic (TLTL). It classifies runtime verification in identifying its distinguishing features to model checking and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232984689",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2063239",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The complexity of designing programs that simultaneously tolerate multiple classes of faults, called multitolerant programs, is in part due to the conflicting nature of the fault tolerance requirements that must be met by a multitolerant program when ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238699062",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2522920",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Service-oriented applications do not fully benefit from standard APIs yet, and many applications fail to use interchangeably all the services that implement a standard service API. This article presents an approach to develop adaptation strategies that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241199660",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2000791",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Provenance refers to the past processes that brought about a given (version of an) object, item or entity. By knowing the provenance of data, users can often better understand, trust, reproduce, and validate it. A provenance-aware application has the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241667829",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2491509",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to detect because static analysis typically cannot precisely identify these redundant references, and existing ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252860963",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2430536",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modeling and verifying complex real-time systems are challenging research problems. The de facto approach is based on Timed Automata, which are finite state automata equipped with clock variables. Timed Automata are deficient in modeling hierarchical ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255030261",
    "type": "paratext"
  },
  {
    "title": "A framework for the checking and refactoring of crosscutting concepts",
    "doi": "https://doi.org/10.1145/2211616.2211618",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Macneil Shonle; William G. Griswold; Sorin Lerner",
    "corresponding_authors": "",
    "abstract": "Programmers employ crosscutting concepts, such as design patterns and other programming idioms, when their design ideas cannot be efficiently or effectively modularized in the underlying programming language. As a result, implementations of these crosscutting concepts can be hard to change even when the code is well structured. In this article, we describe Arcum, a system that supports the modular maintenance of crosscutting concepts. Arcum can be used to both check essential constraints of crosscutting concepts and to substitute crosscutting concept implementations with alternative implementations. Arcum is complementary to existing refactoring systems that focus on meaning-preserving program transformations at the programming-language-semantics level, because Arcum focuses on transformations at the conceptual level. We present the underpinnings of the Arcum approach and show how Arcum can be used to address several classical software engineering problems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2250223391",
    "type": "article"
  },
  {
    "title": "EDITORIAL",
    "doi": "https://doi.org/10.1145/2089116.2089117",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231012755",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2211616",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Software product line engineering is an efficient means of generating a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231208038",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1525880.1525881",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231357984",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1824760",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231977025",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1538942",
    "publication_date": "2009-07-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233088023",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1571629",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237018329",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1555392",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "J-Orchestra is a system that enhances centralized Java programs with distribution capabilities. Operating at the bytecode level, J-Orchestra transforms a centralized Java program (i.e., running on a single Java Virtual Machine (JVM)) into a distributed ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239322326",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1734229",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Web services are increasingly gaining acceptance as a framework for facilitating application-to-application interactions within and across enterprises. It is commonly accepted that a service description should include not only the interface, but also ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243425147",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2089116",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The building of highly cohesive classes is an important objective in object-oriented design. Class cohesion refers to the relatedness of the class members, and it indicates one important aspect of the class design quality. A meaningful class cohesion ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247223684",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1525880",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Program monitors enforce security policies by interposing themselves into the control flow of untrusted software whenever that software attempts to execute security-relevant actions. At the point of interposition, a monitor has authority to permit or ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247785536",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1656250.1656251",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249823354",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1656250",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "There are many examples in the literature of algorithms for synthesizing state machines from scenario-based models. The motivation for these is to automate the transition from scenario-based requirements to early behavioral design models. To date, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249858642",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2377656",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many software reuse tasks involve reusing source code that was not designed in a manner conducive to those tasks, requiring that ad hoc modifications be applied. Such pragmatic reuse tasks are a reality in disciplined industrial practice; they arise for ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254182953",
    "type": "paratext"
  },
  {
    "title": "Introduction to the special section from the ACM international symposium on software testing and analysis (ISSTA 2006)",
    "doi": "https://doi.org/10.1145/1348250.1348252",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "David Notkin; Mauro Pezzè",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2004706431",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1416563",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Although graphical user interfaces (GUIs) constitute a large part of the software being developed today and are typically created using rapid prototyping, there are no effective regression testing techniques for GUIs. The needs of GUI regression testing ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231811160",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3136621",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Michael R. Lyu; David Rosenblum; R Michael",
    "corresponding_authors": "Michael R. Lyu",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234615954",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1276933",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The main drawback of existing software artifact management systems is the lack of automatic or semi-automatic traceability link generation and maintenance. We have improved an artifact management system with a traceability recovery tool based on Latent ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235387196",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1314493.1314494",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235959057",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3007747",
    "publication_date": "2017-05-05",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The abundance of event data in today’s information systems makes it possible to “confront” process models with the actual observed behavior. Process mining techniques use event logs to discover process models that describe the observed behavior, and to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237592725",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1348250.1348251",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240360951",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1314493",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Software reengineering is a costly endeavor, due in part to the ambiguity of where to focus reengineering effort. Coupling and Cohesion metrics, particularly quantitative cohesion metrics, have the potential to aid in this identification and to measure ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240715450",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3129287",
    "publication_date": "2017-10-05",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We carried out a family of controlled experiments to investigate whether the use of abbreviated identifier names, with respect to full-word identifier names, affects fault fixing in C and Java source code. This family consists of an original (or ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245759974",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1391984",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Visual dataflow languages (VDFLs), which include commercial and research systems, have had a substantial impact on end-user programming. Like any other programming languages, whether visual or textual, VDFLs often contain faults. A desire to provide ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246215426",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1363102",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247726177",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1217295.1237801",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Mauro Pezzè; Andreas Zeller; J Whitehead; Carol Matsumoto; Alessandra Carol",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247850781",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1348250",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Finite-state verification techniques are often hampered by the state-explosion problem. One proposed approach for addressing this problem is assume-guarantee reasoning, where a system under analysis is partitioned into subsystems and these subsystems ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248736172",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1217295",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Programs are increasingly organized around features, which are encapsulated using aspects and other linguistic mechanisms. Despite their growing popularity amongst developers, there is a dearth of techniques for computer-aided verification of programs ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248801283",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1189748",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A software modification task often addresses several concerns. A concern is anything a stakeholder may want to consider as a conceptual unit, including features, nonfunctional requirements, and design idioms. In many cases, the source code implementing ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249623008",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3092955",
    "publication_date": "2017-07-20",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "When testing data processing systems, software engineers often use real-world data to perform system-level testing. However, in the presence of new data requirements, software engineers may no longer benefit from having real-world data with which to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251319323",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1189748.1189750",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Carlo Ghezzi",
    "corresponding_authors": "Carlo Ghezzi",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252272705",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/13487689",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Most software reliability growth models work under the assumption that reliability of software grows due to the removal of bugs that cause failures. However, another phenomenon has often been observed—the failure rate of a software product following its ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254061956",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1363102.1363103",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "David Notkin",
    "corresponding_authors": "David Notkin",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254842972",
    "type": "editorial"
  },
  {
    "title": "EDITORIAL: Announcing Six TOSEM Issues Per Year",
    "doi": "https://doi.org/10.1145/3583569",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "As the eagle-eyed among you may have noticed, ACM TOSEM has a new editorial charter. Approved by both the TOSEM editorial board and the ACM Publications Board, it is now posted to the TOSEM website.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4362566372",
    "type": "editorial"
  },
  {
    "title": "A Hypothesis Testing-based Framework for Software Cross-modal Retrieval in Heterogeneous Semantic Spaces",
    "doi": "https://doi.org/10.1145/3591868",
    "publication_date": "2023-04-10",
    "publication_year": 2023,
    "authors": "Hongwei Wei; Xiaohong Su; Cuiyun Gao; Weining Zheng; Wenxin Tao",
    "corresponding_authors": "",
    "abstract": "Software cross-modal retrieval is a popular yet challenging direction, such as bug localization and code search. Previous studies generally map natural language texts and codes into a homogeneous semantic space for similarity measurement. However, it is not easy to accurately capture their similar semantics in a homogeneous semantic space due to the semantic gap. Therefore, we propose to map the multi-modal data into heterogeneous semantic spaces to capture their unique semantics. Specifically, we propose a novel software cross-modal retrieval framework named Deep Hypothesis Testing (DeepHT). In DeepHT, to capture the unique semantics of the code’s control flow structure, all control flow paths (CFPs) in the control flow graph are mapped to a CFP sample set in the sample space. Meanwhile, the text is mapped to a CFP correlation distribution in the distribution space to model its correlation with different CFPs. The matching score is calculated according to how well the sample set obeys the distribution using hypothesis testing. The experimental results on two text-to-code retrieval tasks (i.e., bug localization and code search) and two code-to-text retrieval tasks (i.e., vulnerability knowledge retrieval and historical patch retrieval) show that DeepHT outperforms the baseline methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4363678238",
    "type": "article"
  },
  {
    "title": "<i>Adonis</i> : Practical and Efficient Control Flow Recovery through OS-level Traces",
    "doi": "https://doi.org/10.1145/3607187",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Xuanzhe Liu; Chengxu Yang; Ding Li; Yuhan Zhou; Shaofei Li; Jiali Chen; Zhenpeng Chen",
    "corresponding_authors": "",
    "abstract": "Control flow recovery is critical to promise the software quality, especially for large-scale software in production environment. However, the efficiency of most current control flow recovery techniques is compromised due to their runtime overheads along with deployment and development costs. To tackle this problem, we propose a novel solution, Adonis , which harnesses Operating System (OS) -level traces, such as dynamic library calls and system call traces, to efficiently and safely recover control flows in practice. Adonis operates in two steps: It first identifies the call-sites of trace entries, and then it executes a pairwise symbolic execution to recover valid execution paths. This technique has several advantages. First, Adonis does not require the insertion of any probes into existing applications, thereby minimizing runtime cost . Second, given that OS-level traces are hardware-independent, Adonis can be implemented across various hardware configurations without the need for hardware-specific engineering efforts, thus reducing deployment cost . Third, as Adonis is fully automated and does not depend on manually created logs, it circumvents additional development cost . We conducted an evaluation of Adonis on representative desktop applications and real-world IoT applications. Adonis can faithfully recover the control flow with 86.8% recall and 81.7% precision. Compared to the state-of-the-art log-based approach, Adonis can not only cover all the execution paths recovered but also recover 74.9% of statements that cannot be covered. In addition, the runtime cost of Adonis is 18.3× lower than the instrument-based approach; the analysis time and storage cost (indicative of the deployment cost) of Adonis is 50× smaller and 443× smaller than the hardware-based approach, respectively. To facilitate future replication and extension of this work, we have made the code and data publicly available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4383102486",
    "type": "article"
  },
  {
    "title": "Assessing effectiveness of test suites: what do we know and what should we do?",
    "doi": "https://doi.org/10.1145/3635713",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Peng Zhang; Yang Wang; Xutong Liu; Zeyu Lu; Yibiao Yang; Yanhui Li; Lin Chen; Ziyuan Wang; Chang‐ai Sun; Xiao Yu; Yuming Zhou",
    "corresponding_authors": "",
    "abstract": "Background. Software testing is a critical activity for ensuring the quality and reliability of software systems. To evaluate the effectiveness of different test suites, researchers have developed a variety of metrics. Problem. However, comparing these metrics is challenging due to the lack of a standardized evaluation framework including comprehensive factors. As a result, researchers often focus on single factors (e.g., size), which finally leads to different or even contradictory conclusions. After comparing dozens of pieces of work in detail, we have found two main problems most troubling to our community: (1) researchers tend to oversimplify the description of the ground truth they use, and (2) data involving real defects is not suitable for analysis using traditional statistical indicators. Objective. We aim at scrutinizing the whole process of comparing test suites for our community. Method. To hit this aim, we propose a framework ASSENT (ev A luating te S t S uite E ffective N ess me T rics) to guide the follow-up research for evaluating a test suite effectiveness metric. ASSENT consists of three fundamental components: ground truth, benchmark test suites, and agreement indicator. Its functioning is as follows: first, users clarify the ground truth for determining the real order in effectiveness among test suites. Second, users generate a set of benchmark test suites and derive their ground truth order in effectiveness. Third, users use the metric to derive the order in effectiveness for the same test suites. Finally, users calculate the agreement indicator between the two orders derived by two metrics. Result. With ASSENT, we are able to compare the accuracy of different test suite effectiveness metrics. We apply ASSENT to evaluate representative test suite effectiveness metrics, including mutation score and code coverage metrics. Our results show that, based on the real faults, mutation score, and subsuming mutation score are the best metrics to quantify test suite effectiveness. Meanwhile, by using mutants instead of real faults, test effectiveness will be overestimated by more than 20% in values. Conclusion. We recommend that the standardized evaluation framework ASSENT should be used for evaluating and comparing test effectiveness metrics in the future work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389337887",
    "type": "article"
  },
  {
    "title": "Learning-based Relaxation of Completeness Requirements for Data Entry Forms",
    "doi": "https://doi.org/10.1145/3635708",
    "publication_date": "2023-12-06",
    "publication_year": 2023,
    "authors": "Hichem Belgacem; Xiaochen Li; Domenico Bianculli; Lionel Briand",
    "corresponding_authors": "",
    "abstract": "Data entry forms use completeness requirements to specify the fields that are required or optional to fill for collecting necessary information from different types of users. However, because of the evolving nature of software, some required fields may not be applicable for certain types of users anymore. Nevertheless, they may still be incorrectly marked as required in the form; we call such fields obsolete required fields. Since obsolete required fields usually have “not-null” validation checks before submitting the form, users have to enter meaningless values in such fields to complete the form submission. These meaningless values threaten the quality of the filled data and could negatively affect stakeholders or learning-based tools that use the data. To avoid users filling meaningless values, existing techniques usually rely on manually written rules to identify the obsolete required fields and relax their completeness requirements. However, these techniques are ineffective and costly. In this article, we propose LACQUER, a learning-based automated approach for relaxing the completeness requirements of data entry forms. LACQUER builds Bayesian Network models to automatically learn conditions under which users had to fill meaningless values. To improve its learning ability, LACQUER identifies the cases where a required field is only applicable for a small group of users and uses SMOTE, an oversampling technique, to generate more instances on such fields for effectively mining dependencies on them. During the data entry session, LACQUER predicts the completeness requirement of a target based on the already filled fields and their conditional dependencies in the trained model. Our experimental results show that LACQUER can accurately relax the completeness requirements of required fields in data entry forms with precision values ranging between 0.76 and 0.90 on different datasets. LACQUER can prevent users from filling 20% to 64% of meaningless values, with negative predictive values (i.e., the ability to correctly predict a field as “optional”) between 0.72 and 0.91. Furthermore, LACQUER is efficient; it takes at most 839 ms to predict the completeness requirement of an instance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389386408",
    "type": "article"
  },
  {
    "title": "Acknowledgement of referees 2004",
    "doi": "https://doi.org/10.1145/1061254.1061259",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "ACM Transactions on Software Engineering and Methodology sta",
    "corresponding_authors": "ACM Transactions on Software Engineering and Methodology sta",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2034906622",
    "type": "article"
  },
  {
    "title": "Fault Classes and Fault Coupling in Boolean Specifications",
    "doi": null,
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Vadim Okun; Paul E. Black; Yelena Yesha",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2731674464",
    "type": "article"
  },
  {
    "title": "Farewell Editorial from the Outgoing Editor-in-Chief",
    "doi": "https://doi.org/10.1145/3301288",
    "publication_date": "2019-01-31",
    "publication_year": 2019,
    "authors": "David S. Rosenblum",
    "corresponding_authors": "David S. Rosenblum",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2917009494",
    "type": "article"
  },
  {
    "title": "Aligning the Views of Research Quality in Empirical Software Engineering",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Jefferson Seide Molléri; Emília Mendes; Kai Petersen; Michael Felderer",
    "corresponding_authors": "",
    "abstract": "Context: Research quality is intended to assess the design and reporting of studies. It comprises a series of concepts such as methodological rigor, practical relevance, and conformance to ethical ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2969519069",
    "type": "article"
  },
  {
    "title": "マスクされたソフトウェア実装のサイドチャネル抵抗の検証と定量化【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Gao Pengfei; Zhang Jun; Song Fu; Wang Chao",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3173251597",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1151695",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In partition analysis we divide the input domain to form subdomains on which the system's behaviour should be uniform. Boundary value analysis produces test inputs near each subdomain's boundaries to find failures caused by incorrect implementation of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230814273",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3339833",
    "publication_date": "2019-07-01",
    "publication_year": 2019,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231571112",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3316413",
    "publication_date": "2019-04-09",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Watchdog timers are devices that are commonly used to monitor the health of safety-critical hardware and software systems. Their primary function is to raise an alarm if the monitored systems fail to emit periodic “heartbeats” that signal their well-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231884644",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3208361",
    "publication_date": "2018-06-05",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Background. Recent years have seen an increasing interest in cross-project defect prediction (CPDP), which aims to apply defect prediction models built on source projects to a target project. Currently, a variety of (complex) CPDP models have been ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234421008",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1061254.1061255",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "Laura Dillon; Paola Inverardi; Mehdi Jazayeri; George S. Avrunin; Mary Jean Harrold; Gail C. Murphy; Oscar Nierstrasz; David S. Rosenblum; K Laura",
    "corresponding_authors": "Laura Dillon",
    "abstract": "editorial Free Access Share on Editorial Editor: Carlo Ghezzi View Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 14Issue 2April 2005 pp 119–123https://doi.org/10.1145/1061254.1061255Published:01 April 2005Publication History 0citation567DownloadsMetricsTotal Citations0Total Downloads567Last 12 Months8Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236143330",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3343019",
    "publication_date": "2019-08-17",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236196540",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3363297",
    "publication_date": "2019-10-10",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239553749",
    "type": "editorial"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3317953",
    "publication_date": "2019-04-05",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243690693",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3177744",
    "publication_date": "2018-02-23",
    "publication_year": 2018,
    "authors": "Khanh Nguyen; Kai Wang; Yingyi Bu; Li Tai Fang; Guoqing Xu",
    "corresponding_authors": "",
    "abstract": "The past decade has witnessed increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer’s choice for ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244173958",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1131421",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The concept of cohesion in a class has been the subject of various recent empirical studies and has been measured using many different metrics. In the structured programming paradigm, the software engineering community has adopted an informal yet ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244296447",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1178625",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "System evolution most often implies the integration of legacy components, such as databases, with newly developed ones, leading to mixed architectures that suffer from severe heterogeneity problems. For instance, incorporating a new program in a legacy ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244581064",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3177743",
    "publication_date": "2018-01-12",
    "publication_year": 2018,
    "authors": "Joshua Garcia; Mahmoud Hammad; Sam Malek",
    "corresponding_authors": "",
    "abstract": "We present a criterion for checking local and global deadlock freedom of finite state systems expressed in BIP: a component-based framework for constructing complex distributed systems. Our criterion is evaluated by model-checking a set of subsystems of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245991460",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3264424",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "David S. Rosenblum; D Travis; Travis D. Breaux",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247474513",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3292526",
    "publication_date": "2019-02-23",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Uncertainty in timing properties (e.g., detection time of external events) is a common occurrence in embedded software systems, since these systems interact with complex physical environments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250385474",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3276753",
    "publication_date": "2018-10-08",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "While developers are aware of the importance of comprehensively testing patches, the large effort involved in coming up with relevant test cases means that such testing rarely happens in practice. Furthermore, even when test cases are written to cover ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253270950",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3360049",
    "publication_date": "2019-10-12",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254207549",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3287303",
    "publication_date": "2018-11-22",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading student ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255665915",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3375995",
    "publication_date": "2019-12-16",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We introduce two complementary approaches to monitor decentralized systems. The first approach relies on systems with a centralized specification, i.e., when the specification is written for the behavior of the entire system. To do so, our approach ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256121163",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3205909",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "David S. Rosenblum",
    "corresponding_authors": "David S. Rosenblum",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256275311",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3234930",
    "publication_date": "2018-07-19",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256599352",
    "type": "paratext"
  },
  {
    "title": "自動システムテスト生成におけるSQLデータベースの取り扱い【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Arcuri Andrea; P Galeotti Juan",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3213300604",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3402931",
    "publication_date": "2020-07-04",
    "publication_year": 2020,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233399532",
    "type": "editorial"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3383775",
    "publication_date": "2020-03-20",
    "publication_year": 2020,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256328645",
    "type": "editorial"
  },
  {
    "title": "Reviewers 2002",
    "doi": "https://doi.org/10.1145/839268.839273",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "ACM Transactions on Software Engineering and Methodology sta",
    "corresponding_authors": "ACM Transactions on Software Engineering and Methodology sta",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4214776457",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/839268.839269",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Carlo Ghezzi; Jeffrey N. Magee; Dieter Rombach; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246739563",
    "type": "editorial"
  },
  {
    "title": "Editorial: A Retrospective and Prospective Reflection",
    "doi": "https://doi.org/10.1145/3523278",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4280560300",
    "type": "editorial"
  },
  {
    "title": "Obituary",
    "doi": "https://doi.org/10.1145/606612.606613",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "ACM Transactions on Software Engineering and Methodology sta",
    "corresponding_authors": "ACM Transactions on Software Engineering and Methodology sta",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237683433",
    "type": "paratext"
  },
  {
    "title": "Towards an Anatomy of Software Craftsmanship",
    "doi": "https://doi.org/10.1145/3468504",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Anders Sundelin; Javier González‐Huerta; Krzysztof Wnuk; Tony Gorschek",
    "corresponding_authors": "",
    "abstract": "Context: The concept of software craftsmanship has early roots in computing, and in 2009, the Manifesto for Software Craftsmanship was formulated as a reaction to how the Agile methods were practiced and taught. But software craftsmanship has seldom been studied from a software engineering perspective. Objective: The objective of this article is to systematize an anatomy of software craftsmanship through literature studies and a longitudinal case study. Method: We performed a snowballing literature review based on an initial set of nine papers, resulting in 18 papers and 11 books. We also performed a case study following seven years of software development of a product for the financial market, eliciting qualitative, and quantitative results. We used thematic coding to synthesize the results into categories. Results: The resulting anatomy is centered around four themes, containing 17 principles and 47 hierarchical practices connected to the principles. We present the identified practices based on the experiences gathered from the case study, triangulating with the literature results. Conclusion: We provide our systematically derived anatomy of software craftsmanship with the goal of inspiring more research into the principles and practices of software craftsmanship and how these relate to other principles within software engineering in general.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3204576109",
    "type": "article"
  },
  {
    "title": "H.Y. Chen, T.H. Tse, F.T. Chan, and T.Y. Chen, \"In black and white: an integrated approach to class-level testing of object-oriented programs\"",
    "doi": null,
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "T.H. Tse",
    "corresponding_authors": "T.H. Tse",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3216828889",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3450737",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Mauro Pezzè",
    "corresponding_authors": "Mauro Pezzè",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4211065135",
    "type": "editorial"
  },
  {
    "title": "Comments on “a reduced test suite for protocol conformance testing”",
    "doi": "https://doi.org/10.1145/258077.265733",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Alexandre Petrenko",
    "corresponding_authors": "Alexandre Petrenko",
    "abstract": "A previous ACM TOSEM article of Ph. Bernhard (“A Reduced Test Suite of Protocol Conformance Testing,” ACM Transactions on Software Engineering and Methodology, Vol. 3, No. 3, July 1994, pages 201-220) describes three new versions of the so-called W-method for solving the protocol-testing problem, i.e., solving the Mealy machine equivalence problem. The author claims that these versions all have the same fault detection capability as the original W-method. In this correspondence we prove that the results of that article are incorrect.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2137374629",
    "type": "article"
  },
  {
    "title": "Comments on “the cost of selective recompilation and environment processing”",
    "doi": "https://doi.org/10.1145/210134.210435",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Bevin Brett",
    "corresponding_authors": "Bevin Brett",
    "abstract": "article Free Access Share on Comments on “the cost of selective recompilation and environment processing” Author: Bevin R. Brett Wakatu, Nelson, New Zealand Wakatu, Nelson, New ZealandView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 4Issue 2April 1995 pp 214–216https://doi.org/10.1145/210134.210435Online:01 April 1995Publication History 0citation321DownloadsMetricsTotal Citations0Total Downloads321Last 12 Months2Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2049011770",
    "type": "article"
  },
  {
    "title": "Author Index",
    "doi": "https://doi.org/10.1145/235321.237495",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "Author Index",
    "corresponding_authors": "Author Index",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4205100685",
    "type": "paratext"
  },
  {
    "title": "Authors' response",
    "doi": "https://doi.org/10.1145/210134.210438",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Walter F. Tichy; Rolf Adams; Annette Weinert",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Authors' response Authors: Walter Tichy Univ. of Karlsruhe, Karlsruche, Germany Univ. of Karlsruhe, Karlsruche, GermanyView Profile , Rolf Adams Univ. of Karlsruhe, Karlsruche, Germany Univ. of Karlsruhe, Karlsruche, GermanyView Profile , Annette Weinert Univ. of Karlsruhe, Karlsruche, Germany Univ. of Karlsruhe, Karlsruche, GermanyView Profile Authors Info & Claims ACM Transactions on Software Engineering and MethodologyVolume 4Issue 2April 1995 pp 217–219https://doi.org/10.1145/210134.210438Published:01 April 1995Publication History 0citation277DownloadsMetricsTotal Citations0Total Downloads277Last 12 Months2Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241820229",
    "type": "article"
  }
]