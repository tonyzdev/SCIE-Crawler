[
  {
    "title": "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models",
    "doi": "https://doi.org/10.1145/3434237",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Usman Naseem; Imran Razzak; Shah Khalid Khan; Mukesh Prasad",
    "corresponding_authors": "",
    "abstract": "Word representation has always been an important research area in the history of natural language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP-related tasks. In the end, this survey briefly discusses the commonly used ML- and DL-based classifiers, evaluation metrics, and the applications of these word embeddings in different NLP tasks.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W3175170601",
    "type": "article"
  },
  {
    "title": "A Survey of Knowledge Enhanced Pre-trained Language Models",
    "doi": "https://doi.org/10.1145/3631392",
    "publication_date": "2024-03-01",
    "publication_year": 2024,
    "authors": "Jian Yang; Xinyu Hu; Gang Xiao; Yulong Shen",
    "corresponding_authors": "",
    "abstract": "Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W4392357044",
    "type": "article"
  },
  {
    "title": "Unsupervised Multimodal Machine Translation for Low-resource Distant Language Pairs",
    "doi": "https://doi.org/10.1145/3652161",
    "publication_date": "2024-03-09",
    "publication_year": 2024,
    "authors": "Turghun Tayir; Lin Li",
    "corresponding_authors": "",
    "abstract": "Unsupervised machine translation (UMT) has recently attracted more attention from researchers, enabling models to translate when languages lack parallel corpora. However, the current works mainly consider close language pairs (e.g., English-German and English-French), and the effectiveness of visual content for distant language pairs has yet to be investigated. This article proposes an unsupervised multimodal machine translation model for low-resource distant language pairs. Specifically, we first employ adequate measures such as transliteration and re-ordering to bring distant language pairs closer together. We then use visual content to extend masked language modeling and generate visual masked language modeling for UMT. Finally, empirical experiments are conducted on our distant language pair dataset and the public Multi30k dataset. Experimental results demonstrate the superior performance of our model, with BLEU score improvements of 2.5 and 2.6 on translation for distant language pairs English-Uyghur and Chinese-Uyghur. Moreover, our model also brings remarkable results for close language pairs, improving 2.3 BLEU compared with the existing models in English-German.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4392623252",
    "type": "article"
  },
  {
    "title": "CodeKGC: Code Language Model for Generative Knowledge Graph Construction",
    "doi": "https://doi.org/10.1145/3641850",
    "publication_date": "2024-02-09",
    "publication_year": 2024,
    "authors": "Zhen Bi; Jing Chen; Yinuo Jiang; Feiyu Xiong; Wei Guo; Huajun Chen; Ningyu Zhang",
    "corresponding_authors": "",
    "abstract": "Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provide intermediate steps, thereby improving knowledge extraction abilities. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines. 1",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W4391681328",
    "type": "article"
  },
  {
    "title": "A Hybrid CNN-LSTM: A Deep Learning Approach for Consumer Sentiment Analysis Using Qualitative User-Generated Contents",
    "doi": "https://doi.org/10.1145/3457206",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Praphula Kumar Jain; Vijayalakshmi Saravanan; Rajendra Pamula",
    "corresponding_authors": "",
    "abstract": "With the fastest growth of information and communication technology (ICT), the availability of web content on social media platforms is increasing day by day. Sentiment analysis from online reviews drawing researchers’ attention from various organizations such as academics, government, and private industries. Sentiment analysis has been a hot research topic in Machine Learning (ML) and Natural Language Processing (NLP). Currently, Deep Learning (DL) techniques are implemented in sentiment analysis to get excellent results. This study proposed a hybrid convolutional neural network-long short-term memory (CNN-LSTM) model for sentiment analysis. Our proposed model is being applied with dropout, max pooling, and batch normalization to get results. Experimental analysis carried out on Airlinequality and Twitter airline sentiment datasets. We employed the Keras word embedding approach, which converts texts into vectors of numeric values, where similar words have small vector distances between them. We calculated various parameters, such as accuracy, precision, recall, and F1-measure, to measure the model’s performance. These parameters for the proposed model are better than the classical ML models in sentiment analysis. Our results analysis demonstrates that the proposed model outperforms with 91.3% accuracy in sentiment analysis.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W3186997021",
    "type": "article"
  },
  {
    "title": "Context-aware Emotion Detection from Low-resource Urdu Language Using Deep Neural Network",
    "doi": "https://doi.org/10.1145/3528576",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Muhammad Farrukh Bashir; Abdul Rehman Javed; Muhammad Umair Arshad; Thippa Reddy Gadekallu; Waseem Shahzad; Mirza Omer Beg",
    "corresponding_authors": "",
    "abstract": "Emotion detection (ED) plays a vital role in determining individual interest in any field. Humans use gestures, facial expressions, and voice pitch and choose words to describe their emotions. Significant work has been done to detect emotions from the textual data in English, French, Chinese, and other high-resource languages. However, emotion classification has not been well studied in low-resource languages (i.e., Urdu) due to the lack of labeled corpora. This article presents a publicly available Urdu Nastalique Emotions Dataset ( UNED ) of sentences and paragraphs annotated with different emotions and proposes a deep learning (DL)-based technique for classifying emotions in the UNED corpus. Our annotated UNED corpus has six emotions for both paragraphs and sentences. We perform extensive experimentation to evaluate the quality of the corpus and further classify it using machine learning and DL approaches. Experimental results show that the developed DL-based model performs better than generic machine learning approaches with an F1 score of 85% on the UNED sentence-based corpus and 50% on the UNED paragraph-based corpus.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W4220844134",
    "type": "article"
  },
  {
    "title": "Depression Detection from Social Media Text Analysis using Natural Language Processing Techniques and Hybrid Deep Learning Model",
    "doi": "https://doi.org/10.1145/3569580",
    "publication_date": "2022-11-05",
    "publication_year": 2022,
    "authors": "Vankayala Tejaswini; Korra Sathya Babu; Bibhudatta Sahoo",
    "corresponding_authors": "",
    "abstract": "Depression is a kind of emotion that negatively impacts people's daily lives. The number of people suffering from long-term feelings is increasing every year across the globe. Depressed patients may engage in self-harm behaviors, which occasionally result in suicide. Many psychiatrists struggle to identify the presence of mental illness or negative emotion early to provide a better course of treatment before they reach a critical stage. One of the most challenging problems is detecting depression in people at the earliest possible stage. Researchers are using Natural Language Processing (NLP) techniques to analyze text content uploaded on social media, which helps to design approaches for detecting depression. This work analyses numerous prior studies that used learning techniques to identify depression. The existing methods suffer from better model representation problems to detect depression from the text with high accuracy. The present work addresses a solution to these problems by creating a new hybrid deep learning neural network design with better text representations called “Fasttext Convolution Neural Network with Long Short-Term Memory (FCL).” In addition, this work utilizes the advantage of NLP to simplify the text analysis during the model development. The FCL model comprises fasttext embedding for better text representation considering out-of-vocabulary (OOV) with semantic information, a convolution neural network (CNN) architecture to extract global information, and Long Short-Term Memory (LSTM) architecture to extract local features with dependencies. The present work was implemented on real-world datasets utilized in the literature. The proposed technique provides better results than the state-of-the-art to detect depression with high accuracy.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W4308303479",
    "type": "article"
  },
  {
    "title": "Isolated Arabic Sign Language Recognition Using a Transformer-based Model and Landmark Keypoints",
    "doi": "https://doi.org/10.1145/3584984",
    "publication_date": "2023-02-21",
    "publication_year": 2023,
    "authors": "Sarah Alyami; Hamzah Luqman; Mohammad Hammoudeh",
    "corresponding_authors": "",
    "abstract": "Pose-based approaches for sign language recognition provide light-weight and fast models that can be adopted in real-time applications. This article presents a framework for isolated Arabic sign language recognition using hand and face keypoints. We employed MediaPipe pose estimator for extracting the keypoints of sign gestures in the video stream. Using the extracted keypoints, three models were proposed for sign language recognition: Long-Term Short Memory, Temporal Convolution Networks, and Transformer-based models. Moreover, we investigated the importance of non-manual features for sign language recognition systems and the obtained results showed that combining hand and face keypoints boosted the recognition accuracy by around 4% compared with only hand keypoints. The proposed models were evaluated on Arabic and Argentinian sign languages. Using the KArSL-100 dataset, the proposed pose-based Transformer achieved the highest accuracy of 99.74% and 68.2% in signer-dependent and -independent modes, respectively. Additionally, the Transformer was evaluated on the LSA64 dataset and obtained an accuracy of 98.25% and 91.09% in signer-dependent and -independent modes, respectively. Consequently, the pose-based Transformer outperformed the state-of-the-art techniques on both datasets using keypoints from the signer’s hands and face.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W4321454735",
    "type": "article"
  },
  {
    "title": "Impact of Tokenization on Language Models: An Analysis for Turkish",
    "doi": "https://doi.org/10.1145/3578707",
    "publication_date": "2023-03-25",
    "publication_year": 2023,
    "authors": "Çağrı Toraman; Eyüp Halit Yılmaz; Furkan Şahi̇nuç; Oguzhan Ozcelik",
    "corresponding_authors": "",
    "abstract": "Tokenization is an important text preprocessing step to prepare input tokens for deep language models. WordPiece and BPE are de facto methods employed by important models, such as BERT and GPT. However, the impact of tokenization can be different for morphologically rich languages, such as Turkic languages, in which many words can be generated by adding prefixes and suffixes. We compare five tokenizers at different granularity levels, that is, their outputs vary from the smallest pieces of characters to the surface form of words, including a Morphological-level tokenizer. We train these tokenizers and pretrain medium-sized language models using the RoBERTa pretraining procedure on the Turkish split of the OSCAR corpus. We then fine-tune our models on six downstream tasks. Our experiments, supported by statistical tests, reveal that the morphological-level tokenizer delivers a challenging performance with de facto tokenizers. Furthermore, we find that increasing the vocabulary size improves the performance of Morphological- and Word-level tokenizers more than that of de facto tokenizers. The ratio of the number of vocabulary parameters to the total number of model parameters can be empirically chosen as 20% for de facto tokenizers and 40% for other tokenizers to obtain a reasonable trade-off between model size and performance.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W4360951492",
    "type": "article"
  },
  {
    "title": "A Confusion Method for the Protection of User Topic Privacy in Chinese Keyword-based Book Retrieval",
    "doi": "https://doi.org/10.1145/3571731",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Zongda Wu; Jian Xie; Shigen Shen; Chongze Lin; Guandong Xu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "In this article, aiming at a Chinese keyword-based book search service, from a technological perspective, we propose to modify a user query sequence carefully to confuse the user query topics and thus protect the user topic privacy on the untrusted server, without compromising the accuracy of each book search service. First, we propose a client-based framework for the privacy protection of book search, and then a privacy model to formulate the constraints in terms of accuracy, efficiency, and security, which the cover queries generated based on a user query sequence should meet. Second, we present a modification algorithm for a user query sequence, based on some heuristic strategies, which can quickly generate a cover query sequence meeting the privacy model by replacing, deleting, and adding keywords for each user query. Finally, both theoretical analysis and experimental evaluation demonstrate the effectiveness of the proposed approach, i.e., which can improve the security of users’ topic privacy on the untrusted server without compromising the efficiency, accuracy, and usability of an existing Chinese keyword book search service, so it has a positive impact for the construction of a privacy-preserving text retrieval platform under an untrusted network environment.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4316813696",
    "type": "article"
  },
  {
    "title": "AROMA",
    "doi": "https://doi.org/10.1145/3086575",
    "publication_date": "2017-07-13",
    "publication_year": 2017,
    "authors": "Ahmad Al-Sallab; Ramy Baly; Hazem Hajj; Khaled Shaban; Wassim El‐Hajj; Gilbert Badaro",
    "corresponding_authors": "",
    "abstract": "While research on English opinion mining has already achieved significant progress and success, work on Arabic opinion mining is still lagging. This is mainly due to the relative recency of research efforts in developing natural language processing (NLP) methods for Arabic, handling its morphological complexity, and the lack of large-scale opinion resources for Arabic. To close this gap, we examine the class of models used for English and that do not require extensive use of NLP or opinion resources. In particular, we consider the Recursive Auto Encoder (RAE). However, RAE models are not as successful in Arabic as they are in English, due to their limitations in handling the morphological complexity of Arabic, providing a more complete and comprehensive input features for the auto encoder, and performing semantic composition following the natural way constituents are combined to express the overall meaning. In this article, we propose A R ecursive Deep Learning Model for O pinion M ining in A rabic (AROMA) that addresses these limitations. AROMA was evaluated on three Arabic corpora representing different genres and writing styles. Results show that AROMA achieved significant performance improvements compared to the baseline RAE. It also outperformed several well-known approaches in the literature.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2735966564",
    "type": "article"
  },
  {
    "title": "An Emotion Cause Corpus for Chinese Microblogs with Multiple-User Structures",
    "doi": "https://doi.org/10.1145/3132684",
    "publication_date": "2017-11-02",
    "publication_year": 2017,
    "authors": "Xiyao Cheng; Ying Chen; Bixiao Cheng; Shoushan Li; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "A notably challenging problem in emotion analysis is recognizing the cause of an emotion. Although there have been a few studies on emotion cause detection, most of them work on news reports or a few of them focus on microblogs using a single-user structure (i.e., all texts in a microblog are written by the same user). In this article, we focus on emotion cause detection for Chinese microblogs using a multiple-user structure (i.e., texts in a microblog are successively written by several users). First, based on the fact that the causes of an emotion of a focused user may be provided by other users in a microblog with the multiple-user structure, we design an emotion cause annotation scheme which can deal with such a complicated case, and then provide an emotion cause corpus using the annotation scheme. Second, based on the analysis of the emotion cause corpus, we formalize two emotion cause detection tasks for microblogs (current-subtweet-based emotion cause detection and original-subtweet-based emotion cause detection). Furthermore, in order to examine the difficulty of the two emotion cause detection tasks and the contributions of texts written by different users in a microblog with the multiple-user structure, we choose two popular classification methods (SVM and LSTM) to do emotion cause detection. Our experiments show that the current-subtweet-based emotion cause detection is much more difficult than the original-subtweet-based emotion cause detection, and texts written by different users are very helpful for both emotion cause detection tasks. This study presents a pilot study of emotion cause detection which deals with Chinese microblogs using a complicated structure.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2766095568",
    "type": "article"
  },
  {
    "title": "A Survey of Opinion Mining in Arabic",
    "doi": "https://doi.org/10.1145/3295662",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Gilbert Badaro; Ramy Baly; Hazem Hajj; Wassim El‐Hajj; Khaled Shaban; Nizar Habash; Ahmad Al-Sallab; Ali Hamdi",
    "corresponding_authors": "",
    "abstract": "Opinion-mining or sentiment analysis continues to gain interest in industry and academics. While there has been significant progress in developing models for sentiment analysis, the field remains an active area of research for many languages across the world, and in particular for the Arabic language, which is the fifth most-spoken language and has become the fourth most-used language on the Internet. With the flurry of research activity in Arabic opinion mining, several researchers have provided surveys to capture advances in the field. While these surveys capture a wealth of important progress in the field, the fast pace of advances in machine learning and natural language processing (NLP) necessitates a continuous need for a more up-to-date literature survey. The aim of this article is to provide a comprehensive literature survey for state-of-the-art advances in Arabic opinion mining. The survey goes beyond surveying previous works that were primarily focused on classification models. Instead, this article provides a comprehensive system perspective by covering advances in different aspects of an opinion-mining system, including advances in NLP software tools, lexical sentiment and corpora resources, classification models, and applications of opinion mining. It also presents future directions for opinion mining in Arabic. The survey also covers latest advances in the field, including deep learning advances in Arabic Opinion Mining. The article provides state-of-the-art information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete opinion-mining system. Key insights are captured at the end of each section for particular aspects of the opinion-mining system giving the reader a choice of focusing on particular aspects of interest.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2944780533",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis for a Resource Poor Language—Roman Urdu",
    "doi": "https://doi.org/10.1145/3329709",
    "publication_date": "2019-08-16",
    "publication_year": 2019,
    "authors": "Khawar Mehmood; Daryl Essam; Kamran Shafi; Muhammad Kamran Malik",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis is an important sub-task of Natural Language Processing that aims to determine the polarity of a review. Most of the work done on sentiment analysis is for the resource-rich languages of the world, but very limited work has been done on resource-poor languages. In this work, we focus on developing a Sentiment Analysis System for Roman Urdu, which is a resource-poor language. To this end, a dataset of 11,000 reviews has been gathered from six different domains. Comprehensive annotation guidelines were defined and the dataset was annotated using the multi-annotator methodology. Using the annotated dataset, state-of-the-art algorithms were used to build a sentiment analysis system. To improve the results of these algorithms, four different studies were carried out based on: word-level features, character level features, and feature union. The best results showed that we could reduce the error rate by 12% from the baseline (80.07%). Also, to see if the improvements are statistically significant, we applied t-test and Confidence Interval on the obtained results and found that the best results of each study are statistically significant from the baseline.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2969026449",
    "type": "article"
  },
  {
    "title": "A Sentiment Treebank and Morphologically Enriched Recursive Deep Models for Effective Sentiment Analysis in Arabic",
    "doi": "https://doi.org/10.1145/3086576",
    "publication_date": "2017-07-13",
    "publication_year": 2017,
    "authors": "Ramy Baly; Hazem Hajj; Nizar Habash; Khaled Shaban; Wassim El‐Hajj",
    "corresponding_authors": "",
    "abstract": "Accurate sentiment analysis models encode the sentiment of words and their combinations to predict the overall sentiment of a sentence. This task becomes challenging when applied to morphologically rich languages (MRL). In this article, we evaluate the use of deep learning advances, namely the Recursive Neural Tensor Networks (RNTN), for sentiment analysis in Arabic as a case study of MRLs. While Arabic may not be considered the only representative of all MRLs, the challenges faced and proposed solutions in Arabic are common to many other MRLs. We identify, illustrate, and address MRL-related challenges and show how RNTN is affected by the morphological richness and orthographic ambiguity of the Arabic language. To address the challenges with sentiment extraction from text in MRL, we propose to explore different orthographic features as well as different morphological features at multiple levels of abstraction ranging from raw words to roots. A key requirement for RNTN is the availability of a sentiment treebank; a collection of syntactic parse trees annotated for sentiment at all levels of constituency and that currently only exists in English. Therefore, our contribution also includes the creation of the first Arabic Sentiment Treebank (A r S en TB) that is morphologically and orthographically enriched. Experimental results show that, compared to the basic RNTN proposed for English, our solution achieves significant improvements up to 8% absolute at the phrase level and 10.8% absolute at the sentence level, measured by average F1 score. It also outperforms well-known classifiers including Support Vector Machines, Recursive Auto Encoders, and Long Short-Term Memory by 7.6%, 3.2%, and 1.6% absolute respectively, all models being trained with similar morphological considerations.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2735552604",
    "type": "article"
  },
  {
    "title": "Multilingual Offensive Language Identification for Low-resource Languages",
    "doi": "https://doi.org/10.1145/3457610",
    "publication_date": "2021-11-10",
    "publication_year": 2021,
    "authors": "Tharindu Ranasinghe; Marcos Zampieri",
    "corresponding_authors": "",
    "abstract": "Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g., hate speech, cyberbullying, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this article, we take advantage of available English datasets by applying cross-lingual contextual word embeddings and transfer learning to make predictions in low-resource languages. We project predictions on comparable data in Arabic, Bengali, Danish, Greek, Hindi, Spanish, and Turkish. We report results of 0.8415 F1 macro for Bengali in TRAC-2 shared task [23], 0.8532 F1 macro for Danish and 0.8701 F1 macro for Greek in OffensEval 2020 [58], 0.8568 F1 macro for Hindi in HASOC 2019 shared task [27], and 0.7513 F1 macro for Spanish in in SemEval-2019 Task 5 (HatEval) [7], showing that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages. Additionally, we report competitive performance on Arabic and Turkish using the training and development sets of OffensEval 2020 shared task. The results for all languages confirm the robustness of cross-lingual contextual embeddings and transfer learning for this task.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W3212112365",
    "type": "article"
  },
  {
    "title": "Emoji-Based Sentiment Analysis Using Attention Networks",
    "doi": "https://doi.org/10.1145/3389035",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Yinxia Lou; Yue Zhang; Fei Li; Qian Tao; Donghong Ji",
    "corresponding_authors": "",
    "abstract": "Emojis are frequently used to express moods, emotions, and feelings in social media. There has been much research on emojis and sentiments. However, existing methods mainly face two limitations. First, they treat emojis as binary indicator features and rely on handcrafted features for emoji-based sentiment analysis. Second, they consider the sentiment of emojis and texts separately, not fully exploring the impact of emojis on the sentiment polarity of texts. In this article, we investigate a sentiment analysis model based on bidirectional long short-term memory, and the model has two advantages compared with the existing work. First, it does not need feature engineering. Second, it utilizes the attention approach to model the impact of emojis on text. An evaluation on 10,042 manually labeled Sina Weibo showed that our model achieves much better performance compared with several strong baselines. To facilitate the related research, our corpus will be publicly available at https://github.com/yx100/emoji.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W3032928500",
    "type": "article"
  },
  {
    "title": "A Survey of Offensive Language Detection for the Arabic Language",
    "doi": "https://doi.org/10.1145/3421504",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Fatemah Husain; Özlem Uzuner",
    "corresponding_authors": "",
    "abstract": "The use of offensive language in user-generated content is a serious problem that needs to be addressed with the latest technology. The field of Natural Language Processing (NLP) can support the automatic detection of offensive language. In this survey, we review previous NLP studies that cover Arabic offensive language detection. This survey investigates the state-of-the-art in offensive language detection for the Arabic language, providing a structured overview of previous approaches, including core techniques, tools, resources, methods, and main features used. This work also discusses the limitations and gaps of the previous studies. Findings from this survey emphasize the importance of investing further effort in detecting Arabic offensive language, including the development of benchmark resources and the invention of novel preprocessing and feature extraction techniques.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W3134728912",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis Using XLM-R Transformer and Zero-shot Transfer Learning on Resource-poor Indian Language",
    "doi": "https://doi.org/10.1145/3461764",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Akshi Kumar; Victor Hugo C. de Albuquerque",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis on social media relies on comprehending the natural language and using a robust machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. The cultural miscellanies, geographically limited trending topic hash-tags, access to aboriginal language keyboards, and conversational comfort in native language compound the linguistic challenges of sentiment analysis. This research evaluates the performance of cross-lingual contextual word embeddings and zero-shot transfer learning in projecting predictions from resource-rich English to resource-poor Hindi language. The cross-lingual XLM-RoBERTa classification model is trained and fine-tuned using the English language Benchmark SemEval 2017 dataset Task 4 A and subsequently zero-shot transfer learning is used to evaluate the classification model on two Hindi sentence-level sentiment analysis datasets, namely, IITP-Movie and IITP-Product review datasets. The proposed model compares favorably to state-of-the-art approaches and gives an effective solution to sentence-level (tweet-level) analysis of sentiments in a resource-poor scenario. The proposed model compares favorably to state-of-the-art approaches and achieves an average performance accuracy of 60.93 on both the Hindi datasets.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W3176188073",
    "type": "article"
  },
  {
    "title": "A Two-stage Text Feature Selection Algorithm for Improving Text Classification",
    "doi": "https://doi.org/10.1145/3425781",
    "publication_date": "2021-05-05",
    "publication_year": 2021,
    "authors": "Ashokkumar Palanivinayagam; G. Siva Shankar; Gautam Srivastava; Praveen Kumar Reddy Maddikunta; Thippa Reddy Gadekallu",
    "corresponding_authors": "",
    "abstract": "As the number of digital text documents increases on a daily basis, the classification of text is becoming a challenging task. Each text document consists of a large number of words (or features) that drive down the efficiency of a classification algorithm. This article presents an optimized feature selection algorithm designed to reduce a large number of features to improve the accuracy of the text classification algorithm. The proposed algorithm uses noun-based filtering, a word ranking that enhances the performance of the text classification algorithm. Experiments are carried out on three benchmark datasets, and the results show that the proposed classification algorithm has achieved the maximum accuracy when compared to the existing algorithms. The proposed algorithm is compared to Term Frequency-Inverse Document Frequency, Balanced Accuracy Measure, GINI Index, Information Gain, and Chi-Square. The experimental results clearly show the strength of the proposed algorithm.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W3158639405",
    "type": "article"
  },
  {
    "title": "A Transformer-Based Approach to Multilingual Fake News Detection in Low-Resource Languages",
    "doi": "https://doi.org/10.1145/3472619",
    "publication_date": "2021-11-02",
    "publication_year": 2021,
    "authors": "Arkadipta De; Dibyanayan Bandyopadhyay; Baban Gain; Asif Ekbal",
    "corresponding_authors": "",
    "abstract": "Fake news classification is one of the most interesting problems that has attracted huge attention to the researchers of artificial intelligence, natural language processing, and machine learning (ML). Most of the current works on fake news detection are in the English language, and hence this has limited its widespread usability, especially outside the English literate population. Although there has been a growth in multilingual web content, fake news classification in low-resource languages is still a challenge due to the non-availability of an annotated corpus and tools. This article proposes an effective neural model based on the multilingual Bidirectional Encoder Representations from Transformer (BERT) for domain-agnostic multilingual fake news classification. Large varieties of experiments, including language-specific and domain-specific settings, are conducted. The proposed model achieves high accuracy in domain-specific and domain-agnostic experiments, and it also outperforms the current state-of-the-art models. We perform experiments on zero-shot settings to assess the effectiveness of language-agnostic feature transfer across different languages, showing encouraging results. Cross-domain transfer experiments are also performed to assess language-independent feature transfer of the model. We also offer a multilingual multidomain fake news detection dataset of five languages and seven different domains that could be useful for the research and development in resource-scarce scenarios.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3208249853",
    "type": "article"
  },
  {
    "title": "Blockchain-based Framework for Reducing Fake or Vicious News Spread on Social Media/Messaging Platforms",
    "doi": "https://doi.org/10.1145/3467019",
    "publication_date": "2021-11-01",
    "publication_year": 2021,
    "authors": "Sakshi Dhall; Ashutosh Dhar Dwivedi; Saibal K. Pal; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "With social media becoming the most frequently used mode of modern-day communications, the propagation of fake or vicious news through such modes of communication has emerged as a serious problem. The scope of the problem of fake or vicious news may range from rumour-mongering, with intent to defame someone, to manufacturing false opinions/trends impacting elections and stock exchanges to much more alarming and mala fide repercussions of inciting violence by bad actors, especially in sensitive law-and-order situations. Therefore, curbing fake or vicious news and identifying the source of such news to ensure strict accountability is the need of the hour. Researchers have been working in the area of using text analysis, labelling, artificial intelligence, and machine learning techniques for detecting fake news, but identifying the source or originator of such news for accountability is still a big challenge for which no concrete approach exists as of today. Also, there is another common problematic trend on social media whereby targeted vicious content goes viral to mobilize or instigate people with malicious intent to destabilize normalcy in society. In the proposed solution, we treat both problems of fake news and vicious news together. We propose a blockchain and keyed watermarking-based framework for social media/messaging platforms that will allow the integrity of the posted content as well as ensure accountability on the owner/user of the post. Intrinsic properties of blockchain-like transparency and immutability are advantageous for curbing fake or vicious news. After identification of fake or vicious news, its spread will be immediately curbed through backtracking as well as forward tracking. Also, observing transactions on the blockchain, the density and rate of forwarding of a particular original message going beyond a threshold can easily be checked, which could be identified as a possible malicious attempt to spread objectionable content. If the content is deemed dangerous or inappropriate, its spread will be curbed immediately. The use of the Raft consensus algorithm and bloXroute servers is proposed to enhance throughput and network scalability, respectively. Thus, the framework offers a proactive as well as reactive, practically feasible, and effective solution for curtailment of fake or vicious news on social media/messaging platforms. The proposed work is a framework for solving fake or vicious news spread problems on social media; the complete design specifications are beyond scope of the current work and will be addressed in the future.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W3208515366",
    "type": "article"
  },
  {
    "title": "Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3489141",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Lung‐Hao Lee; Jianhong Li; Liang-Chih Yu",
    "corresponding_authors": "",
    "abstract": "An increasing amount of research has recently focused on dimensional sentiment analysis that represents affective states as continuous numerical values on multiple dimensions, such as valence-arousal (VA) space. Compared to the categorical approach that represents affective states as distinct classes (e.g., positive and negative), the dimensional approach can provide more fine-grained (real-valued) sentiment analysis. However, dimensional sentiment resources with valence-arousal ratings are very rare, especially for the Chinese language. Therefore, this study aims to: (1) Build a Chinese valence-arousal resource called Chinese EmoBank, the first Chinese dimensional sentiment resource featuring various levels of text granularity including 5,512 single words, 2,998 multi-word phrases, 2,582 single sentences, and 2,969 multi-sentence texts. The valence-arousal ratings are annotated by crowdsourcing based on the Self-Assessment Manikin (SAM) rating scale. A corpus cleanup procedure is then performed to improve annotation quality by removing outlier ratings and improper texts. (2) Evaluate the proposed resource using different categories of classifiers such as lexicon-based, regression-based, and neural-network-based methods, and comparing their performance to a similar evaluation of an English dimensional sentiment resource.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W4205334171",
    "type": "article"
  },
  {
    "title": "Arabic Fake News Detection: A Fact Checking Based Deep Learning Approach",
    "doi": "https://doi.org/10.1145/3501401",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Fouzi Harrag; Mohamed Khalil Djahli",
    "corresponding_authors": "",
    "abstract": "Fake news stories can polarize society, particularly during political events. They undermine confidence in the media in general. Current NLP systems are still lacking the ability to properly interpret and classify Arabic fake news. Given the high stakes involved, determining truth in social media has recently become an emerging research that is attracting tremendous attention. Our literature review indicates that applying the state-of-the-art approaches on news content address some challenges in detecting fake news’ characteristics, which needs auxiliary information to make a clear determination. Moreover, the ‘Social-context-based’ and ‘propagation-based’ approaches can be either an alternative or complementary strategy to content-based approaches. The main goal of our research is to develop a model capable of automatically detecting truth given an Arabic news or claim. In particular, we propose a deep neural network approach that can classify fake and real news claims by exploiting ‘Convolutional Neuron Networks’. Our approach attempts to solve the problem from the fact checking perspective, where the fact-checking task involves predicting whether a given news text claim is factually authentic or fake. We opt to use an Arabic balanced corpus to build our model because it unifies stance detection, stance rationale, relevant document retrieval and fact-checking. The model is trained on different well selected attributes. An extensive evaluation has been conducted to demonstrate the ability of the fact-checking task in detecting the Arabic fake news. Our model outperforms the performance of the state-of-the-art approaches when applied to the same Arabic dataset with the highest accuracy of 91%.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W4206826655",
    "type": "article"
  },
  {
    "title": "Static and Dynamic Isolated Indian and Russian Sign Language Recognition with Spatial and Temporal Feature Detection Using Hybrid Neural Network",
    "doi": "https://doi.org/10.1145/3530989",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "E. Rajalakshmi; R Elakkiya; Alexey Prikhodko; Mikhail G. Grif; Maxim Bakaev; Jatinderkumar R. Saini; Ketan Kotecha; V. Subramaniyaswamy",
    "corresponding_authors": "",
    "abstract": "The Sign Language Recognition system intends to recognize the Sign language used by the hearing and vocally impaired populace. The interpretation of isolated sign language from static and dynamic gestures is a difficult study field in machine vision. Managing quick hand movement, facial expression, illumination variations, signer variation, and background complexity are amongst the most serious challenges in this arena. While deep learning-based models have been used to accomplish the entirety of the field's state-of-the-art outcomes, the previous issues have not been fully addressed. To overcome these issues, we propose a Hybrid Neural Network Architecture for the recognition of Isolated Indian and Russian Sign Language. In the case of static gesture recognition, the proposed framework deals with the 3D Convolution Net with an atrous convolution mechanism for spatial feature extraction. For dynamic gesture recognition, the proposed framework is an integration of semantic spatial multi-cue feature detection, extraction, and Temporal-Sequential feature extraction. The semantic spatial multi-cue feature detection and extraction module help in the generation of feature maps for Full-frame, pose, face, and hand. For face and hand detection, GradCam and Camshift algorithm have been used. The temporal and sequential module consists of a modified auto-encoder with a GELU activation function for abstract high-level feature extraction and a hybrid attention layer. The hybrid attention layer is an integration of segmentation and spatial attention mechanism. The proposed work also involves creating a novel multi-signer, single, and double-handed Isolated Sign representation dataset for Indian and Russian Sign Language. The experimentation was done on the novel dataset created. The accuracy obtained for Static Isolated Sign Recognition was 99.76%, and the accuracy obtained for Dynamic Isolated Sign Recognition was 99.85%. We have also compared the performance of our proposed work with other baseline models with benchmark datasets, and our proposed work proved to have better performance in terms of Accuracy metrics.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W4224257285",
    "type": "article"
  },
  {
    "title": "Experience Replay-based Deep Reinforcement Learning for Dialogue Management Optimisation",
    "doi": "https://doi.org/10.1145/3539223",
    "publication_date": "2022-05-25",
    "publication_year": 2022,
    "authors": "Shrikant Malviya; Piyush Kumar; Suyel Namasudra; Uma Shanker Tiwary",
    "corresponding_authors": "",
    "abstract": "Dialogue policy is a crucial component in task-oriented Spoken Dialogue Systems (SDSs). As a decision function, it takes the current dialogue state as input and generates appropriate system’s response. In this paper, we explore the reinforcement learning approaches to solve this problem in an Indic language scenario. Recently, Deep Reinforcement Learning (DRL) has been used to optimise the dialogue policy. However, many DRL approaches are not sample-efficient. Hence, particular attention is given to actor-critic methods based on off-policy reinforcement learning that utilise the Experience Replay (ER) technique for reducing the bias and variance to achieve high sample efficiency. ER based actor-critic methods, such as Advantage Actor-Critic Experience Replay (A2CER) are proven to deliver competitive results in gaming environments that are fully observable and have a very small action-set. While, in SDSs, the states are not fully observable and often have to deal with the large action space. Describing the limitations of traditional methods, i.e., value-based and policy-based methods, such as high variance, low sample-efficiency, and often converging to local optima, we firstly explore the use of A2CER in dialogue policy learning. It is shown to beat the current state-of-the-art deep learning methods for SDS. Secondly, to handle the issues of early-stage performance, we utilise a demonstration corpus to pre-train the models prior to on-line policy learning. We thus experiment with the A2CER on a larger action space and find it significantly faster than the current state-of-the-art. Combining both approaches, we present a novel DRL based dialogue policy optimisation method, A2CER and its effectiveness for a task-oriented SDS in the Indic language.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W4281480706",
    "type": "article"
  },
  {
    "title": "Review of Machine and Deep Learning Techniques in Epileptic Seizure Detection using Physiological Signals and Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3552512",
    "publication_date": "2022-08-03",
    "publication_year": 2022,
    "authors": "Deba Prasad Dash; Maheshkumar H. Kolekar; Chinmay Chakraborty; Mohammad R. Khosravi",
    "corresponding_authors": "",
    "abstract": "Epilepsy is one of the significant neurological disorders affecting nearly 65 million people worldwide. The repeated seizure is characterized as epilepsy. Different algorithms were proposed for efficient seizure detection using intracranial and surface EEG signals. In the last decade, various machine learning techniques based on seizure detection approaches were proposed. This paper discusses different machine learning and deep learning techniques for seizure detection using intracranial and surface EEG signals. A wide range of machine learning techniques such as support vector machine (SVM) classifiers, artificial neural network (ANN) classifier, and deep learning techniques such as a convolutional neural network (CNN) classifier, and long-short term memory (LSTM) network for seizure detection are compared in this paper. The effectiveness of time-domain features, frequency domain features, and time-frequency domain features are discussed along with different machine learning techniques. Along with EEG, other physiological signals such as electrocardiogram are used to enhance seizure detection accuracy which are discussed in this paper. In recent years deep learning techniques based on seizure detection have found good classification accuracy. In this paper, an LSTM deep learning-network-based approach is implemented for seizure detection and compared with state-of-the-art methods. The LSTM based approach achieved 96.5% accuracy in seizure-nonseizure EEG signal classification. Apart from analyzing the physiological signals, sentiment analysis also has potential to detect seizures. Impact Statement- This review paper gives a summary of different research work related to epileptic seizure detection using machine learning and deep learning techniques. Manual seizure detection is time consuming and requires expertise. So the artificial intelligence techniques such as machine learning and deep learning techniques are used for automatic seizure detection. Different physiological signals are used for seizure detection. Different researchers are working on developing automatic seizure detection using EEG, ECG, accelerometer, and sentiment analysis. There is a need for a review paper that can discuss previous techniques and give further research direction. We have discussed different techniques for seizure detection with an accuracy comparison table. It can help the researcher to get an overview of both surface and intracranial EEG-based seizure detection approaches. The new researcher can easily compare different models and decide the model they want to start working on. A deep learning model is discussed to give a practical application of seizure detection. Sentiment analysis is another dimension of seizure detection and summarizing it will give a new prospective to the reader.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W4289544551",
    "type": "article"
  },
  {
    "title": "Arabic ChatGPT Tweets Classification Using RoBERTa and BERT Ensemble Model",
    "doi": "https://doi.org/10.1145/3605889",
    "publication_date": "2023-06-30",
    "publication_year": 2023,
    "authors": "Muhammad Mujahid; Khadija Kanwal; Furqan Rustam; Wajdi Aljedaani; Imran Ashraf",
    "corresponding_authors": "",
    "abstract": "ChatGPT OpenAI, a large-language chatbot model, has gained a lot of attention due to its popularity and impressive performance in many natural language processing tasks. ChatGPT produces superior answers to a wide range of real-world human questions and generates human-like text. The new OpenAI ChatGPT technology may have some strengths and weaknesses at this early stage. Users have reported early opinions about the ChatGPT features, and their feedback is essential to recognize and fix its shortcomings and issues. This study uses the ChatGPT tweets Arabic dataset to automatically find user opinions and sentiments about ChatGPT technology. The dataset is preprocessed and labeled using the TextBlob Arabic Python library into positive, negative, and neutral tweets. Despite extensive works for the English language, languages like Arabic are less studied regarding tweet analysis. Existing literature about Arabic tweet sentiment analysis has mainly focused on machine learning and deep learning models. We collected a total of 27,780 unstructured tweets from Twitter using the Tweepy SNscrape Python library using various hash-tags such as # Chat-GPT, #OpenAI, #Chatbot, Chat-GPT3, and so on. To enhance the model’s performance and reduce computational complexity, unstructured tweets are converted into structured and normalized forms. Tweets contain missing values, URL and HTML tags, stop words, punctuation, diacritics, elongations, and numeric values that have no impact on the model performance; hence, these increase the computational cost. So, these steps are removed with the help of Python preprocessing libraries to enhance text quality and consistency. This study adopts Transformer-based models such as RoBERTa, XLNet, and DistilBERT that automatically classify the tweets. Additionally, a hybrid transformer-based model is proposed to obtain better results. The proposed hybrid model is developed by combining the hidden outputs of the RoBERTA and BERT models using a concatenation layer, then adding dense layers with “Relu” activation employed as a hidden layer to create non-linearity and a “softmax” activation function for multiclass classification. They differ from existing state-of-the-art models due to the enhanced capabilities of both models in text classification. Hybrid models combine the different models to make accurate predictions and reduce bias and enhanced the overall results, while state-of-the-art models are incapable of making accurate predictions. Experiments show that the proposed hybrid model achieves 96.02% accuracy, 100% precision on negative tweets, and 99% recall for neutral tweets. The performance of the proposed model is far better than existing state-of-the-art models.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4382680318",
    "type": "article"
  },
  {
    "title": "Multi-task Pre-training Language Model for Semantic Network Completion",
    "doi": "https://doi.org/10.1145/3627704",
    "publication_date": "2023-10-17",
    "publication_year": 2023,
    "authors": "Da Li; Boqing Zhu; Sen Yang; Kele Xu; Yi Ming; Yukai He; Huaimin Wang",
    "corresponding_authors": "",
    "abstract": "Semantic networks, exemplified by the knowledge graph, serve as a means to represent knowledge by leveraging the structure of a graph. While the knowledge graph exhibits promising potential in the field of natural language processing, it suffers from incompleteness. This article focuses on the task of completing knowledge graphs by predicting linkages between entities, which is fundamental yet critical. Traditional methods based on translational distance struggle when dealing with unseen entities. In contrast, semantic matching presents itself as a potential solution due to its ability to handle such cases. However, semantic matching-based approaches necessitate large-scale datasets for effective training, which are typically unavailable in practical scenarios, hindering their competitive performance. To address this challenge, we propose a novel architecture for knowledge graphs known as LP-BERT, which incorporates a language model. LP-BERT consists of two primary stages: multi-task pre-training and knowledge graph fine-tuning. During the pre-training phase, the model acquires relationship information from triples by predicting either entities or relations through three distinct tasks. In the fine-tuning phase, we introduce a batch-based triple-style negative sampling technique inspired by contrastive learning. This method significantly increases the proportion of negative sampling while maintaining a nearly unchanged training time. Furthermore, we propose a novel data augmentation approach that leverages the inverse relationship of triples to enhance both the performance and robustness of the model. To demonstrate the effectiveness of our proposed framework, we conduct extensive experiments on three widely used knowledge graph datasets: WN18RR, FB15k-237, and UMLS. The experimental results showcase the superiority of our methods, with LP-BERT achieving state-of-the-art performance on the WN18RR and FB15k-237 datasets.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4387708435",
    "type": "article"
  },
  {
    "title": "Image–Text Multimodal Sentiment Analysis Framework of Assamese News Articles Using Late Fusion",
    "doi": "https://doi.org/10.1145/3584861",
    "publication_date": "2023-02-17",
    "publication_year": 2023,
    "authors": "Ringki Das; Thoudam Doren Singh",
    "corresponding_authors": "",
    "abstract": "Before the arrival of the web as a corpus, people detected positive and negative news based on the understanding of the textual content from physical newspaper rather than an automatic identification approach from readily available e-newspapers. Thus, the earlier sentiment analysis approach is based on unimodal data, and less effort is paid to the multimodal data. However, the presence of multimodal information helps us to get a clearer understanding of the sentiment. To the best of our knowledge, less work has been introduced on the image–text multimodal sentiment analysis framework of Assamese, a low-resource Indian language mostly spoken in the northeast part of India. We built an Assamese news articles dataset consisting of news text and associated images and one image caption to conduct an experimental study. Focusing on important words and discriminative regions of the images mostly related to sentiment, two individual unimodal such as textual and visual models are proposed. The visual model is developed using an encoder-decoder–based image caption generation system. An image–text multimodal approach is proposed to explore the internal correlation between textual and visual features for joint sentiment classification. Finally, we propose the multimodal sentiment analysis framework, i.e., Textual Visual Multimodal Fusion, by employing a late fusion scheme to merge the three different modalities for the final sentiment prediction. Experimental results conducted on the Assamese dataset built in-house demonstrate that the contextual integration of multimodal features delivers better performance than unimodal features.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4321221922",
    "type": "article"
  },
  {
    "title": "THAR- Targeted Hate Speech Against Religion: A high-quality Hindi-English code-mixed Dataset with the Application of Deep Learning Models for Automatic Detection",
    "doi": "https://doi.org/10.1145/3653017",
    "publication_date": "2024-03-18",
    "publication_year": 2024,
    "authors": "Deepawali Sharma; Aakash Singh; Vivek Kumar Singh",
    "corresponding_authors": "",
    "abstract": "During the last decade, social media has gained significant popularity as a medium for individuals to express their views on various topics. However, some individuals also exploit the social media platforms to spread hatred through their comments and posts, some of which target individuals, communities or religions. Given the deep emotional connections people have to their religious beliefs, this form of hate speech can be divisive and harmful, and may result in issues of mental health as social disorder. Therefore, there is a need of algorithmic approaches for the automatic detection of instances of hate speech. Most of the existing studies in this area focus on social media content in English, and as a result several low-resource languages lack computational resources for the task. This study attempts to address this research gap by providing a high-quality annotated dataset designed specifically for identifying hate speech against religions in the Hindi-English code-mixed language. This dataset “Targeted Hate Speech Against Religion” (THAR)) consists of 11,549 comments and has been annotated by five independent annotators. It comprises two subtasks: (i) Subtask-1 (Binary classification), (ii) Subtask-2 (multi-class classification). To ensure the quality of annotation, the Fleiss Kappa measure has been employed. The suitability of the dataset is then further explored by applying different standard deep learning, and transformer-based models. The transformer-based model, namely Multilingual Representations for Indian Languages (MuRIL), is found to outperform the other implemented models in both subtasks, achieving macro average and weighted average F1 scores of 0.78 and 0.78 for Subtask-1, and 0.65 and 0.72 for Subtask-2, respectively. The experimental results obtained not only confirm the suitability of the dataset but also advance the research towards automatic detection of hate speech, particularly in the low-resource Hindi-English code-mixed language.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4392926998",
    "type": "article"
  },
  {
    "title": "Hate Speech Detection Research in South Asian Languages: A Survey of Tasks, Datasets and Methods",
    "doi": "https://doi.org/10.1145/3711710",
    "publication_date": "2025-01-08",
    "publication_year": 2025,
    "authors": "Deepawali Sharma; Tanusree Nath; Vedika Gupta; Vivek Kumar Singh",
    "corresponding_authors": "",
    "abstract": "Social media has over the years emerged as a powerful platform for communicating and sharing views, thoughts, and opinions. However, at the same time it is being abused by certain individuals to spread hate against individuals, communities, religions etc. Such content can lead to serious issues of mental health, online well-being, and social order. Therefore, it is very important to have automated methods and approaches for detecting such content from the large volume of posts in social media. Recently there has been several efforts to develop computational approaches towards this end, however, most of these efforts are directed towards content in English language. Only recently studies have started focusing on low resource languages, including those from South Asia. This paper attempts to present a detailed and comprehensive survey of hate speech related research in South Asian languages. The various definitions and terms related to Hate speech in different social media platforms are discussed first. The different tasks in the hate speech research, available datasets and the popular computational approaches used in the South-Asian languages are surveyed in detail. Major patterns identified and the practical implications are presented and discussed, along with a discussion of challenges and opportunities of further research in the area.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406180923",
    "type": "article"
  },
  {
    "title": "LAMGCN:Traditional Chinese Medicine Herb Recommendation via LSTMs with Attention Mechanisms and Graph Convolutional Networks",
    "doi": "https://doi.org/10.1145/3708888",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Wenbo Zhang; Hongbo Dang; Zhenshan Bao; Song Bingyan",
    "corresponding_authors": "",
    "abstract": "Herb recommendation plays a crucial role in the therapeutic process of Traditional Chinese Medicine (TCM), which aims to recommend a set of herbs to treat patients with different symptoms. Previous works used many methods to discover regularities in prescriptions but rarely considered the actual therapeutic process in TCM and the information of herbs was ignored. In this work, we propose LAMGCN(Herb Recommendation via LSTMs with Attention Mechanisms and Graph Convolutional Networks), which takes the syndrome induction process and the herb descriptions into account. We utilize attention mechanisms and graph neural networks to capture the correlation between symptoms and herbs. Extensive experiments have been done and the results demonstrate the effectiveness of our proposed method.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406412494",
    "type": "article"
  },
  {
    "title": "Predictor-Estimator",
    "doi": "https://doi.org/10.1145/3109480",
    "publication_date": "2017-09-15",
    "publication_year": 2017,
    "authors": "Hyun Kim; Hun-Young Jung; Hong-Seok Kwon; Jong-Hyeok Lee; Seung‐Hoon Na",
    "corresponding_authors": "",
    "abstract": "Recently, quality estimation has been attracting increasing interest from machine translation researchers, aiming at finding a good estimator for the “quality” of machine translation output. The common approach for quality estimation is to treat the problem as a supervised regression/classification task using a quality-annotated noisy parallel corpus, called quality estimation data , as training data. However, the available size of quality estimation data remains small, due to the too-expensive cost of creating such data. In addition, most conventional quality estimation approaches rely on manually designed features to model nonlinear relationships between feature vectors and corresponding quality labels. To overcome these problems, this article proposes a novel neural network architecture for quality estimation task—called the predictor-estimator —that considers word prediction as an additional pre-task. The major component of the proposed neural architecture is a word prediction model based on a modified neural machine translation model—a probabilistic model for predicting a target word conditioned on all the other source and target contexts. The underlying assumption is that the word prediction model is highly related to quality estimation models and is therefore able to transfer useful knowledge to quality estimation tasks. Our proposed quality estimation method sequentially trains the following two types of neural models: (1) Predictor : a neural word prediction model trained from parallel corpora and (2) Estimator : a neural quality estimation model trained from quality estimation data. To transfer word a prediction task to a quality estimation task, we generate quality estimation feature vectors from the word prediction model and feed them into the quality estimation model. The experimental results on WMT15 and 16 quality estimation datasets show that our proposed method has great potential in the various sub-challenges.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2754753494",
    "type": "article"
  },
  {
    "title": "A Comparative Analysis on Hindi and English Extractive Text Summarization",
    "doi": "https://doi.org/10.1145/3308754",
    "publication_date": "2019-05-09",
    "publication_year": 2019,
    "authors": "Pradeepika Verma; Sukomal Pal; Hari Om",
    "corresponding_authors": "",
    "abstract": "Text summarization is the process of transfiguring a large documental information into a clear and concise form. In this article, we present a detailed comparative study of various extractive methods for automatic text summarization on Hindi and English text datasets of news articles. We consider 13 different summarization techniques, namely, TextRank, LexRank, Luhn, LSA, Edmundson, ChunkRank, TGraph, UniRank, NN-ED, NN-SE, FE-SE, SummaRuNNer, and MMR-SE, and we evaluate their performance using various performance metrics, such as precision, recall, F 1 , cohesion, non-redundancy, readability, and significance. A thorough analysis is done in eight different parts that exhibits the strengths and limitations of these methods, effect of performance over the summary length, impact of language of a document, and other factors as well. A standard summary evaluation tool (ROUGE) and extensive programmatic evaluation using Python 3.5 in Anaconda environment are used to evaluate their outcome.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2944770383",
    "type": "article"
  },
  {
    "title": "Named Entity Recognition with Word Embeddings and Wikipedia Categories for a Low-Resource Language",
    "doi": "https://doi.org/10.1145/3015467",
    "publication_date": "2017-01-20",
    "publication_year": 2017,
    "authors": "Arjun Das; Debasis Ganguly; Utpal Garain",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a word embedding--based named entity recognition (NER) approach. NER is commonly approached as a sequence labeling task with the application of methods such as conditional random field (CRF). However, for low-resource languages without the presence of sufficiently large training data, methods such as CRF do not perform well. In our work, we make use of the proximity of the vector embeddings of words to approach the NER problem. The hypothesis is that word vectors belonging to the same name category, such as a person’s name, occur in close vicinity in the abstract vector space of the embedded words. Assuming that this clustering hypothesis is true, we apply a standard classification approach on the vectors of words to learn a decision boundary between the NER classes. Our NER experiments are conducted on a morphologically rich and low-resource language, namely Bengali. Our approach significantly outperforms standard baseline CRF approaches that use cluster labels of word embeddings and gazetteers constructed from Wikipedia. Further, we propose an unsupervised approach (that uses an automatically created named entity (NE) gazetteer from Wikipedia in the absence of training data). For a low-resource language, the word vectors obtained from Wikipedia are not sufficient to train a classifier. As a result, we propose to make use of the distance measure between the vector embeddings of words to expand the set of Wikipedia training examples with additional NEs extracted from a monolingual corpus that yield significant improvement in the unsupervised NER performance. In fact, our expansion method performs better than the traditional CRF-based (supervised) approach (i.e., F-score of 65.4% vs. 64.2%). Finally, we compare our proposed approach to the official submission for the IJCNLP-2008 Bengali NER shared task and achieve an overall improvement of F-score 11.26% with respect to the best official system.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2572043993",
    "type": "article"
  },
  {
    "title": "Sign Language Generation System Based on Indian Sign Language Grammar",
    "doi": "https://doi.org/10.1145/3384202",
    "publication_date": "2020-04-23",
    "publication_year": 2020,
    "authors": "Sugandhi; Parteek Kumar; Sanmeet Kaur",
    "corresponding_authors": "",
    "abstract": "Sign Language (SL), also known as gesture-based language, is used by people with hearing loss to convey their messages. SL interpreters are required for people who do not have the knowledge of SL, but interpreters are not readily available. Thus, a machine-based translation system is required to translate the text into SL. In this article, a system is implemented for translating English text into Indian Sign Language (ISL). It acts as a tool for human-computer interaction and eliminates the need for an ISL human interpreter for communicating with people who have hearing loss. The system features a rich corpus of English words and commonly used sentences. It consists of components such as an ISL parser, the Hamburg Notation System, the Signing Gesture Mark-up Language, and 3D avatar animation for generating SL according to ISL grammar. The proposed system has been tested rigorously by SL users. The results proved that the proposed system is highly efficient and achieves an average score of accuracy (i.e., 4.2 for English words and 3.8 for sentences on a scale from 1 to 5). The performance of proposed system has also been evaluated using the BiLingual Evaluation Understudy score, which results in 0.95 accuracy. The proposed system and mobile application together has the potential to bring individuals with hearing loss and their entourage together.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W3018840337",
    "type": "article"
  },
  {
    "title": "A Framework for Extractive Text Summarization Based on Deep Learning Modified Neural Network Classifier",
    "doi": "https://doi.org/10.1145/3392048",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "BalaAnand Muthu; C. B. Sivaparthipan; Priyan Malarvizhi Kumar; Seifedine Kadry; Ching‐Hsien Hsu; Óscar Sanjuán Martínez; Rubén González Crespo",
    "corresponding_authors": "",
    "abstract": "There is an exponential growth of text data over the internet, and it is expected to gain significant growth and attention in the coming years. Extracting meaningful insights from text data is crucially important as it offers value-added solutions to business organizations and end-users. Automatic text summarization (ATS) automates text summarization by reducing the initial size of the text without the loss of key information elements. In this article, we propose a novel text summarization algorithm for documents using Deep Learning Modifier Neural Network (DLMNN) classifier. It generates an informative summary of the documents based on the entropy values. The proposed DLMNN framework comprises six phases. In the initial phase, the input document is pre-processed. Subsequently, the features are extracted using pre-processed data. Next, the most appropriate features are selected using the improved fruit fly optimization algorithm (IFFOA). The entropy value for every chosen feature is computed. These values are then classified into two classes, (a) highest entropy values and (b) lowest entropy values. Finally, the class that holds the highest entropy values is chosen, representing the informative sentences that form the last summary. The results observed from the experiment indicate that the DLMNN classifier gives 81.56, 91.21, and 83.53 of sensitivity, accuracy, specificity, precision, and f-measure. Whereas the existing schemes such as ANN relatively provide lesser value in contrast to DLMNN.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W3041152616",
    "type": "article"
  },
  {
    "title": "KArSL",
    "doi": "https://doi.org/10.1145/3423420",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Ala Addin I. Sidig; Hamzah Luqman; Sabri A. Mahmoud; M. Mohandes",
    "corresponding_authors": "",
    "abstract": "Sign language is the major means of communication for the deaf community. It uses body language and gestures such as hand shapes, lib patterns, and facial expressions to convey a message. Sign language is geography-specific, as it differs from one country to another. Arabic Sign language is used in all Arab countries. The availability of a comprehensive benchmarking database for ArSL is one of the challenges of the automatic recognition of Arabic Sign language. This article introduces KArSL database for ArSL, consisting of 502 signs that cover 11 chapters of ArSL dictionary. Signs in KArSL database are performed by three professional signers, and each sign is repeated 50 times by each signer. The database is recorded using state-of-art multi-modal Microsoft Kinect V2. We also propose three approaches for sign language recognition using this database. The proposed systems are Hidden Markov Models, deep learning images’ classification model applied on an image composed of shots of the video of the sign, and attention-based deep learning captioning system. Recognition accuracies of these systems indicate their suitability for such a large number of Arabic signs. The techniques are also tested on a publicly available database. KArSL database will be made freely available for interested researchers.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3135585008",
    "type": "article"
  },
  {
    "title": "Developing a Vietnamese Tourism Question Answering System Using Knowledge Graph and Deep Learning",
    "doi": "https://doi.org/10.1145/3453651",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Phuc Do; Truong H. V. Phan; Brij B. Gupta",
    "corresponding_authors": "",
    "abstract": "In recent years, Question Answering (QA) systems have increasingly become very popular in many sectors. This study aims to use a knowledge graph and deep learning to develop a QA system for tourism in Vietnam. First, the QA system replies to a user's question about a place in Vietnam. Then, the QA describes it in detail such as when the place was discovered, why the place's name was called like that, and so on. Finally, the system recommends some related tourist attractions to users. Meanwhile, deep learning is used to solve a simple natural language answer, and a knowledge graph is used to infer a natural language answering list related to entities in the question. The study experiments on a manual dataset collected from Vietnamese tourism websites. As a result, the QA system combining the two above approaches provides more information than other systems have done before. Besides that, the system gets 0.83 F1, 0.87 precision on the test set.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W3175462020",
    "type": "article"
  },
  {
    "title": "Improving Readability for Automatic Speech Recognition Transcription",
    "doi": "https://doi.org/10.1145/3557894",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Junwei Liao; Şefik Emre Eskimez; Liyang Lu; Yu Shi; Ming Gong; Linjun Shou; Hong Qu; Michael Zeng",
    "corresponding_authors": "",
    "abstract": "Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other noises common in spoken communication. These readable issues introduced by speakers and ASR systems will impair the performance of downstream tasks and the understanding of human readers. In this work, we present a task called ASR post-processing for readability (APR) and formulate it as a sequence-to-sequence text generation problem. The APR task aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of speakers. We further study the APR task from the benchmark dataset, evaluation metrics, and baseline models: First, to address the lack of task-specific data, we propose a method to construct a dataset for the APR task by using the data collected for grammatical error correction. Second, we utilize metrics adapted or borrowed from similar tasks to evaluate model performance on the APR task. Lastly, we use several typical or adapted pre-trained models as the baseline models for the APR task. Furthermore, we fine-tune the baseline models on the constructed dataset and compare their performance with a traditional pipeline method in terms of proposed evaluation metrics. Experimental results show that all the fine-tuned baseline models perform better than the traditional pipeline method, and our adapted RoBERTa model outperforms the pipeline method by 4.95 and 6.63 BLEU points on two test sets, respectively. The human evaluation and case study further reveal the ability of the proposed model to improve the readability of ASR transcripts.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4292595872",
    "type": "article"
  },
  {
    "title": "Investigating the Effect of Preprocessing Arabic Text on Offensive Language and Hate Speech Detection",
    "doi": "https://doi.org/10.1145/3501398",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Fatemah Husain; Özlem Uzuner",
    "corresponding_authors": "",
    "abstract": "Preprocessing of input text can play a key role in text classification by reducing dimensionality and removing unnecessary content. This study aims to investigate the impact of preprocessing on Arabic offensive language classification. We explore six preprocessing techniques: conversion of emojis to Arabic textual labels, normalization of different forms of Arabic letters, normalization of selected nouns from dialectal Arabic to Modern Standard Arabic, conversion of selected hyponyms to hypernyms, hashtag segmentation, and basic cleaning such as removing numbers, kashidas, diacritics, and HTML tags. We also experiment with raw text and a combination of all six preprocessing techniques. We apply different types of classifiers in our experiments including traditional machine learning, ensemble machine learning, Artificial Neural Networks, and Bidirectional Encoder Representations from Transformers (BERT)-based models to analyze the impact of preprocessing. Our results demonstrate significant variations in the effects of preprocessing on each classifier type and on each dataset. Classifiers that are based on BERT do not benefit from preprocessing, while traditional machine learning classifiers do. However, these results can benefit from validation on larger datasets that cover broader domains and dialects.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4205500136",
    "type": "article"
  },
  {
    "title": "Optimizing Hyperparameters and Performance Analysis of LSTM Model in Detecting Fake News on Social media",
    "doi": "https://doi.org/10.1145/3511897",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "R. R. Rajalaxmi; L V Narasimha Prasad; B. Janakiramaiah; C.S. Pavankumar; N. Neelima; Sathishkumar Veerappampalayam Easwaramoorthy",
    "corresponding_authors": "",
    "abstract": "Fake news detection recently received a lot of attention from the scientific community and demands an optimal solution with high efficiency. Several studies were conducted using unsupervised and supervised learning techniques to address the fake news identification problem. These studies, on the other hand, have some limitations like inefficient model design, improper pre-processing, and poor accuracy. Some factors contributing to poor accuracy include irrelevant features, improper model parameters, imbalanced datasets, and so on. This work proposes an optimized deep learning model for detecting fake news. The developed model uses Long Short-Term Memory (LSTM) to classify fake and real news and utilizes hyperparameter tuning methods such as grid search and random search to customize the hyperparameters of the model. The experimental results indicate that the optimized LSTM model yields 99.65% accuracy using the ISOT dataset and 45.23% accuracy using the LIAR dataset.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4293231407",
    "type": "article"
  },
  {
    "title": "Supervised Machine Learning Method for Ontology-based Financial Decisions in the Stock Market",
    "doi": "https://doi.org/10.1145/3554733",
    "publication_date": "2022-11-05",
    "publication_year": 2022,
    "authors": "Neha Sharma; Mukesh Soni; Sumit Kumar; Rajeev Kumar; Nabamita Deb; Anurag Shrivastava",
    "corresponding_authors": "",
    "abstract": "For changing semantics, ontological and information presentation, as well as computational linguistics for Asian social networks, are one of the most essential platforms for offering enhanced and real-time data mapping, as well as huge data access across diverse big data sources on the web architecture, information extraction mining, statistical modeling and data modeling, database control, and so on. The concept of opinion or sentiment analysis is often used to predict or classify the textual data, sentiment, affect, subjectivity, and other emotional states in online text. Recognizing the message's positive and negative thoughts or opinions by examining the author's goals will aid in a better understanding of the text's content in terms of the stock market. An intelligent ontology and knowledge Asian social network solution can improve the effectiveness of a company's decision making support procedures by deriving important information about users from a wide variety of web sources. However, ontology is concerned primarily with problem-solving knowledge discovery. The utilization of Internet-based modernizations welcomed a significant effect on the Indian stock exchange. News related to the stock market in the most recent decade plays a vital role for the brokers or users. This article focuses on predicting stock market news sentiments based on their polarity and textual information using the concept of ontological knowledge-based Convolution Neural Network (CNN) as a machine learning approach. Optimal features are essential for the sentiment classification model to predict the stock's textual reviews' exact sentiment. Therefore, the swarm-based Artificial Bee Colony (ABC) algorithm is utilized with the Lexicon feature extraction approach using a novel fitness function. The main motivation for combining ABC and CNN is to accelerate model training, which is why the suggested approach is effective in predicting emotions from stock news.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4308262326",
    "type": "article"
  },
  {
    "title": "Albanian Fake News Detection",
    "doi": "https://doi.org/10.1145/3487288",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Ercan Canhasi; Rexhep Shijaku; Erblin Berisha",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed the vast increase of the phenomenon known as the fake news. Among the main reasons for this increase are the continuous growth of internet and social media usage and the real-time information dissemination opportunity offered by them. Deceiving, misleading content, such as the fake news, especially the type made by and for social media users, is becoming eminently hazardous. Hence, the fake news detection problem has become an important research topic. Despite the recent advances in fake news detection, the lack of fake news corpora for the under-resourced languages is compromising the development and the evaluation of existing approaches in these languages. To fill this huge gap, in this article, we investigate the issue of fake news detection for the Albanian language. In it, we present a new public dataset of labeled true and fake news in Albanian and perform an extensive analysis of machine learning methods for fake news detection. We performed a comprehensive feature engineering and feature selection experiments. In doing so, we explored the Albanian language-related feature categories such as the lexical, syntactic, lying-detection, and psycho-linguistic features. Each article was also modeled in four different ways: with the traditional bag-of-words (BoW) and with three distributed text representations using the state-of-the-art Word2Vec, FastText, and BERT methods. Additionally, we investigated the best combination of features and various types of classification methods. The conducted experiments and obtained results from evaluations are finally used to draw some conclusions. They shed light on the potentiality of the methods and the challenges that the Albanian fake news detection presents.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4210816395",
    "type": "article"
  },
  {
    "title": "From Softmax to Nucleusmax: A Novel Sparse Language Model for Chinese Radiology Report Summarization",
    "doi": "https://doi.org/10.1145/3596219",
    "publication_date": "2023-05-13",
    "publication_year": 2023,
    "authors": "Shuai Zhao; Qing Li; Yuer Yang; Jinming Wen; Weiqi Luo",
    "corresponding_authors": "",
    "abstract": "The Chinese radiology report summarization is a crucial component in smart healthcare that employs language models to summarize key findings in radiology reports and communicate these findings to physicians. However, most language models for radiology report summarization utilize a softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is inefficient, reducing model interpretability and giving probability mass to many unrealistic outputs. To tackle this issue, we propose a novel approach named nucleusmax. Nucleusmax is able to mitigate dense outputs and improve model interpretability by truncating the unreliable tail of the probability distribution. In addition, we incorporate nucleusmax with a copy mechanism, a useful technique to avoid professional errors in the generated diagnostic opinions. To further promote the research of radiology report summarization, we also have created a Chinese radiology report summarization dataset, which is freely available. Experimental results showed via both automatic and human evaluation that the proposed approach substantially improves the sparsity and overall quality of outputs over competitive softmax models, producing radiology summaries that approach the quality of those authored by physicians. In general, our work demonstrates the feasibility and prospect of the language model to the domain of radiology and smart healthcare.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4376505484",
    "type": "article"
  },
  {
    "title": "Instance-Aware Prompt Learning for Language Understanding and Generation",
    "doi": "https://doi.org/10.1145/3604613",
    "publication_date": "2023-06-14",
    "publication_year": 2023,
    "authors": "Feihu Jin; J. D. Lu; Jiajun Zhang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Prompt learning has emerged as a new paradigm for leveraging pre-trained language models (PLMs) and has shown promising results in downstream tasks with only a slight increase in parameters. However, the current usage of fixed prompts, whether discrete or continuous, assumes that all samples within a task share the same prompt. This assumption may not hold for tasks with diverse samples that require different prompt information. To address this issue, we propose an instance-aware prompt learning method that learns a different prompt for each instance. Specifically, we suppose that each learnable prompt token has a different contribution to different instances, and we learn the contribution by calculating the relevance score between an instance and each prompt token. The contribution-weighted prompt would be instance aware. We apply our method to both unidirectional and bidirectional PLMs on both language understanding and generation tasks. Extensive experiments demonstrate that our method achieves comparable results using as few as 1.5% of the parameters of PLMs tuned and obtains considerable improvements compared with strong baselines. In particular, our method achieves state-of-the-art results using ALBERT-xxlarge-v2 on the SuperGLUE few-shot learning benchmark. 1",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4380609152",
    "type": "article"
  },
  {
    "title": "Detection of Offensive Language and ITS Severity for Low Resource Language",
    "doi": "https://doi.org/10.1145/3580476",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Ramsha Saeed; Hammad Afzal; Sadaf Abdul Rauf; Naima Iltaf",
    "corresponding_authors": "",
    "abstract": "Continuous proliferation of hate speech in different languages on social media has drawn significant attention from researchers in the past decade. Detecting hate speech is indispensable irrespective of the scale of use of language, as it inflicts huge harm on society. This work presents a first resource for classifying the severity of hate speech in addition to classifying offensive and hate speech content. Current research mostly limits hate speech classification to its primary categories, such as racism, sexism, and hatred of religions. However, hate speech targeted at different protected characteristics also manifests in different forms and intensities. It is important to understand varying severity levels of hate speech so that the most harmful cases of hate speech may be identified and dealt with earlier than the less harmful ones. In this work, we focus on detecting offensive speech, hate speech, and multiple levels of hate speech in the Urdu language. We investigate three primary target categories of hate speech: religion, racism, and national origin. We further divide these categories into levels based on the severity of hate conveyed. The severity levels are referred to as symbolization , insult , and attribution . A corpus comprising more than 20,000 tweets against the corresponding hate speech categories and severity levels is collected and annotated. A comprehensive experimentation scheme is applied using traditional as well as deep learning–based models to examine their impact on hate speech detection. The highest macro-averaged F-score yielded for detecting offensive speech is 86% while the highest F-scores for detecting hate speech with respect to ethnicity, national origin, and religious affiliation are 80%, 81%, and 72%, respectively. This shows that results are very encouraging and would provide a lead towards further investigation in this domain.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4317502704",
    "type": "article"
  },
  {
    "title": "Malayalam Natural Language Processing: Challenges in Building a Phrase-Based Statistical Machine Translation System",
    "doi": "https://doi.org/10.1145/3579163",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Mary Priya Sebastian; G. Santhosh Kumar",
    "corresponding_authors": "",
    "abstract": "Statistical Machine Translation (SMT) is a preferred Machine Translation approach to convert the text in a specific language into another by automatically learning translations using a parallel corpus. SMT has been successful in producing quality translations in many foreign languages, but there are only a few works attempted in South Indian languages. The article discusses on experiments conducted with SMT for Malayalam language and analyzes how the methods defined for SMT in foreign languages affect a Dravidian language, Malayalam. The baseline SMT model does not work for Malayalam due to its unique characteristics like agglutinative nature and morphological richness. Hence, the challenge is to identify where precisely the SMT model has to be modified such that it adapts the challenges of the language peculiarity into the baseline model and give better translations for English to Malayalam translation. The alignments between English and Malayalam sentence pairs, subjected to the training process in SMT, plays a crucial role in producing quality output translation. Therefore, this work focuses on improving the translation model of SMT by refining the alignments between English–Malayalam sentence pairs. The phrase alignment algorithms align the verb and noun phrases in the sentence pairs and develop a new set of alignments for the English–Malayalam sentence pairs. These alignment sets refine the alignments formed from Giza++ produced as a result of EM training algorithm. The improved Phrase-Based SMT model trained using these refined alignments resulted in better translation quality, as indicated by the AER and BLUE scores.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4317502730",
    "type": "article"
  },
  {
    "title": "Improving Multilingual Neural Machine Translation System for Indic Languages",
    "doi": "https://doi.org/10.1145/3587932",
    "publication_date": "2023-03-14",
    "publication_year": 2023,
    "authors": "Sudhansu Bala Das; Atharv Biradar; Tapas Kumar Mishra; Bidyut Kr. Patra",
    "corresponding_authors": "",
    "abstract": "The Machine Translation System (MTS) serves as effective tool for communication by translating text or speech from one language to another language. Recently, neural machine translation (NMT) has become popular for its performance and cost-effectiveness. However, NMT systems are restricted in translating low-resource languages as a huge quantity of data is required to learn useful mappings across languages. The need for an efficient translation system becomes obvious in a large multilingual environment like India. Indian languages (ILs) are still treated as low-resource languages due to unavailability of corpora. In order to address such an asymmetric nature, the multilingual neural machine translation (MNMT) system evolves as an ideal approach in this direction. The MNMT converts many languages using a single model, which is extremely useful in terms of training process and lowering online maintenance costs. It is also helpful for improving low-resource translation. In this article, we propose an MNMT system to address the issues related to low-resource language translation. Our model comprises two MNMT systems, i.e., for English-Indic (one-to-many) and for Indic-English (many-to-one) with a shared encoder-decoder containing 15 language pairs (30 translation directions). Since most of IL pairs have a scanty amount of parallel corpora, not sufficient for training any machine translation model, we explore various augmentation strategies to improve overall translation quality through the proposed model. A state-of-the-art transformer architecture is used to realize the proposed model. In addition, the article addresses the use of language relationships (in terms of dialect, script, etc.), particularly about the role of high-resource languages of the same family in boosting the performance of low-resource languages. Moreover, the experimental results also show the advantage of back-translation and domain adaptation for ILs to enhance the translation quality of both source and target languages. Using all these key approaches, our proposed model emerges to be more efficient than the baseline model in terms of evaluation metrics, i.e., BLEU (BiLingual Evaluation Understudy) score for a set of ILs.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4324157275",
    "type": "article"
  },
  {
    "title": "Knowledge-enhanced Prompt-tuning for Stance Detection",
    "doi": "https://doi.org/10.1145/3588767",
    "publication_date": "2023-03-23",
    "publication_year": 2023,
    "authors": "Hu Huang; Bowen Zhang; Yangyang Li; Baoquan Zhang; Yuxi Sun; Chuyao Luo; Cheng Peng",
    "corresponding_authors": "",
    "abstract": "Investigating public attitudes on social media is important in opinion mining systems. Stance detection aims to analyze the attitude of an opinionated text (e.g., favor, neutral, or against) toward a given target. Existing methods mainly address this problem from the perspective of fine-tuning. Recently, prompt-tuning has achieved success in natural language processing tasks. However, conducting prompt-tuning methods for stance detection in real-world remains a challenge for several reasons: (1) The text form of stance detection is usually short and informal, which makes it difficult to design label words for the verbalizer. (2) The tweet text may not explicitly give the attitude. Instead, users may use various hashtags or background knowledge to express stance-aware perspectives. In this article, we first propose a prompt-tuning-based framework that performs stance detection in a cloze question manner. Specifically, a knowledge-enhanced prompt-tuning framework (KEprompt) method is designed, which consists of an automatic verbalizer (AutoV) and background knowledge injection (BKI). Specifically, in AutoV, we introduce a semantic graph to build a better mapping from the predicted word of the pretrained language model and detection labels. In BKI, we first propose a topic model for learning hashtag representation and introduce ConceptGraph as the supplement of the target. At last, we present a challenging dataset for stance detection, where all stance categories are expressed in an implicit manner. Extensive experiments on a large real-world dataset demonstrate the superiority of KEprompt over state-of-the-art methods.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4360612314",
    "type": "article"
  },
  {
    "title": "Stop the Hate, Spread the Hope: An Ensemble Model for Hope Speech Detection in English and Dravidian Languages",
    "doi": "https://doi.org/10.1145/3716383",
    "publication_date": "2025-02-12",
    "publication_year": 2025,
    "authors": "Deepawali Sharma; Vedika Gupta; Vivek Kumar Singh; Bharathi Raja Chakravarthi",
    "corresponding_authors": "",
    "abstract": "The rise of social media has led to vast amounts of user-generated content, with emotions ranging from joy to anger. Negative comments often target individuals, communities, or brands, prompting successful efforts to detect harmful speech such as hate speech, cyberbullying, and abuse. Recently, another type of speech referred to as ‘Hope Speech’ has gained attention from the research community. Hope speech consists of positive affirmations or words of reassurance, encouragement, consolation or motivation offered to the affected individual/ community during the lean periods of life. However, there has been relatively less research focused on the detection of hope speech, more particularly in low-resource languages. This paper, therefore, attempts to develop an ensemble model for detecting hope speech in some low-resource languages. Data for four different languages, namely English, Kannada, Malayalam and Tamil are obtained and experimented with different deep learning-based models. An ensemble model is proposed to combine the advantages of the better performing models. Experimental results demonstrate the superior performance of the proposed Ensemble (LSTM, mBERT, XLM-RoBERTa) model compared to individual models based on data from all four languages (weighted average F1-score for English is 0.93; for Kannada is 0.74; for Malayalam is 0.82; and for Tamil is 0.60). Thus, the proposed ensemble model proves to be a suitable approach for hope speech detection in the given low resource languages.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4407411181",
    "type": "article"
  },
  {
    "title": "Pivoted Low Resource Multilingual Translation with NER Optimization",
    "doi": "https://doi.org/10.1145/3727876",
    "publication_date": "2025-04-03",
    "publication_year": 2025,
    "authors": "Danang Arbian Sulistyo; Didik Dwi Prasetya; Fadhli Almu’iini Ahda; Aji Prasetya Wibawa",
    "corresponding_authors": "",
    "abstract": "Machine translation (MT) has advanced significantly with neural machine translation (NMT) models like BERT, GPT, and MarianMT, which leverage deep learning to provide more accurate and natural translations. However, low-resource languages such as Javanese and Madurese still face challenges due to limited parallel datasets. Pivot-based translation, which uses an intermediary language like Indonesian, has improved translation quality but struggles with preserving named entities, such as names of people or places. Previous models without Named Entity Recognition (NER) often produce inconsistent or inaccurate translations, losing these critical elements. We propose a hybrid approach to address this, combining NER with pivot-based NMT. By identifying and preserving named entities throughout the translation process, this method enhances translation consistency and accuracy for Javanese-Madurese translations. Our experiments demonstrate that NER-Pivot-NMT achieves perfect preservation rates for named entities, although translation accuracy fluctuates between 0.32 and 0.54 for Javanese-to-Madurese and 0.21 to 0.45 for Madurese-to-Javanese. Despite these variations, the overall translation quality improves compared to traditional direct and pivot translation methods. This study highlights the potential of integrating NER with pivot translation to solve challenges in low-resource language translation. It suggests that future research should further optimize NER models and expand datasets to enhance accuracy and fluency. The findings hold promise for improving translation across other regional languages facing similar constraints.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409118629",
    "type": "article"
  },
  {
    "title": "Chinese Open Relation Extraction and Knowledge Base Establishment",
    "doi": "https://doi.org/10.1145/3162077",
    "publication_date": "2018-02-14",
    "publication_year": 2018,
    "authors": "Shengbin Jia; E Shijia; Maozhen Li; Yang Xiang",
    "corresponding_authors": "",
    "abstract": "Named entity relation extraction is an important subject in the field of information extraction. Although many English extractors have achieved reasonable performance, an effective system for Chinese relation extraction remains undeveloped due to the lack of Chinese annotation corpora and the specificity of Chinese linguistics. Here, we summarize three kinds of unique but common phenomena in Chinese linguistics. In this article, we investigate unsupervised linguistics-based Chinese open relation extraction (ORE), which can automatically discover arbitrary relations without any manually labeled datasets, and research the establishment of a large-scale corpus. By mapping the entity relations into dependency-trees and considering the unique Chinese linguistic characteristics, we propose a novel unsupervised Chinese ORE model based on Dependency Semantic Normal Forms (DSNFs). This model imposes no restrictions on the relative positions among entities and relationships and achieves a high yield by extracting relations mediated by verbs or nouns and processing the parallel clauses. Empirical results from our model demonstrate the effectiveness of this method, which obtains stable performance on four heterogeneous datasets and achieves better precision and recall in comparison with several Chinese ORE systems. Furthermore, a large-scale knowledge base of entity and relation, called COER, is established and published by applying our method to web text, which conquers the trouble of lack of Chinese corpora.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2785631370",
    "type": "article"
  },
  {
    "title": "Urdu Named Entity Recognition and Classification System Using Artificial Neural Network",
    "doi": "https://doi.org/10.1145/3129290",
    "publication_date": "2017-09-15",
    "publication_year": 2017,
    "authors": "Muhammad Kamran Malik",
    "corresponding_authors": "Muhammad Kamran Malik",
    "abstract": "Named Entity Recognition and Classification (NERC) is a process of identifying words and classifying them into person names, location names, organization names, and so on. In this article, we discuss the development of an Urdu Named Entity (NE) corpus, called the Kamran-PU-NE (KPU-NE) corpus, for three entity types, that is, Person, Organization, and Location, and marking the remaining tokens as Others (O). We use two supervised learning algorithms, Hidden Markov Model (HMM) and Artificial Neural Network (ANN), for the development of the Urdu NERC system. We annotate the 652852-token corpus taken from 15 different genres with a total of 44480 NEs. The inter-annotator agreement between the two annotators in terms of Kappa k statistic is 73.41%. With HMM, the highest recorded precision, recall, and f-measure values are 55.98%, 83.11%, and 66.90%, respectively, and with ANN, they are 81.05%, 87.54%, and 84.17%, respectively.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2756253150",
    "type": "article"
  },
  {
    "title": "A Rule-Based Kurdish Text Transliteration System",
    "doi": "https://doi.org/10.1145/3278623",
    "publication_date": "2019-01-18",
    "publication_year": 2019,
    "authors": "Sina Ahmadi",
    "corresponding_authors": "Sina Ahmadi",
    "abstract": "In this article, we present a rule-based approach for transliterating two of the most used orthographies in Sorani Kurdish. Our work consists of detecting a character in a word by removing the possible ambiguities and mapping it into the target orthography. We describe different challenges in Kurdish text mining and propose novel ideas concerning the transliteration task for Sorani Kurdish. Our transliteration system, named Wergor , achieves 82.79% overall precision and more than 99% in detecting the double-usage characters. We also present a manually transliterated corpus for Kurdish.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2963265201",
    "type": "article"
  },
  {
    "title": "Toward Integrated CNN-based Sentiment Analysis of Tweets for Scarce-resource Language—Hindi",
    "doi": "https://doi.org/10.1145/3450447",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Vedika Gupta; Nikita Jain; Shubham Shubham; Agam Madan; Ankit Chaudhary; Qin Xin",
    "corresponding_authors": "",
    "abstract": "Linguistic resources for commonly used languages such as English and Mandarin Chinese are available in abundance, hence the existing research in these languages. However, there are languages for which linguistic resources are scarcely available. One of these languages is the Hindi language. Hindi, being the fourth-most popular language, still lacks in richly populated linguistic resources, owing to the challenges involved in dealing with the Hindi language. This article first explores the machine learning-based approaches—Naïve Bayes, Support Vector Machine, Decision Tree, and Logistic Regression—to analyze the sentiment contained in Hindi language text derived from Twitter. Further, the article presents lexicon-based approaches (Hindi Senti-WordNet, NRC Emotion Lexicon) for sentiment analysis in Hindi while also proposing a Domain-specific Sentiment Dictionary. Finally, an integrated convolutional neural network (CNN)—Recurrent Neural Network and Long Short-term Memory—is proposed to analyze sentiment from Hindi language tweets, a total of 23,767 tweets classified into positive, negative, and neutral. The proposed CNN approach gives an accuracy of 85%.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3176502972",
    "type": "article"
  },
  {
    "title": "A Hindi Image Caption Generation Framework Using Deep Learning",
    "doi": "https://doi.org/10.1145/3432246",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Santosh Kumar Mishra; Rijul Dhir; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Image captioning is the process of generating a textual description of an image that aims to describe the salient parts of the given image. It is an important problem, as it involves computer vision and natural language processing, where computer vision is used for understanding images, and natural language processing is used for language modeling. A lot of works have been done for image captioning for the English language. In this article, we have developed a model for image captioning in the Hindi language. Hindi is the official language of India, and it is the fourth most spoken language in the world, spoken in India and South Asia. To the best of our knowledge, this is the first attempt to generate image captions in the Hindi language. A dataset is manually created by translating well known MSCOCO dataset from English to Hindi. Finally, different types of attention-based architectures are developed for image captioning in the Hindi language. These attention mechanisms are new for the Hindi language, as those have never been used for the Hindi language. The obtained results of the proposed model are compared with several baselines in terms of BLEU scores, and the results show that our model performs better than others. Manual evaluation of the obtained captions in terms of adequacy and fluency also reveals the effectiveness of our proposed approach. Availability of resources : The codes of the article are available at https://github.com/santosh1821cs03/Image_Captioning_Hindi_Language ; The dataset will be made available: http://www.iitp.ac.in/∼ai-nlp-ml/resources.html .",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3138298063",
    "type": "article"
  },
  {
    "title": "An Improved English-to-Mizo Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3445974",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Candy Lalrempuii; Badal Soni; Partha Pakray",
    "corresponding_authors": "",
    "abstract": "Machine Translation is an effort to bridge language barriers and misinterpretations, making communication more convenient through the automatic translation of languages. The quality of translations produced by corpus-based approaches predominantly depends on the availability of a large parallel corpus. Although machine translation of many Indian languages has progressively gained attention, there is very limited research on machine translation and the challenges of using various machine translation techniques for a low-resource language such as Mizo. In this article, we have implemented and compared statistical-based approaches with modern neural-based approaches for the English–Mizo language pair. We have experimented with different tokenization methods, architectures, and configurations. The performance of translations predicted by the trained models has been evaluated using automatic and human evaluation measures. Furthermore, we have analyzed the prediction errors of the models and the quality of predictions based on variations in sentence length and compared the model performance with the existing baselines.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3165048362",
    "type": "article"
  },
  {
    "title": "Two-channel Attention Mechanism Fusion Model of Stock Price Prediction Based on CNN-LSTM",
    "doi": "https://doi.org/10.1145/3453693",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Lin Sun; Wenzheng Xu; Jimin Liu",
    "corresponding_authors": "",
    "abstract": "Using hierarchical CNN, the company's multiple news is characterized as three levels: sentence vectors, chapter vectors, and enterprise sentiment vectors. By combining the stock price data with the news lyric data at the same time, the influence of news on price is used to achieve correlation analysis of news information and stock prices. A two-channel attention mechanism fusion model based on CNN-LSTM is proposed. After the dual-channel feature extraction, the attention layer fusion layer is used to convert the weighted values of LSTM hidden variables, so the stock price can be predicted with the news text.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3183739234",
    "type": "article"
  },
  {
    "title": "HateCircle and Unsupervised Hate Speech Detection Incorporating Emotion and Contextual Semantics",
    "doi": "https://doi.org/10.1145/3576913",
    "publication_date": "2022-12-19",
    "publication_year": 2022,
    "authors": "Sayani Ghosal; Amita Jain",
    "corresponding_authors": "",
    "abstract": "The explosive growth of social media has fueled an extensive increase in online freedom of speech. The worldwide platform of human voice creates possibilities to assail other users without facing any consequences, and flout social etiquettes, resulting in an inevitable increase of hate speech. Nowadays, English hate speech detection is a popular research area, but the prevalence of implicit hate content in regional languages desire effective language-independent models. The proposed research is the first unsupervised Hindi and Bengali hate content detection framework consisting of three significant concepts: HateCircle, hate tweet classification, and code-switch data preparation algorithms. The novel HateCircle method is proposed to detect hate orientation for each term by co-occurrence patterns of words, contextual semantics, and emotion analysis. The efficient multiclass hate tweet classification algorithm is proposed with parts of speech tagging, Euclidean distance, and the Geometric median methods. The detection of hate content is more efficient in the native script compared to the Roman script, so the transliteration algorithm is also proposed for code-switch data preparation. The experimentation evaluates the combination of various lexicons with our enriched hate lexicon that achieves a maximum of 0.74 F1-score for the Hindi and 0.88 F1-score for the Bengali datasets. The novel HateCircle and hate tweet detection framework evaluates with our proposed parts of speech tagging and Geometric median detection methods. Results reveal that HateCircle and hate tweet detection framework also achieves a maximum of 0.73 accuracy for the Hindi and 0.78 accuracy for the Bengali dataset. The experiment results signify that contextual semantic hate speech detection research with a language-independency feature offsets the growth of implicit abusive text in social media.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4313408276",
    "type": "article"
  },
  {
    "title": "An Effective Learning Evaluation Method Based on Text Data with Real-time Attribution - A Case Study for Mathematical Class with Students of Junior Middle School in China",
    "doi": "https://doi.org/10.1145/3474367",
    "publication_date": "2022-03-16",
    "publication_year": 2022,
    "authors": "Shuai Liu; Tenghui He; Jingyi Li; Yating Li; Akshi Kumar",
    "corresponding_authors": "",
    "abstract": "In today's intelligent age, the vigorous development of education-based information analysis technology has had a profound impact on the education and teaching process. The use of computational linguistics technology to extract teaching data for learning evaluation is an important hot domain in this research field. Therefore, the study of student learning assessment methods based on text data has become a key issue. The text data extracted from the education process has attributes related to time and operational attributes, which are important indicators to measure the effect of student learning effect. However, these attributes are not focused by the traditional educational effect evaluation method, which make the learning effect of students difficult to measure comprehensively and effectively. In response to this problem, this article first uses perception technology to extract learning text data based on time and operational attributes. Secondly, according to the real-time attributes of text data, such as time and operation attributes, a learning evaluation method based on real-time text data is proposed. Finally, this article compares the traditional evaluation method with the proposed method. The results show that using real-time attribute text data is more effective in students’ learning measure.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4220912911",
    "type": "article"
  },
  {
    "title": "Scene Graph Semantic Inference for Image and Text Matching",
    "doi": "https://doi.org/10.1145/3563390",
    "publication_date": "2022-09-14",
    "publication_year": 2022,
    "authors": "Jiaming Pei; Kaiyang Zhong; Zhi Yu; Lukun Wang; Kuruva Lakshmanna",
    "corresponding_authors": "",
    "abstract": "With the rapid development of information technology, image and text data have increased dramatically. Image and text matching techniques enable computers to understand information from both visual and text modalities and match them based on semantic content. Existing methods focus on visual and textual object co-occurrence statistics and learning coarse-level associations. However, the lack of intramodal semantic inference leads to the failure of fine-level association between modalities. Scene graphs can capture the interactions between visual and textual objects and model intramodal semantic associations, which are crucial for the understanding of scenes contained in images and text. In this article, we propose a novel scene graph semantic inference network (SGSIN) for image and text matching that effectively learns fine-level semantic information in vision and text to facilitate bridging cross-modal discrepancies. Specifically, we design two matching modules and construct scene graphs within each matching module for aggregating neighborhood information to refine the semantic representation of each object and achieve fine-level alignment of visual and textual modalities. We perform extended experiments in Flickr30K and MSCOCO and achieve state-of-the-art results, which validate the advantages of our proposed approach.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4295679074",
    "type": "article"
  },
  {
    "title": "Rule Based Fuzzy Computing Approach on Self-Supervised Sentiment Polarity Classification with Word Sense Disambiguation in Machine Translation for Hindi Language",
    "doi": "https://doi.org/10.1145/3574130",
    "publication_date": "2023-02-22",
    "publication_year": 2023,
    "authors": "Shweta Chauhan; Jayashree Premkumar Shet; Shehab Mohamed Beram; Vishal Jagota; Mohammed Dighriri; Mohd Wazih Ahmad; Md Shamim Hossain; Ali Rizwan",
    "corresponding_authors": "",
    "abstract": "With increasing globalization, communication among people of diverse cultural backgrounds is also taking place to a very large extent in the present era. Issues like language diversity in various parts of the world can lead to hindrance in communication. The usage of social media and user-generated material has grown at an exponential rate and existing supervised sentiment polarity classification techniques need labelling for the training dataset. In this study, two problems have been analyzed. First, sentiment analysis of the Twitter dataset and sense disambiguation of morphologically rich Hindi language. A rule-based fuzzy logics-based system for self-supervised sentiment classification was used to compute and analyze the self-supervised or completely unsupervised sentiment categorization of a social-media dataset using three types of lexicons. The combination of fuzzy with three different types of lexicons gives sentiment analysis a new path. The unsupervised fuzzy rules integrate the fuzziness of both negative as well as positive scores, and fuzzy logic-based systems can cope with ambiguity and vagueness. The fuzzy-system uses an unsupervised/self-supervised fuzzy rule-based technique to identify text using natural language processing (NLP) and sense of word. We compared the results of fuzzy rule based self-supervised sentiment classification by using three types of lexicons on five different datasets, with unsupervised as well as supervised sentiment classification techniques. Second, using cross-lingual sense embedding rather than cross-lingual word embedding resolves the ambiguity issue. The word sense embeddings are produced for the source languages to learn multiple or various senses of the words. Different evaluation metrics depict an improved performance for English-Hindi language.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4321494962",
    "type": "article"
  },
  {
    "title": "Part-of-Speech Tagging of Odia Language Using Statistical and Deep Learning Based Approaches",
    "doi": "https://doi.org/10.1145/3588900",
    "publication_date": "2023-03-30",
    "publication_year": 2023,
    "authors": "Tusarkanta Dalai; Tapas Kumar Mishra; Pankaj Kumar",
    "corresponding_authors": "",
    "abstract": "Automatic part-of-speech (POS) tagging is a preprocessing step of many natural language processing tasks, such as named entity recognition, speech processing, information extraction, word sense disambiguation, and machine translation. It has already gained promising results in English and European languages. However, in Indian languages, particularly in the Odia language, it is not yet well explored because of the lack of supporting tools, resources, and morphological richness of the language. Unfortunately, we were unable to locate an open source POS tagger for the Odia language, and only a handful of attempts have been made to develop POS taggers for the Odia language. The main contribution of this research work is to present statistical approaches such as the maximum entropy Markov model and conditional random field (CRF), as well as deep learning based approaches, including the convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM) to develop the Odia POS tagger. A publicly accessible corpus annotated with the Bureau of Indian Standards (BIS) tagset is used in our work. However, most of the languages around the globe have used the dataset annotated with the Universal Dependencies (UD) tagset. Hence, to maintain uniformity, the Odia dataset should use the same tagset. Thus, following the BIS and UD guidelines, we constructed a mapping from the BIS tagset to the UD tagset. The maximum entropy Markov model, CRF, Bi-LSTM, and CNN models are trained using the Indian Languages Corpora Initiative corpus with the BIS and UD tagsets. We have experimented with various feature sets as input to the statistical models to prepare a baseline system and observed the impact of constructed feature sets. The deep learning based model includes the Bi-LSTM network, the CNN network, the CRF layer, character sequence information, and a pre-trained word vector. Seven different combinations of neural sequence labeling models are implemented, and their performance measures are investigated. It has been observed that the Bi-LSTM model with the character sequence feature and pre-trained word vector achieved a result with 94.58% accuracy.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4361271935",
    "type": "article"
  },
  {
    "title": "Detection of Hateful Social Media Content for Arabic Language",
    "doi": "https://doi.org/10.1145/3592792",
    "publication_date": "2023-04-17",
    "publication_year": 2023,
    "authors": "Rogayah M. Al-Ibrahim; Mostafa Z. Ali; Hassan Najadat",
    "corresponding_authors": "",
    "abstract": "Social media is a common medium for expression of views, discussion, sharing of content, and promotion of products and ideas. These views are either polite or obscene. The growth of hate speech is one of the negative aspects of the medium and its emergence poses risk factors for society at various levels. Although there are rules and laws for these platforms, they cannot oversee and control all types of content. Thus, there is an urgent need to develop modern algorithms to automatically detect hateful content on social media. Arab society is not isolated from the world, and the usage of social media by its members has highlighted the importance of automated systems that help build an electronic society free of hate and aggression. This article aims to detect hate speech based on Arabic context over the Twitter platform by proposing different novel deep learning architectures in order to provide a thorough analytical study. Also, a comparative study is presented with a different well-known machine learning algorithm, as well as other state-of-the-art algorithms from the literature to be used as a beacon for interested researchers. These models have been applied to the Arabic tweets dataset, which included 15K tweets and 14 features. After training these models, the results obtained for the top two models included an improved bidirectional long short-term memory with an accuracy of 92.20% and a macro F1-score of 92% and a modified convolutional neural network with an accuracy of 92.10% and a macro F1-score of 91%. The results also showed the superiority of the performance of the deep learning models over other models in terms of accuracy.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4366087566",
    "type": "article"
  },
  {
    "title": "A New English/Arabic Parallel Corpus for Phishing Emails",
    "doi": "https://doi.org/10.1145/3606031",
    "publication_date": "2023-06-28",
    "publication_year": 2023,
    "authors": "Said A. Salloum; Tarek Gaber; Sunil Vadera; Khaled Shaalan",
    "corresponding_authors": "",
    "abstract": "Phishing involves malicious activity whereby phishers, in the disguise of legitimate entities, obtain illegitimate access to the victims’ personal and private information, usually through emails. Currently, phishing attacks and threats are being handled effectively through the use of the latest phishing email detection solutions. Most current phishing detection systems assume phishing attacks to be in English, though attacks in other languages are growing. In particular, Arabic is a widely used language and therefore represents a vulnerable target. However, there is a significant shortage of corpora that can be used to develop Arabic phishing detection systems. This article presents the development of a new English-Arabic parallel phishing email corpus that has been developed from the anti-phishing share task text (IWSPA-AP 2018). The email content was to be translated, and the task had been allotted to 10 volunteers who had a university background and were English and Arabic language experts. To evaluate the effectiveness of the new corpus, we develop phishing email detection models using Term Frequency–Inverse Document Frequency and Multilayer Perceptron using 1,258 emails in Arabic and English that have equal ratios of legitimate and phishing emails. The experimental findings show that the accuracy reaches 96.82% for the Arabic dataset and 94.63% for the emails in English, providing some assurance of the potential value of the parallel corpus developed.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4382397213",
    "type": "article"
  },
  {
    "title": "MIMIC: Misogyny Identification in Multimodal Internet Content in Hindi-English Code-Mixed Language",
    "doi": "https://doi.org/10.1145/3656169",
    "publication_date": "2024-04-04",
    "publication_year": 2024,
    "authors": "Aakash Singh; Deepawali Sharma; Vivek Kumar Singh",
    "corresponding_authors": "",
    "abstract": "Over the years, social media has emerged as one of the most popular platforms where people express their views and share thoughts about various aspects. The social media content now includes a variety of components such as text, images, videos etc. One type of interest is memes, which often combine text and images. It is relevant to mention here that, social media being an unregulated platform, sometimes also has instances of discriminatory, offensive and hateful content being posted. Such content adversely affects the online well-being of the users. Therefore, it is very important to develop computational models to automatically detect such content so that appropriate corrective action can be taken. Accordingly, there have been research efforts on automatic detection of such content focused mainly on the texts. However, the fusion of multimodal data (as in memes) creates various challenges in developing computational models that can handle such data, more so in the case of low-resource languages. Among such challenges, the lack of suitable datasets for developing computational models for handling memes in low-resource languages is a major problem. This work attempts to bridge the research gap by providing a large-sized curated dataset comprising 5,054 memes in Hindi-English code-mixed language, which are manually annotated by three independent annotators. It comprises two subtasks: (i) Subtask-1 (Binary classification involving tagging a meme as misogynous or non-misogynous), and (ii) Subtask-2 (multi-label classification of memes into different categories). The data quality is evaluated by computing Krippendorff's alpha. Different computational models are then applied on the data in three settings: text-only, image-only, and multimodal models using fusion techniques. The results show that the proposed multimodal method using the fusion technique may be the preferred choice for the identification of misogyny in multimodal Internet content and that the dataset is suitable for advancing research and development in the area.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4393934416",
    "type": "article"
  },
  {
    "title": "Neural Machine Translation for Low-Resource Languages from a Chinese-centric Perspective: A Survey",
    "doi": "https://doi.org/10.1145/3665244",
    "publication_date": "2024-05-16",
    "publication_year": 2024,
    "authors": "Jinyi Zhang; Ke Su; Haowei Li; Jiannan Mao; Ye Tian; Feng Wen; Chong Guo; Tadahiro Matsumoto",
    "corresponding_authors": "",
    "abstract": "Machine translation–the automatic transformation of one natural language (source language) into another (target language) through computational means–occupies a central role in computational linguistics and stands as a cornerstone of research within the field of Natural Language Processing (NLP). In recent years, the prominence of Neural Machine Translation (NMT) has grown exponentially, offering an advanced framework for machine translation research. It is noted for its superior translation performance, especially when tackling the challenges posed by low-resource language pairs that suffer from a limited corpus of data resources. This article offers an exhaustive exploration of the historical trajectory and advancements in NMT, accompanied by an analysis of the underlying foundational concepts. It subsequently provides a concise demarcation of the unique characteristics associated with low-resource languages and presents a succinct review of pertinent translation models and their applications, specifically within the context of languages with low-resources. Moreover, this article delves deeply into machine translation techniques, highlighting approaches tailored for Chinese-centric low-resource languages. Ultimately, it anticipates upcoming research directions in the realm of low-resource language translation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4396976063",
    "type": "article"
  },
  {
    "title": "Abusive and Hate speech Classification in Arabic Text Using Pre-trained Language Models and Data Augmentation",
    "doi": "https://doi.org/10.1145/3679049",
    "publication_date": "2024-08-03",
    "publication_year": 2024,
    "authors": "Nabil Badri; Férihane Kboubi; Anja Habacha Chaïbi",
    "corresponding_authors": "",
    "abstract": "Hateful content on social media is a worldwide problem that adversely affects not just the targeted individuals but also anyone whose content is accessible. The majority of studies that looked at the automatic identification of inappropriate content addressed the English language, given the availability of resources. Therefore, there are still a number of low-resource languages that need more attention from the community. This article focuses on the Arabic dialect, which has several specificities that make the use of non-Arabic models inappropriate. Our hypothesis is that leveraging pre-trained language models (PLMs) specifically designed for Arabic, along with data augmentation techniques, can significantly enhance the detection of hate speech in Arabic mono- and multi-dialect texts. To test this hypothesis, we conducted a series of experiments addressing three key research questions: (RQ1) Does text augmentation enhance the final results compared to using an unaugmented dataset? (RQ2) Do Arabic PLMs outperform other models utilizing techniques such as fastText and AraVec word embeddings? (RQ3) Does training and fine-tuning models on a multilingual dataset yield better results than training them on a monolingual dataset? Our methodology involved the comparison of PLMs based on transfer learning, specifically examining the performance of DziriBERT, AraBERT v2, and BERT-base-arabic models. We implemented text augmentation techniques and evaluated their impact on model performance. The tools used included fastText and AraVec for word embeddings, as well as various PLMs for transfer learning. The results demonstrate a notable improvement in classification accuracy, with augmented datasets showing an increase in performance metrics (accuracy, precision, recall, and F1-score) by up to 15–21% compared to non-augmented datasets. This underscores the potential of data augmentation in enhancing the models’ ability to generalize across the nuanced spectrum of Arabic dialects.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4401288511",
    "type": "article"
  },
  {
    "title": "Improved Regression Analysis with Ensemble Pipeline Approach for Applications across Multiple Domains",
    "doi": "https://doi.org/10.1145/3645110",
    "publication_date": "2024-02-08",
    "publication_year": 2024,
    "authors": "Debajyoty Banik; Rahul Paul; Rajkumar Singh Rathore; Rutvij H. Jhaveri",
    "corresponding_authors": "",
    "abstract": "In this research, we introduce two new machine learning regression methods: the Ensemble Average and the Pipelined Model. These methods aim to enhance traditional regression analysis for predictive tasks and have undergone thorough evaluation across three datasets, Kaggle House Price, Boston House Price, and California Housing, using various performance metrics. The results consistently show that our models outperform existing methods in terms of accuracy and reliability across all three datasets. The Pipelined Model, in particular, is notable for its ability to combine predictions from multiple models, leading to higher accuracy and impressive scalability. This scalability allows for their application in diverse fields like technology, finance, and healthcare. Furthermore, these models can be adapted for real-time and streaming data analysis, making them valuable for applications such as fraud detection, stock market prediction, and IoT sensor data analysis. Enhancements to the models also make them suitable for big data applications, ensuring their relevance for large datasets and distributed computing environments. It is important to acknowledge some limitations of our models, including potential data biases, specific assumptions, increased complexity, and challenges related to interpretability when using them in practical scenarios. Nevertheless, these innovations advance predictive modeling, and our comprehensive evaluation underscores their potential to provide increased accuracy and reliability across a wide range of applications. The results indicate that the proposed models outperform existing models in terms of accuracy and robustness for all three datasets. The source code can be found at https://huggingface.co/DebajyotyBanik/Ensemble-Pipelined-Regression/tree/main",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4391645794",
    "type": "article"
  },
  {
    "title": "Medical Question Summarization with Entity-driven Contrastive Learning",
    "doi": "https://doi.org/10.1145/3652160",
    "publication_date": "2024-03-11",
    "publication_year": 2024,
    "authors": "Wenpeng Lü; Sibo Wei; Xueping Peng; Yifei Wang; Usman Naseem; Shoujin Wang",
    "corresponding_authors": "",
    "abstract": "By summarizing longer consumer health questions into shorter and essential ones, medical question-answering systems can more accurately understand consumer intentions and retrieve suitable answers. However, medical question summarization is very challenging due to obvious distinctions in health trouble descriptions from patients and doctors. Although deep learning has been applied to successfully address the medical question summarization (MQS) task, two challenges remain: how to correctly capture question focus to model its semantic intention, and how to obtain reliable datasets to fairly evaluate performance. To address these challenges, this article proposes a novel medical question summarization framework based on e ntity-driven c ontrastive l earning (ECL). ECL employs medical entities present in frequently asked questions (FAQs) as focuses and devises an effective mechanism to generate hard negative samples. This approach compels models to focus on essential information and consequently generate more accurate question summaries. Furthermore, we have discovered that some MQS datasets, such as the iCliniq dataset with a 33% duplicate rate, have significant data leakage issues. To ensure an impartial evaluation of the related methods, this article carefully examines leaked samples to reorganize more reasonable datasets. Extensive experiments demonstrate that our ECL method outperforms the existing methods and achieves new state-of-the-art performance, i.e., 52.85, 43.16, 41.31, 43.52 in terms of ROUGE-1 metric on MeQSum, CHQ-Summ, iCliniq, HealthCareMagic dataset, respectively. The code and datasets are available at https://github.com/yrbobo/MQS-ECL",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4392645476",
    "type": "article"
  },
  {
    "title": "Multilingual Neural Machine Translation for Indic to Indic Languages",
    "doi": "https://doi.org/10.1145/3652026",
    "publication_date": "2024-03-12",
    "publication_year": 2024,
    "authors": "Sudhansu Bala Das; Divyajyoti Panda; Tapas Kumar Mishra; Bidyut Kr. Patra; Asif Ekbal",
    "corresponding_authors": "",
    "abstract": "The method of translation from one language to another without human intervention is known as Machine Translation (MT). Multilingual neural machine translation (MNMT) is a technique for MT that builds a single model for multiple languages. It is preferred over other approaches, since it decreases training time and improves translation in low-resource contexts, i.e., for languages that have insufficient corpus. However, good-quality MT models are yet to be built for many scenarios such as for Indic-to-Indic Languages (IL-IL). Hence, this article is an attempt to address and develop the baseline models for low-resource languages i.e., IL-IL (for 11 Indic Languages (ILs)) in a multilingual environment. The models are built on the Samanantar corpus and analyzed on the Flores-200 corpus. All the models are evaluated using standard evaluation metrics i.e., Bilingual Evaluation Understudy (BLEU) score (with the range of 0 to 100). This article examines the effect of the grouping of related languages, namely, East Indo-Aryan (EI), Dravidian (DR), and West Indo-Aryan (WI) on the MNMT model. From the experiments, the results reveal that related language grouping is beneficial for the WI group only while it is detrimental for the EI group and it shows an inconclusive effect on the DR group. The role of pivot-based MNMT models in enhancing translation quality is also investigated in this article. Owing to the presence of large good-quality corpora from English (EN) to ILs, MNMT IL-IL models using EN as a pivot are built and examined. To achieve this, English-Indic Language (EN-IL) models are developed with and without the usage of related languages. Results show that the use of related language grouping is advantageous specifically for EN to ILs. Thus, related language groups are used for the development of pivot MNMT models. It is also observed that the usage of pivot models greatly improves MNMT baselines. Furthermore, the effect of transliteration on ILs is also analyzed in this article. To explore transliteration, the best MNMT models from the previous approaches (in most of cases pivot model using related groups) are determined and built on corpus transliterated from the corresponding scripts to a modified Indian language Transliteration script (ITRANS). The outcome of the experiments indicates that transliteration helps the models built for lexically rich languages, with the best increment of BLEU scores observed in Malayalam (ML) and Tamil (TA), i.e., 6.74 and 4.72, respectively. The BLEU score using transliteration models ranges from 7.03 to 24.29. The best model obtained is the Punjabi (PA)-Hindi (HI) language pair trained on PA-WI transliterated corpus.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4392715732",
    "type": "article"
  },
  {
    "title": "Human vs. Machine: A Comparative Study on the Detection of AI-Generated Content",
    "doi": "https://doi.org/10.1145/3708889",
    "publication_date": "2024-12-19",
    "publication_year": 2024,
    "authors": "Amal Boutadjine; Fouzi Harrag; Khaled Shaalan",
    "corresponding_authors": "",
    "abstract": "The surge in advancements in large language models (LLMs) has expedited the generation of synthetic text imitating human writing styles. This, however, raises concerns about the potential misuse of synthetic textual data, which could compromise trust in online content. Against this backdrop, the present research aims to address the key challenges of detecting LLMs-generated texts. In this study, we used ChatGPT (v 3.5) because of its widespread and capability to comprehend and keep conversational context, allowing it to produce meaningful and contextually suitable responses. The problem revolves around the task of discerning between authentic and artificially generated textual content. To tackle this problem, we first created a dataset containing both real and DeepFake text. Subsequently, we employed transfer-learning (TL) and conducted DeepFake-detection utilizing SOTA large pre-trained LLMs. Furthermore, we conducted validation using benchmark datasets comprising unseen data samples to ensure that the model's performance reflects its ability to generalize to new data. Finally, we discussed this study's theoretical contributions, practical implications, limitations and potential avenues for future research, aiming to formulate strategies for identifying and detecting large-generative-models’ produced texts. The results were promising, with accuracy ranging from 94% to 99%. The comparison between automatic detection and the human ability to detect DeepFake text revealed a significant gap in the human capacity for its identification, emphasizing an increasing need for sophisticated automated detectors. The investigation into AI-generated content detection holds central importance in the age of LLMs and technology convergence. This study is both timely and adds value to the ongoing discussion regarding the challenges associated with the pertinent theme of \"DeepFake text detection\", with a special focus on examining the boundaries of human detection.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4405580523",
    "type": "article"
  },
  {
    "title": "A Hybrid Feature Extraction Algorithm for Devanagari Script",
    "doi": "https://doi.org/10.1145/2710018",
    "publication_date": "2015-11-21",
    "publication_year": 2015,
    "authors": "Deepti Khanduja; Neeta Nain; Subhash Panwar",
    "corresponding_authors": "",
    "abstract": "The efficiency of any character recognition technique is directly dependent on the accuracy of the generated feature set that could uniquely represent a character and hence correctly recognize it. This article proposes a hybrid approach combining the structural features of the character and a mathematical model of curve fitting to simulate the best features of a character. As a preprocessing step, skeletonization of the character is performed using an iterative thinning algorithm based on Raster scan of the character image. Then, a combination of structural features of the character like number of endpoints, loops, and intersection points is calculated. Further, the thinned character image is statistically zoned into partitions, and a quadratic curve-fitting model is applied on each partition forming a feature vector of the coefficients of the optimally fitted curve. This vector is combined with the spatial distribution of the foreground pixels for each zone and hence script-independent feature representation. The approach has been evaluated experimentally on Devanagari scripts. The algorithm achieves an average recognition accuracy of 93.4%.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2249492248",
    "type": "article"
  },
  {
    "title": "Words Are Important",
    "doi": "https://doi.org/10.1145/3195633",
    "publication_date": "2018-05-29",
    "publication_year": 2018,
    "authors": "Mohammad Ehsan Basiri; Arman Kabiri",
    "corresponding_authors": "",
    "abstract": "Lexicon-based sentiment analysis (SA) aims to address the problem of extracting people’s opinions from their comments on the Web using a predefined lexicon of opinionated words. In contrast to the machine learning (ML) approach, lexicon-based methods are domain-independent methods that do not need a large annotated training corpus and hence are faster. This makes the lexicon-based approach prevalent in the SA community. However, the story is different for the Persian language. In contrast to English, using the lexicon-based method in Persian is a new discipline. There are rather limited resources available for SA in Persian, making the accuracy of the existing lexicon-based methods lower than other languages. In the current study, first an exhaustive investigation of the lexicon-based method is performed. Then two new resources are introduced to address the problem of resource scarcity for SA in Persian: a carefully labeled lexicon of sentiment words, PerLex, and a new handmade dataset of about 16,000 rated documents, PerView. Moreover, a new hybrid method using both ML and the lexicon-based approach is presented in which PerLex words are used to train the ML algorithm. Experiments are carried out on our new PerView dataset. Results indicate that the accuracy of PerLex is higher than the existing CNRC, Adjectives, SentiStrength, PerSent, and LexiPers lexicons. In addition, the results show that using PerLex significantly decreases the execution time of the proposed system in comparison to the above-mentioned lexicons. Moreover, the results demonstrate the excellence of using opinionated lexicon terms followed by bigrams as the features employed in the ML method.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2806102057",
    "type": "article"
  },
  {
    "title": "A Deep Neural Network Framework for English Hindi Question Answering",
    "doi": "https://doi.org/10.1145/3359988",
    "publication_date": "2019-11-21",
    "publication_year": 2019,
    "authors": "Deepak Gupta; Asif Ekbal; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a unified deep neural network framework for multilingual question answering (QA). The proposed network deals with the multilingual questions and answers snippets. The input to the network is a pair of factoid question and snippet in the multilingual environment (English and Hindi), and output is the relevant answer from the snippet. We begin by generating the snippet using a graph-based language-independent algorithm, which exploits the lexico-semantic similarity between the sentences. The soft alignment of the question words from the English and Hindi languages has been used to learn the shared representation of the question. The learned shared representation of question and attention-based snippet representation are passed as an input to the answer extraction layer of the network, which extracts the answer span from the snippet. Evaluation on a standard multilingual QA dataset shows the state-of-the-art performance with 39.44 Exact Match (EM) and 44.97 F1 values. Similarly, we achieve the performance of 50.11 Exact Match (EM) and 53.77 F1 values on Translated SQuAD dataset.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2990069357",
    "type": "article"
  },
  {
    "title": "A Hybrid Model for Chinese Spelling Check",
    "doi": "https://doi.org/10.1145/3047405",
    "publication_date": "2017-03-30",
    "publication_year": 2017,
    "authors": "Hai Zhao; Deng Cai; Yang Xin; Yuzhu Wang; Zhongye Jia",
    "corresponding_authors": "",
    "abstract": "Spelling check for Chinese has more challenging difficulties than that for other languages. A hybrid model for Chinese spelling check is presented in this article. The hybrid model consists of three components: one graph-based model for generic errors and two independently trained models for specific errors. In the graph model, a directed acyclic graph is generated for each sentence, and the single-source shortest-path algorithm is performed on the graph to detect and correct general spelling errors at the same time. Prior to that, two types of errors over functional words (characters) are first solved by conditional random fields: the confusion of “在” ( at ) (pinyin is zai in Chinese), “再” ( again , more , then ) (pinyin: zai ) and “的” ( of ) (pinyin: de ), “地” (- ly , adverb-forming particle) (pinyin: de ), and “得” ( so that , have to ) (pinyin: de ). Finally, a rule-based model is exploited to distinguish pronoun usage confusion: “她” ( she ) (pinyin: ta ), “他” ( he ) (pinyin: ta ), and some other common collocation errors. The proposed model is evaluated on the standard datasets released by the SIGHAN Bake-off shared tasks, giving state-of-the-art results.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2601201959",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis of Iraqi Arabic Dialect on Facebook Based on Distributed Representations of Documents",
    "doi": "https://doi.org/10.1145/3278605",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Anwar Alnawas; Nursal Arıcı",
    "corresponding_authors": "",
    "abstract": "Nowadays, social media is used by many people to express their opinions about a variety of topics. Opinion Mining or Sentiment Analysis techniques extract opinions from user generated contents. Over the years, a multitude of Sentiment Analysis studies has been done about the English language with deficiencies of research in all other languages. Unfortunately, Arabic is one of the languages that seems to lack substantial research, despite the rapid growth of its use on social media outlets. Furthermore, specific Arabic dialects should be studied, not just Modern Standard Arabic. In this paper, we experiment sentiments analysis of Iraqi Arabic dialect using word embedding. First, we made a large corpus from previous works to learn word representations. Second, we generated word embedding model by training corpus using Doc2Vec representations based on Paragraph and Distributed Memory Model of Paragraph Vectors (DM-PV) architecture. Lastly, the represented feature used for training four binary classifiers (Logistic Regression, Decision Tree, Support Vector Machine and Naive Bayes) to detect sentiment. We also experimented different values of parameters (window size, dimension and negative samples). In the light of the experiments, it can be concluded that our approach achieves a better performance for Logistic Regression and Support Vector Machine than the other classifiers.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2909471801",
    "type": "article"
  },
  {
    "title": "Arabic Cross-Language Information Retrieval",
    "doi": "https://doi.org/10.1145/2789210",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Bilel Elayeb; Ibrahim Bounhas",
    "corresponding_authors": "",
    "abstract": "Cross-language information retrieval (CLIR) deals with retrieving relevant documents in one language using queries expressed in another language. As CLIR tools rely on translation techniques, they are challenged by the properties of highly derivational and flexional languages like Arabic. Much work has been done on CLIR for different languages including Arabic. In this article, we introduce the reader to the motivations for solving some problems related to Arabic CLIR approaches. The evaluation of these approaches is discussed starting from the 2001 and 2002 TREC Arabic CLIR tracks, which aim to objectively evaluate CLIR systems. We also study many other research works to highlight the unresolved problems or those that require further investigation. These works are discussed in the light of a deep study of the specificities and the tasks of Arabic information retrieval (IR). Particular attention is given to translation techniques and CLIR resources, which are key issues challenging Arabic CLIR. To push research in this field, we discuss how a new standard collection can improve Arabic IR and CLIR tracks.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2327415498",
    "type": "article"
  },
  {
    "title": "Deep Learning for Arabic Error Detection and Correction",
    "doi": "https://doi.org/10.1145/3373266",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Manar Alkhatib; Azza Abdel Monem; Khaled Shaalan",
    "corresponding_authors": "",
    "abstract": "Research on tools for automating the proofreading of Arabic text has received much attention in recent years. There is an increasing demand for applications that can detect and correct Arabic spelling and grammatical errors to improve the quality of Arabic text content and application input. Our review of previous studies indicates that few Arabic spell-checking research efforts appropriately address the detection and correction of ill-formed words that do not conform to the Arabic morphology system. Even fewer systems address the detection and correction of erroneous well-formed Arabic words that are either contextually or semantically inconsistent within the text. We introduce an approach that investigates employing deep neural network technology for error detection in Arabic text. We have developed a systematic framework for spelling and grammar error detection, as well as correction at the word level, based on a bidirectional long short-term memory mechanism and word embedding, in which a polynomial network classifier is at the top of the system. To get conclusive results, we have developed the most significant gold standard annotated corpus to date, containing 15 million fully inflected Arabic words. The data were collected from diverse text sources and genres, in which every erroneous and ill-formed word has been annotated, validated, and manually revised by Arabic specialists. This valuable asset is available for the Arabic natural language processing research community. The experimental results confirm that our proposed system significantly outperforms the performance of Microsoft Word 2013 and Open Office Ayaspell 3.4, which have been used in the literature for evaluating similar research.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3041181542",
    "type": "article"
  },
  {
    "title": "Venue Topic Model–enhanced Joint Graph Modelling for Citation Recommendation in Scholarly Big Data",
    "doi": "https://doi.org/10.1145/3404995",
    "publication_date": "2020-12-01",
    "publication_year": 2020,
    "authors": "Wei Wang; Zhiguo Gong; Jing Ren; Feng Xia; Zhihan Lv; Wei Wei",
    "corresponding_authors": "",
    "abstract": "Natural language processing technologies, such as topic models, have been proven to be effective for scholarly recommendation tasks with the ability to deal with content information. Recently, venue recommendation is becoming an increasingly important research task due to the unprecedented number of publication venues. However, traditional methods focus on either the author’s local network or author-venue similarity, where the multiple relationships between scholars and venues are overlooked, especially the venue–venue interaction. To solve this problem, we propose an author topic model–enhanced joint graph modeling approach that consists of venue topic modeling, venue-specific topic influence modeling, and scholar preference modeling. We first model the venue topic with Latent Dirichlet Allocation. Then, we model the venue-specific topic influence in an asymmetric and low-dimensional way by considering the topic similarity between venues, the top-influence of venues, and the top-susceptibility of venues. The top-influence characterizes venues’ capacity of exerting topic influence on other venues. The top-susceptibility captures venues’ propensity of being topically influenced by other venues. Extensive experiments on two real-world datasets show that our proposed joint graph modeling approach outperforms the state-of-the-art methods.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3106642285",
    "type": "article"
  },
  {
    "title": "Chinese Short Text Classification with Mutual-Attention Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3388970",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ming Hao; Bo Xu; Jingyi Liang; Bowen Zhang; Xu-Cheng Yin",
    "corresponding_authors": "",
    "abstract": "The methods based on the combination of word-level and character-level features can effectively boost performance on Chinese short text classification. A lot of works concatenate two-level features with little processing, which leads to losing feature information. In this work, we propose a novel framework called Mutual-Attention Convolutional Neural Networks, which integrates word and character-level features without losing too much feature information. We first generate two matrices with aligned information of two-level features by multiplying word and character features with a trainable matrix. Then, we stack them as a three-dimensional tensor. Finally, we generate the integrated features using a convolutional neural network. Extensive experiments on six public datasets demonstrate improved performance of our new framework over current methods.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3040779069",
    "type": "article"
  },
  {
    "title": "Word Level Script Identification Using Convolutional Neural Network Enhancement for Scenic Images",
    "doi": "https://doi.org/10.1145/3506699",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Shilpa Mahajan; Rajneesh Rani",
    "corresponding_authors": "",
    "abstract": "Script identification from complex and colorful images is an integral part of the text recognition and classification system. Such images may contain twofold challenges: (1) Challenges related to the camera like blurring effect, non-uniform illumination and noisy background, and so on, and (2) Challenges related to the text shape, orientation, and text size. The present work in this area is much focused on non-Indian scripts. In contrast, Gurumukhi, Hindi, and English scripts play a vital role in communication among Indians and foreigners. In this article, we focus on the above said challenges in the field of identifying the script. Additionally, we have introduced a new dataset that contains Hindi, Gurumukhi, and English scripts from scenic images collected from different sources. We also proposed a CNN-based model, which is capable of distinguishing between the scripts with good accuracy. Performance of the method has been evaluated for own dataset, i.e., NITJDATASET and other benchmarked datasets available for Indian scripts, i.e., CVSI-2015 (Task-1 and Task 4) and ILST. This work is an extension to find the script from strict text background.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4214872252",
    "type": "article"
  },
  {
    "title": "New Vietnamese Corpus for Machine Reading Comprehension of Health News Articles",
    "doi": "https://doi.org/10.1145/3527631",
    "publication_date": "2022-05-02",
    "publication_year": 2022,
    "authors": "Kiet Van Nguyen; Tin Van Huynh; Duc-Vu Nguyen; Anh Gia-Tuan Nguyen; Ngan Luu-Thuy Nguyen",
    "corresponding_authors": "",
    "abstract": "Machine reading comprehension is a natural language understanding task where the computing system is required to read a text and then find the answer to a specific question posed by a human. Large-scale and high-quality corpora are necessary for evaluating machine reading comprehension models. Furthermore, machine reading comprehension (MRC) for the health sector has potential for practical applications; nevertheless, MRC research in this domain is currently scarce. This article presents UIT-ViNewsQA, a new corpus for the Vietnamese language to evaluate MRC models for the healthcare textual domain. The corpus consists of 22,057 human-generated question-answer pairs. Crowd-workers create the questions and answers on a collection of 4,416 online Vietnamese healthcare news articles, where the answers are textual spans extracted from the corresponding articles. We introduce a process for creating a high-quality corpus for the Vietnamese machine reading comprehension task. Linguistically, our corpus accommodates diversity in question and answer types. In addition, we conduct experiments and compare the effectiveness of different MRC methods based on the neural networks and transformer architectures. Experimental results on our corpus show that the MRC system based on ALBERT architecture outperforms the neural network architectures and the BERT-based approach, an exact match score of 65.26% and an F1-score of 84.89%. The best machine model achieves about 10.90% F1-score less efficiently than humans, which proves that exploring machine models on UIT-ViNewsQA to surpass humans is challenging for researchers in the future. Our corpus is publicly available on our website: http://nlp.uit.edu.vn/datasets for research purposes.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3036737707",
    "type": "article"
  },
  {
    "title": "Online Reviews Sentiment Analysis and Product Feature Improvement with Deep Learning",
    "doi": "https://doi.org/10.1145/3522575",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Jihua Cao; Jie Li; Miao Yin; Yunfeng Wang",
    "corresponding_authors": "",
    "abstract": "The text mining of online reviews is currently a popular research direction of e-commerce and is considered the next blue ocean. Online reviews can dig out consumer preferences and provide theoretical guidance for the improvement of product features. However, current research mostly focuses on sentiment analysis methods and rarely involves feature extraction and large-scale data recognition. This article uses word segmentation technology to create a new feature extraction method. With the long short-term memory neural network and latent Dirichlet allocation topic model, we propose a product feature improvement model—CESC (Consumer online reviews–Extract short text–Sentiment analysis–Cluster feature). The model can derive the product features and attitudes that consumers prefer based on consumer online reviews and use it to improve product features. According to the experimental results of three electronic products sold on the e-commerce platform, the model can effectively dig out consumer preferences for online reviews. Enterprises can improve the quality of products and services, better meet the needs of consumers, promote consumers’ consumption, and achieve the enterprises’ goals and values.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4220823956",
    "type": "article"
  },
  {
    "title": "Explainable Deep Attention Active Learning for Sentimental Analytics of Mental Disorder",
    "doi": "https://doi.org/10.1145/3551890",
    "publication_date": "2022-07-27",
    "publication_year": 2022,
    "authors": "Usman Ahmed; Rutvij H. Jhaveri; Gautam Srivastava; Jerry Chun‐Wei Lin",
    "corresponding_authors": "",
    "abstract": "With the increasing use of online mediums, Internet-delivered psychological treatments (IDPs) are becoming an essential tool for improving mental disorders. Online-based health therapies can help a large segment of the population with little resource investment. The task is greatly complicated by the overlapping emotions for specific mental health. Early adoption of a deep learning system presented severe difficulties, including ethical and legal considerations that contributed to a lack of trust. Modern models required highly interpretable, intuitive explanations that humans could understand. To achieve this, we present a deep attention model based on fuzzy classification that uses the linguistic features of patient texts to build emotional lexicons. In medical applications, a diversified dataset generates work. Active learning techniques are used to extend fuzzy rules and the learned dataset gradually. From this, the model can gain a reduction in labeling efforts in mental health applications. In this way, difficulties such as the amount of vocabulary per class, method of generation, the source of data, and the baseline for human performance level can be solved. Moreover, this work illustrates fuzzy explainability by using weighted terms. The proposed method incorporates a subset of unstructured data into the set for training and uses a similarity-based approach. The approach then updates the model training using the new training points in the subsequent cycle of the active learning mechanism. The cycle is repeated until the optimal solution is found. At this point, all unlabeled text is converted into the set for training. The experimental results show that the emotion-based enhancement improves test accuracy and helps develop quality criteria. In the blind test, the bidirectional LSTM architecture with an attention mechanism and fuzzy classification achieved an F1 score of 0.89.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4288050564",
    "type": "article"
  },
  {
    "title": "Human-Computer Collaborative Visual Design Creation Assisted by Artificial Intelligence",
    "doi": "https://doi.org/10.1145/3554735",
    "publication_date": "2022-08-13",
    "publication_year": 2022,
    "authors": "Lihua Huang; Peng Zheng",
    "corresponding_authors": "",
    "abstract": "With the support and promotion of big data and cloud computing, AI has penetrated into every field of people's lives more and more deeply, with its characteristics of sustainable work, extremely fast computing speed, and a certain intelligence. This is an effective way to solve the general lack of demand and productivity of visual design, and relieve the pressure off designers to deal with relatively low-quality and high-demand designs. Therefore, the combination of design and artificial intelligence technology is a necessity. Research on the application of artificial intelligence technology for visual design is also in full swing at home and abroad However, at present, teams at home and abroad are in the exploratory stage. This paper considers whether it is possible to build an intelligent visual design and creation system by using artificial intelligence technology to help graphic communication designers achieve high-quality, high-efficiency, and high-quantity design output. Additionally, this paperexplores how to combine artificial intelligence technology with designers' design workflow so as to form a complementary human-computer cooperation mode. We will explore how to integrate AI technology with designers' design workflow and then create a human-machine collaboration model with complementary advantages to achieve the high quality, high efficiency, and high quantity of design output required by the intelligent visual design creation system being built. Finally, a basic framework of a generative smart human-computer collaborative visual design creation system based on a subset of neural network expert systems in multiple domains and an aggregate of different modules supported by the system is formed, and the working principle and usage process of the system are further elaborated with the example of packaging design.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4291202484",
    "type": "article"
  },
  {
    "title": "Detection and Cross-domain Evaluation of Cyberbullying in Facebook Activity Contents for Turkish",
    "doi": "https://doi.org/10.1145/3580393",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Önder Çoban; Selma Ayşe Özel; Ali İnan",
    "corresponding_authors": "",
    "abstract": "Cyberbullying refers to bullying and harassment of defenseless or vulnerable people such as children, teenagers, and women through any means of communication (e.g., e-mail, text messages, wall posts, tweets) over any online medium (e.g., social media, blogs, online games, virtual reality environments). The effect of cyberbullying may be severe and irreversible and it has become one of the major problems of cyber-societies in today’s electronic world. Prevention of cyberbullying activities as well as the development of timely response mechanisms require automated and accurate detection of cyberbullying acts. This study focuses on the problem of cyberbullying detection over Facebook activity content written in Turkish. Through extensive experiments with the various machine and deep learning algorithms, the best estimator for the task is chosen and then employed for both cross-domain evaluation and profiling of cyber-aggressive users. The results obtained with fivefold cross-validation are evaluated with an average-macro F1 score. These results show that BERT is the best estimator with an average macro F1 of 0.928, and employing it on various datasets collected from different OSN domains produces highly satisfying results. This article also reports detailed profiling of cyber-aggressive users by providing even more information than what is visible to the naked eye.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4316813688",
    "type": "article"
  },
  {
    "title": "Personality Detection using Kernel-based Ensemble Model for Leveraging Social Psychology in Online Networks",
    "doi": "https://doi.org/10.1145/3571584",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Akshi Kumar; Rohit Beniwal; Dipika Jain",
    "corresponding_authors": "",
    "abstract": "The Asian social networking market dominates the world landscape with the highest consumer penetration rate. Businesses and investors often look for winning strategies to attract consumers to increase revenues from sales, advertisements, and other services offered on social media platforms. Social media engagement and online relational cohesion have often been defined within the frameworks of social psychology and personality identification is a possible way in which social psychology can inform, engage, and learn from social media. Personality profiling has many real-world applications, including preference-based recommendation systems, relationship building, and career counseling. This research puts forward a novel kernel-based soft-voting ensemble model for personality detection from natural language, KBSVE-P. The KBSVE-P model is built by first evaluating the performance of various Support Vector Machine (SVM) kernels, namely radial basis function (RBF), linear, sigmoidal, and polynomial, to find the best-suited kernel for automatic personality detection in natural language text. Next, an ensemble of SVM kernels is implemented with a variety of voting techniques, such as soft voting, hard voting, and weighted hard voting. The model is evaluated on the publicly available Kaggle_MBTI dataset and a novel South Asian, Indian, low-resource Hindi language _MBTI (pronounced as vishesh charitr, meaning personality in Hindi) dataset for detecting a user's personality across four personality traits, namely introvert/extrovert (IE), thinking/feeling (TF), sensing/intuitive (SI), and judging/perceiving (JP). The proposed kernel-based ensemble with soft voting, KBSVE-P, outperforms the existing models on English Kaggle-MBTI dataset with an average F-score of 85.677 and achieves an accuracy of 66.89 for the Hindi _MBTI dataset.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4317209547",
    "type": "article"
  },
  {
    "title": "Inculcating Context for Emoji Powered Bengali Hate Speech Detection using Extended Fuzzy SVM and Text Embedding Models",
    "doi": "https://doi.org/10.1145/3589001",
    "publication_date": "2023-03-27",
    "publication_year": 2023,
    "authors": "Sayani Ghosal; Amita Jain; Devendra K. Tayal; Varun G. Menon; Akshi Kumar",
    "corresponding_authors": "",
    "abstract": "The massive growth of social webs offer opportunities to communicate with diverse languages, unstructured text, informal posts, misspelled contents and emojis. Social media users feel comfortable to express their emotions specially emotions with high intensity (hate speech) in their mother tongue. Hate speech in any form targets groups and individuals that may trigger antisocial activities, hate crimes, and terrorist acts. Bengali social media users use Bengali for posting implicit or indirect hate text. Existing Bengali hate speech detection research considers explicit hate speech detection but in actual hate is expressed more in implicit way. In order to detect both implicit and explicit hate speech from low resource content, social webs need highly efficient automated tools. Researchers applied discriminative learning approaches (i.e. SVM, MLP, CNN) to distinguish hate text with only clear-cut outcomes in detecting direct hate speech. The proposed novel Bengali hate speech detection model considers two parallel approaches: (i) It applies extended fuzzy SVM classifier for class imbalanced dataset (FSVMCIL) and multilingual BERT (mBERT) text embedding model to detect first hate label; (ii) Morphological analysis method to detect implicit and explicit hate content with the hate similarity (HS) scheme for second hate label. Linking both labeling methods, this research extracts contextual Bengali hate speech from informal text. This novel HS method considers Word2Vec word embedding model and Bengali hate lexicon. It also considers emoji to text conversion for efficient contextual analysis. This study also conducts extensive experiments for various categories with the Bengali hate speech dataset. It also evaluates the proposed model performance considering weighted F1 score, precision, recall and accuracy parameters. Results reveal significant improvement in Bengali hate speech detection with 2.35% increase in F1- score and 9.11 % increase in accuracy.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4361003594",
    "type": "article"
  },
  {
    "title": "Semantic Relation Extraction: A Review of Approaches, Datasets, and Evaluation Methods With Looking at the Methods and Datasets in the Persian Language",
    "doi": "https://doi.org/10.1145/3592601",
    "publication_date": "2023-04-14",
    "publication_year": 2023,
    "authors": "Hamid Gharagozlou; Javad Mohammadzadeh; Azam Bastanfard; Saeed Shiry Ghidary",
    "corresponding_authors": "",
    "abstract": "A large volume of unstructured data, especially text data, is generated and exchanged daily. Consequently, the importance of extracting patterns and discovering knowledge from textual data is significantly increasing. As the task of automatically recognizing the relations between two or more entities, semantic relation extraction has a prominent role in the exploitation of raw text. This article surveys different approaches and types of relation extraction in English and the most prominent proposed methods in Persian. We also introduce, analyze, and compare the most important datasets available for relation extraction in Persian and English. Furthermore, traditional and emerging evaluation metrics for supervised, semi-supervised, and unsupervised methods are described, along with pointers to commonly used performance evaluation datasets. Finally, we briefly describe challenges in extracting relationships in Persian and English and dataset creation challenges.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4365509816",
    "type": "review"
  },
  {
    "title": "Enhancing Coherence and Diversity in Multi-class Slogan Generation Systems",
    "doi": "https://doi.org/10.1145/3637551",
    "publication_date": "2023-12-15",
    "publication_year": 2023,
    "authors": "Pir Noman Ahmad; Yuanchao Liu; Inam Ullah; Mohammad Shabaz",
    "corresponding_authors": "",
    "abstract": "Many problems related to natural language processing are solved by neural networks and big data. Researchers have previously focused on single-task supervised goals with limited data management to train slogan classification. A multi-task learning framework is used to learn jointly across several tasks related to generating multi-class slogan types. This study proposes a multi-task model named slogan generative adversarial network systems (Slo-GAN) to enhance coherence and diversity in slogan generation, utilizing generative adversarial networks and recurrent neural networks (RNN). Slo-GAN generates a new text slogan-type corpus, and the training generalization process is improved. We explored active learning (AL) and meta-learning (ML) for dataset labeling efficiency. AL reduced annotations by 10% compared to ML but still needed about 70% of the full dataset for baseline performance. The whole framework of Slo-GAN is supervised and trained together on all of these tasks. The text with the higher reporting score level is filtered by Slo-GAN, and a classification accuracy of 87.2% is achieved. We leveraged relevant datasets to perform a cross-domain experiment, reinforcing our assertions regarding both the distinctiveness of our dataset and the challenges of adapting bilingual dialects to one another.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4389785977",
    "type": "article"
  },
  {
    "title": "Arabic Sentiment Analysis for ChatGPT Using Machine Learning Classification Algorithms: A Hyperparameter Optimization Technique",
    "doi": "https://doi.org/10.1145/3638285",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Ahmad Nasayreh; Rabia Emhamed Al Mamlook; Ghassan Samara; Hasan Gharaibeh; Mohammad Aljaidi; Dalia Alzu’bi; Essam Al-Daoud; Laith Abualigah",
    "corresponding_authors": "",
    "abstract": "In the realm of ChatGPT's language capabilities, exploring Arabic Sentiment Analysis emerges as a crucial research focus. This study centers on ChatGPT, a popular machine learning model engaging in dialogues with users, garnering attention for its exceptional performance and widespread impact, particularly in the Arab world. The objective is to assess people's opinions about ChatGPT, categorizing them as positive or negative. Despite abundant research in English, there is a notable gap in Arabic studies. We assembled a dataset from X (formerly known as Twitter), comprising 2,247 tweets, classified by Arabic language specialists. Employing various machine learning algorithms, including Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), and Naïve Bayes (NB), we implemented hyperparameter optimization techniques such as Bayesian optimization, Grid Search, and random search to select the best hyperparameters that contribute to achieving the best performance. Through training and testing, performance enhancements were observed with optimization algorithms. SVM exhibited superior performance, achieving 90% accuracy, 88% precision, 95% recall, and 91% F1 score with Grid Search. These findings contribute valuable insights into ChatGPT's impact in the Arab world, offering a comprehensive understanding of sentiment analysis through machine learning methodologies.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4390880298",
    "type": "article"
  },
  {
    "title": "Leveraging Bidirectionl LSTM with CRFs for Pashto Tagging",
    "doi": "https://doi.org/10.1145/3649456",
    "publication_date": "2024-02-27",
    "publication_year": 2024,
    "authors": "Farooq Zaman; Onaiza Maqbool; Jaweria Kanwal",
    "corresponding_authors": "",
    "abstract": "Part-of-speech tagging plays a vital role in text processing and natural language understanding. Very few attempts have been made in the past for tagging Pashto Part-of-Speech. In this work, we present a Long Short-term Memory–based approach for Pashto part-of-speech tagging with special focus on ambiguity resolution. Initially, we created a corpus of Pashto sentences having words with multiple meanings and their tags. We introduce a powerful sentences representation and new architecture for Pashto text processing. The accuracy of the proposed approach is compared with state-of-the-art Hidden Markov Model. Our Model shows 87.60% accuracy for all words excluding punctuation and 95.45% for ambiguous words; however, Hidden Markov Model shows 78.37% and 44.72% accuracy, respectively. Results show that our approach outperforms Hidden Markov Model in Part-of-Speech tagging for Pashto text.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4392199349",
    "type": "article"
  },
  {
    "title": "Cross-Domain Aspect-Based Sentiment Classification with a Pre-Training and Fine-Tuning Strategy for Low-Resource Domains",
    "doi": "https://doi.org/10.1145/3653299",
    "publication_date": "2024-03-21",
    "publication_year": 2024,
    "authors": "Chuanjun Zhao; Meiling Wu; Xinyi Yang; Xuzhuang Sun; Suge Wang; Deyu Li",
    "corresponding_authors": "",
    "abstract": "Aspect-based sentiment classification (ABSC) is a crucial sub-task of fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspects in a sentence as positive, negative, or neutral. Most existing ABSC methods are based on supervised learning. However, these methods rely heavily on fine-grained labeled training data, which can be scarce in low-resource domains, limiting their effectiveness. To overcome this challenge, we propose a low-resource cross-domain aspect-based sentiment classification (CDABSC) approach based on a pre-training and fine-tuning strategy. This approach applies the pre-training and fine-tuning strategy to an advanced deep learning method designed for ABSC, namely the attention-based encoding graph convolutional network (AEGCN) model. Specifically, a high-resource domain is selected as the source domain, and the AEGCN model is pre-trained using a large amount of fine-grained annotated data from the source domain. The optimal parameters of the model are preserved. Subsequently, a low-resource domain is used as the target domain, and the pre-trained model parameters are used as the initial parameters of the target domain model. The target domain is fine-tuned using a small amount of annotated data to adapt the parameters to the target domain model, improving the accuracy of sentiment classification in the low-resource domain. Finally, experimental validation on two domain benchmark datasets, restaurant and laptop, demonstrates significant outperformance of our approach over the baselines in CDABSC Micro-F1.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4393042705",
    "type": "article"
  },
  {
    "title": "Low Resource Summarization using Pre-trained Language Models",
    "doi": "https://doi.org/10.1145/3675780",
    "publication_date": "2024-07-03",
    "publication_year": 2024,
    "authors": "Mubashir Munaf; Hammad Afzal; Khawir Mahmood; Naima Iltaf",
    "corresponding_authors": "",
    "abstract": "With the advent of Deep Learning-based Artificial Neural Network models, Natural Language Processing (NLP) has witnessed significant improvements in textual data processing in terms of its efficiency and accuracy. However, the research is mostly restricted to high-resource languages such as English, and low-resource languages still suffer from a lack of available resources in terms of training datasets as well as models with even baseline evaluation results. Considering the limited availability of resources for low-resource languages, we propose a methodology for adapting self-attentive transformer-based architecture models (mBERT, mT5) for low-resource summarization, supplemented by the construction of a new baseline dataset (76.5k article, summary pairs) in a low-resource language, Urdu. Choosing news (a publicly available source) as the application domain has the potential to make the proposed methodology useful for reproducing in other languages with limited resources. Our adapted summarization model urT5 with up to 44.78% reduction in size as compared to mT5 can capture contextual information of the low-resource language effectively with an evaluation score (up to 46.35 ROUGE-1, 77 BERTScore) on par with state-of-the-art models in the high-resource language of English (PEGASUS: 47.21, BART: 45.14 on XSUM Dataset) . The proposed method provided a baseline approach toward extractive as well as abstractive summarization with competitive evaluation results in a limited resource setup.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4400292040",
    "type": "article"
  },
  {
    "title": "Supposititious Sarcasm Detection and Sentiment Analysis Coping Hindi Language in Social Networks Harnessing Zipf- Mandelbrot Probabilistic Optimisation and Perplexity Entropy Learning",
    "doi": "https://doi.org/10.1145/3712061",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Himani Pokhriyal; Goonjan Jain",
    "corresponding_authors": "",
    "abstract": "Sarcasm is used to convey the contempt via poking and ridiculing the other person. It is frequently used to make fun of other people by saying unpleasant things. The presence of informal language patterns, modified vocabulary, and speak-text utilized in online communities makes sarcasm detection a challenging task. Due to the occupancy of these terms in the conversation it is challenging to understand and identify sarcasm. Further, sarcasm detection in Hindi, a native Indian language is itself a challenging task to perform. This study proposes a novel method for sarcasm detection in Hindi comments taken from social networking sites using probabilistic non-linear optimization technique. The encapsulations of word embedding algorithms infused in an optimization problem is used for sarcasm detection. The Hindi datasets i.e., News Headlines and Twitter comments are utilized for sarcasm detection. The outcomes of proposed method is assessed using a variety of learning metrics such as accuracy, precision, recall, and F1 score surpassing the existing state-of-the-art methods. Performing ablation study and statistical validation tests within real-world social network confirms the efficacy and reliability of the proposed method. The proposed method gives an outstanding F1 scores of 0.9316% and 0.9359% on Dataset-1 and 2.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406535716",
    "type": "article"
  },
  {
    "title": "Tokenization and Stemming of Limbu Language",
    "doi": "https://doi.org/10.1145/3712018",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Abigail Rai; Samarjeet Borah",
    "corresponding_authors": "",
    "abstract": "Significant issues in tokenization and stemming in natural language processing (NLP) are addressed in this work, which oddly focuses Kingpin on the Limbu language. Two essential preprocessing procedures that work to normalize words by condensing them into compact content and their origin are stemming and tokenization. We introduce a novel stemmer for the Limbu language, which achieves 93.5% accuracy rates, respectively, using the given word bank. The stemmer is preceded by tokenization with the 100% accuracy rate on the current available corpus. The Limbu language presents peculiar issues due to its constrained computational resources and composite nature. This work is a first step toward overcoming the lack of necessary resources for effective stemming in the Limbu language computational work. Advanced algorithms with morphologically structured rules especially chosen for the Limbu language are used in stemming techniques to increase accuracy and speed. The discoveries significantly improve natural language processing (NLP) activities, providing strong tools for search engines, sentiment analysis, and automatic translation systems, as well as marking a first step in computational development for the Limbu language.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406870682",
    "type": "article"
  },
  {
    "title": "A DA-STCM Dual-channel Short Text Classification Method Based on Attention Mechanism",
    "doi": "https://doi.org/10.1145/3715911",
    "publication_date": "2025-02-03",
    "publication_year": 2025,
    "authors": "Yujie Li; Hongfang Zhou; Yuechuan Xin",
    "corresponding_authors": "",
    "abstract": "Short text classification is an important and challenging task in NLP. Compared with long text, short text suffers from insufficient semantic information. Based on a lot of theoretical and experimental analysis, a two-channel short text classification method, DA-STCM, is proposed. This method implements word correction by introducing the Aliyun service and introduces an attention mechanism in the channels to achieve weight assignment. Experimental results show that our proposed method is feasible in short text classification, and the classification accuracy is significantly improved.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407107483",
    "type": "article"
  },
  {
    "title": "Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction",
    "doi": "https://doi.org/10.1145/3714411",
    "publication_date": "2025-02-08",
    "publication_year": 2025,
    "authors": "Jie Chen; Yinlong Wang; Shu Zhao; Peng Zhou; Yanping Zhang",
    "corresponding_authors": "",
    "abstract": "To meet the challenge of incompleteness within Knowledge Graphs, Knowledge Graph Embedding(KGE) has emerged as the fundamental methodology for predicting the missing link(Link Prediction), by mapping entities and relations as low-dimensional vectors in continuous space. However, current KGE models often struggle with the polysemy issue, where entities exhibit different semantic characteristics depending on the relations in which they participate. Such limitation stems from weak interactions between entities and their relation contexts, leading to low expressiveness in modeling complex structures and resulting in inaccurate predictions. To address this, we propose ConQuatE ( Con textualized Quat ernion E mbedding), a model that enhances the representation learning of entities across multiple semantic dimensions by leveraging quaternion rotation to capture diverse relational contexts. In specific, ConQuatE incorporates contextual cues from various connected relations to enrich the original entity representations. Notably, this is achieved through efficient vector transformations in quaternion space, without any extra information required other than original triples. Experimental results demonstrate that our model outperforms state-of-the-art models for Link Prediction on four widely-recognized datasets: FB15k-237, WN18RR, FB15k and WN18.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407276944",
    "type": "article"
  },
  {
    "title": "FedINER: Chinese Industrial Named Entity Recognition with Federated Learning",
    "doi": "https://doi.org/10.1145/3730401",
    "publication_date": "2025-04-16",
    "publication_year": 2025,
    "authors": "Xiaowen Liu; Xiaoli Zhao; Jiale Chen; Zehui Li",
    "corresponding_authors": "",
    "abstract": "Named entity recognition (NER) is foundational in constructing industrial knowledge graphs and is an essential component in the automation of knowledge within the industry. However, due to the particularities of the industrial field, its data involves issues such as business competition and user privacy. Traditional machine learning algorithms concentrate all data on a single machine for training, which may lead to privacy breaches. Federated learning, a distributed machine learning architecture, updates global model parameters through a central server’s aggregation algorithm without exposing private data. Therefore, this paper proposes a privacy-preserving framework for Chinese named entity recognition in the industrial domain, FedINER. The local client model employs a grid-tagging-based approach, which expands upon the character pair tagging system of SOTA models, emphasizing entity boundary information, internal character pair relationships, and the types of relationships. Besides, a bi-axial attention mechanism is utilized to better capture the relevance of relationships within the character pair representation, further optimizing model performance. Then, through federated training, each client benefits from the collective optimization of the global model, thereby improving the performance and accuracy of the model. Experiments on two distinct industrial datasets, conducted under the premise of protecting data privacy and security, have validated the effectiveness of this approach.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409483283",
    "type": "article"
  },
  {
    "title": "Aspect-based Sentiment Analysis for COVID-19: A Heterogeneous Graph Convolutional Network Approach",
    "doi": "https://doi.org/10.1145/3731758",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "Linlin Hou; Wenhui Tu; Ting Yu; Ting Jiang; Mohamed S. Bah; Zenghui Xu; Yu Zhang; Gaoming Yang; Ji Zhang",
    "corresponding_authors": "",
    "abstract": "The epidemic of infectious diseases has a significant impact on society, the economy, and people’s lives. Social media, with its high user participation and rapid information dissemination, plays a crucial role in shaping public opinion. Fine-grained sentiment analysis of public opinion on infectious diseases can provide valuable insights for improving the quality of public services. However, there are few relevant studies on Chinese data due to language complexity and low resources. Moreover, most of the existing approaches utilize the Graph Neural Network (GCN) method by syntactic dependency trees to construct graphs of text, which ignore the potential link relationships between aspects and words. Therefore, to address this limitation, in this paper, we propose a new method based on GCN using aspect-specific heterogeneous graphs, named ASHGCN, which combines BiLSTM, heterogeneous graphs, GCN, the mask and the attention mechanism. We mine social media posts related to COVID-19 for aspect-based sentiment analysis task (ABSA) for ten aspect entity types in both Chinese and English data. The heterogeneous graph is designed with two node types (aspect nodes and non-aspect nodes) and four edge connection types, including various relationships between aspect entities, and between aspect entities and non-aspect entities. In addition, we release a Chinese dataset and an English dataset that include medical and named entities, along with corresponding sentiment labels. Experiments on our datasets, as well as two public datasets, demonstrate that our method greatly improves performance in the ABSA task. Ablation experiments and case studies further support the effectiveness of the proposed approach.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409700800",
    "type": "article"
  },
  {
    "title": "Arabic Manuscripts Alignment, Segmentation, Recognition, and Classification",
    "doi": "https://doi.org/10.1145/3732939",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Hassanin M. Al-Barhamtoshy; Sherif Abdou",
    "corresponding_authors": "",
    "abstract": "This research proposes a novel approach for aligning, segmenting, recognizing, and classifying Arabic manuscripts. The alignment process involves removing noise, correcting rotation and skewing, and aligning the manuscripts with a standard template. This alignment is crucial in determining whether a manuscript is known or unknown. The proposed approach, called Automated Arabic Manuscript Alignment (ArM), takes multiple input images, aligns them, refines their internal parameters, and analyzes each parameter. It then recognizes and maps the regions of interest (RoI) in the manuscript to the corresponding printed or handwritten content. Finally, a suitable optical character recognition (OCR) system is used to recognize the content within each region. The first phase of this research focuses on manuscript alignment to OCR historical and ancient manuscripts. In the second phase, the alignment is combined with OCRing to extract and recognize the text from each RoI in the Arabic manuscripts. Additionally, the classification of Arabic manuscripts into known or unknown categories is accomplished through a digital image processing technique, specifically an analysis of Arabic cards and the extraction of RoI. The aim is to make the entire process more efficient, accurate, and faster. Several experiments were conducted on different regions of the manuscript using the intersection over union (IoU) metric. The default Arabic OCRing achieved the best results with an accuracy of 99.69%. To evaluate the performance of this technology compared to the state-of-the-art, it relies on multiple interconnected modules within the OCRing processing pipeline, including image preprocessing, layout analysis, line and word segmentation, and character recognition. This paper surveys the evaluation process for Arabic OCRing, highlighting the importance of available tools and their influence on the indicator metrics used. The specification and preparation of the benchmarking dataset are discussed, as it significantly impacts the evaluation results. Overall, the comprehensive evaluation of OCRing, accounting for the various subsystems, and the careful consideration of the benchmarking dataset are essential for accurate performance assessment compared to the current progress in the field.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409944764",
    "type": "article"
  },
  {
    "title": "NOVA",
    "doi": "https://doi.org/10.1145/3276773",
    "publication_date": "2018-12-17",
    "publication_year": 2018,
    "authors": "Chenchen Ding; Masao Utiyama; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "A feasible and flexible annotation system is designed for joint tokenization and part-of-speech (POS) tagging to annotate those languages without natural definitions of words . This design was motivated by the fact that word separators are not used in many highly analytic East and Southeast Asian languages. Although several of the languages are well-studied, e.g., Chinese and Japanese, many are understudied with low resources, e.g., Burmese (Myanmar) and Khmer. In the first part of the article, the proposed annotation system, named nova, is introduced. nova contains only four basic tags (n, v, a, and o); these tags can be further modified and combined to adapt complex linguistic phenomena in tokenization and POS tagging. In the second part of the article, the feasibility and flexibility of nova is illustrated from the annotation practice on Burmese and Khmer. The relation between nova and two universal POS tagsets is discussed in the final part of the article.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2905027511",
    "type": "article"
  },
  {
    "title": "Multi-Entity Aspect-Based Sentiment Analysis with Context, Entity, Aspect Memory and Dependency Information",
    "doi": "https://doi.org/10.1145/3321125",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Jun Yang; Runqi Yang; Hengyang Lu; Chongjun Wang; Junyuan Xie",
    "corresponding_authors": "",
    "abstract": "Fine-grained sentiment analysis is a useful tool for producers to understand consumers’ needs as well as complaints about products and related aspects from online platforms. In this article, we define a novel task named “Multi-Entity Aspect-Based Sentiment Analysis (ME-ABSA)”. It investigates the sentiment towards entities and their related aspects. It makes the well-studied aspect-based sentiment analysis a special case of this type, where the number of entities is limited to one. We contribute a new dataset for this task, with multi-entity Chinese posts in it. We propose to model context, entity, and aspect memory to address the task and incorporate dependency information for further improvement. Experiments show that our methods perform significantly better than baseline methods on datasets for both ME-ABSA task and ABSA task. The in-depth analysis further validates the effectiveness of our methods and shows that our methods are capable of generalizing to new (entity, aspect) combinations with little loss of accuracy. This observation indicates that data annotation in real applications can be largely simplified.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2944502595",
    "type": "article"
  },
  {
    "title": "Weakly Supervised POS Tagging without Disambiguation",
    "doi": "https://doi.org/10.1145/3214707",
    "publication_date": "2018-07-21",
    "publication_year": 2018,
    "authors": "Deyu Zhou; Zhikai Zhang; Min-Ling Zhang; Yulan He",
    "corresponding_authors": "",
    "abstract": "Weakly supervised part-of-speech (POS) tagging is to learn to predict the POS tag for a given word in context by making use of partial annotated data instead of the fully tagged corpora. Weakly supervised POS tagging would benefit various natural language processing applications in such languages where tagged corpora are mostly unavailable. In this article, we propose a novel framework for weakly supervised POS tagging based on a dictionary of words with their possible POS tags. In the constrained error-correcting output codes (ECOC)-based approach, a unique L -bit vector is assigned to each POS tag. The set of bitvectors is referred to as a coding matrix with value { 1, -1}. Each column of the coding matrix specifies a dichotomy over the tag space to learn a binary classifier. For each binary classifier, its training data is generated in the following way: each pair of words and its possible POS tags are considered as a positive training example only if the whole set of its possible tags falls into the positive dichotomy specified by the column coding and similarly for negative training examples. Given a word in context, its POS tag is predicted by concatenating the predictive outputs of the L binary classifiers and choosing the tag with the closest distance according to some measure. By incorporating the ECOC strategy, the set of all possible tags for each word is treated as an entirety without the need of performing disambiguation. Moreover, instead of manual feature engineering employed in most previous POS tagging approaches, features for training and testing in the proposed framework are automatically generated using neural language modeling. The proposed framework has been evaluated on three corpora for English, Italian, and Malagasy POS tagging, achieving accuracies of 93.21%, 90.9%, and 84.5% individually, which shows a significant improvement compared to the state-of-the-art approaches.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2883043564",
    "type": "article"
  },
  {
    "title": "Towards Burmese (Myanmar) Morphological Analysis",
    "doi": "https://doi.org/10.1145/3325885",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Chenchen Ding; Hnin Thu Zar Aye; Win Pa Pa; Khin Thandar Nwet; Khin Mar Soe; Masao Utiyama; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "This article presents a comprehensive study on two primary tasks in Burmese (Myanmar) morphological analysis: tokenization and part-of-speech (POS) tagging. Twenty thousand Burmese sentences of newswire are annotated with two-layer tokenization and POS-tagging information, as one component of the Asian Language Treebank Project. The annotated corpus has been released under a CC BY-NC-SA license, and it is the largest open-access database of annotated Burmese when this manuscript was prepared in 2017. Detailed descriptions of the preparation, refinement, and features of the annotated corpus are provided in the first half of the article. Facilitated by the annotated corpus, experiment-based investigations are presented in the second half of the article, wherein the standard sequence-labeling approach of conditional random fields and a long short-term memory (LSTM)-based recurrent neural network (RNN) are applied and discussed. We obtained several general conclusions, covering the effect of joint tokenization and POS-tagging and importance of ensemble from the viewpoint of stabilizing the performance of LSTM-based RNN. This study provides a solid basis for further studies on Burmese processing.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2948735718",
    "type": "article"
  },
  {
    "title": "Urdu Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3329710",
    "publication_date": "2019-06-20",
    "publication_year": 2019,
    "authors": "Safia Kanwal; Muhammad Kamran Malik; Khurram Shahzad; Faisal Aslam; Zubair Nawaz",
    "corresponding_authors": "",
    "abstract": "Named Entity Recognition (NER) plays a pivotal role in various natural language processing tasks, such as machine translation and automatic question-answering systems. Recognizing the importance of NER, a plethora of NER techniques for Western and Asian languages have been developed. However, despite having over 490 million Urdu language speakers worldwide, NER resources for Urdu are either non-existent or inadequate. To fill this gap, this article makes four key contributions. First, we have developed the largest Urdu NER corpus, which contains 926,776 tokens and 99,718 carefully annotated NEs. The developed corpus has at least doubled the number of manually tagged NEs as compared to any of the existing Urdu NER corpora. Second, we have generated six new word embeddings using three different techniques, fastText, Word2vec, and Glove, on two corpora of Urdu text. These are the only publicly available embeddings for the Urdu language, besides the recently released Urdu word embeddings by Facebook. Third, we have pioneered in the application of deep learning techniques, NN and RNN, for Urdu named entity recognition. Finally, we have performed 10-folds of 32 different experiments using the combinations of a traditional supervised learning and deep learning techniques, seven types of word embeddings, and two different Urdu NER datasets. Based on the analysis of the results, several valuable insights are provided about the effectiveness of deep learning techniques, the impact of word embeddings, and variations of datasets.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2949241676",
    "type": "article"
  },
  {
    "title": "Subword Attentive Model for Arabic Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3360016",
    "publication_date": "2020-02-13",
    "publication_year": 2020,
    "authors": "Majdi Beseiso; Haytham H. Elmousalami",
    "corresponding_authors": "",
    "abstract": "Social media data is unstructured data where these big data are exponentially increasing day to day in many different disciplines. Analysis and understanding the semantics of these data are a big challenge due to its variety and huge volume. To address this gap, unstructured Arabic texts have been studied in this work owing to their abundant appearance in social media Web sites. This work addresses the difficulty of handling unstructured social media texts, particularly when the data at hand is very limited. This intelligent data augmentation technique that handles the problem of less availability of data are used. This article has proposed a novel architecture for hand Arabic words classification and understands based on convolutional neural networks (CNNs) and recurrent neural networks. Moreover, the CNN technique is the most powerful for the analysis of Arabic tweets and social network analysis. The main technique used in this work is character-level CNN and a recurrent neural network stacked on top of one another as the classification architecture. These two techniques give 95% accuracy in the Arabic texts dataset.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3007107944",
    "type": "article"
  },
  {
    "title": "Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network",
    "doi": "https://doi.org/10.1145/3460392",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Abdul Rehman Javed; Saif Ur Rehman; Mohib Ullah Khan; Mamoun Alazab; Habib Ullah Khan",
    "corresponding_authors": "",
    "abstract": "With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals’ sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger , an Android-based application that highlights the issue of leaking smartphone users’ privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3173922570",
    "type": "article"
  },
  {
    "title": "Empirical Evaluation of Shallow and Deep Learning Classifiers for Arabic Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3466171",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Ali Bou Nassif; Abdollah Masoud Darya; Ashraf Elnagar",
    "corresponding_authors": "",
    "abstract": "This work presents a detailed comparison of the performance of deep learning models such as convolutional neural networks (CNN), long short-term memory (LSTM), gated recurrent units (GRU), their hybrids, and a selection of shallow learning classifiers for sentiment analysis of Arabic reviews. Additionally, the comparison includes state-of-the-art models such as the transformer architecture and the araBERT pre-trained model. The datasets used in this study are multi-dialect Arabic hotel and book review datasets, which are some of the largest publicly available datasets for Arabic reviews. Results showed deep learning outperforming shallow learning for binary and multi-label classification, in contrast with the results of similar work reported in the literature. This discrepancy in outcome was caused by dataset size as we found it to be proportional to the performance of deep learning models. The performance of deep and shallow learning techniques was analyzed in terms of accuracy and F1 score. The best performing shallow learning technique was Random Forest followed by Decision Tree, and AdaBoost. The deep learning models performed similarly using a default embedding layer, while the transformer model performed best when augmented with araBERT.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3216452752",
    "type": "article"
  },
  {
    "title": "An Effective Approach for Rumor Detection of Arabic Tweets Using eXtreme Gradient Boosting Method",
    "doi": "https://doi.org/10.1145/3461697",
    "publication_date": "2022-01-28",
    "publication_year": 2022,
    "authors": "Abdu Gumaei; Mabrook Al‐Rakhami; Mohammad Mehedi Hassan; Victor Hugo C. de Albuquerque; David Camacho",
    "corresponding_authors": "",
    "abstract": "Twitter is currently one of the most popular microblogging platforms allowing people to post short messages, news, thoughts, and so on. The Twitter user community is growing very fast. It has an average of 328 million active accounts today, making it one of the most common media for getting information during any influential or important event. Because it is freely used by the public, some credibility checking is required, especially when it comes to events of high importance. Automatic rumor detection in Arabic tweets is a challenging task due to the changes in the structural and morphological nature of the Arabic language, which makes the detection of rumors more difficult than in other languages. In this article, we proposed an effective approach for rumor detection of Arabic tweets using an eXtreme gradient boosting (XGBoost) classifier. We conducted a set of experiments on a public dataset that contained a large number of rumor and non-rumor tweets. The model uses a comprehensive set of features, including content-based, user-based, and topic-based features, allowing one to look at credibility from different angles. The experimental results demonstrated that the proposed XGBoost-based approach achieves 97.18% accuracy on 60% of the dataset as a training set, which is the highest accuracy rate compared with the other methods used in recent related work.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4210622371",
    "type": "article"
  },
  {
    "title": "An Intelligent Telugu Handwritten Character Recognition Using Multi-Objective Mayfly Optimization with Deep Learning–Based DenseNet Model",
    "doi": "https://doi.org/10.1145/3520439",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Vijaya Krishna Sonthi; S. Nagarajan; N. Krishnaraj",
    "corresponding_authors": "",
    "abstract": "The handwritten character recognition process has gained significant attention among research communities due to its application in assistive technologies for visually impaired people, human–robot interaction, automated registry for business documents, and so on. Handwritten character recognition of the Telugu language is difficult owing to the absence of a massive dataset and a trained convolutional neural network (CNN). This article introduces an intelligent Telugu character recognition process using a multi-objective mayfly optimization with deep learning (MOMFO-DL) model. The proposed MOMFO-DL technique involves the DenseNet-169 model as a feature extractor to generate a useful set of feature vectors. A functional link neural network (FLNN) is used as a classification model to recognize and classify the printer characters. The design of the MOMFO technique for the parameter optimization of the DenseNet model and FLNN model shows the novelty of the work. The use of MOMFO technique helps to optimally tune the parameters in such a way that the overall performance can be improved. The extensive experimental analysis takes place on benchmark datasets and the outcomes are examined with respect to different measures. The experimental results pointed out the supremacy of the MOMFO technique over the recent state-of-the-art methods.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4220960027",
    "type": "article"
  },
  {
    "title": "A Novel Deep Auto-Encoder Based Linguistics Clustering Model for Social Text",
    "doi": "https://doi.org/10.1145/3527838",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Muhammad Waseem Akram; Muhammad Salman; Muhammad Farrukh Bashir; Syed Muhammad Saad Salman; Thippa Reddy Gadekallu; Abdul Rehman Javed",
    "corresponding_authors": "",
    "abstract": "The wide adoption of media and social media has increased the amount of digital content to an enormous level. Natural language processing (NLP) techniques provide an opportunity to extract and explore meaningful information from a large amount of text. Among natural languages, Urdu is one of the widely used languages worldwide for spoken and written communications. Due to its wide adopt-ability, digital content in the Urdu language is increasing briskly, especially with social media and online NEWS feeds. Government agencies and advertisers must filter and understand the content to analyze the trends and cohorts in their interest and national prerogative. Clustering is considered a baseline and one of the first steps in natural language understanding. There are many state-of-the-art clustering techniques specifically for English, French, and Arabic, but no significant research has been conducted in Urdu language processing. Doing it for short text segments is challenging because of limited features and the absence of meaningful language discourse and nuance. Many rule-based NLP techniques are adopted to overcome these issues, relying on human-designed features and rules. Therefore, these methods do not promise remarkable results. Alongside NLP, deep learning techniques are pretty efficient in capturing contextual information with minimal noise compared to other traditional methods. By taking on this challenging job, we develop a deep learning-based technique for Urdu short text clustering for the very first time without a human-designed feature. In this paper, we propose a method of short text clustering using a deep neural network that automatically learns feature representations and clustering assignments simultaneously. This method learns clustering objectives by converting the high dimensional feature space to a low dimensional feature space. Our experiments on the Urdu NEWS headlines dataset show remarkable results compared to state-of-the-art methods.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4225158996",
    "type": "article"
  },
  {
    "title": "Hybrid Deep Learning Model for Sarcasm Detection in Indian Indigenous Language Using Word-Emoji Embeddings",
    "doi": "https://doi.org/10.1145/3519299",
    "publication_date": "2022-08-10",
    "publication_year": 2022,
    "authors": "Akshi Kumar; Saurabh Raj Sangwan; Adarsh Kumar Singh; Gandharv Wadhwa",
    "corresponding_authors": "",
    "abstract": "Automated sarcasm detection is deemed as a complex natural language processing task and extending it to a morphologically-rich and free-order dominant indigenous Indian language Hindi is another challenge in itself. The scarcity of resources and tools such as annotated corpora, lexicons, dependency parser, Part-of-Speech tagger, and benchmark datasets engorge the linguistic challenges of sarcasm detection in low-resource languages like Hindi. Furthermore, as context incongruity is imperative to detect sarcasm, various linguistic, aural and visual cues can be used to predict target utterance as sarcastic. While pre-trained word embeddings capture the meanings, semantic relationships and different types of contexts in the form of word representations, emojis can also render useful contextual information, analogous to human facial expressions, for gauging sarcasm. Thus, the goal of this research is to demonstrate the use of a hybrid deep learning model trained using two embeddings, namely word and emoji embeddings to detect sarcasm. The model is validated on a Hindi tweets dataset, Sarc-H, manually annotated with sarcastic and non-sarcastic labels. The preliminary results clearly depict the importance of using emojis for sarcasm detection, with our model attaining an accuracy of 97.35% with an F-score of 0.9708. The research validates that automated feature engineering facilitates efficient and repeatable predictive model for detecting sarcasm in indigenous, low-resource languages.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4297830905",
    "type": "article"
  },
  {
    "title": "Fuzzy Contrast Set Based Deep Attention Network for Lexical Analysis and Mental Health Treatment",
    "doi": "https://doi.org/10.1145/3506701",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Usman Ahmed; Jerry Chun‐Wei Lin; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "Internet-delivered psychological treatments (IDPT) consider mental problems based on Internet interaction. With such increased interaction because of the COVID-19 pandemic, more online tools have been widely used to provide evidence-based mental health services. This increase helps cover more population by using fewer resources for mental health treatments. Adaptivity and customization for the remedy routine can help solve mental health issues quickly. In this research, we propose a fuzzy contrast-based model that uses an attention network for positional weighted words and classifies mental patient authored text into distinct symptoms. After that, the trained embedding is used to label mental data. Then the attention network expands its lexicons to adapt to the usage of transfer learning techniques. The proposed model uses similarity and contrast sets to classify the weighted attention words. The fuzzy model then uses the sets to classify the mental health data into distinct classes. Our method is compared with non-embedding and traditional techniques to demonstrate the proposed model. From the experiments, the feature vector can achieve a high ROC curve of 0.82 with problems associated with nine symptoms.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4210357218",
    "type": "article"
  },
  {
    "title": "Exploitation for Multimedia Asian Information Processing and Artificial Intelligence-based Art Design and Teaching in Colleges",
    "doi": "https://doi.org/10.1145/3526219",
    "publication_date": "2022-05-10",
    "publication_year": 2022,
    "authors": "Bo Xu; Jing Jiang",
    "corresponding_authors": "",
    "abstract": "Artificial intelligence has been widely used in art education and learning due to its quick progress. Any creation made with the help of artificial intelligence is referred to as art design. It covers works generated independently by AI systems and works created in collaboration with humans and AI systems. The objective of directing the invention of environmental art design thinking is to stimulate students' learning and innovation abilities and teach students how to put design ideas into effect. Despite the progress of smart technologies, there are several challenges in increasing the teaching capabilities of technical art design courses, such as the influence of different variables, the absence of quantitative research, and the imperfection in the index system. In this paper, the Artificial intelligence-based Art design and teaching (AI-ADT) method in colleges increases the capacity to adapt to AI-oriented art education, establish intelligent teaching methods, and improve AI-oriented art teaching art knowledge and environments. The widespread application of artificial intelligence in design education has become a trend in development. Self-Learning Systems are software that incorporates machine learning techniques to allow computers to learn from and make judgments based on data without the need for explicit programming instructions. The art design profession should confirm and actively adapt to this development trend. Modify the original teaching mode, invent their teaching methods, continually enrich the teaching methods, enhance the quality of teaching, and constantly foster high-quality art design talents in the new age. AI-ADT investigates the optimization of the art design curriculum system in higher education institutions in the context of artificial intelligence. The experimental results show that the proposed method develops smart teaching (98.1%), flexibility (96.5%), performance (93.6%), participation (94.9%), and interaction (95.1%).",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4280563507",
    "type": "article"
  },
  {
    "title": "A Novel Assistive Glove to Convert Arabic Sign Language into Speech",
    "doi": "https://doi.org/10.1145/3545113",
    "publication_date": "2022-06-24",
    "publication_year": 2022,
    "authors": "Mohammad A. Alzubaidi; Mwaffaq Otoom; Areen M. Abu Rwaq",
    "corresponding_authors": "",
    "abstract": "People with speech disorders often communicate through special gestures and sign language gestures. However, other people around them might not understand the meaning of those gestures. The research described in this article is aimed at providing an assistive device to help those people communicate with others by translating their gestures into a spoken voice that others can understand. The proposed device includes an electronic glove that is worn on the hand. It employs an MPU6050 accelerometer/gyro with 6 degrees of freedom to continuously monitor hand orientation and movement, plus a potentiometer for each finger, to monitor changes in finger posture. The signals from the MPU6050 and the potentiometers are routed to an Arduino board, where they are processed to determine the meaning of each gesture, which is then voiced using the audio streams stored in an SD memory card. The audio output drives a speaker, allowing the listener to understand the meaning of each gesture. We built a database with the help of 10 deaf people who cannot speak. We asked them to wear the glove while performing a set of 40 Arabic sign language words and recorded the resulting data stream from the glove. That data was then used to train seven different learning algorithms. The results showed that the Decision Tree learning algorithm achieved the highest accuracy of 98%. A usability study was then conducted to determine the usefulness of the assistive device in real-time.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4283377300",
    "type": "article"
  },
  {
    "title": "A Survey on NLP Resources, Tools, and Techniques for Marathi Language Processing",
    "doi": "https://doi.org/10.1145/3548457",
    "publication_date": "2022-07-13",
    "publication_year": 2022,
    "authors": "Pawan Lahoti; Namita Mittal; Girdhari Singh",
    "corresponding_authors": "",
    "abstract": "Natural Language Processing (NLP) has been in practice for the past couple of decades, and extensive work has been done for the Western languages, particularly the English language. The Eastern counterpart, especially the languages of the Indian subcontinent, needs attention as not much language processing work has been done on these languages. Western languages are rich in dictionaries, WordNet, and associated tools, while Indian languages are lagging behind in this segment. Marathi is the third most spoken language in India and the 15th most spoken language worldwide. Lack of resources, complex linguistic facts, and the inclusion of prevalent dialects of neighbors have resulted in limited work for Marathi. The aim of this study is to provide an insight into the various linguistic resources, tools, and state-of-the-art techniques applied to the processing of the Marathi language. Initially, morphological descriptions of the Marathi language are provided, followed by a discussion on the characteristics of the Marathi language. Thereafter, for Marathi language, the availability of corpus, tools, and techniques to be used to develop NLP tasks is reviewed. Finally, gap analysis is discussed in current research and future directions for this new and dynamic area of research are listed that will benefit the Marathi Language Processing research community.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4285393783",
    "type": "article"
  },
  {
    "title": "Emotion Detection in Code-Mixed Roman Urdu - English Text",
    "doi": "https://doi.org/10.1145/3552515",
    "publication_date": "2022-08-05",
    "publication_year": 2022,
    "authors": "Abdullah Ilyas; Khurram Shahzad; Muhammad Kamran Malik",
    "corresponding_authors": "",
    "abstract": "Emotion detection is a widely studied topic in natural language processing due to its significance in a number of application areas. A plethora of studies have been conducted on emotion detection in European as well as Asian languages. However, a large majority of these studies have been conducted in monolingual settings, whereas little attention has been paid to emotion detection in code-mixed text. Specifically, merely one study has been conducted on emotion detection in Roman Urdu (RU) and English (EN) code-mixed text despite the fact that such text is widely used in social media platforms. A careful examination of the existing study has revealed several issues which justify that this area requires attention of researchers. For instance, more than 37% of the messages in the contemporary corpus are monolingual sentences representing that a purely code-mixed emotion analysis corpus is non-existent. To that end, this study has scrapped 400,000 sentences from three social media platforms to identify 20,000 RU-EN code-mixed sentences. Subsequently, an iterative approach is employed to develop emotion detection guidelines. These guidelines have been used to develop a large RU-EN emotion detection (RU-EN-Emotion) corpus in which 20,000 sentences are annotated as Neutral or Emotion-sentence. The sentences having emotions are further annotated with the respective emotions. Subsequently, 102 experiments are performed to evaluate the effectiveness of six classical machine learning techniques and six deep learning techniques. The results show, (a) CNN is the most effective technique when used with GloVe embeddings, and (b) our developed RU-EN-Emotion corpus is more useful than the contemporary corpus, as it employs a two-level classification approach.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4289936799",
    "type": "article"
  },
  {
    "title": "Attention Mechanism Architecture for Arabic Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3578265",
    "publication_date": "2022-12-29",
    "publication_year": 2022,
    "authors": "Mohamed Berrimi; Mourad Oussalah; Abdelouahab Moussaoui; Mohamed Saidi",
    "corresponding_authors": "",
    "abstract": "This article tackles the problem of sentiment analysis in the Arabic language where a new deep learning model has been put forward. The proposed model uses a hybrid bidirectional gated recurrent unit (BiGRU) and bidirectional long short-term memory (BiLSTM) additive-attention model where the Bidirectional GRU/LSTM reads the individual sentence input from left to right and vice versa, enabling the capture of the contextual information. However, the model is trained on two types of embeddings: FastText and local learnable embeddings. The BiLSTM and BiGRU architectures are put into competition to identify the best hyperparameter set for the model. The developed model has been tested on three large-scale commonly employed Arabic sentiment dataset: large-scale Arabic Book Reviews Dataset (ABRD), Hotel Arabic-Reviews Dataset (HARD), and Books Reviews in the Arabic Dataset (BRAD). The testing results demonstrate that our model outperforms both the baseline models and the state-of-the-art models reported in the original references of these datasets, achieving accuracy scores of 98.6%, 96.19%, 95.65% for LARB, HARD, and BRAD, respectively. Furthermore, to demonstrate the generalization capabilities of our model, the performances of the model have been evaluated on three other natural language processing tasks: news categorization, offensive speech detection, and Russian sentiment analysis. The results demonstrated the developed model is language- and task-independent, which offers new perspectives for the application of the developed models in several other natural language processing challenges.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4313254470",
    "type": "article"
  },
  {
    "title": "Knowledge-based Data Processing for Multilingual Natural Language Analysis",
    "doi": "https://doi.org/10.1145/3583686",
    "publication_date": "2023-02-17",
    "publication_year": 2023,
    "authors": "Deepak Kumar Jain; Yamila GarcÍa-MartÍnez Eyre; Akshi Kumar; Brij B. Gupta; Ketan Kotecha",
    "corresponding_authors": "",
    "abstract": "Natural Language Processing (NLP) aids the empowerment of intelligent machines by enhancing human language understanding for linguistic-based human-computer communication. Recent developments in processing power, as well as the availability of large volumes of linguistic data, have enhanced the demand for data-driven methods for automatic semantic analysis. This paper proposes multilingual data processing using feature extraction with classification using deep learning architectures. Here, the input text data has been collected based on various languages and processed to remove missing values and null values. The processed data has been extracted using Histogram Equalization based Global Local Entropy (HEGLE) and classified using Kernel-based Radial basis Function (Ker_Rad_BF). These architectures could be utilized to process natural language. We present solutions to the multilingual sentiment analysis issue in this research article by implementing algorithms, and we compare precision factors to discover the optimum option for multilingual sentiment analysis. For the HASOC dataset, the proposed HEGLE_ Ker_Rad_BF achieved an accuracy of 98%, a precision of 97%, a recall of 90.5%, an f-1 score of 85%, RMSE of 55.6%, and a loss curve analysis attained 44%. For the TRAC dataset, the accuracy of 98%, the precision attained is 97%, the Recall is 91%, the F-1 score is 87%, and the RMSE of the proposed neural network is 55%.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4321221912",
    "type": "article"
  },
  {
    "title": "Deep Ensemble Network for Sentiment Analysis in Bi-lingual Low-resource Languages",
    "doi": "https://doi.org/10.1145/3600229",
    "publication_date": "2023-05-27",
    "publication_year": 2023,
    "authors": "Pradeep Kumar Roy",
    "corresponding_authors": "Pradeep Kumar Roy",
    "abstract": "Sentiment analysis (SA) is the systematic identification, extraction, quantification, and study of affective states and subjective information using natural language processing. It is widely used for analyzing users’ feedback, such as reviews or social posts. Recently, SA has been one of the favorite research domains in NLP due to their wide range of applications, including E-commerce, healthcare, hotel business, and others. Many machine learning and deep learning-based models exist to predict the sentiment of the user’s post. However, the sentiment analysis in low-resource languages such as Kannada, Malayalam, Telugu, and Tamil received less attention due to language complexity and the low availability of required resources. This research fills the gap by proposing an ensemble model for predicting the sentiment of code-mixed Kannada and Malayalam languages. The ensemble of transformer-based models achieved a promising weighted F 1 -score of 0.66 for Kannada code-mixed language. In contrast, the ensemble model of the deep learning framework performed best by achieving a weighted F 1 -score of 0.72 for the Malayalam dataset, outperforming existing research.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4378530943",
    "type": "article"
  },
  {
    "title": "An Automated Data-driven Machine Intelligence Framework for Mining Knowledge To Classify Fake News Using NLP",
    "doi": "https://doi.org/10.1145/3607253",
    "publication_date": "2023-07-07",
    "publication_year": 2023,
    "authors": "Shikha Mundra; Jaiwanth Reddy; Ankit Mundra; Namita Mittal; Ankit Vidyarthi; Deepak Gupta",
    "corresponding_authors": "",
    "abstract": "The rapid spread of fake news has become a serious concern over the internet. In recent years, social media platforms are widely used for news consumption. These platforms are excellent for their low-cost accessibility and rapid dissemination of news. Contrariwise, it encourages the rapid propagation of ’fake news,’ or low-quality news containing intentionally misleading content. The quick dissemination of fake news has the potential to have devastating consequences for individuals and society as a whole. Therefore, to overcome this problem, this paper proposed an artificial intelligence framework that incorporates ensembles of deep learning features for the classification of fake news. Deep learning approaches such as Multilayer Perceptron (MLP), Convolutional Neural Networks (CNN), and Bidirectional Long Short Term Memory (BILSTM) have been used to extract local and sequential features. To obtain relevant features at the word level, these approaches are initialized using pretrained GLOVE word embedding, which results in, three base learners as GLOVE+MLP, GLOVE+CNN, and GLOVE+BiLSTM. Moreover, to extract features at the sentence level, Bidirectional Encoder Representations from Transformers (BERT) are adopted, which results in, three more base learners as BERT+MLP, BERT+CNN, BERT+BiLSTM. In total, six models are employed as base learners. Later, predictions from the best of these models are ensembled and performance is computed using ensembling techniques. Overall, we have investigated nine ensembling techniques, including weighted voting, bagging, boosting, stacked ensembles like SVC, and logistic regression. The performance is computed using four publicly available datasets regarding the macro average f1-score. We observed that soft weighted voting-based ensemble outperformed other models on three datasets achieving an f1-score of 92.99% (McIntyre), 95.22% (Kaggle), and 78.3% (Gossipcop).",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4383556413",
    "type": "article"
  },
  {
    "title": "Moving From Narrative to Interactive Multi-Modal Sentiment Analysis: A Survey",
    "doi": "https://doi.org/10.1145/3610288",
    "publication_date": "2023-07-22",
    "publication_year": 2023,
    "authors": "Junxia Ma; Lu Rong; Yazhou Zhang; Prayag Tiwari",
    "corresponding_authors": "",
    "abstract": "A growing number of individuals are expressing their opinions and engaging in interactive communication with others through various modalities, including natural language (text), facial gestures (vision), acoustic behaviors (audio), and more. Within the realms of natural language processing (NLP) and artificial intelligence (AI), multi-modal sentiment analysis has consistently remained a fundamental research area. Building upon recent advancements, this survey aims to provide researchers with a comprehensive overview of the state-of-the-art techniques in multi-modal sentiment analysis, specifically focusing on various sentiment interaction tasks. It is worth noting that the existing literature on multi-modal sentiment analysis has rarely delved into the realm of sentiment interaction. This survey presents a novel perspective by outlining the progression of multi-modal sentiment analysis from narrative sentiment to interactive sentiment. Furthermore, it discusses the research background, problem definition, and various approaches in multi-modal sentiment analysis. Additionally, this survey provides insights into the development of multi-modal sarcasm recognition, emphasizing the shift from narrativity to interactivity. Lastly, we summarize the current scientific challenges related to interaction modeling and highlight future development trends in the field.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4385075366",
    "type": "article"
  },
  {
    "title": "Multimodal Religiously Hateful Social Media Memes Classification Based on Textual and Image Data",
    "doi": "https://doi.org/10.1145/3623396",
    "publication_date": "2023-09-16",
    "publication_year": 2023,
    "authors": "Ameer Hamza; Abdul Rehman Javed; Farkhund Iqbal; Amanullah Yasin; Gautam Srivastava; Dawid Połap; Thippa Reddy Gadekallu; Zunera Jalil",
    "corresponding_authors": "",
    "abstract": "Multimodal hateful social media meme detection is an important and challenging problem in the vision-language domain. Recent studies show high accuracy for such multimodal tasks due to datasets that provide better joint multimodal embedding to narrow the semantic gap. Religiously hateful meme detection is not extensively explored among published datasets. While there is a need for higher accuracy on religiously hateful memes, deep learning–based models often suffer from inductive bias. This issue is addressed in this work with the following contributions. First, a religiously hateful memes dataset is created and published publicly to advance hateful religious memes detection research. Over 2000 meme images are collected with their corresponding text. The proposed approach compares and fine-tunes VisualBERT pre-trained on the Conceptual Caption (CC) dataset for the downstream classification task. We also extend the dataset with the Facebook hateful memes dataset. We extract visual features using ResNeXT-152 Aggregated Residual Transformations–based Masked Regions with Convolutional Neural Networks (R-CNN) and Bidirectional Encoder Representations from Transformers (BERT) uncased for textual encoding for the early fusion model. We use the primary evaluation metric of an Area Under the Operator Characters Curve (AUROC) to measure model separability. Results show that the proposed approach has a higher AUROC score of 78%, proving the model’s higher separability performance and an accuracy of 70%. It shows comparatively superior performance considering dataset size and against ensemble-based machine learning approaches.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4386803063",
    "type": "article"
  },
  {
    "title": "Context-Aware Auto-Encoded Graph Neural Model for Dynamic Question Generation using NLP",
    "doi": "https://doi.org/10.1145/3626317",
    "publication_date": "2023-10-05",
    "publication_year": 2023,
    "authors": "Suresh Dara; CH. Srinivasulu; Ch. Madhu Babu; Ananda Ravuri; Tirumala Paruchuri; Abhishek Singh Kilak; Ankit Vidyarthi",
    "corresponding_authors": "",
    "abstract": "Question generation is an important task in natural language processing that involves generating questions from a given text. This paper proposes a novel approach for dynamic question generation using a context-aware auto-encoded graph neural model. Our approach involves constructing a graph representation of the input text, where each node in the graph corresponds to a word or phrase in the text, and the edges represent the relationships between them. We then use an auto-encoder model to learn a compressed representation of the graph that captures the most important information in the input text. Finally, we use the compressed graph representation to generate questions by dynamically selecting nodes and edges based on their relevance to the context of the input text. We evaluate our approach on four benchmark datasets (SQuAD, Natural Questions, TriviaQA, and QuAC) and demonstrate that it outperforms existing state-of-the-art methods for dynamic question generation. In the experimentation, to evaluate the result four performance metrics are used i.e. BLEU, ROUGE, F1-Score, and Accuracy. The result of the proposed approach yields an accuracy of 92% on the SQuAD dataset, 89% with QuAC, and 84% with TriviaQA. while on the natural questions dataset, the model gives 79% accuracy. Our results suggest that the use of graph neural networks and auto-encoder models can significantly improve the accuracy and effectiveness of question generation in NLP. Further research in this area can lead to even more sophisticated models that can generate questions that are even more contextually relevant and natural-sounding.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4387364500",
    "type": "article"
  },
  {
    "title": "Cross-lingual Sentiment Analysis of Tamil Language Using a Multi-stage Deep Learning Architecture",
    "doi": "https://doi.org/10.1145/3631391",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Jothi Prakash; S. Arul Antran Vijay",
    "corresponding_authors": "",
    "abstract": "In recent years, sentiment analysis has become a focal point in natural language processing. Cross-lingual sentiment analysis is a particularly demanding yet essential task that seeks to construct models capable of effectively analyzing sentiments across a variety of languages. The primary motivation behind this research is to bridge the gap in current techniques that often struggle to perform well with low-resource languages, due to the scarcity of large, annotated datasets, and their unique linguistic characteristics. In light of these challenges, we propose a novel Multi-Stage Deep Learning Architecture (MSDLA) for cross-lingual sentiment analysis of the Tamil language, a low-resource language. Our approach utilizes transfer learning from a source language with abundant resources to overcome data limitations. Our proposed model significantly outperforms existing methods on the Tamil Movie Review dataset, achieving an accuracy, precision, recall, and F1-score of 0.8772, 0.8614, 0.8825, and 0.8718, respectively. ANOVA statistical comparison demonstrates that the MSDLA’s improvements over other models, including mT5, XLM, mBERT, ULMFiT, BiLSTM, LSTM with Attention, and ALBERT with Hugging Face English Embedding are significant, with p-values all less than 0.005. Ablation studies confirm the importance of both cross-lingual semantic attention and domain adaptation in our architecture. Without these components, the model’s performance drops to 0.8342 and 0.8043 in accuracy, respectively. Furthermore, MSDLA demonstrates robust cross-domain performance on the Tamil News Classification and Thirukkural datasets, achieving an accuracy of 0.8551 and 0.8624, respectively, significantly outperforming the baseline models. These findings illustrate the robustness and efficacy of our approach, making a significant contribution to cross-lingual sentiment analysis techniques, especially for low-resource languages.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4388230897",
    "type": "article"
  },
  {
    "title": "Understanding the Performance of AI Algorithms in Text-Based Emotion Detection for Conversational Agents",
    "doi": "https://doi.org/10.1145/3643133",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "Sheetal Kusal; Shruti Patil; Jyoti Choudrie; Ketan Kotecha",
    "corresponding_authors": "",
    "abstract": "Current industry trends demand automation in every aspect, where machines could replace humans. Recent advancements in conversational agents have grabbed a lot of attention from industries, markets, and businesses. Building conversational agents that exhibit human communication characteristics is a need in today's marketplace. Thus, by accumulating emotions, we can build emotionally aware conversational agents. Emotion detection in text-based dialogues has turned into a pivotal component of conversational agents, enhancing their ability to understand and respond to users’ emotional states. This article extensively compares various artificial intelligence techniques adapted to text-based emotion detection for conversational agents. The study covers a wide range of methods, from machine learning models to cutting-edge pre-trained models and deep learning models. We evaluate the performance of these techniques on the benchmark unbalanced Topical-Chat and balanced Empathetic Dialogue datasets. This article offers an overview of the practical implications of emotion detection techniques in conversational systems and their impact on user response. The outcomes of this work contribute to the ongoing development of empathetic conversational agents, emphasizing natural human-machine interactions.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391402376",
    "type": "article"
  },
  {
    "title": "Learning to Recommend Related Entities With Serendipity for Web Search Users",
    "doi": "https://doi.org/10.1145/3185663",
    "publication_date": "2018-04-23",
    "publication_year": 2018,
    "authors": "Jizhou Huang; Shiqiang Ding; Haifeng Wang; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Entity recommendation, providing entity suggestions to assist users in discovering interesting information, has become an indispensable feature of today’s Web search engine. However, the majority of existing entity recommendation methods are not designed to boost the performance in terms of serendipity, which also plays an important role in the appreciation of users for a recommendation system. To keep users engaged, it is important to take into account serendipity when building an entity recommendation system. In this article, we propose a learning to recommend framework that consists of two components: related entity finding and candidate entity ranking. To boost serendipity performance, three different sets of features that correlate with the three aspects of serendipity are employed in the proposed framework. Extensive experiments are conducted on large-scale, real-world datasets collected from a widely used commercial Web search engine. The experiments show that our method significantly outperforms several strong baseline methods. An analysis on the impact of features reveals that the set of interestingness features is the most powerful feature set, and the set of unexpectedness features can significantly contribute to recommendation effectiveness. In addition, online controlled experiments conducted on a commercial Web search engine demonstrate that our method can significantly improve user engagement against multiple baseline methods. This further confirms the effectiveness of the proposed framework.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2802223636",
    "type": "article"
  },
  {
    "title": "Filtered Pseudo-parallel Corpus Improves Low-resource Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3341726",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Aizhan Imankulova; Takayuki Sato; Mamoru Komachi",
    "corresponding_authors": "",
    "abstract": "Large-scale parallel corpora are essential for training high-quality machine translation systems; however, such corpora are not freely available for many language translation pairs. Previously, training data has been augmented by pseudo-parallel corpora obtained by using machine translation models to translate monolingual corpora into the source language. However, in low-resource language pairs, in which only low-accurate machine translation systems can be used, translation quality degrades when a pseudo-parallel corpus is naively used. To improve machine translation performance with low-resource language pairs, we propose a method to effectively expand the training data via filtering the pseudo-parallel corpus using quality estimation based on sentence-level round-trip translation. For experiments with three language pairs that utilized small, medium, and large size parallel corpora, BLEU scores significantly improved for low-resource language pairs. Additionally, the effects of iterative bootstrapping on translation performance quality is investigated; resultingly, it is confirmed that bootstrapping can further improve the translation performance.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2986030184",
    "type": "article"
  },
  {
    "title": "Fuzzy Hindi WordNet and Word Sense Disambiguation Using Fuzzy Graph Connectivity Measures",
    "doi": "https://doi.org/10.1145/2790079",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Amita Jain; D. K. Lobiyal",
    "corresponding_authors": "",
    "abstract": "In this article, we propose Fuzzy Hindi WordNet, which is an extended version of Hindi WordNet. The proposed idea of fuzzy relations and their role in modeling Fuzzy Hindi WordNet is explained. We mathematically define fuzzy relations and the composition of these fuzzy relations for this extended version. We show that the concept of composition of fuzzy relations can be used to infer a relation between two words that otherwise are not directly related in Hindi WordNet. Then we propose fuzzy graph connectivity measures that include both local and global measures. These measures are used in determining the significance of a concept (which is represented as a vertex in the fuzzy graph) in a specific context. Finally, we show how these extended measures solve the problem of word sense disambiguation (WSD) effectively, which is useful in many natural language processing applications to improve their performance. Experiments on standard sense tagged corpus for WSD show better results when Fuzzy Hindi WordNet is used in place of Hindi WordNet.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2197489394",
    "type": "article"
  },
  {
    "title": "Constructing a WordNet for Turkish Using Manual and Automatic Annotation",
    "doi": "https://doi.org/10.1145/3185664",
    "publication_date": "2018-04-23",
    "publication_year": 2018,
    "authors": "Razieh Ehsani; Ercan Solak; Olcay Taner Yıldız",
    "corresponding_authors": "",
    "abstract": "In this article, we summarize the methodology and the results of our 2-year-long efforts to construct a comprehensive WordNet for Turkish. In our approach, we mine a dictionary for synonym candidate pairs and manually mark the senses in which the candidates are synonymous. We marked every pair twice by different human annotators. We derive the synsets by finding the connected components of the graph whose edges are synonym senses. We also mined Turkish Wikipedia for hypernym relations among the senses. We analyzed the resulting WordNet to highlight the difficulties brought about by the dictionary construction methods of lexicographers. After splitting the unusually large synsets, we used random walk–based clustering that resulted in a Zipfian distribution of synset sizes. We compared our results to BalkaNet and automatic thesaurus construction methods using variation of information metric. Our Turkish WordNet is available online.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2799600422",
    "type": "article"
  },
  {
    "title": "Transliteration of Arabizi into Arabic Script for Tunisian Dialect",
    "doi": "https://doi.org/10.1145/3364319",
    "publication_date": "2019-11-28",
    "publication_year": 2019,
    "authors": "Abir Masmoudi; Mariem Ellouze; Mourad Khrouf; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "The evolution of information and communication technology has markedly influenced communication between correspondents. This evolution has facilitated the transmission of information and has engendered new forms of written communication (email, chat, SMS, comments, etc.). Most of these messages and comments are written in Latin script, also called Arabizi . Moreover, the language used in social media and SMS messaging is characterized by the use of informal and non-standard vocabulary, such as repeated letters for emphasis, typos, non-standard abbreviations, and nonlinguistic content like emoticons. Since the Tunisian dialect suffers from the unavailability of basic tools and linguistic resources compared to Modern Standard Arabic, we resort to the use of these written sources as a starting point to build large corpora automatically. In the context of natural language processing and to benefit from these networks’ data, transliterating from Arabizi to Arabic script is a necessary step because most recently available tools for processing the Tunisian dialect expect Arabic script input. Indeed, the transliteration task can help construct and enrich parallel corpora and dictionaries for the Tunisian dialect and can be useful for developing various natural language processing applications such as sentiment analysis, opinion mining, topic detection, and machine translation. In this article, we focus on converting the Tunisian dialect text that is written in Latin script to Arabic script following the Conventional Orthography for Dialectal Arabic. Then, we propose two models to transliterate Arabizi into Arabic script for the Tunisian dialect, namely a rule-based model and a discriminative model as a sequence classification task based on conditional random fields). In the first model, we use a set of transliteration rules to convert the Tunisian dialect Arabizi texts to Arabic script. In the second model, transliteration is performed both at word and character levels. In the end, our models got a character error rate of 10.47%.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3002109251",
    "type": "article"
  },
  {
    "title": "Using Bisect K-Means Clustering Technique in the Analysis of Arabic Documents",
    "doi": "https://doi.org/10.1145/2812809",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Diab Abuaiadah",
    "corresponding_authors": "Diab Abuaiadah",
    "abstract": "In this article, I have investigated the performance of the bisect K-means clustering algorithm compared to the standard K-means algorithm in the analysis of Arabic documents. The experiments included five commonly used similarity and distance functions (Pearson correlation coefficient, cosine, Jaccard coefficient, Euclidean distance, and averaged Kullback-Leibler divergence) and three leading stemmers. Using the purity measure, the bisect K-means clearly outperformed the standard K-means in all settings with varying margins. For the bisect K-means, the best purity reached 0.927 when using the Pearson correlation coefficient function, while for the standard K-means, the best purity reached 0.884 when using the Jaccard coefficient function. Removing stop words significantly improved the results of the bisect K-means but produced minor improvements in the results of the standard K-means. Stemming provided additional minor improvement in all settings except the combination of the averaged Kullback-Leibler divergence function and the root-based stemmer, where the purity was deteriorated by more than 10%. These experiments were conducted using a dataset with nine categories, each of which contains 300 documents.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2284481408",
    "type": "article"
  },
  {
    "title": "BenLem (A Bengali Lemmatizer) and Its Role in WSD",
    "doi": "https://doi.org/10.1145/2835494",
    "publication_date": "2016-02-26",
    "publication_year": 2016,
    "authors": "Abhisek Chakrabarty; Utpal Garain",
    "corresponding_authors": "",
    "abstract": "A lemmatization algorithm for Bengali has been developed and evaluated. Its effectiveness for word sense disambiguation (WSD) is also investigated. One of the key challenges for computer processing of highly inflected languages is to deal with the frequent morphological variations of the root words appearing in the text. Therefore, a lemmatizer is essential for developing natural language processing (NLP) tools for such languages. In this experiment, Bengali, which is the national language of Bangladesh and the second most popular language in the Indian subcontinent, has been taken as a reference. In order to design the Bengali lemmatizer (named as BenLem), possible transformations through which surface words are formed from lemmas are studied so that appropriate reverse transformations can be applied on a surface word to get the corresponding lemma back. BenLem is found to be capable of handling both inflectional and derivational morphology in Bengali. It is evaluated on a set of 18 news articles taken from the FIRE Bengali News Corpus consisting of 3,342 surface words (excluding proper nouns) and found to be 81.95% accurate. The role of the lemmatizer is then investigated for Bengali WSD. Ten highly polysemous Bengali words are considered for sense disambiguation. The FIRE corpus and a collection of Tagore’s short stories are considered for creating the WSD dataset. Different WSD systems are considered for this experiment, and it is noticed that BenLem improves the performance of all the WSD systems and the improvements are statistically significant.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2285811659",
    "type": "article"
  },
  {
    "title": "Graph-based Multimodal Ranking Models for Multimodal Summarization",
    "doi": "https://doi.org/10.1145/3445794",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Junnan Zhu; Lu Xiang; Yu Zhou; Jiajun Zhang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Multimodal summarization aims to extract the most important information from the multimedia input. It is becoming increasingly popular due to the rapid growth of multimedia data in recent years. There are various researches focusing on different multimodal summarization tasks. However, the existing methods can only generate single-modal output or multimodal output. In addition, most of them need a lot of annotated samples for training, which makes it difficult to be generalized to other tasks or domains. Motivated by this, we propose a unified framework for multimodal summarization that can cover both single-modal output summarization and multimodal output summarization. In our framework, we consider three different scenarios and propose the respective unsupervised graph-based multimodal summarization models without the requirement of any manually annotated document-summary pairs for training: (1) generic multimodal ranking, (2) modal-dominated multimodal ranking, and (3) non-redundant text-image multimodal ranking. Furthermore, an image-text similarity estimation model is introduced to measure the semantic similarity between image and text. Experiments show that our proposed models outperform the single-modal summarization methods on both automatic and human evaluation metrics. Besides, our models can also improve the single-modal summarization with the guidance of the multimedia information. This study can be applied as the benchmark for further study on multimodal summarization task.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3165529210",
    "type": "article"
  },
  {
    "title": "A Novel Attack on Monochrome and Greyscale Devanagari CAPTCHAs",
    "doi": "https://doi.org/10.1145/3439798",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Mohinder Kumar; Manish Kumar Jindal; Munish Kumar",
    "corresponding_authors": "",
    "abstract": "The use of computer programs in breaching web site security is common today. CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) and human interaction proofs are the cost-effective solution to these kinds of computer attacks on web sites. These CAPTCHAs are available in many forms, such as those based on text, images and audio. A CAPTCHA must be secure enough that it cannot be broken by a computer program, and it must be usable enough that humans can easily understand it. The most popular is the text-based scheme. Most text-based CAPTCHAs are based on the English language and are not usable by the native people of India. Research has proven that native people are more comfortable with native language–based CAPTCHA. Devanagari-based CAPTCHAs are also available, but the security aspect has not been tested. Unfortunately, English language–based CAPTCHAs are successfully broken. Therefore, it is important to test the security of Devanagari script-based CAPTCHAs. We picked five unique monochrome CAPTCHAs and five unique greyscale CAPTCHAs for testing security. We achieved 88.13% to 97.6% segmentation rates on these schemes and generated six types of features for these segmented characters, such as raw pixels, zoning, projection, Scale-Invariant Feature Transform (SIFT), Speeded-Up Robust Features (SURF) and Oriented Fast and Rotated BRIEF (ORB). For classification, we used three classifiers for comparative analyses. Using k-Nearest Neighbour (k-NN), Support Vector Machine (SVM) and Random Forest, we achieved high recognition on monochrome and greyscale schemes. For monochrome Devanagari CAPTCHAs, the recognition rate of k-NN ranges from 64.78% to 82.39%, SVM ranges from 76.46% to 91.34% and Random Forest ranges from 80.34% to 91.28%. For greyscale Devanagari CAPTCHAs, the recognition rate of k-NN ranges from 67.52% to 85.47%, SVM ranges from 76.9% to 91.71% and Random Forest ranges from 83.07% to 92.13%. We achieved a breaking rate for monochrome schemes of 66% to 85% and for greyscale schemes of 73% to 93%.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3164893804",
    "type": "article"
  },
  {
    "title": "Two New Large Corpora for Vietnamese Aspect-based Sentiment Analysis at Sentence Level",
    "doi": "https://doi.org/10.1145/3446678",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Dang Van Thin; Ngan Luu-Thuy Nguyen; Tri Minh Truong; Lac Si Le; Duy Tin Vo",
    "corresponding_authors": "",
    "abstract": "Aspect-based sentiment analysis has been studied in both research and industrial communities over recent years. For the low-resource languages, the standard benchmark corpora play an important role in the development of methods. In this article, we introduce two benchmark corpora with the largest sizes at sentence-level for two tasks: Aspect Category Detection and Aspect Polarity Classification in Vietnamese. Our corpora are annotated with high inter-annotator agreements for the restaurant and hotel domains. The release of our corpora would push forward the low-resource language processing community. In addition, we deploy and compare the effectiveness of supervised learning methods with a single and multi-task approach based on deep learning architectures. Experimental results on our corpora show that the multi-task approach based on BERT architecture outperforms the neural network architectures and the single approach. Our corpora and source code are published on this footnoted site. 1",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3165505241",
    "type": "article"
  },
  {
    "title": "A Framework for Indonesian Grammar Error Correction",
    "doi": "https://doi.org/10.1145/3440993",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Nankai Lin; Boyu Chen; Xiaotian Lin; Kanoksak Wattanachote; Shengyi Jiang",
    "corresponding_authors": "",
    "abstract": "Grammatical Error Correction (GEC) is a challenge in Natural Language Processing research. Although many researchers have been focusing on GEC in universal languages such as English or Chinese, few studies focus on Indonesian, which is a low-resource language. In this article, we proposed a GEC framework that has the potential to be a baseline method for Indonesian GEC tasks. This framework treats GEC as a multi-classification task. It integrates different language embedding models and deep learning models to correct 10 types of Part of Speech (POS) error in Indonesian text. In addition, we constructed an Indonesian corpus that can be utilized as an evaluation dataset for Indonesian GEC research. Our framework was evaluated on this dataset. Results showed that the Long Short-Term Memory model based on word-embedding achieved the best performance. Its overall macro-average F 0.5 in correcting 10 POS error types reached 0.551. Results also showed that the framework can be trained on a low-resource dataset.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3165574448",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis of Sinhala News Comments",
    "doi": "https://doi.org/10.1145/3445035",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Surangika Ranathunga; Isuru Liyanage",
    "corresponding_authors": "",
    "abstract": "Sinhala is a low-resource language, for which basic language and linguistic tools have not been properly defined. This affects the development of NLP-based end-user applications for Sinhala. Thus, when implementing NLP tools such as sentiment analyzers, we have to rely only on language-independent techniques. This article presents the use of such language-independent techniques in implementing a sentiment analysis system for Sinhala news comments. We demonstrate that for low-resource languages such as Sinhala, the use of recently introduced word embedding models as semantic features can compensate for the lack of well-developed language-specific linguistic or language resources, and text classification with acceptable accuracy is indeed possible using both traditional statistical classifiers and Deep Learning models. The developed classification models, a corpus of 8.9 million tokens extracted from Sinhala news articles and user comments, and Sinhala Word2Vec and fastText word embedding models are now available for public use; 9,048 news comments annotated with POSITIVE/NEGATIVE/NEUTRAL polarities have also been released.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3093406901",
    "type": "article"
  },
  {
    "title": "Multi-Objective Heuristic Decision Making and Benchmarking for Mobile Applications in English Language Learning",
    "doi": "https://doi.org/10.1145/3439799",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Chunhe Zhao; BalaAnand Muthu; P. Mohamed Shakeel",
    "corresponding_authors": "",
    "abstract": "This research proposes to evaluate and analyze the decision matrix for learner's English mobile applications (EMAs) based on multi-objective heuristic decision making with a view to listening, speaking, reading, and writing. Because of the number of criteria, the significance of parameters, and variance in results, EMAs are difficult. Decision making has built on the combination of listening, speaking, reading, and writing and EMA evaluation criteria for students. The requirements are adapted from a framework of pre-school education. Six alternatives and 17 skills as a requirement are included in decision-making results. The six EMA are then assessed, with six English learning experts distributing a review form. The application subsequently is evaluated using the best-worst method and preference-order technique (TOPSIS) using multi-objective heuristic decision making methods. The best-worst method is used to measure requirements, whereas TOPSIS is used to test and assess the applications. In two cases, namely person and group, TOPSIS is used. Internal and external aggregations are used throughout the group context. In effect, the aim of evaluating the proposed study and comparing it to six relative studies with scenarios and benchmarking checklists is to develop an objectives validation framework for e-apps.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3175733165",
    "type": "article"
  },
  {
    "title": "Low-resource Neural Machine Translation: Methods and Trends",
    "doi": "https://doi.org/10.1145/3524300",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Shumin Shi; Xing Wu; Rihai Su; Heyan Huang",
    "corresponding_authors": "",
    "abstract": "Neural Machine Translation (NMT) brings promising improvements in translation quality, but until recently, these models rely on large-scale parallel corpora. As such corpora only exist on a handful of language pairs, the translation performance is far from the desired effect in the majority of low-resource languages. Thus, developing low-resource language translation techniques is crucial and it has become a popular research field in neural machine translation. In this article, we make an overall review of existing deep learning techniques in low-resource NMT. We first show the research status as well as some widely used low-resource datasets. Then, we categorize the existing methods and show some representative works detailedly. Finally, we summarize the common characters among them and outline the future directions in this field.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4221017237",
    "type": "article"
  },
  {
    "title": "Am I a Resource-Poor Language? Data Sets, Embeddings, Models and Analysis for four different NLP Tasks in Telugu Language",
    "doi": "https://doi.org/10.1145/3531535",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Mounika Marreddy; Subba Reddy Oota; Lakshmi Sireesha Vakada; Venkata Charan Chinni; Radhika Mamidi",
    "corresponding_authors": "",
    "abstract": "Due to the lack of a large annotated corpus, many resource-poor Indian languages struggle to reap the benefits of recent deep feature representations in Natural Language Processing (NLP) . Moreover, adopting existing language models trained on large English corpora for Indian languages is often limited by data availability, rich morphological variation, syntax, and semantic differences. In this paper, we explore the traditional to recent efficient representations to overcome the challenges of a low resource language, Telugu. In particular, our main objective is to mitigate the low-resource problem for Telugu. Overall, we present several contributions to a resource-poor language viz. Telugu. (i) a large annotated data (35,142 sentences in each task) for multiple NLP tasks such as sentiment analysis, emotion identification, hate-speech detection, and sarcasm detection, (ii) we create different lexicons for sentiment, emotion, and hate-speech for improving the efficiency of the models, (iii) pretrained word and sentence embeddings, and (iv) different pretrained language models for Telugu such as ELMo-Te , BERT-Te , RoBERTa-Te , ALBERT-Te , and DistilBERT-Te on a large Telugu corpus consisting of 8,015,588 sentences (1,637,408 sentences from Telugu Wikipedia and 6,378,180 sentences crawled from different Telugu websites). Further, we show that these representations significantly improve the performance of four NLP tasks and present the benchmark results for Telugu. We argue that our pretrained embeddings are competitive or better than the existing multilingual pretrained models: mBERT , XLM-R , and IndicBERT . Lastly, the fine-tuning of pretrained models show higher performance than linear probing results on four NLP tasks with the following F1-scores: Sentiment (68.72), Emotion (58.04), Hate-Speech (64.27), and Sarcasm (77.93). We also experiment on publicly available Telugu datasets (Named Entity Recognition, Article Genre Classification, and Sentiment Analysis) and find that our Telugu pretrained language models ( BERT-Te and RoBERTa-Te ) outperform the state-of-the-art system except for the sentiment task. We open-source our corpus, four different datasets, lexicons, embeddings, and code https://github.com/Cha14ran/DREAM-T. The pretrained Transformer models for Telugu are available at https://huggingface.co/ltrctelugu.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4225112709",
    "type": "article"
  },
  {
    "title": "Integrating Heterogeneous Ontologies in Asian Languages Through Compact Genetic Algorithm with Annealing Re-sample Inheritance Mechanism",
    "doi": "https://doi.org/10.1145/3519298",
    "publication_date": "2022-02-24",
    "publication_year": 2022,
    "authors": "Xingsi Xue; Wenyu Liu",
    "corresponding_authors": "",
    "abstract": "An ontology is a state-of-the-art knowledge modeling technique in the natural language domain, which has been widely used to overcome the linguistic barriers in Asian and European countries’ intelligent applications. However, due to the different knowledge backgrounds of ontology developers, the entities in the ontologies could be defined in different ways, which hamper the communications among the intelligent applications built on them. How to find the semantic relationships among the entities that are lexicalized in different languages is called the Cross-lingual Ontology Matching problem (COM), which is a challenge problem in the ontology matching domain. To face this challenge, being inspired by the success of the Genetic Algorithm (GA) in the ontology matching domain, this work proposes a Compact GA with Annealing Re-sample Inheritance mechanism (CGA-ARI) to efficiently address the COM problem. In particular, a Cross-lingual Similarity Metric (CSM) is presented to distinguish two cross-lingual entities, a discrete optimal model is built to define the COM problem, and the compact encoding mechanism and the Annealing Re-sample Inheritance mechanism (ARI) are introduced to improve CGA’s searching performance. The experiment uses Multifarm track to test CGA-ARI’s performance, which includes 45 ontology pairs in different languages. The experimental results show that CGA-ARI is able to significantly improve the performance of GA and CGA and determine better alignments than state-of-the-art ontology matching systems.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4213420832",
    "type": "article"
  },
  {
    "title": "An Overview of Indian Spoken Language Recognition from Machine Learning Perspective",
    "doi": "https://doi.org/10.1145/3523179",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Spandan Dey; Md Sahidullah; Goutam Saha",
    "corresponding_authors": "",
    "abstract": "Automatic spoken language identification (LID) is a very important research field in the era of multilingual voice-command-based human-computer interaction. A front-end LID module helps to improve the performance of many speech-based applications in the multilingual scenario. India is a populous country with diverse cultures and languages. The majority of the Indian population needs to use their respective native languages for verbal interaction with machines. Therefore, the development of efficient Indian spoken language recognition systems is useful for adapting smart technologies in every section of Indian society. The field of Indian LID has started gaining momentum since the early 2000s, mainly due to the development of several standard multilingual speech corpora for the Indian languages. Even though significant research progress has already been made in this field, to the best of our knowledge, there are not many attempts to analytically review them collectively. In this work, we have conducted one of the very first attempts to present a comprehensive review of the Indian spoken language recognition research field. In-depth analysis has been presented to emphasize the unique challenges of low-resource and mutual influences for developing LID systems in the Indian contexts. Several essential aspects of the Indian LID research, such as the detailed description of the available speech corpora, the major research contributions, including the earlier attempts based on statistical modeling to the recent approaches based on different neural network architectures, and the future research trends are discussed. This review work will help assess the state of the present Indian LID research by any active researcher or any research enthusiasts from related fields.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4220991896",
    "type": "article"
  },
  {
    "title": "Social Network Analysis: A Survey on Measure, Structure, Language Information Analysis, Privacy, and Applications",
    "doi": "https://doi.org/10.1145/3539732",
    "publication_date": "2022-06-08",
    "publication_year": 2022,
    "authors": "Shashank Sheshar Singh; Vishal Srivastava; Ajay Kumar; Shailendra Tiwari; Dilbag Singh; Heung-No Lee",
    "corresponding_authors": "",
    "abstract": "The rapid growth in popularity of online social networks provides new opportunities in computer science, sociology, math, information studies, biology, business, and more. Social network analysis (SNA) is a paramount technique supporting understanding social relationships and networks. Accordingly, certain studies and reviews have been presented focusing on information dissemination, influence analysis, link prediction, and more. However, the ultimate aim is for social network background knowledge and analysis to solve real-world social network problems. SNA still has several research challenges in this context, including users’ privacy in online social networks. Inspired by these facts, we have presented a survey on social network analysis techniques, visualization, structure, privacy, and applications. This detailed study has started with the basics of network representation, structure, and measures. Our primary focus is on SNA applications with state-of-the-art techniques. We further provide a comparative analysis of recent developments on SNA problems in the sequel. The privacy preservation with SNA is also surveyed. In the end, research challenges and future directions are discussed to suggest to researchers a starting point for their research.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4281871631",
    "type": "article"
  },
  {
    "title": "Exploring Multi-lingual, Multi-task, and Adversarial Learning for Low-resource Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3514498",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Mamta Mamta; Asif Ekbal; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Deep learning has become most prominent in solving various Natural Language Processing (NLP) tasks including sentiment analysis. However, these techniques require a considerably large amount of annotated corpus, which is not easy to obtain for most of the languages, especially under the scenario of low-resource settings. In this article, we propose a deep multi-task multi-lingual adversarial framework to solve the resource-scarcity problem of sentiment analysis by leveraging the useful and relevant knowledge from a high-resource language. To transfer the knowledge between the different languages, both the languages are mapped to the shared semantic space using cross-lingual word embeddings. We evaluate our proposed architecture on a low-resource language, Hindi, using English as the high-resource language. Experiments show that our proposed model achieves an accuracy of 60.09% for the movie review dataset and 72.14% for the product review dataset. The effectiveness of our proposed approach is demonstrated with significant performance gains over the state-of-the-art systems and translation-based baselines.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4292596092",
    "type": "article"
  },
  {
    "title": "Abstractive Summarization of Text Document in Malayalam Language: Enhancing Attention Model Using POS Tagging Feature",
    "doi": "https://doi.org/10.1145/3561819",
    "publication_date": "2022-09-10",
    "publication_year": 2022,
    "authors": "Sindhya K Nambiar; David Peter S; Sumam Mary Idicula",
    "corresponding_authors": "",
    "abstract": "Over the past few years, researchers are showing huge interest in sentiment analysis and summarization of documents. The primary reason being that huge volumes of information are available in textual format, and this data has proven helpful for real-world applications and challenges. The sentiment analysis of a document will help the user comprehend the content’s emotional intent. Abstractive summarization algorithms generate a condensed version of the text, which can then be used to determine the emotion represented in the text using sentiment analysis. Recent research in abstractive summarization concentrates on neural network-based models, rather than conjunctions-based approaches, which might improve the overall efficiency. Neural network models like attention mechanism are tried out to handle complex works with promising results. The proposed work aims to present a novel framework that incorporates the part of speech tagging feature to the word embedding layer, which is then used as the input to the attention mechanism. With POS feature being part of the input layer, this framework is capable of dealing with words containing contextual and morphological information. The relevance of POS tagging here is due to its strong reliance on the language’s syntactic, contextual, and morphological information. The three main elements in the work are pre-processing, POS tagging feature in the embedding phase, and the incorporation of it into the attention mechanism. The word embedding provides the semantic concept about the word, while the POS tags give an idea about how significant the words are in the context of the content, which corresponds to the syntactic information. The proposed work was carried out in Malayalam, one of the prominent Indian languages. A widely used and accepted dataset from the English language was translated to Malayalam for conducting the experiments. The proposed framework gives a ROUGE score of 28, which outperformed the baseline models.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4296270221",
    "type": "article"
  },
  {
    "title": "KenSwQuAD—A Question Answering Dataset for Swahili Low-resource Language",
    "doi": "https://doi.org/10.1145/3578553",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Barack Wanjawa; Lilian Wanzare; Florence Indede; Owen McOnyango; Lawrence Muchemi; Edward Ombui",
    "corresponding_authors": "",
    "abstract": "The need for question-answering (QA) datasets in low-resource languages is the motivation of this research, leading to the development of the Kencorpus Swahili Question Answering Dataset (KenSwQuAD). This dataset is annotated from raw story texts of Swahili, a low-resource language that is predominantly spoken in eastern Africa and in other parts of the world. Question-answering datasets are important for machine comprehension of natural language for tasks such as internet search and dialog systems. Machine learning systems need training data such as the gold-standard question-answering set developed in this research. The research engaged annotators to formulate QA pairs from Swahili texts collected by the Kencorpus project, a Kenyan languages corpus. The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA pairs each, resulting in a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the annotated texts confirmed that the QA pairs were all correctly annotated. A proof of concept on applying the set to the QA task confirmed that the dataset can be usable for such tasks. KenSwQuAD has also contributed to resourcing of the Swahili language.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4317036142",
    "type": "article"
  },
  {
    "title": "MABERT: Mask-Attention-Based BERT for Chinese Event Extraction",
    "doi": "https://doi.org/10.1145/3597455",
    "publication_date": "2023-05-19",
    "publication_year": 2023,
    "authors": "Ling Ding; Xiaojun Chen; Jian Wei; Yang Xiang",
    "corresponding_authors": "",
    "abstract": "Event extraction is an essential but challenging task in information extraction. This task has considerably benefited from pre-trained language models, such as BERT. However, when it comes to the trigger-word mismatch problem in languages without natural delimiters, existing methods ignore the complement of lexical information to BERT. In addition, the inherent multi-role noise problem could limit the performance of methods when one sentence contains multiple events. In this article, we propose a Mask-Attention-based BERT (MABERT) framework for Chinese event extraction to address the above problems. Firstly, in order to avoid trigger-word mismatch and integrate lexical features into BERT layers directly, a mask-attention-based transformer augmented with two mask matrices is devised to replace the original one in BERT. By the mask-attention-based transformer, the character sequence interacts with external lexical semantics sufficiently and keeps its structure information at the same time. Moreover, against the multi-role noise problem, we make use of event type information from representation and classification, two aspects to enrich entity features, where type markers and event-schema-based mask matrix are proposed. Experimental results on the widely used ACE2005 dataset show the effectiveness of our proposed MABERT on Chinese event extraction task compared with other state-of-the-art methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4377096731",
    "type": "article"
  },
  {
    "title": "Aspect-Based Sentiment Analysis for Arabic Food Delivery Reviews",
    "doi": "https://doi.org/10.1145/3605146",
    "publication_date": "2023-06-18",
    "publication_year": 2023,
    "authors": "Ibrahim Al-Jarrah; Ahmad Mustafa; Hassan Najadat",
    "corresponding_authors": "",
    "abstract": "Business customers and consumers share their reviews online on social platforms such as Twitter. Therefore, Twitter data sentiment analysis is extremely useful for both research and commercial purposes. Manually analyzing reviews takes a long time and effort, hence, automatic sentiment analysis is required. In this article, we address aspect-based sentiment analysis for Arabic food delivery reviews using several deep learning approaches. In particular, we propose to use Transformer-based models (GigaBERT and AraBERT), Bi-LSTM-CRF, and LSTM, as well as a classical machine learning algorithm (SVM). We also present our dataset of food delivery service reviews, which we collected from Twitter. We annotated them and used them for training and evaluating our approaches. The experiments show that both GigaBERT and AraBERT outperformed the other models in all the tasks. The Transformer-based models received F1-scores of 77% in the aspect terms detection task, 82% in the Aspect category detection task, and 81% in the aspect polarity detection task, gaining 2%, 4%, and 4% over Bi-LSTM-CRF and LSTM in the first, second, and third tasks, respectively.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4381125117",
    "type": "article"
  },
  {
    "title": "End-to-end Multi-modal Low-resourced Speech Keywords Recognition Using Sequential Conv2D Nets",
    "doi": "https://doi.org/10.1145/3606019",
    "publication_date": "2023-07-24",
    "publication_year": 2023,
    "authors": "Pooja Gambhir; Amita Dev; Poonam Bansal; Deepak Kumar Sharma",
    "corresponding_authors": "",
    "abstract": "Advanced Neural Networks are widely used to recognize multi-modal conversational speech with significant improvements in accuracy automatically. Significantly, Convolutional Neural sheets have retreated cutting-edge performance in Automatic Voice Recognition (AVR) recently more appropriately in English; however, the Hindi language has not been explored and examined well on AVR systems. The work in this article has exposed a three-layered two-dimensional Sequential Convolutional neural architecture. The Sequential Conv2D is an end-to-end system that can instantaneously exploit speech signal spectral and temporal structures. The network has been trained and tested on different cepstral features such as Frequency and Time variant-Mel-Filters, Gamma-tone Filter Cepstral Quantities, Bark-Filter band Coefficients, and Spectrogram features of speech structures. The experiment was performed on two low-resourced speech command datasets; Hindi with 27,145 Speech Keywords developed by TIFR and 23,664 (1-s utterances) of English speech commands by Google TensorFlow and AIY English Speech Commands. The experimental outcome showed that the model achieves significant performance of Convolutional layers trained on spectrograms with 91.60% accuracy, compared to that achieved in other cepstral feature labels for English speech. However, the model achieved an accuracy of 69.65% for Hindi audio words in which bark-frequency cepstral coefficients features outperformed spectrogram features.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4385215764",
    "type": "article"
  },
  {
    "title": "Research On the Optimization of Ideological and Political Education in Universities Integrating Artificial Intelligence Technology Under the Guidance of Curriculum Ideological and Political Thinking",
    "doi": "https://doi.org/10.1145/3611012",
    "publication_date": "2023-08-25",
    "publication_year": 2023,
    "authors": "Fuxiang Dong; Shuangli Dong",
    "corresponding_authors": "",
    "abstract": "Using the association rules among both data mining (DM) and Artificial Intelligence (AI) technology, this paper proposes concepts and techniques for enhancing the curriculum environment based on the modernization of the educational field curriculum scheme of Ideological and Political Education (IPE) classes in universities and colleges. The study analyze the inter-subjectivity concept to the model development from the conceptual level, the benchmark value of the whole setting, multidimensional spacetime, and the leading importance of the Internet governance idea to the reformation of IPE in the colleges and universities. The findings assess the issues with IPE transformation and development in colleges and universities and suggest solutions, as well as four mechanisms of \"methodical design underlined in the overview, team-work cooperation of team development, augmentation of environmental development, and double endorsement of quality management”. The concept of developing a” three-stage full atmosphere\" network IPE architecture for universities and colleges is presented as one of the remedies to enhance the efficiency of the IPE in universities. This idea is based on experience summary, theoretical analysis, and empirical study. The timeliness concerns of IPE transformation in universities and colleges are resolved by using in-depth comprehension and evaluation of the necessary knowledge of DM-AI, which is then used to apply the appropriate DM-AI methodologies. This allows the relevant administrators to quickly learn the crucial data in complicated problems and suggest solutions for subsequent decision-making.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4386168788",
    "type": "article"
  },
  {
    "title": "EnML: Multi-label Ensemble Learning for Urdu Text Classification",
    "doi": "https://doi.org/10.1145/3616111",
    "publication_date": "2023-08-28",
    "publication_year": 2023,
    "authors": "Faiza Mehmood; Rehab Shahzadi; Hina Ghafoor; Muhammad Nabeel Asim; Muhammad Usman Ghani Khan; Waqar Mahmood; Andreas Dengel",
    "corresponding_authors": "",
    "abstract": "Exponential growth of electronic data requires advanced multi-label classification approaches for the development of natural language processing (NLP) applications such as recommendation systems, drug reaction detection, hate speech detection, and opinion recognition/mining. To date, several machine and deep learning–based multi-label classification methodologies have been proposed for English, French, German, Chinese, Arabic, and other developed languages. Urdu is the 11th largest language in the world and has no computer-aided multi-label textual news classification approach. Unlike other languages, Urdu is lacking multi-label text classification datasets that can be used to benchmark the performance of existing machine and deep learning methodologies. With an aim to accelerate and expedite research for the development of Urdu multi-label text classification–based applications, this article provides multiple contributions as follows: First, it provides a manually annotated multi-label textual news classification dataset for the Urdu language. Second, it benchmarks the performance of traditional machine learning approaches particularly by adapting three data transformation approaches along with three top-performing machine learning classifiers and four algorithm adaptation-based approaches. Third, it benchmarks performance of 16 existing deep learning approaches and the four most widely used language models. Finally, it provides an ensemble approach that reaps the benefits of three different deep learning architectures to precisely predict different classes associated with a particular Urdu textual document. Experimental results reveal that proposed ensemble approach performance values (87% accuracy, 92% F1-score, and 8% hamming loss) are significantly higher than adapted machine and deep learning–based approaches.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4386219923",
    "type": "article"
  },
  {
    "title": "Deep Learning based Multilingual Speech Synthesis using Multi Feature Fusion Methods",
    "doi": "https://doi.org/10.1145/3618110",
    "publication_date": "2023-09-04",
    "publication_year": 2023,
    "authors": "Praveena Nuthakki; Madhavi Katamaneni; J. N. Chandra Sekhar; Kumari Gubbala; Bullarao Domathoti; Venkata Rao Maddumala; Kumar Raja Jetti",
    "corresponding_authors": "",
    "abstract": "The poor intelligibility and out-of-the-ordinary nature of the traditional concatenation speech synthesis technologies are two major problems. CNN's context deep learning approaches aren't robust enough for sensitive speech synthesis. Our suggested approach may satisfy such needs and modify the complexities of voice synthesis. The suggested model's minimal aperiodic distortion makes it an excellent candidate for a communication recognition model. Our suggested method is as close to human speech as possible, despite the fact that speech synthesis has a number of audible flaws. Additionally, there is excellent hard work to be done in incorporating sentiment analysis into text categorization using natural language processing. The intensity of feeling varies greatly from nation to country. To improve their voice synthesis outputs, models need to include more and more concealed layers &amp; nodes into the updated mixture density network. For our suggested algorithm to perform at its best, we need a more robust network foundation and optimization methods. We hope that after reading this article and trying out the example data provided, both experienced researchers and those just starting out would have a better grasp of the steps involved in creating a deep learning approach. Overcoming fitting issues with less data in training, the model is making progress. More space is needed to hold the input parameters in the DL-based method.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4386421885",
    "type": "article"
  },
  {
    "title": "Efficient Low-Resource Medical Information Processing Based on Semantic Analysis and Granular Computing",
    "doi": "https://doi.org/10.1145/3626319",
    "publication_date": "2023-10-04",
    "publication_year": 2023,
    "authors": "Yu-Ting Cheng; Chao Zhang; Arun Kumar Sangaiah; Xueqing Fan; Anna Wang; Liyin Wang; Yixin Liu",
    "corresponding_authors": "",
    "abstract": "The rise of the digital economy and e-commerce has fostered a movement towards efficient low-resource medical information processing, a trend that holds great importance in the healthcare sector. Diabetes, being a widespread chronic condition, has witnessed the introduction of glucometers, which offer patients a convenient method of monitoring their blood sugar levels. However, it is worth noting that a considerable proportion of online comments may be subject to emotional bias or contain inaccurate information. Furthermore, the performance of glucometers can be influenced by several attributes, including price, accuracy and portability, thereby potentially complicating the decision-making process for consumers. Semantic analysis can be employed to acquire valuable information, aiding consumers in reasonably choosing the suitable glucometer. This paper utilizes the benefits of granular computing, an emerging computing paradigm, to effectively handle incomplete and uncertain medical information. It employs generalized fuzzy sets, rough sets and three-way decisions (TWD) techniques to boost the accuracy and reliability of medical information fusion. Subsequently, the MABAC (Multi-Attribute Border Approximation Area Comparison) method is utilized to evaluate the reviews of every glucometer, calculate their aggregated scores, and rank and compare them. Ultimately, in light of consumers’ needs and trade-offs, the glucometer with the highest score can be selected. The proposed approach comprehensively considers the weight and priority of multiple attributes, reduces information overload and mitigates selection difficulties, thereby enhancing the accuracy and reliability of low-resource medical information processing.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4387344366",
    "type": "article"
  },
  {
    "title": "An Expert System for Indian Sign Language Recognition Using Spatial Attention–based Feature and Temporal Feature",
    "doi": "https://doi.org/10.1145/3643824",
    "publication_date": "2024-02-03",
    "publication_year": 2024,
    "authors": "Soumen Das; Saroj Kr. Biswas; Biswajit Purkayastha",
    "corresponding_authors": "",
    "abstract": "Sign Language (SL) is the only means of communication for the hearing-impaired people. Normal people have difficulty understanding SL, resulting in a communication barrier between hearing impaired people and hearing community. However, the Sign Language Recognition System (SLRS) has helped to bridge the communication gap. Many SLRs are proposed for recognizing SL; however, a limited number of works are reported for Indian Sign Language (ISL). Most of the existing SLRS focus on global features other than the Region of Interest (ROI). Focusing more on the hand region and extracting local features from the ROI improves system accuracy. The attention mechanism is a widely used technique for emphasizing the ROI. However, only a few SLRS used the attention method. They employed the Convolution Block Attention Module and temporal attention but Spatial Attention (SA) is not utilized in previous SLRS. Therefore, a novel SA based SLRS named Spatial Attention-based Sign Language Recognition Module (SASLRM) is proposed to recognize ISL words for emergency situations. SASLRM recognizes ISL words by combining convolution features from a pretrained VGG-19 model and attention features from a SA module. The proposed model accomplished an average accuracy of 95.627% on the ISL dataset. The proposed SASLRM is further validated on LSA64, WLASL, and Cambridge Hand Gesture Recognition datasets where, the proposed model reached an accuracy of 97.84%, 98.86%, and 98.22%, respectively. The results indicate the effectiveness of the proposed SLRS in comparison with the existing SLRS.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391510018",
    "type": "article"
  },
  {
    "title": "Conditional Random Fields for Korean Morpheme Segmentation and POS Tagging",
    "doi": "https://doi.org/10.1145/2700051",
    "publication_date": "2015-06-12",
    "publication_year": 2015,
    "authors": "Seung‐Hoon Na",
    "corresponding_authors": "Seung‐Hoon Na",
    "abstract": "There has been recent interest in statistical approaches to Korean morphological analysis. However, previous studies have been based mostly on generative models, including a hidden Markov model (HMM), without utilizing discriminative models such as a conditional random field (CRF). We present a two-stage discriminative approach based on CRFs for Korean morphological analysis. Similar to methods used for Chinese, we perform two disambiguation procedures based on CRFs: (1) morpheme segmentation and (2) POS tagging. In morpheme segmentation, an input sentence is segmented into sequences of morphemes, where a morpheme unit is either atomic or compound. In the POS tagging procedure, each morpheme (atomic or compound) is assigned a POS tag. Once POS tagging is complete, we carry out a post-processing of the compound morphemes, where each compound morpheme is further decomposed into atomic morphemes, which is based on pre-analyzed patterns and generalized HMMs obtained from the given tagged corpus. Experimental results show the promise of our proposed method.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1447868166",
    "type": "article"
  },
  {
    "title": "End-to-End Korean Part-of-Speech Tagging Using Copying Mechanism",
    "doi": "https://doi.org/10.1145/3178458",
    "publication_date": "2018-02-14",
    "publication_year": 2018,
    "authors": "Sangkeun Jung; Changki Lee; Hyunsun Hwang",
    "corresponding_authors": "",
    "abstract": "In this article, we introduce a novel neural architecture for the end-to-end Korean Part-of-Speech (POS) tagging problem. To address the problem, we extend the present recurrent neural network-based sequence-to-sequence models to deal with the key challenges in this task: rare word generation and POS tagging. To overcome these issues, Input-Feeding and Copying mechanism are adopted. Although our approach does not require any manual features or preprocessed pattern matching dictionaries, our best single model achieves an F-score of 97.08. This is competitive with the current state-of-the-art model (F-score 98.03), which requires extensive manual feature processing.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2786669038",
    "type": "article"
  },
  {
    "title": "Keyword Extraction from Arabic Documents using Term Equivalence Classes",
    "doi": "https://doi.org/10.1145/2665077",
    "publication_date": "2015-04-20",
    "publication_year": 2015,
    "authors": "Arafat Awajan",
    "corresponding_authors": "Arafat Awajan",
    "abstract": "The rapid growth of the Internet and other computing facilities in recent years has resulted in the creation of a large amount of text in electronic form, which has increased the interest in and importance of different automatic text processing applications, including keyword extraction and term indexing. Although keywords are very useful for many applications, most documents available online are not provided with keywords. We describe a method for extracting keywords from Arabic documents. This method identifies the keywords by combining linguistics and statistical analysis of the text without using prior knowledge from its domain or information from any related corpus. The text is preprocessed to extract the main linguistic information, such as the roots and morphological patterns of derivative words. A cleaning phase is then applied to eliminate the meaningless words from the text. The most frequent terms are clustered into equivalence classes in which the derivative words generated from the same root and the non-derivative words generated from the same stem are placed together, and their count is accumulated. A vector space model is then used to capture the most frequent N-gram in the text. Experiments carried out using a real-world dataset show that the proposed method achieves good results with an average precision of 31% and average recall of 53% when tested against manually assigned keywords.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2202210819",
    "type": "article"
  },
  {
    "title": "Low-Resource Machine Transliteration Using Recurrent Neural Networks",
    "doi": "https://doi.org/10.1145/3265752",
    "publication_date": "2019-01-16",
    "publication_year": 2019,
    "authors": "Ngoc Tan Le; Fatiha Sadat; Lucie Ménard; Điền Đinh",
    "corresponding_authors": "",
    "abstract": "Grapheme-to-phoneme models are key components in automatic speech recognition and text-to-speech systems. With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful. These models are based on initial alignments between grapheme source and phoneme target sequences. Inspired by sequence-to-sequence recurrent neural network--based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pretrained source and target embeddings to overcome the transliteration problem for a low-resource languages pair. Evaluation and experiments involving French and Vietnamese showed that with only a small bilingual pronunciation dictionary available for training the transliteration models, promising results were obtained with a large increase in BLEU scores and a reduction in Translation Error Rate (TER) and Phoneme Error Rate (PER). Moreover, we compared our proposed neural network--based transliteration approach with a statistical one.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2910606374",
    "type": "article"
  },
  {
    "title": "Unsupervised Joint PoS Tagging and Stemming for Agglutinative Languages",
    "doi": "https://doi.org/10.1145/3292398",
    "publication_date": "2019-01-25",
    "publication_year": 2019,
    "authors": "Necva Bölücü; Burcu Can",
    "corresponding_authors": "",
    "abstract": "The number of possible word forms is theoretically infinite in agglutinative languages. This brings up the out-of-vocabulary (OOV) issue for part-of-speech (PoS) tagging in agglutinative languages. Since inflectional morphology does not change the PoS tag of a word, we propose to learn stems along with PoS tags simultaneously. Therefore, we aim to overcome the sparsity problem by reducing word forms into their stems. We adopt a Bayesian model that is fully unsupervised. We build a Hidden Markov Model for PoS tagging where the stems are emitted through hidden states. Several versions of the model are introduced in order to observe the effects of different dependencies throughout the corpus, such as the dependency between stems and PoS tags or between PoS tags and affixes. Additionally, we use neural word embeddings to estimate the semantic similarity between the word form and stem. We use the semantic similarity as prior information to discover the actual stem of a word since inflection does not change the meaning of a word. We compare our models with other unsupervised stemming and PoS tagging models on Turkish, Hungarian, Finnish, Basque, and English. The results show that a joint model for PoS tagging and stemming improves on an independent PoS tagger and stemmer in agglutinative languages.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2914324790",
    "type": "article"
  },
  {
    "title": "Multi-Channel Embedding Convolutional Neural Network Model for Arabic Sentiment Classification",
    "doi": "https://doi.org/10.1145/3314941",
    "publication_date": "2019-05-21",
    "publication_year": 2019,
    "authors": "Abdelghani Dahou; Shengwu Xiong; Junwei Zhou; Mohamed Abd Elaziz",
    "corresponding_authors": "",
    "abstract": "With the advent of social network services, Arabs’ opinions on the web have attracted many researchers in recent years toward detecting and classifying sentiments in Arabic tweets and reviews. However, the impact of word embeddings vectors (WEVs) initialization and dataset balance on Arabic sentiment classification using deep learning has not been thoroughly studied. In this article, a multi-channel embedding convolutional neural network (MCE-CNN) is proposed to improve Arabic sentiment classification by learning sentiment features from different text domains, word, and character n-grams levels. MCE-CNN encodes a combination of different pre-trained word embeddings into the embedding block at each embedding channel and trains these channels in parallel. Besides, a separate feature extraction module implemented in a CNN block is used to extract more relevant sentiment features. These channels and blocks help to start training on high-quality WEVs and fine-tuning them. The performance of MCE-CNN is evaluated on several standard balanced and imbalanced datasets to reflect real-world use cases. Experimental results show that MCE-CNN provides a high classification accuracy and benefits from the second embedding channel on both standard Arabic and dialectal Arabic text, which outperforms state-of-the-art methods.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2945847947",
    "type": "article"
  },
  {
    "title": "Diacritic-Based Matching of Arabic Words",
    "doi": "https://doi.org/10.1145/3242177",
    "publication_date": "2018-12-14",
    "publication_year": 2018,
    "authors": "Mustafa Jarrar; Fadi A. Zaraket; Rami Asia; Hamzeh Amayreh",
    "corresponding_authors": "",
    "abstract": "Words in Arabic consist of letters and short vowel symbols called diacritics inscribed atop regular letters. Changing diacritics may change the syntax and semantics of a word; turning it into another. This results in difficulties when comparing words based solely on string matching. Typically, Arabic NLP applications resort to morphological analysis to battle ambiguity originating from this and other challenges. In this article, we introduce three alternative algorithms to compare two words with possibly different diacritics. We propose the Subsume knowledge-based algorithm, the Imply rule-based algorithm, and the Alike machine-learning-based algorithm. We evaluated the soundness, completeness, and accuracy of the algorithms against a large dataset of 86,886 word pairs. Our evaluation shows that the accuracy of Subsume (100%), Imply (99.32%), and Alike (99.53%). Although accurate, Subsume was able to judge only 75% of the data. Both Subsume and Imply are sound, while Alike is not. We demonstrate the utility of the algorithms using a real-life use case -- in lemma disambiguation and in linking hundreds of Arabic dictionaries.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2997186977",
    "type": "article"
  },
  {
    "title": "A Constraint Approach to Pivot-Based Bilingual Dictionary Induction",
    "doi": "https://doi.org/10.1145/2723144",
    "publication_date": "2015-11-21",
    "publication_year": 2015,
    "authors": "Mairidan Wushouer; Donghui Lin; Toru Ishida; Katsutoshi Hirayama",
    "corresponding_authors": "",
    "abstract": "High-quality bilingual dictionaries are very useful, but such resources are rarely available for lower-density language pairs, especially for those that are closely related. Using a third language to link two other languages is a well-known solution and usually requires only two input bilingual dictionaries A-B and B-C to automatically induce the new one, A-C . This approach, however, has never been demonstrated to utilize the complete structures of the input bilingual dictionaries, and this is a key failing because the dropped meanings negatively influence the result. This article proposes a constraint approach to pivot-based dictionary induction where language A and C are closely related. We create constraints from language similarity and model the structures of the input dictionaries as a Boolean optimization problem, which is then formulated within the Weighted Partial Max-SAT framework, an extension of Boolean Satisfiability (SAT). All of the encoded CNF (Conjunctive Normal Form), the predominant input language of modern SAT/MAX-SAT solvers, formulas are evaluated by a solver to produce the target (output) bilingual dictionary. Moreover, we discuss alternative formalizations as a comparison study. We designed a tool that uses the Sat4j library as the default solver to implement our method and conducted an experiment in which the output bilingual dictionary achieved better quality than the baseline method.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2243775030",
    "type": "article"
  },
  {
    "title": "Word Segmentation for Burmese (Myanmar)",
    "doi": "https://doi.org/10.1145/2846095",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "Chenchen Ding; Ye Kyaw Thu; Masao Utiyama; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "Experiments on various word segmentation approaches for the Burmese language are conducted and discussed in this note. Specifically, dictionary-based, statistical, and machine learning approaches are tested. Experimental results demonstrate that statistical and machine learning approaches perform significantly better than dictionary-based approaches. We believe that this note, based on an annotated corpus of relatively considerable size (containing approximately a half million words), is the first systematic comparison of word segmentation approaches for Burmese. This work aims to discover the properties and proper approaches to Burmese textual processing and to promote further researches on this understudied language.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2398978578",
    "type": "article"
  },
  {
    "title": "Arabic Authorship Attribution",
    "doi": "https://doi.org/10.1145/3236391",
    "publication_date": "2018-11-12",
    "publication_year": 2018,
    "authors": "Malik H. Altakrori; Farkhund Iqbal; Benjamin C. M. Fung; Steven H. H. Ding; Abdallah Tubaishat",
    "corresponding_authors": "",
    "abstract": "Law enforcement faces problems in tracing the true identity of offenders in cybercrime investigations. Most offenders mask their true identity, impersonate people of high authority, or use identity deception and obfuscation tactics to avoid detection and traceability. To address the problem of anonymity, authorship analysis is used to identify individuals by their writing styles without knowing their actual identities. Most authorship studies are dedicated to English due to its widespread use over the Internet, but recent cyber-attacks such as the distribution of Stuxnet indicate that Internet crimes are not limited to a certain community, language, culture, ideology, or ethnicity. To effectively investigate cybercrime and to address the problem of anonymity in online communication, there is a pressing need to study authorship analysis of languages such as Arabic, Chinese, Turkish, and so on. Arabic, the focus of this study, is the fourth most widely used language on the Internet. This study investigates authorship of Arabic discourse/text, especially tiny text, Twitter posts. We benchmark the performance of a profile-based approach that uses n -grams as features and compare it with state-of-the-art instance-based classification techniques. Then we adapt an event-visualization tool that is developed for English to accommodate both Arabic and English languages and visualize the result of the attribution evidence. In addition, we investigate the relative effect of the training set, the length of tweets, and the number of authors on authorship classification accuracy. Finally, we show that diacritics have an insignificant effect on the attribution process and part-of-speech tags are less effective than character-level and word-level n -grams.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2901907079",
    "type": "article"
  },
  {
    "title": "Improving NER Tagging Performance in Low-Resource Languages via Multilingual Learning",
    "doi": "https://doi.org/10.1145/3238797",
    "publication_date": "2018-12-14",
    "publication_year": 2018,
    "authors": "Rudra Murthy; Mitesh M. Khapra; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Existing supervised solutions for Named Entity Recognition (NER) typically rely on a large annotated corpus. Collecting large amounts of NER annotated corpus is time-consuming and requires considerable human effort. However, collecting small amounts of annotated corpus for any language is feasible, but the performance degrades due to data sparsity. We address the data sparsity by borrowing features from the data of a closely related language. We use hierarchical neural networks to train a supervised NER system. The feature borrowing from a closely related language happens via the shared layers of the network. The neural network is trained on the combined dataset of the low-resource language and a closely related language, also termed Multilingual Learning. Unlike existing systems, we share all layers of the network between the two languages. We apply multilingual learning for NER in Indian languages and empirically show the benefits over a monolingual deep learning system and a traditional machine-learning system with some feature engineering. Using multilingual learning, we show that the low-resource language NER performance increases mainly due to (1) increased named entity vocabulary, (2) cross-lingual subword features, and (3) multilingual learning playing the role of regularization.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2903619933",
    "type": "article"
  },
  {
    "title": "<i>AyaTEC</i>",
    "doi": "https://doi.org/10.1145/3400396",
    "publication_date": "2020-10-02",
    "publication_year": 2020,
    "authors": "Rana Malhas; Tamer Elsayed",
    "corresponding_authors": "",
    "abstract": "The absence of publicly available reusable test collections for Arabic question answering on the Holy Qur’an has impeded the possibility of fairly comparing the performance of systems in that domain. In this article, we introduce AyaTEC , a reusable test collection for verse-based question answering on the Holy Qur’an, which serves as a common experimental testbed for this task. AyaTEC includes 207 questions (with their corresponding 1,762 answers) covering 11 topic categories of the Holy Qur’an that target the information needs of both curious and skeptical users. To the best of our effort, the answers to the questions (each represented as a sequence of verses) in AyaTEC were exhaustive—that is, all qur’anic verses that directly answered the questions were exhaustively extracted and annotated. To facilitate the use of AyaTEC in evaluating the systems designed for that task, we propose several evaluation measures to support the different types of questions and the nature of verse-based answers while integrating the concept of partial matching of answers in the evaluation.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3090081112",
    "type": "article"
  },
  {
    "title": "Native Language Identification of Fluent and Advanced Non-Native Writers",
    "doi": "https://doi.org/10.1145/3383202",
    "publication_date": "2020-04-11",
    "publication_year": 2020,
    "authors": "Raheem Sarwar; Attapol Rutherford; Saeed‐Ul Hassan; Thanawin Rakthanmanon; Sarana Nutanong",
    "corresponding_authors": "",
    "abstract": "Native Language Identification (NLI) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language. Most existing studies investigate this task for educational applications such as second language acquisition and require the learner corpora. This article performs NLI in a challenging context of the user-generated-content (UGC) where authors are fluent and advanced non-native speakers of a second language. Existing NLI studies with UGC (i) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets, (ii) are unable to capture the variations of the language-usage-patterns within a text sample, and (iii) are not associated with any outlier handling mechanism. Moreover, since there is a sizable number of people who have acquired non-English second languages due to the economic and immigration policies, there is a need to gauge the applicability of NLI with UGC to other languages. Unlike existing solutions, we define a topic-independent feature space, which makes our solution generalizable to other domains and datasets. Based on our feature space, we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample. Specifically, we represent each text sample as a point set and identify the top- k stylistically similar text samples (SSTs) from the corpus. We then apply the probabilistic k nearest neighbors’ classifier on the identified top- k SSTs to predict the native languages of the authors. To conduct experiments, we create three new corpora where each corpus is written in a different language, namely, English, French , and German . Our experimental studies show that our solution outperforms competitive methods and reports more than 80% accuracy across languages.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3015227914",
    "type": "article"
  },
  {
    "title": "Named Entity Recognition and Classification for Punjabi Shahmukhi",
    "doi": "https://doi.org/10.1145/3383306",
    "publication_date": "2020-04-17",
    "publication_year": 2020,
    "authors": "Muhammad Tayyab Ahmad; Muhammad Kamran Malik; Khurram Shahzad; Faisal Aslam; Asif Iqbal; Zubair Nawaz; Faisal Bukhari",
    "corresponding_authors": "",
    "abstract": "Named entity recognition (NER) refers to the identification of proper nouns from natural language text and classifying them into named entity types, such as person, location, and organization. Due to the widespread applications of NER, numerous NER techniques and benchmark datasets have been developed for both Western and Asian languages. Even though Shahmukhi script of the Punjabi language has been used by nearly three fourths of the Punjabi speakers worldwide, Gurmukhi has been the main focus of research activities. Specifically, a benchmark NER corpus for Shahmukhi is non-existent, which has thwarted the commencement of NER research for the Shahmukhi script. To this end, this article presents the development and specifications of the first-ever NER corpus for Shahmukhi. The newly developed corpus is composed of 318,275 tokens and 16,300 named entities, including 11,147 persons, 3,140 locations, and 2,013 organizations. To establish the strength of our corpus, we have compared the specifications of our corpus with its Gurmukhi counterparts. Furthermore, we have demonstrated the usability of our corpus using five supervised learning techniques, including two state-of-the-art deep learning techniques. The results are compared, and valuable insights about the behaviors of the most effective technique are discussed.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3017222382",
    "type": "article"
  },
  {
    "title": "Chinese Event Extraction via Graph Attention Network",
    "doi": "https://doi.org/10.1145/3494533",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Xiaohua Wu; Tengrui Wang; Youping Fan; Fangjian Yu",
    "corresponding_authors": "",
    "abstract": "Event extraction plays an important role in natural language processing (NLP) applications, including question answering and information retrieval. Most of the previous state-of-the-art methods were lack of ability in capturing features in long range. Recent methods applied dependency tree via dependency-bridge and attention-based graph. However, most of the automatic processing tools used in those methods show poor performance on Chinese texts due to mismatching between word segmentation and labels, which results in error propagation. In this article, we propose a novel character-level C hinese e vent e xtraction framework via graph a ttention network (CAEE). We build our model upon the sequence labeling model, but enhance it with word information by incorporating the word lexicon into the character representations. We further exploit the inter-dependencies between event triggers and argument by building a word-character-based graph network via syntactic shortcut arcs with dependency-parsing. The architecture of the graph minimizes error propagation, which is the result of the error detection of the word boundaries in the processing of Chinese texts. To demonstrate the effectiveness of our work, we build a large-scale real-world corpus consisting of announcements of Chinese financial news without golden entities. Experiments on the corpus show that our approach achieves competitive results compared with previous work in the field of Chinese texts.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4206670423",
    "type": "article"
  },
  {
    "title": "Secure IoMT Pattern Recognition and Exploitation for Multimedia Information Processing using Private Blockchain and Fuzzy Logic",
    "doi": "https://doi.org/10.1145/3523283",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Taher M. Ghazal; Mohammad Kamrul Hasan; Siti Norul Huda Abdallah; Khairul Azmi Abubakkar",
    "corresponding_authors": "",
    "abstract": "The Internet of Medical Things (IoMT) is a definite IoT connecting atmosphere that contracts with communication via intelligent medical equipment. Activity detection, motion tracking, information extraction, consumer retrieval, etc., have all been solved due to advances in the autonomous study of human behavior from multimedia information processing. Though the IoT connecting atmosphere enables and provisions our daily actions, it too has certain disadvantages. IoTgrieves from numerous safety and confidentiality challenges, such as reiteration, impersonation, man-in-the-middle, remote hijacking, privileged-insider attack, denial of service (DoS) attacks, password guessing, and malware bouts. Hence, in this paper, Private Blockchain and Fuzzy Logic based Attack Detection system (PBFL-ADS) has been proposed for secure IoMT disease prediction using Multimedia Information Processing techniques. The proposed method utilizes Bayesian inference-based trust administration to perceived malevolent nodes in Health Smartphone Structures (HSS) for PBFL-ADS. This paper focuses on the specific form of IoMT called HSS because smartphones have been widely used in the healthcare profession. Then, blockchains have been utilized to improve Bayesian trust management's efficacy in detecting hostile nodes in HSSs. The effectiveness of the suggested technique has been assessed, and test results show that blockchain technology helps identify fraudulent nodes with an acceptable workload. The proposed PBFL-ADS method increases the HS2 scenario 1.1, processor utilization at nodes 33.5%, pattern recognition ratio 92.1%.and server utilization 40.1%.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4221107245",
    "type": "article"
  },
  {
    "title": "QEST: Quantized and Efficient Scene Text Detector Using Deep Learning",
    "doi": "https://doi.org/10.1145/3526217",
    "publication_date": "2022-03-26",
    "publication_year": 2022,
    "authors": "Kanak Manjari; Madhushi Verma; Gaurav Singal; Suyel Namasudra",
    "corresponding_authors": "",
    "abstract": "Scene text detection is complicated and one of the most challenging tasks due to different environmental restrictions, such as illuminations, lighting conditions, tiny and curved texts, and many more. Most of the works on scene text detection have overlooked the primary goal of increasing model accuracy and efficiency, resulting in heavy-weight models that require more processing resources. A novel lightweight model has been developed in this article to improve the accuracy and efficiency of scene text detection. The proposed model relies on ResNet50 and MobileNetV2 as backbones with quantization used to make the resulting model lightweight. During quantization, the precision has been changed from float32 to float16 and int8 for making the model lightweight. In terms of inference time and Floating-Point Operations Per Second, the proposed method outperforms the state-of-the-art techniques by around 30–100 times. Here, well-known datasets, i.e., ICDAR2015 and ICDAR2019, have been utilized for training and testing to validate the performance of the proposed model. Finally, the findings and discussion indicate that the proposed model is more efficient than the existing schemes.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4221126462",
    "type": "article"
  },
  {
    "title": "Multi-level Taxonomy Review for Sign Language Recognition: Emphasis on Indian Sign Language",
    "doi": "https://doi.org/10.1145/3530259",
    "publication_date": "2022-09-05",
    "publication_year": 2022,
    "authors": "Nimratveer Kaur Bahia; Rajneesh Rani",
    "corresponding_authors": "",
    "abstract": "With the phenomenal increase in image and video databases, there is an increase in the human-computer interaction that recognizes Sign Language. Exchanging information using different gestures between two people is sign language, known as non-verbal communication. Sign language recognition is already done in various languages; however, for Indian Sign Language, there is no adequate amount of work done. This article presents a review on sign language recognition for multiple languages. Data acquisition methods have been over-viewed in four ways (a) Glove-based, (b) Kinect-based, (c) Leap motion controller, and (d) Vision-based. Some of them have pros and cons that have also been discussed for every data acquisition method. Applications of sign language recognition are also discussed. Furthermore, this review also creates a coherent taxonomy to represent the modern research divided into three levels: Level 1 Elementary level (Recognition of sign characters), Level 2 Advanced level (Recognition of sign words), and Level 3 Professional level (Sentence interpretation). The available challenges and issues for each level are also explored in this research to provide valuable perceptions into technological environments. Various publicly available datasets for different sign languages are also discussed. An efficient review of this article shows that the significant exploration of communication via sign acknowledgment has been performed on static, dynamic, isolated, and continuous gestures using various acquisition methods. Comprehensively, the hope is that this study will enable readers to learn new pathways and gain knowledge to carry out further research work in the domain related to sign language recognition.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4294599160",
    "type": "article"
  },
  {
    "title": "A Framework for Online Hate Speech Detection on Code-mixed Hindi-English Text and Hindi Text in Devanagari",
    "doi": "https://doi.org/10.1145/3568673",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Abhishek Chopra; Deepak Kumar Sharma; Aashna Jha; Uttam Ghosh",
    "corresponding_authors": "",
    "abstract": "Social Media has been growing and has provided the world with a platform to opine, debate, display, and discuss like never before. It has a major influence in research areas that analyze human behavior and social groups, and the phenomenon of social interactions is even being used in areas such as Internet of Things. This constant stream of data connecting individuals and organizations across the globe has had a tremendous impact on the functioning of society and even has the power to sway elections. Despite having numerous benefits, social media has certain issues such as the prevalence of fake news, which has also led to the rise of the hate speech phenomenon. Due to lax security throughout these social media platforms, these issues continue to exist without any repercussions. This leads to cyberbullying, defamation, and presents grave security concerns. Even though some work has been done independently on native scripts, hate speech detection, and code-mixed data, there exists a lack of academic work and research in the area of detecting hate speech in transliterated code-mixed data and in-text containing native language scripts. Research in this field is inhibited greatly due to the multiple variations in grammar and spelling and in general a lack of availability of annotated datasets, especially when it comes to native languages. This article comes up with a method to automate hate speech detection in code-mixed and native language text. The article presents an architecture containing a Tabnet classifier-based model trained on features extracted using MuRIL from transliterated code-mixed textual data. The article also shows that the same model works well on features extracted from text in Devanagari despite being trained on transliterated data.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4306873631",
    "type": "article"
  },
  {
    "title": "A Transformer Based Approach for Abuse Detection in Code Mixed Indic Languages.",
    "doi": "https://doi.org/10.1145/3571818",
    "publication_date": "2022-11-23",
    "publication_year": 2022,
    "authors": "Vibhuti Bansal; Mrinal Tyagi; Rajesh Sharma; Vedika Gupta; Qin Xin",
    "corresponding_authors": "",
    "abstract": "The advancement in the number of online social media platforms has entailed active participation from the web users globally. This has also lead to subsequent increase in the cyberbullying cases online. Such incidents diminish an individual’s reputation or defame a community, also posing a threat to the privacy of users in cyberspace. Traditionally, manual checks and handling mechanisms have been used to deal with such textual content. However, an automatic computer-based approach would provide far better solutions to this problem. Existing approaches to automate this task majorly involves classical machine learning models which tend to perform poorly on low resource languages. Owing to the varied background and language of web users, the cyberspace witnesses the presence of multilingual text. An integrated approach to accommodate multilingual text could be the appropriate solution. This paper explores various methods to detect abusive content in 13 Indic code-mixed languages. Firstly, baseline classical machine learning models are compared with Transformer based architecture. Secondly, the paper presents the experimental analysis of four state-of-the-art transformer-based models vis à vis XLM-RoBERTa, indic-BERT, MurilBert and mBERT, out of which XLM Roberta with BiGRU outperforms. Thirdly, the experimental setup of the best performing model XLM-RoBERTa is fed with emoji embeddings that leads to further enhancement of overall performance of the employed model. Finally, the model is trained with the combined dataset of 13 Indic languages, to compare its performance with those of individual language models. The performance of combined model surpassed those of the individual models in terms of F1 score and accuracy, supporting the fact that combined model fits the data better possibly due to its code-mixed nature. This model reports a F1 score of 0.88 on test data while rendering a training loss of 0.28, validation loss of 0.31 and an AUC score of 0.94 for both training and validation.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4309764780",
    "type": "article"
  },
  {
    "title": "Dynamic Convolution-based Encoder-Decoder Framework for Image Captioning in Hindi",
    "doi": "https://doi.org/10.1145/3573891",
    "publication_date": "2022-12-26",
    "publication_year": 2022,
    "authors": "Santosh Kumar Mishra; Sushant Sinha; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "In sequence-to-sequence modeling tasks, such as image captioning, machine translation, and visual question answering, encoder-decoder architectures are state of the art. An encoder, convolutional neural network (CNN) encodes input images into fixed dimensional vector representation in the image captioning task, whereas a decoder, a recurrent neural network, performs language modeling and generates the target descriptions. Recent CNNs use the same operation over every pixel; however, all the image pixels are not equally important. To address this, the proposed method uses a dynamic convolution-based encoder for image encoding or feature extraction, Long-Short-Term-Memory as a decoder for language modeling, and X-Linear attention to make the system robust. Encoders, attentions, and decoders are important aspects of the image captioning task; therefore, we experiment with various encoders, decoders, and attention mechanisms. Most of the works for image captioning have been carried out for the English language in the existing literature. We propose a novel approach for caption generation from images in Hindi. Hindi, widely spoken in South Asia and India, is the fourth most-spoken language globally; it is India’s official language. The proposed method utilizes dynamic convolution operation on the encoder side to obtain a better image encoding quality. The Hindi image captioning dataset is manually created by translating the popular MSCOCO dataset from English to Hindi. In terms of BLEU scores, the performance of the proposed method is compared with other baselines, and the results obtained show that the proposed method outperforms different baselines. Manual human assessment in terms of adequacy and fluency of the captions generated further determines the efficacy of the proposed method in generating good-quality captions.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4312219151",
    "type": "article"
  },
  {
    "title": "DEAF-BSL: Deep lEArning Framework for British Sign Language recognition",
    "doi": "https://doi.org/10.1145/3513004",
    "publication_date": "2022-06-17",
    "publication_year": 2022,
    "authors": "Krishan Kumar",
    "corresponding_authors": "Krishan Kumar",
    "abstract": "The recent development of disability studies in academic bodies has expedited the promotion of investigation on disability. With computer-aided tools, communication between the impaired person and someone who does not understand sign language could be accessible. A large number of people across the world are using sign language (e.g., British Sign Language (BSL) , Asian Sign Language (ASL) , Indian Sign Language (ISL) , etc.) with hand gestures for communication. In BSL recognition, the involvement of both hands overlapping each other becomes the main challenge. Moreover, BSL comprises ambiguous signs concerning viewpoint. However, existing traditional techniques seem in-stable, less accurate, and inefficient. In this work, the BSL fingerspelling alphabet recognition problem explores using a Deep learning framework to address the above-mentioned concerns. Convolutional Neural Network (CNN) is employed to detect and recognize for classification of 26 alphabets after being trained on the BSL corpus dataset. The proposed work outperforms the existing works with better precision (6%), recall (4%), and F-measure (5̃%). It reported better results on the BSL corpus dataset and webcam videos. The model achieved better accuracy (98.0%) for a large lexicon of words than previous models (Goh &amp; Holden [ 6 ]: 69.5%, Rambhau [ 9 ]: 79.2%, and Liwicki et al. [ 8 ]: 92.5%). The 3D CNN-based proposal performs robust hand detection, much more accurate sign recognition, more scalability, and less ambiguity in BSL finger-spelling recognition.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4283019958",
    "type": "article"
  },
  {
    "title": "General and Domain-adaptive Chinese Spelling Check with Error-consistent Pretraining",
    "doi": "https://doi.org/10.1145/3564271",
    "publication_date": "2022-09-21",
    "publication_year": 2022,
    "authors": "Qi Lv; Ziqiang Cao; Lei Geng; Chunhui Ai; Xu Yan; Guohong Fu",
    "corresponding_authors": "",
    "abstract": "The lack of label data is one of the significant bottlenecks for Chinese Spelling Check (CSC). Existing researches use the method of automatic generation by exploiting unlabeled data to expand the supervised corpus. However, there is a big gap between the real input scenario and automatic generated corpus. Thus, we develop a competitive general speller ECSpell which adopts the Error Consistent masking strategy to create data for pretraining. This error consistency masking strategy is used to specify the error types of automatically generated sentences which is consistent with real scene. The experimental result indicates our model outperforms previous state-of-the-art models on the general benchmark. Moreover, spellers often work within a particular domain in real life. Due to lots of uncommon domain terms, experiments on our built domain specific datasets show that general models perform terribly. Inspired by the common practice of input methods, we propose to add an alterable user dictionary to handle the zero-shot domain adaption problem. Specifically, we attach a User Dictionary guided inference module (UD) to a general token classification based speller. Our experiments demonstrate that ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other baselines largely, even approaching the performance on the general benchmark.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4296712373",
    "type": "article"
  },
  {
    "title": "State of the Art of Automation in Sign Language: A Systematic Review",
    "doi": "https://doi.org/10.1145/3564769",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Rakesh Kumar Attar; Vishal Goyal; Lalit Goyal",
    "corresponding_authors": "",
    "abstract": "Sign language is the fundamental communication language of deaf people. Efforts to develop sign language generation systems can make the life of these people smooth and effortless. Despite the importance of sign language generation systems, there is a paucity of a systematic literature review. This is the foremost recognizable scholastic literature review of sign language generation systems. It presents a scholastic database of the literature between 1998 and 2020 and suggests classification criteria to systematize research studies. Four hundred fourteen research studies were recognized and reviewed for their direct pertinence to sign language generation systems. One hundred sixty-two research studies were subsequently chosen, examined, and classified. Each of the 162 chosen research papers was categorized based on 30 sign languages and was further comparatively analyzed based on seven comparison parameters (input form, translation technologies, application domain, use of parsers/grammars, manual/non-manual features, accuracy, and output form). It is evident from our research findings that the majority of research on sign language generation was carried out using data-driven approaches in the absence of proper grammar rules and generated only manual signs. This research study may provide researchers a roadmap toward future research directions and facilitate the compilation of information in the field of sign language generation.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4297999603",
    "type": "review"
  },
  {
    "title": "Can You Understand Why I Am Crying? A Decision-making System for Classifying Infants’ Cry Languages Based on DeepSVM Model",
    "doi": "https://doi.org/10.1145/3579032",
    "publication_date": "2023-01-23",
    "publication_year": 2023,
    "authors": "Khosro Rezaee; Hossein Ghayoumi Zadeh; Lianyong Qi; Hamidreza Rabiee; Mohammad R. Khosravi",
    "corresponding_authors": "",
    "abstract": "Scientific and therapeutic advances in perinatology and neonatology have improved the survival prospects of preterm and extremely-low-birth-weight infants. Infants’ cries are a valuable noninvasive tool for monitoring their neurologic health, especially if they are premature. Automatic acoustic analysis and data mining are employed in this study to determine the discriminative features of preterm and full-term infant cries. The use of machine learning for recognizing sounds in a newborn's cry language has received less attention than previous methods for analyzing the sounds. Moreover, to extract appropriate features from infant cries, adequate knowledge and appropriate signal descriptors are required. Accordingly, to analyze infant cry language, we propose an approach that uses fractal descriptors to extract discriminant features from spectrograms of windowed signals, followed by iterative neighborhood component analysis (iNCA) to select appropriate features. Additionally, the improved deep support vector machine (DeepSVM) is used to classify the infants’ crying types and their meanings. The proposed method is verified using a newborn sound dataset. According to the classification of five types of crying perception based on various characteristics, 98.34% of all crying perceptions have been recognized. Although there are many classes examined, the feature extraction method based on the fractal method and our optimal classification have a much higher diagnostic accuracy compared with similar methods for analyzing baby crying language. The proposed method can overcome many problems associated with analyzing babies’ crying sounds and understanding their language, such as uncertainty and unusual errors in classification.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4317816596",
    "type": "article"
  },
  {
    "title": "Short Text Classification of Chinese with Label Information Assisting",
    "doi": "https://doi.org/10.1145/3582301",
    "publication_date": "2023-02-08",
    "publication_year": 2023,
    "authors": "Qianqian Xu; Junjie Peng; Cangzhi Zheng; Shuhua Tan; Fen Yi; Feng Cheng",
    "corresponding_authors": "",
    "abstract": "As a common language form in oral communication, short text is hard to be used in the applications such as intent understanding, text classification and so on due to its limited content and information, as well as irregular expression and missing components. To increase the availability of short texts in real applications, we propose a Label Information Assisting-based Model (LIAM) for Chinese short text classification. In the model, we jointly use sentence-level features and word-level features to reduce text information loss. And the sentence-level features are fused with relevant label information by the Label Information Extending and Fusion (LIEF) module while the word-level features are also enhanced with assistance of relevant label information. By utilizing the text-related information from labels as extended information, the model enriches and enhances the features of short text, benefiting classification. To verify the correctness and effectiveness of the proposed method, we conduct extensive experiments on four Chinese datasets and six sub-datasets with different models. The experimental results show that LIAM presented can effectively enrich information for text and much improve the performance of short text classification. It performs much better than other methods do. What is more, the less the training set, the greater the advantages of the model.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4319594527",
    "type": "article"
  },
  {
    "title": "Urdu Short Paraphrase Detection at Sentence Level",
    "doi": "https://doi.org/10.1145/3586009",
    "publication_date": "2023-03-05",
    "publication_year": 2023,
    "authors": "Hamza Hafeez; Iqra Muneer; Muhammad Sharjeel; Muhammad Adnan Ashraf; Rao Muhammad Adeel Nawab",
    "corresponding_authors": "",
    "abstract": "Paraphrase detection systems uncover the relationship between two text fragments and classify them as paraphrased when they convey the same idea; otherwise non-paraphrased. Previously, the researchers have mainly focused on developing resources for the English language for paraphrase detection. There have been very few efforts for paraphrase detection in South Asian languages. However, no research has been conducted on sentence-level paraphrase detection in Urdu, a low-resourced language. It is mainly due to the unavailability of the corpora that focus on the sentence level. The available related studies on the Urdu language only focus on text reuse detection tasks at the passage and document levels. Therefore, this study aims to develop a large-scale manually annotated benchmark Urdu paraphrase detection corpus at the sentence level, based on real cases from journalism. The proposed Urdu Sentential Paraphrases (USP) corpus contains 4,900 sentences (2,941 paraphrased and 1,959 non-paraphrased), manually collected from the Urdu newspapers. Moreover, several techniques were proposed, developed, and compared as a secondary contribution, including Word Embedding (WE), Sentence Transformers (ST), and feature-fusion techniques. N-gram is treated as the baseline technique for our research. The experimental results indicate that our proposed feature-fusion technique is the most suitable for the Urdu paraphrase detection task. Furthermore, the performance increases when features of the proposed (ST) and baseline (N-gram) are combined for the classification task. In addition, The proposed techniques have also been applied to the UPPC corpus to check their performance at the document level. The best result we obtained using the feature fusion technique ( F 1 = 0.855). Our corpus is available and free to download for research purposes.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4323262316",
    "type": "article"
  },
  {
    "title": "Vietnamese Sentiment Analysis: An Overview and Comparative Study of Fine-tuning Pretrained Language Models",
    "doi": "https://doi.org/10.1145/3589131",
    "publication_date": "2023-04-04",
    "publication_year": 2023,
    "authors": "Dang Van Thin; Duong Ngoc Hao; Ngan Luu-Thuy Nguyen",
    "corresponding_authors": "",
    "abstract": "Sentiment Analysis (SA) is one of the most active research areas in the Natural Language Processing (NLP) field due to its potential for business and society. With the development of language representation models, numerous methods have shown promising efficiency in fine-tuning pre-trained language models in NLP downstream tasks. For Vietnamese, many available pre-trained language models were also released, including the monolingual and multilingual language models. Unfortunately, all of these models were trained on different architectures, pre-trained data, and pre-processing steps; consequently, fine-tuning these models can be expected to yield different effectiveness. In addition, there is no study focusing on evaluating the performance of these models on the same datasets for the SA task up to now. This article presents a fine-tuning approach to investigate the performance of different pre-trained language models for the Vietnamese SA task. The experimental results show the superior performance of the monolingual PhoBERT model and ViT5 model in comparison with previous studies and provide new state-of-the-art performances on five benchmark Vietnamese SA datasets. To the best of our knowledge, our study is the first attempt to investigate the performance of fine-tuning Transformer-based models on five datasets with different domains and sizes for the Vietnamese SA task.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4362554255",
    "type": "article"
  },
  {
    "title": "Towards Enhanced Identification of Emotion from Resource-Constrained Language through a novel Multilingual BERT Approach",
    "doi": "https://doi.org/10.1145/3592794",
    "publication_date": "2023-04-19",
    "publication_year": 2023,
    "authors": "Nadia Ali; Abdallah Tubaishat; Feras Al‐Obeidat; Mohammad Shabaz; Muhammad Waqas; Zahid Halim; Imad Rida; Sajid Anwar",
    "corresponding_authors": "",
    "abstract": "Emotion identification from text has recently gained attention due to its versatile ability to analyze human-machine interaction. This work focuses on detecting emotions from textual data. Languages, like English, Chinese, and German are widely used for text classification, however, limited research is done on resource-poor oriental languages. Roman Urdu (RU) is a resource-constrained language extensively used across Asia. This work focuses on predicting emotions from RU text. For this, a dataset is collected from different social media domains and based on Paul Ekman's theory it is annotated with six basic emotions, i.e., happy, surprise, angry, sad, fear, and disgusting. Dense word embedding representations of different languages is adopted that utilize existing pre-trained models. BERT is additionally pre-trained and fine-tuned for the classification task. The proposed approach is compared with baseline machine learning and deep learning algorithms. Additionally, a comparison of the current work is also performed with different approaches for the same task. Based on the empirical evaluation, the proposed approach performs better than the existing state-of-the-art with an average accuracy of 91%.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4366407879",
    "type": "article"
  },
  {
    "title": "Methodical Systematic Review of Abstractive Summarization and Natural Language Processing Models for Biomedical Health Informatics: Approaches, Metrics and Challenges",
    "doi": "https://doi.org/10.1145/3600230",
    "publication_date": "2023-05-31",
    "publication_year": 2023,
    "authors": "Praveen Kumar Katwe; Aditya Khamparia; Deepak Gupta; Ashit Kumar Dutta",
    "corresponding_authors": "",
    "abstract": "Text summarization tasks are primarily very useful for decision support systems and provide a source for useful data for training of bots as they can reduce and retain the useful information from the large corpus. This review article is for studying the literature that already exists in context of abstractive summarization and application of NLP language models in biomedical and associated healthcare applications. In past decade with trends like bigdata, IOT, enormous amount of data is getting processed in all structured, unstructured and semi structured formats. This review provides a comprehensive literature survey in research trends for abstractive summarization, foundations of machine translation and evolution of language models. This review identifies the potential of language model to provide a possible methodology for improving the performance and accuracy of various tasks in summarization. Deep neural network-based language models have now been the widely accepted state of art for various abstractive summarization and there exists an enormous scope to improvise and tune the language models for domain specific use case. This study shows current systems lack in faithfulness to original content and control of degree of hallucination. This review also details on the evaluation criteria and need for automated metrics and attempts to provide guideline for evaluation for abstractive summarization for health informatics.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4378837867",
    "type": "article"
  },
  {
    "title": "Human Emotion Recognition Based on Machine Learning Algorithms with low Resource Environment",
    "doi": "https://doi.org/10.1145/3640340",
    "publication_date": "2024-01-26",
    "publication_year": 2024,
    "authors": "P. Asha; V. Hemamalini; Poongodaia.; N. Swapna; K. L. S. Soujanya; Vaishali Gaikwad",
    "corresponding_authors": "",
    "abstract": "It is difficult to discover significant audio elements and conduct systematic comparison analyses when trying to automatically detect emotions in speech. In situations when it is desirable to reduce memory and processing constraints, this research deals with emotion recognition. One way to achieve this is by reducing the amount of features. In this study, propose \"Active Feature Selection\" (AFS) method and compares it against different state-of-the-art techniques. According to the results, smaller subsets of features than the complete feature set can produce accuracy that is comparable to or better than the full feature set. The memory and processing requirements of an emotion identification system will be reduced, which can minimise the hurdles to using health monitoring technology. The results show by using 696 characteristics, the AFS technique for emobase yields a Unweighted average recall (UAR) of 75.8%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391262794",
    "type": "article"
  },
  {
    "title": "Enriching Urdu NER with BERT Embedding, Data Augmentation, and Hybrid Encoder-CNN Architecture",
    "doi": "https://doi.org/10.1145/3648362",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "Anil Ahmed; Degen Huang; Syed Yasser Arafat; Imran Hameed .",
    "corresponding_authors": "",
    "abstract": "Named Entity Recognition (NER) is an indispensable component of Natural Language Processing (NLP), which aims to identify and classify entities within text data. While Deep Learning (DL) models have excelled in NER for well-resourced languages such as English, Spanish, and Chinese, they face significant hurdles when dealing with low-resource languages such as Urdu. These challenges stem from the intricate linguistic characteristics of Urdu, including morphological diversity, a context-dependent lexicon, and the scarcity of training data. This study addresses these issues by focusing on Urdu Named Entity Recognition (U-NER) and introducing three key contributions. First, various pre-trained embedding methods are employed, encompassing Word2vec (W2V), GloVe, FastText, Bidirectional Encoder Representations from Transformers (BERT), and Embeddings from language models (ELMo). In particular, fine-tuning is performed on BERT BASE and ELMo using Urdu Wikipedia and news articles. Second, a novel generative Data Augmentation (DA) technique replaces Named Entities (NEs) with mask tokens, employing pre-trained masked language models to predict masked tokens, effectively expanding the training dataset. Finally, the study introduces a novel hybrid model combining a Transformer Encoder with a Convolutional Neural Network (CNN) to capture the intricate morphology of Urdu. These modules enable the model to handle polysemy, extract short- and long-range dependencies, and enhance learning capacity. Empirical experiments demonstrate that the proposed model, incorporating BERT embeddings and an innovative DA approach, attains the highest F1-score of 93.99%, highlighting its efficacy for the U-NER task.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391840625",
    "type": "article"
  },
  {
    "title": "UrduAspectNet: Fusing Transformers and Dual GCN for Urdu Aspect-Based Sentiment Detection",
    "doi": "https://doi.org/10.1145/3663367",
    "publication_date": "2024-05-04",
    "publication_year": 2024,
    "authors": "Kamran Aziz; Aizihaierjiang Yusufu; Jun Zhou; Donghong Ji; Muhammad Shahid Iqbal; Shijie Wang; Hassan Jalil Hadi; Zhengming Yuan",
    "corresponding_authors": "",
    "abstract": "Urdu, characterized by its intricate morphological structure and linguistic nuances, presents distinct challenges in computational sentiment analysis. Addressing these, we introduce ”UrduAspectNet” – a dedicated model tailored for Aspect-Based Sentiment Analysis (ABSA) in Urdu. Central to our approach is a rigorous preprocessing phase. Leveraging the Stanza library, we extract Part-of-Speech (POS) tags and lemmas, ensuring Urdu’s linguistic intricacies are aptly represented. To probe the effectiveness of different embeddings, we trained our model using both mBERT and XLM-R embeddings, comparing their performances to identify the most effective representation for Urdu ABSA. Recognizing the nuanced inter-relationships between words, especially in Urdu’s flexible syntactic constructs, our model incorporates a dual Graph Convolutional Network (GCN) layer.Addressing the challenge of the absence of a dedicated Urdu ABSA dataset, we curated our own, collecting over 4,603 news headlines from various domains, such as politics, entertainment, business, and sports. These headlines, sourced from diverse news platforms, not only identify prevalent aspects but also pinpoints their sentiment polarities, categorized as positive, negative, or neutral. Despite the inherent complexities of Urdu, such as its colloquial expressions and idioms, ”UrduAspectNet” showcases remarkable efficacy. Initial comparisons between mBERT and XLM-R embeddings integrated with dual GCN provide valuable insights into their respective strengths in the context of Urdu ABSA. With broad applications spanning media analytics, business insights, and socio-cultural analysis, ”UrduAspectNet” is positioned as a pivotal benchmark in Urdu ABSA research.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4396643107",
    "type": "article"
  },
  {
    "title": "CHUNAV: Analyzing Hindi Hate Speech and Targeted Groups in Indian Election Discourse",
    "doi": "https://doi.org/10.1145/3665245",
    "publication_date": "2024-05-16",
    "publication_year": 2024,
    "authors": "Farhan Ahmad Jafri; Kritesh Rauniyar; Surendrabikram Thapa; M. Siddiqui; Matloob Khushi; Usman Naseem",
    "corresponding_authors": "",
    "abstract": "In the ever-evolving landscape of online discourse and political dialogue, the rise of hate speech poses a significant challenge to maintaining a respectful and inclusive digital environment. The context becomes particularly complex when considering the Hindi language—a low-resource language with limited available data. To address this pressing concern, we introduce the CHUNAV dataset—a collection of 11,457 Hindi tweets gathered during assembly elections in various states. CHUNAV is purpose-built for hate speech categorization and the identification of target groups. The dataset is a valuable resource for exploring hate speech within the distinctive socio-political context of Indian elections. The tweets within CHUNAV have been meticulously categorized into “Hate” and “Non-Hate” labels, and further subdivided to pinpoint the specific targets of hate speech, including “Individual”, “Organization”, and “Community” labels (as shown in Figure 1). Furthermore, this paper presents multiple benchmark models for hate speech detection, along with an innovative ensemble and oversampling-based method. The paper also delves into the results of topic modeling, all aimed at effectively addressing hate speech and target identification in the Hindi language. This contribution seeks to advance the field of hate speech analysis and foster a safer and more inclusive online space within the distinctive realm of Indian Assembly Elections.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4396957210",
    "type": "article"
  },
  {
    "title": "GS2F: Multimodal Fake News Detection Utilizing Graph Structure and Guided Semantic Fusion",
    "doi": "https://doi.org/10.1145/3708536",
    "publication_date": "2024-12-16",
    "publication_year": 2024,
    "authors": "Dong Zhou; Q. Ouyang; Nankai Lin; Yongmei Zhou; Aimin Yang",
    "corresponding_authors": "",
    "abstract": "The prevalence of fake news online has become a significant societal concern. To combat this, multimodal detection techniques based on images and text have shown promise. Yet, these methods struggle to analyze complex relationships within and between modalities due to the diverse discriminative elements in the news content. In addition, research on multimodal and multi-class fake news detection remains insufficient. To address the above challenges, in this paper, we propose a novel detection model, GS 2 F, leveraging g raph s tructure and g uided s emantic f usion. Specifically, we construct a multimodal graph structure to align two modalities and employ graph contrastive learning for refined fusion representations. Furthermore, a guided semantic fusion module is introduced to maximize the utilization of single-modal information and a dynamic contribution assignment layer is designed to weigh the importance of image, text, and multimodal features. Experimental results on Fakeddit demonstrate that our model outperforms existing methods, marking a step forward in the multimodal and multi-class fake news detection.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4405452901",
    "type": "article"
  },
  {
    "title": "Graph-Based Bilingual Word Embedding for Statistical Machine Translation",
    "doi": "https://doi.org/10.1145/3203078",
    "publication_date": "2018-07-25",
    "publication_year": 2018,
    "authors": "Rui Wang; Hai Zhao; Sabine Ploux; Bao‐Liang Lu; Masao Utiyama; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "Most of the existing methods for bilingual word embedding only consider shallow context or simple co-occurrence information. In this paper, we propose a latent bilingual sense unit (Bilingual Sense Clique, BSC), which is derived from a maximum complete sub-graph of pointwise mutual information based graph over bilingual corpus. In this way, we treat source and target words equally and a separated bilingual projection processing that have to be used in most existing works is not necessary any more. Several dimension reduction methods are evaluated to summarize the BSC-word relationship. The proposed method is evaluated on bilingual lexicon translation tasks and empirical results show that bilingual sense embedding methods outperform existing bilingual word embedding methods.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3121416198",
    "type": "article"
  },
  {
    "title": "Multilingual Topic Models for Bilingual Dictionary Extraction",
    "doi": "https://doi.org/10.1145/2699939",
    "publication_date": "2015-06-12",
    "publication_year": 2015,
    "authors": "Xiaodong Liu; Kevin Duh; Yūji Matsumoto",
    "corresponding_authors": "",
    "abstract": "A machine-readable bilingual dictionary plays a crucial role in many natural language processing tasks, such as statistical machine translation and cross-language information retrieval. In this article, we propose a framework for extracting a bilingual dictionary from comparable corpora by exploiting a novel combination of topic modeling and word aligners such as the IBM models. Using a multilingual topic model, we first convert a comparable document -aligned corpus into a parallel topic -aligned corpus. This novel topic-aligned corpus is similar in structure to the sentence -aligned corpus frequently employed in statistical machine translation and allows us to extract a bilingual dictionary using a word alignment model. The main advantages of our framework is that (1) no seed dictionary is necessary for bootstrapping the process, and (2) multilingual comparable corpora in more than two languages can also be exploited. In our experiments on a large-scale Wikipedia dataset, we demonstrate that our approach can extract higher precision dictionaries compared to previous approaches and that our method improves further as we add more languages to the dataset.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2265955221",
    "type": "article"
  },
  {
    "title": "Translating Low-Resource Languages by Vocabulary Adaptation from Close Counterparts",
    "doi": "https://doi.org/10.1145/3099556",
    "publication_date": "2017-09-08",
    "publication_year": 2017,
    "authors": "Peyman Passban; Qun Liu; Andy Way",
    "corresponding_authors": "",
    "abstract": "Some natural languages belong to the same family or share similar syntactic and/or semantic regularities. This property persuades researchers to share computational models across languages and benefit from high-quality models to boost existing low-performance counterparts. In this article, we follow a similar idea, whereby we develop statistical and neural machine translation (MT) engines that are trained on one language pair but are used to translate another language. First we train a reliable model for a high-resource language, and then we exploit cross-lingual similarities and adapt the model to work for a close language with almost zero resources. We chose Turkish (Tr) and Azeri or Azerbaijani (Az) as the proposed pair in our experiments. Azeri suffers from lack of resources as there is almost no bilingual corpus for this language. Via our techniques, we are able to train an engine for the Az → English (En) direction, which is able to outperform all other existing models.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2754166059",
    "type": "article"
  },
  {
    "title": "CLASENTI",
    "doi": "https://doi.org/10.1145/3209885",
    "publication_date": "2018-07-21",
    "publication_year": 2018,
    "authors": "Ali Hamdi; Khaled Shaban; Anazida Zainal",
    "corresponding_authors": "",
    "abstract": "Arabic text sentiment analysis suffers from low accuracy due to Arabic-specific challenges (e.g., limited resources, morphological complexity, and dialects) and general linguistic issues (e.g., fuzziness, implicit sentiment, sarcasm, and spam). The limited resources problem requires efforts to build new and improved Arabic corpora and lexica. We propose a class-specific sentiment analysis (CLASENTI) framework. The framework includes a new annotation approach to build multi-faceted Arabic corpus and lexicon allowing for simultaneous annotation of different facets, including domains, dialects, linguistic issues, and polarity strengths. Each of these facets has multiple classes (e.g., the nine classes representing dialects found in the Arab world). The new corpus and lexicon annotations facilitate the development of new class-specific classification models and polarity strength calculation. For the new sentiment classification models, we propose a hybrid model combining corpus-based and lexicon-based models. The corpus-based model has two interrelated phases to build; (1) full-corpus classification models for all facets; and (2) class-specific models trained on filtered subsets of the corpus according to the performances of the full-corpus models. To calculate polarity strengths, the lexicon-based model filters the annotated lexicon based on the specific classes of the domain and dialect. As a case study, we collect and annotate 15274 reviews from various sources, including surveys, Facebook comments, and Twitter posts, pertaining to governmental services. In addition, we develop a new web-based application to apply the proposed framework on the case study. CLASENTI framework reaches up to 95% accuracy and 93% F1-Score surpassing the best-known sentiment classifiers implemented in Scikit-learn library that achieve 82% accuracy and 81% F1-Score for Arabic when tested on the same dataset.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2884753007",
    "type": "article"
  },
  {
    "title": "Neural Conversation Generation with Auxiliary Emotional Supervised Models",
    "doi": "https://doi.org/10.1145/3344788",
    "publication_date": "2019-09-17",
    "publication_year": 2019,
    "authors": "Guangyou Zhou; Yizhen Fang; Yehong Peng; Jiaheng Lu",
    "corresponding_authors": "",
    "abstract": "An important aspect of developing dialogue agents involves endowing a conversation system with emotion perception and interaction. Most existing emotion dialogue models lack the adaptability and extensibility of different scenes because of their limitation to require a specified emotion category or their reliance on a fixed emotional dictionary. To overcome these limitations, we propose a neural conversation generation with auxiliary emotional supervised model (nCG-ESM) comprising a sequence-to-sequence (Seq2Seq) generation model and an emotional classifier used as an auxiliary model. The emotional classifier was trained to predict the emotion distributions of the dialogues, which were then used as emotion supervised signals to guide the generation model to generate diverse emotional responses. The proposed nCG-ESM is flexible enough to generate responses with emotional diversity, including specified or unspecified emotions, which can be adapted and extended to different scenarios. We conducted extensive experiments on the popular dataset of Weibo post--response pairs. Experimental results showed that the proposed model was capable of producing more diverse, appropriate, and emotionally rich responses, yielding substantial gains in diversity scores and human evaluations.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2997679291",
    "type": "article"
  },
  {
    "title": "Multi-Round Transfer Learning for Low-Resource NMT Using Multiple High-Resource Languages",
    "doi": "https://doi.org/10.1145/3314945",
    "publication_date": "2019-05-21",
    "publication_year": 2019,
    "authors": "Mieradilijiang Maimaiti; Yang Liu; Huanbo Luan; Maosong Sun",
    "corresponding_authors": "",
    "abstract": "Neural machine translation (NMT) has made remarkable progress in recent years, but the performance of NMT suffers from a data sparsity problem since large-scale parallel corpora are only readily available for high-resource languages (HRLs). In recent days, transfer learning (TL) has been used widely in low-resource languages (LRLs) machine translation, while TL is becoming one of the vital directions for addressing the data sparsity problem in low-resource NMT. As a solution, a transfer learning method in NMT is generally obtained via initializing the low-resource model (child) with the high-resource model (parent). However, leveraging the original TL to low-resource models is neither able to make full use of highly related multiple HRLs nor to receive different parameters from the same parents. In order to exploit multiple HRLs effectively, we present a language-independent and straightforward multi-round transfer learning (MRTL) approach to low-resource NMT. Besides, with the intention of reducing the differences between high-resource and low-resource languages at the character level, we introduce a unified transliteration method for various language families, which are both semantically and syntactically highly analogous with each other. Experiments on low-resource datasets show that our approaches are effective, significantly outperform the state-of-the-art methods, and yield improvements of up to 5.63 BLEU points.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2945381748",
    "type": "article"
  },
  {
    "title": "Improving Code-mixed POS Tagging Using Code-mixed Embeddings",
    "doi": "https://doi.org/10.1145/3380967",
    "publication_date": "2020-03-29",
    "publication_year": 2020,
    "authors": "S. Nagesh Bhattu; Satya Krishna Nunna; D. V. L. N. Somayajulu; Binay Pradhan",
    "corresponding_authors": "",
    "abstract": "Social media data has become invaluable component of business analytics. A multitude of nuances of social media text make the job of conventional text analytical tools difficult. Code-mixing of text is a phenomenon prevalent among social media users, wherein words used are borrowed from multiple languages, though written in the commonly understood roman script. All the existing supervised learning methods for tasks such as Parts Of Speech (POS) tagging for code-mixed social media (CMSM) text typically depend on a large amount of training data. Preparation of such large training data is resource-intensive, requiring expertise in multiple languages. Though the preparation of small dataset is possible, the out of vocabulary (OOV) words pose major difficulty, while learning models from CMSM text as the number of different ways of writing non-native words in roman script is huge. POS tagging for code-mixed text is non-trivial, as tagging should deal with syntactic rules of multiple languages. The important research question addressed by this article is whether abundantly available unlabeled data can help in resolving the difficulties posed by code-mixed text for POS tagging. We develop an approach for scraping and building word embeddings for code-mixed text illustrating it for Bengali-English, Hindi-English, and Telugu-English code-mixing scenarios. We used a hierarchical deep recurrent neural network with linear-chain CRF layer on top of it to improve the performance of POS tagging in CMSM text by capturing contextual word features and character-sequence–based information. We prepared a labeled resource for POS tagging of CMSM text by correcting 19% of labels from an existing resource. A detailed analysis of the performance of our approach with varying levels of code-mixing is provided. The results indicate that the F1-score of our approach with custom embeddings is better than the CRF-based baseline by 5.81%, 5.69%, and 6.3% in Bengali, Hindi , and Telugu languages, respectively.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3015144506",
    "type": "article"
  },
  {
    "title": "Cyberbullying Detection, Based on the FastText and Word Similarity Schemes",
    "doi": "https://doi.org/10.1145/3398191",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Kun Wang; Yanpeng Cui; Jianwei Hu; Yu Zhang; Wei Zhao; Luming Feng",
    "corresponding_authors": "",
    "abstract": "With recent developments in online social networks (OSNs), these services are widely applied in daily lives. On the other hand, cyberbullying, which is a relatively new type of harassment through the internet-based electronic devices, is rising in online social networks. Accordingly, scholars are attracted to investigating cyberbullying behaviors. Studies show that cyberbullying has a devastating effect on mental health, especially for teenagers. In order to reduce or even stop cyberbullying, different machine learning techniques are applied and numerous studies have been conducted so far. However, conventional detection schemes still have challenges, such as low accuracy. Therefore, it is of significant importance to find an efficient detection solution in the natural language processing and machine learning communities. In the present study, characteristics of cyberbullying are initially analyzed from vocabulary and syntax points of view. Then a new detection algorithm is proposed based on FastText and word similarity schemes. Finally, experiments are carried out to evaluate the effectiveness and performance of the proposed method. Obtained results show that the proposed algorithm can effectively improve the detection accuracy and recall rate of cyberbullying detection.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3041453066",
    "type": "article"
  },
  {
    "title": "A Discourse-Based Approach for Arabic Question Answering",
    "doi": "https://doi.org/10.1145/2988238",
    "publication_date": "2016-11-04",
    "publication_year": 2016,
    "authors": "Jawad Sadek; Farid Meziane",
    "corresponding_authors": "",
    "abstract": "The treatment of complex questions with explanatory answers involves searching for arguments in texts. Because of the prominent role that discourse relations play in reflecting text producers’ intentions, capturing the underlying structure of text constitutes a good instructor in this issue. From our extensive review, a system for automatic discourse analysis that creates full rhetorical structures in large-scale Arabic texts is currently unavailable. This is due to the high computational complexity involved in processing a large number of hypothesized relations associated with large texts. Therefore, more practical approaches should be investigated. This article presents a new Arabic Text Parser oriented for question-answering systems dealing with لماذا “why” and كيف “how to” questions. The Text Parser presented here considers the sentence as the basic unit of text and incorporates a set of heuristics to avoid computational explosion. With this approach, the developed question-answering system reached a significant improvement over the baseline with a Recall of 68% and MRR of 0.62.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2342468395",
    "type": "article"
  },
  {
    "title": "Online Handwritten Gurmukhi Strokes Dataset Based on Minimal Set of Words",
    "doi": "https://doi.org/10.1145/2896318",
    "publication_date": "2016-06-27",
    "publication_year": 2016,
    "authors": "Sukhdeep Singh; Anuj Sharma; Indu Chhabra",
    "corresponding_authors": "",
    "abstract": "The online handwriting data are an integral part of data analysis and classification research, as collected handwritten data offers many challenges to group handwritten stroke classes. The present work has been done for grouping handwritten strokes from the Indic script Gurmukhi. Gurmukhi is the script of the popular and widely spoken language Punjabi. The present work includes development of the dataset of Gurmukhi words in the context of online handwriting recognition for real-life use applications, such as maps navigation. We have collected the data of 100 writers from the largest cities in the Punjab region. The writers’ variations, such as writing skill level (beginner, moderate, and expert), gender, right or left handedness, and their adaptability to digital handwriting, have been considered in dataset development. We have introduced a novel technique to form handwritten stroke classes based on a limited set of words. The presence of all alphabets including vowels of Gurmukhi script has been considered before selection of a word. The developed dataset includes 39,411 strokes from handwritten words and forms 72 classes of strokes after using a k-means clustering technique and manual verification through expert and moderate writers. We have achieved recognition results using the Hidden Markov Model as 87.10%, 85.43%, and 84.33% for middle zone strokes when using training data as 66%, 50%, and 80% of the developed dataset. The present work is a step in a direction to find groups for unknown handwriting strokes with reasonably higher levels of accuracy.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2465630396",
    "type": "article"
  },
  {
    "title": "A Deep Learning–based Approach for Emotions Classification in Big Corpus of Imbalanced Tweets",
    "doi": "https://doi.org/10.1145/3410570",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Nasir Jamal; Xianqiao Chen; Fadi Al‐Turjman; Farhan Ullah",
    "corresponding_authors": "",
    "abstract": "Emotions detection in natural languages is very effective in analyzing the user's mood about a concerned product, news, topic, and so on. However, it is really a challenging task to extract important features from a burst of raw social text, as emotions are subjective with limited fuzzy boundaries. These subjective features can be conveyed in various perceptions and terminologies. In this article, we proposed an IoT-based framework for emotions classification of tweets using a hybrid approach of Term Frequency Inverse Document Frequency (TFIDF) and deep learning model. First, the raw tweets are filtered using the tokenization method for capturing useful features without noisy information. Second, the TFIDF statistical technique is applied to estimate the importance of features locally as well as globally. Third, the Adaptive Synthetic (ADASYN) class balancing technique is applied to solve the imbalance class issue among different classes of emotions. Finally, a deep learning model is designed to predict the emotions with dynamic epoch curves. The proposed methodology is analyzed on two different Twitter emotions datasets. The dynamic epoch curves are shown to show the behavior of test and train data points. It is proved that this methodology outperformed the popular state-of-the-art methods.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3136343621",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis in Hindi—A Survey on the State-of-the-art Techniques",
    "doi": "https://doi.org/10.1145/3469722",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Dhanashree Kulkarni; Sunil S. Rodd",
    "corresponding_authors": "",
    "abstract": "Sentiment Analysis (SA) has been a core interest in the field of text mining research, dealing with computational processing of sentiments, views, and subjective nature of the text. Due to the availability of extensive web-based data in Indian languages such as Hindi, Marathi, Kannada, Tamil, and so on. It has become extremely significant to analyze this data and recover valuable and relevant information. Hindi being the first language of the majority of the population in India, SA in Hindi has turned out to be a critical task particularly for companies and government organizations. This research portrays a systematic review specifically in the field of Hindi SA. The major contribution of this article includes the categorization of numerous articles based on techniques that have attracted researchers in performing SA tasks in Hindi language. This survey classifies these state-of-the-art computational intelligence techniques into four major categories namely lexicon-based techniques, machine learning techniques, deep learning techniques, and hybrid techniques. It discusses the importance of these techniques based on different aspects such as their impact on the issues of SA, levels of analysis, and performance evaluation measures. The research puts forward a comprehensive overview of the majority of the work done in Hindi SA. This study will help researchers in finding out resources such as annotated datasets, linguistic resources, and lexical resources. This survey delivers some significant findings and presents overall future research directions in the field of Hindi SA.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3215328431",
    "type": "article"
  },
  {
    "title": "Part-of-Speech (POS) Tagging Using Deep Learning-Based Approaches on the Designed Khasi POS Corpus",
    "doi": "https://doi.org/10.1145/3488381",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Sunita Warjri; Partha Pakray; Saralin A. Lyngdoh; Arnab Kumar Maji",
    "corresponding_authors": "",
    "abstract": "Part-of-speech (POS) tagging is one of the research challenging fields in natural language processing (NLP). It requires good knowledge of a particular language with large amounts of data or corpora for feature engineering, which can lead to achieving a good performance of the tagger. Our main contribution in this research work is the designed Khasi POS corpus. Till date, there has been no form of any kind of Khasi corpus developed or formally developed. In the present designed Khasi POS corpus, each word is tagged manually using the designed tagset. Methods of deep learning have been used to experiment with our designed Khasi POS corpus. The POS tagger based on BiLSTM, combinations of BiLSTM with CRF, and character-based embedding with BiLSTM are presented. The main challenges of understanding and handling Natural Language toward Computational linguistics to encounter are anticipated. In the presently designed corpus, we have tried to solve the problems of ambiguities of words concerning their context usage, and also the orthography problems that arise in the designed POS corpus. The designed Khasi corpus size is around 96,100 tokens and consists of 6,616 distinct words. Initially, while running the first few sets of data of around 41,000 tokens in our experiment the taggers are found to yield considerably accurate results. When the Khasi corpus size has been increased to 96,100 tokens, we see an increase in accuracy rate and the analyses are more pertinent. As results, accuracy of 96.81% is achieved for the BiLSTM method, 96.98% for BiLSTM with CRF technique, and 95.86% for character-based with LSTM. Concerning substantial research from the NLP perspectives for Khasi, we also present some of the recently existing POS taggers and other NLP works on the Khasi language for comparative purposes.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4200294139",
    "type": "article"
  },
  {
    "title": "Fake News Classification: A Quantitative Research Description",
    "doi": "https://doi.org/10.1145/3447650",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Rachna Jain; Deepak Kumar Jain; Dharana; Nitika Sharma",
    "corresponding_authors": "",
    "abstract": "Social media can render content circulating to reach millions with a knack to influence people, despite the questionable authencity of the facts. Internet sources are the most convenient and easy approach to obtain any information these days. Fake news has become the topic of interest for academicians and the rest of society. This kind of propaganda has the power to influence the general perception, offering political groups the ability to control the results of democratic affairs such as elections. Automatic identification of fake news has emerged as one of the significant problems due to the high risks involved. It is challenging in a way because of the complexity levels of accurately interpreting the data. An extensive search has already been performed on English language news data. Our work presents a comparative analysis of fake news classifiers on the low resource Bengali language ‘ban fake news’ dataset from Kaggle. The analysis presented compares deep learning techniques such as LSTM (Long short-term Memory) and BiLSTM (Bi-directional Long short-term Memory) and machine learning methods like Naive Bayes, Passive Aggressive Classifier (PAC), and Random Forest. The comparison has been drawn based on classification metrics such as accuracy, precision, recall, and F1 score. The deep learning method BiLSTM shows 55.92% accuracy while Random Forest, in contrast, has outperformed all the other methods with an accuracy of 62.37%. The work presented in this paper sets a basis for researchers to select the optimum classifiers for their approach towards fake news detection.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4200302203",
    "type": "article"
  },
  {
    "title": "Aspect-based Sentiment Analysis using Dependency Parsing",
    "doi": "https://doi.org/10.1145/3485243",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Sujata Rani; Parteek Kumar",
    "corresponding_authors": "",
    "abstract": "In this paper, an aspect-based Sentiment Analysis (SA) system for Hindi is presented. The proposed system assigns a separate sentiment towards the different aspects of a sentence as well as it evaluates the overall sentiment expressed in a sentence. In this work, Hindi Dependency Parser (HDP) is used to determine the association between an aspect word and a sentiment word (using Hindi SentiWordNet) and works on the idea that closely connected words come together to express a sentiment about a certain aspect. By generating a dependency graph, the system assigns the sentiment to an aspect having a minimum distance between them and computes the overall polarity of the sentence. The system achieves an accuracy of 83.2% on a corpus of movie reviews and its results are compared with baselines as well as existing works on SA. From the results, it has been observed that the proposed system has the potential to be used in emerging applications like SA of product reviews, social media analysis, etc.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4200472398",
    "type": "article"
  },
  {
    "title": "Arabic Diacritic Recovery Using a Feature-rich biLSTM Model",
    "doi": "https://doi.org/10.1145/3434235",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Kareem Darwish; Ahmed Abdelalí; Hamdy Mubarak; Mohamed Eldesouki",
    "corresponding_authors": "",
    "abstract": "Diacritics (short vowels) are typically omitted when writing Arabic text, and readers have to reintroduce them to correctly pronounce words. There are two types of Arabic diacritics: The first are core-word diacritics (CW), which specify the lexical selection, and the second are case endings (CE), which typically appear at the end of word stems and generally specify their syntactic roles. Recovering CEs is relatively harder than recovering core-word diacritics due to inter-word dependencies, which are often distant. In this article, we use feature-rich recurrent neural network model that use a variety of linguistic and surface-level features to recover both core word diacritics and case endings. Our model surpasses all previous state-of-the-art systems with a CW error rate (CWER) of 2.9% and a CE error rate (CEER) of 3.7% for Modern Standard Arabic (MSA) and CWER of 2.2% and CEER of 2.5% for Classical Arabic (CA). When combining diacritized word cores with case endings, the resultant word error rates are 6.0% and 4.3% for MSA and CA, respectively. This highlights the effectiveness of feature engineering for such deep neural models.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3154883498",
    "type": "article"
  },
  {
    "title": "An Embedding-Based Topic Model for Document Classification",
    "doi": "https://doi.org/10.1145/3431728",
    "publication_date": "2021-05-05",
    "publication_year": 2021,
    "authors": "Sattar Seifollahi; Massimo Piccardi; Alireza Jolfaei",
    "corresponding_authors": "",
    "abstract": "Topic modeling is an unsupervised learning task that discovers the hidden topics in a collection of documents. In turn, the discovered topics can be used for summarizing, organizing, and understanding the documents in the collection. Most of the existing techniques for topic modeling are derivatives of the Latent Dirichlet Allocation which uses a bag-of-word assumption for the documents. However, bag-of-words models completely dismiss the relationships between the words. For this reason, this article presents a two-stage algorithm for topic modelling that leverages word embeddings and word co-occurrence. In the first stage, we determine the topic-word distributions by soft-clustering a random set of embedded n -grams from the documents. In the second stage, we determine the document-topic distributions by sampling the topics of each document from the topic-word distributions. This approach leverages the distributional properties of word embeddings instead of using the bag-of-words assumption. Experimental results on various data sets from an Australian compensation organization show the remarkable comparative effectiveness of the proposed algorithm in a task of document classification.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3159498945",
    "type": "article"
  },
  {
    "title": "Detecting Arabic Spam Reviews in Social Networks Based on Classification Algorithms",
    "doi": "https://doi.org/10.1145/3476115",
    "publication_date": "2021-11-01",
    "publication_year": 2021,
    "authors": "Hassan Najadat; Mohammad A. Alzubaidi; Islam Qarqaz",
    "corresponding_authors": "",
    "abstract": "Reviews or comments that users leave on social media have great importance for companies and business entities. New product ideas can be evaluated based on customer reactions. However, this use of social media is complicated by those who post spam on social media in the form of reviews and comments. Designing methodologies to automatically detect and block social media spam is complicated by the fact that spammers continuously develop new ways to leave their spam comments. Researchers have proposed several methods to detect English spam reviews. However, few studies have been conducted to detect Arabic spam reviews. This article proposes a keyword-based method for detecting Arabic spam reviews. Keywords or Features are subsets of words from the original text that are labelled as important. A term's weight, Term Frequency–Inverse Document Frequency (TF-IDF) matrix, and filter methods (such as information gain, chi-squared, deviation, correlation, and uncertainty) have been used to extract keywords from Arabic text. The method proposed in this article detects Arabic spam in Facebook comments. The dataset consists of 3,000 Arabic comments extracted from Facebook pages. Four different machine learning algorithms are used in the detection process, including C4.5, kNN, SVM, and Naïve Bayes classifiers. The results show that the Decision Tree classifier outperforms the other classification algorithms, with a detection accuracy of 92.63%.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3209312188",
    "type": "article"
  },
  {
    "title": "Low Resource Neural Machine Translation: Assamese to/from Other Indo-Aryan (Indic) Languages",
    "doi": "https://doi.org/10.1145/3469721",
    "publication_date": "2021-11-16",
    "publication_year": 2021,
    "authors": "Rupjyoti Baruah; Rajesh Kumar Mundotiya; Anil Kumar Singh",
    "corresponding_authors": "",
    "abstract": "Machine translation (MT) systems have been built using numerous different techniques for bridging the language barriers. These techniques are broadly categorized into approaches like Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). End-to-end NMT systems significantly outperform SMT in translation quality on many language pairs, especially those with the adequate parallel corpus. We report comparative experiments on baseline MT systems for Assamese to other Indo-Aryan languages (in both translation directions) using the traditional Phrase-Based SMT as well as some more successful NMT architectures, namely basic sequence-to-sequence model with attention, Transformer, and finetuned Transformer. The results are evaluated using the most prominent and popular standard automatic metric BLEU (BiLingual Evaluation Understudy), as well as other well-known metrics for exploring the performance of different baseline MT systems, since this is the first such work involving Assamese. The evaluation scores are compared for SMT and NMT models for the effectiveness of bi-directional language pairs involving Assamese and other Indo-Aryan languages (Bangla, Gujarati, Hindi, Marathi, Odia, Sinhalese, and Urdu). The highest BLEU scores obtained are for Assamese to Sinhalese for SMT (35.63) and the Assamese to Bangla for NMT systems (seq2seq is 50.92, Transformer is 50.01, and finetuned Transformer is 50.19). We also try to relate the results with the language characteristics, distances, family trees, domains, data sizes, and sentence lengths. We find that the effect of the domain is the most important factor affecting the results for the given data domains and sizes. We compare our results with the only existing MT system for Assamese (Bing Translator) and also with pairs involving Hindi.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3214292156",
    "type": "article"
  },
  {
    "title": "Efficient Channel Attention Based Encoder–Decoder Approach for Image Captioning in Hindi",
    "doi": "https://doi.org/10.1145/3483597",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Santosh Kumar Mishra; Gaurav Rai; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Image captioning refers to the process of generating a textual description that describes objects and activities present in a given image. It connects two fields of artificial intelligence, computer vision, and natural language processing. Computer vision and natural language processing deal with image understanding and language modeling, respectively. In the existing literature, most of the works have been carried out for image captioning in the English language. This article presents a novel method for image captioning in the Hindi language using encoder–decoder based deep learning architecture with efficient channel attention. The key contribution of this work is the deployment of an efficient channel attention mechanism with bahdanau attention and a gated recurrent unit for developing an image captioning model in the Hindi language. Color images usually consist of three channels, namely red, green, and blue. The channel attention mechanism focuses on an image’s important channel while performing the convolution, which is basically to assign higher importance to specific channels over others. The channel attention mechanism has been shown to have great potential for improving the efficiency of deep convolution neural networks (CNNs). The proposed encoder–decoder architecture utilizes the recently introduced ECA-NET CNN to integrate the channel attention mechanism. Hindi is the fourth most spoken language globally, widely spoken in India and South Asia; it is India’s official language. By translating the well-known MSCOCO dataset from English to Hindi, a dataset for image captioning in Hindi is manually created. The efficiency of the proposed method is compared with other baselines in terms of Bilingual Evaluation Understudy (BLEU) scores, and the results obtained illustrate that the method proposed outperforms other baselines. The proposed method has attained improvements of 0.59%, 2.51%, 4.38%, and 3.30% in terms of BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores, respectively, with respect to the state-of-the-art. Qualities of the generated captions are further assessed manually in terms of adequacy and fluency to illustrate the proposed method’s efficacy.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4200435073",
    "type": "article"
  },
  {
    "title": "Multi-domain Spoken Language Understanding Using Domain- and Task-aware Parameterization",
    "doi": "https://doi.org/10.1145/3502198",
    "publication_date": "2022-01-20",
    "publication_year": 2022,
    "authors": "Libo Qin; Fuxuan Wei; Minheng Ni; Yue Zhang; Wanxiang Che; Yangming Li; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Spoken language understanding (SLU) has been addressed as a supervised learning problem, where a set of training data is available for each domain. However, annotating data for a new domain can be both financially costly and non-scalable. One existing approach solves the problem by conducting multi-domain learning where parameters are shared for joint training across domains, which is domain-agnostic and task-agnostic . In the article, we propose to improve the parameterization of this method by using domain-specific and task-specific model parameters for fine-grained knowledge representation and transfer. Experiments on five domains show that our model is more effective for multi-domain SLU and obtain the best results. In addition, we show its transferability when adapting to a new domain with little data, outperforming the prior best model by 12.4%. Finally, we explore the strong pre-trained model in our framework and find that the contributions from our framework do not fully overlap with contextualized word representations (RoBERTa).",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3023207088",
    "type": "article"
  },
  {
    "title": "A New Amharic Speech Emotion Dataset and Classification Benchmark",
    "doi": "https://doi.org/10.1145/3529759",
    "publication_date": "2022-05-02",
    "publication_year": 2022,
    "authors": "Ephrem Afele Retta; Eiad Almekhlafi; Richard F. E. Sutcliffe; Mustafa Mhamed; Haider Ali; Jun Feng",
    "corresponding_authors": "",
    "abstract": "In this article we present the Amharic Speech Emotion Dataset (ASED), which covers four dialects (Gojjam, Wollo, Shewa, and Gonder) and five different emotions (neutral, fearful, happy, sad, and angry). We believe it is the first Speech Emotion Recognition (SER) dataset for the Amharic language. Sixty-five volunteer participants, all native speakers of Amharic, recorded 2,474 sound samples, 2 to 4 seconds in length. Eight judges (two for each dialect) assigned emotions to the samples with high agreement level (Fleiss kappa = 0.8). The resulting dataset is freely available for download. Next, we developed a four-layer variant of the well-known VGG model, which we call VGGb. Three experiments were then carried out using VGGb for SER, using ASED. First, we investigated which features work best for Amharic, FilterBank, Mel Spectrogram, or Mel-frequency Cepstral Coefficient (MFCC). This was done by training three VGGb SER models on ASED, using FilterBank, Mel Spectrogram, and MFCC features, respectively. Four forms of training were tried, standard cross-validation and three variants based on sentences, dialects, and speaker groups. Thus, a sentence used for training would not be used for testing, and the same for a dialect and speaker group. MFCC features were superior under all four training schemes. MFCC was therefore adopted for Experiment 2, where VGGb and three well-known existing models were compared on ASED: RESNet50, AlexNet, and LSTM. VGGb was found to have very good accuracy (90.73%) as well as the fastest training time. In Experiment 3, the performance of VGGb was compared when trained on two existing SER datasets—RAVDESS (English) and EMO-DB (German)—as well as on ASED (Amharic). Results are comparable across these languages, with ASED being the highest. This suggests that VGGb can be successfully applied to other languages. We hope that ASED will encourage researchers to explore the Amharic language and to experiment with other models for Amharic SER.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4221140133",
    "type": "article"
  },
  {
    "title": "Using Fuzzy Clustering with Deep Learning Models for Detection of COVID-19 Disinformation",
    "doi": "https://doi.org/10.1145/3548458",
    "publication_date": "2022-07-14",
    "publication_year": 2022,
    "authors": "Mu‐Yen Chen; Yi-Wei Lai",
    "corresponding_authors": "",
    "abstract": "Since the beginning of 2020, the COVID-19 pandemic has killed millions of people around the world, leading to a worldwide panic that has fueled the rapid and widespread dissemination of COVID-19-related disinformation on social media. The phenomenon, described by the World Health Organization (WHO) as an \"indodemic\" presents a serious challenge to governments and public health authorities, but the spread of misinformation has made human detection less efficient than the rate of spread. While there have been many studies developing automated detection techniques for COVID-19 fake news, the results often refer to high accuracy but rarely to model detection time. This research uses fuzzy theory to extract features and uses multiple deep learning model frameworks to detect Chinese and English COVID-19 misinformation. With the reduction of text features, the detection time of the model is significantly reduced, and the model accuracy does not drop excessively. This study designs two different feature extraction methods based on fuzzy classification and compares the results with different deep learning models. BiLSTM was found to provide the best detection results for COVID-19 misinformation by directly using deep learning models, with 99% accuracy in English and 86% accuracy in Chinese. Applying fuzzy clustering to English COVID-19 fake news data features maintains 99% accuracy while reducing detection time by 10%. For Chinese misinformation, detection time is reduced by 15% at the cost of an 8% drop in accuracy.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4285390770",
    "type": "article"
  },
  {
    "title": "A Comparative Study of Speaker Role Identification in Air Traffic Communication Using Deep Learning Approaches",
    "doi": "https://doi.org/10.1145/3572792",
    "publication_date": "2022-11-24",
    "publication_year": 2022,
    "authors": "Dongyue Guo; Jianwei Zhang; Bo Yang; Yi Lin",
    "corresponding_authors": "",
    "abstract": "Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this article, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech-and-text-based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches worked for the SRI task, and the proposed MMSRINet shows competitive performance and robustness compared with the other methods on both seen and unseen data, achieving 98.56% and 98.08% accuracy, respectively.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4309857275",
    "type": "article"
  },
  {
    "title": "Research on pattern recognition of different music types in the context of AI with the help of multimedia information processing",
    "doi": "https://doi.org/10.1145/3523284",
    "publication_date": "2023-02-13",
    "publication_year": 2023,
    "authors": "Wei Sun; Revathi Sundarasekar",
    "corresponding_authors": "",
    "abstract": "Music is a form of art in which the sounds are timed and organized. Music is a kind of entertainment that mixes sounds in a way that people like, find fascinating, or to which they desire to dance. Most music is created via one or more people's vocal or instrumental efforts. By the dictionary, music is defined as having at least one of the following three elements: rhythm, melody, and harmony. Music is utilized in therapy because of its apparent benefits on behavior. Various physiological circumstances have different metabolite expression patterns that can be studied using pattern recognition in multimedia information processing. Music therapy includes various activities, including singing, playing instruments, dancing, and listening to music. Music-making with artificial intelligence (AI) uses neural networks, which are massive collections of computer bits that aim to stimulate brain activity. The neural network (NN) may be bombarded with music to see if it picks up on patterns the way the human brain does when repeatedly exposed to new stimuli. It will get the hang of them eventually. Experts believed that AI would be unable to generate music unless it first mimics a human-created data collection. By providing a conceptual paradigm for multimedia information processing. The end effect will be entirely different depending on how many hours of music are placed into it. For AI to learn from patterns or features in data on its own, it needs big data (BD), fast, repeated processing, and complex algorithms. The use of technology makes the process of creating analytical models much faster. A new AI-BD tool is an opportunity, not a danger for people currently working as artists. People are beginning to ask what constitutes acceptable work as AI grows more prominent in the music and art industries to gain efficiency of 97.8%. Future music will be heavily impacted by listeners' bodies and emotions all the time. For example, wearable technology may detect a person's mood and play the music that matches it. It is the next step in personalization. The AI-BD methodology improves the efficiency, accuracy, etc., compared to other existing models by gaining 97.8%, performance analysis 97.2%, reliability ratio 95.6%, and survivability analysis 98.2%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4320490929",
    "type": "article"
  },
  {
    "title": "OdeBERT: One-stage Deep-supervised Early-exiting BERT for Fast Inference in User Intent Classification",
    "doi": "https://doi.org/10.1145/3587464",
    "publication_date": "2023-03-13",
    "publication_year": 2023,
    "authors": "Yuanxia Liu; Tianyong Hao; Hai Liu; Yuanyuan Mu; Heng Weng; Fu Lee Wang",
    "corresponding_authors": "",
    "abstract": "User intent classification is a vital task for analyzing users’ essential requirements from the users’ input query in information retrieval systems, question answering systems, and dialogue systems. Pre-trained language model Bidirectional Encoder Representation from Transformers (BERT) has been widely applied to the user intent classification task. However, BERT is compute intensive and time-consuming during inference and usually causes latency in real-time applications. To improve the inference efficiency of BERT for the user intent classification task, this article proposes a new network named one-stage deep-supervised early-exiting BERT as one-stage deep-supervised early-exiting BERT (OdeBERT). In addition, a deep supervision strategy is developed to incorporate the network with internal classifiers by one-stage joint training to improve the learning process of classifiers by extracting discriminative category features. Experiments are conducted on publicly available datasets, including ECDT, SNIPS, and FDQuestion. The results show that the OdeBERT can speed up original BERT 12 times faster at most with the same performance, outperforming state-of-the-art baseline methods.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4324056402",
    "type": "article"
  },
  {
    "title": "LFWE: Linguistic Feature Based Word Embedding for Hindi Fake News Detection",
    "doi": "https://doi.org/10.1145/3589764",
    "publication_date": "2023-03-31",
    "publication_year": 2023,
    "authors": "Richa Sharma; Arti Arya",
    "corresponding_authors": "",
    "abstract": "It is essential for research communities to investigate ways for authenticating news. The use of linguistic feature based analysis to automatically detect false news is gaining popularity among the scientific community. However, such techniques are exclusively created for English, leaving low-resource languages like Hindi behind. To address this issue, we constructed a novel annotated Hindi Fake News (HinFakeNews) dataset of roughly 33,300 articles that can be utilized to develop autonomous fake news detection systems. This work provides a two-stage benchmark model for identifying fake news in Hindi using machine learning. The proposed model, LFWE (Linguistic Feature Based Word Embedding), generates word embedding over linguistic features. This article focuses on 23 key linguistic features (15 extracted and 08 derived) for successful detection of Hindi fake news. These features are grouped as lexical, semantic, syntactic, psycho-linguistic, readability, and quantity features. The contribution is twofold. In the first phase, the dataset is preprocessed and linguistic features are extracted. In the second phase, feature sets are generated as word embeddings, and an Ensemble voting classification is carried out on the feature sets. According to experimental findings, the LFWE model accurately detects and classifies fake news in Hindi with an accuracy of 98.49%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4362472874",
    "type": "article"
  },
  {
    "title": "Increasing the Social Interaction of Autism Child using Virtual Reality Intervention (VRI)",
    "doi": "https://doi.org/10.1145/3592855",
    "publication_date": "2023-04-19",
    "publication_year": 2023,
    "authors": "T. Manju; Magesh; S Padmavathi; Durairaj",
    "corresponding_authors": "T. Manju",
    "abstract": "Nowadays there is an increase in number of autisms, a neuro-developmental disorder across the world. The level of autism varies with the symptoms such as inattention, interaction, social communication, repetitive behaviors, irritability and the like. Early recovery of a child from autism is necessary to live in a normal socio-communicable life. To measure the inattention of the autism child by enhancing the visual perception through virtual environment. The proposed Virtual Reality Intervention (VRI) enhances visual perception, learning, and social interaction. The proposed method observes the attention level of the autism child through eye tracking or eye movements who interacts with virtual world using eye tracking methodology. As eye tracking is the major component to measure the reduced looking time of objects and subjects which considered being the earliest signs of autism spectrum disorder (ASD). For observing the attention of kids during testing, Eye movements and gestures plays a major role. Using head position and eye pupil direction, attention has been analyzed. Quantitative and Qualitative findings conclude that the inattention of autism child can gradually be reduced by iterating the virtual therapy through eye ball tracking technique. Qualitative finding is done using Aberrant Behavior Checklist (ABC) and quantitative using eye-pupil and head position. Autism affected children can easily recovered from this inattention symptom by continuous iterations on virtual therapy. Similar virtual therapies can also be provided to address other symptoms of autism.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4366407895",
    "type": "article"
  },
  {
    "title": "Natural Language Processing Pretraining Language Model for Computer Intelligent Recognition Technology",
    "doi": "https://doi.org/10.1145/3605210",
    "publication_date": "2023-06-20",
    "publication_year": 2023,
    "authors": "Dong Jun",
    "corresponding_authors": "Dong Jun",
    "abstract": "Computer intelligent recognition technology refers to the use of computer vision, Natural Language Processing (NLP), machine learning and other technologies to enable computers to recognize, analyze, understand and answer human language and behavior. The common applications of computer intelligent recognition technology include image recognition, NLP, face recognition, target tracking, and other fields. NLP is a field of computer science, which involves the interaction between computers and natural languages. NLP technology can be used to process, analyze and generate natural language data, such as text, voice and image. Common NLP technology applications include language translation, emotion analysis, text classification, speech recognition and question answering system. Language model is a machine learning model, which uses a large number of text data for training to learn language patterns and relationships in text data. Although the language model has made great progress in the past few years, it still faces some challenges, including: poor semantic understanding, confusion in multilingual processing, slow language processing and other shortcomings. Therefore, in order to optimize these shortcomings, this article would study the pre-training language model based on NLP technology, which aimed at using NLP technology to optimize and improve the performance of the language model, thus optimizing the computer intelligent recognition technology. The model had a higher language understanding ability and more accurate prediction ability. In addition, the model could learn language rules and structures by using a large number of corpus, so as to better understand natural language. Through experiments, it could be known that the data size and total computing time of the traditional Generative Pretrained Transformer-2 (GPT-2) language model were 10 GB and 97 hours respectively. The data size and total computing time of BERT (Bidirectional Encoder Representations from Transformer) were 12 GB and 86 hours respectively. The data size and total computing time of the pre-training language model based on NLP were 18 GB and 71 hours respectively. Obviously, the pre-training language model based on NLP had a larger data size and shorter computing time. The experimental results showed that the NLP technology could better optimize the language model and effectively improve its various capabilities. This article opened up a new development direction for computer intelligent recognition technology and provided excellent technical support for the development of language models.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4381334416",
    "type": "article"
  },
  {
    "title": "Assessing Urdu Language Processing Tools via Statistical and Outlier Detection Methods on Urdu Tweets",
    "doi": "https://doi.org/10.1145/3622939",
    "publication_date": "2023-09-08",
    "publication_year": 2023,
    "authors": "Zoya; Seemab Latif; Rabia Latif; Hammad Majeed; Nor Shahida Mohd Jamail",
    "corresponding_authors": "",
    "abstract": "Text pre-processing is a crucial step in Natural Language Processing (NLP) applications, particularly for handling informal and noisy content on social media. Word-level tokenization plays a vital role in text pre-processing by removing stop words, filtering irrelevant characters, and retaining relevant tokens. These tokens are essential for constructing meaningful n-grams within advanced NLP frameworks used for data modeling. However, tokenization in low-resource languages like Urdu presents challenges due to language complexity and limited resources. Conventional space-based methods and direct application of language-specific tools often result in erroneous tokens in Urdu Language Processing (ULP). This hinders language models from effectively learning language-specific and domain-specific tokens, leading to sub-optimal results for downstream tasks such as aspect mining, topic modeling, and Named Entity Recognition (NER). To address this issue for Urdu, we have proposed a data pre-processing technique that detects outliers using the Inter-Quartile-Range (IQR) method and proposed normalization algorithms for creating useful lexicons in conjunction with existing technologies. We have collected approximately 50 million Urdu tweets using the Twitter API and conducted the performance analysis of existing language-specific tokenizers (Urduhack and Space-based tokenizer). Dataset variants were created based on the language-specific tokenizers, and we performed statistical analysis tests and visualization techniques to compare tokenization results before and after applying the proposed outlier detection and normalization method. Our findings highlighted the noticeable improvement in token size distributions, handling of informal language tokens, and misspelled and lengthy tokens. The Urduhack tokenizer combined with the proposed outlier detection and normalization yielded tokens with the best-fitted distribution in ULP. Its effectiveness has been evaluated through the task of topic modeling using Non-negative Matrix Factorization (NMF) and Latent Dirichlet allocation (LDA). The results demonstrated new and distinct topics using unigram features while achieving highly coherent topics when utilizing bigram features. For the traditional space-based method, the results consistently demonstrated improved coherence and precision scores. However, the NMF topic modeling with bigram features outperformed LDA topic modeling with bigram features.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386546962",
    "type": "article"
  },
  {
    "title": "Comparison Study on Critical Components in Composition Model for Phrase Representation",
    "doi": "https://doi.org/10.1145/3010088",
    "publication_date": "2017-01-20",
    "publication_year": 2017,
    "authors": "Shaonan Wang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Phrase representation, an important step in many NLP tasks, involves representing phrases as continuous-valued vectors. This article presents detailed comparisons concerning the effects of word vectors, training data, and the composition and objective function used in a composition model for phrase representation. Specifically, we first discuss how the augmented word representations affect the performance of the composition model. Then, we investigate whether different types of training data influence the performance of the composition model and, if so, how they influence it. Finally, we evaluate combinations of different composition and objective functions and discuss the factors related to composition model performance. All evaluations were conducted in both English and Chinese. Our main findings are as follows: (1) The Additive model with semantic enhanced word vectors performs comparably to the state-of-the-art model; (2) The Additive model which updates augmented word vectors and the Matrix model with semantic enhanced word vectors systematically outperforms the state-of-the-art model in bigram and multi-word phrase similarity task, respectively; (3) Representing the high frequency phrases by estimating their surrounding contexts is a good training objective for bigram phrase similarity tasks; and (4) The performance gain of composition model with semantic enhanced word vectors is due to the composition function and the greater weight attached to important words. Previous works focus on the composition function; however, our findings indicate that other components in the composition model (especially word representation) make a critical difference in phrase representation.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2574738034",
    "type": "article"
  },
  {
    "title": "Application of Structural and Topological Features to Recognize Online Handwritten Bangla Characters",
    "doi": "https://doi.org/10.1145/3178457",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Shibaprasad Sen; Ankan Bhattacharyya; Pawan Kumar Singh; Ram Sarkar; Kaushik Roy; David Doermann",
    "corresponding_authors": "",
    "abstract": "This article presents a set of novel features for robust online Bangla handwritten character recognition. Two feature extraction methods are presented here. The first describes the transition from background to foreground pixels and vice versa. The second uses a combination of topological features and centre-of-gravity- (CG) based circular features where global information, local information, and Circular Quadrant Mass Distribution information have been extracted. The impact of each along with their combination have also been analyzed. A total of 15,000 isolated online Bangla character samples have been collected and used for the evaluation. A Support Vector Machine classifier records the best recognition rate when the transition count feature, CG-based circular features, and topological features are combined.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2787005077",
    "type": "article"
  },
  {
    "title": "A Sense Annotated Corpus for All-Words Urdu Word Sense Disambiguation",
    "doi": "https://doi.org/10.1145/3314940",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Ali Saeed; Rao Muhammad Adeel Nawab; Mark Stevenson; Paul Rayson",
    "corresponding_authors": "",
    "abstract": "Word Sense Disambiguation (WSD) aims to automatically predict the correct sense of a word used in a given context. All human languages exhibit word sense ambiguity, and resolving this ambiguity can be difficult. Standard benchmark resources are required to develop, compare, and evaluate WSD techniques. These are available for many languages, but not for Urdu, despite this being a language with more than 300 million speakers and large volumes of text available digitally. To fill this gap, this study proposes a novel benchmark corpus for the Urdu All-Words WSD task. The corpus contains 5,042 words of Urdu running text in which all ambiguous words (856 instances) are manually tagged with senses from the Urdu Lughat dictionary. A range of baseline WSD models based on n -gram are applied to the corpus, and the best performance (accuracy of 57.71%) is achieved using word 4-gram. The corpus is freely available to the research community to encourage further WSD research in Urdu.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2936459239",
    "type": "article"
  },
  {
    "title": "A Generalized Constraint Approach to Bilingual Dictionary Induction for Low-Resource Language Families",
    "doi": "https://doi.org/10.1145/3138815",
    "publication_date": "2017-11-13",
    "publication_year": 2017,
    "authors": "Arbi Haza Nasution; Yohei Murakami; Toru Ishida",
    "corresponding_authors": "",
    "abstract": "The lack or absence of parallel and comparable corpora makes bilingual lexicon extraction a difficult task for low-resource languages. The pivot language and cognate recognition approaches have been proven useful for inducing bilingual lexicons for such languages. We propose constraint-based bilingual lexicon induction for closely-related languages by extending constraints from the recent pivot-based induction technique and further enabling multiple symmetry assumption cycles to reach many more cognates in the transgraph. We further identify cognate synonyms to obtain many-to-many translation pairs. This paper utilizes four datasets: one Austronesian low-resource language and three Indo-European high-resource languages. We use three constraint-based methods from our previous work, the Inverse Consultation method and translation pairs generated from the Cartesian product of input dictionaries as baselines. We evaluate our result using the metrics of precision, recall and F-score. Our customizable approach allows the user to conduct cross-validation to predict the optimal hyperparameters (cognate threshold and cognate synonym threshold) with various combinations of heuristics and the number of symmetry assumption cycles to gain the highest F-score. Our proposed methods have statistically significant improvement of precision and F-score compared to our previous constraint-based methods. The results show that our method demonstrates the potential to complement other bilingual dictionary creation methods like word alignment models using parallel corpora for high-resource languages while well handling low-resource languages.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3122255303",
    "type": "article"
  },
  {
    "title": "Integrated Parallel Sentence and Fragment Extraction from Comparable Corpora",
    "doi": "https://doi.org/10.1145/2833089",
    "publication_date": "2015-12-11",
    "publication_year": 2015,
    "authors": "Chenhui Chu; Toshiaki Nakazawa; Sadao Kurohashi",
    "corresponding_authors": "",
    "abstract": "Parallel corpora are crucial for statistical machine translation (SMT); however, they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract either parallel sentences or fragments from them for SMT. In this article, we propose an integrated system to extract both parallel sentences and fragments from comparable corpora. We first apply parallel sentence extraction to identify parallel sentences from comparable sentences. We then extract parallel fragments from the comparable sentences. Parallel sentence extraction is based on a parallel sentence candidate filter and classifier for parallel sentence identification. We improve it by proposing a novel filtering strategy and three novel feature sets for classification. Previous studies have found it difficult to accurately extract parallel fragments from comparable sentences. We propose an accurate parallel fragment extraction method that uses an alignment model to locate the parallel fragment candidates and an accurate lexicon-based filter to identify the truly parallel fragments. A case study on the Chinese--Japanese Wikipedia indicates that our proposed methods outperform previously proposed methods, and the parallel data extracted by our system significantly improves SMT performance.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2211796614",
    "type": "article"
  },
  {
    "title": "A Four-Tier Annotated Urdu Handwritten Text Image Dataset for Multidisciplinary Research on Urdu Script",
    "doi": "https://doi.org/10.1145/2857053",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "Prakash Choudhary; Neeta Nain",
    "corresponding_authors": "",
    "abstract": "This article introduces a large handwritten text document image corpus dataset for Urdu script named CALAM (Cursive And Language Adaptive Methodologies). The database contains unconstrained handwritten sentences along with their structural annotations for the offline handwritten text images with their XML representation. Urdu is the fourth most frequently used language in the world, but due to its complex cursive writing script and low resources, it is still a thrust area for document image analysis. Here, a unified approach is applied in the development of an Urdu corpus by collecting printed texts, handwritten texts, and demographic information of writers on a single form. CALAM contains 1,200 handwritten text images, 3,043 lines, 46,664 words, and 101,181 ligatures. For capturing maximum variance among the words and handwritten styles, data collection is distributed among six categories and 14 subcategories. Handwritten forms were filled out by 725 different writers belonging to different geographical regions, ages, and genders with diverse educational backgrounds. A structure has been designed to annotate handwritten Urdu script images at line, word, and ligature levels with an XML standard to provide a ground truth of each image at different levels of annotation. This corpus would be very useful for linguistic research in benchmarking and providing a testbed for evaluation of handwritten text recognition techniques for Urdu script, signature verification, writer identification, digital forensics, classification of printed and handwritten text, categorization of texts as per use, and so on. The experimental results of some recently developed handwritten text line segmentation techniques experimented on the proposed dataset are also presented in the article for asserting its viability and usability.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2406714201",
    "type": "article"
  },
  {
    "title": "The Rule-Based Sundanese Stemmer",
    "doi": "https://doi.org/10.1145/3195634",
    "publication_date": "2018-07-21",
    "publication_year": 2018,
    "authors": "Arie Ardiyanti Suryani; Dwi H. Widyantoro; Ayu Purwarianti; Yayat Sudaryat",
    "corresponding_authors": "",
    "abstract": "Our research proposed an iterative Sundanese stemmer by removing the derivational affixes prior to the inflexional. This scheme was chosen because, in the Sundanese affixation, a confix (one of derivational affix) is applied in the last phase of a morphological process. Moreover, most of Sundanese affixes are derivational, so removing the derivational affix as the first step is reasonable. To handle ambiguity, the last recognized affix was returned as the result. As the baseline, a Confix-Stripping Approach that applies Porter Stemmer for the Indonesian language was used. This stemmer shares similarities in terms of affix type, but uses a different stemming order. To observe whether the baseline stems the Sundanese affixed word properly, some features that were not covered by the baseline, such as the infix and allomorph removal, were added. The evaluation was done using 4,453 unique affixed words collected from Sundanese online magazines. The experiment shows that, as a whole, our stemmer outperforms the modified baseline in terms of recognized affixed type accuracy and properly stemmed affixed words. Our stemmer recognized 68.87% of the Sundanese affixed types and produced 96.79% of the correctly affixed words; the modified baseline resulted in 21.70% and 71.59%, respectively",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2884326673",
    "type": "article"
  },
  {
    "title": "Hindi EmotionNet",
    "doi": "https://doi.org/10.1145/3383330",
    "publication_date": "2020-06-07",
    "publication_year": 2020,
    "authors": "Kanika Garg; D. K. Lobiyal",
    "corresponding_authors": "",
    "abstract": "In this study, we create an emotion lexicon for the Hindi language called Hindi EmotionNet. It can assign emotional affinity to words in IndoWordNet. This lexicon contains 3,839 emotion words, with 1,246 positive and 2,399 negative words. We also introduce ambiguous (217 words) and neutral (95 words) emotions to Hindi. Positive emotion words covered nine types of positive emotions, negative emotion words covered eleven types of negative emotions, ambiguous emotion words covered seven types of ambiguous emotions, and neutral emotion words covered two neutral emotions. The proposed Hindi EmotionNet was then applied to opinion classification and emotion classification. We introduce a centrality-based approach for emotion classification that uses degree, closeness, betweenness, and page rank as centrality measures. We also created a dataset of Hindi based on screenplays, stories, and blogs in the language. We translated emotion data from SemEval 2017 into Hindi for further comparison. The proposed approach delivered promising results on opinion and emotion classification, with an accuracy of 85.78% for the former and 75.91% for the latter.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3034503310",
    "type": "article"
  },
  {
    "title": "Global Encoding for Long Chinese Text Summarization",
    "doi": "https://doi.org/10.1145/3407911",
    "publication_date": "2020-10-06",
    "publication_year": 2020,
    "authors": "Xuefeng Xi; Zhou Pi; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Text summarization is one of the significant tasks of natural language processing, which automatically converts text into a summary. Some summarization systems, for short/long English, and short Chinese text, benefit from advances in the neural encoder-decoder model because of the availability of large datasets. However, the long Chinese text summarization research has been limited to datasets of a couple of hundred instances. This article aims to explore the long Chinese text summarization task. To begin with, we construct a first large-scale, long Chinese text summarization corpus, the Long Chinese Summarization of Police Inquiry Record Text (LCSPIRT). Based on this corpus, we propose a sequence-to-sequence (Seq2Seq) model that incorporates a global encoding process with an attention mechanism. Our model achieves a competitive result on the LCSPIRT corpus compared with several benchmark methods.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3091863267",
    "type": "article"
  },
  {
    "title": "The Impact of Weighting Schemes and Stemming Process on Topic Modeling of Arabic Long and Short Texts",
    "doi": "https://doi.org/10.1145/3405843",
    "publication_date": "2020-11-12",
    "publication_year": 2020,
    "authors": "Tinghuai Ma; Raeed Al-Sabri; Lejun Zhang; Bockarie Daniel Marah; Najla Al-Nabhan",
    "corresponding_authors": "",
    "abstract": "In this article, first a comprehensive study of the impact of term weighting schemes on the topic modeling performance (i.e., LDA and DMM) on Arabic long and short texts is presented. We investigate six term weighting methods including Word count method (standard topic models), TFIDF, PMI, BDC, CLPB, and CEW. Moreover, we propose a novel combination term weighting scheme, namely, CmTLB. We utilize the mTFIDF that takes into account the missing terms and the number of the documents in which the term appears when calculating the term weight. For further robust term weight, we combine mTFIDF with two weighting methods. We evaluate CmTLB against the studied weighting schemes by the quality of the learned topics (topic visualization and topic coherence), classification, and clustering tasks. We applied weighting schemes to Latent Dirichlet allocation (LDA) and Dirichlet multinomial mixture (DMM) on eight Arabic long and short document datasets, respectively. The experiment results outline that appropriate weighting schemes can effectively improve topic modeling performance on Arabic texts. More importantly, our proposed CmTLB significantly outperforms the other weighting schemes. Secondly, we investigate whether the Arabic stemming process can improve topic modeling performance. We study the three approaches of Arabic stemming including root-based, stem-based, and statistical approaches. We also train topic models with weighting schemes on documents after applying four stemmers related to different stemming approaches. The results outline that applying the stemming process not only reduces the dimensionality of term-document matrix leading to fast estimation process, but also show enhancement of topic modeling performance both on short and long Arabic documents. Moreover, Farasa stemmer achieves the highest performance in most cases, since it prevents the ambiguity that may happen because of the blind removal of the affixes such as in root-based or stem-based stemmers.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3100443943",
    "type": "article"
  },
  {
    "title": "Deep Interactive Memory Network for Aspect-Level Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3402886",
    "publication_date": "2020-12-01",
    "publication_year": 2020,
    "authors": "Chengai Sun; Liangyu Lv; Gang Tian; Tailu Liu",
    "corresponding_authors": "",
    "abstract": "The goal of aspect-level sentiment analysis is to identify the sentiment polarity of a specific opinion target expressed; it is a fine-grained sentiment analysis task. Most of the existing works study how to better use the target information to model the sentence without using the interactive information between the sentence and target. In this article, we argue that the prediction of aspect-level sentiment polarity depends on both context and target. First, we propose a new model based on LSTM and the attention mechanism to predict the sentiment of each target in the review, the matrix-interactive attention network (M-IAN) that models target and context, respectively. M-IAN use an attention matrix to learn the interactive attention of context and target and generates the final representations of target and context. Then we introduce two gate networks based on M-IAN to build a deep interactive memory network to capture multiple interactions of target and context. The deep interactive memory network can excellently formulate specific memory for different targets, which is helpful in sentiment analysis. The experimental results of Restaurant and Laptop datasets of SemEval 2014 validate the effectiveness of our model.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3109383772",
    "type": "article"
  },
  {
    "title": "Preordering using a Target-Language Parser via Cross-Language Syntactic Projection for Statistical Machine Translation",
    "doi": "https://doi.org/10.1145/2699925",
    "publication_date": "2015-06-12",
    "publication_year": 2015,
    "authors": "Isao Goto; Masao Utiyama; Eiichiro Sumita; Sadao Kurohashi",
    "corresponding_authors": "",
    "abstract": "When translating between languages with widely different word orders, word reordering can present a major challenge. Although some word reordering methods do not employ source-language syntactic structures, such structures are inherently useful for word reordering. However, high-quality syntactic parsers are not available for many languages. We propose a preordering method using a target-language syntactic parser to process source-language syntactic structures without a source-language syntactic parser. To train our preordering model based on ITG, we produced syntactic constituent structures for source-language training sentences by (1) parsing target-language training sentences, (2) projecting constituent structures of the target-language sentences to the corresponding source-language sentences, (3) selecting parallel sentences with highly synchronized parallel structures, (4) producing probabilistic models for parsing using the projected partial structures and the Pitman-Yor process, and (5) parsing to produce full binary syntactic structures maximally synchronized with the corresponding target-language syntactic structures, using the constraints of the projected partial structures and the probabilistic models. Our ITG-based preordering model is trained using the produced binary syntactic structures and word alignments. The proposed method facilitates the learning of ITG by producing highly synchronized parallel syntactic structures based on cross-language syntactic projection and sentence selection. The preordering model jointly parses input sentences and identifies their reordered structures. Experiments with Japanese--English and Chinese--English patent translation indicate that our method outperforms existing methods, including string-to-tree syntax-based SMT, a preordering method that does not require a parser, and a preordering method that uses a source-language dependency parser.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2250043191",
    "type": "article"
  },
  {
    "title": "Lipi Gnani",
    "doi": "https://doi.org/10.1145/3387632",
    "publication_date": "2020-05-18",
    "publication_year": 2020,
    "authors": "Himanshu Kumar; A. G. Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "A Kannada OCR, called Lipi Gnani , has been designed and developed from scratch, with the motivation of it being able to convert printed text or poetry in Kannada script, without any restriction on vocabulary. The training and test sets have been collected from more than 35 books published from 1970 to 2002, and this includes books written in Halegannada and pages containing Sanskrit slokas written in Kannada script. The coverage of the OCR is nearly complete in the sense that it recognizes all punctuation marks, special symbols, and Indo-Arabic and Kannada numerals. Several minor and major original contributions have been done in developing this OCR at different processing stages, such as binarization, character segmentation, recognition, and Unicode mapping. This has created a Kannada OCR that performs as good as, and in some cases better than, Google’s Tesseract OCR, as shown by the results. To the best of our knowledge, this is the maiden report of a complete Kannada OCR, handling all issues involved. Currently, there is no dictionary-based postprocessing, and the obtained results are due solely to the recognition process. Four benchmark test databases containing scanned pages from books in Kannada, Sanskrit, Konkani, and Tulu languages, but all of them printed in Kannada script, have been created. The word-level recognition accuracy of Lipi Gnani is 5.3% higher on the Kannada dataset than that of Google’s Tesseract OCR, 8.5% higher on the Sanskrit dataset, and 23.4% higher on the datasets of Konkani and Tulu.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3027621902",
    "type": "article"
  },
  {
    "title": "Hate Speech Detection in Roman Urdu",
    "doi": "https://doi.org/10.1145/3414524",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Muhammad Moin Khan; Khurram Shahzad; Muhammad Kamran Malik",
    "corresponding_authors": "",
    "abstract": "Hate speech is a specific type of controversial content that is widely legislated as a crime that must be identified and blocked. However, due to the sheer volume and velocity of the Twitter data stream, hate speech detection cannot be performed manually. To address this issue, several studies have been conducted for hate speech detection in European languages, whereas little attention has been paid to low-resource South Asian languages, making the social media vulnerable for millions of users. In particular, to the best of our knowledge, no study has been conducted for hate speech detection in Roman Urdu text, which is widely used in the sub-continent. In this study, we have scrapped more than 90,000 tweets and manually parsed them to identify 5,000 Roman Urdu tweets. Subsequently, we have employed an iterative approach to develop guidelines and used them for generating the Hate Speech Roman Urdu 2020 corpus. The tweets in the this corpus are classified at three levels: Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another contribution, we have used five supervised learning techniques, including a deep learning technique, to evaluate and compare their effectiveness for hate speech detection. The results show that Logistic Regression outperformed all other techniques, including deep learning techniques for the two levels of classification, by achieved an F1 score of 0.906 for distinguishing between Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate speech tweets.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3189446895",
    "type": "article"
  },
  {
    "title": "An Unsupervised Approach for Sentiment Analysis on Social Media Short Text Classification in Roman Urdu",
    "doi": "https://doi.org/10.1145/3474119",
    "publication_date": "2021-11-03",
    "publication_year": 2021,
    "authors": "Toqir A. Rana; Kiran Shahzadi; Tauseef Rana; Hafiz Ahsan Arshad; Mohammad Tubishat",
    "corresponding_authors": "",
    "abstract": "During the last two decades, sentiment analysis, also known as opinion mining, has become one of the most explored research areas in Natural Language Processing (NLP) and data mining. Sentiment analysis focuses on the sentiments or opinions of consumers expressed over social media or different web sites. Due to exposure on the Internet, sentiment analysis has attracted vast numbers of researchers over the globe. A large amount of research has been conducted in English, Chinese, and other languages used worldwide. However, Roman Urdu has been neglected despite being the third most used language for communication in the world, covering millions of users around the globe. Although some techniques have been proposed for sentiment analysis in Roman Urdu, these techniques are limited to a specific domain or developed incorrectly due to the unavailability of language resources available for Roman Urdu. Therefore, in this article, we are proposing an unsupervised approach for sentiment analysis in Roman Urdu. First, the proposed model normalizes the text to overcome spelling variations of different words. After normalizing text, we have used Roman Urdu and English opinion lexicons to correctly identify users’ opinions from the text. We have also incorporated negation terms and stemming to assign polarities to each extracted opinion. Furthermore, our model assigns a score to each sentence on the basis of the polarities of extracted opinions and classifies each sentence as positive, negative, or neutral. In order to verify our approach, we have conducted experiments on two publicly available datasets for Roman Urdu and compared our approach with the existing model. Results have demonstrated that our approach outperforms existing models for sentiment analysis tasks in Roman Urdu. Furthermore, our approach does not suffer from domain dependency.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3210214031",
    "type": "article"
  },
  {
    "title": "Leveraging Multilingual News Websites for Building a Kurdish Parallel Corpus",
    "doi": "https://doi.org/10.1145/3511806",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Sina Ahmadi; Hossein Hassani; Daban Q. Jaff",
    "corresponding_authors": "",
    "abstract": "Machine translation has been a major motivation of development in natural language processing. Despite the burgeoning achievements in creating more efficient machine translation systems, thanks to deep learning methods, parallel corpora have remained indispensable for progress in the field. In an attempt to create parallel corpora for the Kurdish language, in this article, we describe our approach in retrieving potentially alignable news articles from multi-language websites and manually align them across dialects and languages based on lexical similarity and transliteration of scripts. We present a corpus containing 12,327 translation pairs in the two major dialects of Kurdish, Sorani and Kurmanji. We also provide 1,797 and 650 translation pairs in English-Kurmanji and English-Sorani. The corpus is publicly available under the CC BY-NC-SA 4.0 license. 1",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3091683906",
    "type": "article"
  },
  {
    "title": "Breaking the Curse of Class Imbalance: Bangla Text Classification",
    "doi": "https://doi.org/10.1145/3511601",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Md. Rafi-Ur-Rashid; M. Mahbub; Muhammad Abdullah Adnan",
    "corresponding_authors": "",
    "abstract": "This article addresses the class imbalance issue in a low-resource language called Bengali. As a use-case, we choose one of the most fundamental NLP tasks, i.e., text classification, where we utilize three benchmark text corpora: fake-news dataset, sentiment analysis dataset, and song lyrics dataset. Each of them contains a critical class imbalance. We attempt to tackle the problem by applying several strategies that include data augmentation with synthetic samples via text and embedding generation in order to augment the proportion of the minority samples. Moreover, we apply ensembling of deep learning models by subsetting the majority samples. Additionally, we enforce the focal loss function for class-imbalanced data classification. We also apply the outlier detection technique, data resampling, and hidden feature extraction to improve the minority-f1 score. All of our experimentations are entirely focused on textual content analysis, which results in a more than 90% minority f1 score for each of the three tasks. It is an excellent outcome on such highly class-imbalanced datasets.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4210609034",
    "type": "article"
  },
  {
    "title": "The Comparison of Language Models with a Novel Text Filtering Approach for Turkish Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3557892",
    "publication_date": "2022-08-17",
    "publication_year": 2022,
    "authors": "Zekeriya Anıl Güven",
    "corresponding_authors": "Zekeriya Anıl Güven",
    "abstract": "Today, comments can be made on many topics on web platforms with the development of the internet. Analyzing the data of these comments is essential for companies and data scientists. There are many methods for analyzing data. Recently, language models have also been used in many studies for sentiment analysis or text classification. In this study, Turkish sentiment analysis is performed using language models on hotel and movie review datasets. The language models are chosen because they are rarely used in Turkish literature. The pre-trained BERT, ALBERT, ELECTRA, and DistilBERT models for the Turkish language are trained and tested with these datasets. In addition, a text filtering method, which removes the words that can provide the opposition sentiment in the positive or negative labeled text, is proposed for sentiment analysis. These datasets obtained by this method are also retrained with language models and the accuracy values of their models are measured. The results of this study are compared with previous studies using the same datasets. As a result of the analysis, the accuracy values obtain state-of-the-art results with language models compared to previous studies. The best performance has been achieved by training the ELECTRA language model using the proposed text filtering method.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4292092922",
    "type": "article"
  },
  {
    "title": "An Object Localization-based Dense Image Captioning Framework in Hindi",
    "doi": "https://doi.org/10.1145/3558391",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Santosh Kumar Mishra; Harshit; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Dense image captioning is a task that requires generating localized captions in natural language for multiple regions of an image. This task leverages its functionalities from both computer vision for recognizing regions in an image and natural language processing for generating captions. Numerous works have been carried out on dense image captioning for resource-rich languages like English; however, resource-poor languages like Hindi are not explored. Hindi is one of India’s official languages and is the third most spoken language in the world. This article proposes a dense image captioning model to describe different segments of an image by generating more than one caption in the Hindi language. For localized image recognition and language modeling, we employ Faster R-CNN and Long Short-Term Memory (LSTM), respectively. Apart from this, we conduct various experiments using gated recurrent units (GRUs) and attention mechanism. By manually translating the well-known Visual Genome dataset from English to Hindi, a dataset has been created for dense image captioning in Hindi. The experiments conducted on the newly constructed Hindi dense image captioning dataset illustrate the efficacy of the proposed method over the state-of-the-art methods.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4292595909",
    "type": "article"
  },
  {
    "title": "Active Learning for Name Entity Recognition with External Knowledge",
    "doi": "https://doi.org/10.1145/3593023",
    "publication_date": "2023-04-18",
    "publication_year": 2023,
    "authors": "Ying Ma; Yu Zhang; Arun Kumar Sangaiah; Ming Yan; Guoqi Li; Tian Wang",
    "corresponding_authors": "",
    "abstract": "Named Entity Recognition (NER) is an important task in knowledge extraction, which targets extracting structural information from unstructured text. To fully employ the prior-knowledge of the pre-trained language models, some research works formulate the NER task into the machine reading comprehension form (MRC-form) to enhance their model generalization capability of commonsense knowledge. However, this transformation still faces the data-hungry issue with limited training data for the specific NER tasks. To address the low-resource issue in NER, we introduce a method named active multi-task-based NER (AMT-NER), which is a two-stage multi-task active learning training model. Specifically, A multi-task learning module is first introduced into AMT-NER to improve its representation capability in low-resource NER tasks. Then, a two-stage training strategy is proposed to optimize AMT-NER multi-task learning. An associated task of Natural Language Inference (NLI) is also employed to enhance its commonsense knowledge further. More importantly, AMT-NER introduces an active learning module, uncertainty selective, to actively filter training data to help the NER model learn efficiently. Besides, we also find different external supportive data under different pipelines improves model performance differently in the NER tasks. Extensive experiments are performed to show the superiority of our method, which also proves our findings that the introduction of external knowledge is significant and effective in the MRC-form NER tasks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4366274281",
    "type": "article"
  },
  {
    "title": "Adversarial Multi-task Learning for Efficient Chinese Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3603626",
    "publication_date": "2023-06-06",
    "publication_year": 2023,
    "authors": "Yibo Yan; Peng Zhu; Dawei Cheng; Fangzhou Yang; Yifeng Luo",
    "corresponding_authors": "",
    "abstract": "Named entity recognition (NER) is a fundamental task for information extraction applications. NER is challenging because of semantic ambiguities in academic literature, especially for non-Latin languages. Besides word semantic information, recognizing Chinese named entities needs to consider word boundary information, as words contained in Chinese texts are not separated with spaces. Leveraging word boundary information could help to determine entity boundaries and thus improve entity recognition performance. In this article, we propose to combine word boundary information and semantic information for named entity recognition based on multi-task adversarial learning. Specifically, we learn commonly shared boundary information of entities from multiple kinds of tasks, including Chinese word segmentation (CWS), part-of-speech (POS) tagging, and entity recognition, with adversarial learning. We learn task-specific semantic information of words from these tasks and combine the learned boundary information with the semantic information to improve entity recognition with multi-task learning. We then propose a compression method based on improved clustering to accelerate the proposed model. We conduct extensive experiments on four public benchmark datasets and two private datasets, compared with state-of-the-art baseline models, and the experimental results demonstrate that our model achieves considerable performance improvements on various evaluation datasets.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4379513518",
    "type": "article"
  },
  {
    "title": "Construction of Mizo: English Parallel Corpus for Machine Translation",
    "doi": "https://doi.org/10.1145/3610404",
    "publication_date": "2023-07-21",
    "publication_year": 2023,
    "authors": "Thangkhanhau Haulai; Jamal Hussain",
    "corresponding_authors": "",
    "abstract": "Parallel corpus is a key component of statistical and Neural Machine Translation (NMT). While most research focuses on machine translation, corpus creation studies are limited for many languages, and no research paper on a Mizo–English corpus exists yet. A high-quality parallel corpus is required for Natural Language Processing activities including machine translation, Chatbots, Transliteration, and Cross-Language Information Retrieval. This work aims to investigate parallel corpus creation techniques and apply them to the Mizo–English language pair. Another goal is to test machine translation on the newly constructed corpus. We contributed to LF Aligner tool to support Mizo language for Mizo sentence alignment in corpus development. Our effort created the first large-scale Mizo–English parallel corpus with over 529K sentences. The pre-processed corpus was used for Mizo-to-English NMT. It was evaluated using BLEU, Character F1 Score (ChrF), and Translation Edit Rate (TER) scores. Our system achieved BLEU 45.08, ChrF 65.36, and TER 41.16, setting a new benchmark for Mizo-to-English translation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4385065630",
    "type": "article"
  },
  {
    "title": "Enhancing Low-Resource Sanskrit-Hindi Translation through Deep Learning with Ayurvedic Text",
    "doi": "https://doi.org/10.1145/3637439",
    "publication_date": "2023-12-15",
    "publication_year": 2023,
    "authors": "Nandini Sethi; Amita Dev; Poonam Bansal; Deepak Kumar Sharma; Deepak Gupta",
    "corresponding_authors": "",
    "abstract": "The Machine Translation (MT) community is interested in Neural Machine Translation (NMT) because it can persevere continues data over a range of input and output phrase lengths. The NMT system employs a cutting-edge attention mechanism that enables concentrating on a particular input vector of the input sentence rather than obtaining an entire vector for every input sentence. While the Neural approach enhances translation superiority by addressing issues with enduring dependencies and making contextual analysis more accessible, it also requires a sufficient parallel corpus for training, which is challenging in dialects with limited resources. The primary goal of this research is to tackle the distinctive difficulties associated with translating Ayurvedic texts using neural machine translation (NMT). Ayurvedic texts pose unique challenges due to their specialized vocabulary and domain-specific terminology, necessitating a customized approach to achieve precise and meaningful translations. To create a reliable neural machine translation system for the Sanskrit-Hindi language pair, this study utilized a manually created parallel corpus containing Ayurvedic texts and general domain texts. With the help of this corpus, two NMT models were constructed: an attention-based transformer model (NMT2) and an attention-based encoder-decoder recurrent neural network (NMT1). These models integrated deep neural network techniques to optimize the translation process and overcome the constraints of limited training data and complex grammatical structures of Sanskrit. For Sanskrit-to-Hindi, the transformer-based approach received a higher average BLEU score of 48.1 percent and a uni-gram BLEU Score of 56.86 percent.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4389785737",
    "type": "article"
  },
  {
    "title": "PsyChatbot: A Psychological Counseling Agent Towards Depressed Chinese Population Based on Cognitive Behavioural Therapy",
    "doi": "https://doi.org/10.1145/3676962",
    "publication_date": "2024-07-05",
    "publication_year": 2024,
    "authors": "T. Chen; Ying Shen; Xuri Chen; Lin Zhang",
    "corresponding_authors": "",
    "abstract": "Nowadays, depression has been widely concerned due to the growing depressed population. Depression is a global mental problem, the worst case of which can lead to suicide. However, factors such as high treatment costs and social stigma prevent people from obtaining effective treatments. Chatbot technology is one of the main attempts to solve the problem. But as far as we know, existing chatbot systems designed for depressed people are still sporadic, and most of them have some non-negligible limitations. Specifically, existing systems simply guide users to release their negative emotions or provide some general advice. They cannot offer personalized advice for users’ specific problems. In addition, most of them only support English speakers, despite the fact that depressed Chinese constitute a large population. Psychological counseling systems for the depressed Chinese population with improved responsiveness are temporarily lacking. As an attempt to fill in the research gap to some extent, we design a novel Chinese psychological chatbot system, namely PsyChatbot. First, we establish a counseling dialogue framework based on Cognitive Behavioral Therapy (CBT), which guides users to reflect on themselves and helps them discover their negative perceptions. Then, we propose a retrieval-based Q&amp;A algorithm to provide suitable suggestions for users’ specific problems. Last but not least, we construct a large-scale Chinese counseling Q&amp;A corpus, which contains nearly 89,000 psychological Q&amp;A triples. Experimental results have demonstrated the effectiveness of PsyChatbot. The source code and data has been released at https://github.com/slptongji/PsyChatbot.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400360789",
    "type": "article"
  },
  {
    "title": "A CDT-Styled End-to-End Chinese Discourse Parser",
    "doi": "https://doi.org/10.1145/3099557",
    "publication_date": "2017-07-13",
    "publication_year": 2017,
    "authors": "Fang Kong; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Discourse parsing is a challenging task and plays a critical role in discourse analysis. Since the release of the Rhetorical Structure Theory Discourse Treebank and the Penn Discourse Treebank, the research on English discourse parsing has attracted increasing attention and achieved considerable success in recent years. At the same time, some preliminary research on certain subtasks about discourse parsing for other languages, such as Chinese, has been conducted. In this article, we present an end-to-end Chinese discourse parser with the Connective-Driven Dependency Tree scheme, which consists of multiple components in a pipeline architecture, such as the elementary discourse unit (EDU) detector, discourse relation recognizer, discourse parse tree generator, and attribution labeler. In particular, the attribution labeler determines two attributions (i.e., sense and centering) for every nonterminal node (i.e., discourse relation) in the discourse parse trees. Systematically, our parser detects all EDUs in a free text, generates the discourse parse tree in a bottom-up way, and determines the sense and centering attributions for all nonterminal nodes by traversing the discourse parse tree. Comprehensive evaluation on the Connective-Driven Dependency Treebank corpus from both component-wise and error-cascading perspectives is conducted to illustrate how each component performs in isolation, and how the pipeline performs with error propagation. Finally, it shows that our end-to-end Chinese discourse parser achieves an overall F1 score of 20% with full automation.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2558814298",
    "type": "article"
  },
  {
    "title": "A Supervised Learning Approach for Authorship Attribution of Bengali Literary Texts",
    "doi": "https://doi.org/10.1145/3099473",
    "publication_date": "2017-08-16",
    "publication_year": 2017,
    "authors": "Shanta Phani; Shibamouli Lahiri; Arindam Biswas",
    "corresponding_authors": "",
    "abstract": "Authorship Attribution is a long-standing problem in Natural Language Processing. Several statistical and computational methods have been used to find a solution to this problem. In this article, we have proposed methods to deal with the authorship attribution problem in Bengali. More specifically, we proposed a supervised framework consisting of lexical and shallow features and investigated the possibility of using topic-modeling-inspired features, to classify documents according to their authors. We have created a corpus from nearly all the literary works of three eminent Bengali authors, consisting of 3,000 disjoint samples. Our models showed better performance than the state-of-the-art, with more than 98% test accuracy for the shallow features and 100% test accuracy for the topic-based features. Further experiments with GloVe vectors [Pennington et al. 2014] showed comparable results, but flexible patterns based on content words and high-frequency words [Schwartz et al. 2013] failed to perform as well as expected.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2745704873",
    "type": "article"
  },
  {
    "title": "Improving Word Embedding Coverage in Less-Resourced Languages Through Multi-Linguality and Cross-Linguality",
    "doi": "https://doi.org/10.1145/3273931",
    "publication_date": "2018-12-17",
    "publication_year": 2018,
    "authors": "Md Shad Akhtar; Palaash Sawant; Sukanta Sen; Asif Ekbal; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "In the era of deep learning-based systems, efficient input representation is one of the primary requisites in solving various problems related to Natural Language Processing (NLP), data mining, text mining, and the like. Absence of adequate representation for an input introduces the problem of data sparsity, and it poses a great challenge to solve the underlying problem. The problem is more intensified with resource-poor languages due to the absence of a sufficiently large corpus required to train a word embedding model. In this work, we propose an effective method to improve the word embedding coverage in less-resourced languages by leveraging bilingual word embeddings learned from different corpora. We train and evaluate deep Long Short Term Memory (LSTM)-based architecture and show the effectiveness of the proposed approach for two aspect-level sentiment analysis tasks (i.e., aspect term extraction and sentiment classification). The neural network architecture is further assisted by hand-crafted features for prediction. We apply the proposed model in two experimental setups: multi-lingual and cross-lingual. Experimental results show the effectiveness of the proposed approach against the state-of-the-art methods.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2903702626",
    "type": "article"
  },
  {
    "title": "POS Tag-enhanced Coarse-to-fine Attention for Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3321124",
    "publication_date": "2019-04-22",
    "publication_year": 2019,
    "authors": "Yongjing Yin; Jinsong Su; Huating Wen; Jiali Zeng; Yang Liu; Yidong Chen",
    "corresponding_authors": "",
    "abstract": "Although neural machine translation (NMT) has certain capability to implicitly learn semantic information of sentences, we explore and show that Part-of-Speech (POS) tags can be explicitly incorporated into the attention mechanism of NMT effectively to yield further improvements. In this article, we propose an NMT model with tag-enhanced attention mechanism. In our model, NMT and POS tagging are jointly modeled via multi-task learning. Besides following common practice to enrich encoder annotations by introducing predicted source POS tags, we exploit predicted target POS tags to refine attention model in a coarse-to-fine manner. Specifically, we first implement a coarse attention operation solely on source annotations and target hidden state, where the produced context vector is applied to update target hidden state used for target POS tagging. Then, we perform a fine attention operation that extends the coarse one by further exploiting the predicted target POS tags. Finally, we facilitate word prediction by simultaneously utilizing the context vector from fine attention and the predicted target POS tags. Experimental results and further analyses on Chinese-English and Japanese-English translation tasks demonstrate the superiority of our proposed model over the conventional NMT models. We release our code at https://github.com/middlekisser/PEA-NMT.git.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2941839832",
    "type": "article"
  },
  {
    "title": "Identifying and Analyzing Different Aspects of English-Hindi Code-Switching in Twitter",
    "doi": "https://doi.org/10.1145/3314935",
    "publication_date": "2019-07-23",
    "publication_year": 2019,
    "authors": "Koustav Rudra; Ashish Sharma; Kalika Bali; Monojit Choudhury; Niloy Ganguly",
    "corresponding_authors": "",
    "abstract": "Code-switching or the juxtaposition of linguistic units from two or more languages in a single utterance, has, in recent times, become very common in text, thanks to social media and other computer mediated forms of communication. In this exploratory study of English-Hindi code-switching on Twitter, we automatically create a large corpus of code-switched tweets and devise techniques to identify the relationship between successive components in a code-switched tweet. More specifically, we identify pragmatic functions such as narrative-evaluative, negative reinforcement, translation or semantically equivalent statements, and so on characterizing the relation between successive components. We analyze the difference/similarity between switching patterns in code-switched and monolingual multi-component tweets. We observe strong dominance of narrative-evaluative (non-opinion to opinion or vice versa) switching in case of both code-switched and monolingual multi-component tweets in around 40% of cases. Polarity switching appears to be a prevalent switching phenomenon (10%) specifically in code-switched tweets (three to four times higher than monolingual multi-component tweets) where preference of expressing negative sentiment in Hindi is approximately twice compared to English. Positive reinforcement appears to be an important pragmatic function for English multi-component tweets, whereas negative reinforcement plays a key role for Devanagari multi-component tweets. Our results also indicate that the extent and nature of code-switching also strongly depend on the topic (sports, politics, etc.) of discussion.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2962857395",
    "type": "article"
  },
  {
    "title": "Printed Text Image Database for Sindhi OCR",
    "doi": "https://doi.org/10.1145/2846093",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "Dil Nawaz Hakro; Abdullah Zawawi Talib",
    "corresponding_authors": "",
    "abstract": "Document Image Understanding (DIU) and Electronic Document Management are active fields of research involving image understanding, interpretation, efficient handling, and routing of documents as well as their retrieval. Research on most of the noncursive scripts (Latin) has matured, whereas research on the cursive (connected) scripts is still moving toward perfection. Many researchers are currently working on the cursive scripts (Arabic and other scripts adopting it) around the world so that the difficulties and challenges in document understanding and handling of these scripts can be overcome. Sindhi script has the largest extension of the original Arabic alphabet among languages adopting the Arabic script; it contains 52 characters, compared to 28 characters in the original Arabic alphabet, in order to accommodate more sounds for the language. There are 24 differentiating characters with some possessing four dots. For Sindhi OCR research and development, a database is needed for training and testing of Sindhi text images. We have developed a large database containing over 4 billion words and 15 billion characters in 150 various fonts in four font weights and four styles. The database contents were collected from various sources including websites, books, and theses. A custom-built application was also developed to create a text image from a text document that supports various fonts and sizes. The database considers words, characters, characters with spaces, and lines. The database is freely available as a partial or full database by sending an email to one of the authors.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2401776823",
    "type": "article"
  },
  {
    "title": "Approaches to Temporal Expression Recognition in Hindi",
    "doi": "https://doi.org/10.1145/2629574",
    "publication_date": "2015-01-30",
    "publication_year": 2015,
    "authors": "Nitin Ramrakhiyani; Prasenjit Majumder",
    "corresponding_authors": "",
    "abstract": "Temporal annotation of plain text is considered a useful component of modern information retrieval tasks. In this work, different approaches for identification and classification of temporal expressions in Hindi are developed and analyzed. First, a rule-based approach is developed, which takes plain text as input and based on a set of hand-crafted rules, produces a tagged output with identified temporal expressions. This approach performs with a strict F1-measure of 0.83. In another approach, a CRF-based classifier is trained with human tagged data and is then tested on a test dataset. The trained classifier identifies the time expressions from plain text and further classifies them to various classes. This approach performs with a strict F1-measure of 0.78. Next, the CRF is replaced by an SVM-based classifier and the same experiment is performed with the same features. This approach is shown to be comparable to the CRF and performs with a strict F1-measure of 0.77. Using the rule base information as an additional feature enhances the performances to 0.86 and 0.84 for the CRF and SVM respectively. With three different comparable systems performing the extraction task, merging them to take advantage of their positives is the next step. As the first merge experiment, rule-based tagged data is fed to the CRF and SVM classifiers as additional training data. Evaluation results report an increase in F1-measure of the CRF from 0.78 to 0.8. Second, a voting-based approach is implemented, which chooses the best class for each token from the outputs of the three approaches. This approach results in the best performance for this task with a strict F1-measure of 0.88. In this process a reusable gold standard dataset for temporal tagging in Hindi is also developed. Named the ILTIMEX2012 corpus, it consists of 300 manually tagged Hindi news documents.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2405243274",
    "type": "article"
  },
  {
    "title": "Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency",
    "doi": "https://doi.org/10.1145/3314934",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Bingzhen Wei; Xuancheng Ren; Yi Zhang; Xiaoyan Cai; Qi Su; Xu Sun",
    "corresponding_authors": "",
    "abstract": "Abstractive text summarization is a highly difficult problem, and the sequence-to-sequence model has shown success in improving the performance on the task. However, the generated summaries are often inconsistent with the source content in semantics. In such cases, when generating summaries, the model selects semantically unrelated words with respect to the source content as the most probable output. The problem can be attributed to heuristically constructed training data, where summaries can be unrelated to the source content, thus containing semantically unrelated words and spurious word correspondence. In this article, we propose a regularization approach for the sequence-to-sequence model and make use of what the model has learned to regularize the learning objective to alleviate the effect of the problem. In addition, we propose a practical human evaluation method to address the problem that the existing automatic evaluation method does not evaluate the semantic consistency with the source content properly. Experimental results demonstrate the effectiveness of the proposed approach, which outperforms almost all the existing models. Especially, the proposed approach improves the semantic consistency by 4% in terms of human evaluation.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2964299493",
    "type": "article"
  },
  {
    "title": "<i>StyloThai:</i>",
    "doi": "https://doi.org/10.1145/3365832",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Raheem Sarwar; Thanasarn Porthaveepong; Attapol Rutherford; Thanawin Rakthanmanon; Sarana Nutanong",
    "corresponding_authors": "",
    "abstract": "Authorship identification helps to identify the true author of a given anonymous document from a set of candidate authors. The applications of this task can be found in several domains, such as law enforcement agencies and information retrieval. These application domains are not limited to a specific language, community, or ethnicity. However, most of the existing solutions are designed for English, and a little attention has been paid to Thai. These existing solutions are not directly applicable to Thai due to the linguistic differences between these two languages. Moreover, the existing solution designed for Thai is unable to (i) handle outliers in the dataset, (ii) scale when the size of the candidate authors set increases, and (iii) perform well when the number of writing samples for each candidate author is low. We identify a stylometric feature space for the Thai authorship identification task. Based on our feature space, we present an authorship identification solution that uses the probabilistic k nearest neighbors classifier by transforming each document into a collection of point sets. Specifically, this document transformation allows us to (i) use set distance measures associated with an outlier handling mechanism, (ii) capture stylistic variations within a document, and (iii) produce multiple predictions for a query document. We create a new Thai authorship identification corpus containing 547 documents from 200 authors, which is significantly larger than the corpus used by the existing study (an increase of 32 folds in terms of the number of candidate authors). The experimental results show that our solution can overcome the limitations of the existing solution and outperforms all competitors with an accuracy level of 91.02%. Moreover, we investigate the effectiveness of each stylometric features category with the help of an ablation study. We found that combining all categories of the stylometric features outperforms the other combinations. Finally, we cross compare the feature spaces and classification methods of all solutions. We found that (i) our solution can scale as the number of candidate authors increases, (ii) our method outperforms all the competitors, and (iii) our feature space provides better performance than the feature space used by the existing study.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3014540851",
    "type": "article"
  },
  {
    "title": "Joint Model of Entity Recognition and Relation Extraction with Self-attention Mechanism",
    "doi": "https://doi.org/10.1145/3387634",
    "publication_date": "2020-05-23",
    "publication_year": 2020,
    "authors": "Maofu Liu; Yukun Zhang; Wenjie Li; Donghong Ji",
    "corresponding_authors": "",
    "abstract": "In recent years, the joint model of entity recognition (ER) and relation extraction (RE) has attracted more and more attention in the healthcare and medical domains. However, there are some problems with the prior work. The joint model cannot extract all the relations for a specific entity, and the majority of joint models heavily rely on complex artificial features or professional natural language processing (NLP) tools. In this article, we construct a novel joint model that can simultaneously extract all medical entities and relations from medicine Chinese instructions. Moreover, the self-attention mechanism is introduced to the joint model to learn word intra-sentence dependencies. The proposed model is evaluated using a medicine Chinese instruction dataset that we collect and an open dataset provided in CoNLL-2004. Experimental results show that the model with self-attention achieves the state-of-the-art performance.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3029589014",
    "type": "article"
  },
  {
    "title": "Robust Arabic Text Categorization by Combining Convolutional and Recurrent Neural Networks",
    "doi": "https://doi.org/10.1145/3390092",
    "publication_date": "2020-07-01",
    "publication_year": 2020,
    "authors": "Mohamed Seghir Hadj Ameur; Riadh Belkebir; Ahmed Guessoum",
    "corresponding_authors": "",
    "abstract": "Text Categorization is an important task in the area of Natural Language Processing (NLP). Its goal is to learn a model that can accurately classify any textual document for a given language into one of a set of predefined categories. In the context of the Arabic language, several approaches have been proposed to tackle this problem, many of which are based on the bag-of-words assumption. Even though these methods usually produce good results for the classification task, they often fail to capture contextual dependencies from textual data. On the other hand, deep learning architectures that are usually based on Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs) do not suffer from such a limitation and have recently shown very promising results in various NLP applications. In this work, we use deep learning models that combine RNN and CNN for the task of Arabic text categorization using static, dynamic, and fine-tuned word embeddings. The experimental results reported on the Open Source Arabic Corpora (OSAC) dataset have shown the effectiveness and high performance of our proposed models.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3038324521",
    "type": "article"
  },
  {
    "title": "A Survey of the Model Transfer Approaches to Cross-Lingual Dependency Parsing",
    "doi": "https://doi.org/10.1145/3383772",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Ayan Kumar Das; Sudeshna Sarkar",
    "corresponding_authors": "",
    "abstract": "Cross-lingual dependency parsing approaches have been employed to develop dependency parsers for the languages for which little or no treebanks are available using the treebanks of other languages. A language for which the cross-lingual parser is developed is usually referred to as the target language and the language whose treebank is used to train the cross-lingual parser model is referred to as the source language. The cross-lingual parsing approaches for dependency parsing may be broadly classified into three categories: model transfer, annotation projection, and treebank translation. This survey provides an overview of the various aspects of the model transfer approach of cross-lingual dependency parsing. In this survey, we present a classification of the model transfer approaches based on the different aspects of the method. We discuss some of the challenges associated with cross-lingual parsing and the techniques used to address these challenges. In order to address the difference in vocabulary between two languages, some approaches use only non-lexical features of the words to train the models while others use shared representations of the words. Some approaches address the morphological differences by chunk-level transfer rather than word-level transfer. The syntactic differences between the source and target languages are sometimes addressed by transforming the source language treebanks or by combining the resources of multiple source languages. Besides cross-lingual transfer parser models may be developed for a specific target language or it may be trained to parse sentences of multiple languages. With respect to the above-mentioned aspects, we look at the different ways in which the methods can be classified. We further classify and discuss the different approaches from the perspective of the corresponding aspects. We also demonstrate the performance of the transferred models under different settings corresponding to the classification aspects on a common dataset.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3038734866",
    "type": "article"
  },
  {
    "title": "Role of Discourse Information in Urdu Sentiment Classification",
    "doi": "https://doi.org/10.1145/3300050",
    "publication_date": "2019-05-21",
    "publication_year": 2019,
    "authors": "Muhammad Awais; Muhammad Shoaib",
    "corresponding_authors": "",
    "abstract": "In computational linguistics, sentiment analysis refers to the classification of opinions in a positive class or a negative class. There exist a lot of different methods for sentiment analysis of the English language, but the literature lacks the availability of methods and techniques for Urdu, which is the largely spoken language in the South Asian sub-continent and the national language of Pakistan. The currently available techniques, such as adjective count method known as Bag of Words (BoW), is not sufficient for classification of complex sentiment written in the Urdu language. Also, the performance of available machine-learning techniques (with legacy features), for classification of Urdu sentiments, are not comparable with the achieved accuracy of other languages. In the case of the English language, the discourse information (sub-sentence-level information) boosts the performance of both the BoW method and machine-learning techniques, but there are very few works available that have tested the context-level information for the sentiment analysis of the Urdu language. This research aims to extract the discourse information from the Urdu sentiments and utilise the discourse information to improve the performance and reduce the error rate of existing techniques for Urdu Sentiment classification. The proposed solution extracts the discourse information, suggests a new set of features for machine-learning techniques, and introduces a set of rules to extend the capabilities of the BoW model. The results show that the task has been enhanced significantly and the performance metrics such as recall, precision, and accuracy are increased by 31.25%, 8.46%, and 21.6%, respectively. In future, the proposed technique can be extended to sentiments with more than two sub-opinions, such as for blogs, reviews, and TV talk shows.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3041518834",
    "type": "article"
  },
  {
    "title": "Extracting Arabic Causal Relations Using Linguistic Patterns",
    "doi": "https://doi.org/10.1145/2800786",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "Jawad Sadek; Farid Meziane",
    "corresponding_authors": "",
    "abstract": "Identifying semantic relations is a crucial step in discourse analysis and is useful for many applications in both language and speech technology. Automatic detection of Causal relations therefore has gained popularity in the literature within different frameworks. The aim of this article is the automatic detection and extraction of Causal relations that are explicitly expressed in Arabic texts. To fulfill this goal, a Pattern Recognizer model was developed to signal the presence of cause--effect information within sentences from nonspecific domain texts. This model incorporates approximately 700 linguistic patterns so that parts of the sentence representing the cause and those representing the effect can be distinguished. The patterns were constructed based on different sets of syntactic features by analyzing a large untagged Arabic corpus. In addition, the model was boosted with three independent algorithms to deal with certain types of grammatical particles that indicate causation. With this approach, the proposed model achieved an overall recall of 81% and a precision of 78%. Evaluation results revealed that the justification particles play a key role in detecting Causal relations. To the best of our knowledge, no previous studies have been dedicated to dealing with this type of relation in the Arabic language.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2296064179",
    "type": "article"
  },
  {
    "title": "Boosted Web Named Entity Recognition via Tri-Training",
    "doi": "https://doi.org/10.1145/2963100",
    "publication_date": "2016-10-14",
    "publication_year": 2016,
    "authors": "Chien-Lung Chou; Chia‐Hui Chang; Ya-Yun Huang",
    "corresponding_authors": "",
    "abstract": "Named entity extraction is a fundamental task for many natural language processing applications on the web. Existing studies rely on annotated training data, which is quite expensive to obtain large datasets, limiting the effectiveness of recognition. In this research, we propose a semisupervised learning approach for web named entity recognition (NER) model construction via automatic labeling and tri-training. The former utilizes structured resources containing known named entities for automatic labeling, while the latter makes use of unlabeled examples to improve the extraction performance. Since this automatically labeled training data may contain noise, a self-testing procedure is used as a follow-up to remove low-confidence annotation and prepare higher-quality training data. Furthermore, we modify tri-training for sequence labeling and derive a proper initialization for large dataset training to improve entity recognition. Finally, we apply this semisupervised learning framework for person name recognition, business organization name recognition, and location name extraction. In the task of Chinese NER, an F-measure of 0.911, 0.849, and 0.845 can be achieved, for person, business organization, and location NER, respectively. The same framework is also applied for English and Japanese business organization name recognition and obtains models with performance of a 0.832 and 0.803 F-measure.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2530283981",
    "type": "article"
  },
  {
    "title": "Context-Dependent Sequence-to-Sequence Turkish Spelling Correction",
    "doi": "https://doi.org/10.1145/3383200",
    "publication_date": "2020-04-17",
    "publication_year": 2020,
    "authors": "Osman Büyük",
    "corresponding_authors": "Osman Büyük",
    "abstract": "In this article, we make use of sequence-to-sequence (seq2seq) models for spelling correction in the agglutinative Turkish language. In the baseline system, misspelled and target words are split into their letters and the letter sequences are fed into the seq2seq model. We prefer letters as the unit of the model due to the agglutinative nature of Turkish, which results in an impractical dictionary size when words are used as a dictionary unit. In order to improve the baseline performance, we incorporate right and left context of the misspelled words. All context words are represented with their first three consonants in the context-dependent model. We train the seq2seq models using a large text corpus collected automatically from the Internet. The corpus contains approximately 4 million sentences. We randomly introduce substitution, deletion, and insertion spelling errors to the words in the corpus. We test the performance of the proposed context-dependent seq2seq model using synthetic and realistic test sets. The synthetic test set is constructed similar to the training set. The realistic test set contains human-made misspellings from Twitter messages. In the experiments, we observed that the proposed context-dependent model performs significantly better than the baseline system. Its correction accuracy reaches 94% on the synthetic dataset. Additionally, the proposed method provides 2.1% absolute improvement over a state-of-the-art Turkish spelling correction system on the Twitter test set.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3025357899",
    "type": "article"
  },
  {
    "title": "Movie Recommendation System to Solve Data Sparsity Using Collaborative Filtering Approach",
    "doi": "https://doi.org/10.1145/3459091",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "R. Lavanya; B. Bharathi",
    "corresponding_authors": "",
    "abstract": "With the increase in numbers of multimedia technologies around us, movies and videos on social media and OTT platforms are growing, making it confusing for users to decide which one to watch for. For this, movie recommendation systems are widely used. It has been observed that two-thirds of the films watched on Netflix are the recommended ones to its users. The target of this work is to use implicit feedback given by other users to recommend movies, i.e., ratings given by them. Implicit feedback will help to enhance Data Sparsity as for a replacement logged-in user, the system won't have details of their past liked movies. So, matching the similarity with other users is often a plus point to recommend movies that they would like. The anticipated result will depend upon the positive attitude; i.e., if the predicted rating is high, then it'll be recommended; otherwise it'll not be recommended. The performance of the methodology is measured with accuracy and precision values for different strategies. It gives the best accuracy and highest precision values using Logistic Regression (LR) and lowest recall value as compared to other algorithms. This technique gives an accuracy, precision, and recall value of 81.9%, 69.82%, and 32.5%, respectively, using LR.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3185841886",
    "type": "article"
  },
  {
    "title": "A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person",
    "doi": "https://doi.org/10.1145/3447651",
    "publication_date": "2021-11-02",
    "publication_year": 2021,
    "authors": "Rajat Subhra Bhowmick; Isha Ganguli; Jayanta Paul; Jaya Sil",
    "corresponding_authors": "",
    "abstract": "In today’s era of digitization, social media platforms play a significant role in networking and influencing the perception of the general population. Social network sites have recently been used to carry out harmful attacks against individuals, including political and theological figures, intellectuals, sports and movie stars, and other prominent dignitaries, which may or may not be intentional. However, the exchange of such information across the general population inevitably contributes to social-economic, socio-political turmoil, and even physical violence in society. By classifying the derogatory content of a social media post, this research work helps to eradicate and discourage the upsetting propagation of such hate campaigns. Social networking posts today often include the picture of Memes along with textual remarks and comments, which throw new challenges and opportunities to the research community while identifying the attacks. This article proposes a multimodal deep learning framework by utilizing ensembles of computer vision and natural language processing techniques to train an encapsulated transformer network for handling the classification problem. The proposed framework utilizes the fine-tuned state-of-the-art deep learning-based models (e.g., BERT, Electra) for multilingual text analysis along with face recognition and the optical character recognition model for Meme picture comprehension. For the study, a new Facebook meme-post dataset is created with recorded baseline results. The subject of the created dataset and context of the work is more geared toward multilingual Indian society. The findings demonstrate the efficacy of the proposed method in the identification of social media meme posts featuring derogatory content about a famous/recognized individual.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3210887046",
    "type": "article"
  },
  {
    "title": "The Effects of Negative Online Reviews on Consumer Perception, Attitude and Purchase Intention: Experimental Investigation of the Amount, Quality, and Presentation Order of eWOM",
    "doi": "https://doi.org/10.1145/3426883",
    "publication_date": "2021-05-05",
    "publication_year": 2021,
    "authors": "Hsiu-Li Liao; Zhenyu Huang; Su-Houn Liu",
    "corresponding_authors": "",
    "abstract": "The quick growth and fast spread of electronic word-of-mouth (eWOM) have created a new threat to Internet merchants and marketers through paid online reviewers flooding sites with product and service reviews that could confuse and deter customers. This study examined the effects of the posts by paid reviewers—specifically, the negative reviews—on consumers’ risk perception, product attitude, and purchase intention. While extant research examined negative eWOM as an information source, little attention has been paid to the role of a hired reviewer's post aimed at destroying the reputation of certain targets (Internet Water Army Attack [IWAA]). To gain a better understanding of this phenomenon, three experiments were conducted to investigate the effects of the amount, quality, and presentation order of negative eWOM on consumers’ perception change and decision making. We tested the hypothesis using a test environment that mimicked a PTT online forum ( https://www.ptt.cc/ ) in Taiwan. Three simulation cases (smartphone, digital camera, and tablet) based on real-world events were used. A total of 193 participants completed all three experiments and provided valid responses. The results of this study are mostly consistent with previous research findings that online marketing is greatly threatened by negative eWOM. Nevertheless, it was also found that the effects of the amount, quality, and presentation order of negative eWOM are more complicated than we have anticipated. The findings revealed that IWAA can effectively increase customers’ risk perception toward a product and change their attitude and purchase intention.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3157257939",
    "type": "article"
  },
  {
    "title": "Building Arabic Paraphrasing Benchmark based on Transformation Rules",
    "doi": "https://doi.org/10.1145/3446770",
    "publication_date": "2021-06-09",
    "publication_year": 2021,
    "authors": "Marwah Alian; Arafat Awajan; Ahmad Alhasan; Raeda Akuzhia",
    "corresponding_authors": "",
    "abstract": "Measuring semantic similarity between short texts is an important task in many applications of natural language processing, such as paraphrasing identification. This process requires a benchmark of sentence pairs that are labeled by Arab linguists and considered a standard that can be used by researchers when evaluating their results. This research describes an Arabic paraphrasing benchmark to be a good standard for evaluation algorithms that are developed to measure semantic similarity for Arabic sentences to detect paraphrasing in the same language. The transformed sentences are in accordance with a set of rules for Arabic paraphrasing. These sentences are constructed from the words in the Arabic word semantic similarity dataset and from different Arabic books, educational texts, and lexicons. The proposed benchmark consists of 1,010 sentence pairs wherein each pair is tagged with scores determining semantic similarity and paraphrasing. The quality of the data is assessed using statistical analysis for the distribution of the sentences over the Arabic transformation rules and exploration through hierarchical clustering (HCL). Our exploration using HCL shows that the sentences in the proposed benchmark are grouped into 27 clusters representing different subjects. The inter-annotator agreement measures show a moderate agreement for the annotations of the graduate students and a poor reliability for the annotations of the undergraduate students.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3169089067",
    "type": "article"
  },
  {
    "title": "Improving Data Augmentation for Low-Resource NMT Guided by POS-Tagging and Paraphrase Embedding",
    "doi": "https://doi.org/10.1145/3464427",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Mieradilijiang Maimaiti; Yang Liu; Huanbo Luan; Zegao Pan; Maosong Sun",
    "corresponding_authors": "",
    "abstract": "Data augmentation is an approach for several text generation tasks. Generally, in the machine translation paradigm, mainly in low-resource language scenarios, many data augmentation methods have been proposed. The most used approaches for generating pseudo data mainly lay in word omission, random sampling, or replacing some words in the text. However, previous methods barely guarantee the quality of augmented data. In this work, we try to build the data by using paraphrase embedding and POS-Tagging. Namely, we generate the fake monolingual corpus by replacing the main four POS-Tagging labels, such as noun, adjective, adverb, and verb, based on both the paraphrase table and their similarity. We select the bigger corpus size of the paraphrase table with word level and obtain the word embedding of each word in the table, then calculate the cosine similarity between these words and tagged words in the original sequence. In addition, we exploit the ranking algorithm to choose highly similar words to reduce semantic errors and leverage the POS-Tagging replacement to mitigate syntactic error to some extent. Experimental results show that our augmentation method consistently outperforms all previous SOTA methods on the low-resource language pairs in seven language pairs from four corpora by 1.16 to 2.39 BLEU points.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3193922060",
    "type": "article"
  },
  {
    "title": "Research on Extraction of Useful Tourism Online Reviews Based on Multimodal Feature Fusion",
    "doi": "https://doi.org/10.1145/3453694",
    "publication_date": "2021-09-01",
    "publication_year": 2021,
    "authors": "Meng Li",
    "corresponding_authors": "Meng Li",
    "abstract": "To effectively identify the influencing factors of the perceived usefulness of multimodal data in online reviews of tourism products, this article explores the optimization method of online tourism products based on user-generated content and conducts feature fusion of multimodal data in online reviews of tourism products from the perspective of data fusion analysis. Therefore, based on the word vector model, this article proposes a method to select the seed word set of emotion dictionary. In this method, emotional words are represented in vector form and the distance between word vectors is calculated to form the selection criteria and classification basis of seed word set, and then the sentiment dictionary of online review is formed by category judgment. This article takes the real online review data of tourism products as the research object, carries out descriptive statistical analysis, uses machine learning and deep learning methods, carries out text vector embedding and image content recognition, integrates image and text feature vector, constructs multimodal online review usefulness classification model, and conducts model test. The experimental results show that, compared with the single-mode reviews containing only text or pictures, the multimodal reviews combined with text and pictures can better predict the usefulness of online reviews, improve the quality of online reviews, give full play to the potential value of user-generated content, provide optimization ideas for product providers, and provide decision support for product consumers.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3196315368",
    "type": "article"
  },
  {
    "title": "A Comprehensive Guideline for Bengali Sentiment Annotation",
    "doi": "https://doi.org/10.1145/3474363",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Md. Saddam Hossain Mukta; Md. Adnanul Islam; Faisal Ahamed Khan; Afjal Hossain; Shuvanon Razik; Shazzad Hossain; Jalal Mahmud",
    "corresponding_authors": "",
    "abstract": "Sentiment Analysis (SA) is a Natural Language Processing (NLP) and an Information Extraction (IE) task that primarily aims to obtain the writer’s feelings expressed in positive or negative by analyzing a large number of documents. SA is also widely studied in the fields of data mining, web mining, text mining, and information retrieval. The fundamental task in sentiment analysis is to classify the polarity of a given content as Positive, Negative, or Neutral . Although extensive research has been conducted in this area of computational linguistics, most of the research work has been carried out in the context of English language. However, Bengali sentiment expression has varying degree of sentiment labels, which can be plausibly distinct from English language. Therefore, sentiment assessment of Bengali language is undeniably important to be developed and executed properly. In sentiment analysis, the prediction potential of an automatic modeling is completely dependent on the quality of dataset annotation. Bengali sentiment annotation is a challenging task due to diversified structures (syntax) of the language and its different degrees of innate sentiments (i.e., weakly and strongly positive/negative sentiments). Thus, in this article, we propose a novel and precise guideline for the researchers, linguistic experts, and referees to annotate Bengali sentences immaculately with a view to building effective datasets for automatic sentiment prediction efficiently.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3210147767",
    "type": "article"
  },
  {
    "title": "Hybrid Pipeline for Building Arabic Tunisian Dialect-standard Arabic Neural Machine Translation Model from Scratch",
    "doi": "https://doi.org/10.1145/3568674",
    "publication_date": "2022-11-02",
    "publication_year": 2022,
    "authors": "Saméh Kchaou; Rahma Boujelbane; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "Deep Learning is one of the most promising technologies compared to other methods in the context of machine translation. It has been proven to achieve impressive results on large amounts of parallel data for well-endowed languages. Nevertheless, for low-resource languages such as the Arabic Dialects, Deep Learning models failed due to the lack of available parallel corpora. In this article, we present a method to create a parallel corpus to build an effective NMT model able to translate into MSA, Tunisian Dialect texts present in social networks. For this, we propose a set of data augmentation methods aiming to increase the size of the state-of-the-art parallel corpus. By evaluating the impact of this step, we noticed that it has effectively boosted both the size and the quality of the corpus. Then, using the resulted corpus, we compare the effectiveness of CNN, RNN and transformers models to translate Tunisian Dialect into MSA. Experiments show that a better translation is achieved by the transformer model with a BLEU score of 60 vs., respectively, 33.36 and 53.98 with RNN and CNN models.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4307922498",
    "type": "article"
  },
  {
    "title": "Approaches, Methods, and Resources for Assessing the Readability of Arabic Texts",
    "doi": "https://doi.org/10.1145/3571510",
    "publication_date": "2022-11-17",
    "publication_year": 2022,
    "authors": "Naoual Nassiri; Violetta Cavalli‐Sforza; Abdelhak Lakhouaja",
    "corresponding_authors": "",
    "abstract": "Text readability assessment is a well-known problem that has acquired even more importance in today’s information-rich world. In this article, we survey various approaches to measuring and assessing the readability of texts. Our specific goal is to provide a perspective on the state-of-the-art in readability assessment research for Arabic, which differs significantly from other languages on which readability studies have tended to focus. We provide background on readability assessment research and tools for English, for which readability studies are the most advanced. We then survey approaches adopted for Arabic, both classical formula-based approaches and studies that combine Machine Learning (ML) with Natural Language Processing (NLP) techniques. The works we cover target text corpora for different audiences: school-age first language readers (L1), foreign language learners (L2), and adult readers in non-academic contexts. Therefore, we explore differences between reading in L1 and L2 and consider how they play out specifically in Arabic after describing language characteristics that may impact readability. Finally, we highlight challenges for Arabic readability research and propose multiple future directions to improve readability assessment and related applications that would benefit from more attention.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4309265306",
    "type": "article"
  },
  {
    "title": "Arabic Span Extraction-based Reading Comprehension Benchmark (ASER) and Neural Baseline Models",
    "doi": "https://doi.org/10.1145/3579047",
    "publication_date": "2023-01-10",
    "publication_year": 2023,
    "authors": "Mariam Biltawi; Arafat Awajan; Sara Tedmori",
    "corresponding_authors": "",
    "abstract": "Machine reading comprehension (MRC) requires machines to read and answer questions about a given text. This can be achieved through either predicting answers or extracting them. Extracting answers from text involves predicting the first and last index of the answer span within the paragraph. Training machines to answer questions requires datasets that are created for such a purpose. The lack of availability of benchmarking datasets for the Arabic language has hindered research into machine reading comprehension from Arabic text. The aim of this article is to propose an Arabic Span-Extraction-based Reading Comprehension Benchmark (ASER) and complement it with neural baseline models for performance evaluations. Detailed steps are depicted for building and evaluating ASER, which is an Arabic dataset created manually for the task of machine reading comprehension. It contains 10,000 records from different domains and is divided into training and testing sets. The results of ASER evaluation led to the conclusion that it is a challenging benchmark since the answers have varying lengths and human performance resulted in an exact match of 42%. On the other hand, two main baseline models were the focus of ASER experimentation: the sequence-to-sequence (Seq2Seq) model with different neural networks and the bidirectional attention flow (BIDAF) model. These experiments were implemented using different embeddings, and the results showed an exact match with lower values than human performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4315481729",
    "type": "article"
  },
  {
    "title": "An Intelligent Approach Based on Cleaning up of Inutile Contents for Extremism Detection and Classification in Social Networks",
    "doi": "https://doi.org/10.1145/3575802",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Adel Berhoum; Mohammed Charaf Eddine Meftah; Abdelkader Laouid; Mohammad Hammoudeh",
    "corresponding_authors": "",
    "abstract": "Extremism is a growing threat worldwide that presents a significant danger to public safety and national security. Social networks provide extremists with spaces to spread their ideas through commentaries or tweets, often in Asian English. In this paper, we propose an intelligent approach that cleans the text’s content, analyzes its sentiment, and extracts its features after converting it to digital data for machine learning treatments. We apply 16 intelligent machine learning classifiers for extremism detection and classification. The proposed artificial intelligence methods for Asian English language data are used to extract the essential features from the text. Our evaluation of the proposed model with an extremism dataset proves its effectiveness compared to the standard classification models based on various performance metrics. The proposed model achieves 93,6% accuracy for extremism detection and 97,0% for extremism classification.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4317503083",
    "type": "article"
  },
  {
    "title": "Automated Generation of Human-readable Natural Arabic Text from RDF Data",
    "doi": "https://doi.org/10.1145/3582262",
    "publication_date": "2023-01-25",
    "publication_year": 2023,
    "authors": "Roudy Touma; Hazem Hajj; Wassim El‐Hajj; Khaled Shaban",
    "corresponding_authors": "",
    "abstract": "With the advances in Natural Language Processing (NLP), the industry has been moving towards human-directed artificial intelligence (AI) solutions. Recently, chatbots and automated news generation have captured a lot of attention. The goal is to automatically generate readable text from tabular data or web data commonly represented in Resource Description Framework (RDF) format. The problem can then be formulated as Data-to-text (D2T) generation from structured non-linguistic data into human-readable natural language. Despite the significant work done for the English language, no efforts are being directed towards low-resource languages like the Arabic language. This work promotes the development of the first RDF data-to-text (D2T) generation system for the Arabic language while trying to address the low-resource limitation. We develop several models for the Arabic D2T task using transfer learning from large language models (LLM) such as AraBERT, AraGPT2, and mT5. These models include a baseline Bi-LSTM Sequence-to-Sequence (Seq2Seq) model, as well as encoder-decoder transformers like BERT2BERT, BERT2GPT, and T5. We then provide a detailed comparative study highlighting the strengths and limitations of these methods setting the stage for further advancement in the field. We also introduce a new Arabic dataset (AraWebNLG) that can be used for new model development in the field. To ensure a comprehensive evaluation, general-purpose automated metrics (BLEU and Perplexity scores) are used as well as task-specific human evaluation metrics related to the accuracy of the content selection and fluency of the generated text. The results highlight the importance of pre-training on a large corpus of Arabic data and show that transfer learning from AraBERT gives the best performance. Text-to-text pre-training using mT5 achieves second best performance results even with multilingual weights.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4317935388",
    "type": "article"
  },
  {
    "title": "Deep Neural Network with Embedding Fusion for Chinese Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3570328",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Kaifang Long; Han Zhao; Zengzhen Shao; Yang Cao; Yanfang Geng; Yintai Sun; Weizhi Xu; Hui Yu",
    "corresponding_authors": "",
    "abstract": "Chinese Named Entity Recognition (NER) is an essential task in natural language processing, and its performance directly impacts the downstream tasks. The main challenges in Chinese NER are the high dependence of named entities on context and the lack of word boundary information. Therefore, how to integrate relevant knowledge into the corresponding entity has become the primary task for Chinese NER. Both the lattice LSTM model and the WC-LSTM model did not make excellent use of contextual information. Additionally, the lattice LSTM model had a complex structure and did not exploit the word information well. To address the preceding problems, we propose a Chinese NER method based on the deep neural network with multiple ways of embedding fusion. First, we use a convolutional neural network to combine the contextual information of the input sequence and apply a self-attention mechanism to integrate lexicon knowledge, compensating for the lack of word boundaries. The word feature, context feature, bigram feature, and bigram context feature are obtained for each character. Second, four different features are used to fuse information at the embedding layer. As a result, four different word embeddings are obtained through cascading. Last, the fused feature information is input to the encoding and decoding layer. Experiments on three datasets show that our model can effectively improve the performance of Chinese NER.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4319964355",
    "type": "article"
  },
  {
    "title": "An Exhaustive Literature Review of Hadith Text Mining",
    "doi": "https://doi.org/10.1145/3588315",
    "publication_date": "2023-03-17",
    "publication_year": 2023,
    "authors": "Mohamed Atef Mosa",
    "corresponding_authors": "Mohamed Atef Mosa",
    "abstract": "The Quran and the hadith of the Prophet are the two sources of legislation for Muslims. Sharia rulings and laws are not only derived from the Quran but also the bulk of them come through hadith. Understanding the hadith, its classification, and verification of its authenticity is vital to reach detailed rulings, as the volume of the hadith is many times greater than the volume of the Quran. As a result, mining in the hadith text is one of the things that has attracted the attention of researchers in the past few years. In this study, we conducted a survey of all the techniques and systems related to the mining of the hadith in its two parts, the Al-Matn and the Al-Sanad. On the other hand, the challenges and obstacles which confronted researchers have been shown; in addition, some suggested tips were highlighted to overcome those challenges. Furthermore, the most essential modern techniques used in the classification of Arabic texts, which gave a high degree of efficiency, were highlighted as milestones for future studies.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4327742572",
    "type": "article"
  },
  {
    "title": "FeDDkw – Federated Learning with Dynamic Kullback–Leibler-divergence Weight",
    "doi": "https://doi.org/10.1145/3594779",
    "publication_date": "2023-04-28",
    "publication_year": 2023,
    "authors": "Boyuan Li; Shengbo Chen; Keping Yu",
    "corresponding_authors": "",
    "abstract": "Federated learning (FL) has emerged as a promising framework for collaborative machine learning. As one of the most well-known bottlenecks of FL, data heterogeneity, i.e., non-IID data, has seriously hampered the convergence rate and model accuracy of FL. Although there are many works that aim to deal with this problem, none of them have directly exploited the data heterogeneity information, and thus cannot handle the problem effectively in general. In this paper, we try to answer the following fundamental question, whether this data heterogeneity information can be utilized to enhance the system performance. To this end, we propose FedDkw – Federated Learning with Dynamic KL-divergence Weight. Specifically, in every round, while uploading the model to the server, each participated client also piggybacks the distribution of the training data. The server maintains a global distribution, which is updated after receiving the distributions of all clients in each round. Then the weight of each client i is proportional to the \\(\\frac{1}{KL(i)} \\) , where KL ( i ) is the Kullback-Leibler (KL) divergence between the client’s distribution and the global counterpart. This indicates that the clients whose distribution is closer to the global one should be assigned with a larger weight, which coincides with our intuition. Furthermore, since uploading the clients’ data distribution in FedDkw may bring about potential security risks, we further propose FedDkw++ to avoid this procedure. In particular, after each client uploads its model, the server uses its local data as the input to the model, and the obtained outcome is used as an estimation of the client’s distribution (we name this method “data distribution inference”). We have conducted extensive experiments in various scenarios. The results show that our algorithm can significantly accelerate the convergence rate than the current state-of-art algorithms under heterogeneous data.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4367319645",
    "type": "article"
  },
  {
    "title": "Multi-Label Text Classification Model Based on Multi-Level Constraint Augmentation and Label Association Attention",
    "doi": "https://doi.org/10.1145/3586008",
    "publication_date": "2023-05-01",
    "publication_year": 2023,
    "authors": "Xiao Wei; Jianbao Huang; Rui Zhao; Hang Yu; Zheng Xu",
    "corresponding_authors": "",
    "abstract": "In the multi-label text classification task, a text usually corresponds to multiple label categories, and the labels have correlation and hierarchical structure. However, when the label hierarchy is unknown, the number of various labels is not balanced, which makes it difficult for the model to classify low-frequency labels. In addition, labels have semantic similarities that make it difficult for the model to distinguish between them. In this article, we propose a multi-label text classification model based on multi-level constraint augmentation and label association attention. Compared with traditional methods, our method has two contributions: (1) In order to alleviate the problem of unbalanced number of different label categories and ensure the rationality of sample generation, we propose a data augmentation method based on multi-level constraints. In the process of sample generation, this method uses historical generation information, sample original text information, and sample topic to constrain the generated text. (2) In order to make the model recognize the associated labels accurately, we propose an interaction mechanism based on label association attention and filter gate. This method combines text information and label weight information. At the same time, our classification model considers the important weights of text sentences and effectively utilizes the co-occurrence relationship between labels. Experimental results on three benchmark datasets show that our model outperforms state-of-the-art methods on all main evaluation metrics, especially on low-frequency label prediction with sparse samples.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4367604458",
    "type": "article"
  },
  {
    "title": "Alabib-65: A Realistic Dataset for Algerian Sign Language Recognition",
    "doi": "https://doi.org/10.1145/3596909",
    "publication_date": "2023-05-10",
    "publication_year": 2023,
    "authors": "Kenza Khellas; Rachid Seghir",
    "corresponding_authors": "",
    "abstract": "Sign language recognition (SLR) is a promising research field that aims to blur boundaries between Deaf and hearing people by creating a system that can transcribe signs into a written or vocal language. There is a growing body of literature that investigates the recognition of different sign languages, especially American sign language. So far, to the best of our knowledge, no study has considered the Algerian SLR. This is mainly due to the lack of datasets. To address this issue, we created the Alabib-65, the first Algerian Sign Language dataset. It consists of up to 6,238 Videos recorded from 41 native signers under realistic settings. This dataset is challenging due to a variety of reasons. First, there is a little inter-class variability. The 65 sign classes are similar in terms of hands’ configuration, placement, or movement and can share the same sub-parts. Second, there is a large intra-class variability. Furthermore, compared to other SL datasets that were collected from an indoor environment with a static and simple background, our videos were recorded from both indoor and outdoor environments with 22 backgrounds varying from simple to cluttered, and from static to dynamic. To underpin future research, we provided baseline results on this new dataset using state-of-the-art machine learning methods, namely: IDTFs with Fisher vector and SVM-classifier, VGG16-GRU, I3D, I3D-GRU, and I3D-GRU-Attention. The results show the validity and the challenges of our dataset.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4376117721",
    "type": "article"
  },
  {
    "title": "Swahili Speech Dataset Development and Improved Pre-training Method for Spoken Digit Recognition",
    "doi": "https://doi.org/10.1145/3597494",
    "publication_date": "2023-05-20",
    "publication_year": 2023,
    "authors": "Alexander Rogath Kivaisi; Qingjie Zhao; Jimmy Mbelwa",
    "corresponding_authors": "",
    "abstract": "Speech dataset is an essential component in building commercial speech applications. However, low-resource languages such as Swahili lack such a resource that is vital for spoken digit recognition. For languages where such resources exist, they are usually insufficient. Thus, pre-training methods have been used with external resources to improve continuous speech recognition. However, to the best of our knowledge, no study has investigated the effect of pre-training methods specifically for spoken digit recognition. This study aimed at addressing these problems. First, we developed a Swahili spoken digit dataset for Swahili spoken digit recognition. Then, we investigated the effect of cross-lingual and multi-lingual pre-training methods on spoken digit recognition. Finally, we proposed an effective language-independent pre-training method for spoken digit recognition. The proposed method has the advantage of incorporating target language data during the pre-training stage that leads to an optimal solution when using less training data. Experiments on Swahili (being developed), English, and Gujarati datasets show that our method achieves better performance compared with all the baselines listed in this study.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4377140520",
    "type": "article"
  },
  {
    "title": "Construction of English Speech Recognition Model by Fusing CNN and Random Deep Factorization TDNN",
    "doi": "https://doi.org/10.1145/3597456",
    "publication_date": "2023-05-23",
    "publication_year": 2023,
    "authors": "Shi Qiu",
    "corresponding_authors": "Shi Qiu",
    "abstract": "In current society, speech recognition can perform a variety of functions, such as completing voice commands, enabling speech processing, spoken language translation and facilitating communication. Therefore, the study of speech recognition technology is of high value. However, current speech recognition techniques focus on among clearly expressed spoken words, which poses great challenges for recognition with spoken pronunciation or dialect pronunciation. Some scholars currently use a model combining time-delay neural networks and long and short-term memory networks to build speech recognition systems, but the performance in acoustic recognition is poor. Therefore, the study proposes a convolutional neural network (CNN), time-delay neural network (TDNN) and output-gate projected Gated recurrent by analyzing the deep neural network unit (OPGRU) combined with a composite English speech recognition model. The model can optimize the acoustic model after the introduction of CNN, and the model can accurately recognize pronunciation features and make the model have a wider recognition range. The proposed composite model is compared with the Word error rate (Wer) and runtime metrics in the Mozilla Common Voice dataset. The Wer result of the composite model is 23.42% and the running time is 1418 s. The Wer result of the composite model is 24.61% and the running time is 1385 s. Compared with the TDNN-OPGRU model, the Wer of the composite model decreases by 1.19% but the running time increases by 33 s. The accuracy of the composite model is higher than that of the TDNN-OPGRU model. From a comprehensive consideration, the speech recognition model accuracy has higher priority, so the composite model proposed in the study has better performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4377824862",
    "type": "article"
  },
  {
    "title": "Prompt-based for Low-Resource Tibetan Text Classification",
    "doi": "https://doi.org/10.1145/3603168",
    "publication_date": "2023-05-31",
    "publication_year": 2023,
    "authors": "Bo An",
    "corresponding_authors": "Bo An",
    "abstract": "Text classification is a critical and foundational task in Tibetan natural language processing, it plays a crucial role in various applications, such as sentiment analysis and information extraction. However, the limited availability of annotated data poses a significant challenge to Tibetan natural language processing. This paper proposes a prompt learning-based method for low-resource Tibetan text classification to overcome this challenge. This method utilizes pre-trained language models to learn text representation and generation capabilities on a large-scale unsupervised Tibetan corpus, enabling few-shot Tibetan text classification. Experimental results demonstrate that the proposed method significantly improves the performance of Tibetan text classification in low-resource scenarios. This work provides a new research idea and method for low-resource language processing, such as Tibetan natural language processing. Hopefully, it will inspire subsequent work on low-resource language processing.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4378830714",
    "type": "article"
  },
  {
    "title": "Context-aware Answer Selection in Community Question Answering Exploiting Spatial Temporal Bidirectional Long Short-Term Memory",
    "doi": "https://doi.org/10.1145/3603398",
    "publication_date": "2023-06-05",
    "publication_year": 2023,
    "authors": "Muzamil Ahmed; Hikmat Ullah Khan; Muhammad Attique Khan; Usman Tariq; Seifedine Kadry",
    "corresponding_authors": "",
    "abstract": "Community Question Answering (CQA) sites provide knowledge sharing facility as the users can post questions and other users can share their answers. The selection of top-quality answers from the set of answers in a thread is a significant and challenging task in Natural Language Processing (NLP). To address this issue, we propose a deep learning based spatial temporal Bidirectional Long Short-Term Memory (Bi-LSTM) algorithm. The existing studies mainly focus only computing semantic similarity between questions and answers using votes given by the users. The proposed hybrid approach, based on both forward and backward, consider question to answer and answer to answer similarity. The forward LSTM captures the spatial impact of the answer to estimate the relevancy, whereas the backward LSTM learns temporal features with the answer to predict the best quality answer. Moreover, spatial Bi-LSTM captures past and future dependencies for a better understanding of context and to improve the effectiveness of answer selection. For extracting meaningful information from noisy text data, data is preprocessed following standard steps such as tokenization, parsing, lemmatization, stop words removal, part of speech tagging and entities extraction. Word embeddings-based Paragraph to vector (par2vec) has additional input nodes to represent paragraph information in vector for context understanding. The empirical analysis carried out on the SemEval CQA dataset shows that the proposed model outperforms state-of-art answer selection approaches.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4379388368",
    "type": "article"
  },
  {
    "title": "Deep Learning-based Sequence Labeling Tools for Nepali",
    "doi": "https://doi.org/10.1145/3606696",
    "publication_date": "2023-07-07",
    "publication_year": 2023,
    "authors": "Pooja Rai; Sanjay Chatterji; Byung‐Gyu Kim",
    "corresponding_authors": "",
    "abstract": "A Part-of-Speech (POS) tagger and Chunker (or shallow parser) are sequence labeling tools, crucial for improving the accuracy of Natural Language Processing (NLP) tasks like parsing, named entity recognition, sentiment analysis, information extraction, and so on. Developing such tools for a low-resource language is an arduous task. Nepali is a relatively resource-poor Indian language and has not been able to evolve from a computational perspective. Therefore, we present effective part-of-speech tagging and chunking tools for the Nepali text using sequential deep learning models—Bidirectional Long Short-Term Memory Network with a Conditional Random Field Layer (BI-LSTM-CRF) and other LSTM-based models exploring both character and word embeddings of the Nepali texts. Word Embedding has been used to capture syntactic as well as semantic information whereas character embedding has been applied to capture the morphological as well as shape information of words and also to handle the out-of-vocabulary problem. The developed chunker is the first statistical chunker for the Nepali language. A baseline model with a Conditional Random Field has also been developed to identify the optimum feature set for the aforementioned tasks. The BI-LSTM-CRF model produced an accuracy of 99.20% and 98.40%, for Nepali POS tagging and chunking, respectively. This is the highest-ever accuracy for Nepali. Thorough error analysis and observations have also been reported with examples. The developed tools can help advance research in Nepali language processing, improve the accuracy of language technology applications, and contribute to the preservation and promotion of the Nepali language.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4383556271",
    "type": "article"
  },
  {
    "title": "MahaEmoSen: Towards Emotion-aware Multimodal Marathi Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3618057",
    "publication_date": "2023-09-02",
    "publication_year": 2023,
    "authors": "Prasad Chaudhari; Pankaj Nandeshwar; Shubhi Bansal; Nagendra Kumar",
    "corresponding_authors": "",
    "abstract": "With the advent of the Internet, social media platforms have witnessed an enormous increase in user-generated textual and visual content. Microblogs on platforms such as Twitter are extremely useful for comprehending how individuals feel about a specific issue through their posted texts, images, and videos. Owing to the plethora of content generated, it is necessary to derive an insight of its emotional and sentimental inclination. Individuals express themselves in a variety of languages and, lately, the number of people preferring native languages has been consistently increasing. Marathi language is predominantly spoken in the Indian state of Maharashtra. However, sentiment analysis in Marathi has rarely been addressed. In light of the above, we propose an emotion-aware multimodal Marathi sentiment analysis method (MahaEmoSen). Unlike the existing studies, we leverage emotions embedded in tweets besides assimilating the content-based information from the textual and visual modalities of social media posts to perform a sentiment classification. We mitigate the problem of small training sets by implementing data augmentation techniques. A word-level attention mechanism is applied on the textual modality for contextual inference and filtering out noisy words from tweets. Experimental outcomes on real-world social media datasets demonstrate that our proposed method outperforms the existing methods for Marathi sentiment analysis in resource-constrained circumstances.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4386392721",
    "type": "article"
  },
  {
    "title": "Hindi Text Summarization Using Sequence to Sequence Neural Network",
    "doi": "https://doi.org/10.1145/3624013",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Namrata Kumari; Pardeep Singh",
    "corresponding_authors": "",
    "abstract": "Text summarizing reduces a large block of text data to a precise, short, and intelligible text that conveys the whole meaning of the actual text in a few words while maintaining the original context. Due to a lack of relevant summaries, it is hard to understand the main idea of the document. Text summarization using the abstractive technique is well-studied in English, although it is still in its infancy in Indian regional languages. In this study, we investigate the effectiveness of using a sequence-to-sequence (Seq2Seq) neural network based on attention and its optimization for text summarization for the Hindi language (HiATS), explicitly comparing the Adam and RMSprop optimizers. Our method allows the model to take the Hindi language dataset and, as output, provides a concise summary that accurately reflects the gist of the original text. The performance of the models will be evaluated using Rouge-1 and Rouge-2 metrics.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4386710242",
    "type": "article"
  },
  {
    "title": "A Synergistic Bidirectional LSTM and N-gram Multi-channel CNN Approach Based on BERT and FastText for Arabic Event Identification",
    "doi": "https://doi.org/10.1145/3626568",
    "publication_date": "2023-10-04",
    "publication_year": 2023,
    "authors": "Nafaa Haffar; Mounir Zrigui",
    "corresponding_authors": "",
    "abstract": "Event extraction from texts continues to pose a challenge for many NLP systems. This article presents a novel neural network architecture that can extract and classify events from Arabic sentences. The model combines word representations and Part-Of-Speech (POS) tags and uses a bidirectional LSTM layer and a dual combined convolutional neural network. The first layer of the network focuses on sentence representations, while the second layer focuses on POS representations. The model takes advantage of both N-gram character features from FastText and contextual representations from bidirectional encoder representations from transformers. This combination proves to be successful, as evidenced by the good results obtained from evaluating the model on the Arabic TimeML corpus. Our results show that combining both contextual and N-gram representations outperforms the traditional skip-gram model.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4387343788",
    "type": "article"
  },
  {
    "title": "Fintech Key-Phrase: A New Chinese Financial High-Tech Dataset Accelerating Expression-Level Information Retrieval",
    "doi": "https://doi.org/10.1145/3627989",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Weiqiang Jin; Biao Zhao; Yu Zhang; G. X. Sun; Hang Yu",
    "corresponding_authors": "",
    "abstract": "Expression-level information extraction is a challenging task in natural language processing (NLP), which aims to retrieve crucial semantic information from linguistic documents. However, there is a lack of up-to-date data resources for accelerating expression-level information extraction, particularly in the Chinese financial high technology field. To address this gap, we introduce Fintech Key-Phrase: a human-annotated key-phrase dataset for the Chinese financial high technology domain. This dataset comprises over 12K paragraphs along with annotated domain-specific key-phrases. We extract the publicly released reports, Chinese management’s discussion and analysis (CMD&amp;A), from the renowned Chinese research data services platform (CNRDS) and then filter the reports related to high technology. The high technology key-phrases are annotated following pre-defined philosophy guidelines to ensure annotation quality. In order to better understand the limitations and challenges in the purposed dataset, we conducted comprehensive noise evaluation experiments for the Fintech Key-Phrase, including annotation consistency assessment and absolute annotation quality evaluation. To demonstrate the usefulness of our released Fintech Key-Phrase in retrieving valuable information in the Chinese financial high technology field, we evaluate its significance using several superior information retrieval systems as representative baselines and report corresponding performance statistics. Additionally, we further applied ChatGPT to the text augmentation approach of the Fintech Key-Phrase dataset. Extensive comparative experiments demonstrate that the augmented Fintech Key-Phrase dataset significantly improved the coverage and accuracy of extracting key phrases in the finance and high-tech domains. We believe that this dataset can facilitate scientific research and exploration in the Chinese financial high technology field. We have made the Fintech Key-Phrase dataset and the experimental code of the adopted baselines accessible on Github: https://github.com/albert-jin/Fintech-Key-Phrase. To encourage newcomers to participate in the financial high-tech domain information retrieval research, we have developed a series of tools, including an open website 1 and corresponding real-time information retrieval APIs. 2",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4388230746",
    "type": "article"
  },
  {
    "title": "Topic Modeling based Text Classification Regarding Islamophobia using Word Embedding and Transformers Techniques",
    "doi": "https://doi.org/10.1145/3626318",
    "publication_date": "2023-11-01",
    "publication_year": 2023,
    "authors": "Ammar Saeed; Hikmat Ullah Khan; Achyut Shankar; Talha Imran; Danish Khan; M. Kamran; Muhammad Attique Khan",
    "corresponding_authors": "",
    "abstract": "Islamophobia is a rising area of concern in the current era where Muslims face discrimination and receive negative perspectives towards their religion, Islam. Islamophobia is a type of racism that is being practiced by individuals, groups, and organizations worldwide. Moreover, the ease of access to social media platforms and their augmented usage has also contributed to spreading hate speech, false information, and negative opinions about Islam. In this research study, we focused to detect Islamophobic textual content shared on various social media platforms. We explored the state-of-the-art techniques being followed in text data mining and Natural Language Processing (NLP). Topic modelling algorithm Latent Dirichlet Allocation is used to find top topics. Then, word embedding approaches such as Word2Vec and Global Vectors for word representation (GloVe) are used as feature extraction techniques. For text classification, we utilized modern text analysis techniques of transformers-based Deep Learning algorithms named Bidirectional Encoders Representation from Transformers (BERT) and Generative Pre-Trained Transformer (GPT). For results comparison, we conducted an extensive empirical analysis of Machine Learning algorithms and Deep Learning using conventional textual features such as the Term Frequency-Inverse Document Frequency, N-gram, and Bag of words (BoW). The empirical based results evaluated using standard performance evaluation measures show that the proposed approach effectively detects the textual content related to Islamophobia. In the corpus of the study under Machine Learning models Support Vector Machine (SVM) performed best with an F1 score of 91%. The Transformer based core NLP models and the Deep Learning model Convolutional Neural Network (CNN) when combined with GloVe performed best among all the techniques except SVM with BoW. GPT, SVM when combined with BoW and BERT yielded the best F1 score of 92%, 92% and 91.9% respectively, while CNN performed slightly poor with an F1 score of 91%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4388230772",
    "type": "article"
  },
  {
    "title": "Deep Learning-based POS Tagger and Chunker for Odia Language Using Pre-trained Transformers",
    "doi": "https://doi.org/10.1145/3637877",
    "publication_date": "2023-12-16",
    "publication_year": 2023,
    "authors": "Tusarkanta Dalai; Tapas Kumar Mishra; Pankaj Kumar",
    "corresponding_authors": "",
    "abstract": "Developing effective natural language processing (NLP) tools for low-resourced languages poses significant challenges. This article centers its attention on the task of Part-of-speech (POS) tagging and chunking, which pertains to the identification and categorization of linguistic units within sentences. POS tagging and Chunking have already produced positive results in English and other European languages. However, in Indian languages, particularly in Odia language, it is not yet well explored because of the lack of supporting tools, resources, and its complex linguistic morphology. This study presents the building of a manually annotated dataset for Odia phrase chunking task and the development of a deep learning-based model specifically tailored to accommodate the distinctive properties of the language. The process of annotating the Odia chunking corpus involved the utilization of inside-outside-begin labels, which were tagged by using designed Odia chunking tagset. We utilize the constructed Odia chunking dataset to build Odia chunker based on deep learning techniques, employing state-of-the-art architectures. Various techniques, such as Recurrent Neural Networks, Convolutional Neural Networks, and transformer-based models, are investigated to determine the most effective approach for Odia POS tagging and chunking. In addition, we conduct experiments utilizing diverse input representations, including Odia word embeddings, character-level representations, and sub-word units, to effectively capture the complex linguistic characteristics of the Odia language. Numerous experiments are conducted that evaluate the performance of our Odia POS tagger and chunker, employing standard evaluation metrics and making comparisons with existing approaches. The results demonstrate that our transformer-based tagger and chunker achieves superior accuracy and robustness in identifying and categorizing linguistic POS tags and chunks within Odia sentences. It outperforms existing work and exhibits consistent performance across diverse linguistic contexts and sentence structures. The developed Odia POS tagger and chunker have enormous potential for a variety of NLP applications, including information extraction, syntactic parsing, and machine translation, all of which are tailored to the low-resource Odia language. This work contributes to developing NLP tools and technologies for low-resource languages, thereby facilitating enhanced language processing capabilities in various linguistic contexts.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4389850326",
    "type": "article"
  },
  {
    "title": "Transliteration Characteristics in Romanized Assamese Language Social Media Text and Machine Transliteration",
    "doi": "https://doi.org/10.1145/3639565",
    "publication_date": "2024-01-06",
    "publication_year": 2024,
    "authors": "Hemanta Baruah; Sanasam Ranbir Singh; Priyankoo Sarmah",
    "corresponding_authors": "",
    "abstract": "This article aims to understand different transliteration behaviors of Romanized Assamese text on social media. Assamese, a language that belongs to the Indo-Aryan language family, is also among the 22 scheduled languages in India. With the increasing popularity of social media in India and also the common use of the English Qwerty keyboard, Indian users on social media express themselves in their native languages, but using the Roman/Latin script. Unlike some other popular South Asian languages (say Pinyin for Chinese), Indian languages do not have a common standard romanization convention for writing on social media platforms. Assamese and English are two very different orthographical languages. Thus, considering both orthographic and phonemic characteristics of the language, this study tries to explain how Assamese vowels, vowel diacritics, and consonants are represented in Roman transliterated form. From a dataset of romanized Assamese social media texts collected from three popular social media sites: (Facebook, YouTube, and X (formerly known as Twitter)), 1 we have manually labeled them with their native Assamese script. A comparison analysis is also carried out between the transliterated Assamese social media texts with six different Assamese romanization schemes that reflect how Assamese users on social media do not adhere to any fixed romanization scheme. We have built three separate character-level transliteration models from our dataset. One using a traditional phrase-based statistical machine transliteration model, (1) PBSMT model and two separate neural transliteration models, (2) BiLSTM neural seq2seq model with attention, and (3) Neural transformer model. A thorough error analysis has been performed on the transliteration result obtained from the three state-of-the-art models mentioned above. This may help to build a more robust machine transliteration system for the Assamese social media domain in the future. Finally, an attention analysis experiment is also carried out with the help of attention weight scores taken from the character-level BiLSTM neural seq2seq transliteration model built from our dataset.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390636177",
    "type": "article"
  },
  {
    "title": "Multi-view Image Fusion Using Ensemble Deep Learning Algorithm For MRI And CT Images",
    "doi": "https://doi.org/10.1145/3640811",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "N. Thenmoezhi; B. Perumal; A. Lakshmi",
    "corresponding_authors": "",
    "abstract": "Medical image fusions are crucial elements in image-based health care diagnostics or therapies and generic applications of computer visions. However, the majority of existing methods suffer from noise distortion that affects the overall output. When pictures are distorted by noises, classical fusion techniques perform badly. Hence, fusion techniques that properly maintain information comprehensively from multiple faulty pictures need to be created. This work presents Enhanced Lion Swarm Optimization (ESLO) with Ensemble Deep Learning (EDL) to address the aforementioned issues. The primary steps in this study include image fusions, segmentation, noise reduction, feature extraction, picture classification, and feature selection. Adaptive Median Filters are first used for noise removal in sequence to enhance image quality by eliminating noises. The MRIs and CT images are then segmented using the Region Growing–based k -Means Clustering (RKMC) algorithm to separate the images into their component regions or objects. Images in black and white are divided into image. In the white image, the RKMC algorithm successfully considered the earlier tumour probability. The next step is feature extraction, which is accomplished by using the Modified Principal Component Analysis (MPCA) to draw out the most informative aspects of the images. Then the ELSO algorithm is applied for optimal feature selection, which is computed by best fitness values. After that, multi-view image fusions of multi modal images derive lower-, middle-, and higher-level image contents. It is done by using Deep Convolution Neural Network (DCNN) and the Tissue-Aware Conditional Generative Adversarial Network (TAcGAN) algorithm, which fuses the multi-view features and relevant image features, and it is used for real-time applications. ELSO +EDL algorithm gives better results in terms of accuracy, Peak Signal-To-Noise Ratio (PSNR), and lower Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE) when compared to other existing algorithms.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391402687",
    "type": "article"
  },
  {
    "title": "Seq2Set2Seq: A Two-stage Disentangled Method for Reply Keyword Generation in Social Media",
    "doi": "https://doi.org/10.1145/3644074",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Jie Liu; Yaguang Li; Shizhu He; Shun Wu; Kang Liu; Shenping Liu; Jiong Wang; Qing Zhang",
    "corresponding_authors": "",
    "abstract": "Social media produces large amounts of content every day. How to predict the potential influences of the contents from a social reply feedback perspective is a key issue that has not been explored. Thus, we propose a novel task named reply keyword prediction in social media, which aims to predict the keywords in the potential replies in as many aspects as possible. One prerequisite challenge is that the accessible social media datasets labeling such keywords remain absent. To solve this issue, we propose a new dataset, 1 to study the reply keyword prediction in social media. This task could be seen as a single-turn dialogue keyword prediction for open-domain dialogue system. However, existing methods for dialogue keyword prediction cannot be adopted directly, which has two main drawbacks. First, they do not provide an explicit mechanism to model topic complementarity between keywords which is crucial in social media to controllably model all aspects of replies. Second, the collocations of keywords are not explicitly modeled, which also makes it less controllable to optimize for fine-grained prediction since the context information is much less than that in dialogue. To address these issues, we propose a two-stage disentangled framework, which can optimize the complementarity and collocation explicitly in a disentangled fashion. In the first stage, we use a sequence-to-set paradigm via multi-label prediction and determinantal point processes, to generate a set of keyword seeds satisfying the complementarity. In the second stage, we adopt a set-to-sequence paradigm via seq2seq model with the keyword seeds guidance from the set, to generate the more-fine-grained keywords with collocation. Experiments show that this method can generate not only a more diverse set of keywords but also more relevant and consistent keywords. Furthermore, the keywords obtained based on this method can achieve better reply generation results in the retrieval-based system than others.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391568857",
    "type": "article"
  },
  {
    "title": "A Study for Enhancing Low-resource Thai-Myanmar-English Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3645111",
    "publication_date": "2024-02-13",
    "publication_year": 2024,
    "authors": "Mya Ei San; Sasiporn Usanavasin; Ye Kyaw Thu; Manabu Okumura",
    "corresponding_authors": "",
    "abstract": "Several methodologies have recently been proposed to enhance the performance of low-resource Neural Machine Translation (NMT). However, these techniques have yet to be explored thoroughly in the low-resource Thai and Myanmar languages. Therefore, we first applied augmentation techniques such as SwitchOut and Ciphertext Based Data Augmentation (CipherDAug) to improve NMT performance in these languages. Second, we enhanced the NMT performance by fine-tuning the pre-trained Multilingual Denoising BART model (mBART), where BART denotes Bidirectional and Auto-Regressive Transformer. We implemented three NMT systems: namely, Transformer+SwitchOut, Multi-Source Transformer+CipherDAug, and fine-tuned mBART in the bidirectional translations of Thai-English-Myanmar language pairs from the ASEAN-MT corpus. Experimental results showed that Multi-Source Transformer+CipherDAug significantly improved Bilingual Evaluation Understudy (BLEU), Character n-gram F-score (ChrF) , and Translation Error Rate (TER) scores over the first baseline Transformer and second baseline Edit-Based Transformer. The model achieved notable BLEU scores: 37.9 (English-to-Thai), 42.7 (Thai-to-English), 28.9 (English-to-Myanmar), 31.2 (Myanmar-to-English), 25.3 (Thai-to-Myanmar), and 25.5 (Myanmar-to-Thai). The fine-tuned mBART model also considerably outperformed the two baselines, except for the Myanmar-to-English pair. SwitchOut improved over the second baseline in all pairs and performed similarly to the first baseline in most cases. Last, we performed detailed analyses verifying that the CipherDAug and mBART models potentially facilitate improving low-resource NMT performance in Thai and Myanmar languages.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391786484",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis Method of Epidemic-related Microblog Based on Hesitation Theory",
    "doi": "https://doi.org/10.1145/3648360",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "Yang Yu; Dong Qiu; Huanyu Wan",
    "corresponding_authors": "",
    "abstract": "The COVID-19 pandemic in 2020 brought an unprecedented global crisis. After two years of control efforts, life gradually returned to the pre-pandemic state, but localized outbreaks continued to occur. Toward the end of 2022, COVID-19 resurged in China, leading to another disruption of people’s lives and work. Many pieces of information on social media reflected people’s views and emotions toward the second outbreak, which showed distinct differences compared to the first outbreak in 2020. To explore people’s emotional attitudes toward the pandemic at different stages and the underlying reasons, this study collected microblog data from November 2022 to January 2023 and from January to June 2020, encompassing Chinese reactions to the COVID-19 pandemic. Based on hesitancy and the Fuzzy Intuition theory, we proposed a hypothesis: hesitancy can be integrated into machine learning models to select suitable corpora for training, which not only improves accuracy but also enhances model efficiency. Based on this hypothesis, we designed a hesitancy-integrated model. The experimental results demonstrated the model’s positive performance on a self-constructed database. By applying this model to analyze people’s attitudes toward the pandemic, we obtained their sentiments in different months. We found that the most negative emotions appeared at the beginning of the pandemic, followed by emotional fluctuations influenced by social events, ultimately showing an overall positive trend. Combining word cloud techniques and the Latent Dirichlet Allocation (LDA) model effectively helped explore the reasons behind the changes in pandemic attitude.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391840446",
    "type": "article"
  },
  {
    "title": "SCT: Summary Caption Technique for Retrieving Relevant Images in Alignment with Multimodal Abstractive Summary",
    "doi": "https://doi.org/10.1145/3645029",
    "publication_date": "2024-02-17",
    "publication_year": 2024,
    "authors": "Shaik Rafi; Ranjita Das",
    "corresponding_authors": "",
    "abstract": "This work proposes an efficient Summary Caption Technique that considers the multimodal summary and image captions as input to retrieve the correspondence images from the captions that are highly influential to the multimodal summary. Matching a multimodal summary with an appropriate image is a challenging task in computer vision and natural language processing. Merging in these fields is tedious, though the research community has steadily focused on cross-modal retrieval. These issues include the visual question-answering, matching queries with the images, and semantic relationship matching between two modalities for retrieving the corresponding image. Relevant works consider questions to match the relationship of visual information and object detection and to match the text with visual information and employing structural-level representation to align the images with the text. However, these techniques are primarily focused on retrieving the images to text or for image captioning. But less effort has been spent on retrieving relevant images for the multimodal summary. Hence, our proposed technique extracts and merge features in the Hybrid Image Text layer and captions in the semantic embeddings with word2vec where the contextual features and semantic relationships are compared and matched with each vector between the modalities, with cosine semantic similarity. In cross-modal retrieval, we achieve top five related images and align the relevant images to the multimodal summary that achieves the highest cosine score among the retrieved images. The model has been trained with seq-to-seq modal with 100 epochs, besides reducing the information loss by the sparse categorical cross entropy. Further, experimenting with the multimodal summarization with multimodal output dataset, in cross-modal retrieval, helps to evaluate the quality of image alignment with an image-precision metric that demonstrate the best results.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391897129",
    "type": "article"
  },
  {
    "title": "Opinion Mining on Social Media Text Using Optimized Deep Belief Networks",
    "doi": "https://doi.org/10.1145/3649502",
    "publication_date": "2024-03-02",
    "publication_year": 2024,
    "authors": "Sethu Vinayaga Vadivu; P. Nagaraj; B. S. Murugan",
    "corresponding_authors": "",
    "abstract": "In the digital world, most people spend their leisure and precious time on social media networks such as Facebook, Twitter. Instagram, and so on. Moreover, users post their views of products, services, political parties on their social sites. This information is viewed by many other users and brands. With the aid of these posts and tweets, the emotions, polarities of users are extracted to obtain the opinion about products or services. To analyze these posts sentiment analysis or opinion mining techniques are applied. Subsequently, this field rapidly attracts many researchers to conduct their research work due to the availability of an enormous number of data on social media networks. Further, this method can also be used to analyze the text to extract the sentiments which are classified as moderate, neutral, low extreme, and high extreme. However, the extraction of sentiment is an arduous one from the social media datasets, since it includes formal and informal texts, emojis, symbols. Hence to extract the feature vector from the accessed social media datasets and to perform accurate classification to group the texts based on the appropriate sentiments we proposed a novel method known as, Deep Belief Network-based Dynamic Grouping-based Cooperative optimization method DBN based DGCO. Exploiting this method the data are preprocessed to attain the required format of text and henceforth the feature vectors are extracted by the ICS algorithm. Furthermore, the extracted datasets are classified and grouped into moderate, neutral, low extreme, and high extreme with DBN based DGCO method. For experimental analysis, we have taken two social media datasets and analyzed the performance of the proposed method in terms of performance metrics such as accuracy/precision, recall, F1 Score, and ROC with HEMOS, WOA-SITO, PDCNN, and NB-LSVC state-of-art works. The acquired accuracy/precision, recall, and F1 Score, of our proposed ICS-DBN-DGCO method, are 89%, 80%, 98.2%, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392348023",
    "type": "article"
  },
  {
    "title": "Knowledge-Enriched Prompt for Low-Resource Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3659948",
    "publication_date": "2024-04-17",
    "publication_year": 2024,
    "authors": "Wenlong Hou; Weidong Zhao; Xianhui Liu; Wenyan Guo",
    "corresponding_authors": "",
    "abstract": "Named Entity Recognition (NER) in low-resource settings aims to identify and categorize entities in a sentence with limited labeled data. Although prompt-based methods have succeeded in low-resource perspectives, challenges persist in effectively harnessing information and optimizing computational efficiency. In this work, we present a novel prompt-based method to enhance low-resource NER without exhaustive template tuning. First, we construct knowledge-enriched prompts by integrating representative entities and background information to provide informative supervision tailored to each entity type. Then, we introduce an efficient reverse generative framework inspired by question answering (QA), which avoids redundant computations. Finally, we reduce costs by generating entities from their types while retaining model reasoning capacity. Experiment results demonstrate that our method outperforms other baselines on three datasets under few-shot settings.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4394880537",
    "type": "article"
  },
  {
    "title": "MizBERT: A Mizo BERT Model",
    "doi": "https://doi.org/10.1145/3666003",
    "publication_date": "2024-05-25",
    "publication_year": 2024,
    "authors": "Robert Lalramhluna; Sandeep Kumar Dash; Partha Pakray",
    "corresponding_authors": "",
    "abstract": "This research investigates the utilization of pre-trained BERT transformers within the context of the Mizo language. BERT, an abbreviation for Bidirectional Encoder Representations from Transformers, symbolizes Google’s forefront neural network approach to Natural Language Processing (NLP), renowned for its remarkable performance across various NLP tasks. However, its efficacy in handling low-resource languages such as Mizo remains largely unexplored. In this study, we introduce MizBERT , a specialized Mizo language model. Through extensive pre-training on a corpus collected from diverse online platforms, MizBERT has been tailored to accommodate the nuances of the Mizo language. Evaluation of MizBERT’s capabilities is conducted using two primary metrics: masked language modeling and perplexity, yielding scores of 76.12% and 3.2565, respectively. Additionally, its performance in a text classification task is examined. Results indicate that MizBERT outperforms both the Multilingual BERT model and the Support Vector Machine algorithm, achieving an accuracy of 98.92%. This underscores MizBERT’s proficiency in understanding and processing the intricacies inherent in the Mizo language.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399017786",
    "type": "article"
  },
  {
    "title": "Optimizing Uyghur Speech Synthesis by Combining Pretrained Cross-Lingual Model",
    "doi": "https://doi.org/10.1145/3675397",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Kexin Lu; Zhihua Huang; Mingming Yin; Ke Chen",
    "corresponding_authors": "",
    "abstract": "End-to-end speech synthesis methodologies have exhibited considerable advancements for languages with abundant corpus resources. Nevertheless, such achievements are yet to be realized for languages constrained by limited corpora. This manuscript delineates a novel strategy that leverages contextual encoding information to augment the naturalness of the speech synthesized through FastSpeech2, particularly under resource-scarce conditions. Initially, we harness the cross-linguistic model XLM-RoBERTa to extract contextual features, which serve as an auxiliary input to the mel-spectrum decoder of FastSpeech2. Subsequently, we refine the mel-spectrum prediction module to mitigate the overfitting dilemma encountered by FastSpeech2 amidst scant training datasets. To this end, Conformer blocks, rather than traditional Transformer blocks, are employed within both the encoder and decoder to concentrate intensively on varying levels and granularities of feature information. Additionally, we introduce a token-average mechanism to equalize pitch and energy attributes at the frame level. The empirical outcomes indicate that our pre-training with the LJ Speech dataset, followed by fine-tuning using a modest 10-minute paired Uyghur corpus, yields satisfactory synthesized Uyghur speech. Relative to the baseline framework, our proposed technique halves the character error rate and enhances the mean opinion score by over 0.6. Similar results were observed in Mandarin Chinese experimental evaluations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400123644",
    "type": "article"
  },
  {
    "title": "Towards Vietnamese Question and Answer Generation: An Empirical Study",
    "doi": "https://doi.org/10.1145/3675781",
    "publication_date": "2024-06-29",
    "publication_year": 2024,
    "authors": "Phạm Quốc Hùng; Huu-Loi Le; Minh Dang Nhat; Khang Tran T.; Manh Tran-Tien; Viet-Hung Dang; Huy-The Vu; Minh-Tien Nguyen; Xuan-Hieu Phan",
    "corresponding_authors": "",
    "abstract": "Question-answer generation (QAG) is a challenging task that generates both questions and answers from a given input paragraph context. The QAG task has recently achieved promising results thanks to the appearance of large pre-trained language models, yet, QAG models are mainly implemented in common languages, e.g., English. There still remains a gap in domain and language adaptation of these QAG models to low-resource languages such as Vietnamese. To address the gap, this article presents a large-scale and systematic study of QAG in Vietnamese. To do that, we first implement several QAG models by using the common fine-tuning techniques based on powerful pre-trained language models. We next introduce a set of instructions designed for the QAG task. These instructions are used to fine-tuned the pre-trained language and large language models. Extensive experimental results of both automatic and human evaluation on five benchmark machine reading comprehension datasets show two important points. First, the instruction-tuning method has the potential to enhance the performance of QAG models. Second, large language models trained in English need more data for fine-tuning to work well on the downstream QAG tasks of low-resource languages. We also provide a prototype system to demonstrate how our QAG models actually work. The code for fine-tuning QAG models and instructions are also made available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400151568",
    "type": "article"
  },
  {
    "title": "CNN-Based Models for Emotion and Sentiment Analysis Using Speech Data",
    "doi": "https://doi.org/10.1145/3687303",
    "publication_date": "2024-08-08",
    "publication_year": 2024,
    "authors": "Anjum Madan; Devender Kumar",
    "corresponding_authors": "",
    "abstract": "The study aims to present an in-depth Sentiment Analysis (SA) grounded by the presence of emotions in the speech signals. Nowadays, all kinds of web-based applications ranging from social media platforms and video-sharing sites to e-commerce applications provide support for Human–Computer Interfaces (HCIs). These media applications allow users to share their experiences in all forms such as text, audio, video, GIF, and so on. The most natural and fundamental form of expressing oneself is through speech. Speech-Based Sentiment Analysis (SBSA) is the task of gaining insights into speech signals. It aims to classify the statement as neutral, negative, or positive. On the other hand, Speech Emotion Recognition (SER) categorizes speech signals into the following emotions: disgust, fear, sadness, anger, happiness, and neutral. It is necessary to recognize the sentiments along with the profoundness of the emotions in the speech signals. To cater to the above idea, a methodology is proposed defining a text-oriented SA model using the combination of CNN and Bi-LSTM techniques along with an embedding layer, applied to the text obtained from speech signals; achieving an accuracy of 84.49%. Also, the proposed methodology suggests an Emotion Analysis (EA) model based on the CNN technique highlighting the type of emotion present in the speech signal with an accuracy measure of 95.12%. The presented architecture can also be applied to different other domains like product review systems, video recommendation systems, education, health, security, and so on.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401427245",
    "type": "article"
  },
  {
    "title": "Stacked Classification Approach using Optimized Hybrid Deep Learning Model for Early Prediction of Behaviour Changes on Social Media",
    "doi": "https://doi.org/10.1145/3689906",
    "publication_date": "2024-08-27",
    "publication_year": 2024,
    "authors": "Shiv Shankar Prasad Shukla; Maheshwari Prasad Singh",
    "corresponding_authors": "",
    "abstract": "Detecting signs of suicidal thoughts on social media is paramount for preventing suicides, given the platforms' role as primary outlets for emotional expression. Traditional embedding techniques focus solely on semantic analysis and lack the sentiment analysis essential for capturing emotions. This limitation poses challenges in developing high-accuracy models. Additionally, previous studies often rely on a single dataset, further constraining their effectiveness. To overcome these challenges, this study proposes an innovative approach that integrates embedding techniques such as BERT, which offers semantic and syntactic analysis of the posts, with sentiment analysis provided by VADER scores extracted from the VADER sentiment analysis tool. The identified features are then input into the proposed optimised hybrid deep learning model, specifically the Bi-GRU and Attention incorporated with Stacked or Stacking Classifier (Decision Tree, Random Forest, Gradient Boost, as the base classifier and XGBoost as meta classifier), which undergoes optimisation using the grid search technique to enhance detection capabilities. In evaluations, the model achieved an impressive accuracy and F1-score of 98% on the Reddit dataset and 97% on the twitter (formally known as X) dataset. The research evaluates the efficacy of several machine learning models, encompassing Decision Trees, Random Forests, Gradient Boosting, and XGBoost. Moreover, it examines sophisticated models like LSTM with Attention, Bi-LSTM with Attention, and Bi-GRU with Attention, augmented with word embeddings such as BERT, MUSE, and fastText, alongside the fusion of sentiment VADER score. These results emphasise the promise of a holistic strategy that combines advanced feature embedding techniques with semantic features, showcasing a notably efficient detection of suicidal ideation on social media.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401916097",
    "type": "article"
  },
  {
    "title": "A survey on NLP tasks, resources and techniques for low-resource Telugu-English code-mixed text",
    "doi": "https://doi.org/10.1145/3695766",
    "publication_date": "2024-09-10",
    "publication_year": 2024,
    "authors": "Sandeep Maddu; Viziananda Row Sanapala",
    "corresponding_authors": "",
    "abstract": "With the proliferation of informal content on various social media platforms in the form of posts, comments, and feedback, the importance of analyzing text in code-mixed form is gaining importance. Telugu, a low-resource Indian language, has a lot of online content being generated in code-mixed form. However, the lack of large corpora, annotated data and Natural Language Processing (NLP) resources are impeding research on Telugu-English code-mixed data. This paper provides a survey of existing literature on Telugu-English code-mixed text in the areas of resources, POS tagging, Named Entity Recognition, language identification, sentiment analysis, application tasks, dialog systems, and Question-Answering. Various datasets being used by the researchers in the field, along with methods applied to them are detailed. Research gaps are identified to provide future direction for researchers working in this field.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402404354",
    "type": "article"
  },
  {
    "title": "EADRE: Event-type Aware Dynamic Representation of Entities in Document-level Event Extraction",
    "doi": "https://doi.org/10.1145/3695767",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Guangjun Zhang; Zhang Hu; Ru Li; Hongye Tan",
    "corresponding_authors": "",
    "abstract": "Document-level Event Extraction (DEE) aims to identify event types and arguments from one document. However, existing methods fail to consider semantic distinctions between multiple mentions of one entity, and ignore dynamic representation of entities across multiple events simultaneously.Therefore, the models cannot capture flexible and specific entity representations in different event types. In this paper, we propose EADRE( E vent-type A ware D ynamic R epresentation of E ntities). Specifically, we use cross-attention between mentions and event-type prototypes to obtain event-type aware mention features. Then, we propose Adaptive Soft Gate(ASGate) that adaptively selects mention features to reduce the influence of event-unrelated mentions. EADRE introduce no more than 1% new parameters compared with the base model and has good transportability. Experiments on two public datasets show that EADRE improves the performance of multi-event extraction by 2.6% and 3.1% as well as outperforms previous state-of-the-art baselines by 0.2% and 1.6% with lower resource consumption without the use of pre-trained models. Further experimental analysis shows that EADRE significantly improves extraction performance in O2M and M2M multi-event scenarios.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402524834",
    "type": "article"
  },
  {
    "title": "A Novel Approach for Sentiment Analysis of a Low Resource Language Using Deep Learning Models",
    "doi": "https://doi.org/10.1145/3696789",
    "publication_date": "2024-09-23",
    "publication_year": 2024,
    "authors": "Naeem Ahmed; Rashid Amin; Hamza Aldabbas; Muhammad Imran Saeed; Muhammad Bilal; Houbing Song",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis is a process of dealing with people's opinions, remarks, and comments to extract valuable insights from them. Sentiment analysis can be used for various purposes like market analysis, campaign monitoring, decision-making, etc. In recent years, there has been much research on sentiment classification, particularly in English. However, these existing approaches used for the English language cannot be applied to the Urdu language. The substantial rise in communication traffic, including audio, text, video, and pictures, has significantly shifted the Internet of Things (IoT) from scalar to Multimedia Internet of Things (MIoT). So far, the integration of MIoT and NLP systems has received less attention, but it has evolved as a novel research paradigm for smart applications. This article proposes deep learning techniques for sentence-level Urdu sentiment analysis (Urdu SA) for MIoT. Our approach consists of various phases, i.e., data gathering, text preprocessing, model training, testing, and evaluation. A data set of 25 thousand Urdu reviews are used for training the proposed models. This data set is built by scraping various Urdu blogs and social media platforms, and some part of the IMDB data set is used after translating it into the Urdu language. Native Urdu speakers do data annotation, and various preprocessing techniques, i.e., tokenization, stemming, etc., are applied. The two deep learning models, i.e., Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM), are trained on preprocessed Urdu reviews to find their sentiments in this article. Both models are tested using various combinations of hyperparameters, and each model's accuracy and F1 scores are evaluated. The study results show that the LSTM model outperforms the CNN model by achieving a 96% accuracy and 91% F1 score.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402728114",
    "type": "article"
  },
  {
    "title": "Improving Transition-Based Dependency Parsing of Hindi and Urdu by Modeling Syntactically Relevant Phenomena",
    "doi": "https://doi.org/10.1145/3005447",
    "publication_date": "2017-01-20",
    "publication_year": 2017,
    "authors": "Riyaz Ahmad Bhat; Irshad Ahmad Bhat; Dipti Misra Sharma",
    "corresponding_authors": "",
    "abstract": "In recent years, transition-based parsers have shown promise in terms of efficiency and accuracy. Though these parsers have been extensively explored for multiple Indian languages, there is still considerable scope for improvement by properly incorporating syntactically relevant information. In this article, we enhance transition-based parsing of Hindi and Urdu by redefining the features and feature extraction procedures that have been previously proposed in the parsing literature of Indian languages. We propose and empirically show that properly incorporating syntactically relevant information like case marking, complex predication and grammatical agreement in an arc-eager parsing model can significantly improve parsing accuracy. Our experiments show an absolute improvement of ∼2% LAS for parsing of both Hindi and Urdu over a competitive baseline which uses rich features like part-of-speech (POS) tags, chunk tags, cluster ids and lemmas. We also propose some heuristics to identify ezafe constructions in Urdu texts which show promising results in parsing these constructions.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2576170458",
    "type": "article"
  },
  {
    "title": "Animation of Fingerspelled Words and Number Signs of the Sinhala Sign Language",
    "doi": "https://doi.org/10.1145/3092743",
    "publication_date": "2017-08-16",
    "publication_year": 2017,
    "authors": "M. Punchimudiyanse; R.G.N. Meegama",
    "corresponding_authors": "",
    "abstract": "Sign language is the primary communication medium of the aurally handicapped community. Often, a sign gesture is mapped to a word or a phrase in a spoken language and named as a conversational sign. A fingerspelling sign is a special sign derived to show a single character that matches a character in the alphabet of a given language. This enables the deaf community to express words that do not have a conversational sign, such as a name, using a letter-by-letter technique. Sinhala Sign Language (SSL) uses a phonetic pronunciation mechanism to decode such words due to the presence of one or more modifiers after a consonant. Expressing numbers also have a similar notation, and it is broken down into parts before interpretation in sign gestures. This article presents the variations implemented to make the 3D avatar-based interpreter system look similar to an actual fingerspelled SSL by a human interpreter. To accomplish the task, a phonetic English-based 3D avatar animation system is developed with Blender animation software. The conversion of Sinhala Unicode text to phonetic English and numbers written in digits to sign gestures is done with a Visual Basic.NET (VB.NET) application. The presented application has 61 SSL fingerspelling signs and 40 SSL number signs. It is capable of interpreting any word written using the modern Sinhala alphabet without conversational signs and interprets the numbers that go up to the billions. This is a helpful tool in teaching SSL fingerspelling and number signs of SSL to deaf children.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2749164842",
    "type": "article"
  },
  {
    "title": "Automatic Diacritics Restoration for Tunisian Dialect",
    "doi": "https://doi.org/10.1145/3297278",
    "publication_date": "2019-07-12",
    "publication_year": 2019,
    "authors": "Abir Masmoudi; Salima Mdhaffar; Rahma Sellami; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "Modern Standard Arabic, as well as Arabic dialect languages, are usually written without diacritics. The absence of these marks constitute a real problem in the automatic processing of these data by NLP tools. Indeed, writing Arabic without diacritics introduces several types of ambiguity. First, a word without diacratics could have many possible meanings depending on their diacritization. Second, undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology [12]. In fact, the agglutination property of Arabic might produce a problem that can only be resolved using diacritics. Third, without diacritics a word could have many possible parts of speech (POS) instead of one. This is the case with the words that have the same spelling and POS tag but a different lexical sense, or words that have the same spelling but different POS tags and lexical senses [8]. Finally, there is ambiguity at the grammatical level (syntactic ambiguity). In this article, we propose the first work that investigates the automatic diacritization of Tunisian Dialect texts. We first describe our annotation guidelines and procedure. Then, we propose two major models, namely a statistical machine translation (SMT) and a discriminative model as a sequence classification task based on Conditional Random Fields (CRF). In the second approach, we integrate POS features to influence the generation of diacritics. Diacritics restoration was performed at both the word and the character levels. The results showed high scores of automatic diacritization based on the CRF system (Word Error Rate (WER) 21.44% for CRF and WER 34.6% for SMT).",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2957234659",
    "type": "article"
  },
  {
    "title": "SentiFars",
    "doi": "https://doi.org/10.1145/3345627",
    "publication_date": "2019-09-17",
    "publication_year": 2019,
    "authors": "Rahim Dehkharghani",
    "corresponding_authors": "Rahim Dehkharghani",
    "abstract": "There is no doubt about the usefulness of public opinion toward different issues in social media and the World Wide Web. Extracting the feelings of people about an issue from text is not straightforward. Polarity lexicons that assign polarity tags or scores to words and phrases play an important role in sentiment analysis systems. As English is the richest language in this area, getting benefits from existing English resources in order to build new ones has attracted the interest of many researchers in recent years. In this article, we propose a new translation-based approach for building polarity resources in resource-lean languages such as Persian. The results of empirical evaluation of the proposed approach prove its effectiveness. The generated resource is the largest publicly available polarity lexicon for Persian.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2973742151",
    "type": "article"
  },
  {
    "title": "Classification of Printed Gujarati Characters Using Low-Level Stroke Features",
    "doi": "https://doi.org/10.1145/2856105",
    "publication_date": "2016-04-12",
    "publication_year": 2016,
    "authors": "Mukesh M. Goswami; Suman K. Mitra",
    "corresponding_authors": "",
    "abstract": "This article presents an elegant technique for extracting the low-level stroke features, such as endpoints, junction points, line elements, and curve elements, from offline printed text using a template matching approach. The proposed features are used to classify a subset of characters from Gujarati script. The database consists of approximately 16,782 samples of 42 middle-zone symbols from the Gujarati character set collected from three different sources: machine printed books, newspapers, and laser printed documents. The purpose of this division is to add variety in terms of size, font type, style, ink variation, and boundary deformation. The experiments are performed on the database using a k-nearest neighbor (kNN) classifier and results are compared with other widely used structural features, namely Chain Codes (CC), Directional Element Features (DEF), and Histogram of Oriented Gradients (HoG). The results show that the features are quite robust against the variations and give comparable performance with other existing works.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2341084382",
    "type": "article"
  },
  {
    "title": "Word Re-Segmentation in Chinese-Vietnamese Machine Translation",
    "doi": "https://doi.org/10.1145/2988237",
    "publication_date": "2016-11-04",
    "publication_year": 2016,
    "authors": "Phuoc Tran; Điền Đinh; Long Nguyen",
    "corresponding_authors": "",
    "abstract": "In isolated languages, such as Chinese and Vietnamese, words are not separated by spaces, and a word may be formed by one or more syllables. Therefore, word segmentation (WS) is usually the first process that is implemented in the machine translation process. WS in the source and target languages is based on different training corpora, and WS approaches may not be the same. Therefore, the WS that results in these two languages are not often homologous, and thus word alignment results in many 1-n and n-1 alignment pairs in statistical machine translation, which degrades the performance of machine translation. In this article, we will adjust the WS for both Chinese and Vietnamese in particular and for isolated language pairs in general and make the word boundary of the two languages more symmetric in order to strengthen 1-1 alignments and enhance machine translation performance. We have tested this method on the Computational Linguistics Center’s corpus, which consists of 35,623 sentence pairs. The experimental results show that our method has significantly improved the performance of machine translation compared to the baseline translation system, WS translation system, and anchor language-based WS translation systems.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2550455130",
    "type": "article"
  },
  {
    "title": "Implicit Discourse Relation Recognition for English and Chinese with Multiview Modeling and Effective Representation Learning",
    "doi": "https://doi.org/10.1145/3028772",
    "publication_date": "2017-03-17",
    "publication_year": 2017,
    "authors": "Haoran Li; Jiajun Zhang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Discourse relations between two text segments play an important role in many Natural Language Processing (NLP) tasks. The connectives strongly indicate the sense of discourse relations, while in fact, there are no connectives in a large proportion of discourse relations, that is, implicit discourse relations. Compared with explicit relations, implicit relations are much harder to detect and have drawn significant attention. Until now, there have been many studies focusing on English implicit discourse relations, and few studies address implicit relation recognition in Chinese even though the implicit discourse relations in Chinese are more common than those in English. In our work, both the English and Chinese languages are our focus. The key to implicit relation prediction is to properly model the semantics of the two discourse arguments, as well as the contextual interaction between them. To achieve this goal, we propose a neural network based framework that consists of two hierarchies. The first one is the model hierarchy, in which we propose a max-margin learning method to explore the implicit discourse relation from multiple views. The second one is the feature hierarchy, in which we learn multilevel distributed representations from words, arguments, and syntactic structures to sentences. We have conducted experiments on the standard benchmarks of English and Chinese, and the results show that compared with several methods our proposed method can achieve the best performance in most cases.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2603077646",
    "type": "article"
  },
  {
    "title": "Handwritten Manipuri Meetei-Mayek Classification Using Convolutional Neural Network",
    "doi": "https://doi.org/10.1145/3309497",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Kishorjit Nongmeikapam; Wahengbam Kanan Kumar; Oinam Nickson Meetei; Themrichon Tuithung",
    "corresponding_authors": "",
    "abstract": "A new technique for classifying all 56 different characters of the Manipuri Meetei-Mayek (MMM) is proposed herein. The characters are grouped under five categories, which are Eeyek Eepee (original alphabets), Lom Eeyek (additional letters), Cheising Eeyek (digits), Lonsum Eeyek (letters with short endings), and Cheitap Eeyek (vowel signs. Two related works proposed by previous researchers are studied for understanding the benefits claimed by the proposed deep learning approach in handwritten Manipuri Meetei-Mayek. (1) Histogram of Oriented (HOG) with SVM classifier is implemented for thoroughly understanding how HOG features can influence accuracy. (2) The handwritten samples are trained using simple Convolutional Neural Network (CNN) and compared with the proposed CNN-based architecture. Significant progress has been made in the field of Optical Character Recognition (OCR) for well-known Indian languages as well as globally popular languages. Our work is novel in the sense that there is no record of work available to date that is able to classify all 56 classes of the MMM. It will also serve as a pre-cursor for developing end-to-end OCR software for translating old manuscripts, newspaper archives, books, and so on.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2944554424",
    "type": "article"
  },
  {
    "title": "On the Usage of a Classical Arabic Corpus as a Language Resource",
    "doi": "https://doi.org/10.1145/3277591",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Ibrahim Bounhas",
    "corresponding_authors": "Ibrahim Bounhas",
    "abstract": "This article presents a literature review of computer-science-related research applied on hadith, a kind of Arabic narration which appeared in the 7th century. We study and compare existent works in several fields of Natural Language Processing (NLP), Information Retrieval (IR), and Knowledge Extraction (KE). Thus, we illicit their main drawbacks and identify some perspectives, which may be considered by the research community. We also study the characteristics of these types of documents, by enumerating the advantages/limits of using hadith as a language resource. Moreover, our study shows that previous studies used different collections of hadiths, thus making it hard to compare their results objectively. Besides, many preprocessing steps are recurrent through these applications, thus wasting a lot of time. Consequently, the key issues for building generic language resources from hadiths are discussed, taking into account the relevance of related literature and the wide community of researchers that are interested in these narrations. The ultimate goal is to structure hadith books for multiple usages, thus building common collections which may be exploited in future applications.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2997918148",
    "type": "article"
  },
  {
    "title": "Classification of Ancient Handwritten Tamil Characters on Palm Leaf Inscription Using Modified Adaptive Backpropagation Neural Network with GLCM Features",
    "doi": "https://doi.org/10.1145/3406209",
    "publication_date": "2020-10-02",
    "publication_year": 2020,
    "authors": "M. Poornima Devi; M. Sornam",
    "corresponding_authors": "",
    "abstract": "The core aspiration of this proposed work is to classify Tamil characters inscribed in the palm leaf manuscript using an Artificial Neural Network. Tamil palm leaf manuscript characters in the form of images were processed and segmented using contour-based convex hull bounding box segmentation. The segmented characters were transformed into two forms: Binary Coded Value and the Gray-Level Co-occurrence Matrix (GLCM) feature. The features extracted from the segmented characters were trained by the proposed method of the Modified Adaptive Backpropagation Network (MABPN) algorithm with Shannon activation function. Weight initialization plays an important role in the Backpropagation Neural Network, and hence Nguyen-Widrow weight initialization was introduced to initialize the weights instead of random weight initialization in the proposed method. The models evaluated are MABPN with Shannon activation function using Nguyen-Widrow weight initialization in two forms of input: Binary Coded Value and GLCM feature extracted values. The proposed method with GLCM features as input gave a promising result over binary coded transform.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3090494216",
    "type": "article"
  },
  {
    "title": "An Approach to Construct a Named Entity Annotated English-Vietnamese Bilingual Corpus",
    "doi": "https://doi.org/10.1145/2990191",
    "publication_date": "2016-10-14",
    "publication_year": 2016,
    "authors": "Long Nguyen; Điền Đinh; Phuoc Tran",
    "corresponding_authors": "",
    "abstract": "Manually constructing an annotated Named Entity (NE) in a bilingual corpus is a time-consuming, labor--intensive, and expensive process, but this is necessary for natural language processing (NLP) tasks such as cross-lingual information retrieval, cross-lingual information extraction, machine translation, etc. In this article, we present an automatic approach to construct an annotated NE in English-Vietnamese bilingual corpus from a bilingual parallel corpus by proposing an aligned NE method. Basing this corpus on a bilingual corpus in which the initial NEs are extracted from its own language separately, the approach tries to correct unrecognized NEs or incorrectly recognized NEs before aligning the NEs by using a variety of bilingual constraints. The generated corpus not only improves the NE recognition results but also creates alignments between English NEs and Vietnamese NEs, which are necessary for training NE translation models. The experimental results show that the approach outperforms the baseline methods effectively. In the English-Vietnamese NE alignment task, the F-measure increases from 68.58% to 79.77%. Thanks to the improvement of the NE recognition quality, the proposed method also increases significantly: the F-measure goes from 84.85% to 88.66% for the English side and from 75.71% to 85.55% for the Vietnamese side. By providing the additional semantic information for the machine translation systems, the BLEU score increases from 33.04% to 45.11%.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2531841301",
    "type": "article"
  },
  {
    "title": "Automatic Indonesian Sentiment Lexicon Curation with Sentiment Valence Tuning for Social Media Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3425632",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Rini Wijayanti; Andria Arisal",
    "corresponding_authors": "",
    "abstract": "A novel Indonesian sentiment lexicon (SentIL -- Sentiment Indonesian Lexicon) is created with an automatic pipeline; from creating sentiment seed words, adding new words with slang words, emoticons, and from the given dictionary and sentiment corpus, until tuning sentiment value with tagged sentiment corpus. It begins by taking seed words from WordNet Bahasa that mapped with sentiment value from English SentiWordNet . The seed words are enriched by combining the dictionary-based method with words’ synonyms and antonyms, and corpus-based methods with word embedding for word similarity that trained in positive and negative sentiment corpus from online marketplaces review and Twitter data. The valence score of each lexicon is recalculated based on its relative occurrence in the corpus. We also add some famous slang words and emoticons to enrich the lexicon. Our experiment shows that the proposed method can provide an increase of 3.5 times lexicon number as well as improve the accuracy of 80.9% for online review and 95.7% for Twitter data, and they are better than other published and available Indonesian sentiment lexicons.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3135298638",
    "type": "article"
  },
  {
    "title": "Plan Optimization to Bilingual Dictionary Induction for Low-resource Language Families",
    "doi": "https://doi.org/10.1145/3448215",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Arbi Haza Nasution; Yohei Murakami; Toru Ishida",
    "corresponding_authors": "",
    "abstract": "Creating bilingual dictionary is the first crucial step in enriching low-resource languages. Especially for the closely related ones, it has been shown that the constraint-based approach is useful for inducing bilingual lexicons from two bilingual dictionaries via the pivot language. However, if there are no available machine-readable dictionaries as input, we need to consider manual creation by bilingual native speakers. To reach a goal of comprehensively create multiple bilingual dictionaries, even if we already have several existing machine-readable bilingual dictionaries, it is still difficult to determine the execution order of the constraint-based approach to reducing the total cost. Plan optimization is crucial in composing the order of bilingual dictionaries creation with the consideration of the methods and their costs. We formalize the plan optimization for creating bilingual dictionaries by utilizing Markov Decision Process (MDP) with the goal to get a more accurate estimation of the most feasible optimal plan with the least total cost before fully implementing the constraint-based bilingual lexicon induction. We model a prior beta distribution of bilingual lexicon induction precision with language similarity and polysemy of the topology as and parameters. It is further used to model cost function and state transition probability. We estimated the cost of all investment plans as a baseline for evaluating the proposed MDP-based approach with total cost as an evaluation metric. After utilizing the posterior beta distribution in the first batch of experiments to construct the prior beta distribution in the second batch of experiments, the result shows 61.5% of cost reduction compared to the estimated all investment plans and 39.4% of cost reduction compared to the estimated MDP optimal plan. The MDP-based proposal outperformed the baseline on the total cost.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3139251967",
    "type": "article"
  },
  {
    "title": "A Systematic Review on Hadith Authentication and Classification Methods",
    "doi": "https://doi.org/10.1145/3434236",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Farid Binbeshr; Amirrudin Kamsin; Manal Mohammed",
    "corresponding_authors": "",
    "abstract": "Background : A hadith refers to sayings, actions, and characteristics of the Prophet Muhammad peace be upon him. The authenticity of hadiths is crucial, because they constitute the source of legislation for Muslims with the Holy Quran. Classifying hadiths into groups is a matter of importance as well, to make them easy to search and recognize. Objective : To report the results of a systematic review concerning hadith authentication and classification methods. Data sources : Original articles found in ACM, IEEE Xplore, ScienceDirect, Scopus, Web of Science, Springer Link, and Wiley Online Library. Study selection criteria : Only original articles written in English and dealing with hadith authentication and classification. Reviews, editorial, letters, grey literature, and restricted or incomplete articles are excluded. Data extraction : Two authors were assigned to extract data using a predefined data extraction form to answer research questions and assess studies quality. Results : A total of 27 studies were included in this review. There are 14 studies in authentication and 13 studies in classification. Most of the selected studies (17 of 27) were published in conferences, while the others (10 of 27) were published in scientific journals. Research in the area of hadith authentication and classification has received more attention in recent years (2016–2019). Conclusions : Hadith authentication methods are classified into machine learning, rule-based, and a hybrid of rule-based and machine learning and rule-based and statistical methods. Hadith classification methods are classified into machine learning and rule-based. All classification studies used Matn, while the majority of authentication studies used isnad. As a dataset source, Sahih Al-Bukhari was used by most studies. None of the used datasets is publicly available as a benchmark dataset, either in hadith authentication or classification. Recall and Precision are the most frequent evaluation metrics used by the selected studies.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3160185768",
    "type": "review"
  },
  {
    "title": "SACNN: Self-attentive Convolutional Neural Network Model for Natural Language Inference",
    "doi": "https://doi.org/10.1145/3426884",
    "publication_date": "2021-05-31",
    "publication_year": 2021,
    "authors": "Waris Quamer; Praphula Kumar Jain; Arpit Rai; Vijayalakshmi Saravanan; Rajendra Pamula; Chiranjeev Kumar",
    "corresponding_authors": "",
    "abstract": "Inference has been central problem for understanding and reasoning in artificial intelligence. Especially, Natural Language Inference is an interesting problem that has attracted the attention of many researchers. Natural language inference intends to predict whether a hypothesis sentence can be inferred from the premise sentence. Most prior works rely on a simplistic association between the premise and hypothesis sentence pairs, which is not sufficient for learning complex relationships between them. The strategy also fails to exploit local context information fully. Long Short Term Memory (LSTM) or gated recurrent units networks (GRU) are not effective in modeling long-term dependencies, and their schemes are far more complex as compared to Convolutional Neural Networks (CNN). To address this problem of long-term dependency, and to involve context for modeling better representation of a sentence, in this article, a general Self-Attentive Convolution Neural Network (SACNN) is presented for natural language inference and sentence pair modeling tasks. The proposed model uses CNNs to integrate mutual interactions between sentences, and each sentence with their counterparts is taken into consideration for the formulation of their representation. Moreover, the self-attention mechanism helps fully exploit the context semantics and long-term dependencies within a sentence. Experimental results proved that SACNN was able to outperform strong baselines and achieved an accuracy of 89.7% on the stanford natural language inference (SNLI) dataset.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3167698605",
    "type": "article"
  },
  {
    "title": "Denigrate Comment Detection in Low-Resource Hindi Language Using Attention-Based Residual Networks",
    "doi": "https://doi.org/10.1145/3431729",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Saurabh Raj Sangwan; M. P. S. Bhatia",
    "corresponding_authors": "",
    "abstract": "Cyberspace has been recognized as a conducive environment for use of various hostile, direct, and indirect behavioural tactics to target individuals or groups. Denigration is one of the most frequently used cyberbullying ploys to actively damage, humiliate, and disparage the online reputation of target by sending, posting, or publishing cruel rumours, gossip, and untrue statements. Previous pertinent studies report detecting profane, vulgar, and offensive words primarily in the English language. This research puts forward a model to detect online denigration bullying in low-resource Hindi language using attention residual networks. The proposed model Hindi Denigrate Comment–Attention Residual Network (HDC-ARN) intends to uncover defamatory posts (denigrate comments) written in Hindi language which stake and vilify a person or an entity in public. Data with 942 denigrate comments and 1499 non-denigrate comments is scraped using certain hashtags from two recent trending events in India: Tablighi Jamaat spiked Covid-19 (April 2020, Event 1) and Sushant Singh Rajput Death (June 2020: Event 2). Only text-based features, that is, the actual content of the post, are considered. The pre-trained word embedding for Hindi language from fastText is used. The model has three ResNet blocks with an attention layer that generates a post vector for a single input, which is passed through a sigmoid activation function to get the final output as either denigrate (positive class) or non-denigrate (negative class). An F-1 score of 0.642 is achieved on the dataset.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3217129148",
    "type": "article"
  },
  {
    "title": "Unsupervised Parallel Sentences of Machine Translation for Asian Language Pairs",
    "doi": "https://doi.org/10.1145/3486677",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Shaolin Zhu; Chenggang Mi; Tianqi Li; Yong Yang; Xu Chun",
    "corresponding_authors": "",
    "abstract": "Parallel sentence pairs play a very important role in many natural language processing tasks, especially cross-lingual tasks such as machine translation. So far, many Asian language pairs lack bilingual parallel sentences. As collecting bilingual parallel data is very time-consuming and difficult, it is very important for many low-resource Asian language pairs. While existing methods have shown encouraging results, they rely on bilingual data seriously or have some drawbacks in an unsupervised situation. To address these issues, we propose a new unsupervised similarity calculation and dynamic selection metric to obtain parallel sentence pairs in an unsupervised situation. First, our method maps bilingual word embedding by postdoc adversarial training, which rotates the source space to match the target without parallel data. Then, we introduce a new cross-domain similarity adaption to obtain parallel sentence pairs. Experimental results on real-world datasets show that our model can obtain better accuracy and recall on mining parallel sentence pairs. We also show that the extracted bilingual sentence corpora can significantly improve the performance of neural machine translation.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4210287993",
    "type": "article"
  },
  {
    "title": "Word Sense Disambiguation using Cooperative Game Theory and Fuzzy Hindi WordNet based on ConceptNet",
    "doi": "https://doi.org/10.1145/3502739",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Goonjan Jain; D. K. Lobiyal",
    "corresponding_authors": "",
    "abstract": "Natural Language is fuzzy in nature. The fuzziness of Hindi language was captured in the Fuzzy Hindi WordNet (FHWN) . FHWN assigned membership values to fuzzy relationships by consulting experts from various domains. However, these membership values need to be corrected. In the proposed work, we compute the membership values of fuzzy semantic relations using ConceptNet. Later, we perform WSD of Hindi text using cooperative game theoretic approach. We used the Shapley Value centrality measure where we predict which coalition of players (word senses) proves to be the most beneficial. We tested and compared our algorithm with the existing state-of-the-art approaches of Hindi on three datasets and results are better on all the three datasets. One more notable aspect is that the results are quite stable even if the fuzzy membership values of fuzzy graphs changes.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4214868430",
    "type": "article"
  },
  {
    "title": "An Arabic Manuscript Regions Detection, Recognition and Its Applications for OCRing",
    "doi": "https://doi.org/10.1145/3532609",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Hassanin M. Al-Barhamtoshy; Kamal Jambi; Mohsen Rashwan; Sherif Abdou",
    "corresponding_authors": "",
    "abstract": "The problem of Region of Interest (RoI) in document layout analysis and document recognition has recently become an essential topic in OCRing systems. Arabic manuscript layout analysis and OCRing recognition using language detection, document category, and RoI with Keras and TensorFlow are terms of the state-of-the-art that should be investigated. This article investigates the problem of Arabic manuscript recognition problems with respect to in OCRing-based recognition. A new framework architecture, which integrates Fast Gradient Sign Method (FGSM) using Keras and TensorFlow with adversarial image generation during training procedure is proposed. Also, the article tries to improve the OCRing accuracy of the image enhancement, alignment, layout analysis, and recognition using deep learning in multilingual system. RoIs detections will be performed using a custom trained deep learning model using bounding box regression with Keras and TensorFlow. This topic investigates an extension of Page Segmentation Method (PSM) to enhance OCRing parameter modes and enhances Arabic OCRing system accuracy from reinforcement strategy. Therefore, the article achieves a significant improvement of OCRing results due to the three parameters: language identification, document category, and RoI types (Table, Title, Paragraph, figure, and list). This model is based on “region proposal algorithm” as a basis of CNN object detectors to find the number of the RoIs. Therefore, the proposed framework performs three distinctive tasks: (1) CNN architecture for adversarial training, (2) an implementation of the FGSM with Keras and TensorFlow, and (3) an adversarial training script implementation with the CNN and the FGSM method. The experiments on Arabic manuscript dataset including Arabic text, English/Arabic documents, and Latin digits’ datasets, demonstrate the accuracy of the proposed method. Moreover, the proposed framework performs well and succeeded in defending against adversarial attacks or adversarial images. The experimental results on our collected dataset illustrate the novelty of our proposed framework over the other existing PSM methods to be extended and updated to improve the quality of the OCRing system. The results show that the influence of PSM after expanding using the RoI types, language ID, and document/manuscript category can improve the OCRing accuracy. Also, the experimental results show significant performance by the new framework model with accuracy reached to 99% compared to relative methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4225146035",
    "type": "article"
  },
  {
    "title": "Mulan: A Multiple Residual Article-Wise Attention Network for Legal Judgment Prediction",
    "doi": "https://doi.org/10.1145/3503157",
    "publication_date": "2022-04-04",
    "publication_year": 2022,
    "authors": "Junyi Chen; Lan Du; Ming Liu; Xiabing Zhou",
    "corresponding_authors": "",
    "abstract": "Legal judgment prediction (LJP) is used to predict judgment results based on the description of individual legal cases. In order to be more suitable for actual application scenarios in which the case has cited multiple articles and has multiple charges, we formulate legal judgment prediction as a multiple label learning problem and present a deep learning model that can effectively encode the content of each legal case via a multi-residual convolution neural network and the semantics of law articles via an article encoder. An article-wise attention mechanism is proposed to fuse the two types of encoded information. Experimental results derived on the CAIL2018 datasets show that our model provides a significant performance improvement over the existing neural models in predicting relevant law articles and charges.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4225787342",
    "type": "article"
  },
  {
    "title": "Social Relationship Analysis Using State-of-the-art Embeddings",
    "doi": "https://doi.org/10.1145/3539608",
    "publication_date": "2022-06-01",
    "publication_year": 2022,
    "authors": "Sibgha Anwar; Mirza Omer Beg; Kiran Saleem; Zeeshan Ahmed; Abdul Rehman Javed; Usman Tariq",
    "corresponding_authors": "",
    "abstract": "Detection of human relationships from their interactions on social media is a challenging problem with a wide range of applications in different areas, like targeted marketing, cyber-crime, fraud, defense, planning, and human resource, to name a few. All previous work in this area has only dealt with the most basic types of relationships. The proposed approach goes beyond the previous work to efficiently handle the hierarchy of social relationships. This article introduces a novel technique named Quantifiable Social Relationship (QSR) analysis for quantifying social relationships to analyze relationships between agents from their textual conversations. QSR uses cross-disciplinary techniques from computational linguistics and cognitive psychology to identify relationships. QSR utilizes sentiment and behavioral styles displayed in the conversations for mapping them onto level II relationship categories. Then, for identifying the level III relationship categories, QSR uses level II relationships, sentiments, interactions, and word embeddings as key features. QSR employs natural language processing techniques for feature engineering and state-of-the-art embeddings generated by word2vec, global vectors (glove), and bidirectional encoder representations from transformers (bert). QSR combines the intrinsic conversational features with word embeddings for classifying relationships. QSR achieves an accuracy of up to 89% for classifying relationship subtypes. The evaluation shows that QSR can accurately identify the hierarchical relationships between agents by extracting intrinsic and extrinsic features from textual conversations between agents.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4281622042",
    "type": "article"
  },
  {
    "title": "Joint Intent Detection Model for Task-oriented Human-Computer Dialogue System using Asynchronous Training",
    "doi": "https://doi.org/10.1145/3558096",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Yirui Wu; Hao Li; Lilai Zhang; Chen Dong; Qian Huang; Shaohua Wan",
    "corresponding_authors": "",
    "abstract": "How to accurately understand low-resource languages is the core of the task-oriented human-computer dialogue system. Language understanding consists of two sub-tasks, i.e., intent detection and slot filling. Intent detection still faces challenges due to semantic ambiguity and implicit intentions with users’ input. Moreover, separately modeling intent detection and slot filling significantly decrease the correctness and relevance between questions and answers. To address these issues, we propose a joint intent detection method using asynchronous training strategy. The proposed method firstly encodes local text information extracted by CNN and relationship information among words emphasized by attention structure. Later, a joint intent detection model with asynchronous training strategy is proposed by either fusing hidden states of intent detection and slot filling layers, or adopting the key information to fine-tune the whole network, greatly increasing the relevance of intent detection and slot filling subtasks. The accuracy achieved by the proposed method tested on an open-source airline travel dataset and a self-collected electricity service dataset, i.e., ATIS and ECSF, are 97.49% and 89.68%, respectively, which proves the effectiveness of joint learning and asynchronous training.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4292595679",
    "type": "article"
  },
  {
    "title": "Cognitive Hybrid Deep Learning-based Multi-modal Sentiment Analysis for Online Product Reviews",
    "doi": "https://doi.org/10.1145/3615356",
    "publication_date": "2023-08-11",
    "publication_year": 2023,
    "authors": "Ashwin Perti; Amit Sinha; Ankit Vidyarthi",
    "corresponding_authors": "",
    "abstract": "Recently the field of sentiment analysis has gained a lot of attraction in literature. The idea that a machine can dynamically spot the text’s sentiments is fascinating. In this paper, we propose a method to classify the textual sentiments in Twitter feeds. In particular, we focus on analyzing the tweets of products as either positive or negative. The proposed technique utilizes a deep learning schema to learn and predict the sentiment by extracting features directly from the text. Specifically, we use Convolutional Neural Networks with different convolutional layers. Further, we experiment with LSTMs and try an ensemble of multiple models to get the best results. We employ an n-gram-based word embeddings approach to get the machine-level word representations. Testing of the method is conducted on real-world datasets. We have discovered that the ensemble technique yields the best results after conducting experiments on a huge corpus of more than one million tweets. To be specific, we get an accuracy of 84.95%. The proposed method is also compared with several existing methods. An extensive numerical investigation has revealed the superiority of the proposed work in actual deployment scenarios.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4385762086",
    "type": "article"
  },
  {
    "title": "A Novel Multi-hop Query Answering System Based on a Large Knowledge Graph and Distributed Computing",
    "doi": "https://doi.org/10.1145/3711824",
    "publication_date": "2025-01-08",
    "publication_year": 2025,
    "authors": "Trung Phan Hong; Phuc Do",
    "corresponding_authors": "",
    "abstract": "An automated query answering system (QAS) is a very useful application in organizations. Therefore, there is a lot of research to build, develop and improve it. In this paper, we present a method to build a multi-hop query answering system (MQAS) based on a large knowledge graph (KG) , Bidirectional Encoder Representations from Transformers (BERT) , and the indexing structure K-Dimensional Tree (KD-Tree) . The large KG provides knowledge for MQAS. BERT is used to transform questions into vectors. KD-Tree helps find the right answers quickly. On the other hand, we also propose a solution for distributed indexing of large vector spaces by building a distributed indexing structure called Distributed KD-Tree (DKD-Tree) . In addition, we also present experiments and evaluation results to demonstrate the effectiveness of our solution.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406180819",
    "type": "article"
  },
  {
    "title": "Ge'ez Grammar Error Handling Using Neural Machine Translation Approach",
    "doi": "https://doi.org/10.1145/3711829",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Eshete Derb Emiru; Desalegn Mamo Wendyifraw",
    "corresponding_authors": "",
    "abstract": "The goal of natural language processing (NLP), which has recently gained popularity, is to improve the capacity of computers to comprehend and interact with human language. Consequently, in order to converse using natural language, it's crucial that spoken language be grammatically correct, especially for Ge'ez language. Geéz language sentences must follow certain norms of agreement in terms of number, person, gender, tense, and other factors in order to be considered grammatically correct. If the input sentence in Geéz language is improper, it can have problems with subject-verb agreement, object-verb agreement, adjective-noun agreement, and adverb-verb agreement. The goal of the proposed work is to provide a neural machine translation approach for detecting and correcting grammar errors in Ge'ez sentences. We have prepared manually 11490 Geéz parallel corpuses (Geéz language grammatically incorrect and grammatically correct sentences). After we have prepared a parallel Ge'ez sentence, we have used normalization, tokenization, padding, and one hot encoding as preprocesses. We have used two deep learning algorithms, including a bidirectional long-short-term memory encoder decoder and a long-short-term memory encoder decoder, for training the proposed model. Keras and TensorFlow were used for importing the required libraries, and we used the Python 3.7 environment for implementation. Two test cases are used for the evaluation technique. The first one is for the long-short-term memory encoder decoder model, and the second one is for the bidirectional long-short-term memory encoder decoder model. Finally, the bidirectional long short-term memory encoder decoder model achieved best results with an accuracy of 82%, recall of 82%, precision of 85%, and f1 measure of 83% with balanced error type classes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406241422",
    "type": "article"
  },
  {
    "title": "Detecting Empathy in Roman Urdu Using Transfer Learning",
    "doi": "https://doi.org/10.1145/3711825",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Hafsa Sattar; Mishaal Munir; Muhammad Kamran Malik; Zara Nasar",
    "corresponding_authors": "",
    "abstract": "Empathy is to understand the other person’s feelings and emotions to get familiar with their situation. This work aims to create a Roman Urdu empathy dataset containing two types of sentences, either “Depressed” or “Not-Depressed” and execute different machine learning, deep learning, and pre-trained models on the dataset for experimentation. The dataset has been created by extorting depressed Roman Urdu and motivational sentences from web pages and social media websites such as; Facebook, Twitter, and YouTube. Multiple evaluation results led to the observation that the extraction of attributes from the union of word and character-level features improve the accuracy of machine learning algorithms. The extraction of features by the unification of character-level unigram, bigram, trigram, and 5-gram feature extraction methods demonstrated improved results. In contrast, character-level unigrams and word-level trigrams have retrieved fewer significant parameters from sentences. The Naive Bayes (NB) and Random Forest (RF) have exhibited the lowest accuracies on word-level, character-level, and the union of word and character-level features as well. However, Logistic Regression (LR), Linear Support Vector Classifier (Linear SVC), and Support Vector Machine (SVM) have obtained better accuracy in all three feature extraction methods. The perceptron-based algorithm has illustrated improved results than most of the machine learning methods. Almost all deep learning algorithms produce results that are a step ahead of those of machine learning and perceptron-based models. Analysis of deep learning experiments shows that FastText word embeddings support the models in achieving high accuracy. Recurrent Neural Network (RNN), and a combination of Convolutional Neural Network and Gated Recurrent Unit (CNN-GRU) performed well overall on all word and sentence embeddings. On the other hand, Regional Convolutional Neural Network (RCNN) had the lowest final results across all three types of embeddings. Observing the outputs of pre-trained models revealed that they produce more precise findings than many machine learning and deep learning models. In comparison to all pre-trained models, Universal Language Model Fine-Tuning (ULMFiT) has outperformed the rest. Hence, the results show that pre-trained language models have produced outstanding results on the Roman Urdu empathy dataset.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406245412",
    "type": "article"
  },
  {
    "title": "PersianMHQA: A Dataset for Open Domain Persian Multi-hop Question Answering Based on Wikipedia Encyclopedia",
    "doi": "https://doi.org/10.1145/3711826",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Mohammed Hazem Taji; Arash Ghafouri; Hassan Naderi; Behrouz Minaei‐Bidgoli",
    "corresponding_authors": "",
    "abstract": "Today, one of the most important tasks in natural language processing is answering user questions. Especially, users' questions nowadays moved from simple questions to complex questions. In recent years, several question answering datasets have been produced for Persian language, but none of them support complex open-domain and explainable questions. In this article, the PersianMHQA dataset is introduced which is the first open-domain question answering dataset for complex questions based on the unstructured Persian Wikipedia encyclopedia. This dataset contains 7000 complex questions and sentence-level supporting facts are provided for each question that allows question answering systems to explain the predictions. The questions in this dataset are diverse and explainable and are not limited to any previous knowledge base. Various types of complexity are provided in this dataset, and the questions are designed in such a way that answering them requires reasoning over more than one paragraph.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406245437",
    "type": "article"
  },
  {
    "title": "PhonemeVec: A Phoneme-Level Contextual Prosody Representation For Speech Synthesis",
    "doi": "https://doi.org/10.1145/3711828",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Shiming Wang; Liping Chen; Yang Ai; Yajun Hu; Zhen-Hua Ling",
    "corresponding_authors": "",
    "abstract": "Recently, fine-grained prosody representations have emerged and attracted growing attention to address the one-to-many problem in text-to-speech (TTS). In this paper, we propose the PhonemeVec, a pre-trained prosody representations with considering the contextual information. To obtain the contextual prosody representations, we improve the data2vec framework according to the characteristics of prosody to extract the PhonemeVec from the low-band mel-spectrogram, and pre-train on a 960 hours Chinese corpus with high quality and diverse pronunciation. PhonemeVec is subsequently integrated into FastSpeech2, supervising the prosody modeling of the text encoder. Experiments conducted on the Blizzard Challenge 2019 dataset show that the integration of PhonemeVec results in the synthesis of more natural speech. Additionally, objective evaluations confirm that the application of PhonemeVec reduces the distortions between the generated speech and original recordings in terms of duration and F0. Audio samples can be found in http://home.ustc.edu.cn/~wsmzzz/PhonemeVec/demo.html.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406401036",
    "type": "article"
  },
  {
    "title": "Improving Neural Machine Translation in the Field of Electrical Engineering by Using Sentence Backbone Information",
    "doi": "https://doi.org/10.1145/3712261",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "B Teng; Yuan Chen; Juwei Zhang",
    "corresponding_authors": "",
    "abstract": "Due to the limited availability of corpora in the field of Electrical Engineering and the presence of numerous specialized terms, neural machine translation (NMT) performs poorly in translating the sentence backbone information when it is applied to corpora in the field of Electrical Engineering. In response to this issue, A method to improve NMT by using the sentence backbone information is proposed in this paper. In the proposed method, the source language sentences are used as the input of the Sentence Backbone Information Extraction Model to obtain the sentence backbone information, and then the sentence backbone information are incorporated as an auxiliary during the training process of the NMT model. Furthermore, a module called the Sentence Backbone Information Enhancement Module is introduced. It utilizes the dependency parse trees of the source language sentences to generate the sentence backbone mask matrices. These matrices are then applied to the encoder to force the NMT model to pay more attention to the backbones of sentences. On the English-Chinese parallel corpus in the field of Electrical Engineering, the proposed method in this paper outperforms the Transformer baseline translation model by 1.25 BLEU points. And it outperforms the baseline model in both METEOR and ROUGE-L evaluation metrics. It indicates that the proposed method in this paper can effectively improve translation performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406402338",
    "type": "article"
  },
  {
    "title": "A Multimodal Approach for Hate and Offensive Content Detection in Tamil: From Corpus Creation to Model Development",
    "doi": "https://doi.org/10.1145/3712260",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Jayanth Mohan; Spandana Reddy Mekapati; B Premjith; Jyothish Lal G; Bharathi Raja Chakravarthi",
    "corresponding_authors": "",
    "abstract": "Detecting hate speech on social media platforms is vital to mitigate technology-facilitated violence (TFV). Extensive research has targeted widely spoken languages like English, but there’s a notable gap in studying hate speech detection in low-resource languages like Tamil. Additionally, with social media platforms now supporting various modalities, including text, speech, and video, effective techniques for hate speech detection in multimodal formats, especially videos, are crucial. However, detecting hate speech in Tamil presents unique challenges due to its morphology and code-mixing nature. This paper presents a comprehensive approach for detecting hate speech in Tamil, with a focus on multimodal data. We introduce a new dataset, the Multimodal Tamil Hate (MATH) dataset, comprising videos along with their audio and textual transcripts, annotated with four categories of hate speech: offensive, sexist, racist, and casteist. To classify hate speech categories, we leverage transformer-based models. Through a series of experiments, we evaluated the performance of each modality individually and explored their fusion using multimodal approach. BERT-based models were used for textual analysis to extract informative features, the TimeSformer model was employed for video modality, and Wav2vec2 was used for audio modality. Specifically, we attained 81.82% accuracy and a 68.65% F1 score for the text modality, 63.63% accuracy and a 50.60% F1 score for the audio modality, and 45.45% accuracy and a 33.64% F1 score for the video modality. By integrating optimal combinations of models from each modality and employing machine learning classifiers, we achieved an accuracy of 81.82% and an F1 score of 66.67% in our hate speech classification task. Our research findings highlight the effectiveness of employing a multimodal approach for hate speech detection in Tamil, showcasing its efficacy in curbing the dissemination of hateful content on social media platforms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406534197",
    "type": "article"
  },
  {
    "title": "Sarcasm Identification and Classification in Hindi Newspaper Headlines",
    "doi": "https://doi.org/10.1145/3714469",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "I. Ahmad; Praveen Gatla; Rajesh Kumar Mundotiya",
    "corresponding_authors": "",
    "abstract": "Sarcasm identification in textual data is the most captivating area of research in the current research trends. It is a challenging task for humans as well as for the computer. In this paper, we have tried to identify sarcasm in the Hindi newspaper headlines of two of the most-read Hindi newspapers in India, namely Hindustan and Dainik Jagran. Initially, we collected 88,518 Hindi newspaper headlines and identified 1,945 headlines to be sarcastic, which we have considered for the present study. The headlines taken into consideration belong to the political domain and were published during some of the recent Legislative Assembly Elections of 2020, 2021 and 2022. Various machine learning and deep learning techniques have been used to develop the baseline models. It justifies the assumption that sarcastic text does not always bear a negative sentiment. It may bear a positive sentiment depending on the context. The present paper aims at the creation of a dataset consisting of 1,945 Hindi newspaper headlines, training and testing machine learning and deep learning models, namely Extra Trees Classifier, Random Forest Classifier, XGBClassifier, fasttext-stackedTCN and mBERT-stackedTCN for sarcasm identification on the dataset and comparing the results obtained by the models after the experiment. Out of all the choosen models, the Random Forest Classifier performs better with F 1 score of 92.11 before data augmentation and and 90.68 after data augmentation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406803498",
    "type": "article"
  },
  {
    "title": "DB-QM: A Comparative Quality Measurement and Its Prospective on Persian/Arabic Databases for OCR",
    "doi": "https://doi.org/10.1145/3711714",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Seyyed Amir Hadi Minoofam; Azam Bastanfard; Mohammad Reza Keyvanpour",
    "corresponding_authors": "",
    "abstract": "In Optical Character Recognition (OCR), state-of-the-art algorithms are applied to the same databases to compare performance and cost. Various benchmark databases have been created recently in Persian script to facilitate OCR application development. Unfortunately, there is a lack of coherent categorization and systematic identification in Persian and other languages about how to choose the databases to provide a suitable platform for evaluation. This paper provides an analytical framework called DB-QM (DataBase Qualitative Measurement) to achieve a macro vision for assessing Persian OCR databases. In our proposed framework, three components are available: First, a categorization of Persian databases is proposed. Therefore, the databases are considered from their content point of view. Second, several quantitative and qualitative evaluation criteria are introduced and categorized based on the nature of databases. Finally, a discussion about the strengths and weaknesses of databases is made on the proposed criteria. It concerns a comparison among databases and the critical points about how to select one for testing algorithms. In addition, the main challenges around database improvement have been taken into account. Our analytical discussion not only clarifies the superiority of one database to another but provides a diverse discipline on how to use the appropriate databases. Also, critical challenges for the enhancement of new databases are highlighted.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406870718",
    "type": "article"
  },
  {
    "title": "DialoguePFM:Prompt-based Fusion Model for Emotion Recognition in Conversation",
    "doi": "https://doi.org/10.1145/3714410",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Yu Tian; Junhui Li; Suyang Zhu; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Emotion recognition in conversation (ERC) presents a significant challenge in natural language processing. In this study, we propose the Prompt-based Fusion Model (DialoguePFM), which innovatively introduces an emotion representation that conveys the emotion label via mask token in a pre-defined prompt. We then refine both emotion and utterance representations by capturing comprehensive dialogue information using a novel speaker-aware attention mechanism, which distinguishes between self and other speakers. Subsequently, these refined representations are merged before being inputted into the classifier. Empirical evaluations conducted on three English ERC datasets and one Chinese ERC dataset reveal that our proposed model either outperforms or matches the performance of state-of-the-art baselines, underscoring its effectiveness across different languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406915043",
    "type": "article"
  },
  {
    "title": "A Survey of Document Stemming Algorithms in Information Retrieval Systems",
    "doi": "https://doi.org/10.1145/3715120",
    "publication_date": "2025-02-03",
    "publication_year": 2025,
    "authors": "Mona Alyousf; Mohamad Firas Al-halabi",
    "corresponding_authors": "",
    "abstract": "With the increase in the growth and diversity of databases and the enormity of their contents, there has become an urgent need to find advanced techniques in Natural Language Processing (NLP) applications, especially in the field of Information Retrieval (IR). One of the most popular techniques that can improve information retrieval is the stemming of text documents. Given the importance of stemming for information retrieval systems, in this paper, we present a detailed study of the adopted stemming approaches and the working mechanism of the various algorithms that follow each approach. We analyzed and evaluated the most important algorithms by comparing them based on specific criteria, including their strength in stemming, their advantages, and the disadvantages of each. Based on this comparison, we can identify the weaknesses that each stemming algorithm suffers from. We mainly aim through the study that we conducted in this paper to try to overcome the weaknesses of these algorithms and take advantage of their most important advantages to develop a new more efficient stemming algorithm for the English language.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407107975",
    "type": "article"
  },
  {
    "title": "Bidirectional Directed Acyclic Graph Neural Network for Aspect-level Sentiment Classification",
    "doi": "https://doi.org/10.1145/3716501",
    "publication_date": "2025-02-08",
    "publication_year": 2025,
    "authors": "Junjie Xu; Luwei Xiao; Anran Wu; Tianlong Ma; Daoguo Dong; Liang He",
    "corresponding_authors": "",
    "abstract": "To achieve outstanding aspect-level sentiment analysis (ASC), it is crucial to reduce the distance between aspect terms and opinion words. Recently, advanced methods in ASC use graph neural network (GNN)-based methods to leverage the syntactic dependency within the sentence, which can shorten the distance through syntactical dependencies. However, existing approaches that utilize GNNs have difficulty extracting long-distance relations in the dependency tree due to the over-smoothing problem resulting from stacking GNN layers, which limits their ability to detect remote relations. To solve this issue, we propose a Bidirectional Directed Acyclic Graph (BDAG) to reconstruct syntactic dependencies, and a Bidirectional Directed Acyclic Graph Neural Network (BDAGNN) to efficiently propagate multi-hop sentiment information. We also enhance the BDAG with affective commonsense knowledge from SenticNet for comprehensive sentiment classification. BDAGNN we proposed obtains partial state-of-the-art performance on four benchmark datasets, indicating the feasibility of encoding syntactic structure with BDAG.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407276942",
    "type": "article"
  },
  {
    "title": "Exploring Semantic Attributes for Image Caption Synthesis in Low-Resource Assamese Language",
    "doi": "https://doi.org/10.1145/3717612",
    "publication_date": "2025-02-14",
    "publication_year": 2025,
    "authors": "P Choudhury; Prithwijit Guha; Sukumar Nandi",
    "corresponding_authors": "",
    "abstract": "Research on image caption generation has predominantly focused on resource-rich languages like English, leaving resource-poor languages (like Assamese and several others) largely understudied. In this context, this paper leverages both visual and semantic attribute based features for generating captions in Assamese language. Semantic attributes refer to the significant words that represent higher-level knowledge about the image content. This work contributes through the effective use of features derived from semantic words in low resource Assamese language. The second contribution is the proposal of a Visual-Semantic Self-Attention (VSSA) module for the combination of features derived from images and semantic attributes. The VSSA module enables the image captioning model to dynamically attend to relevant regions of the image as well as the important semantic attributes, thereby leading to more contextually relevant and linguistically accurate Assamese captions. Moreover, the VSSA module is incorporated into a Transformer model to leverage the stacked attention for performance improvement. The model is trained by using both cross-entropy loss optimization and reinforcement learning approach. The effectiveness of the proposed model is evaluated through both qualitative and quantitative analyses (using BLEU-n and CIDEr metrics). The proposed model shows significant performance improvement in Assamese caption synthesis compared to previous methods, achieving 93.7% CIDEr score on the COCO-Assamese Caption (COCO-AC) dataset.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407583788",
    "type": "article"
  },
  {
    "title": "Urdu Word Sense Disambiguation: Leveraging Contextual Stacked Embedding, Siamese Transformer Encoder 1DCNN-BiLSTM, and Gloss Data Augmentation",
    "doi": "https://doi.org/10.1145/3719293",
    "publication_date": "2025-02-22",
    "publication_year": 2025,
    "authors": "Anil Ahmed; Degen Huang; Syed Yasser Arafat; Khawaja Iftekhar Rashid",
    "corresponding_authors": "",
    "abstract": "Word Sense Disambiguation (WSD) in Natural Language Processing (NLP) is crucial for discerning the correct meaning of words with multiple senses in various contexts. Recent advancements in this field, particularly Deep Learning (DL) and sophisticated language models like BERT and GPT, have significantly improved WSD performance. However, challenges persist, especially with languages like Urdu, which are known for their linguistic complexity and limited digital resources compared to English. This study addresses the challenge of advancing WSD in Urdu by developing and applying tailored Data Augmentation (DA) techniques. We introduce an innovative approach, Prompt Engineering with Retrieval Augmented Generation (RAG), leveraging GPT-3.5-turbo to generate context-sensitive Gloss Definitions (GD). Additionally, we employ sentence-level and word-level DA techniques, including Back Translation (BT) and Masked Word Prediction (MWP). To enhance sentence understanding, we combine three BERT embedding models: mBERT, mDistilBERT, and Roberta_Urdu, facilitating a more nuanced comprehension of sentences and improving word disambiguation in complex linguistic contexts. Furthermore, we propose a novel network architecture merging Transformer Encoder (TE)-CNN and TE-BiLSTM models with Multi-Head Self-Attention (MHSA), One-Dimensional Convolutional Neural Network (1DCNN), and Bidirectional Long Short-Term Memory (BiLSTM). This architecture is tailored to address polysemy and capture short and long-range dependencies critical for effective WSD in Urdu. Empirical evaluations on Lexical Sample (LS) and All Word (AW) tasks demonstrate the effectiveness of our approach, achieving an 88.9% F1 Score on the LS and a 79.2% F1 Score on AW tasks. These results underscore the importance of language-specific approaches and the potential of DA and advanced modeling techniques in overcoming challenges associated with WSD in languages with limited resources.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407857790",
    "type": "article"
  },
  {
    "title": "A Hybrid Statistical and Rule-based Approach to Extremely Low-resource Machine Transliteration",
    "doi": "https://doi.org/10.1145/3720542",
    "publication_date": "2025-02-26",
    "publication_year": 2025,
    "authors": "P. Connor",
    "corresponding_authors": "P. Connor",
    "abstract": "Machine transliteration work has focused primarily on languages with large volumes of parallel corpus, and between language pairs whose orthographies are very different. In contrast, a large proportion of the world’s languages have vastly fewer resources and employ Roman-like alphabets often with large degrees of orthographic overlap with high-resource languages. We propose that machine transliteration between languages with few training examples can be accomplished by a noisy-channel-like statistical model captured in a human editable format with practical rule-based capabilities built-in. This hybrid approach allows users to take advantage of an algorithm to find and apply common transformations in context while providing rigorous control over the output. Effectiveness is evaluated on the Bible names translation matrix dataset of Wu et al. (2018), covering 591 languages that involve 590 names on average per language pair. Our approach slightly exceeds past results and explores several features targeted at benefiting the extremely low-resource language domain.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407974294",
    "type": "article"
  },
  {
    "title": "Towards an Online Text-Based Person Search in Vietnamese Language",
    "doi": "https://doi.org/10.1145/3721979",
    "publication_date": "2025-03-06",
    "publication_year": 2025,
    "authors": "Thi-Hoai Phan; Hoang-Son Bui; Tri Le; Thi-Ngoc-Diep Do; Thuy-Binh Nguyen; Hong-Quan Nguyen; Thanh-Hai Tran; Thi Thanh Thuy Pham; Thi‐Lan Le",
    "corresponding_authors": "",
    "abstract": "In recent years, many efforts have been dedicated to text-based person search, thanks to its potential applications in various domains. However, most of these works focus on person search via queries in English and conduct offline evaluations. Despite some promising results for text-based person search in English, several challenges still prevent its widespread use in practical situations when deployed in minor languages. This paper extends person search to Vietnamese language. In terms of linguistics, English and Vietnamese belong to two different language families. In addition to the difference in vocabulary, these two languages also have opposite word structures and syntactic structures. The contributions of the paper are twofold. First, based on the network architecture of ViTAA model [23], a framework for person search through Vietnamese queries has been developed. In this framework, to take into account specific characteristics of Vietnamese language, the word tokenizing, Parts of Speech (PoS) tagging techniques of different natural language processing tools, including Underthesea, including Underthesea [21], SEACoreNLP [19], PhoNLP [11] have been investigated in order to extract language elements from Vietnamese descriptions. Our investigation shows that selecting a suitable pre-processing technique can improve person search performance by 1.28% at R@1. When incorporating these preprocessing techniques with person search model, the best accuracy was achieved with 27.08%, 51.38% and 63.00% at rank-1, rank-5 and rank-10 respectively. Second, for the first time, an online evaluation of person search through natural language queries has been conducted. A web-based application has been developed to serve online evaluation scenarios with different groups of end-users. An extensive evaluation was conducted with 30 subjects and 115 queries. Upon analyzing the experimental results, there are uncovered open issues and suggestions for future improvements in person search.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408198458",
    "type": "article"
  },
  {
    "title": "Dongba Machine Translation with Transfer Learning: Leveraging Pre-trained Ancient Chinese Models",
    "doi": "https://doi.org/10.1145/3721980",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Xinchen Ma; Mingjun Lan; Wenbo Hu; Yue Lu",
    "corresponding_authors": "",
    "abstract": "The Dongba script, a logographic writing system used by the Naxi people in religious activities, faces challenges in translation due to the advanced age of Dongba script experts and the time-consuming nature of manual deciphering. This study focuses on translating the resource-scarce Dongba script into Modern Chinese using a novel approach based on cross-lingual transfer learning from Ancient Chinese. By examining translation patterns from Ancient Chinese to Modern Chinese, we determine the feasibility of transferring knowledge from Ancient Chinese to Dongba script translation. We propose the Dongba Machine Translation Model (DMTM), a pre-trained, low-resource machine translation model that utilizes the linguistic similarities between Ancient Chinese and Dongba script to improve translation quality. The model undergoes pre-training on a large-scale Ancient Chinese corpus and fine-tuning on a small-scale Dongba script corpus, enabling effective knowledge transfer. To address the scarcity of Dongba script translation resources, we present Dongba Corpus 1.0, a fine-grained parallel dataset of Dongba script and Modern Chinese. Experimental results demonstrate that our proposed DMTM achieves a translation score of 50.01% BLEU on the test set. As no prior methods exist for Dongba script translation, we compared various architectures commonly used in low-resource translation tasks, and DMTM exhibited the best performance with a 5.39% improvement over alternative architectures tested. The implementation codes and dataset for our approach are available at https://github.com/Chloe-mxxxc/DMTM.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408218286",
    "type": "article"
  },
  {
    "title": "ConVerSum: A Contrastive Learning-Based Approach for Data-Scarce Solution of Cross-Lingual Summarization Beyond Direct Equivalents",
    "doi": "https://doi.org/10.1145/3722109",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Sanzana Karim Lora; M. Sohel Rahman; Rifat Shahriyar",
    "corresponding_authors": "",
    "abstract": "Cross-lingual summarization (CLS) is a sophisticated branch in Natural Language Processing that demands models to accurately translate and summarize articles from different source languages. Despite the improvement of the subsequent studies, This area still needs data-efficient solutions along with effective training methodologies. To the best of our knowledge, there is no feasible solution for CLS when there is no available high-quality CLS data. In this paper, we propose a novel data-efficient approach, ConVerSum , for CLS leveraging the power of con trastive learning, generating ver satile candidate sum maries in different languages based on the given source document and contrasting these summaries with reference summaries concerning the given documents. After that, we train the model with a contrastive ranking loss. Then, we rigorously evaluate the proposed approach against current methodologies and compare it to powerful Large Language Models (LLMs)- Gemini, GPT 3.5, and GPT 4o proving our model performs better for low-resource languages’ CLS. These findings represent a substantial improvement in the area, opening the door to more efficient and accurate cross-lingual summarizing techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408232479",
    "type": "article"
  },
  {
    "title": "SiTSE: Sinhala Text Simplification Dataset and Evaluation",
    "doi": "https://doi.org/10.1145/3723160",
    "publication_date": "2025-03-12",
    "publication_year": 2025,
    "authors": "Surangika Ranathunga; Rumesh Madhusanka; Himashi Rathnayake; Lahiru de Silva; Thamindu Aluthwala; Saman Peramuna; Ravi Shekhar",
    "corresponding_authors": "",
    "abstract": "Text Simplification is a task that has been minimally explored for low-resource languages. Consequently, there are only a few manually curated datasets. In this paper, we present a human curated sentence-level text simplification dataset for the Sinhala language. Our evaluation dataset contains 1,000 complex sentences and corresponding 3,000 simplified sentences produced by three different human annotators. We model the text simplification task as a zero-shot and zero resource sequence-to-sequence (seq-seq) task on the multilingual language models mT5 and mBART. We exploit auxiliary data from related seq-seq tasks and explore the possibility of using intermediate task transfer learning (ITTL). Our analysis shows that ITTL outperforms the previously proposed zero-resource methods for text simplification. Our findings also highlight the challenges in evaluating text simplification systems, and support the calls for improved metrics for measuring the quality of automated text simplification systems that would suit low-resource languages as well. Our code and data are publicly available: https://github.com/brainsharks-fyp17/Sinhala-Text-Simplification-Dataset-and-Evaluation",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408343567",
    "type": "article"
  },
  {
    "title": "Intelligent De-Identification of Medical Discharge Summaries Using Hybrid NLP Techniques",
    "doi": "https://doi.org/10.1145/3724118",
    "publication_date": "2025-03-16",
    "publication_year": 2025,
    "authors": "Ahmad Mortadi; Waleed Nazih; Mohamed I. Eldesouki; Yasser Hifny",
    "corresponding_authors": "",
    "abstract": "Medical discharge summaries are vital documents in healthcare, often containing Personally Identifiable Information (PII), raising concerns regarding privacy and regulatory compliance. This paper proposes a cutting-edge approach that utilizes intelligent data de-identification to address this challenge. This paper employs Natural Language Processing (NLP) techniques such as Named Entity Recognition (NER), a hybrid approach that integrates Machine Learning (ML) models, Regular Expressions (REGEX)-based recognizers, and extensive lists of names and addresses. The proposed method focuses on achieving a delicate balance between extracting valuable insights from data and safeguarding sensitive information. The evaluation against benchmarks demonstrates significant improvements in de-identification performance, particularly in discharge summaries. We present findings from our system’s evaluation of synthesized discharge summaries, the OntoNotes dataset, and the CoNLL-2003 dataset, demonstrating its effectiveness in anonymizing diverse medical text sources.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408488453",
    "type": "article"
  },
  {
    "title": "Low Complexity Speech Secure Hash Retrieval Algorithm Based on KDTree Nearest Neighbor Search",
    "doi": "https://doi.org/10.1145/3723161",
    "publication_date": "2025-03-16",
    "publication_year": 2025,
    "authors": "Yibo Huang; An Li; Qiuyu Zhang",
    "corresponding_authors": "",
    "abstract": "With the continuous growth of dimension in retrieval system, only a few data points are distributed near the center (empty space phenomenon), and the distance between data points in high-dimensional space is nearly equal (dimensional effect), resulting in high complexity and low accuracy in retrieval. Aiming at the above problems, this paper designs a speech secure hash retrieval scheme. In this scheme, the spectral subband centroids(SSCS) of speech is extracted to generate the feature vector, then the biometric template index is established by KDTree classification, and the specific SHA256-Ushiki chaotic encryption algorithm key is allocated to each index. The security framework is constructed according to the cancelable biometric template generated by the combination of classification and distribution key, and the binary hash vector is generated, then hash vector is encrypted. Experimental results show that through the establishment of KDTree cancelable biometric template index, the super rectangular region of the K dimensional space is constructed, which effectively solves the empty space phenomenon and the dimensional effect. Through the KDTree nearest neighbor search, the algorithm reduces the number of matches between classes, which effectively reduces computational complexity and accuracy problems. The tampering comparison of mobile terminal realizes the content verifiable retrieval. The speech encryption effectively prevents the leakage of plaintext and ensures security of the speech storage and transmission process.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408488458",
    "type": "article"
  },
  {
    "title": "A Structure-aware Approach Leveraging Semantic Relevance Graph for Annotation Alignment of Chinese Classics",
    "doi": "https://doi.org/10.1145/3725733",
    "publication_date": "2025-03-21",
    "publication_year": 2025,
    "authors": "Wei Li; Yi Li; Yanqiu Shao; Mengxi Bi",
    "corresponding_authors": "",
    "abstract": "Throughout the long history of China, ideologists have annotated classical texts, providing a valuable resource for scholarly study. Many of these annotations are presented as entire paragraphs, each sentence of which must be linked to the corresponding classical sentences, also in paragraph form. However, there has been a lack of definitions and datasets for this specific task. In this paper, we propose a definition for the task of annotation alignment of Chinese classics and present a corresponding dataset. We observe that directly matching an annotation sentence to a classical sentence may encounter challenges due to limitations in classical Chinese semantic models. Furthermore, an annotation sentence may not be directly related to the target classical sentence but may instead be more closely associated with other neighboring annotation sentences. Based on these observations, we propose a semantic relevance graph-based approach that leverages task-specific structural features. Our proposed method achieves a macro precision of 82.17 and a micro precision of 86.55, significantly surpassing all baseline models including large language models, which attests to the effectiveness of our approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408688822",
    "type": "article"
  },
  {
    "title": "LatVis: Large-scale Task-specific Language Model for Low-resource Vietnamese Multi-document Summarization",
    "doi": "https://doi.org/10.1145/3725848",
    "publication_date": "2025-03-25",
    "publication_year": 2025,
    "authors": "The Anh Le; Hai Son Le",
    "corresponding_authors": "",
    "abstract": "The Vietnamese multi-document summarization task faces three key challenges including the long input sequence problem, human-like summary generation, and the scarcity of labeled data. Transformer-based models, enhanced by parallel computation architectures and attention mechanisms, partially mitigate the challenges posed by long input sequences. Additionally, when trained on large-scale text corpora, these models achieve impressive performance in text generation, approaching human-level performance. Furthermore, pre-training with self-supervised learning objectives is an effective strategy to compensate for the scarcity of labeled data. Based on these considerations, this paper leverages large corpora of unlabeled Vietnamese text to build a large-scale pseudo-labeled multi-document dataset, which is then used to pre-train a Vietnamese task-specific language model, LatVis. Experimental results demonstrate that, even without fine-tuning, the pre-trained model achieves competitive performance compared to several previous models. After fine-tuning on approximately 300 samples, LatVis obtains notable Rouge Scores of 76.7%, 78.9%, and 73.9% for Rouge-1 F1, and 50.2%, 55.0%, and 46.7% for Rouge-2 F1 on the VMDS, ViMS, VLSP datasets, respectively. To the best of our knowledge, this is the first publicly large-scale task-specific language model pre-trained specifically for the Vietnamese multi-document summarization. This work highlights the potential of task-specific language models for advancing natural language processing for low-resource languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408811769",
    "type": "article"
  },
  {
    "title": "Tibetan Question Generation Based on Key Sentence and Knowledge Graph",
    "doi": "https://doi.org/10.1145/3725531",
    "publication_date": "2025-03-25",
    "publication_year": 2025,
    "authors": "Yan Zhuang; Yuan Sun; Yijie Li; Sisi Liu; Xiaobing Zhao",
    "corresponding_authors": "",
    "abstract": "Question generation aims to generate questions according to the given context and answer, and it has made significant progress in both Chinese and English languages. However, research on Tibetan question generation is still in the early stages, with key challenges including the omission of crucial keywords that render questions unanswerable. Existing large-scale models do not provide robust support for low-resource languages, such as GPT or BERT. To solve the problem, this paper proposes to generate Tibetan questions based on key sentences and the knowledge graph. The question generator is based on the Transformer model to better understand context and multiple sources of input information. We identify key sentences to leverage closely related information, and construct a knowledge graph to incorporate more distantly related information. The results show that the BLEU-4 reaches 43.92 on TibetanQA, surpassing existing models in Tibetan question generation and significantly improving the answerability of the generated questions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408831370",
    "type": "article"
  },
  {
    "title": "Improving Code-Mixed Hate Detection by Native Sample Mixing: A Case Study for Hindi-English Code-Mixed Scenario",
    "doi": "https://doi.org/10.1145/3726866",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Debajyoti Mazumder; Aakash Kumar; Jasabanta Patro",
    "corresponding_authors": "",
    "abstract": "Hate detection has long been a challenging task for the NLP community. The task becomes complex in a code-mixed environment because the models must understand the context and the hate expressed through language alteration. Compared to the monolingual setup, we see much less work on code-mixed hate as large-scale annotated hate corpora are unavailable for the study. To overcome this bottleneck, we propose using native language hate samples (native language samples/ native samples hereafter). We hypothesise that in the era of multilingual language models (MLMs), hate in code-mixed settings can be detected by majorly relying on the native language samples. Even though the NLP literature reports the effectiveness of MLMs on hate detection in many cross-lingual settings, their extensive evaluation in a code-mixed scenario is yet to be done. This paper attempts to fill this gap through rigorous empirical experiments. We considered the Hindi-English code-mixed setup as a case study as we have the linguistic expertise for the same. Some of the interesting observations we got are: (i) adding native hate samples in the code-mixed training set, even in small quantity, improved the performance of MLMs for code-mixed hate detection, (ii) MLMs trained with native samples alone observed to be detecting code-mixed hate to a large extent, (iii) the visualisation of attention scores revealed that, when native samples were included in training, MLMs could better focus on the hate emitting words in the code-mixed context, and (iv) finally, when hate is subjective or sarcastic, naively mixing native samples doesn’t help much to detect code-mixed hate. We have released the data and code repository to reproduce the reported results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408944674",
    "type": "article"
  },
  {
    "title": "A Linguistic Approach Towards Automatic Acquisition and Classification of Common Chinese Affective Events",
    "doi": "https://doi.org/10.1145/3726526",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Wang Ya; Cungen Cao; Xin Huang; Ming Hui; Wei Zheng",
    "corresponding_authors": "",
    "abstract": "Affective events are events that are typically associated with a positive or negative emotional state. For example, get food is a desirable event, while suffer from asthma is an undesirable event. Identifying affective events and their polarities is essential for many artificial intelligence (AI) tasks. Since sentiments toward an affective event is often implicit, affective event acquisition and classification have always been two great challenges. Encounter events are a special type of affective events. The goal of this paper is to automatically acquire and classify stereotypically positive and negative encounter events in Chinese. First of all, we collect a comprehensive list of Chinese encounter event indicators based on the literature related to Chinese encounter events. Then, we automatically extract candidate encounter events from a text corpus using these indicators as keywords. In order to acquire high-quality encounter events, we design an event filtering scheme and generate a novel stopword list. Finally, we classify these obtained encounter events exploiting the indicators. Experimental results show that the proposed approaches to acquire and classify Chinese encounter events perform promisingly well. One key advantage of the method is that no labelling and training of data is required. The automatically constructed common affective event knowledge base contains more than 38,000 encounter events annotated with affective polarity labels (negative or positive), which is currently the only resource of common affective events in Chinese language. Related resources including encounter event indicators and encounter events will be released after the publication of this paper.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409062158",
    "type": "article"
  },
  {
    "title": "TFT-TL: Token-Level Filter Training Transfer Learning for Low-Resource Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3732779",
    "publication_date": "2025-04-28",
    "publication_year": 2025,
    "authors": "Wei Dai; Dongfang Han; Turdi Tohti; Yi Liang; Zicheng Zuo; Yuanyuan Liao; Qingwen Yang",
    "corresponding_authors": "",
    "abstract": "Transfer learning plays a crucial role in low-resource machine translation by addressing the challenge of poor model performance due to limited data in low-resource languages, thereby improving translation accuracy. Current research methods not only utilize pre-trained parent models for parameter initialization and fine-tuning but also use the soft labels output by these parent models to enhance the consistency between parent and child models.However, even if the parent model performs well, there are still instances where certain token predictions are unstable. During training, if the child model incorporates these unstable token predictions, it can hinder its learning effectiveness; the child model might not fully comprehend the parent model’s prediction strategy, potentially affecting overall translation performance. To address this, we propose a training strategy called Token-Level Filter Training, designed to effectively filter out unstable token predictions from the parent model, thereby transferring the parent model’s positive knowledge to the child model. Additionally, we introduce a hierarchical ranking loss method to help the child model better learn the parent model’s prediction strategies and sequence order, thus enhancing translation accuracy and fluency. Experimental results show that our method outperforms baseline methods on the public datasets Global Voices (Id, Ca, Hu, Pl) and WMT17 (Turkish-English), with BLEU score improvements of 1.47, 0.91, 0.50, 0.54, and 0.55 respectively. These results demonstrate the effectiveness and superiority of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409893140",
    "type": "article"
  },
  {
    "title": "Optimizing Answer Generator in Vietnamese Legal Question Answering Systems Using Language Models",
    "doi": "https://doi.org/10.1145/3732938",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Huong Le Thanh; Ngoc - Anh Luu; Thanh Bon Nguyen; Tuan Minh Dao; Dinh Viet Sang",
    "corresponding_authors": "",
    "abstract": "The development of large language models (LLMs) such as ChatGPT and Gemini has led to impressive advancements in question answering (QA) systems. However, they often rely on generic knowledge from the internet, resulting in hallucinated answers when applied to domain-specific QA tasks. Furthermore, their operational dependence on powerful GPUs poses challenges for practical software deployment. Building a QA systems for low-resource languages like Vietnamese is even more challenging due to the scarcity of labeled data and limited pre-trained language models. In this study, we aim to construct a Vietnamese legal QA system using a retrieval-augmented generation approach to reduce incorrect outputs. Our focus is on improving answer generation accuracy by training small-scale LLMs suitable for real-world deployment. Our contributions are: (i) constructing Vietnamese legal provisions and QA datasets for training the system; and (ii) proposing methods to fine-tune language models with QA capabilities in the legal domain. Experimental results demonstrate that it is possible to train an LLM with fewer computational resources and a smaller dataset while maintaining effectiveness. Our findings highlight that designing an efficient training and fine-tuning strategy is crucial for overcoming these challenges, particularly in the context of Vietnamese legal question-answering tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409944830",
    "type": "article"
  },
  {
    "title": "Deep Stable Learning for Cross-lingual Dependency Parsing",
    "doi": "https://doi.org/10.1145/3735509",
    "publication_date": "2025-05-09",
    "publication_year": 2025,
    "authors": "Haijiang Liu; Chen Qiu; Q.Y. Li; Maofu Liu; Li Wang; Jinguang Gu",
    "corresponding_authors": "",
    "abstract": "The Cross-lingual Dependency Parsing (XDP) task poses a significant challenge due to the differences in dependency structures between training and testing languages, known as the out-of-distribution (OOD) problem. Our research delved into this issue in the XDP dataset by selecting 43 languages from 22 language families. We found that the primary factor of the OOD problem is the unbalanced length distribution among languages. To address the impact of the OOD problem, we propose deep stable learning for Cross-lingual Dependency Parsing (SL-XDP), which utilizes deep stable learning with a feature fusion module. In detail, we implemented five feature fusion operations for generating comprehensive representations with dependency relations and the deep stable learning algorithm to decorrelate dependency structures with sequence length. Our experiments on Universal Dependencies have demonstrated that SL-XDP can lessen the impact of the OOD problem and improve the model generalization among 21 languages, with a maximum improvement of 18%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410237444",
    "type": "article"
  },
  {
    "title": "Addressing the Complexity of Dialectal Arabic: An Enhanced Encoder-Decoder Ensemble Approach for Optimized Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3735972",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "Djalila Boughareb; Rima Boughareb; Samir Hallaci; Mohammed Raid Elislam Boukherouba; Hamid Séridi",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis has become an essential tool in understanding global narratives across social, economic, political, and commercial sectors. As social media platforms increasingly produce vast amounts of uncontrolled textual data, the need to analyze content in regional languages has grown significantly. This paper focuses on the unique challenges posed by sentiment analysis in Arabic and dialectal Arabic, with a special emphasis on the Algerian Dialects often referred to as Darija, which is characterized by its linguistic diversity and multilingual nature. One of the primary issues addressed is the lack of annotated datasets for this dialect and the complexities of accurately interpreting this diverse language. To tackle these challenges, we present DZDialect, a new dataset comprising 117569 annotated comments, and we explore various advanced methodologies for sentiment analysis. Furthermore, the study compares the performance of machine learning algorithms (SVM, NBM, KNN), deep learning algorithms (LSTM, CNN) using wor2vec as word embedding tool, and transformer-based classifiers (AraBERT Base, AraBERT Mini, AraBERT Medium, DistilBERT Multilingual, and AraGPT-2) in classifying social media posts into positive or negative sentiment categories. In addition, we introduce an innovative ensemble architecture that combines multiple pre-trained models for enhanced performance. The system leverages DistilBERT and AraBERT Base as encoders, while AraGPT-2 serves as the decoder. These components work in concert through a sophisticated stacking and voting mechanism. The results reveal promising accuracy rates, with the AraBERT Base model achieving 87.9%, the LSTM model 85%, and the SVM classifier 82%. The stacking model attained an accuracy of 91.1%, while the majority voting model reached 90%. This research contributes valuable insights into sentiment analysis for dialectal Arabic, with practical implications for real-world applications across various sectors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410444405",
    "type": "article"
  },
  {
    "title": "AKER: Arabic Knowledge-Enriched Reader for Machine Reading Comprehension",
    "doi": "https://doi.org/10.1145/3736164",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Eman Albilali; Nora Al-Twairesh; Manar Hosny",
    "corresponding_authors": "",
    "abstract": "Machine reading comprehension aims to understand a passage and answer a given question by selecting a span from the passage. Recently, pre-trained language models achieved state-of-the-art results on Arabic machine reading comprehension, yet a broad body of works suggests that BERT-based variant models fail to encode and associate common sense facts and world knowledge. To alleviate this weakness, we propose an Arabic knowledge enriched reader model, which fuses external knowledge into contextual representation using an attention and gating mechanism. We learn and generate Arabic knowledge graph embeddings that represent information from Arabic Wikidata and utilize this representation when fusing knowledge. We adopted a knowledge graph embedding scoring function to select the most relevant concepts to the context from the knowledge graph. We evaluated our approach on multiple Arabic machine reading comprehension datasets. Despite leveraging a comparatively smaller pre-trained language model, our approach significantly outperforms large language models in Arabic machine reading comprehension across multiple benchmark datasets, achieving substantial gains in both EM and F1 scores.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410548482",
    "type": "article"
  },
  {
    "title": "Enhanced Chinese-Vietnamese Cross-Language Event Detection via Aligned Knowledge Event Graph",
    "doi": "https://doi.org/10.1145/3736410",
    "publication_date": "2025-05-27",
    "publication_year": 2025,
    "authors": "Yuxin Huang; Yuanlin Yang; Zhengtao Yu; Yantuan Xian; Yan Xiang",
    "corresponding_authors": "",
    "abstract": "Chinese-Vietnamese cross-language event detection aims to cluster texts that describe the same events in Chinese and Vietnamese into corresponding event clusters. However, because Vietnamese is a low-resource language, directly using multilingual pre-trained models to align event representations in Chinese and Vietnamese texts yields suboptimal results, leading to poor performance in cross-lingual event detection. To address this challenge, we propose a method to enhance cross-lingual event detection between Chinese and Vietnamese by utilizing an aligned knowledge event graph. By leveraging aligned event knowledge, such as personal and place names, to establish correlations between events in different languages, we construct a cross-lingual aligned knowledge event graph. Under the constraint of relational associations, we use contrastive learning to model the similarities and differences between various events, making the representations of the same events in different languages more compact. This approach improves the model’s ability to represent Chinese-Vietnamese cross-lingual event texts and enhances the effectiveness of cross-lingual event detection. Experimental results demonstrate that our method, on multiple multilingual pre-trained models, achieves significant improvements across evaluation metrics such as normalized mutual information, adjusted normalized mutual information, and the adjusted rand coefficient.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410781869",
    "type": "article"
  },
  {
    "title": "Indic-ST: A Large-Scale Multilingual Corpus for Low-Resource Speech-to-Text Translation",
    "doi": "https://doi.org/10.1145/3736720",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "Nivedita Sethiya; Saanvi Nair; Puneet Walia; Chandresh Kumar Maurya",
    "corresponding_authors": "",
    "abstract": "We introduce Indic-ST, a novel dataset for speech-to-text translation (ST) task from English to Indic languages to bridge the performance gap. ST involves converting spoken input in one language into written text in another, playing a key role in real-world applications like subtitling, lecture transcription, and multilingual communication systems. Despite several efforts like Meta’s seamless m4t, OpenAI’s Whisper, or Google USM model, the performance of ST models on low-resource languages lags to that of English (or high-resource languages like European languages). Indic-ST is compiled from four distinct domains: conversational audio, religious texts, education, and news, which combined results in the Indic-ST dataset. To the best of our knowledge, this is the largest low-resource ST data covering approximately 6800 hours of English speech in the real human voice and text in 15 Indic languages with diverse scripts totaling approximately 900GB in size. To assess the usefulness of the dataset, we present the baseline performance of individual language pairs using state-of-the-art ST models. We also present a unified multilingual English-to-Indic-ST model. The code and dataset are available at https://github.com/Nivedita5/Indic-ST.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410829339",
    "type": "article"
  },
  {
    "title": "Open-ended Autoregressive Visual Storytelling via Parameter Efficient Instruction Tuning",
    "doi": "https://doi.org/10.1145/3736759",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "L. C. Liu; Aiwen Jiang; Y. Chen; Changhong Liu; Qi Huang; Mingwen Wang",
    "corresponding_authors": "",
    "abstract": "Visual storytelling (VIST) involves generating coherent, creative, and vivid narrative for a collection of images. It remains an immense challenge within cross-modal domain. Traditional mainstream storytelling work were less proficient in handling long sequential relationships. Though large-scale visual-language pre-training (VLP) models demonstrated promising prospect on cross-modal tasks. So far they still were not particularly adept at handling tasks involving image sequences. Moreover, the reference descriptions in the available VIST benchmark dataset are short and simplistic, which constrains model’s potential capabilities. Current models struggle to produce truly rich and vivid narratives. Therefore, in this paper, we will address these deficiencies, and contribute from both dataset and innovative model aspects. Firstly, by leveraging large language model (LLM), we have constructed a new dataset VIST++ which can enrich vivid narratives for open-ended image sequences. The dataset has potential on providing beneficial support for future model learning. Secondly, we have proposed an innovative auto-regressive story generation model named ReStoryGen. It can be applied to image sequences of varying lengths in an open-ended way. We have performed extensive experiments and evaluations in terms of visual grounding, coherence and non-redundancy. The experiment results have convincingly demonstrated ReStoryGen achieves impressive outcomes through utilizing parameter-efficient instruction-tuning. Related source codes and models are distributed on Github https://github.com/lixinliu1995/story_gen.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410830113",
    "type": "article"
  },
  {
    "title": "AraEventCoref: An Arabic Event Coreference Dataset and LLM Benchmarks",
    "doi": "https://doi.org/10.1145/3743047",
    "publication_date": "2025-06-05",
    "publication_year": 2025,
    "authors": "Mohammed Aldawsari; Omer Salih Dawood",
    "corresponding_authors": "",
    "abstract": "Event coreference resolution is a critical task in Natural Language Processing (NLP), enabling applications such as information extraction, text summarization, and question answering. However, resolving event coreference in Arabic presents unique challenges due to the language’s rich morphology, complex syntax, and lack of annotated resources. This article introduces AraEventCoref , the first publicly available Arabic event coreference dataset, comprising 50 annotated news articles with 1,381 events and 159 coreference chains. The dataset’s annotation agreement achieved a CoNLL score of 75.8%, ensuring high reliability across B 3 , MUC, and CEAF e} metrics. Additionally, event triggers were annotated with an inter-annotator agreement of 96% using Cohen’s Kappa, further validating dataset quality. To establish benchmarks, we developed a fine-tuned CamelBERT-msa model as a strong baseline and evaluated state-of-the-art Arabic large language models (LLMs) using both bilingual and Arabic-only prompts. Results demonstrate the effectiveness of fine-tuning for domain-specific adaptation and reveal the impact of bilingual prompting on LLM performance. By providing a high-quality dataset and benchmarking results, this work lays a foundation for advancing Arabic event coreference research and supports future developments in event relation extraction.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411066278",
    "type": "article"
  },
  {
    "title": "Towards Accurate Recognition of Historical Arabic Manuscripts: A Novel Dataset and a Generalizable Pipeline",
    "doi": "https://doi.org/10.1145/3744243",
    "publication_date": "2025-06-08",
    "publication_year": 2025,
    "authors": "Hakim Bouchal; Ahror Belaid; Farid Meziane",
    "corresponding_authors": "",
    "abstract": "In today’s digital world, we are committed to digitizing thousands of handwritten transcriptions to preserve their content. Historical Arabic Handwritten Text Recognition (HAHTR) remains a challenge for computer vision systems, due to the many difficulties inherently associated with document image quality and the complexity of Arabic script. In this work, we address the problem of recognizing historical Arabic documents that adapts to different writing styles and degrees of legibility. We developed a system that is able to recognize a whole page of a historical Arabic handwritten text in two consecutive steps comprising text line detection and recognition he proposed approach performs detection using bounding boxes followed by a neural network-based model for character-level text recognition. However, the lack of data hinders the mass digitization of Arabic historical documents. Therefore, we provide a new and freely available dataset, focusing on diverse handwriting styles to facilitate a strong generalization of the trained model. This dataset will significantly benefits researchers and practitioners by accelerating progress in the field of HAHTR. Extensive experimental work demonstrates that the recognition models are effective when trained with different sources of data, and having different writing styles does not penalize the model’s ability to generalize but rather enhances it. Additionally, we define and develop a new metric to evaluate model robustness against character misclassification, particularly for characters with similar patterns. The experiments conducted demonstrated that the proposed HAHTR pipeline is accurate and highly generalizable, as well as the validity of bounding box methods for detecting text lines. The training approach with different data sources enabled us to surpass the state-of-the-art results with 5.7% of Character Error Rate (CER) on the KHATT database.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411123811",
    "type": "article"
  },
  {
    "title": "Entity Naming in NLP: Hybrid Approach GPT Transformer and Multi-level RNN",
    "doi": "https://doi.org/10.1145/3744903",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Ahmed A. Abdulhamed; Prabhat Ranjan Singh; Shengwu Xiong",
    "corresponding_authors": "",
    "abstract": "This research addresses the limitations of existing entity-naming algorithms in natural language processing when the algorithm faces the complexities of polysemy and intricate sentence structures. We propose a novel Transformer-multi-level fusion recurrent Neural Network (T-MFRNN) model that integrates transformer-based layers for contextual understanding with a multi-level fusion recurrent neural network to capture temporal dependencies. Through rigorous experimentation on prominent NLP models like BERT, GPT2, ELECTRA, and XNet, the proposed T-MFRNN demonstrated a significant performance enhancement. Compared to existing methods, such as the boundary assembly model (BAM), the T-MFRNN exhibits a marked improvement of up to 18.66% in the F1-score, highlighting its superior ability to accurately label entities in complex biomedical texts. The T-MFRNN model’s outstanding performance underscores its potential as a robust solution for biomedical entity naming applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411377611",
    "type": "article"
  },
  {
    "title": "IMDP: A Unify Dialogue Framework with Awareness and Understanding for Implicit Personalized Dialogue Generation",
    "doi": "https://doi.org/10.1145/3674733",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Yuanying Wang; Fuyong Xu; Yingzheng Zhu; G. G. Wang; Peiyu Liu; Ran Lu",
    "corresponding_authors": "",
    "abstract": "Personalized chatbots concentrate on learning human personalities, making them act similar to real users. When it is authorized to respond to other people’s messages, it has the same way of speaking as the user. Many personalized methods have been proposed to use several persona descriptions or key-value-based persona information to assign a personality for dialogue chatbots. Most of them employ explicit user profiles. However, obtaining generous explicit user profiles are extremely time-consuming and requires tremendous manual labor. In addition, explicit user profiles cannot be updated as the user’s interests change. In this article, we propose a generation-based personalized chatbot model, IMDPchat, that learns latent user representation from the abundant users’ dialogue history. Specially, we train a personalized language model to build a global user profile using dialogue responses. To take full advantage of users’ information used in the historical dialogue, we establish a key-value memory network and construct a post-sensitive personalized selection module. The above two parts is context-aware: we endow higher weights to historical post-response pairs that are connected to the current post. To predict more personalized responses, we design a personalized response decoder that can well integrate two decoding modes, including generating tokens and copying personalized words. Experimental results indicate that the IMDPchat model outperforms previous baselines remarkably.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411378425",
    "type": "article"
  },
  {
    "title": "Exploring the Effectiveness of Pre-training Language Models with Incorporation of Diglossia for Hong Kong Content",
    "doi": "https://doi.org/10.1145/3744341",
    "publication_date": "2025-06-18",
    "publication_year": 2025,
    "authors": "Yiu Cheong Yung; Ying-Jia Lin; Hung‐Yu Kao",
    "corresponding_authors": "",
    "abstract": "In this article, we present our works to create the first Hong Kong content-based public pre-training dataset and the experiments which resulted in the creation of ELECTRA-based models for commonly used languages in Hong Kong. The creation of pre-training dataset is required for us to study the effect of diglossia on Hong Kong language model, and this is the first ever study on the effect starting all the way from dataset creation phase. Our experiment shows that removing diglossia from pre-training data hurts model performance. We will release our data and models to encourage future studies in Hong Kong languages. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411403559",
    "type": "article"
  },
  {
    "title": "Decorrelation-Based Self-Supervised Visual Representation Learning for Writer Identification",
    "doi": "https://doi.org/10.1145/3746062",
    "publication_date": "2025-06-24",
    "publication_year": 2025,
    "authors": "Arkadip Maitra; S.K. Mitra; Siladittya Manna; Saumik Bhattacharya; Umapada Pal",
    "corresponding_authors": "",
    "abstract": "Self-supervised learning has developed rapidly over the last decade and has been applied in many areas of computer vision. Decorrelation-based self-supervised pretraining has shown great promise among non-contrastive algorithms, yielding performance at par with supervised and contrastive self-supervised baselines. In this work, we explore the decorrelation-based paradigm of self-supervised learning and apply the same to learning disentangled stroke features for writer identification. Here, we propose a modified formulation of the decorrelation-based framework named SWIS which was proposed for signature verification by standardizing the features along each dimension on top of the existing framework. We show that the proposed framework outperforms the contemporary self-supervised learning framework on the writer identification benchmark by 0.89%, 0.56%, and 1.23% on word-level images and 1.15%, 0.10%, and 0.39% on page-level images on IAM, CVL ,and Firemaker datasets, respectively. The proposed framework achieves word level accuracy of 87.94%, 84.80%, 93.32%, 74.24% and page level accuracy of 97.09%, 95.58%, 96.87%, 98.40% on AHAWP, IAM, CVL and Firemaker datasets, respectively, outperforming several recent supervised methods as well.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411576475",
    "type": "article"
  },
  {
    "title": "Textile AI-Enhanced Translation System Based on Mapping Probability and In-context Learning",
    "doi": "https://doi.org/10.1145/3746227",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Hu Weilin; Wei Lin; Qizheng Li; Xiaona Yu; Chengyan Zhu",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) , such as ChatGPT, offer powerful customized and personalized translation services, and have been increasingly integrated into various specialized fields. However, when translating low-frequency terms in professional domains, LLMs frequently produce mistranslations and omissions due to insufficient training of term mapping relationships. We calculate the mapping probabilities of keywords from approximately 80,000 bilingual (Chinese-to-English) academic papers in the textile domain and apply them through in-context learning (ICL) to enhance the textile-specific translation performance of five LLMs: GPT-4o-Mini, Grok-3, Claude-3.7-Sonnet-all, Gemini-2.0-Flash and Deepseek-Chat. The results indicated that providing LLMs with simple in-context learning prompts containing the most probable English translations for 14,373 Chinese textile terms led to approximately 40% improvement in terminology translation accuracy while significantly reducing mistranslations and omissions. Building on this foundation, we introduce chain-of-thought (CoT) mechanisms to terminology translation tasks, simulating human translators’ reasoning processes to address the contextual limitations of traditional dictionary-based methods. Our developed textile AI-enhanced translation system, based on mapping probabilities and in-context learning frameworks, effectively avoids common errors in textile terminology translation that persist.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411698389",
    "type": "article"
  },
  {
    "title": "Mizo Automatic Speech Recognition: Leveraging Wav2vec 2.0 and XLS-R for Enhanced Accuracy in Low-Resource Language Processing",
    "doi": "https://doi.org/10.1145/3746063",
    "publication_date": "2025-06-28",
    "publication_year": 2025,
    "authors": "Andrew Bawitlung; Sandeep Kumar Dash; Radha Mohan Pattanayak",
    "corresponding_authors": "",
    "abstract": "This study introduces a Mizo Automatic Speech Recognition (ASR) approach by fine-tuning Wav2vec 2.0 and XLS-R models. The research presents the newly developed Mizo speech dataset, MiZonal v1.0 which significantly contributes to the advancement of low-resource language processing and plays a crucial role in preserving the Mizo language, thereby enhancing the training and assessment of speech models for this underrepresented language. It focuses on evaluating the effectiveness of these models in handling Mizo speech data, with particular emphasis on their performance in converting numerical numbers into Mizo cardinal words, which have a positive effect on the Word Error Rate (WER). The findings reveal that while the Wav2vec-Base-Mizo-Lus model achieved a WER of 16.59%, the XLS-R-300M-Mizo-Lus model outperformed it significantly, achieving a WER of 11.84% and setting a new benchmark for accuracy in the Mizo language. This shows the importance of using large multilingual speech recognition models and the cross-lingual abilities of models such as XLS-R are essential for ASR tasks in low-resource languages, leading to progress in Mizo speech technology and its applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411757614",
    "type": "article"
  },
  {
    "title": "A Survey on Dialect Arabic Processing and Analysis: Recent Advances and Future Trends",
    "doi": "https://doi.org/10.1145/3747290",
    "publication_date": "2025-07-03",
    "publication_year": 2025,
    "authors": "Abdelghani Dahou; Abdelhalim Hafedh Dahou; Mohamed Amine Chéragui; Amin Abdedaiem; Mohammed A. A. Al‐qaness; Mohamed Abd Elaziz; Ahmed A. Ewees; Zhonglong Zheng",
    "corresponding_authors": "",
    "abstract": "Advances in language models have enabled significant strides in developing language technologies tailored for analyzing and processing Dialectical Arabic (DA), which exhibits unique linguistic features and variations compared to standard Arabic. This progress has sparked a surge of interest in various research tasks within the Arabic Natural Language Processing (ANLP) domain, encompassing areas such as sentiment analysis, dialect identification, normalization and classification, fake news detection, and part-of-speech tagging. The primary objective of this survey paper is to provide a comprehensive overview of the advancements made in dialectical ANLP from 2014 to 2024. A thorough analysis is undertaken, covering a corpus of approximately 200 research papers, to offer insights into the latest developments, resources, and applications concerning dialectical Arabic. By identifying and discussing the challenges and opportunities for future research, this study aspires to serve as a valuable reference for researchers, practitioners, and enthusiasts interested in the subject matter. Central to the investigation are the recent strides in natural language processing techniques that pertain to dialectical Arabic, namely DA sentiment analysis, DA identification, DA classification, DA normalization, DA part-of-speech tagging, and the role of DA in fake news detection, among other applications. Each research category is meticulously examined, providing a comprehensive understanding of their respective contributions, significance, encountered challenges, and the availability of pertinent datasets. This exhaustive survey paper encompasses existing studies within dialectical Arabic research categories. As a result, readers are presented with a detailed reference source in pursuing advancements and innovations within this field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411976830",
    "type": "article"
  },
  {
    "title": "A Novel Dataset for Arabic Domain Specific Term Extraction and Comparative Evaluation of BERT-Based Models for Arabic Term Extraction",
    "doi": "https://doi.org/10.1145/3748323",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "Abdulmohsen Al-Thubaity",
    "corresponding_authors": "Abdulmohsen Al-Thubaity",
    "abstract": "Automatic term extraction from domain-specific corpora is a well-known challenge in natural language processing, with applications in machine translation, information retrieval, text classification, ontology building, and thesaurus construction. Unlike English, where various approaches have been explored, Arabic automatic term extraction has relied heavily on rule-based or statistical methods due to the lack of annotated datasets. This paper introduces the first annotated dataset for Arabic automatic term extraction (AraATE) in the field of Arabic linguistics. AraATE comprises 4,148 sentences and 155,502 tokens, annotated with 4,362 single and multi-word Arabic linguistic terms. The dataset covers diverse areas of Arabic linguistics, including lexicography, semantics, pragmatics, phonetics, and semiotics. Additionally, this paper presents the results of fine-tuning five BERT-based models using AraATE. The findings indicate that AraBERTv0.2-base, CAMeLBERT-MSA, and AraBERTv0.2-large exhibit comparable F1 scores (0.82, 0.81, and 0.81). However, no statistically significant difference was observed in the performance of these models. The availability of AraATE will facilitate Arabic term extraction by serving as a benchmarking dataset for different approaches. Nevertheless, the field still requires additional benchmarking datasets that cover other domains.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412161747",
    "type": "article"
  },
  {
    "title": "Dependency Scoring Learning and Corpus Boosting for Translation-Based Cross-Lingual Dependency Parsing",
    "doi": "https://doi.org/10.1145/3748315",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "Huiyao Chen; Xin Zhang; J. Chen; Meishan Zhang; Min Zhang",
    "corresponding_authors": "",
    "abstract": "Dependency parsing is a fundamental task in natural language processing that involves identifying the grammatical relationships between words in a sentence. One promising approach for performing this task in languages lacking annotated treebanks is treebank translation, which utilizes word alignments to map dependencies from a source treebank to the corresponding target translation. However, due to language differences and the limitations of word alignment tools, this method would inevitably generate noise during mapping. To reduce the effect of noise, we first exploit MetaNet to compute quality scores for each dependency and identify low-score ones as noise. MetaNet is a fake teacher that learns to score homework (dependencies) by comparing answers from the top student (strong parser) and the regular student (weak parser) without knowing the correct answer (gold-standard). With the scoring capability of MetaNet, we design an iterative algorithm to boost the target treebank quality, which trains with high-quality dependencies and relabels the low-quality dependencies. Our method achieves better results than the originally translated treebanks and shows highly competitive performances with prior methods on the Universal Dependency Treebanks v2.2. We also provide detailed analysis and discussions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412161757",
    "type": "article"
  },
  {
    "title": "Hinglish Cross-Accent Model Agnostic Meta-Learning Automatic Speech Recognition",
    "doi": "https://doi.org/10.1145/3748322",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "Sanskar Singh; S. R. Kushwaha; Avantika Singh; Shaifu Gupta",
    "corresponding_authors": "",
    "abstract": "Mother tongues and regional dialects have a substantial impact on pronunciation, leading to a range of complex and unique accents. This complexity increases in a diverse country such as India, which has code-mixed languages, which necessitates the development of an Automatic Speech Recognition(ASR) system capable of accommodating these variations effectively. In order to address this, we suggest a Hinglish voice recognition task that is cross-accented and uses the Hindi+English code-mixed conversational data to evaluate how well a model can adapt to different accents. The model-agnostic meta-learning (MAML) technique is extended by our accent-agnostic method to enable quick adaptation to unknown accents. We show through numerous experiments that our method works well. It outperforms joint multi-accent training in both mixed and cross-region settings, as measured by word error rate(WER), which is enhanced by 3-5%. Further, we investigate the effects of few-shot fine-tuning on mixed and cross-region samples, and ultimately revealing important insights into the pronunciation of Hindi-accented audio across different geographical areas of the country.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412161775",
    "type": "article"
  },
  {
    "title": "Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages",
    "doi": "https://doi.org/10.1145/3748311",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "Baban Gain; Dibyanayan Bandyopadhyay; Subhasis Mukherjee; Chandranath Adak; Asif Ekbal",
    "corresponding_authors": "",
    "abstract": "Neural Machine Translation (NMT) has made remarkable progress using large-scale textual data, but the potential of incorporating multimodal inputs, especially visual information, remains underexplored in high-resource settings. While prior research has focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model handle textual noise. Multimodal models slightly outperform text-only models in noisy settings, even when random images are used. The study’s experiments translate from English to Hindi, Bengali, and Malayalam, significantly outperforming state-of-the-art benchmarks. Interestingly, the effect of visual context varies with the level of source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features perform better in high-noise scenarios. This sheds light on the role of visual context, especially in noisy settings, and opens up a new research direction for Noisy Neural Machine Translation in multimodal setups. The research emphasizes the importance of combining visual and textual information to improve translation across various environments. Our code is publicly available at https://github.com/babangain/indicMMT.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412161955",
    "type": "article"
  },
  {
    "title": "Towards Development of New Language Resource for Urdu: The Large Vocabulary Word Embeddings",
    "doi": "https://doi.org/10.1145/3748308",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "Fatima Tuz Zuhra; Khalid Saleem",
    "corresponding_authors": "",
    "abstract": "Urdu is a resource-poor language as it lacks natural language processing (NLP) resources. NLP resources include word embeddings, treebanks, parsers, part-of-speech taggers, tokenizers, stemmers, morphological analyzers, and text visualization tools. We propose word embeddings for Urdu language. The vocabulary size of the word embeddings affects the accuracy of the NLP systems. The larger the vocabulary size is, the less the out of vocabulary words the NLP system will encounter. We also show that if word embeddings are trained on a larger amount of text, then they encode more semantic information as compared to those trained on smaller text. We propose word embeddings that have a vocabulary size of 456905 which is higher than the vocabulary sizes of two state-of-the-art Urdu word embeddings which have vocabulary sizes of 160413 and 102214. We have compared the proposed Urdu word embeddings with the state-of-the-art word embeddings for word similarity and transition-based dependency parsing tasks. The datasets used for the word similarity experiments are WordSim353 and SimLex999. The dataset used for the dependency parsing experiments is the Urdu dependencies’ treebank from the Universal Dependencies (UD) version 2.11. We have trained our proposed word embeddings on three corpora which contain Urdu-news-dataset-1M, the CLE-Urdu-Digest corpus, and the news corpus that we have compiled by scrapping Urdu news from Urdupoint website. The intrinsic evaluation for word similarity task shows that our proposed word embeddings have achieved the highest reported and significant Spearman ranked correlation co-efficient value of 0.693 for WordSim353 dataset and the highest and significant Spearman ranked correlation co-efficient value of 0.426 and Pearson correlation co-efficient value of 0.453 for SimLex999 dataset, in comparison with the two state-of-the-art word embeddings. The extrinsic evaluation of the proposed word embeddings for the dependency parsing task results in an increase in the Labelled Attachment Score (LAS) by \\(\\approx 5.16 \\% \\) and \\(\\approx 10.17\\% \\) from the LAS achieved by the state-of-the-art embeddings. Similarly, the proposed large vocabulary word embeddings have increased the Unlabelled Attachment Score (UAS) of Urdu dependency parsing by \\(\\approx 5.44\\% \\) and \\(\\approx 5.08\\% \\) as compared to the UAS of the two state-of-the-art word embeddings. It is also shown that the proposed solution results in less number of out-of-vocabulary words as compared to the state-of-the-art.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412161981",
    "type": "article"
  },
  {
    "title": "Loanword Identification in Social Media Texts with Extended Code-Switching Datasets",
    "doi": "https://doi.org/10.1145/3748317",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "Chenggang Mi; Shaoliang Xie; Yu Li; Zimo He",
    "corresponding_authors": "",
    "abstract": "As a new data augmentation method for cross-lingual natural language processing task, loanword identification has attracted more and more attentions in recent years. However, previous studies on this topic mainly focus on loanwords that were borrowed through long time language contact. In recent years, many new loanwords appear in social media texts such as Twitter, Weixin, Weibo et al. Due to that most of these new loanwords do not follow the rules of lexical borrowing, it is very difficult to achieve promising performance by the existing loanword identification methods. In this study, we propose a novel loanword identification method in social media texts based on a BERT-BLSTM-CRF model with extended code-switching data. To allievate the data sparsity exising in model training, three code-switching data generation strategies are introduced, such as high frequency phrase replacement, machine translation generation and multi-criteria searching. We also describe a multilingual loanword identification model, which can identify loanwords from different donor languages with one model. Experimental results on several datasets show that our model outperforms other baseline systems significantly.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412162018",
    "type": "article"
  },
  {
    "title": "AdvAudio: A New Information Hiding Method via Fooling Automatic Speech Recognition Model",
    "doi": "https://doi.org/10.1145/3748309",
    "publication_date": "2025-07-10",
    "publication_year": 2025,
    "authors": "X. Wang; Yehao Kong; Luyuan Xie; Shengfang Zhai; Tairui Wang; Boyan Chen; Junkai Liang; Xin Zhang",
    "corresponding_authors": "",
    "abstract": "Audio is an important medium in people’s daily life, secret information can be embedded into audio for covert communication. However, traditional audio information hiding techniques cannot achieve large hiding capacity and good imperceptibility at the same time, and rely on complex encryption, which limits their applicability in resource-constrained Internet of Things (IoT) environments. In this paper, we propose a new audio information hiding method, named AdvAudio, which can achieve large high capacity, as well as good imperceptibility, without reliance on cryptographic encryption. Specifically, AdvAudio leverages adversarial example technique to train a well-designed perturbation for cover audio and the secret information can only be extracted by the private automatic speech recognition (ASR) model. To achieve this, we implement two adversarial example algorithms tailored for both online transmission and physical-world transmission scenarios. In particular, our embedding algorithm dynamically adjusts the addition of simulated environmental noise depending on whether the audio is intended to propagate in the physical world. The iterative optimization process is guided by targeted adversarial attack objectives, ensuring that the private ASR model decodes the embedded secret information accurately. Taking DeepSpeech as the private model, we implement a prototype of AdvAudio, which achieves a high embedding capacity of 383.8 bps with excellent imperceptibility, yielding a Perceptual Evaluation of Speech Quality (PESQ) score of 2.351. Furthermore, it offers robust security, achieving a 100% defense success rate against both internal and external attacks. In the physical world, AdvAudio still maintains effectiveness across 6 different types of noise and retaining 82% accuracy even under sudden loud noises. Additionally, the secret information can only be extracted in the target environment, with a success rate of 26%, and 0% in non-target environments. In the future, we aim to enhance the steganalysis resistance of AdvAudio and explore its potential applications in various environments or with alternative ASR models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412162046",
    "type": "article"
  },
  {
    "title": "Urdu Sentential Paraphrased Plagiarism Detection Using Large Language Models",
    "doi": "https://doi.org/10.1145/3748320",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "Hafiz Rizwan Iqbal; Muhammad Sharjeel; Jawad Shafi; Usama Mehmood; Agha Ali Raza",
    "corresponding_authors": "",
    "abstract": "Plagiarism, the unauthorized reuse of text, fueled by the ease of access to online content, is a pressing concern for academia, publishers, and authors. Paraphrasing, a common tactic in textual plagiarism, compounds the problem further. The automatic detection of paraphrased plagiarism in text documents is a fundamental task in Natural Language Processing (NLP), crucial for maintaining academic integrity and authenticity. This paper presents an extensive investigation into Urdu sentential paraphrased plagiarism detection leveraging advanced Deep Neural Networks (DNNs) and Large Language Models (LLMs). The study builds upon the foundational work and proposes modifications to the Deep Text Reuse and Paraphrased Plagiarism Detection (D-TRaPPD) architecture to incorporate state-of-the-art pre-trained LLMs. The proposed approach, SELLM-D-TRaPPD, integrates various language models, including contextualized sentence embedding-based LLMs, language-agnostic and multilingual transformer-based LLMs, and multilingual knowledge-distilled transformer-based LLMs. We evaluated these models against three benchmark Urdu sentential paraphrase corpora—Urdu Sentential Paraphrase Corpus, Urdu Short Text Reuse Corpus, and Semi-automatic Urdu Sentential Paraphrase Corpus. The results demonstrate the effectiveness of SELLM-D-TRaPPD with LLMs, achieving F1 scores of 92.09%, 96.70%, and 98.23%, respectively. A comparative analysis with existing state-of-the-art methods shows significant performance improvements, establishing SELLM-D-TRaPPD as the new leading approach for Urdu sentential paraphrased plagiarism detection. These findings highlight the value of leveraging advanced neural network architectures and pre-trained LLMs in improving the accuracy and effectiveness of paraphrased plagiarism detection in Urdu, addressing a crucial gap in Urdu NLP research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412196052",
    "type": "article"
  },
  {
    "title": "MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to Indian Languages",
    "doi": "https://doi.org/10.1145/3748313",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "S. Mahapatra; Debtanu Datta; Shila Soni; Adrijit Goswami; Saptarshi Ghosh",
    "corresponding_authors": "",
    "abstract": "Most legal text in the Indian judiciary is written in complex English due to historical reasons. However, only a small fraction of the Indian population is comfortable in reading English. Hence legal text needs to be made available in various Indian languages, possibly by translating the available legal text from English. Though there has been a lot of research on translation to and between Indian languages, to our knowledge, there has not been much prior work on such translation in the legal domain. In this work, we construct the first high-quality legal parallel corpus containing aligned text units in English and nine Indian languages, that includes several low-resource languages. We also benchmark the performance of a wide variety of Machine Translation (MT) systems over this corpus, including commercial MT systems, open-source MT systems and Large Language Models. Through a comprehensive survey by Law practitioners, we check how satisfied they are with the translations by some of these MT systems, and how well automatic MT evaluation metrics agree with the opinions of Law practitioners.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412196109",
    "type": "article"
  },
  {
    "title": "KaEnLandetector: Rule-Based Language Annotation and Transformer-Based Language Detection for Kannada-English Code-Mixed Text",
    "doi": "https://doi.org/10.1145/3748310",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "K. Rashmi; H S Guruprasad; B R Shambhavi",
    "corresponding_authors": "",
    "abstract": "In multilingual societies, people tend to mix multiple languages for communication. This phenomenon is known as code-mixing or code-switching. This is visible more on social media platforms and e-commerce websites to share their opinions and feelings. Automatic language detection has become a crucial step in language processing for subsequent tasks. This study focuses on detecting the language of Kannada-English code-mixed sentences at the word level. The dataset is prepared by annotation based on rules prior to the detection. The transformer-based approach is applied using BERT and its variants. The models label words as English, Kannada, Mixed, Named Entity, and Universal for the given code-mixed texts. The highest accuracy (98%) was obtained using XLM-RoBERTa.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412196174",
    "type": "article"
  },
  {
    "title": "From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences",
    "doi": "https://doi.org/10.1145/3748312",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Prashant Kodali; Anmol Goel; Likhith Asapu; Vamshi Bonagiri; Anirudh Govil; Monojit Choudhury; Ponnurangam Kumaraguru; Manish Shrivastava",
    "corresponding_authors": "",
    "abstract": "Current computational approaches for analysing or generating code-mixed sentences do not explicitly model “naturalness” or “acceptability” of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences. Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text. To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi (en-hi) code-mixed text. Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media. Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset. Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models when trained solely using code-mixing metrics as features are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta and Bernice outperform IndicBERT across different configurations. Among Encoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder models are not able to outperform Encoder-only models. Decoder-only models perform the best when compared to all other MLLMS, with Llama 3.2 - 3B models outperforming similarly sized Qwen, Phi models. Comparison with zero and fewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks. Zero-shot transfer from English-Hindi to English-Telugu acceptability judgments using our model checkpoints proves superior to random baselines, enabling application to other code-mixed language pairs and providing further avenues of research. We publicly release our human-annotated dataset, trained checkpoints, code-mix corpus, and code for data generation and model training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412393281",
    "type": "article"
  },
  {
    "title": "Fake Reviews Detection on E-Commerce Websites Using Novel User Behavioral Features: An Experimental Study",
    "doi": "https://doi.org/10.1145/3748493",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Nimra Mughal; Ghulam Mujtaba; Muhammad Hussain Mughal; Abdul Manaf; Zainab Umair Kamangar",
    "corresponding_authors": "",
    "abstract": "The trend of writing fake reviews has recently increased with the rapid growth of e-commerce websites. Fake reviews are usually written to promote or demote the targeted products to affect the customer's decision and thus achieve a competitive advantage. Several techniques have been proposed to detect fake reviews written in English, and promising results have been obtained in the literature. Nevertheless, detecting fake reviews for low-resource languages (such as Roman Urdu) is still in the infancy stage and suffers from low classification results for two main reasons. Firstly, the existing studies mostly worked on textual features or lingual features. Secondly, the datasets used in existing studies are highly imbalanced, and proper attention to this issue may further enhance the performance. Therefore, to address these weaknesses and further enhance the performance, we have identified three types of discriminative features: review textual features, review lingual features, and review behavioral features using the Daraz dataset. Moreover, we evaluated LSTM-based text generation techniques for textual features and the random undersampling and oversampling for behavioral and lingual features to deal with class imbalance problems. Finally, we empirically evaluated the performance of machine learning and deep learning algorithms in classifying fake reviews written in the Roman Urdu language. The experimental results show that user behavioral features play a vital role in detecting fake reviews. Moreover, it was found that text generation is ineffective for balancing the textual data because the informative feature for fake review detection depends on the user behavioral features compared to textual features. Finally, the experimental results show that gradient boosting (GB) outperformed other models and improved 3% accuracy from the baseline study.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412393322",
    "type": "article"
  },
  {
    "title": "A Bilingual Legal NER Dataset and Semantics-Aware Cross-Lingual Label Transfer Method for Low-Resource Languages",
    "doi": "https://doi.org/10.1145/3748325",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Paerhati Tulajiang; Yuanyuan Sun; Yuanyu Zhang; Yingying Le; Kelaiti Xiao; Hongfei Lin",
    "corresponding_authors": "",
    "abstract": "Named Entity Recognition (NER) in specialized domains for low-resource languages remains a significant challenge due to data scarcity and the complexity of domain-specific terminology. Existing cross-lingual approaches—spanning model-transfer and data-transfer paradigms—often suffer from semantic drift and inadequate domain adaptation. To address these limitations, we introduce BiLegalNERD, the first bilingual Chinese–Uyghur legal NER dataset, constructed via a semantics-aware label transfer strategy. We further propose CUTLM, a cross-lingual annotation method that combines dual translation with Levenshtein-based alignment to ensure high-fidelity preservation of entity boundaries across languages. In addition, we present BiLegalNER, a domain-adapted multilingual NER model incorporating vocabulary expansion and bilingual fine-tuning, significantly enhancing performance on Uyghur legal texts. Experiments demonstrate that BiLegalNER achieves state-of-the-art results, with F1-scores of 86.65% on automatically generated training data and 89.11% on fully human-annotated data—outperforming the strongest multilingual baseline by 4.18% and 4.64%, respectively. Moreover, CUTLM surpasses prior cross-lingual transfer methods by up to 9.89%, confirming its effectiveness in preserving entity integrity during label projection. These findings establish a new benchmark for Uyghur legal NER and provide a scalable framework for cross-lingual NER in low-resource settings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412393339",
    "type": "article"
  },
  {
    "title": "A Novel Benchmark for Persian Table-to-Text Generation: A New Dataset and Baseline Experiments",
    "doi": "https://doi.org/10.1145/3748648",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Parman Mohammadalizadeh; Leila Safari",
    "corresponding_authors": "",
    "abstract": "The ability to comprehend and articulate structured data tables into natural language presents a pivotal yet challenging endeavor for automated systems. While substantial strides have been made in English Table-to-Text Generation (T2T) with the aid of large training datasets and advancements in deep neural networks, comparatively limited attention has been given to low-resourced languages. This paper addresses this gap by introducing the inaugural large-scale dataset for Persian T2T, comprising 128,628 table-text pairs extracted from Persian Wikipedia articles. Detailed statistical analysis and insights are presented to characterize the dataset, providing a comprehensive understanding of the intricacies inherent in Persian T2T tasks. Along with describing the dataset, the study uses this dataset to finetune multilingual pre-trained language models like mT5 and GPT-2 in order to set solid baselines. Evaluation using automated metrics—including BLEU, ROUGE-L, and METEOR—over test data reveals substantial scope for future improvements in modeling. Furthermore, targeted human assessments expose model deficiencies in the coherent generation of lengthy text and interpreting tables, particularly with respect to ambiguous spans. By offering well-matched yet complex table-description pairs and conducting rigorous comparative analyses, this paper establishes a novel benchmark, driving progress in low-resourced conditional text generation and the effectiveness of cross-lingual models. This work not only contributes significantly but also sets the stage for further exploration of this critical yet under-explored task for Persian language processing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412393348",
    "type": "article"
  },
  {
    "title": "Computational Linguistic Approach to Orthographic Representation of Egyptian Arabic: Challenges and Implications",
    "doi": "https://doi.org/10.1145/3748324",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Amany Fashwan; Sameh Alansary",
    "corresponding_authors": "",
    "abstract": "In the past, Arabic Dialects (AD) have been poorly documented linguistically due to the lack of written forms and orthographies. However, in recent years, AD have become more widely used as a means of communication since social media and the everywhere availability of the internet have created a massive overflow of information and textual data, leading to a growing interest in Natural Language Processing (NLP) for these dialects. The highly inflectional morphology and the lack of standard orthography for AD pose an important challenge for NLP work. In this paper, we handle the problem of lacking standard orthography during our work to build a morphological analyzer for Egyptian Arabic (EGY). To identify the guidelines for detecting conventional orthography, we depend on a corpus of 597,000 words that were gathered from various sources and genres. While analyzing the corpus morphologically, we handle the conventional orthography problem by assigning each word the conventional EGY Lemma and stem as close as possible to the EGY pronunciation no matter how it is typically written. Nevertheless, there are some common phenomena and complex cases involved in detecting conventional orthography during the morphological annotation process. Therefore, we take a closer look at and discuss these common phenomena and complex cases as we detect conventional orthography. These conventional orthographies are represented in a manner that facilitates the parsing of them correctly by the morphological analyzer. We tested the coverage of our morphological analyzer and compared it to one of the state-of-the-art morphological analyzers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412393358",
    "type": "article"
  },
  {
    "title": "Advancements in Transformer-Based Models for Enhanced Hate Speech Detection in Arabic: Addressing Dialectal Variations and Cross-Platform Challenges",
    "doi": "https://doi.org/10.1145/3748492",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Ahmed Fat’hAlalim; Yongjian Liu; Qing Xie; Nahla Ibrahim",
    "corresponding_authors": "",
    "abstract": "The rise of social media platforms has greatly amplified the spread of hate speech, which poses serious societal risks. The automated detection of hate speech on social media, especially in low-resource Arabic language, presents unique challenges owing to linguistic diversity, dialectal differences, and regional nuances. However, most current research efforts primarily focus on a single social media platform, which hinders the ability to address dialect differences, as residents of Arab regions often favor one platform over another. This study provides an in-depth analysis of Arabic hate speech detection using advanced transformer-based models across three datasets collected from diverse social media platforms. Our models include multilingual, monolingual models pretrained in Arabic, and models that employ transfer learning from rich-resource English. To provide a thorough evaluation, we also compared the performance of our transformer-based models with two baseline models: LR and NBSVM, highlighting their relative effectiveness in detecting hate speech across multi-dialect and multi-platforms. Our analysis includes the effects of oversampling, data augmentation, and model interpretability using the LIME method. The monolingual transformer-based models, in particular, demonstrated significant performance improvements, setting new benchmarks for F1-scores and surpassing traditional models, our best classifier achieved an F1-score of 98.82%. Additionally, we conducted thorough cross-validation across datasets to evaluate the models’ generalization capabilities. Our research significantly advances Arabic hate speech detection by tackling these complexities and laying a solid foundation for future research. However, challenges remain in detecting subtle forms of hate speech, such as implicit hate speech and fine-grained distinctions between offensive content.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412393365",
    "type": "article"
  },
  {
    "title": "Breaking Barriers in URDU WSD: The Transfer Learning Enriched MAKS Framework",
    "doi": "https://doi.org/10.1145/3748319",
    "publication_date": "2025-07-15",
    "publication_year": 2025,
    "authors": "Sarfraz Bibi; Sohail Asghar; Muhammad Zubair",
    "corresponding_authors": "",
    "abstract": "Word Sense Disambiguation (WSD) poses a significant challenge in Natural Language Processing (NLP), particularly for languages with complex morphology and semantics like Urdu. In this paper, we present Multi-layered FrAmeworK for WSD (MAKS), a comprehensive framework designed to address the nuances of Urdu WSD. MAKS comprises four integral layers: corpus creation, pre-processing, transfer layer, and classifier layer. The Corpus Creation stage involves the compilation of a diverse and representative corpus of Urdu text data. Pre-processing entails standardization and normalization of raw text through tokenization and other techniques. The Transfer Layer utilizes advanced methods such as XLM tokenization and XLM-RoBERTa for feature extraction and data preparation. Finally, the Classifier Layer employs an ensemble approach with Support Vector Machines (SVM) and Random Forests (RF) to classify tokenized data and enhance WSD accuracy. Through a systematic integration of these layers, MAKS achieves robustness and context-awareness in resolving word senses within Urdu text. Experimental evaluations demonstrate the accuracy and effectiveness of MAKS in addressing the unique challenges of Urdu WSD. Overall, MAKS offers a versatile and powerful framework for advancing Urdu language processing and understanding.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412455757",
    "type": "article"
  },
  {
    "title": "Continual Learning for Image Captioning in Hindi",
    "doi": "https://doi.org/10.1145/3749641",
    "publication_date": "2025-07-21",
    "publication_year": 2025,
    "authors": "Santosh Kumar Mishra; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Continual learning, alternatively referred to as incremental learning or lifelong learning, represents a learning paradigm enabling an agent to acquire new information without compromising its retention of previously learned knowledge. Continual learning research has resulted in a number of ways to prevent catastrophic forgetting in deep neural networks. Apparently, continuous learning of recurrent models has never been used to solve the image captioning problems in the Hindi language. This research presents a novel approach to employ continual learning in the context of generating image captions in Hindi. Given its widespread usage in South Asia and its official status in India, Hindi ranks as the third most spoken language worldwide. We look at the continual learning of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) based models for image captioning in a systematic way. In continuous image captioning problems, we incorporate an attention-based method that explicitly addresses the transient aspect of vocabulary. Using MS-COCO datasets, we employ our methods to solve the incremental image captioning problem. Our findings show that the proposed method can learn five captioning tasks in consecutive order without forgetting the one it has already learned.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412519200",
    "type": "article"
  },
  {
    "title": "Hi-MetaCap: Configuring Object Relational Transformer in Meta-Learning Environment for Image Captioning in Hindi",
    "doi": "https://doi.org/10.1145/3749642",
    "publication_date": "2025-07-21",
    "publication_year": 2025,
    "authors": "Santosh Kumar Mishra; Soham Chakraborty; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "This paper proposes a meta-learning-based, few-shot image captioning framework based on an ensemble of object-relational transformer models and a self-distillation strategy. Unlike traditional approaches, the proposed framework can train models using non-paired images and captions. In each iteration, multiple base models are trained on distinct data samples, forming an ensemble that generates pseudo-captions with confidence-based weighting. An efficient pseudo-feature generation method based on gradient descent enables learning from non-paired captions. These pseudo-features and pseudo-captions are then used to train the base models in subsequent iterations. The obtained results show that using just 1% of the paired image-caption training data significantly improves performance and generates meaningful captions. The proposed framework demonstrates the potential to reduce reliance on large, paired datasets while maintaining high-quality image captioning performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412519249",
    "type": "article"
  },
  {
    "title": "Mapping of the Nepali Dependency Treebank to Universal Dependencies",
    "doi": "https://doi.org/10.1145/3749643",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Ayan Das; Pooja Rai; Sanjay Chatterji",
    "corresponding_authors": "",
    "abstract": "Universal Dependencies (UD) have garnered notable focus for the systematic assessment of cross-lingual methods in the task of dependency parsing. In this paper, we present our initiative towards the development of a dependency treebank for the resource-poor language: Nepali, within the framework of Universal Dependencies (UD). To this end, we have mapped the Nepali treebank to the Universal Dependencies scheme. A detailed mapping procedure tailored to meet the requirements of the Nepali language has been outlined with examples. Experiments on dependency parsing with the UD-mapped Nepali treebank achieved UAS and LAS scores of 78 and 63.1 respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412593995",
    "type": "article"
  },
  {
    "title": "Corpus Fusion and Text Summarization Extraction for Multi-Feature Enhanced Entity Alignment",
    "doi": "https://doi.org/10.1145/3744558",
    "publication_date": "2025-08-01",
    "publication_year": 2025,
    "authors": "Gang Liu; Wenli Yang; Tongli Wang; He Zhihao",
    "corresponding_authors": "",
    "abstract": "Cross-lingual entity alignment endeavors to identify semantically similar entities within a knowledge graph, facilitating knowledge complementarity and enriching cross-lingual knowledge. In the context of knowledge-driven tasks such as cross-lingual question answering and knowledge recommendation, cross-lingual entity alignment can effectively enhancing the performance of these applications built upon cross-lingual knowledge graphs. However, the current methodologies exhibit constraints in efficiently extracting and combining features of multiple entities, rendering them unable to fully harness the wealth of extensive information provided by the knowledge graph. To address this challenge, we propose CFSE, a novel multi-feature enhanced fusion model, which includes deep extraction of complex entity relationship, name, and attribute features. Complex entity relationship features are extracted based on corpus fusion and RotatE model. Additionally, an algorithm based on BERT for multilingual text summarization was introduced to extract entity name and attribute features. Through comprehensive entity feature extraction, CFSE not only further improves the alignment accuracy, but also helps to maximize the depth mining of knowledge graph information. The effectiveness of CFSE in cross-lingual entity alignment applications was demonstrated through experimental results on the DBP15K dataset.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412841376",
    "type": "article"
  },
  {
    "title": "A Unified Prompt-based Framework with Label Semantic Expansion for Joint Multi-intent Detection and Slot Filling.",
    "doi": "https://doi.org/10.1145/3749372",
    "publication_date": "2025-08-01",
    "publication_year": 2025,
    "authors": "Yu Wang; Jinmao Xu; Miao Wang; Tianrui Li; Xuanren Qu",
    "corresponding_authors": "",
    "abstract": "Natural Language Understanding (NLU), which includes both intent detection (ID) and slot filling (SF), is essential for extracting crucial textual information, significantly influencing subsequent tasks and applications. Traditionally, ID and SF were treated as separate tasks, but the trend has shifted towards joint models that can handle complex scenarios, including multiple intents and slots. Recent advancements in prompt learning provide a unified framework for this purpose, but current prompt-based approaches do not fully leverage the semantic richness of intent and slot labels, and the possibility of using prompt learning to model the correlations between ID and SF for multi-intent scenarios has yet to be fully explored. To overcome these limitations and align with the joint modeling approach, a novel unified prompt-based framework, LSE-NLU, is proposed, which encompasses five subtasks designed to detect multiple intents and fill slots concurrently. Additionally, experimental results and further analysis demonstrate that our proposed model achieves new state-of-the-art performance on two public multi-intent SLU datasets. Specifically, it achieves an overall accuracy improvement of 2.9% on the MixATIS dataset and 0.2% on the MixSNIPS dataset compared to previous best models, confirming its potential in enhancing NLU performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412841379",
    "type": "article"
  },
  {
    "title": "Fine-Grained Arabic Dialect Identification: Investigating Various Approaches Across Multiple Datasets",
    "doi": "https://doi.org/10.1145/3758093",
    "publication_date": "2025-08-02",
    "publication_year": 2025,
    "authors": "Férihane Kboubi; Anja Habacha Chaïbi",
    "corresponding_authors": "",
    "abstract": "Arabic, as a language, encompasses numerous dialectical variations. Despite sharing a common vocabulary, these variants exhibit significant differences across territories, even within the same country. The Arabic dialects identification (ADI) is important for many NLP tasks and can be conducted as a preliminary step. In this work, we focus on the fine-grained ADI task, investigating the effectiveness of various feature extraction techniques and classification algorithms through a comparative study across three datasets: MADAR, NADI and NADAR. The first two are available online and well known in the literature. While we construct the third one by merging the first two datasets. We experimented with three groups of models: machine learning models, deep learning models and pre-trained language models. Our goal is to analyze the efficiency of these models for a fine-grained ADI task from different perspectives. Specifically, we analyzed the variation in classifier performance based on various criteria: the training set size, the considered dialect, and the dataset type (whether it is parallel, balanced or not). We employed the Explainable AI (XAI) technique LIME to investigate the interpretability of the models. Our analysis of the results yielded several key insights that guided our conclusions : (1) classical ML models have competitive performances compared to sophisticated neuronal models in terms of training time and F1-score. (2) The size of the dataset has a positive impact on the performance of the classifiers but in some experiments, larger datasets resulted in decreased performance. (3) The confusion errors mainly concern the dialects of geographically close regions. (4) Parallel corpora are more adapted to training ADI models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412855218",
    "type": "article"
  },
  {
    "title": "Optimized Malayalam Handwritten Character Recognition Model Using a Novel DSC and Stacked Bi-LSTM with Data Augmentation",
    "doi": "https://doi.org/10.1145/3756011",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "Bineesh Jose; K. P. Pushpalatha",
    "corresponding_authors": "",
    "abstract": "Handwritten character recognition (HCR) is a field of computer science and artificial intelligence that involves the recognition of handwritten characters and symbols, typically from digital images or scans. Malayalam has many unique letters with complex shapes, resemblance between different characters and cursive nature. Because of this behaviour of the language, Malayalam Handwritten Character Recognition (MHCR) is a tough task. Only a few HCR systems can accurately classify all types of Malayalam characters. This study introduces a novel hybrid model named MHCR, which combines an optimized “Depthwise Separable Convolution(DSC)” with “stacked Bidirectional Long Short-Term Memory (Bi-LSTM)”. The factorization process using DSC results in a significant reduction in both model size and computational cost which is novel in HCR field. The integration of these two components definitely improves the overall performance of the model and it significantly reduce the parameter size of the Proposed Model. Throughout the experiment, the proposed model employed a Depthwise Separable Convolution for feature extraction. These extracted features were subsequently classified using “stacked Bidirectional Long Short-Term Memory (Bi-LSTM)” units. Optimization of parameters has been achieved at Feature Extraction level using DSC. High interclass similarities of Malayalam characters which lead to misclassification can be solved by using Bi-LSTM sequence learning model. This contributions will definitely fill the major research gap such as lack of optimization and misclassification identified through literature review. For training and evaluating the model, MNIST dataset as well as an augmented version of self-generated dataset was used. With 99.75% accuracy, the model recognised 90 different classes of Malayalam handwritten characters.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412915339",
    "type": "article"
  },
  {
    "title": "Mixup Helps Translation, But Do the Coefficients and the Selection Strategy Influence Translation Quality?",
    "doi": "https://doi.org/10.1145/3750043",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "Yifei Zhou; Yves Lepage",
    "corresponding_authors": "",
    "abstract": "Mixup, an interpolation-based method that implicitly generates synthetic examples for training, has shown effectiveness in tasks such as image and text classification. Standard mixup randomly interpolates two samples of images and their labels. In this article, we apply mixup to low-resource machine translation tasks by interpolating in the hidden space. We investigate the impact of different mixing coefficients on this technique. We also explore whether semantically related or unrelated samples provide more benefits for interpolation compared to random selection. To investigate this, we extend the standard mixup approach by selecting samples based on distance and experimenting with different sampling settings. Our experiments are conducted across several low-resource language pairs, including Lower Sorbian and Upper Sorbian, Lower Sorbian and German, and Upper Sorbian and German. Through systematic experiments on multiple language pairs, we evaluate the effectiveness of mixup data augmentation in improving low-resource machine translation performance. Our findings indicate that the standard mixup technique enhances the quality of machine translation, resulting in an average increase of 1.9 BLEU points over the baseline Transformer model. The choice of mixing coefficients has minimal impact on translation quality, which suggests that fine-tuning these coefficients is not essential to benefit from mixup. In addition, the standard mixup performs robustly, as selecting either the most similar or most dissimilar samples for mixing does not provide a significant improvement over it.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412915422",
    "type": "article"
  },
  {
    "title": "DDM4TST: Diffusion Model for Fine-grained Text Style Transfer by Disentangled Representation",
    "doi": "https://doi.org/10.1145/3749195",
    "publication_date": "2025-07-19",
    "publication_year": 2025,
    "authors": "Cencen Liu; Wen Jun Yin; Yi Xu; Qiugang Zhan; Dongyang Zhang; Raza Ahmad",
    "corresponding_authors": "",
    "abstract": "Fine-grained Text Style Transfer (FTST) aims to make targeted and precise modifications to specific stylistic components of a sentence. Existing methods typically attempt to disentangle style and content representations for FTST. However, style and content are inherently abstract, making them difficult to formalize and manipulate independently. To address this, we propose a novel framework, Disentanglement Diffusion Model for Text Style Transfer (DDM4TST) , which reformulates style transformation as either semantic or syntactic transformation. By learning disentangled representations, the model enables accurate and fine-grained style control. Specifically, we construct a feature parsing module to effectively separate semantic and syntactic representations. We then incorporate a diffusion model conditioned on these disentangled representations, allowing for fine-grained control of stylistic attributes while preserving the core content of the original sentence. This integration into the denoising process enhances the controllability and precision of the style transformation. Extensive experiments on the benchmark StylePTB dataset demonstrate that our model consistently outperforms widely adopted baselines. The results validate the effectiveness of our approach in achieving high-quality style transformation while maintaining content fidelity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412960906",
    "type": "article"
  },
  {
    "title": "Inner-character and Inner-word Features Based Representation Learning for Chinese Word Embedding",
    "doi": "https://doi.org/10.1145/3748316",
    "publication_date": "2025-07-18",
    "publication_year": 2025,
    "authors": "Yun Zhang; Yongguo Liu; Jiajing Zhu; Zhi Chen; Shuangqing Zhai; Xindong Wu",
    "corresponding_authors": "",
    "abstract": "Chinese word embedding is a significant task in natural language processing (NLP). Most researchers explored Chinese word embedding according to radical, component, stroke n -gram and character features. Besides these features, Chinese characters still have structure and pinyin characteristics. In this paper, we propose ensemble ssp2vec and connective ssp2vec to utilize inner-character features (stroke, structure and pinyin) for learning Chinese word embeddings. Then we design hierarchical ssp2vec to forecast the contexts according to the combination of inner-character (stroke, structure and pinyin) and inner-word features (character) of Chinese words to explore different feature combination ways for learning feature relevance and comprehending word semantics, where feature substring is proposed to learn the relevancy of stroke, structure and pinyin. Experimental results for word analogy, word similarity, text classification and named entity recognition tasks demonstrate that the proposed methods outperform most state-of-the-art models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413060628",
    "type": "article"
  },
  {
    "title": "Mongolian Emotional Speech Synthesis Based on CGAN and Improved FastSpeech2",
    "doi": "https://doi.org/10.1145/3749102",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "Qing-Dao-Er-Ji Ren; Yang Yang; Lele Wang",
    "corresponding_authors": "",
    "abstract": "Mongolian speech synthesis is a technology that converts Mongolian text into Mongolian speech. In order to improve the emotional expressiveness of synthesized speech, this article first proposed a lightweight Mongolian phoneme pre-training model WFST-MnG2P based on weighted finite state transition machine. Secondly, as a representative low-resource language, Mongolian currently has no open source emotional speech corpus. For this reason, a Mongolian emotional speech corpus containing 7 discrete emotions was constructed, totaling about 2.25 hours. Finally, since the non-autoregressive acoustic model can reduce word skipping, word missing, repeated pronunciation, etc. and speed up the speech synthesis speed, this article proposes a Mongolian emotional speech synthesis model based on conditional generative adversarial network and improved FastSpeech2. Experimental results show that the average MOS score of emotional speech on the self-built Mongolian emotional speech corpus is 3.69, and the model can synthesize Mongolian emotional speech with rich multi-dimensional emotions and more robustness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413079894",
    "type": "article"
  },
  {
    "title": "Enhanced Prosody Modeling and Character Voice Controlling for Audiobook Speech Synthesis",
    "doi": "https://doi.org/10.1145/3749644",
    "publication_date": "2025-08-11",
    "publication_year": 2025,
    "authors": "Ning-Qian Wu; Zhen-Hua Ling",
    "corresponding_authors": "",
    "abstract": "Conventional speech synthesis techniques have made significant strides towards achieving human-like performance. However, the domain of audiobook speech synthesis still presents notable challenges. On one hand, the speech in audiobooks exhibits rich prosodic expressiveness, posing substantial difficulties in prosody modeling. On the other hand, the reader of audiobooks uses different voices to perform dialogues of different characters, which has been inadequately explored in existing speech synthesis methods. To address the first challenge, we integrate discourse-scale prosody modeling into the conventional autoencoder-based framework and introduce generative adversarial networks (GANs) for phoneme-level prosody code prediction. Regarding the second challenge, we further explore a character voice encoder based on the pretrained speaker verification model, integrating it into our proposed method. Experimental results validate that the proposed method enhances the prosodic expressiveness of synthesized audiobook speech. Moreover, it demonstrates the capacity to produce distinctive voices for different audiobook characters without compromising the naturalness of the synthesized speech.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413221907",
    "type": "article"
  },
  {
    "title": "Abstractive Summarization for Urdu Video Description Generation",
    "doi": "https://doi.org/10.1145/3762992",
    "publication_date": "2025-08-27",
    "publication_year": 2025,
    "authors": "Ali Faheem; Faizad Ullah; Muhammad Sohaib Ayub; Asim Karim",
    "corresponding_authors": "",
    "abstract": "Automatic summarization condenses content while retaining key ideas and details. Urdu, with over 230 million speakers globally, is one of the most widely spoken languages. The rise of Urdu content on social media platforms has driven the need for tools that enhance accessibility and engagement. The growing popularity of social media has increased the number of Urdu instructional videos. Well-written video descriptions can boost viewer engagement and improve search engine optimization; however, many lack these. Therefore, an automatic description generation system for Urdu videos is needed, which can be achieved by abstractive summarization of video transcripts. However, such public datasets are not available in Urdu. To address this problem, we investigate the usability of high-resource language datasets for Urdu abstractive text summarization. We created the first Urdu video transcription dataset Urdu How2 and evaluated its quality using intrinsic evaluation. We leverage transfer learning, a technique where knowledge from pretrained models (like mT5) is adapted to new tasks, to develop the uT5 model for generating Urdu text summaries. We further trained the model to improve its Urdu text generation capability. The machine-generated summaries are evaluated using ROUGE scores, human evaluation scores, and adversarial evaluation, providing a reliable assessment of the quality of generated descriptions and the robustness of the model against noisy text data. The human evaluation shows the proposed method generates accurate and coherent summaries compared to the translated ground truth. To the best of our knowledge, this is the first attempt to utilize a cross-lingual dataset for Urdu abstractive text summarization and video description generation. This research enhances Urdu content accessibility and lays the groundwork for advancing multilingual content generation and multimodal analysis in other low-resource languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413743384",
    "type": "article"
  },
  {
    "title": "Continuous Adaptive Knowledge Distillation for Few-Shot Relation Extraction",
    "doi": "https://doi.org/10.1145/3765749",
    "publication_date": "2025-09-03",
    "publication_year": 2025,
    "authors": "Shuo Zhao; Jianyong Duan; Li He; Hao Wang; Qing Zhang; Jie Liu",
    "corresponding_authors": "",
    "abstract": "The goal of continuous few-shot relation extraction is to enable the model to continuously learn new relation types under conditions with limited labeled training data while avoiding the forgetting of previously learned relations. The primary challenges include catastrophic forgetting of old relations and overfitting due to data sparsity. To address these challenges, this paper proposes a hierarchical distillation model that innovatively combines contrastive distillation with orthogonal adversarial distillation techniques. Specifically, we introduce a contrastive distillation approach in the feature distillation layer, integrating adaptive cosine techniques and negative sampling strategies to ensure that the model effectively retains and utilizes knowledge from previous tasks when learning new ones. Additionally, we employ orthogonal adversarial distillation in the hidden distillation layer to alleviate overfitting in low-resource scenarios. Experimental results demonstrate that our proposed method significantly outperforms the current state-of-the-art models for continuous few-shot relation extraction on two benchmark datasets, validating its effectiveness in handling few-shot data and knowledge transfer.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413941648",
    "type": "article"
  },
  {
    "title": "ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training",
    "doi": "https://doi.org/10.1145/3748321",
    "publication_date": "2025-09-03",
    "publication_year": 2025,
    "authors": "Jiayang Wu; Wensheng Gan; Jiahao Zhang; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "In the current development of large language models (LLMs), it is important to ensure the accuracy and reliability of the underlying data sources. LLMs are critical for various applications, but they often suffer from hallucinations and inaccuracies due to knowledge gaps in the training data. Knowledge graphs (KGs), as a powerful structural tool, could serve as a vital external information source to mitigate the aforementioned issues. By providing a structured and comprehensive understanding of real-world data, KGs enhance the performance and reliability of LLMs. However, it is common that errors exist in KGs while extracting triplets from unstructured data to construct KGs. This could lead to degraded performance in downstream tasks such as question-answering and recommender systems. Therefore, anomaly detection in KGs is essential to identify and correct these errors. This paper presents an anomaly detection algorithm in knowledge graphs with dual-channel learning (ADKGD). ADKGD leverages a dual-channel learning approach to enhance representation learning from both the entity-view and triplet-view perspectives. Furthermore, using a cross-layer approach, our framework integrates internal information aggregation and context information aggregation. We introduce a kullback-leibler (KL)-loss component to improve the accuracy of the scoring function between the dual channels. To evaluate ADKGD’s performance, we conduct empirical studies on three real-world KGs: WN18RR, FB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms state-of-the-art anomaly detection algorithms. The source code and datasets are publicly available at https://github.com/csjywu1/ADKGD.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413941680",
    "type": "article"
  },
  {
    "title": "Data-Augmented and Retrieval-Augmented Context Enrichment in Chinese Media Bias Detection",
    "doi": "https://doi.org/10.1145/3765898",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Luyang Lin; Jing Li; Kam‐Fai Wong",
    "corresponding_authors": "",
    "abstract": "Warning : This paper contains content that may be offensive or controversial. With the increasing pursuit of objective reports, automatically understanding media bias has drawn more attention in recent research. However, most of previous work examines media bias from Western ideology, such as the left and right in the political spectrum, which is not applicable to Chinese outlets. Based on the previous lexical bias and informational bias structure, we refine it from the Chinese perspective and go one step further to craft data with 7 fine-grained labels. To be specific, we first construct a dataset with Chinese news reports annotated by our newly designed system, and then conduct substantial experiments on it. However, the scale of the annotated data is not enough for the latest deep-learning technology, and the cost of human annotation in media bias, which needs a lot of professional knowledge, is too expensive. Thus, we explore some context enrichment methods to automatically improve these problems. In Data-Augmented Context Enrichment (DACE), we enlarge the training data; while in Retrieval-Augmented Context Enrichment (RACE), we improve information retrieval methods to select valuable information and integrate it into our models to better understand bias. Our results show that both methods outperform our baselines, while RACE methods are more efficient and have more potential.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413990653",
    "type": "article"
  },
  {
    "title": "Entity-focused Chinese Spelling Correction: Dataset and Approach",
    "doi": "https://doi.org/10.1145/3765761",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Xiao Liu; S. H. Zhu; Ying Li; Xin Chen; Yantuan Xian",
    "corresponding_authors": "",
    "abstract": "Due to the strong representation capability of pre-trained language models, Chinese spelling correction models have significantly improved. However, pre-trained language models focus on contextualized information and treat all words equally, thus ignoring the entity information. In practical application, entity words are the most difficult part to handle in various artificial intelligence tasks, i.e., machine translation, optical character recognition, and automatic speech recognition. To address this issue, we first construct an entity-focused Chinese spelling correction dataset (EFCSC), the first public spelling correction corpus to emphasize entity errors. Furthermore, we propose an entity knowledge injected language model (EKILM) designed for entity-focused spelling correction, which injects entity information into pre-trained language models, thus ensuring traditional spelling correction models pay more attention to entity words. Experiments on several benchmark datasets show that our proposed model outperforms all strong baseline models, leading to state-of-the-art results on all datasets. Extensive experiments and detailed analyses demonstrate that our proposed model enhances entity error correction ability without damaging the normal spelling correction performance. Our code and dataset will be released at https://github.com/DPloved/EFCSC to facilitate future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413991340",
    "type": "article"
  },
  {
    "title": "PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords",
    "doi": "https://doi.org/10.1145/3765962",
    "publication_date": "2025-09-05",
    "publication_year": 2025,
    "authors": "Panyut Sriwirote; Attapol Rutherford; Jalinee Thapiang; Vasan Timtong",
    "corresponding_authors": "",
    "abstract": "Although WangchanBERTa has become the de facto standard in transformer-based Thai language modeling, it still has shortcomings in regard to the understanding of foreign words, most notably English words, which are often borrowed without orthographic assimilation into Thai in many contexts. We identify the lack of foreign vocabulary in WangchanBERTa’s tokenizer as the main source of these shortcomings. We then expand WangchanBERTa’s vocabulary via vocabulary transfer from XLM-R’s pretrained tokenizer and pretrain a new model using the expanded tokenizer, starting from WangchanBERTa’s checkpoint, on a new dataset that is larger than the one used to train WangchanBERTa. Our results show that our new pretrained model, PhayaThaiBERT, outperforms WangchanBERTa in many downstream tasks and datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414019649",
    "type": "article"
  },
  {
    "title": "Considering Student's Cognitive Abilities-Entity Level and Entirety Level Sentiment Analysis of Teaching Evaluation",
    "doi": "https://doi.org/10.1145/3766517",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "Ting Cai; Yu Xiong; Xiaozhuan Qin; Yao Yu",
    "corresponding_authors": "",
    "abstract": "Student Teaching Feedback Sentiment Analysis (STFSA) plays a crucial role in evaluating teaching effectiveness, and its analysis results are influenced by both teaching process organization and students’ course cognition. However, current sentiment analysis of students’ feedback faces the challenges of integrating fine-grained entity-level sentiment and multi-polar sentiment in overall sentiment analysis. Therefore, we propose a novel sentiment analysis strategy for Entity-level and Entirety-level Teaching evaluation Sentiment analysis that considers students’ Cognitive abilities (EETSC). This strategy utilizes two-level networks: one for the entity level network based on dual attention mechanism to conduct fine-grained sentiment analysis of students’ feedback, and the other for the overall sentiment analysis network that integrates students’ personalized cognitive abilities to adjust the sentiment characteristics of different entities and obtain more accurate and reasonable emotions at the overall level. Experimental results show that, compared to the state-of-the-art baseline, EETSC achieves an accuracy of 86.34% and an F1 score of 77.13% in the recognition of six types of teaching entity sentiments, representing improvements of 2.19% and 2.18%, respectively. For entirety-level sentiment recognition, EETSC achieves an accuracy of 87.36% and an F1 score of 78.02%, with improvements of 0.76% and 1.21%. Further experimental analysis indicates that EETSC can alleviate the problem of sentiment polarity conflicts in teaching evaluations and provides a solution for integrating students’ cognitive states into teaching sentiment analysis in the field of educational natural language processing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414128107",
    "type": "article"
  },
  {
    "title": "Gamified Crowd-sourcing for Word Sense Disambiguation of Turkish",
    "doi": "https://doi.org/10.1145/3767160",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Dilara TORUNOGLU SELAMET; Ali Şentaş; Gülşen Eryiğit",
    "corresponding_authors": "",
    "abstract": "Word sense disambiguation (WSD) is the process of determining the correct meaning of a word based on its context in a sentence, a task that remains one of the core challenges in natural language processing (NLP) despite the advancements made by LLMs. Although LLMs have improved WSD performance, challenges remain—especially in nuanced contexts, low-resource languages, and domain-specific terminology. These gaps can hinder the accuracy of LLMs in high-stakes applications and multilingual settings. In response to the need for WSD, our study introduces a novel approach to WSD data collection through gamified crowdsourcing, which, to our knowledge, has not been previously applied in this field. A messaging bot method has been used to engage a wide, diverse audience to generate high-quality WSD data. Using a multiplayer game format, native speakers provide examples for different senses of ambiguous words and rate others’ contributions. Together with the suggested enhancements, this platform attracted a diverse range of participants—spanning ages, backgrounds, and genders—20 times more than a similar crowdsourcing method applied to a different NLP task, and sustained their engagement over an extended period. Unlike conventional academic crowdsourcing pools, this approach focuses on drawing individuals from varied backgrounds beyond academia or AI-focused communities. It not only gathered data but also encouraged participants to evaluate each other’s contributions, creating a rich and reliable dataset. Our findings suggest that gamified crowdsourcing can be a powerful tool for creating WSD corpora. Our approach not only supports future WSD research but also contributes valuable training data for developing more precise and resilient language models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414244060",
    "type": "article"
  },
  {
    "title": "Political Bias Detection from Hindi News Using Neutrosophic Sets, MuRil and Extended Hindi SentiWordNet",
    "doi": "https://doi.org/10.1145/3767316",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Sayani Ghosal; Aditya Bachhawat; Amita Jain; Devendra K. Tayal",
    "corresponding_authors": "",
    "abstract": "The Media is stated as the “fourth pillar of democracy\" that influences day to day life of each individual. The falling rank of India in the World Press Freedom Index (WPFI) portrays biasness in news articles, especially for political news articles. The biased headlines morph the perception of the people regarding the topic of interest and fill them up with baseless prejudice. Currently, thousands of Hindi newspaper publishers in India have more than 100 million subscribers. Political bias detection from news is a fresh research area, researchers did not considers contextual analysis yet. This is the first political news bias detection model that considers neutrosophic sets for contextual word analysis to address inconsistency and indeterminacy, newly enhanced Hindi SentiWordNet and MuRil language model for encoded output and multiclass classification. This proposed novel model transforms input words into a neutrosophic domain and is characterized by degree of truth, indeterminacy and false membership components. This set applies the concept of indeterminacy that improves the biasness detection accuracy for each news headline. The experimentation and evaluation of proposed model shows the better accuracy than the state of the art methods, MuRil language model combined with neutrosophic sets improves 37.5% to 50% accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414245313",
    "type": "article"
  },
  {
    "title": "A Robust and Linguistically-Aware Hate Speech Detection System for Roman Urdu",
    "doi": "https://doi.org/10.1145/3768571",
    "publication_date": "2025-09-19",
    "publication_year": 2025,
    "authors": "Ehtesham Hashmi; Hishamuddin Ahmad; Muhammad Tayyab Mazhar; Sule Yildirim Yayilgan; Mehtab Afzal; Sarang Shaikh",
    "corresponding_authors": "",
    "abstract": "Social media sites have developed into a common space for individuals to share their concerns and opinions. There is a chance for individuals and organizations to participate in online behavior that breaches accepted social norms because of the preservation of anonymity and the freedom to communicate ideas without restriction. This leads to a rise in the degree and intensity of hate speech in the online environment. Urdu is the national language of Pakistan and is also widely spoken across several other countries, with over 170 million speakers worldwide. This research addresses the detection of hate speech in Roman Urdu, a prevalent language in Asia, where limited resources exist for mitigating hate speech compared to English. Leveraging machine learning, deep learning, ensemble learning, and natural language processing, we developed a system proficient in understanding Roman Urdu language and culture, capable of identifying diverse hate speech manifestations like abusive language, religious hate, sexism, and racism. We expanded the Roman Urdu Hate Speech and Offensive Language Detection dataset to encompass 30,955 instances, incorporating a novel ”Racism” category. Our dataset includes various classes of hate speech such as abusive/offensive, religious hate, sexism, and racism, each reflecting distinct patterns of discriminatory language prevalent in Roman Urdu. After executing text pre-processing, we utilized feature extraction techniques such as Bag of Words and Term Frequency-Inverse Document Frequency embeddings. For model building, we employed several supervised machine learning algorithms, including Random Forest, Decision Tree, Multinomial Naive Bayes, Support Vector Machine, and ensemble methods, coupled with K-Fold cross-validation for robust validation. Additionally, unsupervised learning techniques such as the Gaussian Mixture Model and k-means clustering were also implemented. Deep learning approaches, including Bidirectional Encoder Representations from Transformers, Convolutional Neural Networks, Long Short-Term Memory networks, and multilingual BERT, were explored. Among these, mBERT distinguished itself by achieving an impressive accuracy of 92%, notably surpassing the baseline performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414354004",
    "type": "article"
  },
  {
    "title": "HuEID: Hybrid Deep Learning for Cyberbullying Detection using Multi-Modal Urdu Text and Emojis",
    "doi": "https://doi.org/10.1145/3769294",
    "publication_date": "2025-09-24",
    "publication_year": 2025,
    "authors": "Sidra Tahir; Asif Nawaz",
    "corresponding_authors": "",
    "abstract": "In the age of digital communication, social media platforms have become essential to our daily lives, providing unprecedented opportunities for interaction and information sharing. However, these platforms have also become a source for cyberbullying, which can have distressing effects on individuals and communities. Despite extensive research, detecting cyberbullying in multilingual contexts, particularly in languages like Urdu, remains a significant challenge. This paper presents HuEID: an innovative approach for cyberbullying detection in Urdu text by using multi-modal social media data and employing a hybrid deep learning model. HuEID method integrates textual data and emojis, capturing the nuanced expressions commonly found in social media platforms. The process begins with comprehensive multi-modal data collection and meticulous data preprocessing. It utilizes a Bi-LSTM for effective text feature extraction and a CNN for emoji feature extraction, resulting in a robust bullying detection system through Transfer Learning. The proposed model performs better in identifying cyberbullying instances, highlighting the critical role of multi-modal data and advanced deep learning techniques. Numerous experiments have been conducted to show the performance efficiency of the HuEID. The experimental results indicated that the HuEID attained superior accuracy with the value of 94% on dataset 1 and 97% on dataset 2. When compared with the benchmark methods, the performance of HuEID shows 7% improvement in accuracy and a 20% improvement in F1 score.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414474918",
    "type": "article"
  },
  {
    "title": "Complex Concept-Based Readability Estimation from Arabic Curriculum",
    "doi": "https://doi.org/10.1145/3770070",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Sultan Almujaiwel; Damith Premasiri; Tharindu Ranasinghe; Mahmoud El‐Haj; Ruslan Mitkov",
    "corresponding_authors": "",
    "abstract": "This paper presents an approach to readability estimation that focuses on conceptual rather than linguistic complexity, using the extensive SaudiTextBooks textbooks. We introduce DARES 2.0 , an enhanced concept-based readability training dataset designed to estimate the readability of Saudi educational texts. Building on DARES 1.0, DARES 2.0 extends the scope of conceptual complexity by replacing repetitive concepts and manually revising the input features with unique terms and their surrounding contexts from the SaudiTextBooks, spanning grades 1 to 12. The refined DARES 2.0 is employed to fine-tune pre-trained transformer models, including XLM-R Base, mBERT, AraELECTRA, AraBERTv2, and CAMeLBERTmix. The findings suggest that both the dataset and experimental setup require further development to ensure a larger, higher-quality dataset and to support more extensive fine-tuning experiments, in addition to exploring transfer learning from other languages and enhancing the diversity and richness of Arabic concepts. These developments pave the way for further advancements in concept-based readability estimation in educational contexts in future work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414780056",
    "type": "article"
  },
  {
    "title": "An Invasive Embedding Model in Favor of Low-Resource Languages Understanding",
    "doi": "https://doi.org/10.1145/3771926",
    "publication_date": "2025-10-15",
    "publication_year": 2025,
    "authors": "Saedeh Tahery; Saeed Farzi",
    "corresponding_authors": "",
    "abstract": "The contextual representations generated by multilingual pre-trained language models involve semantic content that conveys the meaning of sentences, as well as language-dependent nuances that indicate language-specific information. However, for cross-lingual machine learning tasks, particularly those suffering from data scarcity, the language-specific information blended in the contextual representation not only increases complexity but also compromises performance. This challenge is especially critical for natural language understanding (NLU) tasks such as intent detection (ID) and slot filling (SF), which are fundamental for the functionality of multilingual dialogue systems. Central to the idea is eradicating language-specific information from the contextual representation generated by an encoder through an adversarial manner, while preserving semantic information through input reconstruction via a decoder. In this regard, we propose an encoder-decoder model that employs adversarial learning techniques to enhance knowledge transferability across diverse languages for cross-lingual NLU tasks. Experimental results on two publicly available datasets, Facebook-multilingual (XTOD) and Persian-ATIS, demonstrate that our model significantly outperforms its main counterparts and achieves competitive results compared to state-of-the-art models across diverse languages in zero-shot scenarios. Notably, our findings highlight that achieving language-independent representations through adversarial learning followed by multi-task learning improves the model's performance in terms of accuracy and F1-score for NLU tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415209373",
    "type": "article"
  },
  {
    "title": "Using Communities of Words Derived from Multilingual Word Vectors for Cross-Language Information Retrieval in Indian Languages",
    "doi": "https://doi.org/10.1145/3208358",
    "publication_date": "2018-12-17",
    "publication_year": 2018,
    "authors": "Paheli Bhattacharya; Pawan Goyal; Sudeshna Sarkar",
    "corresponding_authors": "",
    "abstract": "We investigate the use of word embeddings for query translation to improve precision in cross-language information retrieval (CLIR). Word vectors represent words in a distributional space such that syntactically or semantically similar words are close to each other in this space. Multilingual word embeddings are constructed in such a way that similar words across languages have similar vector representations. We explore the effective use of bilingual and multilingual word embeddings learned from comparable corpora of Indic languages to the task of CLIR. We propose a clustering method based on the multilingual word vectors to group similar words across languages. For this we construct a graph with words from multiple languages as nodes and with edges connecting words with similar vectors. We use the Louvain method for community detection to find communities in this graph. We show that choosing target language words as query translations from the clusters or communities containing the query terms helps in improving CLIR. We also find that better-quality query translations are obtained when words from more languages are used to do the clustering even when the additional languages are neither the source nor the target languages. This is probably because having more similar words across multiple languages helps define well-defined dense subclusters that help us obtain precise query translations. In this article, we demonstrate the use of multilingual word embeddings and word clusters for CLIR involving Indic languages. We also make available a tool for obtaining related words and the visualizations of the multilingual word vectors for English, Hindi, Bengali, Marathi, Gujarati, and Tamil.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2905037098",
    "type": "article"
  },
  {
    "title": "Deep Contextualized Word Embeddings for Universal Dependency Parsing",
    "doi": "https://doi.org/10.1145/3326497",
    "publication_date": "2019-07-13",
    "publication_year": 2019,
    "authors": "Yijia Liu; Wanxiang Che; Yuxuan Wang; Bo Zheng; Bing Qin; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Deep contextualized word embeddings (Embeddings from Language Model, short for ELMo), as an emerging and effective replacement for the static word embeddings, have achieved success on a bunch of syntactic and semantic NLP problems. However, little is known about what is responsible for the improvements. In this article, we focus on the effect of ELMo for a typical syntax problem—universal POS tagging and dependency parsing. We incorporate ELMo as additional word embeddings into the state-of-the-art POS tagger and dependency parser, and it leads to consistent performance improvements. Experimental results show the model using ELMo outperforms the state-of-the-art baseline by an average of 0.91 for POS tagging and 1.11 for dependency parsing. Further analysis reveals that the improvements mainly result from the ELMo’s better abstraction ability on the out-of-vocabulary (OOV) words, and the character-level word representation in ELMo contributes a lot to the abstraction. Based on ELMo’s advantage on OOV, experiments that simulate low-resource settings are conducted and the results show that deep contextualized word embeddings are effective for data-insufficient tasks where the OOV problem is severe.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2961010423",
    "type": "article"
  },
  {
    "title": "Korean Part-of-speech Tagging Based on Morpheme Generation",
    "doi": "https://doi.org/10.1145/3373608",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Hyun-Je Song; Seong-Bae Park",
    "corresponding_authors": "",
    "abstract": "Two major problems of Korean part-of-speech (POS) tagging are that the word-spacing unit is not mapped one-to-one to a POS tag and that morphemes should be recovered during POS tagging. Therefore, this article proposes a novel two-step Korean POS tagger that solves the problems. This tagger first generates a sequence of lemmatized and recovered morphemes that can be mapped one-to-one to a POS tag using an encoder-decoder architecture derived from a POS-tagged corpus. Then, the POS tag of each morpheme in the generated sequence is finally determined by a standard sequence labeling method. Since the knowledge for segmenting and recovering morphemes is extracted automatically from a POS-tagged corpus by an encoder-decoder architecture, the POS tagger is constructed without a dictionary nor handcrafted linguistic rules. The experimental results on a standard dataset show that the proposed method outperforms existing POS taggers with its state-of-the-art performance.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3009966295",
    "type": "article"
  },
  {
    "title": "Morphological Segmentation to Improve Crosslingual Word Embeddings for Low Resource Languages",
    "doi": "https://doi.org/10.1145/3390298",
    "publication_date": "2020-06-21",
    "publication_year": 2020,
    "authors": "Santwana Chimalamarri; Dinkar Sitaram; Ashritha Jain",
    "corresponding_authors": "",
    "abstract": "Crosslingual word embeddings developed from multiple parallel corpora help in understanding the relationships between languages and improving the prediction quality of machine translation. However, in low resource languages with complex and agglutinative morphologies, inducing good-quality crosslingual embeddings becomes challenging due to the problem of complex morphological forms and rare words. This is true even for languages that share common linguistic structure. In our work, we have shown that performing a simple morphological segmentation upon the corpora prior to the generation of crosslingual word embeddings for both roots and suffixes greatly improves the prediction quality and captures semantic similarities more effectively. To exhibit this, we have chosen two related languages: Telugu and Kannada of the Dravidian language family. We have also tested our method upon a widely spoken North Indian language, Hindi, belonging to the Indo-European language family, and have observed encouraging results.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3038489922",
    "type": "article"
  },
  {
    "title": "Description of the Chinese-to-Spanish Rule-Based Machine Translation System Developed Using a Hybrid Combination of Human Annotation and Statistical Techniques",
    "doi": "https://doi.org/10.1145/2738045",
    "publication_date": "2015-11-21",
    "publication_year": 2015,
    "authors": "Marta R. Costa‐jussà; Jordi Centelles",
    "corresponding_authors": "",
    "abstract": "Two of the most popular Machine Translation (MT) paradigms are rule based (RBMT) and corpus based, which include the statistical systems (SMT). When scarce parallel corpus is available, RBMT becomes particularly attractive. This is the case of the Chinese--Spanish language pair. This article presents the first RBMT system for Chinese to Spanish. We describe a hybrid method for constructing this system taking advantage of available resources such as parallel corpora that are used to extract dictionaries and lexical and structural transfer rules. The final system is freely available online and open source. Although performance lags behind standard SMT systems for an in-domain test set, the results show that the RBMT’s coverage is competitive and it outperforms the SMT system in an out-of-domain test set. This RBMT system is available to the general public, it can be further enhanced, and it opens up the possibility of creating future hybrid MT systems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2242292500",
    "type": "article"
  },
  {
    "title": "Improving Handwritten Arabic Character Recognition by Modeling Human Handwriting Distortions",
    "doi": "https://doi.org/10.1145/2764456",
    "publication_date": "2015-11-21",
    "publication_year": 2015,
    "authors": "Maad Shatnawi; Sherief Abdallah",
    "corresponding_authors": "",
    "abstract": "Handwritten Arabic character recognition systems face several challenges, including the unlimited variation in human handwriting and the unavailability of large public databases of handwritten characters and words. The use of synthetic data for training and testing handwritten character recognition systems is one of the possible solutions to provide several variations for these characters and to overcome the lack of large databases. While this can be using arbitrary distortions, such as image noise and randomized affine transformations, such distortions are not realistic. In this work, we model real distortions in handwriting using real handwritten Arabic character examples and then use these distortion models to synthesize handwritten examples that are more realistic. We show that the use of our proposed approach leads to significant improvements across different machine-learning classification algorithms.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2242996593",
    "type": "article"
  },
  {
    "title": "Chinese Spelling Checker Based on an Inverted Index List with a Rescoring Mechanism",
    "doi": "https://doi.org/10.1145/2826235",
    "publication_date": "2015-11-11",
    "publication_year": 2015,
    "authors": "Jui‐Feng Yeh; Wen-Yi Chen; Mao-Chuan Su",
    "corresponding_authors": "",
    "abstract": "An approach is proposed for Chinese spelling error detection and correction, in which an inverted index list with a rescoring mechanism is used. The inverted index list is a structure for mapping from word to desired sentence, and for representing nodes in lattices constructed through character expansion (according to predefined phonologically and visually similar character sets). Pruning based on a contextual dependency confidence measure was used to markedly reduce the search space and computational complexity. Relevant mapping relations between the original input and desired input were obtained using a scoring mechanism composed of class-based language and maximum entropy correction models containing character, word, and contextual features. The proposed method was evaluated using data sets provided by SigHan 7 bakeoff. The experimental results show that the proposed method achieved acceptable performance in terms of recall rate or precision rate in error sentence detection and error location detection, and it outperformed other approaches in error location detection and correction.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2296547479",
    "type": "article"
  },
  {
    "title": "Bigram Language Models and Reevaluation Strategy for Improved Recognition of Online Handwritten Tamil Words",
    "doi": "https://doi.org/10.1145/2671014",
    "publication_date": "2015-04-20",
    "publication_year": 2015,
    "authors": "Suresh Sundaram; A. G. Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "This article describes a postprocessing strategy for online, handwritten, isolated Tamil words. Contributions have been made with regard to two issues hardly addressed in the online Indic word recognition literature, namely, use of (1) language models exploiting the idiosyncrasies of Indic scripts and (2) expert classifiers for the disambiguation of confused symbols. The input word is first segmented into its individual symbols, which are recognized using a primary support vector machine (SVM) classifier. Thereafter, we enhance the recognition accuracy by utilizing (i) a bigram language model at the symbol or character level and (ii) expert classifiers for reevaluating and disambiguating the different sets of confused symbols. The symbol-level bigram model is used in a traditional Viterbi framework. The concept of a character comprising multiple symbols is unique to Dravidian languages such as Tamil. This multi-symbol feature of Tamil characters has been exploited in proposing a novel, prefix-tree-based character-level bigram model that does not use Viterbi search; rather it reduces the search space for each input symbol based on its left context. For disambiguating confused symbols, a dynamic time-warping approach is proposed to automatically identify the parts of the online trace that discriminates between the confused classes. Fine classification of these regions by dedicated expert SVMs reduces the extent of confusions between such symbols. The integration of segmentation, prefix-tree-based language model and disambiguation of confused symbols is presented on a set of 15,000 handwritten isolated online Tamil words. Our results show recognition accuracies of 93.0% and 81.6% at the symbol and word level, respectively, as compared to the baseline classifier performance of 88.4% and 65.1%, respectively.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2402235474",
    "type": "article"
  },
  {
    "title": "A Semisupervised Tag-Transition-Based Markovian Model for Uyghur Morphology Analysis",
    "doi": "https://doi.org/10.1145/2968410",
    "publication_date": "2016-11-04",
    "publication_year": 2016,
    "authors": "Eziz Tursun; Debasis Ganguly; Turghun Osman; Yating Yang; Ghalip Abdukerim; Junlin Zhou; Qun Liu",
    "corresponding_authors": "",
    "abstract": "Morphological analysis, which includes analysis of part-of-speech (POS) tagging, stemming, and morpheme segmentation, is one of the key components in natural language processing (NLP), particularly for agglutinative languages. In this article, we investigate the morphological analysis of the Uyghur language, which is the native language of the people in the Xinjiang Uyghur autonomous region of western China. Morphological analysis of Uyghur is challenging primarily because of factors such as (1) ambiguities arising due to the likelihood of association of a multiple number of POS tags with a word stem or a multiple number of functional tags with a word suffix, (2) ambiguous morpheme boundaries, and (3) complex morphopholonogy of the language. Further, the unavailability of a manually annotated training set in the Uyghur language for the purpose of word segmentation makes Uyghur morphological analysis more difficult. In our proposed work, we address these challenges by undertaking a semisupervised approach of learning a Markov model with the help of a manually constructed dictionary of “suffix to tag” mappings in order to predict the most likely tag transitions in the Uyghur morpheme sequence. Due to the linguistic characteristics of Uyghur, we incorporate a prior belief in our model for favoring word segmentations with a lower number of morpheme units. Empirical evaluation of our proposed model shows an accuracy of about 82%. We further improve the effectiveness of the tag transition model with an active learning paradigm. In particular, we manually investigated a subset of words for which the model prediction ambiguity was within the top 20%. Manually incorporating rules to handle these erroneous cases resulted in an overall accuracy of 93.81%.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2554013724",
    "type": "article"
  },
  {
    "title": "Loanword Identification in Low-Resource Languages with Minimal Supervision",
    "doi": "https://doi.org/10.1145/3374212",
    "publication_date": "2020-02-20",
    "publication_year": 2020,
    "authors": "Chenggang Mi; Lei Xie; Yanning Zhang",
    "corresponding_authors": "",
    "abstract": "Bilingual resources play a very important role in many natural language processing tasks, especially the tasks in cross-lingual scenarios. However, it is expensive and time consuming to build such resources. Lexical borrowing happens in almost every language. This inspires us to detect these loanwords effectively, and to use the “loanword (in receipt language)”-“donor word (in donor language)” to extend the bilingual resource for NLP tasks in low-resource languages. In this article, we propose a novel method to identify loanwords in Uyghur. The most important advantage of this method is that the model only relies on large amount of monolingual corpora and only a small scale of annotated data. Our loanword identification model includes two parts: loanword candidate generation and loanword prediction. In the first part, we use two large-scale monolingual corpora and a small bilingual dictionary to train a cross-lingual embedding model. Since semantic unrelated words often cannot be treated as loanword pairs, a loanword candidate list will be generated according to this model and a word list in Uyghur. In the second part, we predict from the preceding candidates based on a log-linear model that integrates several features such as pronunciation similarity, part-of-speech tags, and hybrid language modeling. To evaluate the effectiveness of our proposed method, we conduct two types of experiments: loanword identification and OOV translation. Experimental results showed that (1) our proposed method achieved significant F1 improvements compared to other models in all four loanword identification tasks in Uyghur, and (2) after extending the existing translation models with loanword identification results, OOV rates in several language pairs reduced significantly and the translation performance improved.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3031089630",
    "type": "article"
  },
  {
    "title": "A Link Prediction Approach for Accurately Mapping a Large-scale Arabic Lexical Resource to English WordNet",
    "doi": "https://doi.org/10.1145/3404854",
    "publication_date": "2020-10-13",
    "publication_year": 2020,
    "authors": "Gilbert Badaro; Hazem Hajj; Nizar Habash",
    "corresponding_authors": "",
    "abstract": "Success of Natural Language Processing (NLP) models, just like all advanced machine learning models, rely heavily on large -scale lexical resources. For English, English WordNet (EWN) is a leading example of a large-scale resource that has enabled advances in Natural Language Understanding (NLU) tasks such as word sense disambiguation, question answering, sentiment analysis, and emotion recognition. EWN includes sets of cognitive synonyms called synsets, which are interlinked by means of conceptual-semantic and lexical relations and where each synset expresses a distinct concept. However, other languages are still lagging behind in having large-scale and rich lexical resources similar to EWN. In this article, we focus on enabling the development of such resources for Arabic. While there have been efforts in developing an Arabic WordNet (AWN), the current version of AWN has its limitations in size and in lacking transliteration standards, which are important for compatibility with Arabic NLP tools. Previous efforts for extending AWN resulted in a lexicon, called ArSenL, that overcame the size and the transliteration standard limitation but was limited in accuracy due to the heuristic approach that only considered surface matching between the English definitions from the Standard Arabic Morphological Analyzer (SAMA) and EWN synset terms, and that resulted in inaccurate mapping of Arabic lemmas to EWN’s synsets. Furthermore, there has been limited exploration of other expansion methods due to expensive manual validation needed. To address these limitations of simultaneously having large-scale size with high accuracy and standard representations, the mapping problem is formulated as a link prediction problem between a large-scale Arabic lexicon and EWN, where a word in one lexicon is linked to a word in another lexicon if the two words are semantically related. We use a semi-supervised approach to create a training dataset by finding common terms in the large-scale Arabic resource and AWN. This set of data becomes implicitly linked to EWN and can be used for training and evaluating prediction models. We propose the use of a two-step Boosting method, where the first step aims at linking English translations of SAMA’s terms to EWN’s synsets. The second step uses surface similarity between SAMA’s glosses and EWN’s synsets. The method results in a new large-scale Arabic lexicon that we call ArSenL 2.0 as a sequel to the previously developed sentiment lexicon ArSenL. A comprehensive study covering both intrinsic and extrinsic evaluations shows the superiority of the method compared to several baseline and state-of-the-art link prediction methods. Compared to previously developed ArSenL, ArSenL 2.0 included a larger set of sentimentally charged adjectives and verbs. It also showed higher linking accuracy on the ground truth data compared to previous ArSenL. For extrinsic evaluation, ArSenL 2.0 was used for sentiment analysis and showed, here, too, higher accuracy compared to previous ArSenL.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3096082174",
    "type": "article"
  },
  {
    "title": "Deep Learning Approach for the Morphological Synthesis in Malayalam and Tamil at the Character Level",
    "doi": "https://doi.org/10.1145/3457976",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "B. Premjith; K. P. Soman",
    "corresponding_authors": "",
    "abstract": "Morphological synthesis is one of the main components of Machine Translation (MT) frameworks, especially when any one or both of the source and target languages are morphologically rich. Morphological synthesis is the process of combining two words or two morphemes according to the Sandhi rules of the morphologically rich language. Malayalam and Tamil are two languages in India which are morphologically abundant as well as agglutinative. Morphological synthesis of a word in these two languages is challenging basically because of the following reasons: (1) Abundance in morphology; (2) Complex Sandhi rules; (3) The possibilty in Malayalam to form words by combining words that belong to different syntactic categories (for example, noun and verb); and (4) The construction of a sentence by combining multiple words. We formulated the task of the morphological generation of nouns and verbs of Malayalam and Tamil as a character-to-character sequence tagging problem. In this article, we used deep learning architectures like Recurrent Neural Network (RNN) , Long Short-Term Memory Networks (LSTM) , Gated Recurrent Unit (GRU) , and their stacked and bidirectional versions for the implementation of morphological synthesis at the character level. In addition to that, we investigated the performance of the combination of the aforementioned deep learning architectures and the Conditional Random Field (CRF) in the morphological synthesis of nouns and verbs in Malayalam and Tamil. We observed that the addition of CRF to the Bidirectional LSTM/GRU architecture achieved more than 99% accuracy in the morphological synthesis of Malayalam and Tamil nouns and verbs.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3194100994",
    "type": "article"
  },
  {
    "title": "Persian Fake News Detection: Neural Representation and Classification at Word and Text Levels",
    "doi": "https://doi.org/10.1145/3472620",
    "publication_date": "2021-11-01",
    "publication_year": 2021,
    "authors": "Mohammadreza Samadi; Maryam Mousavian; Saeedeh Momtazi",
    "corresponding_authors": "",
    "abstract": "Nowadays, broadcasting news on social media and websites has grown at a swifter pace, which has had negative impacts on both the general public and governments; hence, this has urged us to build a fake news detection system. Contextualized word embeddings have achieved great success in recent years due to their power to embed both syntactic and semantic features of textual contents. In this article, we aim to address the problem of the lack of fake news datasets in Persian by introducing a new dataset crawled from different news agencies, and propose two deep models based on the Bidirectional Encoder Representations from Transformers model (BERT), which is a deep contextualized pre-trained model for extracting valuable features. In our proposed models, we benefit from two different settings of BERT, namely pool-based representation, which provides a representation for the whole document, and sequence representation, which provides a representation for each token of the document. In the former one, we connect a Single Layer Perceptron (SLP) to the BERT to use the embedding directly for detecting fake news. The latter one uses Convolutional Neural Network (CNN) after the BERT’s embedding layer to extract extra features based on the collocation of words in a corpus. Furthermore, we present the TAJ dataset, which is a new Persian fake news dataset crawled from news agencies’ websites. We evaluate our proposed models on the newly provided TAJ dataset as well as the two different Persian rumor datasets as baselines. The results indicate the effectiveness of using deep contextualized embedding approaches for the fake news detection task. We also show that both BERT-SLP and BERT-CNN models achieve superior performance to the previous baselines and traditional machine learning models, with 15.58% and 17.1% improvement compared to the reported results by Zamani et al. [ 30 ], and 11.29% and 11.18% improvement compared to the reported results by Jahanbakhsh-Nagadeh et al. [ 9 ].",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3209702869",
    "type": "article"
  },
  {
    "title": "Converting Continuous-Space Language Models into <i>N</i> -gram Language Models with Efficient Bilingual Pruning for Statistical Machine Translation",
    "doi": "https://doi.org/10.1145/2843942",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Rui Wang; Masao Utiyama; Isao Goto; Eiichiro Sumita; Hai Zhao; Bao‐Liang Lu",
    "corresponding_authors": "",
    "abstract": "The Language Model (LM) is an essential component of Statistical Machine Translation (SMT). In this article, we focus on developing efficient methods for LM construction. Our main contribution is that we propose a Natural N -grams based Converting (NNGC) method for transforming a Continuous-Space Language Model (CSLM) to a Back-off N -gram Language Model (BNLM). Furthermore, a Bilingual LM Pruning (BLMP) approach is developed for enhancing LMs in SMT decoding and speeding up CSLM converting. The proposed pruning and converting methods can convert a large LM efficiently by working jointly. That is, a LM can be effectively pruned before it is converted from CSLM without sacrificing performance, and further improved if an additional corpus contains out-of-domain information. For different SMT tasks, our experimental results indicate that the proposed NNGC and BLMP methods outperform the existing counterpart approaches significantly in BLEU and computational cost.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2222501782",
    "type": "article"
  },
  {
    "title": "Inter-, Intra-, and Extra-Chunk Pre-Ordering for Statistical Japanese-to-English Machine Translation",
    "doi": "https://doi.org/10.1145/2818381",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Chenchen Ding; Keisuke Sakanushi; Hirona Touji; Mikio Yamamoto",
    "corresponding_authors": "",
    "abstract": "A rule-based pre-ordering approach is proposed for statistical Japanese-to-English machine translation using the dependency structure of source-side sentences. A Japanese sentence is pre-ordered to an English-like order at the morpheme level for a statistical machine translation system during the training and decoding phase to resolve the reordering problem. In this article, extra-chunk pre-ordering of morphemes is proposed, which allows Japanese functional morphemes to move across chunk boundaries. This contrasts with the intra-chunk reordering used in previous approaches, which restricts the reordering of morphemes within a chunk. Linguistically oriented discussions show that correct pre-ordering cannot be realized without extra-chunk movement of morphemes. The proposed approach is compared with five rule-based pre-ordering approaches designed for Japanese-to-English translation and with a language independent statistical pre-ordering approach on a standard patent dataset and on a news dataset obtained by crawling Internet news sites. Two state-of-the-art statistical machine translation systems, one phrase-based and the other hierarchical phrase-based, are used in experiments. Experimental results show that the proposed approach outperforms the compared approaches on automatic reordering measures (Kendall’s τ, Spearman’s ρ, fuzzy reordering score, and test set RIBES) and on the automatic translation precision measure of test set BLEU score.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2228644385",
    "type": "article"
  },
  {
    "title": "Linguistically Driven Multi-Task Pre-Training for Low-Resource Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3491065",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Zhuoyuan Mao; Chenhui Chu; Sadao Kurohashi",
    "corresponding_authors": "",
    "abstract": "In the present study, we propose novel sequence-to-sequence pre-training objectives for low-resource machine translation (NMT): Japanese-specific sequence to sequence (JASS) for language pairs involving Japanese as the source or target language, and English-specific sequence to sequence (ENSS) for language pairs involving English. JASS focuses on masking and reordering Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on phrase structure masking and reordering tasks. Experiments on ASPEC Japanese–English &amp; Japanese–Chinese, Wikipedia Japanese–Chinese, News English–Korean corpora demonstrate that JASS and ENSS outperform MASS and other existing language-agnostic pre-training methods by up to +2.9 BLEU points for the Japanese–English tasks, up to +7.0 BLEU points for the Japanese–Chinese tasks and up to +1.3 BLEU points for English–Korean tasks. Empirical analysis, which focuses on the relationship between individual parts in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and ENSS. Adequacy evaluation using LASER, human evaluation, and case studies reveals that our proposed methods significantly outperform pre-training methods without injected linguistic knowledge and they have a larger positive impact on the adequacy as compared to the fluency.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4205756254",
    "type": "article"
  },
  {
    "title": "Roman-Urdu-Parl: Roman-Urdu and Urdu Parallel Corpus for Urdu Language Understanding",
    "doi": "https://doi.org/10.1145/3464424",
    "publication_date": "2022-01-18",
    "publication_year": 2022,
    "authors": "Mehreen Alam; Sibt ul Hussain",
    "corresponding_authors": "",
    "abstract": "Availability of corpora is a basic requirement for conducting research in a particular language. Unfortunately, for a morphologically rich language like Urdu, despite being used by over a 100 million people around the globe, the dearth of corpora is a major reason for the lack of attention and advancement in research. To this end, we present the first-ever large-scale publicly available Roman-Urdu parallel corpus, Roman-Urdu-Parl, with 6.37 million sentence-pairs. It is a huge corpus collected from diverse sources, annotated using crowd-sourcing techniques, and also assured for quality. It has a total of 92.76 million Roman-Urdu words, 92.85 million Urdu words, Roman-Urdu vocabulary of 42.9 K words, and Urdu vocabulary of 43.8 K words. Roman-Urdu-Parl has been built to ensure that it not only captures the morphological and linguistic features of the language but also the heterogeneity and variations arising due to demographic conditions. We validate the authenticity and quality of our corpus by using it to address two natural language processing research problems, that is, on learning word embeddings and building a machine transliteration system. Our contribution of the corpus leads to exceptional results in both settings, for example, our machine transliteration system sets a new state-of-the-art with a Bilingual Evaluation Understudy (BLEU) score of 84.67. We believe that Roman-Urdu-Parl can serve as fuel for igniting and advancing works in many research areas related to the Urdu language.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4206706025",
    "type": "article"
  },
  {
    "title": "Impact of Feature Extraction and Feature Selection Algorithms on Punjabi Speech Emotion Recognition Using Convolutional Neural Network",
    "doi": "https://doi.org/10.1145/3511888",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Kamaldeep Kaur; Parminder Singh",
    "corresponding_authors": "",
    "abstract": "As a challenge to refine the spontaneity and productivity of a machine and human coherence, speech emotion recognition has been an overriding area of research. The trustability and fulfillment of emotion recognition are largely involved with the feature extraction and selection processes. An important role is played in exploring and distinguishing audio content during the feature extraction phase. Also, the features that have been extracted should be resilient to a number of disturbances and reliable enough for an adequate classification system. This article focuses on three main components of a Speech Emotion Recognition (SER) process. The first one is the optimal feature extraction method for a Punjabi SER system. The second one is the use of an appropriate feature selection method that selects effectual features from the ones extracted in the first step and removes the redundant features to improve the conduct of emotion recognition. The third one is the classification model that has been used further for emotion recognition. So the scope of this article is to explain the three main steps of the Punjabi SER system: feature extraction, feature selection, and emotion recognition with classifier. The results have been calculated and compared for number of feature set combinations, with and without a feature selection process. A total of 10 experiments are carried out, and various performance metrics such as precision, recall, F1-score, accuracy, and so on, are used to demonstrate the results.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4220810489",
    "type": "article"
  },
  {
    "title": "An Integrated Topic Modelling and Graph Neural Network for Improving Cross-lingual Text Classification",
    "doi": "https://doi.org/10.1145/3530260",
    "publication_date": "2022-04-14",
    "publication_year": 2022,
    "authors": "Tham Vo",
    "corresponding_authors": "Tham Vo",
    "abstract": "In recent years, along with the dramatic developments of deep learning in the natural language processing (NLP) domain, notable multilingual pre-trained language techniques have been proposed. These recent multilingual text analysis and mining models have demonstrated state-of-the-art performance in several primitive NLP tasks, including cross-lingual text classification (CLC). However, these recent multilingual pre-trained language models still suffer limitations regarding their adaptation for specific task-driven fine-tuning in the context of low-resource languages. Moreover, they also encounter problems related to the capability of preserving the global semantic (e.g., topic, etc.) and long-range relationships between words to better fine-tune and effectively handle the cross-lingual text classification task. To meet these challenges, in this article, we propose a novel topic-driven multi-typed text graph attention–based representation learning method for dealing with the cross-lingual text classification problem called TG-CTC. In the proposed TG-CTC model, we utilize a novel fused topic-driven multi-typed text graph representation to jointly learn the rich-schematic structural and global semantic information of texts to effectively handle the CLC task. More specifically, we integrate the heterogeneous text graph attention network with the neural topic modelling approach to enrich the semantic information of learned textual representations in the context of multiple languages. Extensive experiments in benchmark multilingual datasets showed the effectiveness of the proposed TG-CTC model compared with the contemporary state-of-the-art baselines.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4224135931",
    "type": "article"
  },
  {
    "title": "A Study on the Performance of Recurrent Neural Network based Models in Maithili Part of Speech Tagging",
    "doi": "https://doi.org/10.1145/3540260",
    "publication_date": "2022-06-01",
    "publication_year": 2022,
    "authors": "Ankur Priyadarshi; Sujan Kumar Saha",
    "corresponding_authors": "",
    "abstract": "This article presents our effort in developing a Maithili Part of Speech (POS) tagger. Substantial effort has been devoted to developing POS taggers in several Indian languages, including Hindi, Bengali, Tamil, Telugu, Kannada, Punjabi, and Marathi; but Maithili did not achieve much attention from the research community. Maithili is one of the official languages of India, with around 50 million native speakers. So, we worked on developing a POS tagger in Maithili. For the development, we use a manually annotated in-house Maithili corpus containing 56,126 tokens. The tagset contains 27 tags. We train a conditional random fields (CRF) classifier to prepare a baseline system that achieves an accuracy of 82.67%. Then, we employ several recurrent neural networks (RNN)-based models, including Long-short Term Memory (LSTM), Gated Recurrent Unit (GRU), LSTM with a CRF layer (LSTM-CRF), and GRU with a CRF layer (GRU-CRF) and perform a comparative study. We also study the effect of both word embedding and character embedding in the task. The highest accuracy of the system is 91.53%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4281627467",
    "type": "article"
  },
  {
    "title": "Stance Detection with a Multi-Target Adversarial Attention Network",
    "doi": "https://doi.org/10.1145/3544490",
    "publication_date": "2022-06-16",
    "publication_year": 2022,
    "authors": "Qingying Sun; Xuefeng Xi; Jiajun Sun; Zhongqing Wang; Huiyan Xu",
    "corresponding_authors": "",
    "abstract": "Stance detection aims to assign a stance label (in favor or against) to a post towards a specific target. In the literature, there are many studies focusing on this topic, and most of them treat stance detection as a supervised learning task. Therefore, a new classifier needs to be built from scratch on a well-prepared set of ground-truth data whenever predictions are needed for an unseen target. However, it is difficult to annotate the stance of a post, since a stance is a subjective attitude towards a target. Hence, it is necessary to learn the information from unlabeled data or other target data to help stance detection with a certain target. In this study, we propose a multi-target stance detection framework to integrate multi-target data together for stance detection. Since topic and sentiment are two important factors to identify the stance of a post in multi-target data, we propose an adversarial attention network to integrate multi-target data by detecting and connecting topic and sentiment information. In particular, the adversarial network is utilized to determine the topic and the sentiment of each post to collect some target-invariant information for stance detection. In addition, the attention mechanism is utilized to connect posts with a similar topic or sentiment to acquire some key information for stance detection. The experimental results not only demonstrate the effectiveness of the proposed model, but also indicate the importance of the topic and the sentiment information for stance detection using multi-target data.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4283018544",
    "type": "article"
  },
  {
    "title": "Phase-based Cepstral features for Automatic Speech Emotion Recognition of Low Resource Indian languages",
    "doi": "https://doi.org/10.1145/3563944",
    "publication_date": "2022-09-21",
    "publication_year": 2022,
    "authors": "Chinmay Chakraborty; Tusar Kanti Dash; Ganapati Panda; Sandeep Singh Solanki",
    "corresponding_authors": "",
    "abstract": "Automatic speech emotion recognition (SER) is a crucial task in communication-based systems, where feature extraction plays an important role. Recently, a lot of SER models have been developed and implemented successfully in English and other western languages. However, the performance of the traditional Indian languages in SER is not up to the mark. This problem of SER in low-resource Indian languages mainly the Bengali language is dealt with in this paper. In the first step, the relevant phase-based information from the speech signal is extracted in the form of phase-based cepstral features (PBCC) using cepstral, and statistical analysis. Several pre-processing techniques are combined with features extraction and gradient boosting machine-based classifier in the proposed SER model. Finally, the evaluation and comparison of simulation results on speaker-dependent, speaker-independent tests are performed using multiple language datasets, and independent test sets. It is observed that the proposed PBCC features-based model is performing well with an average of 96% emotion recognition efficiency as compared to standard methods.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4296538609",
    "type": "article"
  },
  {
    "title": "Development of an Efficient Method to Detect Mixed Social Media Data with Tamil-English Code Using Machine Learning Techniques",
    "doi": "https://doi.org/10.1145/3563775",
    "publication_date": "2022-09-22",
    "publication_year": 2022,
    "authors": "F. H. A. Shibly; Uzzal Sharma; H. M. M. Naleer",
    "corresponding_authors": "",
    "abstract": "On social networking sites, online hate speech has become more prevalent due to the quick expansion of mobile computing and Web technology. Previous research has found that being exposed to Internet hate speech has substantial offline implications for historically disadvantaged communities. Therefore, there is a lot of interest in research on automated hate-based comment and post detection. Hate speech can have an influence on any population group, but some are more vulnerable than others. From this background, detecting and reporting such hate related comments and posts can help to avoid the harmful effects of hate speech. There are some studies available on this context and it was found that machine learning algorithms are more efficient in detecting abusive texts in social media. In this research, we applied selected seven machine learning algorithms such as Support Vector Machine (SVM), Naïve Bayes (NB), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Gradient Boost (GB) and K Nearest Neighbor ( K NN) to detect hate speech and compare the performances of those algorithms to develop an ensemble model. Researchers collected and combined Tamil – English code-mixed hate speech tweets dataset which was created in HASOC. This dataset's tweets are divided into two groups: not offensive and offensive. This dataset includes 35,442 tweets. In this research, NB has obtained highest F1 scores in detecting offensive and not offensive tweets with highest weighted average. But SVM has obtained highest accuracy in detecting Tamil – English hate speech texts with 80% in 10-fold cross-validation. Based on the stand-alone performances, researchers developed two ensemble classifiers including max-voting and averaging ensemble. Averaging ensemble classification obtained 90.67% in accuracy. The research study's findings are significant because these results can be applied as a model for Tamil – English code-mixed hate speech to evaluate future research works using various algorithms for identifying hate contents more accurately and professionally.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4296614633",
    "type": "article"
  },
  {
    "title": "Tamil Offensive Language Detection: Supervised versus Unsupervised Learning Approaches",
    "doi": "https://doi.org/10.1145/3575860",
    "publication_date": "2022-12-16",
    "publication_year": 2022,
    "authors": "Vimala Balakrishnan; Vithyatheri Govindan; Kumanan N. Govaichelvan",
    "corresponding_authors": "",
    "abstract": "Studies on natural language processing are mainly conducted in English, with very few exploring languages that are under-resourced, including the Dravidian languages. We present a novel work in detecting offensive language using a corpus collected from YouTube containing comments in Tamil. The study specifically aims to compare two machine learning approaches—namely, supervised and unsupervised—to detect offensive patterns in textual communications. In the first setup, offensive language detection models were developed using traditional machine learning algorithms such as Random Forest, Logistic Regression, Support Vector Machine, and AdaBoost, and assessed based on human labeling. Conversely, we used K -means ( K = 2) to cluster the unlabeled data before training the same set of machine learning algorithms to detect offensive communications. Performance scores indicate unsupervised clustering to be more effective than human labeling with ensemble classifiers achieving an impressive accuracy of 99.70% and 99.87% respectively for balanced and imbalanced datasets, hence showing that the unsupervised approach can be used effectively to detect offensive language in low-resourced languages.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4311604304",
    "type": "article"
  },
  {
    "title": "Multi-task Label-wise Transformer for Chinese Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3576025",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Xuelei Wang; Xirong Xu; Degen Huang; Ting Zhang",
    "corresponding_authors": "",
    "abstract": "Benefiting from the improvement of positional encoding and the introduction of lexical knowledge, Transformer has achieved superior performance than the prevailing BiLSTM-based models in named entity recognition (NER) task. However, existing Transformer-based models for Chinese NER pay less attention to the information captured by the bottom layers of Transformer and the significance of representation subspace where each head of Transformer is projected. In this article, we propose M ulti- T ask L abel- W ise T ransformer (MTLWT). From a global perspective, we assign entity boundary prediction (EBP) and entity type prediction (ETP) tasks to the first two layers. In this way, we stimulate lower layers to participate more in constructing character representation. Besides, in each multi-head self-attention (MHSA) layer, we provide a specific focus for each individual head, making the head project into a significant subspace. Experiments on four datasets from different domains show that our proposed model achieves comparable performance with other state-of-the-art models. In particular, MTLWT outperforms the other frameworks without external knowledge on all the datasets.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4317209446",
    "type": "article"
  },
  {
    "title": "Semantic Tagging for the Urdu Language: Annotated Corpus and Multi-Target Classification Methods",
    "doi": "https://doi.org/10.1145/3582496",
    "publication_date": "2023-02-16",
    "publication_year": 2023,
    "authors": "Jawad Shafi; Rao Muhammad Adeel Nawab; Paul Rayson",
    "corresponding_authors": "",
    "abstract": "Extracting and analysing meaning-related information from natural language data has attracted the attention of researchers in various fields, such as natural language processing, corpus linguistics, information retrieval, and data science. An important aspect of such automatic information extraction and analysis is the annotation of language data using semantic tagging tools. Different semantic tagging tools have been designed to carry out various levels of semantic analysis, for instance, named entity recognition and disambiguation, sentiment analysis, word sense disambiguation, content analysis, and semantic role labelling. Common to all of these tasks, in the supervised setting, is the requirement for a manually semantically annotated corpus, which acts as a knowledge base from which to train and test potential word and phrase-level sense annotations. Many benchmark corpora have been developed for various semantic tagging tasks, but most are for English and other European languages. There is a dearth of semantically annotated corpora for the Urdu language, which is widely spoken and used around the world. To fill this gap, this study presents a large benchmark corpus and methods for the semantic tagging task for the Urdu language. The proposed corpus contains 8,000 tokens in the following domains or genres: news, social media, Wikipedia, and historical text (each domain having 2K tokens). The corpus has been manually annotated with 21 major semantic fields and 232 sub-fields with the USAS (UCREL Semantic Analysis System) semantic taxonomy which provides a comprehensive set of semantic fields for coarse-grained annotation. Each word in our proposed corpus has been annotated with at least one and up to nine semantic field tags to provide a detailed semantic analysis of the language data, which allowed us to treat the problem of semantic tagging as a supervised multi-target classification task. To demonstrate how our proposed corpus can be used for the development and evaluation of Urdu semantic tagging methods, we extracted local, topical and semantic features from the proposed corpus and applied seven different supervised multi-target classifiers to them. Results show an accuracy of 94% on our proposed corpus which is free and publicly available to download.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4321089622",
    "type": "article"
  },
  {
    "title": "A Novel Neural Machine Translation Approach for low-resource Sanskrit-Hindi Language pair",
    "doi": "https://doi.org/10.1145/3591207",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Nandini Sethi; Amita Dev; Poonam Bansal",
    "corresponding_authors": "",
    "abstract": "Sanskrit is one of the earliest native languages and is correctly described as \"the gods' language\" because of its wide use in Indian religious literature from the past. However, it is becoming less popular in modern India. Due in significant part to the need for more materials for translation both in and out of Sanskrit, it is no longer commonly utilized. This study explores the feasibility of using machine translation (MT) to provide a link between Sanskrit and, one of the earliest native languages, and its contemporary descendant Hindi. A study was conducted between existing modelling methodologies, notably Statistical machine translation (SMT), and the proposed novel deep learning-based Machine translation strategy using a manually created parallel corpus for the Sanskrit-Hindi language pair. While SMT creates interpretations by mapping phrases from the languages of the source and destination, statistical models, and bilingual text corpora for learning parameters, neural machine translation (NMT) frequently models entire phrases in a single integrated model, using a convolutional neural network to calculate the probability of a word sequence. The proposed NMT model is implemented using an encoder-decoder with an attention mechanism paradigm and the inclusion of gated recurrent units. Our approach involved development of a novel model for Sanskrit-Hindi machine translation using deep learning and the creation of parallel corpora for the Sanskrit-Hindi language pair. The proposed model is evaluated on automated and human-based metrics, and results show that our proposed deep learning-based model outperforms statistical modelling techniques on Moses, surpassing them both with a BLEU score of 53.8% compared to 34.56%. This article examines the undiscovered area of machine translation from Sanskrit to Hindi and discusses the main benefits and drawbacks of statistical and neural machine translation while providing a fresh viewpoint on the subject.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4362716491",
    "type": "article"
  },
  {
    "title": "Mexican Sign Language Corpus: Towards an Automatic Translator",
    "doi": "https://doi.org/10.1145/3591471",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Felipe Trujillo-Romero; Gibran García-Bautista",
    "corresponding_authors": "",
    "abstract": "The development of the Sign Language Corpus has been motivated by its great utility and application to various purposes and research areas. However, some countries do not have their own Sign Language Corpus. Developing a corpus thereby benefits the community of people with speech disabilities in diverse areas such as education. Thus, the motivation to develop this work is to present an advance toward constructing an RGB-D corpus of Mexican Sign Language captured by a Kinect sensor. A total of 90,000 samples of 570 words and 30 phrases interpreted by 150 people who commonly use Mexican Sign Language were collected. Of the participants, 86 were women and 64 were men, aged between 12 and 60 years old. The Mexican Sign Language Corpus was recorded by signers from three different regions of the south of Mexico. The constructed corpus contains depth, color, point clouds, and human skeleton positions. Six hundred of the most used words were selected from 17 semantic fields, considering the variability in the movement of both hands. After training a neural network, the performance developed by the recognition system was 98.62%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4362721016",
    "type": "article"
  },
  {
    "title": "Ensemble Classifier for Hindi Hostile Content Detection",
    "doi": "https://doi.org/10.1145/3591353",
    "publication_date": "2023-04-10",
    "publication_year": 2023,
    "authors": "Angana Chakraborty; Subhankar Joardar; Arif Ahmed Sekh",
    "corresponding_authors": "",
    "abstract": "Detection of hostile content from social media posts (Facebook, Twitter, etc.) is a demanding task in the field of Natural Language Processing. The increase of hostile content in different electronic media has opened up new challenges in language understanding. It becomes more difficult in regional languages. AI-based solutions are required to identify hostile content on a large scale. Although a satisfactory amount of research has been carried out in the English language, finding hostile content in regional languages is still under development due to the unavailability of suitable datasets and tools. In terms of the number of speakers, Hindi ranks third in the world and first on the Indian subcontinent. The objective of this article is to design a hostile content detection system in Hindi using coarse-grained (binary) classification and fine-grained (multi-class, multi-label) classification. We note that different baseline learning methods with different pre-trained language models perform differently. Using the Constraint 2021 Hindi Dataset, this research proposes a Bidirectional Encoder Representations from Transformers–(BERT) based contextual embedding technique with a concatenation of emoji2vec embeddings to classify social media posts in Hindi Devanagari script as hostile or non-hostile. Additionally, for the fine-grained tasks where hostile posts are sub-categorized as defamation, fake, hate, and offensive, we develop an ensemble classifier varying different learning methods and embedding models. With an F1-Score of 0.9721, it is found that our proposed Indic-BERT+emoji model outperforms the baseline model and other existing models for the coarse-grained task. We have also observed that our proposed ensemble method provides better results than the existing models and the baseline model for the fine-grained tasks with F1-Scores of 0.43, 0.82, 0.58, and 0.62 for the defamation, fake, hate, and offensive classes, respectively. The code and the data are available at https://github.com/skarifahmed/hostile .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4363678333",
    "type": "article"
  },
  {
    "title": "Collaborative Fuzzy Linguistic Learning to Low-Resource and Robust Decision System Based on Bounded Rationality",
    "doi": "https://doi.org/10.1145/3592605",
    "publication_date": "2023-04-12",
    "publication_year": 2023,
    "authors": "Chao Zhang; Xiaochuan Li; Arun Kumar Sangaiah; Wentao Li; Baoli Wang; Feng Cao; Xuekui Shangguan",
    "corresponding_authors": "",
    "abstract": "Low-resource languages are challenging to process intelligent decision systems due to limited data and resources. As an effective way of processing low-resource languages in intelligent decision systems, fuzzy linguistic approaches excel in transforming original uncertain linguistic information into highly structured data and learning valid decision rules between complex data structures. However, existing fuzzy linguistic methods may not fully capture realistic features of multi-attribute group decision-making (MAGDM), such as incomplete and hesitant linguistic expressions, stable information fusion, and bounded rationality of decision-makers (DMs). Therefore, it is necessary to develop a collaborative fuzzy language learning system based on bounded rationality, low-resource and robust decision-making. Specifically, we present a new multi-granularity (MG) group decision-making (GDM) scheme by using MULTIMOORA (Multi-Objective Optimization by Ratio Analysis plus the full MULTIplicative form) and PT (Prospect Theory) for incomplete hesitant fuzzy linguistic information systems (I-HFL-ISs), where MG GDM aims to discover knowledge from complex MAGDM problems with MG features. To achieve the above goal, we first introduce the concept of MG-I-HFL-ISs to represent incomplete, hesitant and imprecise linguistic evaluation information offered by multiple decision-makers (DMs). Then, we apply a valid transformation scheme to convert MG-I-HFL-ISs into MG-HFL-ISs, and use the MG probability rough set (PRS) to develop a series of MG-HFL-PRSs with the support of MULTIMOORA. Afterwards, an HFL MG GDM method is designed by integrating MULTIMOORA and PT for solving MAGDM problems with MG-I-HFL-ISs. The proposed method can effectively synthesize low-resource languages and mine useful decision-making knowledge. At last, a drug selection case and a simulated case are performed for showing the rationality of the designed HFL MG GDM scheme.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4365144936",
    "type": "article"
  },
  {
    "title": "Dataset Enhancement and Multilingual Transfer for Named Entity Recognition in the Indonesian Language",
    "doi": "https://doi.org/10.1145/3592854",
    "publication_date": "2023-04-18",
    "publication_year": 2023,
    "authors": "Siti Oryza Khairunnisa; Zhousi Chen; Mamoru Komachi",
    "corresponding_authors": "",
    "abstract": "Named entity recognition in the Indonesian language has significantly developed in recent years. However, it still lacks standardized publicly available corpora; a small dataset is available but suffers from inconsistent annotations. Therefore, we re-annotated the dataset to improve its consistency and benefit the community. Our re-annotation led to better training results from an effective baseline model consisting of bidirectional long short-term memory and conditional random fields. To fully utilize the limited available data, we utilized better contextualization and transferred external knowledge by exploiting monolingual and multilingual pre-trained language models, such as IndoBERT and XLM-RoBERTa. In addition to the general improvement from the language models, we observed that the monolingual model is more sensitive, while the multilingual ones show advantages in rich morphological knowledge. We also applied cross-lingual transfer learning to utilize high-resource corpora in other languages. We adopted English, Spanish, Dutch, and German as the source languages for the target Indonesian language and found that Dutch plays a special role in the data transfer method due to morphological similarity attributable to historical reasons.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4366274332",
    "type": "article"
  },
  {
    "title": "Bagging: An Ensemble Approach for Recognition of Handwritten Place Names in Gurumukhi Script",
    "doi": "https://doi.org/10.1145/3593024",
    "publication_date": "2023-04-20",
    "publication_year": 2023,
    "authors": "Harmandeep Kaur; Munish Kumar; Aastha Gupta; Monika Sachdeva; Ajay Mittal; Krishan Kumar",
    "corresponding_authors": "",
    "abstract": "In this article, the authors present an effort to recognize handwritten Gurumukhi place names for use in postal automation. Five feature extraction techniques (zoning, horizontal peak extent, vertical peak extent, diagonal, and centroid) have been analyzed and optimized using Principal Component Analysis (PCA). Four classification methods ( k -Nearest Neighbor ( k -NN), decision tree, random forest, and Convolutional Neural Network (CNN)) have been utilized to classify the handwritten word images. To enhance the recognition results, the authors have employed Bootstrap Aggregation (Bagging) with a majority voting scheme. The authors used a public benchmark dataset of 40,000 handwritten place-name samples in the Punjabi language for their experimental work. The experiments were conducted using a 70:30 partitioning approach, where 70% of the data was utilized for training and the remaining 30% for testing. The system achieved a maximum recognition accuracy of 96.98% by utilizing a combination of zoning, vertical peak extent, and diagonal features, and a minimum Mean Squared Error (MSE) of 0.86% based on a combination of zoning and horizontal peak extent features with a majority voting scheme through ensemble (Bagging) methodology.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4366598879",
    "type": "article"
  },
  {
    "title": "TPoet: Topic-Enhanced Chinese Poetry Generation",
    "doi": "https://doi.org/10.1145/3593805",
    "publication_date": "2023-04-21",
    "publication_year": 2023,
    "authors": "Liang Yang; Zhexu Shen; Fengqing Zhou; Hongfei Lin; Junpeng Li",
    "corresponding_authors": "",
    "abstract": "Chinese poetry generation has been a challenging part of natural language processing due to the unique literariness and aesthetics of poetry. In most cases, the content of poetry is topic related. In other words, specific thoughts or emotions are usually expressed regarding given topics. However, topic information is rarely taken into consideration in current studies about poetry generation models. In this article, we propose a topic-enhanced Chinese poetry generation model called TPoet in which the topic model is integrated into the Transformer-based auto-regressive text generation model. By feeding topic information to the input layer and heterogeneous attention mechanism, TPoet can implicitly learn the latent information of topic distribution. In addition, by setting multiple identifiers such as segment, rhyme, and tone, the model can explicitly learn the constraints of generated poems. Extensive experimental results show that the quality of TPoet-generated poems outperforms the current advanced models or systems, and the topic consistency and diversity in generated poems have been significantly improved as well.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4366684380",
    "type": "article"
  },
  {
    "title": "Detecting Dravidian Offensive Posts in MIoT: A Hybrid Deep Learning Framework",
    "doi": "https://doi.org/10.1145/3592602",
    "publication_date": "2023-04-24",
    "publication_year": 2023,
    "authors": "Abhinav Kumar; Sunil Saumya; Ashish Singh",
    "corresponding_authors": "",
    "abstract": "Hate speech and Offensive Posts (OP) detection on Smart Multimedia Internet of Things (MIoT) have been an active issue for researchers. MIoT media texts in non-native English-speaking countries are often code-mixed or script mixed/switched. This paper proposes an ensemble-based Deep Learning (DL) framework comprised of a Convolutional Neural Network (CNN) and a Dense Neural Network (DNN) for identifying hate and OP in Malayalam Code-Mixed (MCM), Tamil Code-Mixed (TCM), and Malayalam Script-Mixed (MSM) MIoT media postings. Word-level and character-level features are utilized in the convolutional neural network. In contrast, the dense neural network uses character-level Term Frequency-Inverse Document Frequency (TF-IDF) features. The inclusion of character-level features in the proposed ensemble framework resulted in state-of-the-art performance for TCM and MCM datasets, with weighted F 1 -score of 0.91 and 0.78, respectively, and comparable performance for MSM posts, with a weighted F 1 -score of 0.95.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4366830328",
    "type": "article"
  },
  {
    "title": "Using Data Augmentation and Bidirectional Encoder Representations from Transformers for Improving Punjabi Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3595861",
    "publication_date": "2023-05-04",
    "publication_year": 2023,
    "authors": "Hamza Khalid; Ghulam Murtaza; Qaiser Abbas",
    "corresponding_authors": "",
    "abstract": "Named entity recognition (NER) is a task of proper noun identification from natural language text and classification into various types such as location, person, and organization. Due to NER's applications in different natural language processing (NLP) tasks, numerous NER approaches and benchmark datasets have been proposed. However, developing NER techniques for low-resource languages is still limited due to the absence of substantial training datasets. Punjabi is a classic example of low resource language. Although various researchers have worked on Punjabi, they focused on the Gurmukhi script. To overcome the challenges in developing NER for the Shahmukhi script, we present an improved technique for Punjabi NER for the Shahmukhi script in this paper. We firstly extend the existing dataset by adding new NER classes by leveraging a novel Pool of Words data augmentation strategy. Our extended dataset has 11,31,509 tokens and 1,25,789 labeled entities with more named entities (NEs) than the older dataset. In the next step, we fine-tuned a transformer model known as Bidirectional Encoder Representations from Transformers (BERT) for the NER task. We performed experiments using the proposed approach on a new and older dataset version, showing that our method achieved competitive results.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4368371908",
    "type": "article"
  },
  {
    "title": "Low-resource Multilingual Neural Translation Using Linguistic Feature-based Relevance Mechanisms",
    "doi": "https://doi.org/10.1145/3594631",
    "publication_date": "2023-05-18",
    "publication_year": 2023,
    "authors": "Abhisek Chakrabarty; Raj Dabre; Chenchen Ding; Masao Utiyama; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "This article investigates approaches to effectively harness source-side linguistic features for low-resource multilingual neural machine translation (MNMT). Previous works focus on using various features of a word such as lemma, part-of-speech tag, dependency label, and so on, to improve translation quality in a low-resource scenario. However, these studies deal with bilingual translation and do not focus on using features in multilingual training setups. Our work focuses on this particular point and experiments with low-resource multilingual models incorporating source-side linguistic features. Although techniques for integrating features into an NMT model such as concatenation and feature relevance perform quite well in bilingual settings, they do not work well in multilingual settings. To remedy this, we propose the use of dummy features and language indicator features in MNMT models. Experiments are conducted on English to Asian language translation on a multilingual, multi-parallel corpus spanning English and eight Asian languages where for each language pair, the training data size does not exceed 20,000 parallel sentences. After establishing strong bilingual baselines using feature relevance mechanisms and multilingual baselines without any features, we show that our proposed dummy features and language indicator features, in combination with feature relevance mechanisms, yield significant improvements in BLEU points for all language pairs. We then analyze our models from the perspectives of model sizes, the impact of individual linguistic features, validation perplexity computed during training, visualization of the activations of the relevance mechanisms, and exhaustive tuning of hyperparameters. We also report preliminary results for multilingual multi-way models using linguistic features.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4377021651",
    "type": "article"
  },
  {
    "title": "Transfer Learning-based Forensic Analysis and Classification of E-Mail Content",
    "doi": "https://doi.org/10.1145/3604592",
    "publication_date": "2023-06-28",
    "publication_year": 2023,
    "authors": "Farkhund Iqbal; Abdul Rehman Javed; Rutvij H. Jhaveri; Ahmad Almadhor; Umar Farooq",
    "corresponding_authors": "",
    "abstract": "research-article Free Access Share on Transfer Learning-based Forensic Analysis and Classification of E-Mail ContentJust Accepted Authors: Farkhund Iqbal College of Technological Innovation Zayed University, UAE College of Technological Innovation Zayed University, UAEView Profile , Abdul Rehman Javed Department of Electrical and Computer Engineering Lebanese American University, Lebanon Department of Electrical and Computer Engineering Lebanese American University, LebanonView Profile , Rutvij H. Jhaveri Department of Computer Science and Engineering, School of Technology Pandit Deendayal Energy University, India Department of Computer Science and Engineering, School of Technology Pandit Deendayal Energy University, IndiaView Profile , Ahmad Almadhor Department of Computer Engineering and Networks, College of Computer and Information Sciences Jouf University, Saudi Arabia Department of Computer Engineering and Networks, College of Computer and Information Sciences Jouf University, Saudi ArabiaView Profile , Umar Farooq Department of Computer Science National University of Computer and Emerging Sciences, Pakistan Department of Computer Science National University of Computer and Emerging Sciences, PakistanView Profile Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingAccepted on June 2023https://doi.org/10.1145/3604592Published:28 June 2023Publication History 0citation78DownloadsMetricsTotal Citations0Total Downloads78Last 12 Months78Last 6 weeks78 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4382397748",
    "type": "article"
  },
  {
    "title": "Pelican Optimization Algorithm with Deep Learning for Aspect based Sentiment Analysis on Asian Low Resource Languages",
    "doi": "https://doi.org/10.1145/3608949",
    "publication_date": "2023-07-12",
    "publication_year": 2023,
    "authors": "Hussein Ali Rasool; Firas Abedi; Ayad Ghany Ismaeel; Ali Hashim Abbas; Raed Khalid; Ahmed Alkhayyat; Mustafa Musa Jaber; Ankit Garg",
    "corresponding_authors": "",
    "abstract": "Recently, sentiment analysis (SA) has become more popular as it is crucial to moderate and examine the data from the internet. It contains several applications, such as social media monitoring, market research, and opinion mining. Aspect Based Sentiment Analysis (ABSA) is a domain of SA that manages sentiment at a better level. ABSA classifies sentiment in terms of all the aspects for obtaining superior insights as sentiment expressed. A major contribution has been developed in ABSA, and then this progress can be restricted only to some languages with suitable resources. One common method is to utilize machine learning (ML) approaches, namely Neural Networks (NN), Support Vector Machines (SVM), and Naive Bayes (NB), together with Asian and low language-specific resources. These resources offer data on the sentiment polarity (neutral, positive, or negative) of phrases and words that are generally utilized in low-resource languages. In this aspect, this study develops a new Pelican Optimization Algorithm with Deep Learning for ABSA (POADL-ABSA) on Asian and Low Resource Languages. The proposed POADL-ABSA technique focuses on the detection and classification of sentiments. To accomplish this, the POADL-ABSA technique encompasses various levels of operations such as pre-processed, feature vector conversion, and classification. In addition, the POADL-ABSA technique employs the BERT model for feature vector extraction. Besides, attention-based bi-directional long short-term memory (ABiLSTM) system was used for the recognition and classification of sentiments. Finally, the POA was utilized for optimum hyperparameter selection of the ABiLSTM model, and it helps in attaining enhanced sentiment classification results. To ensure the improvised performance of the IAOADL-ABSA technique, an extensive experimental outcome the IAOADL-ABSA technique surpassed other models with acc{u}_y , pre{c}_n , rec{a}_l , and {F}_{score} of 98.72%, 98.71%, 98.72%, and 98.71%, respectively. Therefore, the IAOADL-ABSA technique can be employed for accurate classification results.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4384069800",
    "type": "article"
  },
  {
    "title": "A Pragmatic Analysis of Machine Translation Techniques for Preserving the Authenticity of the Sanskrit Language",
    "doi": "https://doi.org/10.1145/3610582",
    "publication_date": "2023-07-25",
    "publication_year": 2023,
    "authors": "Nandini Sethi; Amita Dev; Poonam Bansal; Deepak Kumar Sharma; Deepak Gupta",
    "corresponding_authors": "",
    "abstract": "Machine Translation has been a field of study for over six decades, but it has acquired substantial prominence in the last decade as processing capacity in personal computers has increased. The purpose of this paper is to discuss the usage of Sanskrit as a source, target, or supporting language in various Machine Translation systems. To investigate Machine Translation, researchers use a variety of strategies, including corpus-based, direct, and rule-based approaches. The primary goal of employing Sanskrit in Machine Translation is to evaluate its appropriateness, lexicon, and performance when proper Machine Translation methods are used. The research examines various modelling strategies for developing a machine translation system, specifically Statistical and Neural Machine Translation, in order to bridge the gap between Sanskrit and its current successor, Hindi. Interpretations are formed in Statistical Machine Translation by matching words from the source and target languages with statistical models and bilingual text corpora to learn parameters. Neural Machine Translation, on the other hand, uses an artificial neural network to predict the likelihood of a word sequence, frequently modelling entire phrases within a single integrated model. Neural Machine Translation is implemented using an encoder-decoder architecture with an attention mechanism. One of the most significant contributions of this paper is the use of different data sources, data collecting, and scraping to create a complete dataset. According to the study's findings, Neural Machine Translation outperforms the Statistical Machine Translation modelling technique. Furthermore, the paper examines the distinctive qualities of the Sanskrit language as well as the difficulties encountered by researchers in digesting Sanskrit while constructing the machine translation system. This study investigates the use of Sanskrit in Machine Translation and analyses several modelling methods, such as Statistical and Neural Machine Translation. The paper emphasizes the advantages of Neural Machine Translation and discusses the unique characteristics and challenges of the Sanskrit language in machine translation development.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4385233554",
    "type": "article"
  },
  {
    "title": "Impacts of Social Media Advertising on Purchase Intention and Customer Loyalty in E-Commerce Systems",
    "doi": "https://doi.org/10.1145/3613448",
    "publication_date": "2023-08-07",
    "publication_year": 2023,
    "authors": "Xingyu Duan; Chun-Nan Chen; Mohammad Shokouhifar",
    "corresponding_authors": "",
    "abstract": "The emergence of new technologies has had a noteworthy impact on communication systems, leading to the importance of conducting research in this area due to the significant influence of social media. Marketing operators must implement the necessary infrastructure to identify and fulfill customers' expectations, considering the advantages and the increasing number of users in social networks. This paper presents a comprehensive framework for evaluating the purchase intention of electronic commerce systems, taking into account the impact of social media advertising and customer loyalty. The paper aims to enhance purchase intention in e-commerce through social media advertising and examine influential factors that improve online shopping performance, including social media advertising's effectiveness on customer purchase behavior and brand loyalty. The paper develops a theoretical framework of nine hypotheses to evaluate purchase intention in e-commerce systems, with a focus on the effect of social media advertising on brand loyalty and purchase intention. To validate the proposed model and test the research hypotheses, we make use of information obtained from an international company located in China. The results demonstrate the positive effect of pleasurable motivation on purchase intention, with a significance level of 3.776 and a path coefficient of 0.279. Moreover, an investigation of the positive effect of customer loyalty on the recommended advertisement confirms the hypotheses, with a significance of 32.815 and a path coefficient of 0.788.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4385616903",
    "type": "article"
  },
  {
    "title": "Improving Access to Medical Information for Multilingual Patients using Pipelined Ensemble Average based Machine Translation",
    "doi": "https://doi.org/10.1145/3617372",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Debajyoty Banik; Rahul Paul; Rajkumar Singh Rathore; Rutvij H. Jhaveri",
    "corresponding_authors": "",
    "abstract": "Machine translation has shown potential in improving access to medical information and healthcare services for multilingual patients. This research aims to enhance machine translation accuracy in the medical field, specifically for translating from Hindi to English. The study introduces a new approach that dynamically allocates decoding parameters using regression models, overcoming the limitations of fixed parameters in the decoder. A comprehensive dataset is created to address limited data availability, enabling regression models to predict optimal pruning parameters. The main motivation for the study is the introduction of a regression method for optimizing pruning parameters, which is a novel approach in this context. The proposed approach outperforms existing methods, achieving improved translation accuracy. Standard metrics such as the BLEU score are used to evaluate translations. Ensemble average and pipeline approaches further enhance performance. The improved performance of the proposed models can be attributed to the ensemble of diverse models (Extra Trees, LightGBM, XGBoost, and Random Forest) that employ various techniques to reduce overfitting, enhance prediction accuracy, and improve translation by correcting prediction errors. The study contributes to facilitating the translation and sharing of medical literature, promoting collaboration and knowledge exchange across languages. The research demonstrates the effectiveness of the regression method for optimizing pruning parameters in machine translation, leading to improved translation accuracy in the medical field. The proposed models offer promising results, paving the way for enhanced machine translation systems and promoting collaboration and knowledge exchange in the medical domain. The source code is available at https://huggingface.co/debajyoty/statistical-regression-Based-MT/tree/main/Statistical-Regression-SMT.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386191593",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis of Code-Mixed Telugu-English Data Leveraging Syllable and Word Embeddings",
    "doi": "https://doi.org/10.1145/3620670",
    "publication_date": "2023-09-04",
    "publication_year": 2023,
    "authors": "Upendar Rao Rayala; Karthick Seshadri; Nagesh Bhattu Sristy",
    "corresponding_authors": "",
    "abstract": "Learning the inherent meaning of a word in Natural Language Processing (NLP) has motivated researchers to represent a word at various levels of abstraction, namely character-level, morpheme-level, and subword-level vector representations. Syllable-Aware Word Embedding (SAWE) can effectively handle agglutinative and fusion-based NLP tasks. However, research attempts on assessing the SAWE on such extrinsic NLP tasks has been scanty, especially for low-resource languages in the context of code-mixing with English. A model to learn SAWE to extract semantics at fine-grained subunits of a word is proposed in this article, and the representative ability of the embeddings is assessed through sentiment analysis of code-mixed Telugu-English review corpora. Multilingual societies and advancements in communication technologies have accounted for the prolific usage of mixed data, which renders the State-of-the-Art (SOTA) sentiment analysis models developed based on monolingual data ineffective. Social media users in the Indian subcontinent exhibit a tendency to mix English and their respective native language (using the phonetic form of English) in expressing their opinions or sentiments. A code-mixing scenario provides flexibility to borrow words from a foreign language, usage of shorthand notations, elongation of vowels, and usage of words without following syntactic/grammatical rules, which renders the sentiment analysis of code-mixed data challenging to perform. Deep neural architectures like Long Short-Term Memory and Gated Recurrent Unit networks have been shown to be effective in solving several NLP tasks, such as sequence labeling, named entity recognition, and machine translation. In this article, a framework to perform sentiment analysis on a code-mixed Telugu-English review corpus is implemented. Both word embedding and SAWE are input to a unified deep neural network that contains a two-level Bidirectional Long Short-Term Memory/Gated Recurrent Unit network with Softmax as the output layer. The proposed model leverages the advantages of both word embedding and SAWE, which enable the proposed model to outperform existing SOTA code-mixed sentiment analysis models on a Telugu-English code-mixed dataset of the International Institute of Information Technology–Hyderabad and a dataset curated by the authors. The improvement realized by the proposed model on these datasets is [3% increase in F1-score and 2% increase in accuracy] and [7% increase in F1-score and 5% in accuracy], respectively, in comparison with the best-performing SOTA model.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386422296",
    "type": "article"
  },
  {
    "title": "Fact-checking Vietnamese Information Using Knowledge Graph, Datalog, and KG-BERT",
    "doi": "https://doi.org/10.1145/3624557",
    "publication_date": "2023-09-16",
    "publication_year": 2023,
    "authors": "Huong To Duong; Van Ho; Phuc Do",
    "corresponding_authors": "",
    "abstract": "In the era of digital information, ensuring the accuracy and reliability of information is crucial, making fact-checking a vital process. Currently, English fact-checking has thrived due to various language processing tools and ample datasets. However, the same cannot be said for Vietnamese fact-checking, which faces significant challenges due to the lack of such resources. To address these challenges, we propose a model for checking Vietnamese facts by synthesizing three popular technologies: Knowledge Graph (KG), Datalog, and KG-BERT. The KG serves as the foundation for the fact-checking process, containing a dataset of Vietnamese information. Datalog, a logical programming language, is used with inference rules to complete the knowledge within the Vietnamese KG. KG-BERT, a Deep Learning (DL) model, is then trained on this KG to rapidly and accurately classify information that needs fact-checking. Furthermore, to put Vietnamese complex sentences into the fact-checking model, we present a solution for extracting triples from these sentences. This approach also contributes significantly to the ease of constructing foundational datasets for the Vietnamese KG. To evaluate the model's performance, we create a Vietnamese dataset comprising 130,190 samples to populate the KG. Using Datalog, we enrich this graph with additional knowledge. The KG is then utilized to train the KG-BERT model, achieving an impressive accuracy of 95%. Our proposed solution shows great promise for fact-checking Vietnamese information and has the potential to contribute to the development of fact-checking tools and techniques for other languages. Overall, this research makes a significant contribution to the field of data science by providing an accurate solution for fact-checking information in Vietnamese language contexts.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386803060",
    "type": "article"
  },
  {
    "title": "D{S}^2L{C}^3Net : A Decision Support System for Lung Colon Cancer Classification using Fusion of Deep Neural Networks and Normal Distribution based Gray Wolf Optimization",
    "doi": "https://doi.org/10.1145/3625096",
    "publication_date": "2023-09-21",
    "publication_year": 2023,
    "authors": "Muhammad Arslan Ijaz; Imran Ashraf; Usman Zahid; Awais Yasin; Siddique Ali; Muhammad Attique Khan; Salman A. AlQahtani; Yudong Zhang",
    "corresponding_authors": "",
    "abstract": "Lung and Colorectal (LC) cancer is life-threatening and rapidly developing cancers. According to World Health Organization (WHO), approximately 4.14 million lung and colorectal cancer cases were newly diagnosed, with 2.7 million fatalities. An International Agency for Research on Cancer (IARC) reported that there will be more than 3 million additional instances of colorectal cancer worldwide between 2020 and 2040. Early diagnosis of LC cancer is very helpful for treatment and can save the precious human life. The conventional diagnosis methods are expensive and time consuming. In this work, we present an accurate and efficient model for the classification of Lung and Colorectal (LC) cancer. We utilize two well-known pre-trained deep learning models, ResNet50 and EfficientNetB0, and fine-tuned the both models based on the addition and removal of layers. After the fine-tuning, manual hit and trail based hyperparameters are initialized. Later on, the deep transfer learning was performed and obtained the trained models. Two different feature vectors have been extracted from both models and fused using a priority based serial approach. To further improve the performance of extracted features, Normal Distribution based Gray Wolf Optimization algorithm is employed and obtained the best features that given as input to five classifiers. The output of these five classifiers is then utilized by soft voting technique to generate the final prediction. Experimental results show that the proposed architecture achieved an overall 98.73% accuracy on LC25000 dataset. Furthermore, prediction time was reduced by 19.14%. Comparison with the state-of-the-art techniques shows that the proposed technique obtained the improved performance results.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386928866",
    "type": "article"
  },
  {
    "title": "Speaker-Aware Interactive Graph Attention Network for Emotion Recognition in Conversation",
    "doi": "https://doi.org/10.1145/3627806",
    "publication_date": "2023-11-07",
    "publication_year": 2023,
    "authors": "Zhaohong Jia; Yunwei Shi; Weifeng Liu; Zhenhua Huang; Xiao Sun",
    "corresponding_authors": "",
    "abstract": "Recently, E motion R ecognition in C onversation (ERC) has attracted much attention and has become a hot topic in the field of natural language processing. Conversation is conducted in chronological order; current utterance is more likely influenced by nearby utterances. At the same time, speaker dependency also plays a core role in the conversation dynamic. The combined effect of the sequence-aware information and the speaker-aware information makes the emotion’s dynamic change. However, past works used simple information fusion methods to model the two kinds of information but ignored their interactive influence. Thus, we propose a novel method entitled SIGAT ( S peaker-aware I nteractive G raph A ttention Ne t work) to solve the problem. The core module is a mutual interactive module in which a dual-connection (self-connection and interact-connection) graph attention network is constructed. The advantage of SIGAT is modeling the speaker-aware and sequence-aware information in a unified graph and updating them simultaneously. In this way, we model the interactive influence of them and obtain the final representations, which have richer contextual clues. Experimental results on the four public datasets demonstrate that SIGAT outperforms the state-of-the-art models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388463935",
    "type": "article"
  },
  {
    "title": "Ontology-Based Natural Language Processing for Sentimental Knowledge Analysis Using Deep Learning Architectures",
    "doi": "https://doi.org/10.1145/3624012",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Deepak Kumar Jain; Shamimul Qamar; Saurabh Raj Sangwan; Weiping Ding; Anand J. Kulkarni",
    "corresponding_authors": "",
    "abstract": "When tested with popular datasets, sentiment categorization using deep learning (DL) algorithms will produce positive results. Building a corpus on novel themes to train machine learning methods in sentiment classification with high assurance, however, will be difficult. This study proposes a way for representing efficient features of a dataset into a word embedding layer of DL methods in sentiment classification known as KPRO (knowledge processing and representation based on ontology), a procedure to embed knowledge in the ontology of opinion datasets. This research proposes novel methods in ontology-based natural language processing utilizing feature extraction as well as classification by a DL technique. Here, input text has been taken as web ontology based text and is processed for word embedding. Then the feature mapping is carried out for this processed text using least square mapping in which the sentiment-based text has been mapped for feature extraction. The feature extraction is carried out using a Markov model based auto-feature encoder (MarMod_AuFeaEnCod). Extracted features are classified by utilizing hierarchical convolutional attention networks. Based on this classified output, the sentiment of the text obtained from web data has been analyzed. Results are carried out for Twitter and Facebook ontology-based sentimental analysis datasets in terms of accuracy, precision, recall, F-1 score, RMSE, and loss curve analysis. For the Twitter dataset, the proposed MarMod_AuFeaEnCod_HCAN attains an accuracy of 98%, precision of 95%, recall of 93%, F-1 score of 91%, RMSE of 88%, and loss curve of 70.2%. For Facebook, ontology web dataset analysis is also carried out with the same parameters in which the proposed MarMod_AuFeaEnCod_HCAN acquires accuracy of 96%, precision of 92%, recall of 94%, F-1 score of 91%, RMSE of 77%, and loss curve of 68.2%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388658829",
    "type": "article"
  },
  {
    "title": "Explainable Deep Learning for Mental Health Detection from English and Arabic Social Media Posts",
    "doi": "https://doi.org/10.1145/3632949",
    "publication_date": "2023-11-21",
    "publication_year": 2023,
    "authors": "Abhinav Kumar; Jyoti Kumari; Jiesth Pradhan",
    "corresponding_authors": "",
    "abstract": "The research communities have recently begun exploring the detection of depression through social media, making it a relatively new development in this field. Some research has been done to identify depressive signs from English social media postings, while little work has been done for the Arabic posts. This paper proposes a BERT and Bi-LSTM pipeline for the identification of depressive signs from Arabic social media posts. A fine-tuned RoBERTa-based model is also proposed for English social media posts for depressive state identification. Along with the proposed model, seven conventional machine learning and eight deep learning models are also explored for the identification of depressive signs from Arabic and English social media posts. The performance of the proposed model is validated on two Arabic datasets and one English dataset. The proposed BERT and Bi-LSTM pipeline achieved state-of-the-art performance with an F 1 -score of 1.00 and 0.82 for two different Arabic datasets, whereas the proposed fine-tuned RoBERTa achieved a F 1 -score of 0.60 which is comparable in identifying depressive sign from English social media posts. The majority of the suggested deep learning models are end-to-end, which necessitates a greater explanation for their success. An explainable AI-based model may enhance decision-making, transparency, and interpretability. Therefore, this research identifies where the suggested system learned well and where it failed in recognition of the depression signs that can help in future developments in the field of depression detection.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388845473",
    "type": "article"
  },
  {
    "title": "A Comparative Study of Different Dimensionality Reduction Techniques for Arabic Machine Translation",
    "doi": "https://doi.org/10.1145/3634681",
    "publication_date": "2023-11-27",
    "publication_year": 2023,
    "authors": "Nouhaila Bensalah; Aı̈da Habib; Abdellah Adib; Abdelhamid Ibn El Farouk",
    "corresponding_authors": "",
    "abstract": "Word embeddings are widely deployed in a tremendous range of fundamental natural language processing applications and are also useful for generating representations of paragraphs, sentences, and documents. In some contexts involving constrained memory, it may be beneficial to reduce the size of word embeddings since they represent a core component of several natural language processing tasks. By reducing the dimensionality of word embeddings, their usefulness in memory-limited devices can be significantly improved, yielding gains in many real-world applications. This article aims to provide a comparative study of different dimensionality reduction techniques to generate efficient lower-dimensional word vectors. Based on empirical experiments carried out on the Arabic machine translation task, we found that the post-processing algorithm combined with independent component analysis provides optimal performance over the considered dimensionality reduction techniques. Therefore, we arrive at a new combination of the post-processing algorithm and dimensionality reduction (independent component analysis) techniques, which has not been investigated before. The latter was applied to both contextual and non-contextual word embeddings to reduce the size of the vectors while achieving a better translation quality than the original ones.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4389057691",
    "type": "article"
  },
  {
    "title": "Autoregressive Feature Extraction with Topic Modeling for Aspect-based Sentiment Analysis of Arabic as a Low-resource Language",
    "doi": "https://doi.org/10.1145/3638050",
    "publication_date": "2023-12-27",
    "publication_year": 2023,
    "authors": "Asmaa Hashem Sweidan; Nashwa El-Bendary; Esraa Elhariri",
    "corresponding_authors": "",
    "abstract": "This paper proposes an approach for aspect-based sentiment analysis of Arabic social data, especially the considerable text corpus generated through communications on X (formerly known as Twitter) for expressing opinions in Arabic-language tweets during the COVID-19 pandemic. The proposed approach examines the performance of several pre-trained predictive and autoregressive language models; namely, Bidirectional Encoder Representations from Transformers (BERT) and XLNet, along with topic modeling algorithms; namely, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), for aspect-based sentiment analysis of online Arabic text. In addition, Bidirectional Long Short Term Memory (Bi-LSTM) deep learning model is used to classify the extracted aspects from online reviews. Obtained experimental results indicate that the combined XLNet-NMF model outperforms other implemented state-of-the-art methods through improving the feature extraction of unstructured social media text with achieving values of 0.946 and 0.938, for average sentiment classification accuracy and F-measure, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4390262829",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis of MOOC Reviews Based on Knowledge Dependency Tree",
    "doi": "https://doi.org/10.1145/3713073",
    "publication_date": "2025-01-20",
    "publication_year": 2025,
    "authors": "Haijie Wang; Jiajia Jiao",
    "corresponding_authors": "",
    "abstract": "As an important online learning resource, Massive Open Online Courses have a large amount of comments, which can be exploited by aspect-level sentiment analysis to optimize MOOC teaching from different perspectives. However, there are two essential problems. One is that there is no open-source dataset on Chinese MOOC. The other problem is semantic information confusion caused by inherent polysemy of Chinese words and ambiguous expressions relatively relying on the context. In order to further characterize the special features of Chinese MOOC reviews, we build an open-source dataset with clean 5000 MOOC reviews and propose a sentiment knowledge dependency tree based graph neural network. The proposed model firstly uses the latest term frequency–inverse document frequency algorithm to extract high-frequency words, and combines it with the Semantic Orientation Pointwise Mutual Information algorithm so that a sentiment dictionary in the field of Chinese MOOCs is constructed. Then, the grammatical information of the dependency tree is merged with the sentiment knowledge information of the sentiment dictionary. Next, this novel model uses GCN to capture the long-distance feature information of the sentiment dependency tree, and finally adopts the softmax function for sentiment classification. To further improve the model's performance, we also use Bert to enhance the text representation for higher accuracy. Meanwhile, the comparative experiments demonstrate that our proposed model takes advantages of the customized dependency tree by knowledge dictionary to achieve more accurate sentiment analysis than the state-of-the-art methods under different word embedding approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406627219",
    "type": "article"
  },
  {
    "title": "Tibetan Text Classification based on Prompt Learning and Ensemble Learning",
    "doi": "https://doi.org/10.1145/3711827",
    "publication_date": "2025-01-21",
    "publication_year": 2025,
    "authors": "Chao Tang; Zheng-Hua Tan; Xiaobing Zhao",
    "corresponding_authors": "",
    "abstract": "With the advancement of pre-trained language models, prompt learning has emerged as a trend for text classification. It offers several advantages over traditional machine learning methods, particularly for low-resource natural language processing tasks. Prompt learning enables fine-tuning of pre-trained language models on relatively small datasets, eliminating the need for a large number of expensive labeled samples. This paper proposes an effective approach that combines prompt learning and ensemble learning, aiming to enhance the performance of individual language models in Tibetan text classification tasks. This approach introduces a different perspective by transforming the traditional text classification problem into an entailment relationship exploration. Instead of directly assigning categories to sentences, models judge the sentence category based on whether it is entailed by a given prompt. This innovative method enables us to leverage the strengths of prompt learning and ensemble learning simultaneously, resulting in improved classification accuracy. To evaluate the effectiveness of our approach, this paper conduct extensive experiments on two public datasets, TNCC and WCM. The experimental results demonstrate the effectiveness of our method, achieving weighted F1 scores of 72.72% and 78% on TNCC and WCM, respectively. Compared to traditional machine learning methods, this prompt learning approach exhibits a significant 10% performance gain in low-resource natural language processing tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406677808",
    "type": "article"
  },
  {
    "title": "Multimodal Detection of Offensive Content in Hindi Memes",
    "doi": "https://doi.org/10.1145/3717611",
    "publication_date": "2025-02-17",
    "publication_year": 2025,
    "authors": "Kriti Dubey; Vaishnavi Srivastava; Garima Sharma; Nonita Sharma; Deepak Kumar Sharma; Uttam Ghosh; Osama Alfarraj; Amr Tolba",
    "corresponding_authors": "",
    "abstract": "Activities like sharing of thoughts, advertising business, connecting with peers, and staying updated in this world are highly facilitated by social media platforms. In the unique form of media called memes, information is conveyed through the image-to-text or text-to-image dependency relationship. Popular memes are often driven by viewers rather than marketing or advertising techniques, which indicates the level of engagement of social media users with memes. Given the popularity of memes, a demand for a solution to identify and counteract hate-spreading memes on social media platforms is raised. In this study, a multimodal machine learning approach to detect offensive memes is presented, where the text of memes are spelled in the Devanagari script of the Hindi language. A dataset of 9262 images has been created, and they have been labeled as offensive or not offensive. As the dataset is highly imbalanced, another dataset which has 3732 images is created by undersampling the total dataset and the models are trained for both the imbalanced dataset as well as the balanced dataset. Finally, the classification problem is solved through a multimodal Logistic Regression classifier that utilizes concatenated feature representations of image and text. An accuracy of 0.81 is achieved by the model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407653544",
    "type": "article"
  },
  {
    "title": "MixSong: Diverse and Strictly Formatted Chinese Poetry Generation",
    "doi": "https://doi.org/10.1145/3718331",
    "publication_date": "2025-02-17",
    "publication_year": 2025,
    "authors": "Xinyu Song; Changlin Song; Haolu Yu; Yonghua Zhu; Hong Yao",
    "corresponding_authors": "",
    "abstract": "Chinese poetry, renowned for its elegance and simplicity, is a hallmark of Chinese culture. While neural networks have made significant advancements in generating poetry, balancing diversity with adherence to rigid structural formats remains a challenge. Research indicates that factors such as themes, emotions (e.g., happiness, sadness), and sentiments (e.g., positive, negative) play a crucial role in poetic creation, influencing both the diversity and quality of the generated content.In this paper, we propose MixSong, an autoregressive language model based on the Transformer architecture, designed to incorporate a wide range of conditional factors. MixSong utilizes adversarial training to integrate these factors, enabling the model to implicitly learn distributional information in the latent space. Additionally, we introduce several uniquely customized symbol sets, including paragraph identifiers, position identifiers, rhyme identifiers, tune identifiers, and conditional distinctive identifiers. These symbols help MixSong effectively capture and enforce the constraints necessary for generating high-quality poetry.Extensive experimental results demonstrate that MixSong significantly outperforms existing models in both automatic metrics and human evaluations, achieving notable improvements in both diversity and quality of the generated poetry.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407654582",
    "type": "article"
  },
  {
    "title": "TransQAM: Transformer-based Question Answering System in Malayalam",
    "doi": "https://doi.org/10.1145/3718085",
    "publication_date": "2025-02-19",
    "publication_year": 2025,
    "authors": "Reji Rahmath K; Reghu Raj P. C.; Rafeeque P.C",
    "corresponding_authors": "",
    "abstract": "Question Answering(QA) systems are used to extract the exact answer from a given context. In this study, we have implemented a QA system named TransQAM with BERT and its variants for the low resource Malayalam language. We have considered the transformer models, namely, BERT, multilingual BERT, XLM-RoBERTa, and MuRIL for implementation. Since there is no publicly available Malayalam dataset for QA, we have built and made publicly available a sufficiently large Malayalam QA dataset in SQuAD (Stanford Question Answering Dataset) format with 30k question-answer pairs. We have obtained state-of-the-art results for TransQAM implemented using MuRIL. Due to the advancement of language models and active research, many languages, such as English, have well developed QA systems compared to the unexplored Malayalam language. According to our knowledge, TransQAM is the first QA system in Malayalam that successfully applies transformer models to answer questions and achieves more than 80% accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407749995",
    "type": "article"
  },
  {
    "title": "New Bagging Based Ensemble Learning Algorithm Distinguishing Short and Long Texts for Document Classification",
    "doi": "https://doi.org/10.1145/3718740",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Youwei Wang; Lizhou Feng",
    "corresponding_authors": "",
    "abstract": "To improve the classification accuracy of ensemble learning, a new bootstrap aggregating (Bagging) ensemble learning algorithm distinguishing short and long texts for document classification is proposed. First, the performances of different typical deep learning methods on processing long and short texts are compared, and the optimal base classifiers for long and short texts are selected respectively. Second, the random sampling method in traditional bagging classification algorithms is improved, and a threshold group based random sampling method which can balance the numbers of long and short text subsets is proposed. Moreover, to improve the model inference speed and classification accuracy, the training of long and short text subsets is realized by combining the knowledge distillation theory. Finally, the sample classification probabilities on different categories are considered, and the category similarity information is combined with the traditional weighted voting classifier ensemble method to avoid the problem that the sampling process may decrease the accuracy. Experimental results on multiple datasets show that the algorithm can effectively improve the accuracy of document classification and has obvious advantages over typical deep learning algorithms and ensemble learning algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407785175",
    "type": "article"
  },
  {
    "title": "Knowledge-Based Spatio-Temporal Attention Network for Disease Prediction with Chinese Electronic Medical Records",
    "doi": "https://doi.org/10.1145/3731757",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "Zhichao Zhu; Jianqiang Li; Xi Xu; Qing Zhao",
    "corresponding_authors": "",
    "abstract": "The use of electronic medical record (EMR) to develop automated disease prediction has been suffering from sparse features. Graph neural network (GNN) can model the non-grid data and infer missing features, which is an effective method to solve the above problem. However, the relation of two entity nodes is difficult to explored by GNN, if they cross more than three sentences, which may loss some semantic information for disease inferring. Furthermore, temporal dependency of nodes in EMRs not only provides more information for predicting disease, but also enhances ability of semantic discrimination of node representations. This study proposes a knowledge-based spatio-temporal attention network (KSAN) for disease prediction. Knowledge graph and relation completion are firstly leveraged to build more relations among nodes to supply potential disease-related facts. Secondly, the dependency among nodes are comprehensively aggregated from spatio-temporal multidimension. Meanwhile, the key nodes' weights are increased by matching a key factor set, which provides more goal-directed information. Finally, the representative embedding is generated for disease prediction. The training of KSAN is based on semi-supervised learning, which helps to save resources. KSAN is evaluated using a real-world Chinese EMR dataset, the results indicate that KSAN achieves the best prediction performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409701359",
    "type": "article"
  },
  {
    "title": "Hybrid Approach for Automatic Text Summarization for Low-resourced Amharic Language",
    "doi": "https://doi.org/10.1145/3743677",
    "publication_date": "2025-06-07",
    "publication_year": 2025,
    "authors": "Moges Ahmed Mehamed; Shengwu Xiong; Awet Fesseha",
    "corresponding_authors": "",
    "abstract": "Automatic text summarization creates a concise version of the given document while retaining the original content's core ideas, logical structure, and understandability. Despite extensive research on summarization in English and other languages, there remains a shortage of work in Amharic due to limited resources and the challenges posed by the language's complex morphology, syntax, and semantics. Moreover, several feature selection methods have been put forth for major languages. Still, there are no published works on how well they work with the Amharic language in a limited resource context. Furthermore, before putting all of the features together, their individual effects on the hybrid summarization have still not been well investigated for the Amharic language. Our research identifies the best features and addresses the linguistic challenges in Amharic summarization by presenting a hybrid strategy that combines extractive and abstractive methodology and data scarcity issues. The extractive approach utilizes statistical and semantic features such as sentence position, length, and semantics to extract essential sentences. Integrating semantic features with the abstractive approach yielded promising results, surpassing even the combination of statistical and semantic features with the abstractive approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411121403",
    "type": "article"
  },
  {
    "title": "VietMedKG: Knowledge Graph and Benchmark for Traditional Vietnamese Medicine",
    "doi": "https://doi.org/10.1145/3744740",
    "publication_date": "2025-06-16",
    "publication_year": 2025,
    "authors": "Tam Trinh; Anh Dao; Thi Hong Nhung Hy; Truong Son Hy",
    "corresponding_authors": "",
    "abstract": "Traditional Vietnamese Medicine (TVM) and Traditional Chinese Medicine (TCM) have shared significant similarities due to their geographical location, cultural exchanges, and hot and humid climatic conditions. However, unlike TCM, which has substantial works published to construct a knowledge graph, there is a notable absence of a comprehensive knowledge graph for TVM. This paper presents the first endeavor to build a knowledge graph for TVM based on extensive existing resources from TCM. We name our knowledge graph as VietMedKG. We propose a translation and filtration process to adapt TCM knowledge graphs to TVM, identifying the overlapping and unique elements of TVM. In addition, the constructed knowledge graph is then exploited further for developing a curated benchmark for the knowledge graph-based question-answering problem with the potential to support doctors and patients in assisting doctors and patients in identifying various diseases. Our work will not only bridge the gap between TCM and TVM but also set the foundation for future research into traditional Vietnamese medicine community. Our source code is publicly available at https://github.com/HySonLab/VietMedKG.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411350492",
    "type": "article"
  },
  {
    "title": "Code-Mixed Romanized Hindi Hate Speech Identification: Leveraging BERT Embeddings and Particle Swarm Optimization",
    "doi": "https://doi.org/10.1145/3748326",
    "publication_date": "2025-07-15",
    "publication_year": 2025,
    "authors": "Shubham Shukla; Sushama Nagpal; Sangeeta Sabharwal",
    "corresponding_authors": "",
    "abstract": "The volume of hate speeches and the number of user-generated materials are steadily rising, notably on social media networks. This trend can be seen across the internet. Therefore, it is necessary to recognize this kind of offensive content and remove it to maintain the cleanness of the platform. In spite of the fact that pertinent research was conducted separately for detecting hate speeches and social media code-mixed texts, purpose of this work is to identify hate speeches from social media code-mixed text. In the presented research, experiments for detecting hate speech using Bidirectional Encoder Representations from Transformers (BERT) architecture are carried out with the accessible code-mixed dataset, and optimization is carried out using the Particle Swarm Optimization algorithm (PSO). The results of our proposed methodology showcased notable improvements in the performance over standard BERT model while achieving an accuracy of 95.37% and an F1-score of 95.30. Further, testing on an additional dataset confirms the generalizability of our approach, maintaining a high accuracy of 94.5%. These findings validate the effectiveness of PSO-driven optimization for hate speech detection in low-resource, code-mixed linguistic settings. To offer a comprehensive assessment of our approach, a comparative analysis with several classifiers, including Naive Bayes, Random Forest, XG Boost, CatBoost, KNN, Decision Tree, Adaboost, and SVM, LSTM has also been done.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412455735",
    "type": "article"
  },
  {
    "title": "HiEnWrite: A Hindi-English Bilingual Dataset for Big Five Personality Detection",
    "doi": "https://doi.org/10.1145/3756010",
    "publication_date": "2025-07-26",
    "publication_year": 2025,
    "authors": "Saksham Checker; Madhuri Yadav; Rahul Katarya",
    "corresponding_authors": "",
    "abstract": "Detecting human personality traits is a critical task across various domains, including healthcare, education, and psychology. Recent advancements in artificial intelligence have greatly enhanced the automatic detection of personality traits using writing styles and handwriting analysis. However, existing datasets and studies predominantly focus on English, limiting the generalizability to multilingual contexts. To bridge this gap, this study presents HiEnWrite, a novel Hindi-English bilingual dataset comprising handwritten samples collected from over 400 authors, annotated with Big Five personality scores. The dataset consists of approximately 800 images, annotated through rigorous Big Five questionnaires. We propose and evaluate an end-to-end convolutional neural network (CNN) tailored to detect five core personality traits. Additionally, extensive experimentation is conducted using transfer learning-based approaches with multiple CNN architectures, achieving a maximum correlation of 0.411 with VGG19. The model performance is thoroughly assessed using metrics like root mean square error (RMSE), training/testing durations, and residual analysis, demonstrating the efficacy of the proposed approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412653015",
    "type": "article"
  },
  {
    "title": "Adaptive Hierarchical Prompt for Open-Vocabulary Scene Graph Generation",
    "doi": "https://doi.org/10.1145/3748318",
    "publication_date": "2025-07-28",
    "publication_year": 2025,
    "authors": "Changkai Feng; Tong Xu; Shiwei Wu; Derong Xu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Scene graph has long been treated as the basic tool to summarize the visual semantics from a structural perspective, which always confronts the challenge of capturing open entity and relation classes. Recently, some efforts have been made to enhance the open-vocabulary scene graph generation (OV-SGG) task via prompt tuning techniques. However, as only one prompt is utilized for all the classes, they may confuse the modeling for relations with similar spatial positions or semantics. To address these challenges, in this article, we propose a novel one-stage framework, named R elation-decoupled T ransformer framework with adaptive H ierarchical P rompt ( RTHP ), based on the vision-language model. Specifically, we first develop the dual entities/relations deformable attention module in the relation-decoupled transformer, which decouples relations into subject and object relations, and deploys on entity/relation queries separately. Along this line, we further design the Adaptive Hierarchical Prompt (AHPro) module to model the inherent hierarchical structure of relation classes, which enables categories situated in varying positions to have different prompts. In this way, confusing categories can be easily distinguished by their respective positions in the hierarchical structure. Extensive experimental results demonstrate that our RTHP framework achieves competitive performance for OV-SGG, validating its effectiveness on base classes and generalization capability on novel classes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412903872",
    "type": "article"
  },
  {
    "title": "Text Sentiment Classification Using Knowledge Transfer-Based Matrix Diffractive Riemannian Residual Triangulation Topology Aggregation Model",
    "doi": "https://doi.org/10.1145/3749842",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "VijayalakshmiA Senthilkumar; B. Vinoth Kumar",
    "corresponding_authors": "",
    "abstract": "Text sentiment classification is the process of investigating and categorizing text data based on the sentiment expressed within it, typically into groups such as positive, negative, or neutral. Existing techniques for text sentiment classification often face challenges such as inefficient feature extraction, suboptimal feature selection, overfitting, and high computational complexity. To overcome challenges in text sentiment classification, a comprehensive approach is proposed. The preprocessing phase ensures data integrity and credibility by removing duplicates and unverified reviews, filtering out impure characters, eliminating stop words, and standardizing text through tokenization and punctuation removal. For feature extraction and selection, Term Frequency-Inverse Document Frequency (TF-IDF) coupled with Knowledge Transfer-based Adaptive Differential Evolution (KTADE) optimizes the selection of feature subsets, enhancing the effectiveness of sentiment analysis. Additionally, the Matrix Diffractive Riemannian Residual Triangulation Topology Aggregation (MDRRTTA) neural network model, integrated with a triangulation topology aggregation optimizer, efficiently classifies sentiments, reducing computational complexity and enhancing training speed. This holistic method marks a significant advancement in sentiment analysis methodologies. Proposed model exhibits outstanding performance, achieving remarkable accuracies of 99.56%, 99.35%, and 99.9% for Amazon, Yelp, and Internet Movie Database datasets, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412915430",
    "type": "article"
  },
  {
    "title": "UQuAD+: Benchmark Dataset for Urdu Machine Reading Comprehension",
    "doi": "https://doi.org/10.1145/3759455",
    "publication_date": "2025-08-07",
    "publication_year": 2025,
    "authors": "Samreen Kazi; Shakeel Khoja",
    "corresponding_authors": "",
    "abstract": "Machine Reading Comprehension (MRC) is a key task in Natural Language Understanding that enables automated systems to answer questions based on textual input. While MRC has made substantial progress for high-resource languages, low-resource languages pose significant challenges due to their complex linguistic features. This paper presents a comprehensive human-annotated dataset for Urdu MRC, comprising 20,000 question-answer pairs derived from 1,540 articles across seven domains. Unlike previous translation-based datasets, this dataset contains question-answer pairs created through rigorous crowd-sourcing and expert annotation. The dataset encompasses diverse question types, including both answerable and unanswerable questions, with answers ranging from single words to complete sentences, effectively capturing Urdu’s morphological richness and syntactic diversity. To address the limitations of traditional evaluation metrics like Exact Match (EM) and F 1 in assessing Urdu answers, we propose Semantic Match (SM), a metric designed to measure semantic equivalence between predicted and ground-truth answers. Our evaluation demonstrates the dataset’s increased complexity, with state-of-the-art models achieving only 0.82% SM accuracy. Together, the dataset and evaluation metric establish a robust framework for advancing Urdu MRC research, bridging critical gaps in both dataset quality and evaluation methodology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413039230",
    "type": "article"
  },
  {
    "title": "BERT-based Models for Keyword Extraction from Arabic Scientific Articles",
    "doi": "https://doi.org/10.1145/3761805",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Bilal Babayiğit; Hamza Sattuf; Mohammed Abubaker",
    "corresponding_authors": "",
    "abstract": "Keywords at the beginning of research articles are crucial for conveying the content and main ideas of academic works. They serve as essential tools for researchers to efficiently search for relevant topics. The integration of traditional natural language processing (NLP) techniques with modern deep learning methods has significantly advanced keyword extraction across various languages. However, extracting keywords from Arabic research poses unique challenges due to the language's complex morphology, rich semantics, and a scarcity of available resources. This study aims at identifying and analyzing keywords in a sample of studies from 480 peer-reviewed Arabic journals across diverse scientific fields, leading to the development of a specialized dataset for Arabic keyword extraction. The dataset comprises 38,728 records, each containing abstract and authors-specified keywords. We utilized this novel dataset to train and evaluate several BERT-based models tailored for the Arabic language, including bertBaseArabic, bertBaseQarib, camelBERT, multidialectBERT, and baseAraBERT. Prior to training, the dataset underwent comprehensive pre-processing, including data cleaning, lemmatization, and binary tagging. The keyword extraction task was transformed into a binary classification problem, where tokens were labeled as either keywords or non-keywords, simplifying the learning process. Experimental results indicate that the bertBaseQarib model achieved the highest performance, with an F1-score of 0.951, precision of 0.968, and recall of 0.940. This study highlights the effectiveness of BERT-based models in Arabic keyword extraction and emphasizes the importance of large, diverse datasets in achieving robust NLP outcomes. Future work will focus on expanding the dataset and optimizing model architectures for even better performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413287340",
    "type": "article"
  },
  {
    "title": "Multi-attention ghost pyramid fusion network for script identification of Chinese ancient document images",
    "doi": "https://doi.org/10.1145/3748314",
    "publication_date": "2025-09-08",
    "publication_year": 2025,
    "authors": "Hai Guo; Dawei Zhu; Jingying Zhao; Lingling Tong",
    "corresponding_authors": "",
    "abstract": "Script identification is a key step in document analysis and recognition in multilingual environments. This study proposed a new dataset for script identification algorithms, containing images of ancient documents in 12 different ethnic scripts, including Chinese script, Naxi Dongba script, Yi script, Shui script, Tangut script, ancient Zhuang script, ancient Buyi script, Tibetan script, Dai script, Chagatai script, Mongolian script, and Manchu script. Focusing on the high accuracy required for ancient script identification, this study proposed an method named multi-attention ghost pyramid fusion network (MAGPNet). MAGPNet consists of a feature extraction network, a channel feature pyramid, and a Multi-Headed Self-Attention Bottleneck Block. The feature extraction network utilizes lightweight convolutional modules and parameter-free attention modules to enhance MAGPNet's feature extraction capability while maintaining a lighter structure. The channel feature pyramid increases the model's robustness in processing ancient documents of different scales. The Multi-Headed Self-Attention Bottleneck Block, by introducing a Multi-Headed Self-Attention, focuses on effective features. Experiments demonstrate that MAGPNet achieves a 99.97% accuracy rate on the multilingual ancient document image script identification dataset, maintaining excellent classification performance across multiple datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414062817",
    "type": "article"
  },
  {
    "title": "A Multi-Output BERT Framework for Abusive Comment Detection and Sentiment Analysis on Low-Resource Language",
    "doi": "https://doi.org/10.1145/3766889",
    "publication_date": "2025-09-11",
    "publication_year": 2025,
    "authors": "M. K. Yagnik; M. Hashmi; Deepika Kumar; Khushi Jain; Ekagrah Grover; D. Jude Hemanth",
    "corresponding_authors": "",
    "abstract": "In the modern digital world, social media has become essential for interpersonal interaction by promoting the interchange of ideas and points of view. But there are difficulties in this digital environment, especially concerning rude behaviour and offensive remarks. To address both problems at once, the research focuses on sentiment analysis and abusive comment detection in social media interactions. The dataset contains Hate Speech and Offensive Content Identification (HASOC) data from 2019 to 2021 to identify hate speech in Hindi on various social media platforms. To categorise comments into abusive and non-abusive groups, several BERT models, including mBERT, DistilBERT, RoBERTa, HateBERT, and IndicBERT, have been utilized. Additionally, a comprehensive sentiment analysis of the derogatory comments has been performed. The research presents a stacked ensemble framework for binary (abusive and non-abusive) and multiclass (hate, offensive, and profane) classification that integrates predictions from mBERT, HateBERT, and IndicBERT models. Further, the study provides an integrated approach for providing abusive comment detection and sentiment analysis using a multi-output model. The proposed ensemble model achieves 94% accuracy in binary classification, with precision, recall, and F1 scores all approaching 94%. Multiclass ensemble models yield an accuracy of 93% and associated precision, recall, and F1 scores of 91%, 92%, and 92%, respectively. A comparative analysis using several state-of-the-art techniques has been generated to verify the efficacy of the suggested methodology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414128100",
    "type": "article"
  },
  {
    "title": "Towards Robust Morphosyntactic Analysis of L2 Korean: Evaluating and Fine-Tuning a Korean Language Model",
    "doi": "https://doi.org/10.1145/3767330",
    "publication_date": "2025-09-12",
    "publication_year": 2025,
    "authors": "Hakyung Sung; Gyu‐Ho Shin",
    "corresponding_authors": "",
    "abstract": "Despite the growing use of NLP in second language (L2) research, model accuracy in L2 settings remains underexplored. This study addresses this gap by evaluating and fine-tuning a Korean language model to extract morphosyntactic features (i.e., morpheme tokenization/tagging and dependency parsing) from L2-Korean texts. We begin by evaluating a domain‐general Korean language model on a gold‐annotated L2-Korean treebank. We then fine‐tune the model on L2-Korean data and quantify the resulting gains across diverse L1- and L2- datasets. Finally, we examine how model reliability varies with learner proficiency scores. Three key findings emerge: while the domain-general model excels at morpheme tokenization, it underperforms on morpheme tagging and dependency parsing; fine‐tuning substantially improves adaptability to L2 morphosyntax; and proficiency has minimal effect on morpheme‐level tasks but significantly affects dependency-parsing reliability. These results highlight the importance of incorporating L2 training data to improve morphosyntactic analysis in L2 settings and caution against uncritical reliance on automated dependency annotations, especially when performance varies across proficiency levels.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414266329",
    "type": "article"
  },
  {
    "title": "Contrastive Retrieval Methodology for Turkish Metaphor Detection and Identification",
    "doi": "https://doi.org/10.1145/3770072",
    "publication_date": "2025-09-30",
    "publication_year": 2025,
    "authors": "Emrah İnan",
    "corresponding_authors": "Emrah İnan",
    "abstract": "Metaphorical expressions, as a form of figurative language, are individually limited in their use. However, when both literal and non-literal meanings are considered, they are frequently used in web content. Hence, producing a balanced dataset to learn superior representations is a challenging task, and metaphor detection suffers from a limited training dataset. To alleviate this problem, we present a retrieval-based contrastive learning approach which first identifies candidate metaphors in the input text and then detects metaphorical expressions as a claim verification task in the inherently unbalanced setting of this study. Furthermore, we adapt contrastive learning to make it easier to distinguish between the literal and figurative meanings of the same expression. For the experimental setup, we extract non-literal and literal expressions along with their meanings and sample sentences from a Turkish dictionary. In the metaphor detection subtask, performance evaluation shows that sparse and dense search variations using the Turkish-e5-Large model achieve a Recall@10 (R@10) score of 0.614. Moreover, the SimCSE-TR-Contr-Sample-Meaning model achieves the highest Recall@10 (R@10) of 0.9739 on the generated test dataset for the metaphor identification subtask. In the real-world scenario, it achieves a competitive R@10 score of 0.8684, and these results clearly demonstrate that our model can generalise to this real-world scenario.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414626201",
    "type": "article"
  },
  {
    "title": "Healthcare-Focused Turkish Medical LLM: Training on Real Patient-Doctor Question-Answer Data for Enhanced Medical Insight",
    "doi": "https://doi.org/10.1145/3772000",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Mehmet Bayram; Banu Di̇ri̇; Savaş Yıldırım",
    "corresponding_authors": "",
    "abstract": "The development of a Turkish-specific Large Language Model (LLM) for healthcare presents a unique opportunity to enhance AI’s accessibility and relevance for Turkish-speaking medical practitioners and patients. This study introduces a specialized Turkish Medical LLM fine-tuned on over 167,732 real patient-doctor question-answer pairs sourced from a trusted medical platform and capturing authentic linguistics in Turkish medical language. Utilizing models like LLAMA 3, the fine-tuning process was supported by Low-Rank Adaptation (LoRA) and involved innovative methods to mitigate catastrophic forgetting, including spherical linear interpolation (Slerp) merging. Evaluation of the model’s performance through similarity scores, GPT-3.5 assessments, and expert reviews indicates significant improvement in the model’s ability to generate medically accurate responses. This Turkish Medical LLM demonstrates potential to support medical decision-making and patient interaction in Turkish healthcare settings, offering an essential resource for enhancing AI inclusivity across languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415243152",
    "type": "article"
  },
  {
    "title": "Morphosyntactically-Informed Coreference Resolution for Persian with Adaptive Pruning and Global Context Aggregation",
    "doi": "https://doi.org/10.1145/3772089",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Hassan Haji Mohammadi; Alireza Talebpour; Ahmad Mahmoudi-Aznaveh; Samaneh Yazdani",
    "corresponding_authors": "",
    "abstract": "Coreference resolution in Persian, a task critical to natural language understanding, presents unique challenges due to the language's pro-drop tendencies, flexible word order, and rich morphosyntactic agreement system. This study introduces the first end-to-end (e-2-e) neural architecture for comprehensive Persian coreference resolution, encompassing pronominal, nominal, and named entity mentions. The system leverages ParsBERT and innovatively integrates Joint Mention Detection and Type Classification (JMDTC), an Adaptive Antecedent Pruning Threshold (AAPT), Morphosyntactically-Informed Attention (MIA), and Cross-Segment Coreference with Global Context Aggregation (CS-GCA). By jointly optimizing mention detection and antecedent linking, the system surpasses traditional pipelined approaches, eliminating the need for handcrafted features and complex syntactic parsers. A CoNLL average F1-score of 76.16% was achieved by the system on the Mehr corpus, which represents a 4.03-point improvement compared to the previous state-of-the-art. Furthermore, it demonstrates robust generalization, achieving a CoNLL average F1-score of 74.20% on the RCDAT corpus (evaluated using the Uppsala test set). These findings facilitate scalable coreference resolution in low-resource languages presenting similar morphosyntactic challenges.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415423869",
    "type": "article"
  },
  {
    "title": "Developing Deep Learning Models for Turkish Automatic Punctuation Restoration Using a Novel Dataset",
    "doi": "https://doi.org/10.1145/3772087",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Yasin Görmez; Halil Arslan; Mustafa Lemi ELYAKAN",
    "corresponding_authors": "",
    "abstract": "Today, automatic speech recognition systems are widely used by individuals, institutions, and organizations. However, the lack of punctuation marks in the texts produced by these systems complicates the comprehensibility of the texts and hinders advanced text analysis. Consequently, there is an increasing need for automatic punctuation restoration models. A review of existing studies reveals that most research focuses on the English language, while languages like Turkish, which belong to the agglutinative language group, have been relatively underexplored. In this study, a unique dataset has been created for Turkish automatic punctuation restoration. Models developed using convolutional neural networks, transformer encoder, and FnetEncoder layers were trained and analyzed with this dataset. The hyper-parameters of the developed models were optimized using Bayesian optimization. The analysis results showed that the best performance was achieved by the transformer encoder-based model with an overall F-score of 90.10%. Additionally, all models were observed to be more successful in predicting periods and spaces compared to commas. This study contributes to the literature by focusing on the Turkish language and offers a novel approach to automatic punctuation restoration with the creation of a new dataset and the developed models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415424121",
    "type": "article"
  },
  {
    "title": "Development and Analysis of Speech Recognition Systems for Assamese Language Using HTK",
    "doi": "https://doi.org/10.1145/3137055",
    "publication_date": "2017-10-18",
    "publication_year": 2017,
    "authors": "Himangshu Sarma; Navanath Saharia; Utpal Sharma",
    "corresponding_authors": "",
    "abstract": "Language analysis is very important for the native speaker to connect with the digital world. Assamese is a relatively unexplored language. In this report, we analyze different aspects of speech-to-text processing, starting from building a speech corpus, defining syllable rules, and finally developing a speech search engine of Assamese. We have collected about 20 hours of speech in three (viz., read, extempore, and conversation) modes and transcribed it. We also discuss some issues and challenges faced during development of the corpus. We have developed an automatic syllabification model with 11 rules for the Assamese language and found an accuracy of more than 95% in our result. We found 12 different syllable patterns where 5 are found most frequent. The maximum length of a syllable found is four letters. With the help of Hidden Markov Model Toolkit (HTK) 3.5, we used deep learning based neural network for our speech recognition model, where we obtained 78.05% accuracy for automatic transcription of Assamese speech.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2765080642",
    "type": "article"
  },
  {
    "title": "Empirical Exploring Word-Character Relationship for Chinese Sentence Representation",
    "doi": "https://doi.org/10.1145/3156778",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Shaonan Wang; Jiajun Zhang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of learning compositional Chinese sentence representations, which represent the meaning of a sentence by composing the meanings of its constituent words. In contrast to English, a Chinese word is composed of characters, which contain rich semantic information. However, this information has not been fully exploited by existing methods. In this work, we introduce a novel, mixed character-word architecture to improve the Chinese sentence representations by utilizing rich semantic information of inner-word characters. We propose two novel strategies to reach this purpose. The first one is to use a mask gate on characters, learning the relation among characters in a word. The second one is to use a max-pooling operation on words to adaptively find the optimal mixture of the atomic and compositional word representations. Finally, the proposed architecture is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task. To further verify the generalization ability of our model, we employ the learned sentence representations as features in sentence classification task, question classification task, and sentence entailment task. Results have shown that the proposed mixed character-word sentence representation models outperform both the character-based and word-based models.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2786212628",
    "type": "article"
  },
  {
    "title": "Ancient–Modern Chinese Translation with a New Large Training Dataset",
    "doi": "https://doi.org/10.1145/3325887",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Dayiheng Liu; Kexin Yang; Qian Qu; Jiancheng Lv",
    "corresponding_authors": "",
    "abstract": "Ancient Chinese brings the wisdom and spirit culture of the Chinese nation. Automatic translation from ancient Chinese to modern Chinese helps to inherit and carry forward the quintessence of the ancients. However, the lack of large-scale parallel corpus limits the study of machine translation in ancient–modern Chinese. In this article, we propose an ancient–modern Chinese clause alignment approach based on the characteristics of these two languages. This method combines both lexical-based information and statistical-based information, which achieves 94.2 F1-score on our manual annotation Test set. We use this method to create a new large-scale ancient–modern Chinese parallel corpus that contains 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality ancient–modern Chinese dataset. Furthermore, we analyzed and compared the performance of the SMT and various NMT models on this dataset and provided a strong baseline for this task.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2887558369",
    "type": "article"
  },
  {
    "title": "Online Handwritten Gurmukhi Words Recognition",
    "doi": "https://doi.org/10.1145/3282441",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Sukhdeep Singh; Anuj Sharma",
    "corresponding_authors": "",
    "abstract": "Identification of offline and online handwritten words is a challenging and complex task. In comparison to Latin and Oriental scripts, the research and study of handwriting recognition at word level in Indic scripts is at its initial phases. The two main methods of handwriting recognition are global and analytical. The present work introduces a novel analytical approach for online handwritten Gurmukhi word recognition based on a minimal set of words and recognizes an input Gurmukhi word as a sequence of characters. We employed a sequential step-by-step approach to recognize online handwritten Gurmukhi words. Considering the massive variability in online Gurmukhi handwriting, the present work employs the completely linked non-homogeneous hidden Markov model. In the present study, we considered the dependent, major-dependent, and super-dependent nature of strokes to form Gurmukhi characters in words. On test sets of online handwritten Gurmukhi datasets, the word-level accuracy rates are 85.98%, 84.80%, 82.40%, and 82.20% in four different modes. Besides the online Gurmukhi word recognition, the present work also provides Gurmukhi handwriting analysis study for varying writing styles and proposes novel techniques for zone detection and rearrangement of strokes. Our proposed algorithms have been successfully employed to online handwritten Gurmukhi word recognition in dependent and independent modes of handwriting.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2910603638",
    "type": "article"
  },
  {
    "title": "From Genesis to Creole Language",
    "doi": "https://doi.org/10.1145/3321128",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Hongmin Wang; Jie Yang; Yue Zhang",
    "corresponding_authors": "",
    "abstract": "Singlish can be interesting to the computational linguistics community both linguistically, as a major low-resource creole based on English, and computationally, for information extraction and sentiment analysis of regional social media. In our conference paper, Wang et al. (2017), we investigated part-of-speech (POS) tagging and dependency parsing for Singlish by constructing a treebank under the Universal Dependencies scheme and successfully used neural stacking models to integrate English syntactic knowledge for boosting Singlish POS tagging and dependency parsing, achieving the state-of-the-art accuracies of 89.50% and 84.47% for Singlish POS tagging and dependency, respectively. In this work, we substantially extend Wang et al. (2017) by enlarging the Singlish treebank to more than triple the size and with much more diversity in topics, as well as further exploring neural multi-task models for integrating English syntactic knowledge. Results show that the enlarged treebank has achieved significant relative error reduction of 45.8% and 15.5% on the base model, 27% and 10% on the neural multi-task model, and 21% and 15% on the neural stacking model for POS tagging and dependency parsing, respectively. Moreover, the state-of-the-art Singlish POS tagging and dependency parsing accuracies have been improved to 91.16% and 85.57%, respectively. We make our treebanks and models available for further research.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2948080012",
    "type": "article"
  },
  {
    "title": "Leveraging Additional Resources for Improving Statistical Machine Translation on Asian Low-Resource Languages",
    "doi": "https://doi.org/10.1145/3314936",
    "publication_date": "2019-06-17",
    "publication_year": 2019,
    "authors": "Hai-Long Trieu; Vu Tran; Ashwin Ittoo; Le-Minh Nguyen",
    "corresponding_authors": "",
    "abstract": "Phrase-based machine translation (MT) systems require large bilingual corpora for training. Nevertheless, such large bilingual corpora are unavailable for most language pairs in the world, causing a bottleneck for the development of MT. For the Asian language pairs—Japanese, Indonesian, Malay paired with Vietnamese—they are also not excluded from the case, in which there are no large bilingual corpora on these low-resource language pairs. Furthermore, although the languages are widely used in the world, there is no prior work on MT, which causes an issue for the development of MT on these languages. In this article, we conducted an empirical study of leveraging additional resources to improve MT for the Asian low-resource language pairs: translation from Japanese, Indonesian, and Malay to Vietnamese. We propose an innovative approach that lies in two strategies of building bilingual corpora from comparable data and phrase pivot translation on existing bilingual corpora of the languages paired with English. Bilingual corpora were built from Wikipedia bilingual titles to enhance bilingual data for the low-resource languages. Additionally, we introduced a combined model of the additional resources to create an effective solution to improve MT on the Asian low-resource languages. Experimental results show the effectiveness of our systems with the improvement of +2 to +7 BLEU points. This work contributes to the development of MT on low-resource languages, especially opening a promising direction for the progress of MT on the Asian language pairs.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2951107454",
    "type": "article"
  },
  {
    "title": "Punjabi to ISO 15919 and Roman Transliteration with Phonetic Rectification",
    "doi": "https://doi.org/10.1145/3359991",
    "publication_date": "2020-02-07",
    "publication_year": 2020,
    "authors": "Muhammad Raihan Abbas; Khadim Hussain Asif",
    "corresponding_authors": "",
    "abstract": "Transliteration removes the script barriers. Unfortunately, Punjabi is written in four different scripts, i.e., Gurmukhi, Shahmukhi, Devnagri, and Latin. The Latin script is understandable for nearly all factions of the Punjabi community. The objective of our work is to transliterate the Punjabi Gurmukhi script into Latin script. There has been considerable progress in Punjabi to Latin transliteration, but the accuracy of present-day systems is less than 50% (Google Translator has approximately 45% accuracy). We do not have the facility of a rich parallel corpus for Punjabi, so we cannot use the corpus-based techniques of machine learning that are in vogue these days. The existing systems of transliteration follow grapheme-based approach. The grapheme-based transliteration is unable to handle many scenarios such as tones, inherent schwa, glottal stops, nasalization, and gemination. In this article, the grapheme-based transliteration has been augmented with phonetic rectification where the Punjabi script is rectified phonetically before applying character-to-character mapping. Handling the inherent short vowel schwa was the major challenge in phonetic rectification. Instead of following the fixed syllabic pattern, we devised a generic finite state transducer to insert schwa. The accuracy of our transliteration system is approximately 96.82%.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3016631951",
    "type": "article"
  },
  {
    "title": "Translating Morphologically Rich Indian Languages under Zero-Resource Conditions",
    "doi": "https://doi.org/10.1145/3407912",
    "publication_date": "2020-10-13",
    "publication_year": 2020,
    "authors": "Ashwani Tanwar; Prasenjit Majumder",
    "corresponding_authors": "",
    "abstract": "This work presents an in-depth analysis of machine translations of morphologically-rich Indo-Aryan and Dravidian languages under zero-resource conditions. It focuses on Zero-Shot Systems for these languages and leverages transfer-learning by exploiting target-side monolingual corpora and parallel translations from other languages. These systems are compared with direct translations using the BLEU and TER metrics. Further, Zero-Shot Systems are used as pre-trained models for fine-tuning with real human-generated data taken in different proportions that range from 100 sentences to the entire training set. Performances of the Indo-Aryan and Dravidian languages are compared with a focus on their morphological complexity. The systems with a Dravidian source language performed much better and reached very near to the level of direct translations. This is observed likely due to morphological richness and complexity in the language, which in turn provided more room for transfer-learning in this case. A comparative analysis based on language families has been done. These systems were fine-tuned further, which in turn outperformed direct translations with just 500 parallel sentences for a Dravidian source language. However, systems with an Indo-Aryan source language showed similar performance after getting fine-tuned with 10,000 sentences.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3107607461",
    "type": "article"
  },
  {
    "title": "Using Sub-character Level Information for Neural Machine Translation of Logographic Languages",
    "doi": "https://doi.org/10.1145/3431727",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Longtu Zhang; Mamoru Komachi",
    "corresponding_authors": "",
    "abstract": "Logographic and alphabetic languages (e.g., Chinese vs. English) have different writing systems linguistically. Languages belonging to the same writing system usually exhibit more sharing information, which can be used to facilitate natural language processing tasks such as neural machine translation (NMT). This article takes advantage of the logographic characters in Chinese and Japanese by decomposing them into smaller units, thus more optimally utilizing the information these characters share in the training of NMT systems in both encoding and decoding processes. Experiments show that the proposed method can robustly improve the NMT performance of both “logographic” language pairs (JA–ZH) and “logographic + alphabetic” (JA–EN and ZH–EN) language pairs in both supervised and unsupervised NMT scenarios. Moreover, as the decomposed sequences are usually very long, extra position features for the transformer encoder can help with the modeling of these long sequences. The results also indicate that, theoretically, linguistic features can be manipulated to obtain higher share token rates and further improve the performance of natural language processing systems.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3156662045",
    "type": "article"
  },
  {
    "title": "Dependency Parsing-based Entity Relation Extraction over Chinese Complex Text",
    "doi": "https://doi.org/10.1145/3450273",
    "publication_date": "2021-06-09",
    "publication_year": 2021,
    "authors": "Shanshan Qi; Limin Zheng; Feiyu Shang",
    "corresponding_authors": "",
    "abstract": "Open Relation Extraction (ORE) plays a significant role in the field of Information Extraction. It breaks the limitation that traditional relation extraction must pre-define relational types in the annotated corpus and specific domains restrictions, to realize the goal of extracting entities and the relation between entities in the open domain. However, with the increase of sentence complexity, the precision and recall of Entity Relation Extraction will be significantly reduced. To solve this problem, we present an unsupervised Clause_CORE method based on Chinese grammar and dependency parsing features. Clause_CORE is used for complex sentences processing, including decomposing complex sentence and dynamically complementing sentence components, which can reduce sentences complexity and maintain the integrity of sentences at the same time. Then, we perform dependency parsing for complete sentences and implement open entity relation extraction based on the model constructed by Chinese grammar rules. The experimental results show that the performance of Clause_CORE method is better than that of other advanced Chinese ORE systems on Wikipedia and Sina news datasets, which proves the correctness and effectiveness of the method. The results on mixed datasets of news data and encyclopedia data prove the generalization and portability of the method.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3173107258",
    "type": "article"
  },
  {
    "title": "Linguistic Resources for Bhojpuri, Magahi, and Maithili: Statistics about Them, Their Similarity Estimates, and Baselines for Three Applications",
    "doi": "https://doi.org/10.1145/3458250",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Rajesh Kumar Mundotiya; Manish Kumar Singh; Rahul Kapur; Swasti Mishra; Anil Kumar Singh",
    "corresponding_authors": "",
    "abstract": "Corpus preparation for low-resource languages and for development of human language technology to analyze or computationally process them is a laborious task, primarily due to the unavailability of expert linguists who are native speakers of these languages and also due to the time and resources required. Bhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in the north-eastern parts), are low-resource languages belonging to the Indo-Aryan (or Indic) family. They are closely related to Hindi, which is a relatively high-resource language, which is why we compare them with Hindi. We collected corpora for these three languages from various sources and cleaned them to the extent possible, without changing the data in them. The text belongs to different domains and genres. We calculated some basic statistical measures for these corpora at character, word, syllable, and morpheme levels. These corpora were also annotated with parts-of-speech (POS) and chunk tags. The basic statistical measures were both absolute and relative and were expected to indicate linguistic properties, such as morphological, lexical, phonological, and syntactic complexities (or richness). The results were compared with a standard Hindi corpus. For most of the measures, we tried to match the corpus size across the languages to avoid the effect of corpus size, but in some cases it turned out that using the full corpus was better, even if sizes were very different. Although the results are not very clear, we tried to draw some conclusions about the languages and the corpora. For POS tagging and chunking, the BIS tagset was used to manually annotate the data. The POS-tagged data sizes are 16,067, 14,669, and 12,310 sentences, respectively, for Bhojpuri, Magahi, and Maithili. The sizes for chunking are 9,695 and 1,954 sentences for Bhojpuri and Maithili, respectively. The inter-annotator agreement for these annotations, using Cohen’s Kappa, was 0.92, 0.64, and 0.74, respectively, for the three languages. These (annotated) corpora have been used for developing preliminary automated tools, which include POS tagger, Chunker, and Language Identifier. We have also developed the Bilingual dictionary (Purvanchal languages to Hindi) and a Synset (that can be integrated later in the Indo-WordNet) as additional resources. The main contribution of the work is the creation of basic resources for facilitating further language processing research for these languages, providing some quantitative measures about them and their similarities among themselves and with Hindi. For similarities, we use a somewhat novel measure of language similarity based on an n-gram-based language identification algorithm. An additional contribution is providing baselines for three basic NLP applications (POS tagging, chunking, and language identification) for these closely related languages.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3194993673",
    "type": "article"
  },
  {
    "title": "Semantic Graphical Dependence Parsing Model in Improving English Teaching Abilities",
    "doi": "https://doi.org/10.1145/3425633",
    "publication_date": "2021-05-31",
    "publication_year": 2021,
    "authors": "Erlu Wang; Priyan Malarvizhi Kumar; R. Dinesh Jackson Samuel",
    "corresponding_authors": "",
    "abstract": "It is a very difficult problem to achieve high-order functionality for graphical dependency parsing without growing decoding difficulties. To solve this problem, this article offers a way for Semantic Graphical Dependence Parsing Model (SGDPM) with a language-dependency model and a beam search to represent high-order functions for computer applications. The first approach is to scan a large amount of unnoticed data using a baseline parser. It will build auto-parsed data to create the Language-dependence Model (LDM). The LDM is based on a set of new features during beam search decoding, where it will incorporate the LDM features into the parsing model and utilize the features in parsing models of bilingual text. Our approach has main benefits, which include rich high-order features that are described given the large size and the additional large crude corpus for increasing the difficulty of decoding. Further, SGDPM has been evaluated using the suggested method for parsing tasks of mono-parsing text and bi-parsing text to carry out experiments on the English and Chinese data in the mono-parsing text function using computer applications. Experimental results show that the most accurate Chinese data is obtained with the best known English data systems and their comparable accuracy. Furthermore, the lab-scale experiments on the Chinese/General bilingual information in the bitext parsing process outperform the best recorded existing solutions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3195427419",
    "type": "article"
  },
  {
    "title": "geoGAT: Graph Model Based on Attention Mechanism for Geographic Text Classification",
    "doi": "https://doi.org/10.1145/3434239",
    "publication_date": "2021-09-22",
    "publication_year": 2021,
    "authors": "Weipeng Jing; Xianyang Song; Donglin Di; Houbing Song",
    "corresponding_authors": "",
    "abstract": "In the area of geographic information processing, there are few researches on geographic text classification. However, the application of this task in Chinese is relatively rare. In our work, we intend to implement a method to extract text containing geographical entities from a large number of network texts. The geographic information in these texts is of great practical significance to transportation, urban and rural planning, disaster relief, and other fields. We use the method of graph convolutional neural network with attention mechanism to achieve this function. Graph attention networks (GAT) is an improvement of graph convolutional neural networks (GCN). Compared with GCN, the advantage of GAT is that the attention mechanism is proposed to weight the sum of the characteristics of adjacent vertices. In addition, We construct a Chinese dataset containing geographical classification from multiple datasets of Chinese text classification. The Macro-F Score of the geoGAT we used reached 95% on the new Chinese dataset.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3199503730",
    "type": "article"
  },
  {
    "title": "SGATS: Semantic Graph-based Automatic Text Summarization from Hindi Text Documents",
    "doi": "https://doi.org/10.1145/3464381",
    "publication_date": "2021-09-20",
    "publication_year": 2021,
    "authors": "Manju Lata Joshi; Nisheeth Joshi; Namita Mittal",
    "corresponding_authors": "",
    "abstract": "Creating a coherent summary of the text is a challenging task in the field of Natural Language Processing (NLP). Various Automatic Text Summarization techniques have been developed for abstractive as well as extractive summarization. This study focuses on extractive summarization which is a process containing selected delineative paragraphs or sentences from the original text and combining these into smaller forms than the document(s) to generate a summary. The methods that have been used for extractive summarization are based on a graph-theoretic approach, machine learning, Latent Semantic Analysis (LSA), neural networks, cluster, and fuzzy logic. In this paper, a semantic graph-based approach SGATS (Semantic Graph-based approach for Automatic Text Summarization) is proposed to generate an extractive summary. The proposed approach constructs a semantic graph of the original Hindi text document by establishing a semantic relationship between sentences of the document using Hindi Wordnet ontology as a background knowledge source. Once the semantic graph is constructed, fourteen different graph theoretical measures are applied to rank the document sentences depending on their semantic scores. The proposed approach is applied to two data sets of different domains of Tourism and Health. The performance of the proposed approach is compared with the state-of-the-art TextRank algorithm and human-annotated summary. The performance of the proposed system is evaluated using widely accepted ROUGE measures. The outcomes exhibit that our proposed system produces better results than TextRank for health domain corpus and comparable results for tourism corpus. Further, correlation coefficient methods are applied to find a correlation between eight different graphical measures and it is observed that most of the graphical measures are highly correlated.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3200702136",
    "type": "article"
  },
  {
    "title": "UrduAI: Writeprints for Urdu Authorship Identification",
    "doi": "https://doi.org/10.1145/3476467",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Raheem Sarwar; Saeed‐Ul Hassan",
    "corresponding_authors": "",
    "abstract": "The authorship identification task aims at identifying the original author of an anonymous text sample from a set of candidate authors. It has several application domains such as digital text forensics and information retrieval. These application domains are not limited to a specific language. However, most of the authorship identification studies are focused on English and limited attention has been paid to Urdu. However, existing Urdu authorship identification solutions drop accuracy as the number of training samples per candidate author reduces and when the number of candidate authors increases. Consequently, these solutions are inapplicable to real-world cases. Moreover, due to the unavailability of reliable POS taggers or sentence segmenters, all existing authorship identification studies on Urdu text are limited to the word n-grams features only. To overcome these limitations, we formulate a stylometric feature space, which is not limited to the word n-grams feature only. Based on this feature space, we use an authorship identification solution that transforms each text sample into a point set, retrieves candidate text samples, and relies on the nearest neighbors classifier to predict the original author of the anonymous text sample. To evaluate our solution, we create a significantly larger corpus than existing studies and conduct several experimental studies that show that our solution can overcome the limitations of existing studies and report an accuracy level of 94.03%, which is higher than all previous authorship identification works.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3207676813",
    "type": "article"
  },
  {
    "title": "Authorship Attribution for a Resource Poor Language—Urdu",
    "doi": "https://doi.org/10.1145/3487061",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Zulqarnain Nazir; Khurram Shahzad; Muhammad Kamran Malik; Waheed Anwar; Imran Sarwar Bajwa; Khawar Mehmood",
    "corresponding_authors": "",
    "abstract": "Authorship attribution refers to examining the writing style of authors to determine the likelihood of the original author of a document from a given set of potential authors. Due to the wide range of authorship attribution applications, a plethora of studies have been conducted for various Western, as well as Asian, languages. However, authorship attribution research in the Urdu language has just begun, although Urdu is widely acknowledged as a prominent South Asian language. Furthermore, the existing studies on authorship attribution in Urdu have addressed a considerably easier problem of having less than 20 candidate authors, which is far from the real-world settings. Therefore, the findings from these studies may not be applicable to the real-world settings. To that end, we have made three key contributions: First, we have developed a large authorship attribution corpus for Urdu, which is a low-resource language. The corpus is composed of over 2.6 million tokens and 21,938 news articles by 94 authors, which makes it a closer substitute to the real-world settings. Second, we have analyzed hundreds of stylometry features used in the literature to identify 194 features that are applicable to the Urdu language and developed a taxonomy of these features. Finally, we have performed 66 experiments using two heterogeneous datasets to evaluate the effectiveness of four traditional and three deep learning techniques. The experimental results show the following: (a) Our developed corpus is many folds larger than the existing corpora, and it is more challenging than its counterparts for the authorship attribution task, and (b) Convolutional Neutral Networks is the most effective technique, as it achieved a nearly perfect F1 score of 0.989 for an existing corpus and 0.910 for our newly developed corpus.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4200054345",
    "type": "article"
  },
  {
    "title": "Improving Deep Learning based Automatic Speech Recognition for Gujarati",
    "doi": "https://doi.org/10.1145/3483446",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Deepang Raval; Vyom Pathak; Muktan Patel; Brijesh Bhatt",
    "corresponding_authors": "",
    "abstract": "We present a novel approach for improving the performance of an End-to-End speech recognition system for the Gujarati language. We follow a deep learning-based approach that includes Convolutional Neural Network, Bi-directional Long Short Term Memory layers, Dense layers, and Connectionist Temporal Classification as a loss function. To improve the performance of the system with the limited size of the dataset, we present a combined language model (Word-level language Model and Character-level language model)-based prefix decoding technique and Bidirectional Encoder Representations from Transformers-based post-processing technique. To gain key insights from our Automatic Speech Recognition (ASR) system, we used the inferences from the system and proposed different analysis methods. These insights help us in understanding and improving the ASR system as well as provide intuition into the language used for the ASR system. We have trained the model on the Microsoft Speech Corpus, and we observe a 5.87% decrease in Word Error Rate (WER) with respect to base-model WER.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4200284299",
    "type": "article"
  },
  {
    "title": "An EDU-Based Approach for Thai Multi-Document Summarization and Its Application",
    "doi": "https://doi.org/10.1145/2641567",
    "publication_date": "2015-01-30",
    "publication_year": 2015,
    "authors": "Nongnuch Ketui; Thanaruk Theeramunkong; Chutamanee Onsuwan",
    "corresponding_authors": "",
    "abstract": "Due to lack of a word/phrase/sentence boundary, summarization of Thai multiple documents has several challenges in unit segmentation, unit selection, duplication elimination, and evaluation dataset construction. In this article, we introduce Thai Elementary Discourse Units (TEDUs) and their derivatives, called Combined TEDUs (CTEDUs), and then present our three-stage method of Thai multi-document summarization, that is, unit segmentation, unit-graph formulation, and unit selection and summary generation. To examine performance of our proposed method, a number of experiments are conducted using 50 sets of Thai news articles with their manually constructed reference summaries. Based on measures of ROUGE-1, ROUGE-2, and ROUGE-SU4, the experimental results show that: (1) the TEDU-based summarization outperforms paragraph-based summarization; (2) our proposed graph-based TEDU weighting with importance-based selection achieves the best performance; and (3) unit duplication consideration and weight recalculation help improve summary quality.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2251004269",
    "type": "article"
  },
  {
    "title": "Correcting Chinese Spelling Errors with Word Lattice Decoding",
    "doi": "https://doi.org/10.1145/2791389",
    "publication_date": "2015-11-11",
    "publication_year": 2015,
    "authors": "Yu‐Ming Hsieh; Ming-Hong Bai; Shu-Ling Huang; Keh-Jiann Chen",
    "corresponding_authors": "",
    "abstract": "Chinese spell checkers are more difficult to develop because of two language features: 1) there are no word boundaries, and a character may function as a word or a word morpheme; and 2) the Chinese character set contains more than ten thousand characters. The former makes it difficult for a spell checker to detect spelling errors, and the latter makes it difficult for a spell checker to construct error models. We develop a word lattice decoding model for a Chinese spell checker that addresses these difficulties. The model performs word segmentation and error correction simultaneously, thereby solving the word boundary problem. The model corrects nonword errors as well as real-word errors. In order to better estimate the error distribution of large character sets for error models, we also propose a methodology to extract spelling error samples automatically from the Google web 1T corpus. Due to the large quantity of data in the Google web 1T corpus, many spelling error samples can be extracted, better reflecting spelling error distributions in the real world. Finally, in order to improve the spell checker for real applications, we produce n-best suggestions for spelling error corrections. We test our proposed approach with the Bakeoff 2013 CSC Datasets; the results show that the proposed methods with the error model significantly outperform the performance of Chinese spell checkers that do not use error models.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2252883486",
    "type": "article"
  },
  {
    "title": "Boosting Neural POS Tagger for Farsi Using Morphological Information",
    "doi": "https://doi.org/10.1145/2934676",
    "publication_date": "2016-07-22",
    "publication_year": 2016,
    "authors": "Peyman Passban; Qun Liu; Andy Way",
    "corresponding_authors": "",
    "abstract": "Farsi (Persian) is a low-resource language that suffers from the data sparsity problem and a lack of efficient processing tools. Due to their broad application in natural language processing tasks, part-of-speech (POS) taggers are one of those important tools that should be considered in this respect. Despite recent work on Farsi tagging, there is still room for improvement. The best reported accuracy so far is 96%, which in special cases can rise to 96.9%. The main problem with existing taggers is their inefficiency in coping with out-of-vocabulary (OOV) words. Addressing both problems of accuracy and OOV words, we developed a neural network-based POS tagger (NPT) that performs efficiently on Farsi. Despite using less data, NPT provides better results in comparison to state-of-the-art systems. Our proposed tagger performs with an accuracy of 97.4%, with performance highly influenced by morphological features. We carry out a shallow morphological analysis and show considerable improvement over the baseline configuration.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2503632851",
    "type": "article"
  },
  {
    "title": "Development of a Dataset and a Deep Learning Baseline Named Entity Recognizer for Three Low Resource Languages: Bhojpuri, Maithili, and Magahi",
    "doi": "https://doi.org/10.1145/3533428",
    "publication_date": "2022-07-18",
    "publication_year": 2022,
    "authors": "Rajesh Kumar Mundotiya; Shantanu Kumar; Ajeet Kumar; Umesh Chaudhary; Supriya Chauhan; Swasti Mishra; Praveen Gatla; Anil Kumar Singh",
    "corresponding_authors": "",
    "abstract": "In Natural Language Processing (NLP) pipelines, Named Entity Recognition (NER) is one of the preliminary problems, which marks proper nouns and other named entities such as Location, Person, Organization, Disease and so on. Such entities, without an NER module, adversely affect the performance of a machine translation system. NER helps in overcoming this problem by recognizing and handling such entities separately, although it can be useful in Information Extraction systems also. Bhojpuri, Maithili, and Magahi are low resource languages, usually known as Purvanchal languages. This article focuses on the development of an NER benchmark dataset for Machine Translation systems developed to translate from these languages to Hindi by annotating parts of the available corpora with named entities. Bhojpuri, Maithili, and Magahi corpora of sizes 228,373, 157,468, and 56,190 tokens, respectively, were annotated using 22 entity labels. The annotation considers coarse-grained annotation labels followed by the tagset used in one of the Hindi NER datasets. We also report a Deep Learning baseline that uses an LSTM-CNNs-CRF model. The lower baseline F 1 -scores from the NER tool obtained by using Conditional Random Fields models are 70.56% for Bhojpuri, 73.19% for Maithili, and 84.18% for Magahi. The Deep Learning-based technique (LSTM-CNNs-CRF) achieved 61.41% for Bhojpuri, 71.38% for Maithili, and 86.39% for Magahi. As the results show, LSTM-CNNs-CRF fails to outperform the lower baseline in the case of Bhojpuri and Maithili, which have more data in terms of the number of tokens, but not in terms of the number of named entities. However, the cross-lingual model training of LSTM-CNNs-CRF for Bhojpuri and Maithili performed better than the CRF.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3086782256",
    "type": "article"
  },
  {
    "title": "Revolutionizing Healthcare: NLP, Deep Learning, and WSN Solutions for Managing the COVID-19 Crisis",
    "doi": "https://doi.org/10.1145/3639566",
    "publication_date": "2024-01-05",
    "publication_year": 2024,
    "authors": "P. Ajay; B. Nagaraj; R. Arun Kumar",
    "corresponding_authors": "",
    "abstract": "The COVID-19 outbreak in 2020 catalyzed a global socio-economic upheaval, compelling nations to embrace digital technologies as a means of countering economic downturns and ensuring efficient communication systems. This paper delves into the role of Natural Language Processing (NLP) in harnessing wireless connectivity during the pandemic. The examination assesses how wireless networks have affected various facets of crisis management, including virus tracking, optimizing healthcare, facilitating remote education, and enabling unified communications. Additionally, the article underscores the importance of digital inclusion in mitigating disease outbreaks and reconnecting marginalized communities. To address these challenges, a Dual CNN-based BERT model is proposed. BERT model is used to extract the text features, the internal layers of BERT excel at capturing intricate contextual details concerning words and phrases, rendering them highly valuable as features for a wide array of text analysis tasks. The significance of dual CNN is capturing the unique capability to seamlessly integrate both character-level and word-level information. This fusion of insights from different levels of textual analysis proves especially valuable in handling text data that is noisy, complex, or presents challenges related to misspellings and domain-specific terminology. The proposed model is evaluated using the simulated WSN-based text data for crisis management.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390604915",
    "type": "article"
  },
  {
    "title": "Multi-Criteria Decision-Making Framework with Fuzzy Queries for Multimedia Data Fusion",
    "doi": "https://doi.org/10.1145/3640339",
    "publication_date": "2024-01-16",
    "publication_year": 2024,
    "authors": "Khalid Haseeb; Irshad Ahmad; Mohammad Siraj; Naveed Abbas; Gwanggil Jeon",
    "corresponding_authors": "",
    "abstract": "Multimedia Internet of Things (MIoT) is widely explored in many smart applications for connectivity with wireless communication. Such networks are not like ordinary networks because it has to collect a massive amount of data and are further forwarded to processing systems. As MIoT is very limited in terms of resources for healthcare, smart homes, etc., therefore, energy efficiency with reliable data transmission is a significant research challenge. As smart applications rely on bounded constraints, therefore duplicate and unnecessary data transmission should be minimized. In addition, the timely delivery of data in crucial circumstances has a significant impact on any proposed system. Consequently, this paper presents a fuzzy logic-based edge computing framework to provide cooperative decision-making while avoiding inefficient use of the sensing power of smart devices. The proposed framework can be applied to critical applications to improve response time and processing cost. It consists of the following two functional components: Firstly, it provides the automated routing process with a natural language interface at the sink node. Secondly, to ensure reasonable performance, it also transmits semantic data between sensors using fuzzy queries and security. According to the performance evaluation, the proposed framework significantly outperformed related studies in terms of energy consumption, packet overhead, network throughput, and end-to-end delay.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390918537",
    "type": "article"
  },
  {
    "title": "Fuzzified Deep Learning based Forgery Detection of Signatures in the Healthcare Mission Records",
    "doi": "https://doi.org/10.1145/3641818",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Ishu Priya; Nisha Chaurasia; Ashutosh Kumar Singh; Nakul Mehta; Abhishek Singh Kilak; Ahmed Alkhayyat",
    "corresponding_authors": "",
    "abstract": "In an era subjected to digital solutions, handwritten signatures continue playing a crucial role in identity verification and document authentication. These signatures, a form of bio-metric verification, are unique to every individual, serving as a primitive method for confirming identity and ensuring security of an individual. Signatures, apart from being a means of personal authentication, are often considered a cornerstone in the validation of critical documents and processes, especially within the healthcare sector. In healthcare missions, particularly in the regions that are underdeveloped, hand-written records persist as the primary mode of documentation. The credibility of these handwritten documents hinges on the authenticity of the accompanying signatures, making signature verification a paramount safeguard for the integrity and security of medical information. Nonetheless, traditional offline methods of signature identification can be time-consuming and inefficient, particularly while dealing with a massive volume of documents. This arises the evident need for automated signature verification systems. Our research introduces an innovative signature verification system which synthesizes the strengths of fuzzy logic and CNN (Convolutional Neural Networks) to deliver precise and efficient signature verification. Leveraging the capabilities of Fuzzy Logic for feature representation and CNNs for discriminative learning, our proposed hybrid model offers a compelling solution. Through rigorous training, spanning a mere 28 epochs, our hybrid model exhibits remarkable performance by attaining a training accuracy of 91.29% and a test accuracy of 88.47%, underscoring its robust generalization capacity. In an era of evolving security requirements and the persistent relevance of handwritten signatures, our research links the disparity between tradition and modernity.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391169586",
    "type": "article"
  },
  {
    "title": "Contrastive Language-knowledge Graph Pre-training",
    "doi": "https://doi.org/10.1145/3644820",
    "publication_date": "2024-02-09",
    "publication_year": 2024,
    "authors": "Xiaowei Yuan; Kang Liu; Yequan Wang",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed a surge of academic interest in knowledge-enhanced pre-trained language models (PLMs) that incorporate factual knowledge to enhance knowledge-driven applications. Nevertheless, existing studies primarily focus on shallow, static, and separately pre-trained entity embeddings, with few delving into the potential of deep contextualized knowledge representation for knowledge incorporation. Consequently, the performance gains of such models remain limited. In this article, we introduce a simple yet effective knowledge-enhanced model, College ( Co ntrastive L anguage-Know le dge G raph Pr e -training), which leverages contrastive learning to incorporate factual knowledge into PLMs. This approach maintains the knowledge in its original graph structure to provide the most available information and circumvents the issue of heterogeneous embedding fusion. Experimental results demonstrate that our approach achieves more effective results on several knowledge-intensive tasks compared to previous state-of-the-art methods. Our code and trained models are available at https://github.com/Stacy027/COLLEGE .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391681213",
    "type": "article"
  },
  {
    "title": "MSEConv: A Unified Warping Framework for Video Frame Interpolation",
    "doi": "https://doi.org/10.1145/3648364",
    "publication_date": "2024-02-14",
    "publication_year": 2024,
    "authors": "Xiangling Ding; Pu Huang; Dengyong Zhang; Wei Liang; Feng Li; Gaobo Yang; Xin Liao; Yue Li",
    "corresponding_authors": "",
    "abstract": "Within the context of video frame interpolation, complex motion modeling is the task of capturing, in a video sequence, where the moving objects are located in the interpolated frame, and how to maintain the temporal consistency of motion. Existing video frame interpolation methods typically assign either a fixed size of the motion kernel or a refined optical flow to model complex motions. However, they have the limitation of data redundancy and inaccuracy representation of motion. This paper introduces a unified warping framework, named multi-scale expandable deformable convolution (MSEConv), for simultaneously performing complex motion modeling and frame interpolation. In the proposed framework, a deep fully convolutional neural network with global attention is proposed to estimate multiple small-scale kernel weights with different expansion degrees and adaptive weight allocation for each pixel synthesis. Moreover, most of the kernel-based interpolation methods can be treated as the special case of the proposed MSEConv, thus, MSEConv can be easily transferred to other kernel-based frame interpolation methods for performance improvement. To further improve the robustness of motion occlusions, an operation of mask occlusion is introduced. As a consequence, our proposed MSEConv shows strong performance on par or even better than the state-of-the-art kernel-based frame interpolation works on public datasets. Our source code and visual comparable results are available at https://github.com/Pumpkin123709/MSEConv.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391843422",
    "type": "article"
  },
  {
    "title": "Natural Language Processing System for Text Classification Corpus Based on Machine Learning",
    "doi": "https://doi.org/10.1145/3648361",
    "publication_date": "2024-02-19",
    "publication_year": 2024,
    "authors": "Yawen Su",
    "corresponding_authors": "Yawen Su",
    "abstract": "A classification system for hazardous materials in air traffic control was investigated using the Human Factors Analysis and Classification System (HFACS) framework and natural language processing to prevent hazardous situations in air traffic control. Based on the development of the HFACS standard, an air traffic control hazard classification system will be created. The dangerous data of the aviation safety management system is selected by dead bodies, classified and marked in five levels. Time Frame Return Frequency TextRank text classification method based on key content extraction and text classification model based on Convolutional Neural Network and Bidirectional Encoder Representations from Transforms models were used in the experiment to solve the problem of small samples, many labels and random samples in hazardous environment of air pollution control. The results show that the total cost of model training time and classification accuracy is the highest when the keywords are around 8. As the number of points increases, the time spent in dimensioning decreases and affects accuracy. When the number of points reaches about 93, the time spent in determining the size increases, but the accuracy of the allocation remains close to 0.7, but the increase in the value of time leads to a decrease in the total cost. It has been proven that extracting key content can solve text classification problems for small companies and contribute to further research in the development of security systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391941503",
    "type": "article"
  },
  {
    "title": "A Hybrid Scene Text Script Identification Network for Regional Indian Languages",
    "doi": "https://doi.org/10.1145/3649439",
    "publication_date": "2024-02-24",
    "publication_year": 2024,
    "authors": "Veronica Naosekpam; Nilkanta Sahu",
    "corresponding_authors": "",
    "abstract": "In this work, we introduce WAFFNet, an attention-centric feature fusion architecture tailored for word-level multi-lingual scene text script identification. Motivated by the limitations of traditional approaches that rely exclusively on feature-based methods or deep learning strategies, our approach amalgamates statistical and deep features to bridge the gap. At the core of WAFFNet, we utilized the merits of Local Binary Pattern—a prominent descriptor capturing low-level texture features with high-dimensional, semantically-rich convolutional features. This fusion is judiciously augmented by a spatial attention mechanism, ensuring targeted emphasis on semantically critical regions of the input image. To address the class imbalance problem in multi-class classification scenarios, we employed a weighted objective function. This not only regularizes the learning process but also addresses the class imbalance problem. The architectural integrity of WAFFNet is preserved through an end-to-end training paradigm, leveraging transfer learning to expedite convergence and optimize performance metrics. Considering the under-representation of regional Indian languages in current datasets, we meticulously curated IIITG-STLI2023, a comprehensive dataset encapsulating English alongside six under-represented Indian languages: Hindi, Kannada, Malayalam, Telugu, Bengali, and Manipuri. Rigorous evaluation of the IIITG-STLI2023, as well as the established MLe2e and SIW-13 datasets, underscores WAFFNet’s supremacy over both traditional feature-engineering approaches as well as state-of-the-art deep learning frameworks. Thus, the proposed WAFFNet framework offers a robust and effective solution for language identification in scene text images.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392131305",
    "type": "article"
  },
  {
    "title": "TransVAE-PAM: A Combined Transformer and DAG-based Approach for Enhanced Fake News Detection in Indian Context",
    "doi": "https://doi.org/10.1145/3651160",
    "publication_date": "2024-03-02",
    "publication_year": 2024,
    "authors": "Shivani Tufchi; Tanveer Ahmed; Ashima Yadav; Krishna Kant Agrawal; Ankit Vidyarthi",
    "corresponding_authors": "",
    "abstract": "In this study, we introduce a novel method, “TransVAE-PAM”, for the classification of fake news articles, tailored specifically for the Indian context. The approach capitalizes on state-of-the-art contextual and sentence transformer-based embedding models to generate article embeddings. Furthermore, we also try to address the issue of compact model size. In this respect, we employ a Variational Autoencoder (VAE) and β -VAE to reduce the dimensions of the embeddings, thereby yielding compact latent representations. To capture the thematic essence or important topics in the news articles, we use the Pachinko Allocation Model (PAM) model, a Directed Acyclic Graph (DAG) based approach, to generate meaningful topics. These two facets of representation - the reduced-dimension embeddings from the VAE and the extracted topics from the PAM model - are fused together to create a feature set. This representation is subsequently channeled into five different methods for fake news classification. Furthermore, we use eight distinct transformer-based architectures to test the embedding generation. To validate the feasibility of the proposed approach, we have conducted extensive experimentation on a proprietary dataset. The dataset is sourced from “Times of India” and other online media. Considering the size of the dataset, large-scale experiments are conducted on an NVIDIA supercomputer. Through this comprehensive numerical investigation, we have achieved an accuracy of 96.2% and an F1 score of 96% using the DistilBERT transformer architecture. By complementing the method via topic modeling, we record a performance improvement with the accuracy and F1 score both at 97%. These results indicate a promising direction toward leveraging the combination of advanced topic models into existing classification schemes to enhance research on fake news detection.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392347598",
    "type": "article"
  },
  {
    "title": "Am I Hurt?: Evaluating Psychological Pain Detection in Hindi Text Using Transformer-based Models",
    "doi": "https://doi.org/10.1145/3650206",
    "publication_date": "2024-03-05",
    "publication_year": 2024,
    "authors": "Ravleen Kaur; M. P. S. Bhatia; Akshi Kumar",
    "corresponding_authors": "",
    "abstract": "The automated evaluation of pain is critical for developing effective pain management approaches that seek to alleviate pain while preserving patients’ functioning. Transformer-based models can aid in detecting pain from Hindi text data gathered from social media by leveraging their ability to capture complex language patterns and contextual information. By understanding the nuances and context of Hindi text, transformer models can effectively identify linguistic cues and sentiments and expressions associated with pain, enabling the detection and analysis of pain-related content present in social media posts. The purpose of this research is to analyze the feasibility of utilizing NLP techniques to automatically identify pain within Hindi textual data, providing a valuable tool for pain assessment in Hindi-speaking populations. The research showcases the HindiPainNet model, a deep neural network that employs the IndicBERT model, classifying the dataset into two class labels {pain, no_pain} for detecting pain in Hindi textual data. The model is trained and tested using a novel dataset, दर्द-ए-शायरी (pronounced as Dard-e-Shayari ), curated using posts from social media platforms. The results demonstrate the model's effectiveness, achieving an accuracy of 70.5%. This pioneer research highlights the potential of utilizing textual data from diverse sources to identify and understand pain experiences based on psychosocial factors. This research could pave the path for the development of automated pain assessment tools that help medical professionals comprehend and treat pain in Hindi-speaking populations. Additionally, it opens avenues to conduct further NLP-based multilingual pain detection research, addressing the needs of diverse language communities.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392471831",
    "type": "article"
  },
  {
    "title": "DeepMedFeature: An Accurate Feature Extraction and Drug-Drug Interaction Model for Clinical Text in Medical Informatics",
    "doi": "https://doi.org/10.1145/3651159",
    "publication_date": "2024-03-09",
    "publication_year": 2024,
    "authors": "Manish Malik; S. Jawad; Syed Atif Moqurrab; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "Drug-drug interactions (DDIs) are an important biological phenomenon which can result in medical errors from medical practitioners. Drug interactions can change the molecular structure of interacting agents which may prove to be fatal in the worst case. Finding drug interactions early in diagnosis can be pivotal in side-effect prevention. The growth of big data provides a rich source of information for clinical studies to investigate DDIs. We propose a hierarchical classification model which is double-pass in nature. The first pass predicts the occurrence of an interaction and then the second pass further predicts the type of interaction such as effect, advice, mechanism, and int. We applied different deep learning algorithms with Convolutional Bi-LSTM (ConvBLSTM) proving to be the best. The results show that pre-trained vector embeddings prove to be the most appropriate features. The F1-score of the ConvBLSTM algorithm turned out to be 96.39% and 98.37% in Russian and English language respectively which is greater than the state-of-the-art systems. According to the results, it can be concluded that adding a convolution layer before the bi-directional pass improves model performance in the automatic classification and extraction of drug interactions, using pre-trained vector embeddings such as Fasttext and Bio-Bert.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392619616",
    "type": "article"
  },
  {
    "title": "Topic-aware Masked Attentive Network for Information Cascade Prediction",
    "doi": "https://doi.org/10.1145/3653449",
    "publication_date": "2024-03-21",
    "publication_year": 2024,
    "authors": "Yu Tai; Hongwei Yang; Hui He; Xinglong Wu; Yuanming Shao; Weizhe Zhang; Arun Kumar Sangaiah",
    "corresponding_authors": "",
    "abstract": "Predicting information cascades holds significant practical implications, including applications in public opinion analysis, rumor control, and product recommendation. Existing approaches have generally overlooked the significance of semantic topics in information cascades or disregarded the dissemination relations. Such models are inadequate in capturing the intricate diffusion process within an information network inundated with diverse topics. To address such problems, we propose a neural-based model using Topic-Aware Masked Attentive Network for Information Cascade Prediction (ICP-TMAN) to predict the next infected node of an information cascade. First, we encode the topical text into user representation to perceive the user-topic dependency. Next, we employ a masked attentive network to devise the diffusion context to capture the user-context dependency. Finally, we exploit a deep attention mechanism to model historical infected nodes for user embedding enhancement to capture user-history dependency. The results of extensive experiments conducted on three real-world datasets demonstrate the superiority of ICP-TMAN over existing state-of-the-art approaches.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393042722",
    "type": "article"
  },
  {
    "title": "Learn More Manchu Words with A New Visual-Language Framework",
    "doi": "https://doi.org/10.1145/3652992",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Zhiwei Wang; Siyang Lu; Xiang Wei; Run Su; Yingjun Qi; Wei Lu",
    "corresponding_authors": "",
    "abstract": "Manchu language, a minority language of China, is of significant historical and research value. An increasing number of Manchu documents are digitized into image format for better preservation and study. Recently, many researchers focused on identifying Manchu words in digitized documents. In previous approaches, a variety of Manchu words are recognized based on visual cues. However, we notice that visual-based approaches have some obvious drawbacks. On one hand, it is difficult to distinguish between similar and distorted letters. On the other hand, portions of letters obscured by breakage and stains are hard to identify. To cope with these two challenges, we propose a visual-language framework, namely the Visual-Language framework for Manchu word Recognition (VLMR), which fuses visual and semantic information to accurately recognize Manchu words. Whenever visual information is not available, the language model can automatically associate the semantics of words. The performance of our method is further enhanced by introducing a self-knowledge distillation network. In addition, we created a new handwritten Manchu word dataset named (HMW), which contains 6,721 handwritten Manchu words. The novel approach is evaluated on WMW and HMW. The experiments show that our proposed method achieves state-of-the-art performance on both datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393259411",
    "type": "article"
  },
  {
    "title": "Crossing Linguistic Barriers: Authorship Attribution in Sinhala Texts",
    "doi": "https://doi.org/10.1145/3655620",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Raheem Sarwar; Maneesha Perera; Pin Shen Teh; Raheel Nawaz; Muhammad Umair Hassan",
    "corresponding_authors": "",
    "abstract": "Authorship attribution involves determining the original author of an anonymous text from a pool of potential authors. The author attribution task has applications in several domains, such as plagiarism detection, digital text forensics, and information retrieval. While these applications extend beyond any single language, existing research has predominantly centered on English, posing challenges for application in languages such as Sinhala due to linguistic disparities and a lack of language processing tools. We present the first comprehensive study on cross-topic authorship attribution for Sinhala texts and propose a solution that can effectively perform the authorship attribution task even if the topics within the test and training samples differ. Our solution consists of three main parts: (i) extraction of topic-independent stylometric features, (ii) generation of a small candidate author set with the help of similarity search, and (iii) identification of the true author. Several experimental studies were carried out to demonstrate that the proposed solution can effectively handle real-world scenarios involving a large number of candidate authors and a limited number of text samples for each candidate author.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393342151",
    "type": "article"
  },
  {
    "title": "SUSTEM: An Improved Rule-based Sundanese Stemmer",
    "doi": "https://doi.org/10.1145/3656342",
    "publication_date": "2024-04-05",
    "publication_year": 2024,
    "authors": "Irwan Setiawan; Hung‐Yu Kao",
    "corresponding_authors": "",
    "abstract": "Current Sundanese stemmers either ignore reduplication words or define rules to handle only affixes. There is a significant amount of reduplication words in the Sundanese language. Because of that, it is impossible to achieve superior stemming precision in the Sundanese language without addressing reduplication words. This article presents an improved stemmer for the Sundanese language, which handles affixed and reduplicated words. With a Sundanese root word list, we use a rules-based stemming technique. In our approach, all stems produced by the affixes removal or normalization processes are added to the stem list. Using a stem list can help increase stemmer accuracy by reducing stemming errors caused by affix removal sequence errors or morphological issues. The current Sundanese language stemmer, RBSS, was used as a comparison. Two datasets with 8,218 unique affixed words and reduplication words were evaluated. The results show that our stemmer's strength and accuracy have improved noticeably. The use of stem list and word reduplication rules improved our stemmer's affixed type recognition and allowed us to achieve up to 99.30% accuracy.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393992782",
    "type": "article"
  },
  {
    "title": "Automatic Extractive Text Summarization using Multiple Linguistic Features",
    "doi": "https://doi.org/10.1145/3656471",
    "publication_date": "2024-04-08",
    "publication_year": 2024,
    "authors": "Pooja Gupta; Swati Nigam; Rajiv Singh",
    "corresponding_authors": "",
    "abstract": "Automatic text summarization (ATS) provides a summary of distinct categories of information using natural language processing (NLP). Low-resource languages like Hindi have restricted applications of these techniques. This study proposes a method for automatically generating summaries of Hindi documents using extractive technique. The approach retrieves pertinent sentences from the source documents by employing multiple linguistic features and machine learning (ML) using maximum likelihood estimation (MLE) and maximum entropy (ME). We conducted pre-processing on the input documents, such as eliminating Hindi stop words and stemming. We have obtained 15 linguistic feature scores from each document to identify the phrases with high scores for summary generation. We have performed experiments over BBC News articles, CNN News, DUC 2004, Hindi Text Short Summarization Corpus, Indian Language News Text Summarization Corpus, and Wikipedia Articles for the proposed text summarizer. The Hindi Text Short Summarization Corpus and Indian Language News Text Summarization Corpus datasets are in Hindi, whereas BBC News articles, CNN News, and the DUC 2004 datasets have been translated into Hindi using Google, Microsoft Bing, and Systran translators for experiments. The summarization results have been calculated and shown for Hindi as well as for English to compare the performance of a low and rich-resource language. Multiple ROUGE metrics, along with precision, recall, and F-measure, have been used for the evaluation, which shows the better performance of the proposed method with multiple ROUGE scores. We compare the proposed method with the supervised and unsupervised machine learning methodologies, including support vector machine (SVM), Naive Bayes (NB), decision tree (DT), latent semantic analysis (LSA), latent Dirichlet allocation (LDA), and K-means clustering, and it was found that the proposed method outperforms these methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394569811",
    "type": "article"
  },
  {
    "title": "A Hybrid Deep BiLSTM-CNN for Hate Speech Detection in Multi-social media",
    "doi": "https://doi.org/10.1145/3657635",
    "publication_date": "2024-05-06",
    "publication_year": 2024,
    "authors": "Ashwini Kumar; Santosh Kumar; Kalpdrum Passi; Aniket Mahanti",
    "corresponding_authors": "",
    "abstract": "Nowadays, means of communication among people have changed due to advancements in information technology and the rise of online multi-social media. Many people express their feelings, ideas, and emotions on social media sites such as Instagram, Twitter, Gab, Reddit, Facebook, and YouTube. However, people have misused social media to send hateful messages to specific individuals or groups to create chaos. For various governance authorities, manually identifying hate speech on various social media platforms is a difficult task to avoid such chaos. In this study, a hybrid deep-learning model, where bidirectional long short-term memory (BiLSTM) and convolutional neural network (CNN) are used to classify hate speech in textual data, is proposed. This model incorporates a GLOVE-based word embedding approach, dropout, L2 regularization, and global max pooling to get impressive results. Further, the proposed BiLSTM-CNN model has been evaluated on various datasets to achieve state-of-the-art performance that is superior to the traditional and existing machine learning methods in terms of accuracy, precision, recall, and F1-score.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396671072",
    "type": "article"
  },
  {
    "title": "Enhancing Chinese Event Extraction with Event Trigger Structures",
    "doi": "https://doi.org/10.1145/3663567",
    "publication_date": "2024-05-07",
    "publication_year": 2024,
    "authors": "Fei Li; Kaifang Deng; Yiwen Mo; Y.T. Ji; Chong Teng; Donghong Ji",
    "corresponding_authors": "",
    "abstract": "The dependency syntactic structure is widely used in event extraction. However, the dependency structure reflecting syntactic features is essentially different from the event structure that reflects semantic features, leading to the performance degradation. In this article, we propose to use Event Trigger Structure for Event Extraction (ETSEE), which can compensate the inconsistency between two structures. First, we leverage the ACE2005 dataset as case study, and annotate three kinds of ETSs, that is, “light verb + trigger”, “preposition structures” and “tense + trigger”. Then we design a graph-based event extraction model that jointly identifies triggers and arguments, where the graph consists of both the dependency structure and ETSs. Experiments show that our model significantly outperforms the state-of-the-art methods. Through empirical analysis and manual observation, we find that the ETSs can bring the following benefits: (1) enriching trigger identification features by introducing structural event information; (2) enriching dependency structures with event semantic information; (3) enhancing the interactions between triggers and candidate arguments by shortening their distances in the dependency graph.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396695509",
    "type": "article"
  },
  {
    "title": "KannadaLex: A lexical database with psycholinguistic information",
    "doi": "https://doi.org/10.1145/3670688",
    "publication_date": "2024-06-03",
    "publication_year": 2024,
    "authors": "Shreya R. Aithal; Muralikrishna Shamaiah Narayanarao; Raghavendra Ganiga; B. Ashwath Rao; Govardhan Hegde",
    "corresponding_authors": "",
    "abstract": "Databases containing lexical properties are of primary importance to psycholinguistic research and speech-language therapy. Several lexical databases for different languages have been developed in the recent past, but Kannada, a language spoken by 50.8 million people, has no comprehensive lexical database yet. To address this, KannadaLex , a Kannada lexical database, is built as a language resource that contains orthographic, phonological, and syllabic information about words that are sourced from newspaper articles from the past decade. Along with these vital statistics such as the phonological neighborhood, syllable complexity summed syllable and bigram syllable frequencies, and lemma and inflectional family information are stored. The database is validated by correlating frequency, a well-established psycholinguistic feature, with other numerical features. The developed lexical database contains 170K words from varied disciplines, complete with psycholinguistic features. This KannadaLex is a comprehensive resource for psycholinguists, speech therapists, and linguistic researchers for analyzing Kannada and other similar languages. Psycholinguists require lexical data for choosing stimuli to conduct experiments that study the factors that enable humans to acquire, use, comprehend, and produce language. Speech and language therapists query these databases for developing the most efficient stimuli for evaluating, diagnosing, and treating communication disorders, and rehabilitation of speech after brain injuries.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399291929",
    "type": "article"
  },
  {
    "title": "Automatic Algerian Sarcasm Detection from Texts and Images",
    "doi": "https://doi.org/10.1145/3670403",
    "publication_date": "2024-06-03",
    "publication_year": 2024,
    "authors": "Kheira Zineb Bousmaha; Khaoula Hamadouche; Hadjer Djouabi; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "In recent years, the number of Algerian Internet users has significantly increased, providing a valuable opportunity for collecting and utilizing opinions and sentiments expressed online. They now post not just texts but also images. However, to benefit from this wealth of information, it is crucial to address the challenge of sarcasm detection, which poses a limitation in sentiment analysis. Sarcasm often involves the use of nonliteral and ambiguous language, making its detection complex. To enhance the quality and relevance of sentiment analysis, it is essential to develop effective methods for sarcasm detection. By overcoming this limitation, we can fully harness the expressed online opinions and benefit from their valuable insights for a better understanding of trends and sentiments among the Algerian public. In this work, our aim is to develop a comprehensive system that addresses sarcasm detection in Algerian dialect, encompassing both text and image analysis. We propose a hybrid approach that combines linguistic characteristics and machine learning techniques for text analysis. Additionally, for image analysis, we utilized the deep learning model VGG-19 for image classification, and employed the EasyOCR technique for Arabic text extraction. By integrating these approaches, we strive to create a robust system capable of detecting sarcasm in both textual and visual content in the Algerian dialect. Our system achieved an accuracy of 92.79% for the textual models and 89.28% for the visual model.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399300649",
    "type": "article"
  },
  {
    "title": "Artificial Intelligence application for the analysis of personality traits and disorders in social media: A Survey",
    "doi": "https://doi.org/10.1145/3674971",
    "publication_date": "2024-06-25",
    "publication_year": 2024,
    "authors": "Mourad Ellouze; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "Personality analysis has a positive influence on humanity as it aids in identifying personality traits and disorders. In addition, it facilitates the monitoring of cases and enriches doctors’ knowledge bases, particularly in decision-making processes. This study includes a comprehensive literature review on personality analysis approaches from social media, aiming to gain a thorough understanding of the current studies on personality therapy. Moreover, the objective of this study is to identify various limitations present in these studies and explore potential avenues for enhancement. More specifically, this research begins with an introduction that discusses the main concepts of traits and personality disorders, as well as the importance of psychological analysis. Following that, four cluster studies related to personality analysis on social media are presented: personality traits, personality disorders, detection of links between diseases, and monitoring patient status. Then, the majority of the currently available works for each cluster are exposed. Afterward, a comparative study of the different presented works is proposed. Finally, an outline of plans for further research in this area is provided, detailing potential paths for exploration.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400017430",
    "type": "article"
  },
  {
    "title": "Leveraging Hybrid Adaptive Sine Cosine Algorithm with Deep Learning for Arabic Poem Meter Detection",
    "doi": "https://doi.org/10.1145/3676963",
    "publication_date": "2024-07-10",
    "publication_year": 2024,
    "authors": "Najla I. Al-shathry; Badria Al-onazi; Abdulkhaleq Q. A. Hassan; Shoayee Dlaim Alotaibi; Saud Alotaibi; Faiz Abdullah Alotaibi; Mohammed Elbes; Mrim M. Alnfiai",
    "corresponding_authors": "",
    "abstract": "Poetry is a significant aspect of any language. Many cultures and the history of nations are recognized in poems. Compared to prose, each poem has a rhythmic structure that is quite different. The language has its set of lyrical structures for poems, known as meters. Detecting the meters of Arabic poems is a complicated and lengthy procedure. The text must be encrypted using the Arudi method to classify the poem's meter, which requires complex rule-based transformation before another set of rules classifies the meters. Applying deep learning (DL) to meter classification in Arabic poems includes constructing a neural network to discern rhythmic patterns inherent in various meters. The model can extract essential features, like word lengths or syllable patterns, by tokenizing and preprocessing text datasets. Architectures such as Long Short-Term Memory Networks (LSTM) or Recurrent Neural Networks (RNNs) are fitting solutions to capture temporal relations in poetic verses. This research introduces a Hybrid Meta-heuristics with Deep Learning for the Arabic Poem Meter Detection and Classification (HMDL-APMDC) model. The main intention of the HMDL-APMDC system is to recognize various kinds of meters in Arabic poems. The HMDL-APMDC technique primarily preprocesses the input dataset to make it compatible with the classification process. Besides, the HMDL-APMDC technique applies Convolution and Attention with a Bi-directional Gated Recurrent Unit (CAT-BiGRU) for the automated recognition of meter classes. Furthermore, the adaptive sine-s-cosine particle swarm optimization (ASCA-PSO) algorithm is applied to optimize the hyperparameter tuning of the CAT-BiGRU model, enhancing the meter detection results. A detailed simulation analysis is made to highlight the improved performance of the HMDL-APMDC technique. The empirical outcomes stated that the HMDL-APMDC technique had a superior outcome of 98.53% over recent models under the MetRec dataset.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400498617",
    "type": "article"
  },
  {
    "title": "Artificial Intelligence inspired method for cross-lingual cyberhate detection from low resource languages",
    "doi": "https://doi.org/10.1145/3677176",
    "publication_date": "2024-07-11",
    "publication_year": 2024,
    "authors": "Manpreet Kaur; Munish Saini",
    "corresponding_authors": "",
    "abstract": "The appearance of inflammatory language on social media by college or university students is quite prevalent, inspiring platforms to engage in community safety mechanisms. Escalating hate speech entails creating sophisticated artificial intelligence-based, machine learning, and deep learning algorithms to detect offensive internet content. With a few noteworthy exceptions, the majority of the studies on automatic hate speech recognition have emphasized high-resource languages, mainly English. We bridge this gap by addressing hate speech detection in Punjabi (Gurmukhi), a low-resource Indo-Aryan language articulated in Indian educational institutions. This research identifies cross-lingual hate speech in the code-switched English-Punjabi language used on social media. It proposes an approach combining the best hate speech detection techniques to cover existing state-of-the-art system gaps and limitations. In this method, the Roman Punjabi is transliterated, and then Bidirectional Encoder Representations from Transformer (BERT) based models are employed for hate detection. The proposed model has achieved 0.86 precision and 0.83 recall, and various higher educational institutions could employ it to discover the issues/domains where hate prevails the most.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400551127",
    "type": "article"
  },
  {
    "title": "An Ensemble Neural Network Model For Malayalam Character Recognition From Palm Leaf Manuscripts",
    "doi": "https://doi.org/10.1145/3686311",
    "publication_date": "2024-08-13",
    "publication_year": 2024,
    "authors": "Dhanya Sudarsan; Deepa Sankar",
    "corresponding_authors": "",
    "abstract": "Palm leaf manuscripts (PLMs), crucial for ancient communication hold a wealth of information encompassing culture, art, literature, religion, and medicinal wisdom. Malayalam, Kerala's official language, significantly contributes to medical sciences, making palm scripts invaluable, especially in times of pandemics. This study introduces a ground-breaking model for automatic recognition of characters in Malayalam palm scripts. This is the first significant deep learning-based attempt, to our knowledge, to automate Malayalam character recognition in PLMs. The developed model is a fusion of fine-tuned Convolutional Neural Network (CNN) and Bi-directional Long Short-Term Memory (BiLSTM). Discriminative features were extracted from each character in the manuscript through multiple convolutional layers, and these feature vectors were then classified into their respective character classes using an ensemble deep learning model. The performance of the proposed method was evaluated using a self-generated dataset of old Malayalam PLMs from the period 1800 to 1908 AD. Overcoming challenges such as complex morphology, large character set, similar characters, and a unique writing style, the model achieved an impressive accuracy of 96.40%, outperforming state-of-the-art systems. Notably, the model obtained a negative predictive value (NPV) of 99.3%, positive predictive value (PPV) of 83.33%, sensitivity of 79.55%, specificity of 99.45% and F-Measure of 88.39%.Thus this advancement marks a significant milestone in automatic transcriptions providing a crucial tool for doctors and researchers.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401546102",
    "type": "article"
  },
  {
    "title": "Transformer-Based Topic Modeling for Urdu Translations of the Holy Quran",
    "doi": "https://doi.org/10.1145/3694967",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Amna Zafar; Muhammad Wasim; Shaista Zulfiqar; Talha Waheed; AbuBakar Siddique",
    "corresponding_authors": "",
    "abstract": "Topic modeling enables the discovery of concealed themes and patterns in extensive text collections. It facilitates a thorough examination of the messages present in religious texts. Topic modeling for Quranic verses is a trending study area, with various translations already explored including Bahasa, English, and Arabic. Yet, there is a need for further research, particularly in Urdu translations of the Quran. In this study, we propose applying the BERTopic framework to Urdu translations of The Holy Quran. By leveraging the BERTopic approach, which incorporates a fine-tuned BERT model, we aim to capture the contextual nuances and linguistic complexities unique to the Quran. In this study, we utilized existing Urdu translations of the Quran from eight different translators sourced from Tanzil, a renowned resource for Quranic text and translations. We assessed the performance of our proposed BERTopic model compared to traditional techniques like LDA and NMF, using coherence and diversity metrics. The results indicate that our BERT-based approach outperforms these conventional methods, achieving an average coherence improvement of 0.03 and a diversity score of 0.83. These findings highlight the effectiveness of BERTopic in extracting meaningful topics from Urdu translations of The Holy Quran and contribute to the computational analysis of religious texts, supporting scholarly endeavours in comparative studies of Quranic translations in Urdu.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402369848",
    "type": "article"
  },
  {
    "title": "HindiSumm: A Hindi Abstractive Summarization benchmark dataset",
    "doi": "https://doi.org/10.1145/3696207",
    "publication_date": "2024-09-17",
    "publication_year": 2024,
    "authors": "Geetanjali Singh; Namita Mittal; Satyendra Singh Chouhan",
    "corresponding_authors": "",
    "abstract": "Abstractive Text Summarization (ATS) is a task to create a novel summary by generating fresh sentences incorporating new words or rephrasing the article. It is a complex task as the model needs to understand the semantic similarity between the sentences of the text. To fulfill this, there is a need for a large annotated benchmark dataset, which is available for resource-rich languages such as English and non-indic languages. In contrast, for the less-resourced languages, such as Indic languages, the available datasets are limited and involve very short summary sentences. Hence, a language-specific abstractive summarization dataset called HindiSumm was introduced for Hindi, consisting of 570,000 text-summary pairs from Navbharat Times across 21 domains. The HindiSumm dataset’s efficiency is evaluated extrinsically and intrinsically by using various metrics. Furthermore, two recent multilingual-cased pre-trained models are fine-tuned on the HindiSumm dataset individually. In addition, an ensembled approach using weighted averaging is also incorporated to check the efficacy of the proposed dataset. The model is tested with the in-house created dataset, and results are evaluated on ROUGE scores and show significant improvements of around 13.2% for the proposed HindiSumm compared to other benchmark datasets. In the future, the HindiSumm dataset will promote the progress of ATS for the Indian language.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402557888",
    "type": "article"
  },
  {
    "title": "Context-Aware Adversarial Graph-Based Learning for Multilingual Grammatical Error Correction",
    "doi": "https://doi.org/10.1145/3696106",
    "publication_date": "2024-10-04",
    "publication_year": 2024,
    "authors": "Naresh Kumar; Parveen Kumar; Sushreeta Tripathy; Neelamani Samal; Debasis Gountia; Praveen Gatla; Teekam Singh",
    "corresponding_authors": "",
    "abstract": "Correcting grammatical errors in various language contexts is a crucial and challenging task in the field of natural language processing, commonly referred to as Multilingual Grammatical Error Correction. This paper elaborates the Adversarial Temporal Graph Convolution Model (AT-GCM), which combines the capabilities of MT-5, adversarial learning, and temporal graph convolutional neural network (t-GCN) to achieve accurate progress in multilingual grammatical error correction. The inherent capability of MT-5 to process multiple languages simultaneously serves as a powerful embedding generator for the purpose of multilingual error correction. The t-GCN is employed for the purpose of navigating the temporal context and interdependencies present within words. The assumption that modeling the dynamic interactions among words within the context of temporal relationships improves precision, particularly in languages with complex sentence structures, is supported by research. The utilization of adversarial learning techniques can enhance the generalization capabilities of the model across various language pairings, effectively addressing the challenges associated with low-resource languages. A comprehensive analysis is carried out on a diverse, multilingual dataset comprising various languages, viz. English, Russian, German, Czech, Arabic, and Romanian. The experimental results present significant improvements in grammatical error correction performance compared to state-of-the-art models. Our approach effectively resolves grammatical errors in various linguistic contexts by utilizing a combination of MT-5, adversarial learning, and t-GCN.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403137355",
    "type": "article"
  },
  {
    "title": "A Basic Language Resource Kit Implementation for the Igbo <i>NLP</i> Project",
    "doi": "https://doi.org/10.1145/3146387",
    "publication_date": "2018-01-11",
    "publication_year": 2018,
    "authors": "Ikechukwu Onyenwe; Mark Hepple; Chinedu Uchechukwu; Ignatius Ezeani",
    "corresponding_authors": "",
    "abstract": "Igbo, an African language with around 32 million speakers worldwide, is one of the many languages having few or none of the language processing resources needed for advanced language technology applications. In this article, we describe the approach taken to creating an initial set of resources for Igbo, including an electronic text corpus, a part-of-speech (POS) tagset, and a POS-tagged subcorpus. We discuss the approach taken in gathering texts, the preprocessing of these texts, and the development of the POS tagged corpus. We also discuss some of the problems encountered during corpus and tagset development and the solutions arrived at for these problems.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2783331529",
    "type": "article"
  },
  {
    "title": "Improved Discourse Parsing with Two-Step Neural Transition-Based Model",
    "doi": "https://doi.org/10.1145/3152537",
    "publication_date": "2018-01-11",
    "publication_year": 2018,
    "authors": "Yanyan Jia; Yansong Feng; Ye Yuan; Chao Lv; Chongde Shi; Dongyan Zhao",
    "corresponding_authors": "",
    "abstract": "Discourse parsing aims to identify structures and relationships between different discourse units. Most existing approaches analyze a whole discourse at once, which often fails in distinguishing long-span relations and properly representing discourse units. In this article, we propose a novel parsing model to analyze discourse in a two-step fashion with different feature representations to characterize intra sentence and inter sentence discourse structures, respectively. Our model works in a transition-based framework and benefits from a stack long short-term memory neural network model. Experiments on benchmark tree banks show that our method outperforms traditional 1-step parsing methods in both English and Chinese.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2783661857",
    "type": "article"
  },
  {
    "title": "“UTTAM”",
    "doi": "https://doi.org/10.1145/3264620",
    "publication_date": "2018-11-19",
    "publication_year": 2018,
    "authors": "Amita Jain; Minni Jain; Goonjan Jain; Devendra K. Tayal",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a system called “UTTAM,” for correcting spelling errors in Hindi language text using supervised learning. Unlike other languages, Hindi contains a large set of characters, words with inflections and complex characters, phonetically similar sets of characters, and so on. The complexity increases the possibility of confusion and occasionally leads to entering a wrong character in a word. The existence of spelling errors in text significantly decreases the accuracy of the available resources, like search engine, text editor, and so on. The proposed work is the first approach to correct non-word (Out of Vocabulary) errors as well as real-word errors simultaneously in a sentence of Hindi language. The proposed method investigates the human behavior, i.e., the type and frequency of spelling errors done by humans in Hindi text. Based on the type and frequency of spelling errors, the heterogeneous data is collected in matrices. This data in matrices is used to generate the suitable candidate words for an input word. After generating candidate words, the Viterbi algorithm is applied to perform the word correction. The Viterbi algorithm finds the best sequence of candidate words to correct the input sentence. For Hindi, this work is the first attempt for real-word error correction. For non-word errors, the experiments show that “UTTAM” performs better than the existing systems SpellGuru and Saksham.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2901142819",
    "type": "article"
  },
  {
    "title": "Phrase Table Induction Using Monolingual Data for Low-Resource Statistical Machine Translation",
    "doi": "https://doi.org/10.1145/3168054",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Benjamin Marie; Atsushi Fujita",
    "corresponding_authors": "",
    "abstract": "We propose a new method for inducing a phrase-based translation model from a pair of unrelated monolingual corpora. Our method is able to deal with phrases of arbitrary length and to find phrase pairs that are useful for statistical machine translation, without requiring large parallel or comparable corpora. First, our method generates phrase pairs through coupling source and target phrases separately collected from respective monolingual data. Then, for each phrase pair, we compute features using the monolingual data and a small quantity of parallel sentences. Finally, incorrect phrase pairs are pruned, and a phrase table is made using the remaining phrase pairs. In our experiments on French--Japanese and Spanish--Japanese translation tasks under low-resource conditions, we observe that incorporating a phrase table induced by our method to the machine translation system leads to large improvements in translation quality. Furthermore, we show that a phrase table induced by our method can also be useful in a wide range of configurations, including configurations where we have already access to large parallel corpora and configurations where only small monolingual corpora are available.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2785381227",
    "type": "article"
  },
  {
    "title": "Arabic Speech Act Recognition Techniques",
    "doi": "https://doi.org/10.1145/3170576",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Lina Sherkawi; Nada Ghneim; Oumayma Al Dakkak",
    "corresponding_authors": "",
    "abstract": "This article presents rule-based and statistical-based techniques for Arabic speech act recognition. The proposed techniques classify an utterance into Arabic speech act categories based on three criteria: surface features, cue words, and contextual information. A rule-based expert system has been developed in a bootstrapping manner based on the fact that Arabic language syntax is inherently rule-based. Various machine-learning algorithms have been used to detect Arabic speech act categories: Decision Tree, Naïve Bayes, Neural Network, and SVM. We compare the experimental results for both techniques (machine-learning and rule-based expert systems). Using a corpus of 1,500 sentences, the rule-based expert system achieved an accuracy rate of 98.92%, while the Decision Tree, Naïve Bayes, Neural Network, and SVM achieved an accuracy rate of 97.09%, 96.48%, 93.50%, and 93.70%, respectively.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2787886831",
    "type": "article"
  },
  {
    "title": "Morphological Segmentation and Part-of-Speech Tagging for the Arabic Heritage",
    "doi": "https://doi.org/10.1145/3178459",
    "publication_date": "2018-04-02",
    "publication_year": 2018,
    "authors": "Emad Mohamed",
    "corresponding_authors": "Emad Mohamed",
    "abstract": "We annotate 60,000 words of Classical Arabic (CA) with topics in philosophy, religion, literature, and law with fine-grain segment-based morphological descriptions. We use these annotations for building a morphological segmenter and part-of-speech (POS) tagger for CA. With character-level classification and features from the word and its lexical context, the segmenter achieves a word accuracy of 96.8% with the main issue being a high rate of out-of-vocabulary words. A token-based POS tagger achieves an accuracy of 96.22% with 97.72% on known tokens despite the small size of the corpus. An error analysis shows that most of the tagging errors are results of segmentation and that quality improves with more data being added. The morphological segmenter and tagger have a wide range of potential applications in processing CA, a low-resource variety of the language.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2795367096",
    "type": "article"
  },
  {
    "title": "Domain-specific Named Entity Recognition with Document-Level Optimization",
    "doi": "https://doi.org/10.1145/3213544",
    "publication_date": "2018-07-27",
    "publication_year": 2018,
    "authors": "Limin Wang; Shoushan Li; Qian Yan; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Previous studies normally formulate named entity recognition (NER) as a sequence labeling task and optimize the solution in the sentence level. In this article, we propose a document-level optimization approach to NER and apply it in a domain-specific document-level NER task. As a baseline, we apply a state-of-the-art approach, i.e., long-short-term memory (LSTM), to perform word classification. On this basis, we define a global objective function with the obtained word classification results and achieve global optimization via Integer Linear Programming (ILP). Specifically, in the ILP-based approach, we propose four kinds of constraints, i.e., label transition, entity length, label consistency, and domain-specific regulation constraints, to incorporate various entity recognition knowledge in the document level. Empirical studies demonstrate the effectiveness of the proposed approach to domain-specific document-level NER.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2883943646",
    "type": "article"
  },
  {
    "title": "Comparison of Methods to Annotate Named Entity Corpora",
    "doi": "https://doi.org/10.1145/3218820",
    "publication_date": "2018-07-21",
    "publication_year": 2018,
    "authors": "Kanako Komiya; Masaya Suzuki; Tomoya Iwakura; Minoru Sasaki; Hiroyuki Shinnou",
    "corresponding_authors": "",
    "abstract": "The authors compared two methods for annotating a corpus for the named entity (NE) recognition task using non-expert annotators: (i) revising the results of an existing NE recognizer and (ii) manually annotating the NEs completely. The annotation time, degree of agreement, and performance were evaluated based on the gold standard. Because there were two annotators for one text for each method, two performances were evaluated: the average performance of both annotators and the performance when at least one annotator is correct. The experiments reveal that semi-automatic annotation is faster, achieves better agreement, and performs better on average. However, they also indicate that sometimes, fully manual annotation should be used for some texts whose document types are substantially different from the training data document types. In addition, the machine learning experiments using semi-automatic and fully manually annotated corpora as training data indicate that the F-measures could be better for some texts when manual instead of semi-automatic annotation was used. Finally, experiments using the annotated corpora for training as additional corpora show that (i) the NE recognition performance does not always correspond to the performance of the NE tag annotation and (ii) the system trained with the manually annotated corpus outperforms the system trained with the semi-automatically annotated corpus with respect to newswires, even though the existing NE recognizer was mainly trained with newswires.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2884659665",
    "type": "article"
  },
  {
    "title": "Incorporating Multi-Level User Preference into Document-Level Sentiment Classification",
    "doi": "https://doi.org/10.1145/3234512",
    "publication_date": "2018-11-19",
    "publication_year": 2018,
    "authors": "Junjie Li; Haoran Li; Xiaomian Kang; Haitong Yang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Document-level sentiment classification aims to predict a user’s sentiment polarity in a document about a product. Most existing methods only focus on review contents and ignore users who post reviews. In fact, when reviewing a product, different users have different word-using habits to express opinions (i.e., word-level user preference), care about different attributes of the product (i.e., aspect-level user preference), and have different characteristics to score the review (i.e., polarity-level user preference). These preferences have great influence on interpreting the sentiment of text. To address this issue, we propose a model called Hierarchical User Attention Network (HUAN), which incorporates multi-level user preference into a hierarchical neural network to perform document-level sentiment classification. Specifically, HUAN encodes different kinds of information (word, sentence, aspect, and document) in a hierarchical structure and imports user embedding and user attention mechanism to model these preferences. Empirical results on two real-world datasets show that HUAN achieves state-of-the-art performance. Furthermore, HUAN can also mine important attributes of products for different users.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2901324389",
    "type": "article"
  },
  {
    "title": "Sub-Stroke-Wise Relative Feature for Online Indic Handwriting Recognition",
    "doi": "https://doi.org/10.1145/3264735",
    "publication_date": "2018-12-17",
    "publication_year": 2018,
    "authors": "Nilanjana Bhattacharya; Partha Pratim Roy; Umapada Pal",
    "corresponding_authors": "",
    "abstract": "The main problem of Bangla (Bengali) and Devanagari handwriting recognition is the shape similarity of characters. There are only a few pieces of work on writer-independent cursive online Indian text recognition, and the shape similarity problem needs more attention from the researchers. To handle the shape similarity problem of cursive characters of Bangla and Devanagari scripts, in this article, we propose a new category of features called ‘ sub-stroke-wise relative feature ’ (SRF) which are based on relative information of the constituent parts of the handwritten strokes. Relative information among some of the parts within a character can be a distinctive feature as it scales up small dissimilarities and enhances discrimination among similar-looking shapes. Also, contextual anticipatory phenomena are automatically modeled by this type of feature, as it takes into account the influence of previous and forthcoming strokes. We have tested popular state-of-the-art feature sets as well as proposed SRF using various (up to 20,000-word) lexicons and noticed that SRF significantly outperforms the state-of-the-art feature sets for online Bangla and Devanagari cursive word recognition.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2904240394",
    "type": "article"
  },
  {
    "title": "Experience-based Causality Learning for Intelligent Agents",
    "doi": "https://doi.org/10.1145/3314943",
    "publication_date": "2019-05-21",
    "publication_year": 2019,
    "authors": "Yang Liu; Shaonan Wang; Jiajun Zhang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Understanding causality in text is crucial for intelligent agents. In this article, inspired by human causality learning, we propose an experience-based causality learning framework. Comparing to traditional approaches, which attempt to handle the causality problem relying on textual clues and linguistic resources, we are the first to use experience information for causality learning. Specifically, we first construct various scenarios for intelligent agents, thus, the agents can gain experience from interaction in these scenarios. Then, human participants build a number of training instances for agents of causality learning based on these scenarios. Each instance contains two sentences and a label. Each sentence describes an event that an agent experienced in a scenario, and the label indicates whether the sentence (event) pair has a causal relation. Accordingly, we propose a model that can infer the causality in text using experience by accessing the corresponding event information based on the input sentence pair. Experiment results show that our method can achieve impressive performance on the grounded causality corpus and significantly outperform the conventional approaches. Our work suggests that experience is very important for intelligent agents to understand causality.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2946260552",
    "type": "article"
  },
  {
    "title": "Toward an Effective Igbo Part-of-Speech Tagger",
    "doi": "https://doi.org/10.1145/3314942",
    "publication_date": "2019-05-21",
    "publication_year": 2019,
    "authors": "Ikechukwu Onyenwe; Mark Hepple; Chinedu Uchechukwu; Ignatius Ezeani",
    "corresponding_authors": "",
    "abstract": "Part-of-speech (POS) tagging is a well-established technology for most Western European languages and a few other world languages, but it has not been evaluated on Igbo, an agglutinative African language. This article presents POS tagging experiments conducted using an Igbo corpus as a test bed for identifying the POS taggers and the Machine Learning (ML) methods that can achieve a good performance with the small dataset available for the language. Experiments have been conducted using different well-known POS taggers developed for English or European languages, and different training data styles and sizes. Igbo has a number of language-specific characteristics that present a challenge for effective POS tagging. One interesting case is the wide use of verbs (and nominalizations thereof) that have an inherent noun complement , which form “linked pairs” in the POS tagging scheme, but which may appear discontinuously. Another issue is Igbo’s highly productive agglutinative morphology, which can produce many variant word forms from a given root. This productivity is a key cause of the out-of-vocabulary (OOV) words observed during Igbo tagging. We report results of experiments on a promising direction for improving tagging performance on such morphologically-inflected OOV words.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2946730224",
    "type": "article"
  },
  {
    "title": "Machine Translation Evaluation Metric Based on Dependency Parsing Model",
    "doi": "https://doi.org/10.1145/3312573",
    "publication_date": "2019-06-10",
    "publication_year": 2019,
    "authors": "Hui Yu; Weizhi Xu; Shouxun Lin; Qun Liu",
    "corresponding_authors": "",
    "abstract": "Most of the syntax-based metrics obtain the similarity by comparing the sub-structures extracted from the trees of hypothesis and reference. These sub-structures cannot represent all the information in the trees because their lengths are limited. To sufficiently use the reference syntax information, a new automatic evaluation metric is proposed based on the dependency parsing model. First, a dependency parsing model is trained using the reference dependency tree for each sentence. Then, the hypothesis is parsed by this dependency parsing model and the corresponding hypothesis dependency tree is generated. The quality of hypothesis can be judged by the quality of the hypothesis dependency tree. Unigram F-score is included in the new metric so that lexicon similarity is obtained. According to experimental results, the proposed metric can perform better than METEOR and BLEU on system level and get comparable results with METEOR on sentence level. To further improve the performance, we also propose a combined metric which gets the best performance on the sentence level and on the system level.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2949145155",
    "type": "article"
  },
  {
    "title": "Personalized Query Auto-Completion for Large-Scale POI Search at Baidu Maps",
    "doi": "https://doi.org/10.1145/3394137",
    "publication_date": "2020-06-18",
    "publication_year": 2020,
    "authors": "Ying Li; Jizhou Huang; Miao Fan; Jinyi Lei; Haifeng Wang; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Query auto-completion (QAC) is a featured function that has been widely adopted by many sub-domains of search. It can dramatically reduce the number of typed characters and avoid spelling mistakes. These merits of QAC are highlighted to improve user satisfaction, especially when users intend to type in a query on mobile devices. In this article, we will present our industrial solution to the personalized QAC for the point of interest (POI) search at Baidu Maps, a well-known Web mapping service on mobiles in China. The industrial solution makes a good tradeoff between the offline effectiveness of a novel neural learning model that we devised for feature generation and the online efficiency of an off-the-shelf learning to rank (LTR) approach for the real-time suggestion. Besides some practical lessons from how a real-world QAC system is built and deployed in Baidu Maps to facilitate a large number of users in searching tens of millions of POIs, we mainly explore two specific features for the personalized QAC function of the POI search engine: the spatial-temporal characteristics of POIs and the historically queried POIs of individual users . We leverage the large-volume POI search logs in Baidu Maps to conduct offline evaluations of our personalized QAC model measured by multiple metrics, including Mean Reciprocal Rank (MRR), Success Rate (SR), and normalized Discounted Cumulative Gain (nDCG). Extensive experimental results demonstrate that the personalized model enhanced by the proposed features can achieve substantial improvements (i.e., +3.29% MRR, +3.78% SR@1, +5.17% SR@3, +1.96% SR@5, and +3.62% nDCG@5). After deploying this upgraded model into the POI search engine at Baidu Maps for A/B testing online, we observe that some other critical indicators, such as the average number of keystrokes and the average typing speed at keystrokes in a QAC session, which are also related to user satisfaction, decrease as well by 1.37% and 1.69%, respectively. So the conclusion is that the two kinds of features contributed by us are quite helpful in personalized mapping services for industrial practice.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3036788568",
    "type": "article"
  },
  {
    "title": "A Technique to Calculate National Happiness Index by Analyzing Roman Urdu Messages Posted on Social Media",
    "doi": "https://doi.org/10.1145/3400712",
    "publication_date": "2020-10-26",
    "publication_year": 2020,
    "authors": "Rabia Habiba; Muhammad Awais; Muhammad Shoaib",
    "corresponding_authors": "",
    "abstract": "National Happiness Index (NHI) is a national indicator of development that estimates the economic and social well-being of the nation's individuals. With the proliferation of the internet, people share a significant amount of data on social media websites. We can process the data with different sentiment analysis techniques to calculate the NHI. In the literature, different approaches have been used to calculate NHI, which include the lexicon-based approach and machine learning approach. All of these existing approaches are proposed to calculate NHI for the sentiments written in the English language. However, these methods fail for complex Roman Urdu tweets that contain more than two sub-opinions. There are three primary objectives of the research: (1) to investigate current sentiment analysis techniques are sufficient for the classification of complex Roman Urdu sentiments; (2) to propose rule-based classifier for the classification of Roman Urdu sentiments comprising multiple sub-opinions; (3) to calculate NHI using Roman Urdu sentiments. For this purpose, we proposed the discourse information extractor, the rule-based method (3-RBC), and the machine learning classifier. The experimental results show that 3-RBC is efficient for feature identification, and it is more statistically significant than the baseline classifiers. The 3-RBC has successfully increased the accuracy by 7% and precision by 8%, which provides evidence that the proposed technique significantly increased the calculation of NHI.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3094910050",
    "type": "article"
  },
  {
    "title": "Disambiguating Arabic Words According to Their Historical Appearance in the Document Based on Recurrent Neural Networks",
    "doi": "https://doi.org/10.1145/3410569",
    "publication_date": "2020-10-15",
    "publication_year": 2020,
    "authors": "Rim Laatar; Chafik Aloulou; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "How can we determine the semantic meaning of a word in relation to its context of appearance? We eventually have to grabble with this difficult question, as one of the paramount problems of Natural Language Processing (NLP). In other words, this issue is commonly defined as Word Sense Disambiguation (WSD). The latter is one of the crucial difficulties within the NLP field. In this respect, word vectors extracted from a neural network model have been successfully applied for resolving the WSD problem. Accordingly, this article presents an unprecedented method to disambiguate Arabic words according to both their contextual appearance in a source text and the era in which they emerged. In fact, in the few previous decades, many researchers have been grabbling with Arabic Word Sense Disambiguation. It should be noted that the Arabic language can be divided into three major historical periods: old Arabic, middle-age Arabic, and contemporary Arabic. Actually, contemporary Arabic has proved to be the greatest concern of many researchers. The main gist of our work is to disambiguate Arabic words according to the historical period in which they appeared. To perform such a task, we suggest a method that deploys contextualized word embeddings to better gather valid syntactic and semantic information of the same word by taking into account its contextual uses. The preponderant thing is to convert both the senses and the contextual uses of an ambiguous item to vectors, then determine which of the possible conceptual meanings of the target word is closer to the given context.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3096939275",
    "type": "article"
  },
  {
    "title": "A Novel Resource Optimization Algorithm Based on Clustering and Improved Differential Evolution Strategy Under a Cloud Environment",
    "doi": "https://doi.org/10.1145/3462761",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Zhou Zhou; Fangmin Li; Shuiqiao Yang",
    "corresponding_authors": "",
    "abstract": "Resource optimization algorithm based on clustering and improved differential evolution strategy, as a new global optimized algorithm, has wide applications in language translation, language processing, document understanding, cloud computing, and edge computing due to high efficiency. With the development of deep learning technology and the rise of big data, the resource optimization algorithm encounters a series of challenges, such as the workload imbalance and low resource utilization. To address the preceding problems, this study proposes a novel resource optimization algorithm based on clustering and an improved differential evolution strategy (Multi-objective Task Scheduling Strategy (MTSS)). Three indexes, namely task completion time, execution cost, and workload, of virtual machines are selected and used to build the fitness function of the MTSS algorithm. At the same time, the preprocessing state is set up to cluster according to the resource and task characteristics to reduce the magnitude of their matching scale. Moreover, to solve the workload imbalance among different resource sets, local resource tasks are reallocated using the Q-value method in the MTSS strategy to achieve workload balance of global resources and improve the resource utilization rate. Experiments are carried out to evaluate the effectiveness of the proposed algorithm. Results show that the proposed algorithm outperforms other algorithms in terms of task completion time, execution cost, and workload balancing.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3175096681",
    "type": "article"
  },
  {
    "title": "Research On Pre-Training Method and Generalization Ability of Big Data Recognition Model of the Internet of Things",
    "doi": "https://doi.org/10.1145/3433539",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Junyang Tan; Dan Xia; Shiyun Dong; Honghao Zhu; Binshi Xu",
    "corresponding_authors": "",
    "abstract": "The Internet of Things and big data are currently hot concepts and research fields. The mining, classification, and recognition of big data in the Internet of Things system are the key links that are widely of concern at present. The artificial neural network is beneficial for multi-dimensional data classification and recognition because of its strong feature extraction and self-learning ability. Pre-training is an effective method to address the gradient diffusion problem in deep neural networks and could result in better generalization. This article focuses on the performance of supervised pre-training that uses labelled data. In particular, this pre-training procedure is a simulation that shows the changes in judgment patterns as they progress from primary to mature within the human brain. In this article, the state-of-the-art of neural network pre-training is reviewed. Then, the principles of the auto-encoder and supervised pre-training are introduced in detail. Furthermore, an extended structure of supervised pre-training is proposed. A set of experiments are carried out to compare the performances of different pre-training methods. These experiments include a comparison between the original and pre-trained networks as well as a comparison between the networks with two types of sub-network structures. In addition, a homemade database is established to analyze the influence of pre-training on the generalization ability of neural networks. Finally, an ordinary convolutional neural network is used to verify the applicability of supervised pre-training.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3186501075",
    "type": "article"
  },
  {
    "title": "Towards Tokenization and Part-of-Speech Tagging for Khmer: Data and Discussion",
    "doi": "https://doi.org/10.1145/3464378",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Hour Kaing; Chenchen Ding; Masao Utiyama; Eiichiro Sumita; Sethserey Sam; Sopheap Seng; Katsuhito Sudoh; Satoshi Nakamura",
    "corresponding_authors": "",
    "abstract": "As a highly analytic language, Khmer has considerable ambiguities in tokenization and part-of-speech (POS) tagging processing. This topic is investigated in this study. Specifically, a 20,000-sentence Khmer corpus with manual tokenization and POS-tagging annotation is released after a series of work over the last 4 years. This is the largest morphologically annotated Khmer dataset as of 2020, when this article was prepared. Based on the annotated data, experiments were conducted to establish a comprehensive benchmark on the automatic processing of tokenization and POS-tagging for Khmer. Specifically, a support vector machine, a conditional random field (CRF) , a long short-term memory (LSTM) -based recurrent neural network, and an integrated LSTM-CRF model have been investigated and discussed. As a primary conclusion, processing at morpheme-level is satisfactory for the provided data. However, it is intrinsically difficult to identify further grammatical constituents of compounds or phrases because of the complex analytic features of the language. Syntactic annotation and automatic parsing for Khmer will be scheduled in the near future.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3199380101",
    "type": "article"
  },
  {
    "title": "Legal Judgment Elements Extraction Approach with Law Article-aware Mechanism",
    "doi": "https://doi.org/10.1145/3485244",
    "publication_date": "2021-12-21",
    "publication_year": 2021,
    "authors": "Zhang Hu; Bangze Pan; Ru Li",
    "corresponding_authors": "",
    "abstract": "Legal judgment elements extraction (LJEE) aims to identify the different judgment features from the fact description in legal documents automatically, which helps to improve the accuracy and interpretability of the judgment results. In real court rulings, judges usually need to scan both the fact descriptions and the law articles repeatedly to find out the relevant information, and it is hard to acquire the key judgment features quickly, so legal judgment elements extraction is a crucial and challenging task for legal judgment prediction. However, most existing methods follow the text classification framework, which fails to model the attentive relations of the law articles and the legal judgment elements. To address this issue, we simulate the working process of human judges, and propose a legal judgment elements extraction method with a law article-aware mechanism, which captures the complex semantic correlations of the law article and the legal judgment elements. Experimental results show that our proposed method achieves significant improvements than other state-of-the-art baselines on the element recognition task dataset. Compared with the BERT-CNN model, the proposed “All labels Law Articles Embedding Model (ALEM)” improves the accuracy, recall, and F1 value by 0.5, 1.4 and 1.0, respectively.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4200270401",
    "type": "article"
  },
  {
    "title": "Integrating Multiple Dependency Corpora for Inducing Wide-Coverage Japanese CCG Resources",
    "doi": "https://doi.org/10.1145/2658997",
    "publication_date": "2015-01-30",
    "publication_year": 2015,
    "authors": "Sumire Uematsu; Takuya Matsuzaki; Hiroki Hanaoka; Yusuke Miyao; Hideki Mima",
    "corresponding_authors": "",
    "abstract": "A novel method to induce wide-coverage Combinatory Categorial Grammar (CCG) resources for Japanese is proposed in this article. For some languages including English, the availability of large annotated corpora and the development of data-based induction of lexicalized grammar have enabled deep parsing, i.e., parsing based on lexicalized grammars. However, deep parsing for Japanese has not been widely studied. This is mainly because most Japanese syntactic resources are represented in chunk-based dependency structures, while previous methods for inducing grammars are dependent on tree corpora. To translate syntactic information presented in chunk-based dependencies to phrase structures as accurately as possible, integration of annotation from multiple dependency-based corpora is proposed. Our method first integrates dependency structures and predicate-argument information and converts them into phrase structure trees. The trees are then transformed into CCG derivations in a similar way to previously proposed methods. The quality of the conversion is empirically evaluated in terms of the coverage of the obtained CCG lexicon and the accuracy of the parsing with the grammar. While the transforming process used in this study is specialized for Japanese, the framework of our method would be applicable to other languages for which dependency-based analysis has been regarded as more appropriate than phrase structure-based analysis due to morphosyntactic features.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2205874601",
    "type": "article"
  },
  {
    "title": "Learning Generalized Features for Semantic Role Labeling",
    "doi": "https://doi.org/10.1145/2890496",
    "publication_date": "2016-04-12",
    "publication_year": 2016,
    "authors": "Haitong Yang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "This article makes an effort to improve Semantic Role Labeling (SRL) through learning generalized features. The SRL task is usually treated as a supervised problem. Therefore, a huge set of features are crucial to the performance of SRL systems. But these features often lack generalization powers when predicting an unseen argument. This article proposes a simple approach to relieve the issue. A strong intuition is that arguments occurring in similar syntactic positions are likely to bear the same semantic role, and, analogously, arguments that are lexically similar are likely to represent the same semantic role. Therefore, it will be informative to SRL if syntactic or lexical similar arguments can activate the same feature. Inspired by this, we embed the information of lexicalization and syntax into a feature vector for each argument and then use K -means to make clustering for all feature vectors of training set. For an unseen argument to be predicted, it will belong to the same cluster as its similar arguments of training set. Therefore, the clusters can be thought of as a kind of generalized feature. We evaluate our method on several benchmarks. The experimental results show that our approach can significantly improve the SRL performance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2338697399",
    "type": "article"
  },
  {
    "title": "Arabic Word Sense Disambiguation for Information Retrieval",
    "doi": "https://doi.org/10.1145/3510451",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Mohammed Alaeddine Abderrahim; Mohammed El-Amine Abderrahim",
    "corresponding_authors": "",
    "abstract": "In the context of using semantic resources for information retrieval, the relationship and distance between concepts are considered important for word sense disambiguation. In this article, we experiment with Conceptual Density and Random Walk with graph methods to enhance the performance of the Arabic Information Retrieval System. To do this, a medium-sized corpus was used. The results proved that Random Walk can enhance the performance of the information retrieval system by achieving a mean improvement of 13%, 16%, and 12% in terms of recall, precision, and F-score, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4205957816",
    "type": "article"
  },
  {
    "title": "An Intelligent Unsupervised Approach for Handling Context-Dependent Words in Urdu Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3510830",
    "publication_date": "2022-03-21",
    "publication_year": 2022,
    "authors": "Neelam Mukhtar; Mohammad Abid Khan; Nadia Chiragh; Shah Nazir; Asim Ullah Jan",
    "corresponding_authors": "",
    "abstract": "The characteristic of context dependency in Urdu words needs to be handled carefully while performing Urdu sentiment analysis. In this research, an already constructed Urdu sentiment lexicon of positive and negative words is further expanded by the addition of context-dependent words. These context-dependent words are used with or without conjunctions. Rules are formulated for assigning polarities to those context-dependent words that are surrounded by the positive or negative words. These rules were incorporated in the Urdu sentiment analyzer. Fusion of these rules for handling context-dependent words and the expanded Urdu sentiment lexicon resulted in increasing the accuracy of the Urdu sentiment analyzer from 83.43% to 89.03% with 0.8655 precision, 0.9053 recall, and 0.8799 F-measure, which is a statistically significant improvement.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220755339",
    "type": "article"
  },
  {
    "title": "Dual Discriminator GAN: Restoring Ancient Yi Characters",
    "doi": "https://doi.org/10.1145/3490031",
    "publication_date": "2022-03-14",
    "publication_year": 2022,
    "authors": "Shanxiong Chen; Ye Yang; Liu Xuxin; Shiyu Zhu",
    "corresponding_authors": "",
    "abstract": "In China, the damage of ancient Yi books are serious. Due to the lack of ancient Yi experts, the repairation of ancient Yi books is progressing very slowly. The artificial intelligence is successful in the field of image and text, so it is feasible for the automatic restoration of ancient books. In this article, a generative adversarial networks with dual discriminator (DDGAN) is designed to restore incomplete characters in the ancient Yi literature. The DDGAN integrates the deep convolution generative adversarial network with an ancient Yi comparison discriminator. Through two training stages, it could iteratively optimizes the ancient Yi character generation networks to obtain the text generator According to the loss of comparison discriminator, DDGAN mode could be optimized. The DDGAN model can generate characters to restore the missing stroke in the ancient Yi. The experiment shows that the proposed method achieves a restoration rate of 77.3% when no more than one third of the characters are missing. This work is effective for the protection of Yi ancient books.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220763858",
    "type": "article"
  },
  {
    "title": "Research on the Influence of Artificial Intelligence Technology with web 3.0 on Accounting Education and Its Countermeasures",
    "doi": "https://doi.org/10.1145/3527666",
    "publication_date": "2022-04-04",
    "publication_year": 2022,
    "authors": "Changzhen Li; Xin Zhao",
    "corresponding_authors": "",
    "abstract": "With the continuous development of artificial intelligence (AI) technology, AI technology has been widely used in various fields of social and economic life, bringing great impact to various industries. AI is a section of computer science, a new science technology that studies and develops theories, methods, and application systems used. The development of artificial intelligence technology first has a profound impact on the development of the accounting industry, which leads to the change of the demand for accounting talents. This paper takes the accounting industry as an example, discusses the application of artificial intelligence technology in accounting practice, analyzes its impact on the reform of accounting education, and finally proposes countermeasures and suggestions for accounting education to deal with technological challenges with the blended learning for increasing student retention and engagement in an E-learning environment. Blended learning gives greater flexibility which means being able to complete assignments any place and any time. It successfully improves students’ experience and enhances their engagement. It uses apps, games, or measurable programs and helps to balance the classroom. Practice shows that the development of artificial intelligence technology not only has a profound and extensive impact on the accounting profession and accounting education, but also has a profound impact on engineering education and other fields. An in-depth study of the challenges of artificial intelligence technology to accounting education is also of reference and guiding significance for the reform and development of engineering education and other fields.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4226267115",
    "type": "article"
  },
  {
    "title": "Topic Sentiment Analysis for Twitter Data in Indian Languages Using Composite Kernel SVM and Deep Learning",
    "doi": "https://doi.org/10.1145/3519297",
    "publication_date": "2022-05-25",
    "publication_year": 2022,
    "authors": "Shuverthi Maity; Kamal Sarkar",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis of public opinions on social networks, such as Twitter or Facebook, can provide us with valuable information, which has a wide range of applications. But the efficiency and accuracy of the automated methods for Twitter sentiment analysis are hindered by the special characteristics of the Twitter data. The Twitter data is generally noisy, high-dimensional, and it has complex syntactic and semantic structures. Sentiment analysis of Twitter data in Indian languages is more challenging because the data is multilingual and code-mixed. In this article, we propose various composite kernel functions, each of which is used with Support Vector Machines (SVM) for developing a model for topic sentiment analysis of Twitter data in Indian languages. Each composite kernel function is constructed by taking the weighted summation of multiple single kernel functions defined by us. In addition to our proposed composite kernel SVM method, we use several state-of-the-art deep learning classifiers for topic sentiment classification. Since any suitable Twitter dataset in Indian languages is not available for conducting our experiments, we have developed our own datasets by collecting tweets related to five different Twitter trending topics in India. To prove the robustness and generalization capability of the proposed models, they are also evaluated on the US airline Twitter dataset which is a publicly available benchmark English dataset. The empirical study exhibits that the proposed composite kernel SVM method is effective for the sentiment classification task. In the case of Indian language datasets, the proposed composite kernel SVM method achieves the highest average accuracy of 74% and the highest average F-score of 0.73. On the other hand, the deep learning-based method achieves the average accuracy and the average F-score of 71.31% and 0.70, respectively. In the case of the US airline Twitter dataset, the proposed composite kernel SVM method achieves the average accuracy of 83% and the average F-score of 0.82, which are higher than that of the deep learning-based method.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4281477892",
    "type": "article"
  },
  {
    "title": "A Weak-Region Enhanced Bayesian Classification for Spam Content-Based Filtering",
    "doi": "https://doi.org/10.1145/3510420",
    "publication_date": "2022-07-11",
    "publication_year": 2022,
    "authors": "Vahid Nosrati; Mohsen Rahmani; Alireza Jolfaei; Sattar Seifollahi",
    "corresponding_authors": "",
    "abstract": "This article proposes an improved Bayesian scheme by focusing on the region in which Bayesian may fail to correctly identify labels and improve classification performance by handling those errors. Bayesian method, as a probabilistic classifier, uses Bayes’ theorem to calculate the probability of an instance belonging to a class, where the class label with a maximum probability is assigned to the instance. In a spam detection problem, it can be considered that the prediction of the Bayesian classifier is weak when the probability obtained for classes spam and non-spam are close to each other. Therefore, we define a threshold to determine weak prediction against strong prediction. A hybrid strategy using a two-layer Bayesian approach is presented: basic Bayesian (BBayes) and corrected weak region Bayesian (CWRBayes), which are concerned with strong and weak predictions, respectively. Both techniques, BBayes and CWRBayes, have the same classification mechanism, but they use different feature selection mechanisms. The proposed methods are implemented and evaluated over two datasets of spam e-mails, and the results show that the proposed method has better performance than the baseline of the naïve Bayesian and some other Bayesian variants.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4285039405",
    "type": "article"
  },
  {
    "title": "Developing a Large Benchmark Corpus for Urdu Semantic Word Similarity",
    "doi": "https://doi.org/10.1145/3566124",
    "publication_date": "2022-10-12",
    "publication_year": 2022,
    "authors": "Iqra Muneer; Ghazeefa Fatima; Muhammad Salman Khan; Rao Muhammad Adeel Nawab; Ali Saeed",
    "corresponding_authors": "",
    "abstract": "The semantic word similarity task aims to quantify the degree of similarity between a pair of words. In literature, efforts have been made to create standard evaluation resources to develop, evaluate, and compare various methods for semantic word similarity. The majority of these efforts focused on English and some other languages. However, the problem of semantic word similarity has not been thoroughly explored for South Asian languages, particularly Urdu. To fill this gap, this study presents a large benchmark corpus of 518 word pairs for the Urdu semantic word similarity task, which were manually annotated by 12 annotators. To demonstrate how our proposed corpus can be used for the development and evaluation of Urdu semantic word similarity systems, we applied two state-of-the-art methods: (1) a word embedding–based method and (2) a Sentence Transformer–based method. As another major contribution, we proposed a feature fusion method based on Sentence Transformers and word embedding methods. The best results were obtained using our proposed feature fusion method (the combination of best features of both methods) with a Pearson correlation score of 0.67. To foster research in Urdu (an under-resourced language), our proposed corpus will be free and publicly available for research purposes.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4304693452",
    "type": "article"
  },
  {
    "title": "SPK-CG: Siamese Network based Posterior Knowledge Selection Model for Knowledge Driven Conversation Generation",
    "doi": "https://doi.org/10.1145/3569579",
    "publication_date": "2022-12-14",
    "publication_year": 2022,
    "authors": "Tinghuai Ma; Zheng Zhang; Huan Rong; Najla Al-Nabhan",
    "corresponding_authors": "",
    "abstract": "Building a human-computer conversational system that can communicate with humans is a research hotspot in the field of artificial intelligence. Traditional dialogue systems tend to produce irrelevant and non-information responses, which reduce people’s interest in engaging in a conversation. This often leads to boring conversations. To alleviate this problem, many researchers use external knowledge to assist conversation generation. The accuracy of knowledge selection is the prerequisite to ensure the quality of knowledge conversation. This approach has worked positively to a certain extent, but generally only searches knowledge information based on entity words themselves, without considering the specific conversation context. Therefore, if irrelevant knowledge is retrieved, the quality of conversation generation will be reduced. Motivated by this, we propose a novel neural knowledge-based conversation generation model, named Siamese Network based Posterior Knowledge Selection Model for Knowledge Driven Conversation Generation (SPK-CG) . We have designed a novel knowledge selection mechanism to obtain knowledge information that is highly relevant to the context of the conversation. Specifically, the posterior knowledge distribution is used as a soft label to make the prior distribution consistent with the posterior distribution in the training process. At the same time, in order to narrow the gap between prior and posterior distributions and improve the accuracy of knowledge selection, we leverage siamese network and design multi-granularity matching module for knowledge selection. Compared with previous knowledge-based models, our method can select more appropriate knowledge and use the selected knowledge to generate responses that are more relevant to the conversation context. Extensive automatic and human evaluations demonstrate that our model has advantages over previous baselines.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4311491838",
    "type": "article"
  },
  {
    "title": "Open-Domain Response Generation in Low-Resource Settings using Self-Supervised Pre-Training of Warm-Started Transformers",
    "doi": "https://doi.org/10.1145/3579164",
    "publication_date": "2023-01-05",
    "publication_year": 2023,
    "authors": "Tarek Naous; Zahraa Bassyouni; Bassel Mousi; Hazem Hajj; Wassim El‐Hajj; Khaled Shaban",
    "corresponding_authors": "",
    "abstract": "Learning response generation models constitute the main component of building open-domain dialogue systems. However, training open-domain response generation models requires large amounts of labeled data and pre-trained language generation models that are often nonexistent for low-resource languages. In this article, we propose a framework for training open-domain response generation models in low-resource settings. We consider Dialectal Arabic (DA) as a working example. The framework starts by warm-starting a transformer-based encoder-decoder with pre-trained language model parameters. Next, the resultant encoder-decoder model is adapted to DA by employing self-supervised pre-training on large-scale unlabeled data in the desired dialect. Finally, the model is fine-tuned on a very small labeled dataset for open-domain response generation. The results show significant performance improvements on three spoken Arabic dialects after adopting the framework’s three stages, highlighted by higher BLEU and lower Perplexity scores compared with multiple baseline models. Specifically, our models are capable of generating fluent responses in multiple dialects with an average human-evaluated fluency score above 4. Our data is made publicly available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4313558179",
    "type": "article"
  },
  {
    "title": "Learning Reliable Neural Networks with Distributed Architecture Representations",
    "doi": "https://doi.org/10.1145/3578709",
    "publication_date": "2023-01-04",
    "publication_year": 2023,
    "authors": "Yinqiao Li; Runzhe Cao; Qiaozhi He; Tong Xiao; Jingbo Zhu",
    "corresponding_authors": "",
    "abstract": "Neural architecture search (NAS) has shown the strong performance of learning neural models automatically in recent years. But most NAS systems are unreliable due to the architecture gap brought by discrete representations of atomic architectures. In this article, we improve the performance and robustness of NAS via narrowing the gap between architecture representations. More specifically, we apply a general contraction mapping to model neural networks with distributed representations (Neural Architecture Search with Distributed Architecture Representations (ArchDAR)). Moreover, for a better search result, we present a joint learning approach to integrating distributed representations with advanced architecture search methods. We implement our ArchDAR in a differentiable architecture search model and test learned architectures on the language modeling task. On the Penn Treebank data, it outperforms a strong baseline significantly by 1.8 perplexity scores. Also, the search process with distributed representations is more stable, which yields a faster structural convergence when it works with the differentiable architecture search model.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4313590388",
    "type": "article"
  },
  {
    "title": "Fast and Accurate Framework for Ontology Matching in Web of Things",
    "doi": "https://doi.org/10.1145/3578708",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Asma Belhadi; Youcef Djenouri; Gautam Srivastava; Jerry Chun‐Wei Lin",
    "corresponding_authors": "",
    "abstract": "The Web of Things (WoT) can help with knowledge discovery and interoperability issues in many Internet of Things (IoT) applications. This article focuses on semantic modeling of WoT and proposes a new approach called Decomposition for Ontology Matching (DOM) to discover relevant knowledge by exploring correlations between WoT data using decomposition strategies. The DOM technique adopts several decomposition techniques to order highly linked ontologies of WoT data into similar groups. The main idea is to decompose the instances of each ontology into similar groups and then match instances of similar groups instead of entire instances of two ontologies. Three main algorithms for decomposition have been developed. The first algorithm is based on radar scanning, which determines the distribution of distances between each instance and all other instances to determine the cluster centroid. The second algorithm is based on adaptive grid clustering, where it focuses on distribution information and the construction of spanning trees. The third algorithm is based on split index clustering, where instances are divided into groups of cells from which noise is removed during the merging process. Several studies were conducted with different ontology databases to illustrate the use of the DOM technique. The results show that DOM outperforms state-of-the-art ontology matching models in terms of computational cost while maintaining the quality of the matching. Moreover, these results demonstrate that DOM is capable of handling various large datasets in WoT contexts.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4317042660",
    "type": "article"
  },
  {
    "title": "Emotional Intelligence Attention Unsupervised Learning Using Lexicon Analysis for Irony-based Advertising",
    "doi": "https://doi.org/10.1145/3580496",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Usman Ahmed; Jerry Chun‐Wei Lin; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "Social media platforms have made increasing use of irony in recent years. Users can express their ironic thoughts with audio, video, and images attached to text content. When you use irony, you are making fun of a situation or trying to make a point. It can also express frustration or highlight the absurdity of a situation. The use of irony in social media is likely to continue to increase, no matter the reason. By using syntactic information in conjunction with semantic exploration, we show that attention networks can be enhanced. Using learned embedding, unsupervised learning encodes word order into a joint space. By evaluating the entropy of an example class and adding instances, the active learning method uses the shared representation as a query to retrieve semantically similar sentences from a knowledge base. In this way, the algorithm can identify the instance with the maximum uncertainty and extract the most informative example from the training set. An ironic network trained for each labelled record is used to train a classifier (model). The partial training model and the original labelled data generate pseudo-labels for the unlabeled data. To correctly predict the label of a dataset, a classifier (attention network) updates the pseudo-labels for the remaining datasets. After the experimental evaluation of the 1,021 annotated texts, the proposed model performed better than the baseline models, achieving an F1 score of 0.63 on ironic tasks and 0.59 on non-ironic tasks. We also found that the proposed model generalized well to new instances of datasets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4317434601",
    "type": "article"
  },
  {
    "title": "More Than Syntaxes: Investigating Semantics to Zero-shot Cross-lingual Relation Extraction and Event Argument Role Labelling",
    "doi": "https://doi.org/10.1145/3582261",
    "publication_date": "2023-02-02",
    "publication_year": 2023,
    "authors": "Kaiwen Wei; Li Jin; Zequn Zhang; Zhi Guo; Xiaoyu Li; Qing Liu; Weimiao Feng",
    "corresponding_authors": "Kaiwen Wei",
    "abstract": "Syntactic dependency structures are commonly utilized as language-agnostic features to solve the word order difference issues in zero-shot cross-lingual relation and event extraction tasks. However, while sentences in multiple forms can be employed to express the same meaning, the syntactic structure may vary considerably in specific scenarios. To fix this problem, we find semantics are rarely considered, which could provide a more consistent semantic analysis of sentences and be served as another bridge between different languages. Therefore, in this article, we introduce Syntax and Semantic Driven Network (SSDN) to equip syntax and semantic knowledge across languages simultaneously. Specifically, predicate–argument structures from semantic role labelling are explicitly incorporated into word representations. Then, a semantic-aware relational graph convolutional network and a transformer-based encoder are utilized to model both semantic dependency and syntactic dependency structures, respectively. Finally, a fusion module is introduced to integrate output representations adaptively. We conduct experiments on the widely used Automatic Content Extraction 2005 English, Chinese, and Arabic datasets. The evaluation results demonstrate that the proposed method achieves the state-of-the-art performance. Further study also indicates SSDN could produce robust representations that facilitate the transfer operations across languages.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4318990365",
    "type": "article"
  },
  {
    "title": "Multimedia Technology Based Interactive Translation Learning for Students",
    "doi": "https://doi.org/10.1145/3588569",
    "publication_date": "2023-03-30",
    "publication_year": 2023,
    "authors": "Zhiguo Wang; Hongwei Na",
    "corresponding_authors": "",
    "abstract": "Multimedia technology incorporates into the educational arena to translate traditional educational material into interactive digital mode. This has permitted teachers into the learning environment to design and integrate interactive multimedia learning. Many problems towards giving more attention to students facing teacher's multimedia levels are uneven, and their integration significance, bringing enormous curriculum learning strategies, is not entirely apparent. This research introduces multimedia network interpretation teaching using machine learning (MNIT-ML)in multimedia education design to enhance and strengthen the traditional teaching process and promote various modern approaches to transmit knowledge towards students.Allocation of learning resources (ALR) framework is mainly to enlarge the use and utilization of materials for learning activities from impressed resources into recordings, video content, motion graphics, and other forms of resources. The purpose of the Allocation of Learning Resources (ALR) framework is to aid teachers in making sound decisions about how to distribute available educational materials. Its goal is to guide teachers in making smart choices about how to use limited resources like time, money, and technology in order to improve students' educational achievements. The ALR framework was picked because it offers a methodical strategy for selecting choices that is founded in educational research and best practices. Resource allocation is a key aspect in influencing student outcomes, and efficient allocation can help guarantee that all students have access to the tools they need to succeed. There are a number of alternative frameworks that might direct studies on the distribution of educational resources. The Technological Acceptance Model (TAM) is a common tool for analysing what factors lead to widespread implementation of educational technology. The goal of other frameworks like the Universal Design for Learning (UDL) framework is to help educators create lessons and methods that are inclusive of students of all backgrounds and abilities. The final decision on which framework to use will be determined by the nature of the research issues and the setting in which they are being investigated. In order to make educated decisions, it is crucial to pick a framework that is appropriate for the research issues at hand and that gives a systematic approach based on established educational best practices. The digital promoting resources for teaching (DPRT) method creates a real environment for students to learn, focusing on enhancing training to make students feel the standard language translation skills. The simulation analysis is performed based on security 94.6%, the performance of 95.9%, and privacy, proving the proposed framework's reliability overall ratio of 93.4%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4361271923",
    "type": "article"
  },
  {
    "title": "An Efficient and Accurate Detection of Fake News Using Capsule Transient Auto Encoder",
    "doi": "https://doi.org/10.1145/3589184",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Smita Athanere Parte; Ankur Ratmele; Ritesh Dhanare",
    "corresponding_authors": "",
    "abstract": "Fake news is “news reports that are deliberatively and indisputably fake.” News that uses fake information is becoming a threat. It becomes challenging for humans to distinguish between fake and actual news. It has become necessary to detect fake news, which seeks to determine whether a news document can be believed. Detection of fake news faces challenges in accurate classification, making existing detection algorithms ineffective. In these issues, this article uses a novel Adaptive Capsule Transient Auto Encoder (ACTAE) for effectively detecting fake news. ACTAE is a combined approach of a classifier named Capsule Auto Encoder and an algorithm called Adaptive Transient Search Optimization Algorithm. The overall detection process is performed in various stages, including preprocessing, feature withdrawal, feature selection, and classification and optimization of weight parameters of the classifier for better results. The overall process is executed in Python, proving that ACTAE detects fake news with higher accuracy (99%) and lower error rate.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4362452858",
    "type": "article"
  },
  {
    "title": "Cross-lingual Sentence Embedding for Low-resource Chinese-Vietnamese Based on Contrastive Learning",
    "doi": "https://doi.org/10.1145/3589341",
    "publication_date": "2023-04-18",
    "publication_year": 2023,
    "authors": "Yuxin Huang; Yin L; Zhaoyuan Wu; Enchang Zhu; Zhengtao Yu",
    "corresponding_authors": "",
    "abstract": "Cross-lingual sentence embedding’s goal is mapping sentences with similar semantics but in different languages close together and dissimilar sentences farther apart in the representation space. It is the basis of many downstream tasks such as cross-lingual document matching and cross-lingual summary extraction. At present, the works of cross-lingual sentence embedding tasks mainly focus on languages with large-scale corpus. But low-resource languages such as Chinese-Vietnamese are short of sentence-level parallel corpora and clear cross-lingual monitoring signals, and these works on low-resource languages have poor performances. Therefore, we propose a cross-lingual sentence embedding method based on contrastive learning and effectively fine-tune powerful pretraining mode by constructing sentence-level positive and negative samples to avoid the catastrophic forgetting problem of the traditional fine-tuning pre-trained model based only on small-scale aligned positive samples. First, we construct positive and negative examples by taking parallel Chinese Vietnamese sentences as positive examples and non-parallel sentences as negative examples. Second, we construct a siamese network to get contrastive loss by inputting positive and negative samples and fine-tuning our model. The experimental results show that our method can effectively improve the semantic alignment accuracy of cross-lingual sentence embedding in Chinese and Vietnamese contexts.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4366274340",
    "type": "article"
  },
  {
    "title": "Cross-lingual Text Reuse Detection at Document Level for English-Urdu Language Pair",
    "doi": "https://doi.org/10.1145/3592761",
    "publication_date": "2023-05-01",
    "publication_year": 2023,
    "authors": "Muhammad Sharjeel; Iqra Muneer; Sumaira Nosheen; Rao Muhammad Adeel Nawab; Paul Rayson",
    "corresponding_authors": "",
    "abstract": "In recent years, the problem of Cross-Lingual Text Reuse Detection (CLTRD) has gained the interest of the research community due to the availability of large digital repositories and automatic Machine Translation (MT) systems. These systems are readily available and openly accessible, which makes it easier to reuse text across languages but hard to detect. In previous studies, different corpora and methods have been developed for CLTRD at the sentence/passage level for the English-Urdu language pair. However, there is a lack of large standard corpora and methods for CLTRD for the English-Urdu language pair at the document level. To overcome this limitation, the significant contribution of this study is the development of a large benchmark cross-lingual (English-Urdu) text reuse corpus, called the TREU (Text Reuse for English-Urdu) corpus. It contains English to Urdu real cases of text reuse at the document level. The corpus is manually labelled into three categories (Wholly Derived = 672, Partially Derived = 888, and Non Derived = 697) with the source text in English and the derived text in the Urdu language. Another contribution of this study is the evaluation of the TREU corpus using a diversified range of methods to show its usefulness and how it can be utilized in the development of automatic methods for measuring cross-lingual (English-Urdu) text reuse at the document level. The best evaluation results, for both binary ( F 1 = 0.78) and ternary ( F 1 = 0.66) classification tasks, are obtained using a combination of all Translation plus Mono-lingual Analysis (T+MA) based methods. The TREU corpus is publicly available to promote CLTRD research in an under-resourced language, i.e., Urdu.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4367605051",
    "type": "article"
  },
  {
    "title": "Urdu Speech Emotion Recognition: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3595377",
    "publication_date": "2023-05-02",
    "publication_year": 2023,
    "authors": "Soonh Taj; Ghulam Mujtaba; Sher Muhammad Daudpota; Muhammad Hussain Mughal",
    "corresponding_authors": "",
    "abstract": "Research on Speech Emotion Recognition is becoming more mature day by day, and a lot of research is being carried out on Speech Emotion Recognition in resource-rich languages like English, German, French, and Chinese. Urdu is among the top 10 languages spoken worldwide. Despite its importance, few studies have worked on Urdu Speech emotion as Urdu is recognized as a resource-poor language. The Urdu language lacks publicly available datasets, and for this reason, few researchers have worked on Urdu Speech Emotion Recognition. To the best of our knowledge, no review has been found on Urdu Speech Emotion recognition. This study is the first systematic literature review on Urdu Speech Emotion Recognition, and the primary goal of this study is to provide a detailed analysis of the literature on Urdu Speech Emotion Recognition which includes the datasets, features, pre-processing, approaches, performance metrics, and validation methods used for Urdu Speech Emotion Recognition. This study also highlights the challenges and future directions for Urdu Speech Emotion Recognition.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4367677024",
    "type": "article"
  },
  {
    "title": "Tokenization of Tunisian Arabic: A Comparison between Three Machine Learning Models",
    "doi": "https://doi.org/10.1145/3599234",
    "publication_date": "2023-05-24",
    "publication_year": 2023,
    "authors": "Asma Mekki; Inès Zribi; Mariem Ellouze; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "Tokenization represents the way of segmenting a piece of text into smaller units called tokens. Since Arabic is an agglutinating language by nature, this treatment becomes a crucial preprocessing step for many Natural Language Processing (NLP) applications such as morphological analysis, parsing, machine translation, information extraction, and so on. In this article, we investigate word tokenization task with a rewriting process to rewrite the orthography of the stem. For this task, we are using Tunisian Arabic (TA) text. To the best of the researchers’ knowledge, this is the first study that uses TA for word tokenization. Therefore, we start by collecting and preparing various TA corpora from different sources. Then, we present a comparison of three character-based tokenizers based on Conditional Random Fields (CRF), Support Vector Machines (SVM) and Deep Neural Networks (DNN). The best proposed model using CRF achieved an F-measure result of 88.9%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4377989177",
    "type": "article"
  },
  {
    "title": "Extractive Summarization of Telugu Text Using Modified Text Rank and Maximum Marginal Relevance",
    "doi": "https://doi.org/10.1145/3600224",
    "publication_date": "2023-06-12",
    "publication_year": 2023,
    "authors": "G. L. Anand Babu; Srinivasu Badugu",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of digital content, there is a need for an automatic text summarizer to provide short text from a long text document. Many research works have been presented for extractive text summarization (ETS). This article mainly focuses on the graph-based ETS approach for multiple Telugu text documents. A modified Text-Rank algorithm is employed with the noun and verb count of each sentence in the text as the initial score of each node. To get the optimal features, a novel feature selection algorithm called improved Flamingo Search Algorithm is proposed in this article. Though graph-based ETS is an important approach, the generated summaries are redundant. To reduce the redundancy in the generated summary, maximum marginal relevance is combined with the modified Text-Rank. Different word-embedding techniques such as Fast-Text, Word2vec, TF-IDF, and one-hot encoding are utilized to experiment with the proposed approach. The performance of the proposed text summarization approach is evaluated with BLEU and ROUGE in terms of F-measure, precision, and recall.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4380363480",
    "type": "article"
  },
  {
    "title": "Instagram Text Sentiment Analysis Combining Machine Learning and NLP",
    "doi": "https://doi.org/10.1145/3606370",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Chia-Pang Chan; Jun-He Yang",
    "corresponding_authors": "",
    "abstract": "research-article Free Access Share on Instagram Text Sentiment Analysis Combining Machine Learning and NLPJust Accepted Authors: Chia-Pang Chan Department of Information Management, Cheng Shiu University, Taiwan 10001, China Department of Information Management, Cheng Shiu University, Taiwan 10001, China 0000-0003-0016-8674Search about this author , Jun-He Yang Department of E-Sport Technology Management, Cheng Shiu University, Taiwan 10001, China Department of E-Sport Technology Management, Cheng Shiu University, Taiwan 10001, China 0000-0002-4008-4785Search about this author Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingAccepted on June 2023https://doi.org/10.1145/3606370Published:03 July 2023Publication History 0citation69DownloadsMetricsTotal Citations0Total Downloads69Last 12 Months69Last 6 weeks69 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4382987285",
    "type": "article"
  },
  {
    "title": "Investigating Unsupervised Neural Machine Translation for Low-resource Language Pair English-Mizo via Lexically Enhanced Pre-trained Language Models",
    "doi": "https://doi.org/10.1145/3609222",
    "publication_date": "2023-07-13",
    "publication_year": 2023,
    "authors": "Candy Lalrempuii; Badal Soni",
    "corresponding_authors": "",
    "abstract": "The vast majority of languages in the world at present are considered to be low-resource languages. Since the availability of large parallel data is crucial for the success of most modern machine translation approaches, improving machine translation for low-resource languages is a key challenge. Most unsupervised techniques for translation benefit closely related languages with monolingual data of substantial quantity. To facilitate research in this direction for the extremely low resource language pair English ( en ) and Mizo ( lus ), we have developed a parallel and monolingual corpus for the Mizo language from various news websites. We explore Unsupervised Neural Machine Translation (UNMT) based on the developed monolingual data. We observe that cross-lingual embedding (CLWE) initializations on subword segmented data during pre-training, based on both masked language modelling and sequence-to-sequence generation tasks, improve translation performance. We experiment with cross-lingual alignment and combined alignment and joint training for learning the cross-lingual embedding representations. We also report baseline performances and the impact of CLWE initialization using semi-supervised and supervised neural machine translation. Empirical results show that both CLWE initializations work well for the distant pair English-Mizo compared to the baselines.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4384200848",
    "type": "article"
  },
  {
    "title": "SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3610611",
    "publication_date": "2023-07-26",
    "publication_year": 2023,
    "authors": "Haiyue Song; Raj Dabre; Chenhui Chu; Sadao Kurohashi; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "Sub-word segmentation is an essential pre-processing step for Neural Machine Translation (NMT). Existing work has shown that neural sub-word segmenters are better than Byte-Pair Encoding (BPE), however, they are inefficient as they require parallel corpora, days to train and hours to decode. This paper introduces SelfSeg, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora. SelfSeg takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability and generates the segmentation with the maximum posterior probability, which is calculated using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and we explore several word frequency normalization strategies to accelerate the training phase. Additionally, we propose a regularization mechanism that allows the segmenter to generate various segmentations for one word. To show the effectiveness of our approach, we conduct MT experiments in low-, middle- and high-resource scenarios, where we compare the performance of using different segmentation methods. The experimental results demonstrate that on the low-resource ALT dataset, our method achieves more than 1.2 BLEU score improvement compared with BPE and SentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding (DPE) and Vocabulary Learning via Optimal Transport (VOLT) on average. The regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version of BPE. We also observed significant improvements on IWSLT15 Vi-&gt;En, WMT16 Ro-&gt;En and WMT15 Fi-&gt;En datasets, and competitive results on the WMT14 De-&gt;En and WMT14 Fr-&gt;En datasets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385284112",
    "type": "article"
  },
  {
    "title": "HKG: A Novel Approach for Low Resource Indic Languages to Automatic Knowledge Graph Construction",
    "doi": "https://doi.org/10.1145/3611306",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Preeti Vats; Nonita Sharma; Deepak Kumar Sharma",
    "corresponding_authors": "",
    "abstract": "Knowledge graph (KG), a visual representation of text data as a semantic network, holds enormous promise for the development of more intelligent robots. It leads to significant potential solutions for many tasks like question answering, recommendation, and information retrieval. However, this area is confined to using English text only. Since low-resource languages are now being used in the world of AI, it is necessary to develop a semantic network for them as well. In this research work, the authors provide state-of-the-art techniques for automatic knowledge graph construction for the Hindi language, which is still unexplored in ontology. Constructing a knowledge graph faces several hurdles and obstacles in the linguistic domain, primarily when it deals with the Hindi language. With an emphasis on the Indian perspective, this research intends to introduce a novel approach ‘HKG’ for knowledge graph construction framework for Hindi. It also implements the LSTM model to evaluate the accuracy of newly constructed knowledge graphs and compute different evaluation metrics such as accuracy and F1-score. This knowledge graph evaluates the accuracy of 87.50 using Doc2Vec word embedding with a train-test split of 7:3.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385497368",
    "type": "article"
  },
  {
    "title": "Research on the Detection and Analysis of Students’ Classroom Behavioral Features Based on Deep CNNs",
    "doi": "https://doi.org/10.1145/3615865",
    "publication_date": "2023-08-17",
    "publication_year": 2023,
    "authors": "D. L. G. C. Bao; Wen Su",
    "corresponding_authors": "",
    "abstract": "The continuous development of artificial intelligence technology has caused the traditional education model to begin to change as well. The new educational model of applying intelligent monitoring devices to modern classrooms, thus assisting teachers in teaching students, has been successfully favored by major universities. Based on this, this research attempts to improve the target detection algorithm in the field of deep learning, and proposes a method of detecting and analyzing students' classroom behavior based on deep convolutional neural network, and applies the method to detect students' classroom behavior characteristics, aiming to improve the method of detecting and analyzing students' classroom behavior, assist teachers in student management with intelligent monitoring devices, and thus improve the quality of teaching. The study firstly investigated the core steps of the target detection algorithm, and secondly improved the YOLOv5s algorithm by adding feature fusion layer and reimaging module, and finally obtained the Ghost-4D-YOLOv5s network model. The experimental results show that the Ghost-4D-YOLOv5s model performs well in terms of precision, recall, F1 value and average precision mean. When the number of samples to be tested is 350, the performance of the model achieves 58.2% precision, 59.6% recall, 62.5% F1 value and 62.8% average precision mean value. This indicates that the model constructed in this study has good performance in detecting the behavioral characteristics of students in the classroom. It can be concluded that the model constructed in this study can have a good performance in the detection of students' classroom behavioral characteristics. Through this model, teachers can better manage their students and improve the quality of teaching. In addition, this study highlights the potential of deep learning in the field of education and contributes to the development of intelligent educational models.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385954419",
    "type": "article"
  },
  {
    "title": "Transfer Learning-Based Neural Machine Translation for Low-Resource Languages",
    "doi": "https://doi.org/10.1145/3618111",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Jun Dong",
    "corresponding_authors": "Jun Dong",
    "abstract": "College of Liberal Arts, Ludong University, Yantai 264025, China Chinese Lexicography Research Center, Yantai 264025, China Neural Machine Translation (NMT) improves readability by augmenting sentence suggestions based on the precise likelihood of the words. The word suggestions are trained using learning paradigms through repeated translations, word searches, and user inputs. However, the challenging process is the implication of NMT for low-resource language wherein the chances of false suggestions/ word substitutions are high. So, this article proposes a Likelihood-based Machine Translation Model (LMTM) for low-resource languages. The model uses word frequency and potential substitutions from less-known sentences to identify sentences with high precision. This is achieved through a combination of recurrence and substitutions using transfer learning. The identified high-likelihood words are used for sentence augmentation, and the entire set of words from the generated sentence updates the learning paradigm. The model suggests the highest likelihood words for NMT, preventing sentence falsification and ensuring accurate translations. The proposed model increases likelihood by 9.15%, correctness by 7.46%, and substitutions by 7.7%, respectively. It reduces falsification and time complexity by 9.33% and 8.52%, respectively. Overall, the LMTM improves translation quality for low-resource languages.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386710504",
    "type": "article"
  },
  {
    "title": "Negative Stances Detection from Multilingual Data Streams in Low-Resource Languages on Social Media Using BERT and CNN-Based Transfer Learning Model",
    "doi": "https://doi.org/10.1145/3625821",
    "publication_date": "2023-09-30",
    "publication_year": 2023,
    "authors": "Sanjay Kumar",
    "corresponding_authors": "Sanjay Kumar",
    "abstract": "Online social media allows users to connect with a large number of people across the globe and facilitate the exchange of information efficiently. These platforms cater to many of our day-to-day needs. However, at the same time, social media have been increasingly used to transmit negative stances such as derogatory language, hate speech, and cyberbullying. The task of identifying the negative stances from social media posts or comments or tweets is termed negative stance detection . One of the major challenges associated with negative stance detection is that most of the content published on social media is often in a multilingual format. This work aims to identify negative stances from multilingual data streams in low-resource languages on social media using a hybrid transfer learning and deep convolutional neural network approach. The proposed work starts by preprocessing the multilingual datasets by removing irrelevant information such as special characters and hyperlinks. The processed dataset is then passed through a pretrained BERT (bidirectional encoder representations from Transformers) model to generate embeddings by fine-tuning the model as per the dataset under consideration. The generated word embeddings are then passed to a deep convolutional neural network for extracting the latent features from the texts and removing the unessential information. This helps our model to achieve robustness and effectiveness for efficient learning on the given dataset and make appropriate predictions on zero-shot data. The article utilizes several optimization strategies for examining the impact of fine-tuning different BERT layers on the model’s performance. Intensive experiments on a variety of languages — namely, English, French, Italian, Danish, Arabic, Spanish, Indonesian, German, and Portuguese — are performed. The experimental results demonstrate the effectiveness and efficiency of the proposed framework.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387220126",
    "type": "article"
  },
  {
    "title": "Shortcut Enhanced Syntactic and Semantic Dual-channel Network for Aspect-based Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3629518",
    "publication_date": "2023-10-23",
    "publication_year": 2023,
    "authors": "Xiaoyu Tang; Mengyun Zheng; Jiewen Feng; Jiazheng Huang; Yayun Gong",
    "corresponding_authors": "",
    "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained task that predicts the sentiment polarity of different aspects in the same sentence. The main challenge is how to build a strong dependency between aspects and sentiment. Recently, the graph neural network (GNN) has become the mainstream trend to extract syntactic dependency relations from the syntactic dependency tree. However, further improvements are hampered by the inherent mistake on the syntactic dependency tree. Consequently, this article presents a dual-channel model to investigate whether considering both syntactic dependency and semantic relevance can further improve the performance. Specifically, we propose a multi-head syntactic graph convolution network (MHGCN) module in the syntactic channel, focusing on different aspects of the syntactic flow in parallel. We also design a syntactic local attention mechanism (Syn-LFAM) and a semantic local attention mechanism (Sem-LFAM) to fully exploit the crucial local information, respectively. Moreover, we use the cross semantic-syntax interaction module and gate fusion mechanism to control the combination of semantic and syntax dynamically. The experimental results show that we utilize less resource consumption, and the final model outperforms the state-of-the-art methods on three of the four publicly available datasets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387868934",
    "type": "article"
  },
  {
    "title": "Towards Mental Health Analysis in Social Media for Low-resourced Languages",
    "doi": "https://doi.org/10.1145/3638761",
    "publication_date": "2023-12-30",
    "publication_year": 2023,
    "authors": "Muskan Garg",
    "corresponding_authors": "Muskan Garg",
    "abstract": "The surge in internet use for expression of personal thoughts and beliefs has made it increasingly feasible for the social Natural Language Processing (NLP) research community to find and validate associations between social media posts and mental health status . Cross-sectional and longitudinal studies of low-resourced social media data bring to fore the importance of real-time responsible Artificial Intelligence (AI) models for mental health analysis in native languages. Aiming at classifying research for social computing and tracking advances in the development of learning-based models, we propose a comprehensive survey on mental health analysis for social media and posit the need of analyzing low-resourced social media data for mental health . We first classify three components for computing on social media as: SM - data mining/natural language processing on social media , IA - integrated applications with social media data and user-network modeling, and NM - user and network modeling on social networks. To this end, we posit the need of mental health analysis in different languages of East Asia (e.g., Chinese, Japanese, Korean), South Asia (Hindi, Bengali, Tamil), Southeast Asia (Malay, Thai, Vietnamese), European languages (Spanish, French) and the Middle East (Arabic). Our comprehensive study examines available resources and recent advances in low-resourced languages for different aspects of SM, IA, and NM to discover new frontiers as potential field of research.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390450387",
    "type": "article"
  },
  {
    "title": "Incorporating Prior Knowledge into Word Embedding for Chinese Word Similarity Measurement",
    "doi": "https://doi.org/10.1145/3182622",
    "publication_date": "2018-04-02",
    "publication_year": 2018,
    "authors": "Degen Huang; Jiahuan Pei; Cong Zhang; Kaiyu Huang; Jianjun Ma",
    "corresponding_authors": "",
    "abstract": "Word embedding-based methods have received increasing attention for their flexibility and effectiveness in many natural language-processing (NLP) tasks, including Word Similarity (WS). However, these approaches rely on high-quality corpus and neglect prior knowledge. Lexicon-based methods concentrate on human’s intelligence contained in semantic resources, e.g., Tongyici Cilin, HowNet, and Chinese WordNet, but they have the drawback of being unable to deal with unknown words. This article proposes a three-stage framework for measuring the Chinese word similarity by incorporating prior knowledge obtained from lexicons and statistics into word embedding: in the first stage, we utilize retrieval techniques to crawl the contexts of word pairs from web resources to extend context corpus. In the next stage, we investigate three types of single similarity measurements, including lexicon similarities, statistical similarities, and embedding-based similarities. Finally, we exploit simple combination strategies with math operations and the counter-fitting combination strategy using optimization method. To demonstrate our system’s efficiency, comparable experiments are conducted on the PKU-500 dataset. Our final results are 0.561/0.516 of Spearman/Pearson rank correlation coefficient, which outperform the state-of-the-art performance to the best of our knowledge. Experiment results on Chinese MC-30 and SemEval-2012 datasets show that our system also performs well on other Chinese datasets, which proves its transferability. Besides, our system is not language-specific and can be applied to other languages, e.g., English.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2796055497",
    "type": "article"
  },
  {
    "title": "Pause-Based Phrase Extraction and Effective OOV Handling for Low-Resource Machine Translation Systems",
    "doi": "https://doi.org/10.1145/3265751",
    "publication_date": "2018-12-14",
    "publication_year": 2018,
    "authors": "K. Mrinalini; T. Nagarajan; P. Vijayalakshmi",
    "corresponding_authors": "",
    "abstract": "Machine translation is the core problem for several natural language processing research across the globe. However, building a translation system involving low-resource languages remains a challenge with respect to statistical machine translation (SMT). This work proposes and studies the effect of a phrase-induced hybrid machine translation system for translation from English to Tamil, under a low-resource setting. Unlike conventional hybrid MT systems, the free-word ordering feature of the target language Tamil is exploited to form a re-ordered target language model and to extend the parallel text corpus for training the SMT. In the current work, a novel rule-based phrase-extraction method, implemented using parts-of-speech (POS) and place-of-pause in both languages is proposed, which is used to pre-process the training corpus for developing the back-off phrase-induced SMT. Further, out-of-vocabulary (OOV) words are handled using speech-based transliteration and two-level thesaurus intersection techniques based on the POS tag of the OOV word. To ensure that the input with OOV words does not skip phrase-level translation in the hierarchical model, a phrase-level example-based machine translation approach is adopted to find the closest matching phrase and perform translation followed by OOV replacement. The proposed system results in a bilingual evaluation understudy score of 84.78 and a translation edit rate of 19.12. The performance of the system is compared in terms of adequacy and fluency, with existing translation systems for this specific language pair, and it is observed that the proposed system outperforms its counterparts.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2905134510",
    "type": "article"
  },
  {
    "title": "Adversarial Training for Unknown Word Problems in Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3342482",
    "publication_date": "2019-08-21",
    "publication_year": 2019,
    "authors": "Yatu Ji; Hongxu Hou; Junjie Chen; Nier Wu",
    "corresponding_authors": "",
    "abstract": "Nearly all of the work in neural machine translation (NMT) is limited to a quite restricted vocabulary, crudely treating all other words the same as an &lt; unk &gt; symbol. For the translation of language with abundant morphology, unknown (UNK) words also come from the misunderstanding of the translation model to the morphological changes. In this study, we explore two ways to alleviate the UNK problem in NMT: a new generative adversarial network (added value constraints and semantic enhancement) and a preprocessing technique that mixes morphological noise. The training process is like a win-win game in which the players are three adversarial sub models (generator, filter, and discriminator). In this game, the filter is to emphasize the discriminator’s attention to the negative generations that contain noise and improve the training efficiency. Finally, the discriminator cannot easily discriminate the negative samples generated by the generator with filter and human translations. The experimental results show that the proposed method significantly improves over several strong baseline models across various language pairs and the newly emerged Mongolian-Chinese task is state-of-the-art.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2969591748",
    "type": "article"
  },
  {
    "title": "Explicitly Modeling Word Translations in Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3342353",
    "publication_date": "2019-07-23",
    "publication_year": 2019,
    "authors": "Dong Seog Han; Junhui Li; Yachao Li; Min Zhang; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "In this article, we show that word translations can be explicitly incorporated into NMT effectively to avoid wrong translations. Specifically, we propose three cross-lingual encoders to explicitly incorporate word translations into NMT: (1) Factored encoder, which encodes a word and its translation in a vertical way; (2) Gated encoder, which uses a gated mechanism to selectively control the amount of word translations moving forward; and (3) Mixed encoder, which stitchingly learns a word and its translation annotations over sequences where words and their translations are alternatively mixed. Besides, we first use a simple word dictionary approach and then a word sense disambiguation (WSD) approach to effectively model the word context for better word translation. Experimentation on Chinese-to-English translation demonstrates that all proposed encoders are able to improve the translation accuracy for both traditional RNN-based NMT and recent self-attention-based NMT (hereafter referred to as Transformer ). Specifically, Mixed encoder yields the most significant improvement of 2.0 in BLEU on the RNN-based NMT, while Gated encoder improves 1.2 in BLEU on Transformer . This indicates the usefulness of an WSD approach in modeling word context for better word translation. This also indicates the effectiveness of our proposed cross-lingual encoders in explicitly modeling word translations to avoid wrong translations in NMT. Finally, we discuss in depth how word translations benefit different NMT frameworks from several perspectives.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2998243336",
    "type": "article"
  },
  {
    "title": "Dynamic Updating of the Knowledge Base for a Large-Scale Question Answering System",
    "doi": "https://doi.org/10.1145/3377708",
    "publication_date": "2020-02-20",
    "publication_year": 2020,
    "authors": "Xiao-Yang Liu; Yimeng Zhang; Yukang Liao; Ling Jiang",
    "corresponding_authors": "",
    "abstract": "Today, the knowledge base question answering (KB-QA) system is promising to achieve a large-scale high-quality reply in the e-commerce industry. However, there exist two major challenges to efficiently support large-scale KB-QA systems. On the one hand, it is difficult to serve tens of thousands of online stores (i.e., constrained by the tuning and deployment time), and it would perform poorly if the systems start without a sufficient number of chat records. On the other hand, current KB-QA systems cannot be updated in an efficient way due to the high cost of knowledge base (KB) updating. In this article, we propose an automatic learning scheme for KB-QA systems, called ALKB-QA , using a vector modeling method to address the preceding two main challenges. The ALKB-QA system provides online stores with basic KB templates that are suitable for many common occasions, and this feature enables the ability to deploy chatbots for a large number of online stores in a short time. Then, the KBs are further updated automatically to adapt to their own businesses (meet different specific needs), leading to increased reply accuracy. Our work has three main contributions. First, the proposed ALKB-QA system has a good business model in the e-commerce industry (serving tens of thousands of online stores with low cost), breaking the scalability limitations of existing KB-QA systems. Second, we assess the reply accuracy of the proposed ALKB-QA system using human evaluations, and the results show that it outperforms human annotation-base approaches. Third, we launched our ALKB-QA system as a real-world business application, and it supports tens of thousands of online stores.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3007200444",
    "type": "article"
  },
  {
    "title": "Enhanced Language Modeling with Proximity and Sentence Relatedness Information for Extractive Broadcast News Summarization",
    "doi": "https://doi.org/10.1145/3377407",
    "publication_date": "2020-02-07",
    "publication_year": 2020,
    "authors": "Shih‐Hung Liu; Kuan-Yu Chen; Berlin Chen",
    "corresponding_authors": "",
    "abstract": "The primary task of extractive summarization is to automatically select a set of representative sentences from a text or spoken document that can concisely express the most important theme of the original document. Recently, language modeling (LM) has been proven to be a promising modeling framework for performing this task in an unsupervised manner. However, there still remain three fundamental challenges facing the existing LM-based methods, which we set out to tackle in this article. The first one is how to construct a more accurate sentence model in this framework without resorting to external sources of information. The second is how to take into account sentence-level structural relationships, in addition to word-level information within a document, for important sentence selection. The last one is how to exploit the proximity cues inherent in sentences to obtain a more accurate estimation of respective sentence models. Specifically, for the first and second challenges, we explore a novel, principled approach that generates overlapped clusters to extract sentence relatedness information from the document to be summarized, which can be used not only to enhance the estimation of various sentence models but also to render sentence-level structural relationships within the document, leading to better summarization effectiveness. For the third challenge, we investigate several formulations of proximity cues for use in sentence modeling involved in the LM-based summarization framework, free of the strict bag-of-words assumption. Furthermore, we also present various ensemble methods that seamlessly integrate proximity and sentence relatedness information into sentence modeling. Extensive experiments conducted on a Mandarin broadcast news summarization task show that such integration of proximity and sentence relatedness information is indeed beneficial for speech summarization. Our proposed summarization methods can significantly boost the performance of an LM-based strong baseline (e.g., with a maximum ROUGE-2 improvement of 26.7% relative) and also outperform several state-of-the-art unsupervised methods compared in the article.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3014539864",
    "type": "article"
  },
  {
    "title": "S <sup>3</sup> -NET",
    "doi": "https://doi.org/10.1145/3365679",
    "publication_date": "2020-02-20",
    "publication_year": 2020,
    "authors": "Cheoneum Park; Hee-Jun Song; Changki Lee",
    "corresponding_authors": "",
    "abstract": "Machine reading comprehension question answering (MRC-QA) is the task of understanding the context of a given passage to find a correct answer within it. A passage is composed of several sentences; therefore, the length of the input sentence becomes longer, leading to diminished performance. In this article, we propose S 3 -NET, which adds sentence-based encoding to solve this problem. S 3 -NET, which is based on a simple recurrent unit architecture, is a deep learning model that solves the MRC-QA by applying matching network to sentence-level encoding. In addition, S 3 -NET utilizes self-matching networks to compute attention weight for its own recurrent neural network sequences. We perform MRC-QA for the SQuAD dataset of English and MindsMRC dataset of Korean. The experimental results show that for SQuAD, the S 3 -NET model proposed in this article produces 71.91% and 74.12% exact match and 81.02% and 82.34% F1 in single and ensemble models, respectively, and for MindsMRC, our model achieves 69.43% and 71.28% exact match and 81.53% and 82.77% F1 in single and ensemble models, respectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3015159878",
    "type": "article"
  },
  {
    "title": "Iterative Training of Unsupervised Neural and Statistical Machine Translation Systems",
    "doi": "https://doi.org/10.1145/3389790",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Benjamin Marie; Atsushi Fujita",
    "corresponding_authors": "",
    "abstract": "Recent work achieved remarkable results in training neural machine translation (NMT) systems in a fully unsupervised way, with new and dedicated architectures that only rely on monolingual corpora. However, previous work also showed that unsupervised statistical machine translation (USMT) performs better than unsupervised NMT (UNMT), especially for distant language pairs. To take advantage of the superiority of USMT over UNMT, and considering that SMT suffers from well-known limitations overcome by NMT, we propose to define UNMT as NMT trained with the supervision of synthetic parallel data generated by USMT. This way we can exploit USMT up to its limits while ultimately relying on full-fledged NMT models to generate translations. We show significant improvements in translation quality over previous work and also that further improvements can be obtained by alternatively and iteratively training USMT and UNMT. Without the need of a dedicated architecture for UNMT, our simple approach can straightforwardly benefit from any recent and future advances in supervised NMT. Our systems achieve a new state-of-the-art for unsupervised machine translation in all of our six translation tasks for five diverse language pairs, surpassing even supervised SMT or NMT in some tasks. Furthermore, our analysis shows how crucial the comparability between the monolingual corpora used for unsupervised training is in improving translation quality.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3036377933",
    "type": "article"
  },
  {
    "title": "The Transnational Happiness Study with Big Data Technology",
    "doi": "https://doi.org/10.1145/3412497",
    "publication_date": "2020-11-23",
    "publication_year": 2020,
    "authors": "Lingxi Peng; Haohuai Liu; Yangang Nie; Ying Xie; Xuan Tang; Ping Luo",
    "corresponding_authors": "",
    "abstract": "Happiness is a hot topic in academic circles. The study of happiness involves many disciplines, such as philosophy, psychology, sociology, and economics. However, there are few studies on the quantitative analysis of the factors affecting happiness. In this article, we used the well-known World Values Survey Wave 6 (WV6) dataset to quantitatively analyze the happiness of 57 countries with Big Data techniques. First, we obtained the seven most important factors by constructing happiness decision trees for each country. Calculating the frequencies of these factors, we obtained the 17 most important indicators for the prediction of happiness in the world. Then, we selected five representative countries, namely, Sweden, Japan, India, China, and the USA, and analyzed the indicators with the random forest method. We identified different patterns of factors that influence happiness in different countries. This study is a successful attempt to apply data mining technology in the social sciences, and the results are of practical significance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3109752572",
    "type": "article"
  },
  {
    "title": "Optimisation of the Largest Annotated Tibetan Corpus Combining Rule-based, Memory-based, and Deep-learning Methods",
    "doi": "https://doi.org/10.1145/3409488",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Marieke Meelen; Élie Roux; Nathan W. Hill",
    "corresponding_authors": "",
    "abstract": "This article presents a pipeline that converts collections of Tibetan documents in plain text or XML into a fully segmented and POS-tagged corpus. We apply the pipeline to the large extent collection of the Buddhist Digital Resource Center. The semi-supervised methods presented here not only result in a new and improved version of the largest annotated Tibetan corpus to date, the integration of rule-based, memory-based, and neural-network methods also serves as a good example of how to overcome challenges of under-researched languages. The end-to-end accuracy of our entire automatic pipeline of 91.99% is high enough to make the resulting corpus a useful resource for both linguists and scholars of Tibetan studies.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3134349173",
    "type": "article"
  },
  {
    "title": "Unsupervised Neural Machine Translation for Similar and Distant Language Pairs",
    "doi": "https://doi.org/10.1145/3418059",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Haipeng Sun; Rui Wang; Masao Utiyama; Benjamin Marie; Kehai Chen; Eiichiro Sumita; Tiejun Zhao",
    "corresponding_authors": "",
    "abstract": "Unsupervised neural machine translation (UNMT) has achieved remarkable results for several language pairs, such as French–English and German–English. Most previous studies have focused on modeling UNMT systems; few studies have investigated the effect of UNMT on specific languages. In this article, we first empirically investigate UNMT for four diverse language pairs (French/German/Chinese/Japanese–English). We confirm that the performance of UNMT in translation tasks for similar language pairs (French/German–English) is dramatically better than for distant language pairs (Chinese/Japanese–English). We empirically show that the lack of shared words and different word orderings are the main reasons that lead UNMT to underperform in Chinese/Japanese–English. Based on these findings, we propose several methods, including artificial shared words and pre-ordering, to improve the performance of UNMT for distant language pairs. Moreover, we propose a simple general method to improve translation performance for all these four language pairs. The existing UNMT model can generate a translation of a reasonable quality after a few training epochs owing to a denoising mechanism and shared latent representations. However, learning shared latent representations restricts the performance of translation in both directions, particularly for distant language pairs, while denoising dramatically delays convergence by continuously modifying the training data. To avoid these problems, we propose a simple, yet effective and efficient, approach that (like UNMT) relies solely on monolingual corpora: pseudo-data-based unsupervised neural machine translation. Experimental results for these four language pairs show that our proposed methods significantly outperform UNMT baselines.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3145583208",
    "type": "article"
  },
  {
    "title": "Facebook Tells Me Your Gender: An Exploratory Study of Gender Prediction for Turkish Facebook Users",
    "doi": "https://doi.org/10.1145/3448253",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Önder Çoban; Ali İnan; Selma Ayşe Özel",
    "corresponding_authors": "",
    "abstract": "Online Social Networks (OSNs) are very popular platforms for social interaction. Data posted publicly over OSNs pose various threats against the individual privacy of OSN users. Adversaries can try to predict private attribute values, such as gender, as well as links/connections. Quantifying an adversary’s capacity in inferring the gender of an OSN user is an important first step towards privacy protection. Numerous studies have been made on the problem of predicting the gender of an author/user, especially in the context of the English language. Conversely, studies in this field are quite limited for the Turkish language and specifically in the domain of OSNs. Previous studies for gender prediction of Turkish OSN users have mostly been performed by using the content of tweets and Facebook comments. In this article, we propose using various features, not just user comments, for the gender prediction problem over the Facebook OSN. Unlike existing studies, we exploited features extracted from profile, wall content, and network structure, as well as wall interactions of the user. Therefore, our study differs from the existing work in the broadness of the features considered, machine learning and deep learning methods applied, and the size of the OSN dataset used in the experimental evaluation. Our results indicate that basic profile information provides better results; moreover, using this information together with wall interactions improves prediction quality. We measured the best accuracy value as 0.982, which was obtained by combining profile data and wall interactions of Turkish OSN users. In the wall interactions model, we introduced 34 different features that provide better results than the existing content-based studies for Turkish.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3164030792",
    "type": "article"
  },
  {
    "title": "Cross-lingual Text Reuse Detection Using Translation Plus Monolingual Analysis for English-Urdu Language Pair",
    "doi": "https://doi.org/10.1145/3473331",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Iqra Muneer; Rao Muhammad Adeel Nawab",
    "corresponding_authors": "",
    "abstract": "Cross-Lingual Text Reuse Detection (CLTRD) has recently attracted the attention of the research community due to a large amount of digital text readily available for reuse in multiple languages through online digital repositories. In addition, efficient machine translation systems are freely and readily available to translate text from one language into another, which makes it quite easy to reuse text across languages, and consequently difficult to detect it. In the literature, the most prominent and widely used approach for CLTRD is Translation plus Monolingual Analysis (T+MA). To detect CLTR for English-Urdu language pair, T+MA has been used with lexical approaches, namely, N-gram Overlap, Longest Common Subsequence, and Greedy String Tiling. This clearly shows that T+MA has not been thoroughly explored for the English-Urdu language pair. To fulfill this gap, this study presents an in-depth and detailed comparison of 26 approaches that are based on T+MA. These approaches include semantic similarity approaches (semantic tagger based approaches, WordNet-based approaches), probabilistic approach (Kullback-Leibler distance approach), monolingual word embedding-based approaches siamese recurrent architecture, and monolingual sentence transformer-based approaches for English-Urdu language pair. The evaluation was carried out using the CLEU benchmark corpus, both for the binary and the ternary classification tasks. Our extensive experimentation shows that our proposed approach that is a combination of 26 approaches obtained an F 1 score of 0.77 and 0.61 for the binary and ternary classification tasks, respectively, and outperformed the previously reported approaches [ 41 ] ( F 1 = 0.73) for the binary and ( F 1 = 0.55) for the ternary classification tasks) on the CLEU corpus.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3208597397",
    "type": "article"
  },
  {
    "title": "Multi-task Fuzzy Clustering–Based Multi-task TSK Fuzzy System for Text Sentiment Classification",
    "doi": "https://doi.org/10.1145/3476103",
    "publication_date": "2021-11-18",
    "publication_year": 2021,
    "authors": "Xiaoqing Gu; Kaijian Xia; Yizhang Jiang; Alireza Jolfaei",
    "corresponding_authors": "",
    "abstract": "Text sentiment classification is an important technology for natural language processing. A fuzzy system is a strong tool for processing imprecise or ambiguous data, and it can be used for text sentiment analysis. This article proposes a new formulation of a multi-task Takagi-Sugeno-Kang fuzzy system (TSK FS) modeling, which can be used for text sentiment image classification. Using a novel multi-task fuzzy c-means clustering algorithm, the common (public) information among all tasks and the individual (private) information for each task are extracted. The information about clustering, for example, cluster centers, can be used to learn the antecedent parameters of multi-task TSK fuzzy systems. With the common and individual antecedent parameters obtained, a corresponding multi-task learning mechanism for learning consequent parameters is devised. Accordingly, a multi-task fuzzy clustering–based multi-task TSK fuzzy system (MTFCM-MT-TSK-FS) is proposed. When the proposed model is built, the information conveyed by the fuzzy rules formed is two-fold, including (1) common fuzzy rules representing the inter-task correlation information and (2) individual fuzzy rules depicting the independent information of each task. The experimental results on several text sentiment datasets demonstrate the validity of the proposed model.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3215380716",
    "type": "article"
  },
  {
    "title": "A Deep Content-Based Model for Persian Rumor Verification",
    "doi": "https://doi.org/10.1145/3487289",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Zoleikha Jahanbakhsh-Nagadeh; Mohammad‐Reza Feizi‐Derakhshi; Arash Sharifi",
    "corresponding_authors": "",
    "abstract": "During the development of social media, there has been a transformation in social communication. Despite their positive applications in social interactions and news spread, it also provides an ideal platform for spreading rumors. Rumors can endanger the security of society in normal or critical situations. Therefore, it is important to detect and verify the rumors in the early stage of their spreading. Many research works have focused on social attributes in the social network to solve the problem of rumor detection and verification, while less attention has been paid to content features. The social and structural features of rumors develop over time and are not available in the early stage of rumor. Therefore, this study presented a content-based model to verify the Persian rumors on Twitter and Telegram early. The proposed model demonstrates the important role of content in spreading rumors and generates a better-integrated representation for each source rumor document by fusing its semantic, pragmatic, and syntactic information. First, contextual word embeddings of the source rumor are generated by a hybrid model based on ParsBERT and parallel CapsNets. Then, pragmatic and syntactic features of the rumor are extracted and concatenated with embeddings to capture the rich information for rumor verification. Experimental results on real-world datasets demonstrated that the proposed model significantly outperforms the state-of-the-art models in the early rumor verification task. Also, it can enhance the performance of the classifier from 2% to 11% on Twitter and from 5% to 23% on Telegram. These results validate the model's effectiveness when limited content information is available.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3217296320",
    "type": "article"
  },
  {
    "title": "Bilingual Semantic Role Labeling Inference via Dual Decomposition",
    "doi": "https://doi.org/10.1145/2835493",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Haitong Yang; Yu Zhou; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "This article focuses on bilingual Semantic Role Labeling (SRL); its goal is to annotate semantic roles on both sides of the parallel bilingual texts (bi-texts). Since rich bilingual information is encoded, bilingual SRL has been applied in many natural-language processing (NLP) tasks such as machine translation (MT), cross-lingual information retrieval (IR), and the like. A feasible way of performing bilingual SRL is using monolingual SRL systems to perform SRL on each side of bi-texts separately. However, it is difficult to obtain consistent SRL results on both sides of bi-texts in this way. Some works have tried to jointly infer bilingual SRL because there are many complementary language cues on both sides of bi-texts and they reported better performance than monolingual systems. However, there are two limits in the existing methods. First, the existing methods often require high inference costs due to the complex objective function. Second, the existing methods fully adopt the candidates generated by monolingual SRL systems, but many candidates are discarded in the argument pruning or identification stage of monolingual systems. In this article, we propose two strategies to overcome these limits. We utilize a simple but efficient technique: Dual Decomposition to search for consistent results for both sides of bi-texts. On the other hand, we propose a method called Bi-Directional Projection (BDP) to recover arguments discarded in monolingual SRL systems. We evaluate our method on a standard parallel benchmark: the OntoNotes dataset. The experimental results show that our method yields significant improvements over the state-of-the-art monolingual systems. In addition, our approach is also better and faster than existing methods due to BDP and Dual Decomposition.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2223700962",
    "type": "article"
  },
  {
    "title": "A Hybrid Ranking Approach to Chinese Spelling Check",
    "doi": "https://doi.org/10.1145/2822264",
    "publication_date": "2015-11-11",
    "publication_year": 2015,
    "authors": "Xiaodong Liu; Fei Cheng; Kevin Duh; Yūji Matsumoto",
    "corresponding_authors": "",
    "abstract": "We propose a novel framework for Chinese Spelling Check (CSC), which is an automatic algorithm to detect and correct Chinese spelling errors. Our framework contains two key components: candidate generation and candidate ranking . Our framework differs from previous research, such as Statistical Machine Translation (SMT) based model or Language Model (LM) based model, in that we use both SMT and LM models as components of our framework for generating the correction candidates, in order to obtain maximum recall; to improve the precision, we further employ a Support Vector Machines (SVM) classifier to rank the candidates generated by the SMT and the LM. Experiments show that our framework outperforms other systems, which adopted the same or similar resources as ours in the SIGHAN 7 shared task; even comparing with the state-of-the-art systems, which used more resources, such as a considerable large dictionary, an idiom dictionary and other semantic information, our framework still obtains competitive results. Furthermore, to address the resource scarceness problem for training the SMT model, we generate around 2 million artificial training sentences using the Chinese character confusion sets, which include a set of Chinese characters with similar shapes and similar pronunciations, provided by the SIGHAN 7 shared task.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2293187843",
    "type": "article"
  },
  {
    "title": "Reduplication in Assamese: Identification and Modeling",
    "doi": "https://doi.org/10.1145/3510419",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Dhrubajyoti Pathak; Sukumar Nandi; Priyankoo Sarmah",
    "corresponding_authors": "",
    "abstract": "Reduplication is a productive morphological process widely used in a substantial number of languages in the world. Reduplication is a well-studied phenomenon, and several typological works have provided evidence for different types of reduplication in most of the languages around the world. Addressing reduplication plays a vital role in the efficiency of POS tagger, sentiment analysis, as well as other NLP tasks. However, it is an understudied area in computational linguistics, especially in low-resource languages like Assamese. This article first describes different types of reduplication and their shapes that occur in Assamese. Second, an exhaustive set of reduplication formation rules is compiled that is incorporated to build a system to identify reduplication in Assamese text. The results of the experiments performed on three different domain datasets showed that the rule-based system can identify reduplicated expressions with an average precision, recall, and F1 scores of 94.19%, 98.07%, and 96.07%, respectively. Third, it is shown that the Assamese reduplication processes can be captured through a two-way finite-state transducer (2-way FST). Finally, two broad categories of reduplicative processes along with their corresponding 2-way FST model are presented.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4210784023",
    "type": "article"
  },
  {
    "title": "Linguistic Taboos and Euphemisms in Nepali",
    "doi": "https://doi.org/10.1145/3524111",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Nobal B. Niraula; Saurab Dulal; Diwa Koirala",
    "corresponding_authors": "",
    "abstract": "Languages across the world have words, phrases, and behaviors—the taboos—that are avoided in public communication considering them as obscene or disturbing to the social, religious, and ethical values of society. However, people deliberately use these linguistic taboos and other language constructs to make hurtful, derogatory, and obscene comments. It is nearly impossible to construct a universal set of offensive or taboo terms because offensiveness is determined entirely by different factors such as socio-physical setting, speaker-listener relationship, and word choices. In this article, we present a detailed corpus-based study of offensive language in Nepali. We identify and describe more than 18 different categories of linguistic offenses including politics, religion, race, and sex. We discuss 12 common euphemisms, such as synonym, metaphor, and circumlocution. In addition, we introduce a manually constructed dataset of more than 1,000 offensive and taboo terms popular among contemporary speakers. We describe the first experiments that provide baseline results in detecting offensive language in Nepali. This in-depth study of offensive language and resource will provide a foundation for several downstream tasks, such as offensive language detection and language learning.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4220652357",
    "type": "article"
  },
  {
    "title": "Effective College English Teaching Based on Teacher-student Interactive Model",
    "doi": "https://doi.org/10.1145/3486676",
    "publication_date": "2022-03-25",
    "publication_year": 2022,
    "authors": "Hui Fang; Hongmei Shi; Jiuzhou Zhang; Marimuthu Karuppiah",
    "corresponding_authors": "",
    "abstract": "English has become an utterly crucial device to take part in global verbal exchange and competition. It is essential to enhance English teaching's flexibility to meet the desires to improve the market economy. Therefore, powerful coaching strategies and language identification are considered challenging factors in existing methods. The proposed model includes hypothesized relationships among college students' conception of learning English, their perceptions of the study room environment, and their approaches to learning. They are examined using the Pre-trained Teacher–Student Fixed Interactive Model (PTSFIM). This model proposes a new way to develop the teaching process providing the baseline of record excellence towards a strategic performance control framework for an institute. The traditional strategies emphasize the benefits of the interactive approach and accentuate their effectiveness through Structural Multivariate Equation (SME) analysis in enhancing students' innovative thinking, research, and reasoning abilities. The reciprocal instructional analysis optimizes students' models to memorize for a longer duration. The evaluation of the study's outcomes suggests that interactive learning can assist college students that predict different results in participating inside the speech system and gain the best knowledge. The simulation analysis is performed based on accuracy, performance, and efficiency proves the reliability of the proposed framework.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4220974045",
    "type": "article"
  },
  {
    "title": "A Deep OCR for Degraded Bangla Documents",
    "doi": "https://doi.org/10.1145/3511807",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "Ayan Chaudhury; Partha Sarathi Mukherjee; Sudip Das; C. Biswas; Ujjwal Bhattacharya",
    "corresponding_authors": "",
    "abstract": "Despite the significant success of document image analysis techniques, efficient Optical Character Recognition (OCR) of degraded document images still remains an open problem. Although a body of work has been reported on degraded document recognition for English language, only little attention has been paid to Indic scripts. In this work, we focus on developing a degraded OCR for Bangla, a major Indian language. In general, an OCR system includes segmentation of the foreground text part from the background followed by recognition of the extracted text. The text segmentation module aims to assign the foreground or background label to each pixel of the document image. In this paper, we present a new OCR system which is particularly suitable for degraded quality Bangla document images. The contribution is two fold. In the first phase, we use a semi-supervised Markov Random Field (MRF)- based Generative Adversarial Network (GAN) model (which we call MRF-GAN ) for foreground segmentation of texts from degraded text. In the proposed MRF-GAN , we extend the concept of GAN to a multitask learning mechanism where discriminator-classifier networks differentiate between real/fake images and also assign a foreground or background label to each pixel. In the second phase, we propose to use a new encoder-decoder based recognizer that incorporates an attention-based character to a word prediction model, which has the capability of minimizing Word Error Rate (WER) . We optimize this network using a Multitask based Transfer Learning scheme (MTTL) . We perform experiments on a publicly available degraded Bangla document image dataset as well as on a new degraded printed Hindi document image dataset, which has been created as a part of the present study. Results of the experimentations demonstrate the efficacy of the proposed OCR.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4224305666",
    "type": "article"
  },
  {
    "title": "A Lemmatizer for Low-resource Languages: WSD and Its Role in the Assamese Language",
    "doi": "https://doi.org/10.1145/3502157",
    "publication_date": "2022-05-17",
    "publication_year": 2022,
    "authors": "Arjun Gogoi; Nomi Baruah",
    "corresponding_authors": "",
    "abstract": "The morphological variations of highly inflected languages that appear in a text impede the progress of computer processing and root word determination tasks while extracting an abstract. As a remedy to this difficulty, a lemmatization algorithm is developed, and its effectiveness is evaluated for Word Sense Disambiguation (WSD). Having observed its usefulness, lemmatizer is considered for developing Natural Language Processing tools for languages rich in morphological variations. Among various Indian highly inflected languages, Assamese, spoken by over 14 million people in the North-Eastern region of India, is also one of them. In this present work, after a detailed study on the possible transformations through which surface words are created from lemmas, we have designed an Assamese lemmatizer in such a manner that suitable reverse transformations can be employed on a surface word to derive the co-relative (similar) lemma back. And it has been observed that the lemmatizer is competent to deal with inflectional and derivational morphology in Assamese, and the same was evaluated on various Assamese articles extracted from the Assamese Corpus consisting of 50,000 surface words (excluding proper nouns), and the result that it yielded with 82% accuracy was quite encouraging and satisfying, as Assamese is a low-level language and no research work has been done in the Assamese language regarding the lemmatization of words. Considering the result obtained, the lemmatizer is then evaluated for Assamese WSD. For this purpose, 10 highly polysemous Assamese words are taken into account for sense disambiguation. We have also regarded varied WSD systems and observed that such systems enhance the effectiveness of all the WSD systems, which is statistically significant.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4280637746",
    "type": "article"
  },
  {
    "title": "Multi-Label Annotation and Classification of Arabic Texts Based on Extracted Seed Keyphrases and Bi-Gram Alphabet Feed Forward Neural Networks Model",
    "doi": "https://doi.org/10.1145/3539607",
    "publication_date": "2022-06-01",
    "publication_year": 2022,
    "authors": "Fatma El-Ghannam",
    "corresponding_authors": "Fatma El-Ghannam",
    "abstract": "In natural language processing, text classification is a fundamental problem. Multi-label classification of textual data is a challenging topic in text classification where an instance can be associated with more than one label. This paper presents a multi-label annotation and classification methodology for Arabic text data that is not currently classified as multi-label, aiming to analyze and compare the performance of various multi-label learning approaches. The current work includes two phases: The first involves automatic annotation of hotel reviews with more than one label based on the aspects found in the reviews. In this phase, review data instances were automatically annotated as multi-label based on the extracted seed keyphrases clusters. The second phase involves experiments to compare the performance of various multi-label classification learning methods. In this phase, we introduced different models including a feed-forward networks model that learns a vector representation based on the bi-gram alphabet rather than the commonly used bag-of-words model. The bi-gram alphabet vector representation model has the advantage of having reduced feature dimensions and not requiring natural language processing tools. The results indicated that employing the bi-gram alphabet vector representation feed forward neural network is a competitive solution for the multi-label text classification problem. It has achieved an accuracy of about 75.2%, and standard deviation (0.062).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4281771327",
    "type": "article"
  },
  {
    "title": "An Empirical Study on IPO Model Construction of Undergraduate Education Quality Evaluation in China from the Statistical Pattern Recognition Approach In NLP",
    "doi": "https://doi.org/10.1145/3543851",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Zhaojun Chen; Yanhai Bao; Tongxun Zhu",
    "corresponding_authors": "",
    "abstract": "Based on the analysis of 1,497 samples from the survey of national undergraduate educational administrators, the IPO model of undergraduate education quality evaluation from the perspective of managers can be effectively verified. The quality of higher education needs the accountability of higher education and evaluation of student learning outcomes. The empirical studies show the effects on the various dimensions of quality provisions which were not the same. The main findings are that the input of undergraduate education and teaching can not only directly and positively predict the output of undergraduate education and teaching, but also can positively predict the output of undergraduate education and teaching through the partial mediating effect of the process of undergraduate education and teaching. The research suggests that under the given material conditions, it is essential to enhance the “soft input” of undergraduate education and teaching, to strengthen the process construction of undergraduate education and teaching, to enhance the process quality control in the implementation of humanistic care, to pay attention to the development effect of teachers and students and the development effect of school-running characteristics, and to promote the connotative development of colleges and universities. The study provides a practical framework, model, and guidelines that can be used for undergraduate education institutions to evaluate and enhance the performance to effectively work in society. The quality evaluation of undergraduate education needs to focus from the quality of students' learning outcomes to the comprehensive consideration of “input-process-outcome”.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4292596001",
    "type": "article"
  },
  {
    "title": "Survey of Authorship Identification Tasks on Arabic Texts",
    "doi": "https://doi.org/10.1145/3564156",
    "publication_date": "2022-09-22",
    "publication_year": 2022,
    "authors": "Fatimah Alqahtani; Mischa Döhler",
    "corresponding_authors": "",
    "abstract": "Authorship identification is the process of extracting and analysing the writing styles of authors to identify the authorship. From the writing style, the author and his/her different characteristics can be recognised, which is very useful in digital forensics and cyber investigations. In the literature, authorship identification tasks were addressed on both long and short documents and performed on different languages, such as English, Arabic, Chinese, and Greek. This survey has reviewed the authorship identification tasks for the Arabic language to contribute to this area of research by exploring Arabic language performance and challenges. A total of 27 prominent Arabic studies of each authorship identification domain were reviewed considering the used data, selected features, utilised methods, and results. After a review of the various studies, it was concluded that the results of authorship identification tasks vary based on mostly the selected features and used dataset. Furthermore, the effective features differ from one dataset to another based on the various types of the Arabic language. However, all authorship identification tasks involving the Arabic language face considerable challenges with data pre-processing due to the challenging Arabic concatenative morphology.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4296614649",
    "type": "article"
  },
  {
    "title": "Semantic and Context Understanding for Sentiment Analysis in Hindi Handwritten Character Recognition Using a Multiresolution Technique",
    "doi": "https://doi.org/10.1145/3557895",
    "publication_date": "2022-10-06",
    "publication_year": 2022,
    "authors": "Ankit Kumar; Surbhi Bhatia; Mohammad R. Khosravi; Arwa Mashat; Parul Agarwal",
    "corresponding_authors": "",
    "abstract": "The rapid growth of Web 2.0, which enables people to generate, communicate, and share information, has resulted in an increase in the total number of users. In developing countries, online users’ sentiment influences decision-making, social views, individual consumption decisions, and entity quality monitoring. As a result, more accurate sentiment analysis, particularly in their native language such as Hindi, is preferred over crude binary categorization. This is because of the abundance of web-based data in Indian languages such as Hindi, Marathi, Kannada, Tamil, and so on. Analyzing this data and recovering valuable and relevant information from handwritten text has become extremely important. Despite years of research and development, no optical writing recognition (OCR) system has ever been certified as completely reliable. The first step in any pattern recognition system is feature selection. In many fields, feature selection is studied as a combinatorial optimization problem. The primary goal of feature selection is to reduce the number of redundant and ineffective traits in the recognition system. This feature selection is used to maintain or improve the performance of the classifier used by the recognition system: A support vector machine (SVM) technique could be used to solve this character recognition problem. The Hindi character recognition system recognizes Hindi characters by employing morphological operations, edge detection, HOG feature extraction, and an SVM-based classifier. The proposed model outperformed the current state-of-the-art method, achieving an accuracy of 96.77%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4303185326",
    "type": "article"
  },
  {
    "title": "Chinese Grammatical Error Correction Using Pre-trained Models and Pseudo Data",
    "doi": "https://doi.org/10.1145/3570209",
    "publication_date": "2022-11-02",
    "publication_year": 2022,
    "authors": "Hongfei Wang; Michiki Kurosawa; Satoru Katsumata; Masato Mita; Mamoru Komachi",
    "corresponding_authors": "",
    "abstract": "In recent studies, pre-trained models and pseudo data have been key factors in improving the performance of the English grammatical error correction (GEC) task. However, few studies have examined the role of pre-trained models and pseudo data in the Chinese GEC task. Therefore, we develop Chinese GEC models based on three pre-trained models: Chinese BERT, Chinese T5, and Chinese BART, and then incorporate these models with pseudo data to determine the best configuration for the Chinese GEC task. On the natural language processing and Chinese computing (NLPCC) 2018 GEC shared task test set, all our single models outperform the ensemble models developed by the top team of the shared task. Chinese BART achieves an F score of 37.15, which is a state-of-the-art result. We then combine our Chinese GEC models with three kinds of pseudo data: Lang-8 (MaskGEC), Wiki (MaskGEC), and Wiki (Backtranslation). We find that most models can benefit from pseudo data, and BART+Lang-8 (MaskGEC) is the ideal setting in terms of accuracy and training efficiency. The experimental results demonstrate the effectiveness of the pre-trained models and pseudo data on the Chinese GEC task and provide an easily reproducible and adaptable baseline for future works. Finally, we annotate the error types of the development data; the results show that word-level errors dominate all error types, and word selection errors must be addressed even when using pre-trained models and pseudo data. Our codes are available at https://github.com/wang136906578/BERT-encoder-ChineseGEC .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4307923067",
    "type": "article"
  },
  {
    "title": "A Multi-View Learning Approach for Detecting Personality Disorders Among Arab Social Media Users",
    "doi": "https://doi.org/10.1145/3572906",
    "publication_date": "2022-12-02",
    "publication_year": 2022,
    "authors": "Rehab Duwairi; Zain Halloush",
    "corresponding_authors": "",
    "abstract": "Multi-view fusion approaches have gained increasing interest in the past few years by researchers. This interest comes due to the many perspectives that datasets can be looked at and evaluated. One of the most urging areas that require constant leveraging with latest technologies and multi-perspective judgments is the area of psychology. In this article, a novel multi-view fusion model using deep learning algorithms is presented to detect popular types of personality disorders among Arab users of the Twitter platform in an expert-driven fashion, based on the descriptions of the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition. To the best of our knowledge, the work presented is the first of its kind with no publicly available datasets that report statements around personality disorders in the Arabic language, and thus we created AraPerson, a dataset that consists of 8,000 textual tweets coupled with 8,000 images that prescribe mental statuses for a total of 150 users collected with regular expressions generated under the supervision of domain experts. The dataset was fed into a baseline multi-view model by combining a CNN model with a Bi-LSTM model to detect two types of popular personality disorders by analyzing textual and visual posts on 150 user profiles. The experiments were followed with fusing the DenseNet model with the Bi-LSTM model, experimenting with the effect of using concatenation, addition, and multiplication methods for vectors’ combination. The work presented in this article is unprecedented, specifically in a controversial area such as personality disorders detection among Arab communities. The best reported accuracy is 87%, which is quite promising, as the two types of personality disorders investigated are highly overlapping.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4311119644",
    "type": "article"
  },
  {
    "title": "Exploring Web-based Translation Resources Applied to Hindi-English Cross-Lingual Information Retrieval",
    "doi": "https://doi.org/10.1145/3569010",
    "publication_date": "2022-12-06",
    "publication_year": 2022,
    "authors": "Vijay Kumar Sharma; Namita Mittal; Ankit Vidyarthi; Deepak Gupta",
    "corresponding_authors": "",
    "abstract": "Internet users perceive a multilingual web but are unfamiliar with it due to communication in their regional language called Cross-Lingual Information Retrieval (CLIR). In CLIR, a translation technique is used to translate the user queries into the target document’s language. Conventional translation techniques are based on either a manual dictionary or a parallel corpus, whereas the trending Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) techniques are trained on a parallel corpus. NMT is not so mature for Hindi-English translation, according to the literature, and SMT performs better than the NMT. SMT provides a static translation due to the limited vocabularies in the available parallel corpus. It may not provide the translations for missing or unseen words, whereas the web provides a dynamic interface where multiple users are updating information at the same time. The web may provide the translations for missing or unseen words, and therefore the web is effectively used for technically developed languages like English, German, Spanish, Russian, and Chinese. In this article, different web resources such as Wikipedia, Hindi WordNet and Indo WordNet, ConceptNet, and online dictionary based translation techniques are proposed and applied to Hindi-English CLIR. Wikipedia-based translation approach incorporates three modules—exactly matched, partially matched, and disambiguation—to address the issues of wrong inter-wiki links, partially matched terms, and ambiguous articles. Hindi WordNet and Indo WorNet attribute “English synset” and ConceptNet attributes “Related term” &amp; “Synonymy” are used for obtaining translations. Further, WordNet path similarity is used to disambiguate translations. Various online dictionaries are available that return multiple relevant and irrelevant translations. The proposed approaches are compared to the SMT where the Wikipedia-based approach achieves approximately similar mean average precision to SMT.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4311644716",
    "type": "article"
  },
  {
    "title": "Linguistic-Relationships-Based Approach for Improving Word Alignment",
    "doi": "https://doi.org/10.1145/3133323",
    "publication_date": "2017-10-14",
    "publication_year": 2017,
    "authors": "Phuoc Tran; Điền Đinh; Le Thanh Tan; Long Nguyen",
    "corresponding_authors": "",
    "abstract": "The unsupervised word alignments (such as GIZA++) are widely used in the phrase-based statistical machine translation. The quality of the model is proportional to the size and the quality of the bilingual corpus. However, for low-resource language pairs such as Chinese and Vietnamese, a result of unsupervised word alignment sometimes is of low quality due to the sparse data. In addition, this model does not take advantage of the linguistic relationships to improve performance of word alignment. Chinese and Vietnamese have the same language type and have close linguistic relationships. In this article, we integrate the characteristics of linguistic relationships into the word alignment model to enhance the quality of Chinese-Vietnamese word alignment. These linguistic relationships are Sino-Vietnamese and content word. The experimental results showed that our method improved the performance of word alignment as well as the quality of machine translation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2765190181",
    "type": "article"
  },
  {
    "title": "Response Selection and Automatic Message-Response Expansion in Retrieval-Based QA Systems using Semantic Dependency Pair Model",
    "doi": "https://doi.org/10.1145/3229184",
    "publication_date": "2018-11-12",
    "publication_year": 2018,
    "authors": "Ming-Hsiang Su; Chung‐Hsien Wu; Kun-Yi Huang; Wu-Hsuan Lin",
    "corresponding_authors": "",
    "abstract": "This article presents an approach to response selection and message-response (MR) database expansion from the unstructured data on the psychological consultation websites for a retrieval-based question answering (QA) system in a constrained domain for emotional support and comforting. First, we manually construct an initial MR database based on the articles collected from the psychological consultation websites. The Chinese Knowledge and Information Processing probabilistic context-free grammar is adopted to obtain the semantic dependency graphs (SDGs) of all the messages and responses in the initial MR database. For each sentence in the MR database, all the semantic dependencies, each composed of two words and their semantic relation, are extracted from the SDG of the sentence to form a semantic dependency set. Finally, a matrix with the element representing the correlation between the semantic dependencies of the messages and their corresponding responses is constructed as a semantic dependency pair model (SDPM) for response selection. Moreover, as the number of MR pairs in the psychological consultation websites is increasing day by day, the MR database in the QA system should be expanded to meet the needs of the users. For MR database expansion, the unstructured data from the message board are automatically collected. For the collected data, the supervised latent Dirichlet allocation is adopted for event detection and then the event-based delta Bayesian Information Criterion is used for message and response article segmentation. Each extracted message segment is then fed to the constructed retrieval-based QA system to find the best matched response segment and the matching score is also estimated to verify if the new MR pair is suitable to be included in the expanded MR database. Fivefold cross validation was employed to evaluate the performance of the proposed retrieval-based QA system over the expanded MR database based on SDPM. Compared to the vector space model-based method, the Okapi BM25 model, and the deep learning-based sequence-to-sequence with attention model, the proposed approach achieved a more favorable performance according to a statistical significance test. The retrieval accuracy based on MR expansion was also evaluated and a satisfactory result was obtained confirming the effectiveness of the expanded MR database. In addition, the user's satisfaction score of the proposed system was evaluated using the Cronbach's alpha value and the satisfaction score of the proposed SDPM was higher than those of the methods for comparison.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2901664644",
    "type": "article"
  },
  {
    "title": "A Neural Semantic Parser for Math Problems Incorporating Multi-Sentence Information",
    "doi": "https://doi.org/10.1145/3314939",
    "publication_date": "2019-05-09",
    "publication_year": 2019,
    "authors": "Ruiyong Sun; Yijia Zhao; Qi Zhang; Keyu Ding; Shijin Wang; Wei Cui",
    "corresponding_authors": "",
    "abstract": "In this article, we study the problem of parsing a math problem into logical forms. It is an essential pre-processing step for automatically solving math problems. Most of the existing studies about semantic parsing mainly focused on the single-sentence level. However, for parsing math problems, we need to take the information of multiple sentences into consideration. To achieve the task, we formulate the task as a machine translation problem and extend the sequence-to-sequence model with a novel two-encoder architecture and a word-level selective mechanism. For training and evaluating the proposed method, we construct a large-scale dataset. Experimental results show that the proposed two-encoder architecture and word-level selective mechanism could bring significant improvement. The proposed method can achieve better performance than the state-of-the-art methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2944472252",
    "type": "article"
  },
  {
    "title": "Multi-task Stack Propagation for Neural Quality Estimation",
    "doi": "https://doi.org/10.1145/3321127",
    "publication_date": "2019-05-21",
    "publication_year": 2019,
    "authors": "Hyun Kim; Jong-Hyeok Lee; Seung‐Hoon Na",
    "corresponding_authors": "",
    "abstract": "Quality estimation is an important task in machine translation that has attracted increased interest in recent years. A key problem in translation-quality estimation is the lack of a sufficient amount of the quality annotated training data. To address this shortcoming, the Predictor-Estimator was proposed recently by introducing “word prediction” as an additional pre-subtask that predicts a current target word with consideration of surrounding source and target contexts, resulting in a two-stage neural model composed of a predictor and an estimator . However, the original Predictor-Estimator is not trained on a continuous stacking model but instead in a cascaded manner that separately trains the predictor from the estimator. In addition, the Predictor-Estimator is trained based on single-task learning only, which uses target-specific quality-estimation data without using other training data that are available from other-level quality-estimation tasks. In this article, we thus propose a multi-task stack propagation , which extensively applies stack propagation to fully train the Predictor-Estimator on a continuous stacking architecture and multi-task learning to enhance the training data from related other-level quality-estimation tasks. Experimental results on WMT17 quality-estimation datasets show that the Predictor-Estimator trained with multi-task stack propagation provides statistically significant improvements over the baseline models. In particular, under an ensemble setting, the proposed multi-task stack propagation leads to state-of-the-art performance at all the sentence/word/phrase levels for WMT17 quality estimation tasks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2945797851",
    "type": "article"
  },
  {
    "title": "Chinese Zero Pronoun Resolution",
    "doi": "https://doi.org/10.1145/3321129",
    "publication_date": "2019-06-05",
    "publication_year": 2019,
    "authors": "Fang Kong; Min Zhang; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Chinese zero pronoun (ZP) resolution plays a critical role in discourse analysis. Different from traditional mention-to-mention approaches, this article proposes a chain-to-chain approach to improve the performance of ZP resolution in three aspects. First, consecutive ZPs are clustered into coreferential chains, each working as one independent anaphor as a whole. In this way, those ZPs far away from their overt antecedents can be bridged via other consecutive ZPs in the same coreferential chains and thus better resolved. Second, common noun phrases (NPs) are automatically grouped into coreferential chains using traditional approaches, each working as one independent antecedent candidate as a whole. That is, those NPs occurring in the same coreferential chain are viewed as one antecedent candidate as a whole, and ZP resolution is made between ZP coreferential chains and common NP coreferential chains. In this way, the performance can be much improved due to the effective reduction of the search space by pruning singletons and negative instances. Third and finally, additional features from ZP and common NP coreferential chains are employed to better represent anaphors and their antecedent candidates, respectively. Comprehensive experiments on the OntoNotes V5.0 corpus show that our chain-to-chain approach significantly outperforms the state-of-the-art mention-to-mention approaches. To our knowledge, this is the first work to resolve zero pronouns in a chain-to-chain way.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2952419824",
    "type": "article"
  },
  {
    "title": "Children’s Story Classification in Indian Languages Using Linguistic and Keyword-based Features",
    "doi": "https://doi.org/10.1145/3342356",
    "publication_date": "2019-11-26",
    "publication_year": 2019,
    "authors": "D M Harikrishna; Kanishka Rao",
    "corresponding_authors": "",
    "abstract": "The primary objective of this work is to classify Hindi and Telugu stories into three genres: fable, folk-tale, and legend . In this work, we are proposing a framework for story classification (SC) using keyword and part-of-speech (POS) features. For improving the performance of SC system, feature reduction techniques and combinations of various POS tags are explored. Further, we investigated the performance of SC by dividing the story into parts depending on its semantic structure. In this work, stories are (i) manually divided into parts based on their semantics as introduction, main, and climax ; and (ii) automatically divided into equal parts based on number of sentences in a story as initial, middle, and end . We have also examined sentence increment model, which aims at determining an optimum number of sentences required to identify story genre by incremental selection of sentences in a story. Experiments are conducted on Hindi and Telugu story corpora consisting of 300 and 150 short stories, respectively. The performance of SC system is evaluated using different combinations of keyword and POS-based features, with three well-established machine learning classifiers: (i) Naive Bayes (NB), (ii) k-Nearest Neighbour (KNN), and (iii) Support Vector Machine (SVM). Performance of the classifier is evaluated using 10-fold cross-validation and effectiveness of classifier is measured using precision, recall, and F-measure. From the classification results, it is observed that adding linguistic information boosts the performance of story classification. In view of the structure of the story, main, and initial parts of the story have shown comparatively better performance. The results from the sentence incremental model have indicated that the first nine and seven sentences in Hindi and Telugu stories, respectively, are sufficient for better classification of stories. In most of the studies, SVM models outperformed the other models in classification accuracy.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2990124320",
    "type": "article"
  },
  {
    "title": "Isarn Dharma Word Segmentation Using a Statistical Approach with Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3359990",
    "publication_date": "2020-02-22",
    "publication_year": 2020,
    "authors": "Sittichai Somsap; Pusadee Seresangtakul",
    "corresponding_authors": "",
    "abstract": "In this study, we developed an Isarn Dharma word segmentation system. We mainly focused on solving the word ambiguity and unknown word problems in unsegmented Isarn Dharma text. Ambiguous Isarn Dharma words occur frequently in word construction due to the writing style without tone markers. Thus, words can be interpreted as having different tones and meanings in the same writing text. To overcome these problems, we developed an Isarn Dharma character cluster–(IDCC) based statistical model and affixation and integrated it with the named entity recognition method (IDCC-C-based statistical model and affixation with named entity recognition (NER)). This method integrates the IDCC-based and character-based statistical models to distinguish the word boundaries. The IDCC-based statistical model utilizes the IDCC feature to disambiguate any ambiguous words. The unknown words are handled using the character-based statistical model, based on the character features. In addition, linguistic knowledge is employed to detect the boundaries of a new word based on the construction morphology and NER. In evaluations, we compared the proposed method with various word segmentation methods. The experimental results showed that the proposed method performed slightly better than the other methods when the corpus size increased. Using the test set, the proposed method obtained the best F-measure of 92.19, an F-measure that was better than the IDCC longest matching grouping at 2.85.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3009619352",
    "type": "article"
  },
  {
    "title": "Enhanced Double-Carrier Word Embedding via Phonetics and Writing",
    "doi": "https://doi.org/10.1145/3344920",
    "publication_date": "2020-01-15",
    "publication_year": 2020,
    "authors": "Wenhao Zhu; Xin Jin; Shuang Liu; Zhiguo Lu; Wu Zhang; Ke Yan; Baogang Wei",
    "corresponding_authors": "",
    "abstract": "Word embeddings, which map words into a unified vector space, capture rich semantic information. From a linguistic point of view, words have two carriers, speech and writing. Yet the most recent word embedding models focus on only the writing carrier and ignore the role of the speech carrier in semantic expressions. However, in the development of language, speech appears before writing and plays an important role in the development of writing. For phonetic language systems, the written forms are secondary symbols of spoken ones. Based on this idea, we carried out our work and proposed double-carrier word embedding (DCWE). We used DCWE to conduct a simulation of the generation order of speech and writing. We trained written embedding based on phonetic embedding. The final word embedding fuses writing and phonetic embedding. To illustrate that our model can be applied to most languages, we selected Chinese, English, and Spanish as examples and evaluated these models through word similarity and text classification experiments.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3016795621",
    "type": "article"
  },
  {
    "title": "Towards Integrated Classification Lexicon for Handling Unknown Words in Chinese-Vietnamese Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3373267",
    "publication_date": "2020-04-07",
    "publication_year": 2020,
    "authors": "Wanjin Che; Zhengtao Yu; Zhiqiang Yu; Yonghua Wen; Junjun Guo",
    "corresponding_authors": "",
    "abstract": "In Neural Machine Translation (NMT), due to the limitations of the vocabulary, unknown words cannot be translated properly, which brings suboptimal performance of the translation system. For resource-scarce NMT that have small-scale training corpus, the effect is amplified. The traditional approach of amplifying the scale of the corpus is not applicable, because the parallel corpus is difficult to obtain in a resource-scarce setting; however, it is easy to obtain and utilize external knowledge, bilingual lexicon, and other resources. Therefore, we propose classification lexicon approach for processing unknown words in the Chinese-Vietnamese NMT task. Specifically, three types of unknown Chinese-Vietnamese words are classified and their corresponding classification lexicon are constructed by word alignment, Wikipedia extraction, and rule-based methods, respectively. After translation, the unknown words are restored by lexicon for post-processing. Experiment results on Chinese-Vietnamese, English-Vietnamese, and Mongolian-Chinese translations show that our approach significantly improves the accuracy and the performance of NMT especially in a resource-scarce setting.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3030168743",
    "type": "article"
  },
  {
    "title": "Conducting Natural Language Inference with Word-Pair-Dependency and Local Context",
    "doi": "https://doi.org/10.1145/3377704",
    "publication_date": "2020-02-20",
    "publication_year": 2020,
    "authors": "Qianlong Du; Chengqing Zong; Keh‐Yih Su",
    "corresponding_authors": "",
    "abstract": "This article proposes to conduct natural language inference with novel Enhanced-Relation-Head-Dependent triplets (RHD triplets) , which are constructed via enhancing each word in the RHD triplet with its associated local context. Most previous approaches based on deep neural network (DNN) for this task either perform token alignment without considering syntactic dependency among words, or directly use tree- LSTM to generate passage representation with irrelevant information. To improve token alignment and inference judgment with word-pair-dependency, the RHD triplet structure is first proposed. To avoid incorporating irrelevant information, this proposed approach performs comparison directly on each triplet-pair of the given passage-pair (instead of comparing each triplet in a passage with the content merged from the whole opposite passage). Furthermore, to take local context into consideration while conducting token alignment and inference judgment, we also enhance the words of the triplets with their associated local context to improve the performance. Experimental results show that the proposed approach is better than most previous approaches that adopt tree structures, and its performance is comparable to other state-of-the-art approaches (however, our approach is more human comprehensible).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3045601975",
    "type": "article"
  },
  {
    "title": "A Unified Model for Solving the OOV Problem of Chinese Word Segmentation",
    "doi": "https://doi.org/10.1145/2699940",
    "publication_date": "2015-06-12",
    "publication_year": 2015,
    "authors": "Xiaoqing Li; Chengqing Zong; Keh‐Yih Su",
    "corresponding_authors": "",
    "abstract": "This article proposes a unified, character-based, generative model to incorporate additional resources for solving the out-of-vocabulary (OOV) problem of Chinese word segmentation, within which different types of additional information can be utilized independently in corresponding submodels. This article mainly addresses the following three types of OOV: unseen dictionary words, named entities, and suffix-derived words, none of which are handled well by current approaches. The results show that our approach can effectively improve the performance of the first two types with positive interaction in F-score. Additionally, we also analyze reason that suffix information is not helpful. After integrating the proposed generative model with the corresponding discriminative approach, our evaluation on various corpora---including SIGHAN-2005, CIPS-SIGHAN-2010, and the Chinese Treebank (CTB)---shows that our integrated approach achieves the best performance reported in the literature on all testing sets when additional information and resources are allowed.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2198517985",
    "type": "article"
  },
  {
    "title": "A Fast and Compact Language Model Implementation Using Double-Array Structures",
    "doi": "https://doi.org/10.1145/2873068",
    "publication_date": "2016-04-29",
    "publication_year": 2016,
    "authors": "Jun-ya Norimatsu; M. Yasuhara; Toru Tanaka; Mikio Yamamoto",
    "corresponding_authors": "",
    "abstract": "The language model is a widely used component in fields such as natural language processing, automatic speech recognition, and optical character recognition. In particular, statistical machine translation uses language models, and the translation speed and the amount of memory required are greatly affected by the performance of the language model implementation. We propose a fast and compact implementation of n -gram language models that increases query speed and reduces memory usage by using a double-array structure, which is known to be a fast and compact trie data structure. We propose two types of implementation: one for backward suffix trees and the other for reverse tries. The data structure is optimized for space efficiency by embedding model parameters into otherwise unused spaces in the double-array structure. We show that the reverse trie version of our method is among the smallest state-of-the-art implementations in terms of model size with almost the same speed as the implementation that performs fastest on perplexity calculation tasks. Similarly, we achieve faster decoding while keeping compact model sizes, and we confirm that our method can utilize the efficiency of the double-array structure to achieve a balance between speed and size on translation tasks.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2343984634",
    "type": "article"
  },
  {
    "title": "Combination of Loss-based Active Learning and Semi-supervised Learning for Recognizing Entities in Chinese Electronic Medical Records",
    "doi": "https://doi.org/10.1145/3588314",
    "publication_date": "2023-03-20",
    "publication_year": 2023,
    "authors": "Jinghui Yan; Chengqing Zong; Jinan Xu",
    "corresponding_authors": "",
    "abstract": "The recognition of entities in an electronic medical record (EMR) is especially important to downstream tasks, such as clinical entity normalization and medical dialogue understanding. However, in the medical professional field, training a high-quality named entity recognition system always requires large-scale annotated datasets, which are highly expensive to obtain. In this article, to lower the cost of data annotation and maximizing the use of unlabeled data, we propose a hybrid approach to recognizing the entities in Chinese electronic medical record, which is in combination of loss-based active learning and semi-supervised learning. Specifically, we adopted a dynamic balance strategy to dynamically balance the minimum loss predicted by a named entity recognition decoder and a loss prediction module at different stages in the process. Experimental results demonstrated our proposed framework’s effectiveness and efficiency, achieving higher performances than existing approaches on Chinese EMR entity recognition datasets under limited labeling resources.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4327950399",
    "type": "article"
  },
  {
    "title": "WAD-X: Improving Zero-shot Cross-lingual Transfer via Adapter-based Word Alignment",
    "doi": "https://doi.org/10.1145/3610289",
    "publication_date": "2023-07-19",
    "publication_year": 2023,
    "authors": "Ahtamjan Ahmat; Yating Yang; Bo Ma; Rui Dong; Kaiwen Lu; Lei Wang",
    "corresponding_authors": "",
    "abstract": "Multilingual pre-trained language models (mPLMs) have achieved remarkable performance on zero-shot cross-lingual transfer learning. However, most mPLMs implicitly encourage cross-lingual alignment in pre-training stage, making it hard to capture accurate word alignment across languages. In this paper, we propose Word-align ADapters for Cross-lingual transfer (WAD-X) to explicitly align word representations of mPLMs via language-specific subspace. Taking a mPLM as the backbone model, WAD-X constructs subspace for each source-target language pair via adapters. The adapters use statistical alignment as the prior knowledge to guide word-level aligning in the corresponding bilingual semantic subspace. We evaluate our model across a set of target languages on three zero-shot cross-lingual transfer tasks: part-of-speech tagging (POS), dependency parsing (DP), and sentiment analysis (SA). Experimental results demonstrate that our proposed model improves zero-shot cross-lingual transfer on three benchmarks, with improvements of 2.19, 2.50, and 1.61 points in POS, DP, and SA tasks over strong baselines.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4384825071",
    "type": "article"
  },
  {
    "title": "Text Classification Based on Natural Language Processing and Machine Learning in Multi-Label Corpus",
    "doi": "https://doi.org/10.1145/3617831",
    "publication_date": "2023-08-29",
    "publication_year": 2023,
    "authors": "Haitao Yu; Feng Xiong; Zong-Jin Chen",
    "corresponding_authors": "",
    "abstract": "The rapid development of the Internet has led to a geometric expansion of text information resources online. Among them, corpus, as the basic data source of natural language processing based on statistical language model, its construction and application have become a hot issue in current language processing research. After consulting a large number of relevant literature and materials, it was found that many researchers have provided new ideas for multi label corpus text classification methods. However, this article adds its own understanding and takes this as the direction and basis. In the introduction, the research significance of text classification was introduced, and then academic research and analysis were carried out on the two key sentences of corpus text classification and natural language processing in multi-tag corpus text classification. This article then utilizes an algorithm model to provide a theoretical basis for the study of multi-label corpus text classification methods; At the end of this article, a simulation comparative experiment is conducted, and the experiment is summarized and discussed; In the Enterprise L corpus, the difference in recall rates before and after the use of Entrance 1 was 5.5%, the difference in recall rates before and after the use of Entrance 2 was 7.8%, the difference in recall rates before and after the use of Entrance 3 was 3.3%, and the difference in recall rates before and after the use of Entrance 4 was 4.5%. At the same time, with the continuous research of natural language processing and machine learning, the research on text classification methods of multi tag corpus is also facing new opportunities and challenges.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386246396",
    "type": "article"
  },
  {
    "title": "Data Augmentation based Cross-Lingual Multi-Speaker TTS using DL with Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3628428",
    "publication_date": "2023-10-23",
    "publication_year": 2023,
    "authors": "B. Lalitha; V Madhurima; Nandakrishna Ch; Jampani Satish Babu; J. N. Chandra Sekhar; Patlolla Venkat Reddy",
    "corresponding_authors": "",
    "abstract": "Text to S peech (TTS) algorithms have made tremendous strides in recent years in terms of their ability to generate speech in a single language that sounds as natural as possible. However, because to a lack of available training data, the synthesis of speech from the same person in various languages continues to be difficult. It might be challenging to locate people who are proficient in numerous languages at a level equivalent to native speakers. Voice conversion is one method that may be used to create a polyglot corpus, which can then be used to solve this problem. This entails making use of a voice representation model that has been trained on 53 different languages through the application of hybrid deep learning in order to capture speaker-invariant qualities. In this study, we present a novel approach for the conversion of voices across different languages by employing Generated Adversarial Networks (GANs) to train a multilingual TTS system. The concept of individual likeness loss in order to address the special difficulty of maintaining one's individual speaking identity during the training process can be offered. This work is focused to provide the impression that voice data coming from a variety of languages and speakers was produced by the same individual, and one way to do so is by using this word. In order to determine the extent to which our model is useful, two experiments that compared it against benchmarks that made use of varying degrees of parameter sharing between languages are carried out. The purpose of these experiments was to evaluate the accuracy of pronunciation as well as the caliber of the synthetic voice during transitions between different languages.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387869282",
    "type": "article"
  },
  {
    "title": "Memorizing All for Implicit Discourse Relation Recognition",
    "doi": "https://doi.org/10.1145/3485016",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Kashif Munir; Hongxiao Bai; Hai Zhao; Junhan Zhao",
    "corresponding_authors": "",
    "abstract": "Implicit discourse relation recognition is a challenging task due to the absence of the necessary informative clues from explicit connectives. An implicit discourse relation recognizer has to carefully tackle the semantic similarity of sentence pairs and the severe data sparsity issue. In this article, we learn token embeddings to encode the structure of a sentence from a dependency point of view in their representations and use them to initialize a baseline model to make it really strong. Then, we propose a novel memory component to tackle the data sparsity issue by allowing the model to master the entire training set, which helps in achieving further performance improvement. The memory mechanism adequately memorizes information by pairing representations and discourse relations of all training instances, thus filling the slot of the data-hungry issue in the current implicit discourse relation recognizer. The proposed memory component, if attached with any suitable baseline, can help in performance enhancement. The experiments show that our full model with memorizing the entire training data provides excellent results on PDTB and CDTB datasets, outperforming the baselines by a fair margin.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2970973841",
    "type": "article"
  },
  {
    "title": "A BERT-Based Two-Stage Model for Chinese Chengyu Recommendation",
    "doi": "https://doi.org/10.1145/3453185",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Minghuan Tan; Jing Jiang; Bing Tian Dai",
    "corresponding_authors": "",
    "abstract": "In Chinese, Chengyu are fixed phrases consisting of four characters. As a type of idioms, their meanings usually cannot be derived from their component characters. In this article, we study the task of recommending a Chengyu given a textual context. Observing some of the limitations with existing work, we propose a two-stage model, where during the first stage we re-train a Chinese BERT model by masking out Chengyu from a large Chinese corpus with a wide coverage of Chengyu. During the second stage, we fine-tune the re-trained, Chengyu-oriented BERT on a specific Chengyu recommendation dataset. We evaluate this method on ChID and CCT datasets and find that it can achieve the state of the art on both datasets. Ablation studies show that both stages of training are critical for the performance gain.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3144320030",
    "type": "article"
  },
  {
    "title": "Chinese Spelling Error Detection Using a Fusion Lattice LSTM",
    "doi": "https://doi.org/10.1145/3426882",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Hao Wang; Bin Wang; Jianyong Duan; Jiajun Zhang",
    "corresponding_authors": "",
    "abstract": "Spelling error detection serves as a crucial preprocessing in many natural language processing applications. Unlike English, where every single word is directly typed by keyboard, we have to use an input method to input Chinese characters. The pinyin input method is the most widely used. By intuition, pinyin should be helpful in detecting spelling errors. However, when detect spelling errors, most of the current methods ignore the pinyin information and adopt a pipeline framework that leads to error propagation. In this article, we propose a fusion lattice-LSTM model under the end-to-end framework to integrate character, word, and pinyin features for error detection. Experiments on the SIGHAN Bake-off-2015 dataset show that pinyin is a discriminating feature, and our end-to-end model outperforms the baseline models obviously.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3159821795",
    "type": "article"
  },
  {
    "title": "Reinforced NMT for Sentiment and Content Preservation in Low-resource Scenario",
    "doi": "https://doi.org/10.1145/3450970",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Divya Kumari; Asif Ekbal; Rejwanul Haque; Pushpak Bhattacharyya; Andy Way",
    "corresponding_authors": "",
    "abstract": "The preservation of domain knowledge from source to the target is crucial in any translation workflows. Hence, translation service providers that use machine translation (MT) in production could reasonably expect that the translation process should transfer both the underlying pragmatics and the semantics of the source-side sentences into the target language. However, recent studies suggest that the MT systems often fail to preserve such crucial information (e.g., sentiment, emotion, gender traits) embedded in the source text in the target. In this context, the raw automatic translations are often directly fed to other natural language processing (NLP) applications (e.g., sentiment classifier) in a cross-lingual platform. Hence, the loss of such crucial information during the translation could negatively affect the performance of such downstream NLP tasks that heavily rely on the output of the MT systems. In our current research, we carefully balance both the sides (i.e., sentiment and semantics) during translation, by controlling a global-attention-based neural MT (NMT), to generate translations that encode the underlying sentiment of a source sentence while preserving its non-opinionated semantic content. Toward this, we use a state-of-the-art reinforcement learning method, namely, actor-critic , that includes a novel reward combination module, to fine-tune the NMT system so that it learns to generate translations that are best suited for a downstream task, viz. sentiment classification while ensuring the source-side semantics is intact in the process. Experimental results for Hindi–English language pair show that our proposed method significantly improves the performance of the sentiment classifier and alongside results in an improved NMT system.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3173759002",
    "type": "article"
  },
  {
    "title": "Bi-directional Long Short-Term Memory Model with Semantic Positional Attention for the Question Answering System",
    "doi": "https://doi.org/10.1145/3439800",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Mingwen Bi; Qingchuan Zhang; Min Zuo; Zelong Xu; Qingyu Jin",
    "corresponding_authors": "",
    "abstract": "The intelligent question answering system aims to provide quick and concise feedback on the questions of users. Although the performance of phrase-level and numerous attention models have been improved, the sentence components and position information are not emphasized enough. This article combines Ci-Lin and word2vec to divide all of the words in the question-answer pairs into groups according to the semantics and select one kernel word in each group. The remaining words are common words and realize the semantic mapping mechanism between kernel words and common words. With this Chinese semantic mapping mechanism, the common words in all questions and answers are replaced by the semantic kernel words to realize the normalization of the semantic representation. Meanwhile, based on the bi-directional LSTM model, this article introduces a method of the combination of semantic role labeling and positional context, dividing the sentence into multiple semantic segments according to semantic logic. The weight is given to the neighboring words in the same semantic segment and propose semantic role labeling position attention based on the bi-directional LSTM model (BLSTM-SRLP). The good performance of the BLSTM-SRLP model has been demonstrated in comparative experiments on the food safety field dataset (FS-QA).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3174094678",
    "type": "article"
  },
  {
    "title": "Deep Structured Learning for Natural Language Processing",
    "doi": "https://doi.org/10.1145/3433538",
    "publication_date": "2021-05-31",
    "publication_year": 2021,
    "authors": "Yong Li; Xiao-jun Yang; Min Zuo; Qingyu Jin; Haisheng Li; Qian Cao",
    "corresponding_authors": "",
    "abstract": "The real-time and dissemination characteristics of network information make net-mediated public opinion become more and more important food safety early warning resources, but the data of petabyte (PB) scale growth also bring great difficulties to the research and judgment of network public opinion, especially how to extract the event role of network public opinion from these data and analyze the sentiment tendency of public opinion comment. First, this article takes the public opinion of food safety network as the research point, and a BLSTM-CRF model for automatically marking the role of event is proposed by combining BLSTM and conditional random field organically. Second, the Attention mechanism based on vocabulary in the field of food safety is introduced, the distance-related sequence semantic features are extracted by BLSTM, and the emotional classification of sequence semantic features is realized by using CNN. A kind of Att-BLSTM-CNN model for the analysis of public opinion and emotional tendency in the field of food safety is proposed. Finally, based on the time series, this article combines the role extraction of food safety events and the analysis of emotional tendency and constructs a net-mediated public opinion early warning model in the field of food safety according to the heat of the event and the emotional intensity of the public to food safety public opinion events.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3180567397",
    "type": "article"
  },
  {
    "title": "A Novel Classification Model SA-MPCNN for Power Equipment Defect Text",
    "doi": "https://doi.org/10.1145/3464380",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Xiuxia Tian; Can Li; Bo Zhao",
    "corresponding_authors": "",
    "abstract": "The text classification of power equipment defect is of great significance to equipment health condition evaluation and power equipment maintenance decisions. Most of the existing classification methods do not sufficiently consider the semantic relation between words in the same sentence and cannot extract deep semantic features. To tackle those problems, this article proposes a novel classification method by combining the self-attention mechanism and multi-channel pyramid convolution neural networks. We utilize the bidirectional gated recurrent unit to model the text sequence and, on this basis, improve self-attention layer to dot multiplication on the forward and backward features to obtain the global attention score. Thereby, effective features are enhanced, invalid features are weakened, and important text representation vectors are obtained. To solve the problem that the shallow network structure cannot extract deep semantic features, we design a multi-channel pyramid convolution network, which first extracts deep text features from the channels of different windows and then fuses the text features of each channel. By comparing with the state-of-the-art methods, the model in this article has better performance in text classification of power equipment defects.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3195098600",
    "type": "article"
  },
  {
    "title": "Transfer Learning Based Recurrent Neural Network Algorithm for Linguistic Analysis",
    "doi": "https://doi.org/10.1145/3406204",
    "publication_date": "2021-05-31",
    "publication_year": 2021,
    "authors": "Peipei Jiang; Liailun Chen; Min-Feng Wang",
    "corresponding_authors": "",
    "abstract": "Each language is a system of understanding and skills that allows language users to interact, express thoughts, hypotheses, feelings, wishes, and all that needs to be expressed. Linguistics is the research of these structures in all respects: the composition, usage, and sociology of language, in particular, are the core of linguistics. Machine Learning is the research area that allows machines to learn without being specifically scheduled. In linguistics, the design of writing is understood to be a foundation for many distinct company apps and probably the most useful if incorporated with machine learning methods. Research shows that besides text tagging and algorithm training, there are major problems in the field of Big Data. This article provides a collaborative effort (transfer learning integrated into Recurrent Neural Network) to analyze the distinct kinds of writing between the language's linear and non-computational sides, and to enhance granularity. The outcome demonstrates stronger incorporation of granularity into the language from both sides. Comparative results of machine learning algorithms are used to determine the best way to analyze and interpret the structure of the language.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3198889199",
    "type": "article"
  },
  {
    "title": "A Multi-Classification Sentiment Analysis Model of Chinese Short Text Based on Gated Linear Units and Attention Mechanism",
    "doi": "https://doi.org/10.1145/3464425",
    "publication_date": "2021-09-20",
    "publication_year": 2021,
    "authors": "Lei Liu; Hao Chen; Yinghong Sun",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis of social media texts has become a research hotspot in information processing. Sentiment analysis methods based on the combination of machine learning and sentiment lexicon need to select features. Selected emotional features are often subjective, which can easily lead to overfitted models and poor generalization ability. Sentiment analysis models based on deep learning can automatically extract effective text emotional features, which will greatly improve the accuracy of text sentiment analysis. However, due to the lack of a multi-classification emotional corpus, it cannot accurately express the emotional polarity. Therefore, we propose a multi-classification sentiment analysis model, GLU-RCNN, based on Gated Linear Units and attention mechanism. Our model uses the Gated Linear Units based attention mechanism to integrate the local features extracted by CNN with the semantic features extracted by the LSTM. The local features of short text are extracted and concatenated by using multi-size convolution kernels. At the classification layer, the emotional features extracted by CNN and LSTM are respectively concatenated to express the emotional features of the text. The detailed evaluation on two benchmark datasets shows that the proposed model outperforms state-of-the-art approaches.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3201025125",
    "type": "article"
  },
  {
    "title": "A New Concept of Electronic Text Based on Semantic Coding System for Machine Translation",
    "doi": "https://doi.org/10.1145/3469655",
    "publication_date": "2021-11-02",
    "publication_year": 2021,
    "authors": "Meftah Mohammed Charaf Eddine",
    "corresponding_authors": "Meftah Mohammed Charaf Eddine",
    "abstract": "In the field of machine translation of texts, the ambiguity in both lexical (dictionary) and structural aspects is still one of the difficult problems. Researchers in this field use different approaches, the most important of which is machine learning in its various types. The goal of the approach that we propose in this article is to define a new concept of electronic text, which makes the electronic text free from any lexical or structural ambiguity. We used a semantic coding system that relies on attaching the original electronic text (via the text editor interface) with the meanings intended by the author. The author defines the meaning desired for each word that can be a source of ambiguity. The proposed approach in this article can be used with any type of electronic text (text processing applications, web pages, email text, etc.). Thanks to the approach that we propose and through the experiments that we have conducted using it, we can obtain a very high accuracy rate. We can say that the problem of lexical and structural ambiguity can be completely solved. With this new concept of electronic text, the text file contains not only the text but also with it the true sense of the exact meaning intended by the writer in the form of symbols. These semantic symbols are used during machine translation to obtain a translated text completely free of any lexical and structural ambiguity.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3210656256",
    "type": "article"
  },
  {
    "title": "Investigating the Feasibility of Deep Learning Methods for Urdu Word Sense Disambiguation",
    "doi": "https://doi.org/10.1145/3477578",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Ali Saeed; Rao Muhammad Adeel Nawab; Mark Stevenson",
    "corresponding_authors": "",
    "abstract": "Word Sense Disambiguation (WSD), the process of automatically identifying the correct meaning of a word used in a given context, is a significant challenge in Natural Language Processing. A range of approaches to the problem has been explored by the research community. The majority of these efforts has focused on a relatively small set of languages, particularly English. Research on WSD for South Asian languages, particularly Urdu, is still in its infancy. In recent years, deep learning methods have proved to be extremely successful for a range of Natural Language Processing tasks. The main aim of this study is to apply, evaluate, and compare a range of deep learning methods approaches to Urdu WSD (both Lexical Sample and All-Words) including Simple Recurrent Neural Networks, Long-Short Term Memory, Gated Recurrent Units, Bidirectional Long-Short Term Memory, and Ensemble Learning. The evaluation was carried out on two benchmark corpora: (1) the ULS-WSD-18 corpus and (2) the UAW-WSD-18 corpus. Results (Accuracy = 63.25% and F1-Measure = 0.49) show that a deep learning approach outperforms previously reported results for the Urdu All-Words WSD task, whereas performance using deep learning approaches (Accuracy = 72.63% and F1-Measure = 0.60) are low in comparison to previously reported for the Urdu Lexical Sample task.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3210659525",
    "type": "article"
  },
  {
    "title": "Simple Extensible Deep Learning Model for Automatic Arabic Diacritization",
    "doi": "https://doi.org/10.1145/3480938",
    "publication_date": "2021-11-18",
    "publication_year": 2021,
    "authors": "Hamza Abbad; Shengwu Xiong",
    "corresponding_authors": "",
    "abstract": "Automatic diacritization is an Arabic natural language processing topic based on the sequence labeling task where the labels are the diacritics and the letters are the sequence elements. A letter can have from zero up to two diacritics. The dataset used was a subset of the preprocessed version of the Tashkeela corpus. We developed a deep learning model composed of a stack of four bidirectional long short-term memory hidden layers of the same size and an output layer at every level. The levels correspond to the groups that we classified the diacritics into (short vowels, double case-endings, Shadda, and Sukoon). Before training, the data were divided into input vectors containing letter indexes and outputs vectors containing the indexes of diacritics regarding their groups. Both input and output vectors are concatenated, then a sliding window operation with overlapping is performed to generate continuous and fixed-size data. Such data is used for both training and evaluation. Finally, we realize some tests using the standard metrics with all of their variations and compare our results with two recent state-of-the-art works. Our model achieved 3% diacritization error rate and 8.99% word error rate when including all letters. We have also generated the confusion matrix to show the performances per output and analyzed the mismatches of the first 500 lines to classify the model errors according to their linguistic nature.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3214822634",
    "type": "article"
  },
  {
    "title": "I3rab: A New Arabic Dependency Treebank Based on Arabic Grammatical Theory",
    "doi": "https://doi.org/10.1145/3472295",
    "publication_date": "2021-11-18",
    "publication_year": 2021,
    "authors": "Dana Halabi; Ebaa Fayyoumi; Arafat Awajan",
    "corresponding_authors": "",
    "abstract": "Treebanks are valuable linguistic resources that include the syntactic structure of a language sentence in addition to part-of-speech tags and morphological features. They are mainly utilized in modeling statistical parsers. Although the statistical natural language parser has recently become more accurate for languages such as English, those for the Arabic language still have low accuracy. The purpose of this article is to construct a new Arabic dependency treebank based on the traditional Arabic grammatical theory and the characteristics of the Arabic language, to investigate their effects on the accuracy of statistical parsers. The proposed Arabic dependency treebank, called I3rab, contrasts with existing Arabic dependency treebanks in two main concepts. The first concept is the approach of determining the main word of the sentence, and the second concept is the representation of the joined and covert pronouns. To evaluate I3rab, we compared its performance against a subset of Prague Arabic Dependency Treebank that shares a comparable level of details. The conducted experiments show that the percentage improvement reached up to 10.24% in UAS and 18.42% in LAS.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3217295430",
    "type": "article"
  },
  {
    "title": "Improving Telugu Dependency Parsing using Combinatory Categorial Grammar Supertags",
    "doi": "https://doi.org/10.1145/2693190.2693191",
    "publication_date": "2015-01-30",
    "publication_year": 2015,
    "authors": "B. Venkata Seshu Kumari; R. Rajeshwara Rao",
    "corresponding_authors": "",
    "abstract": "We show that Combinatory Categorial Grammar (CCG) supertags can improve Telugu dependency parsing. In this process, we first extract a CCG lexicon from the dependency treebank. Using both the CCG lexicon and the dependency treebank, we create a CCG treebank using a chart parser. Exploring different morphological features of Telugu, we develop a supertagger using maximum entropy models. We provide CCG supertags as features to the Telugu dependency parser (MST parser). We get an improvement of 1.8% in the unlabelled attachment score and 2.2% in the labelled attachment score. Our results show that CCG supertags improve the MST parser, especially on verbal arguments for which it has weak rates of recovery.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2261428231",
    "type": "article"
  },
  {
    "title": "A Probabilistic Framework for Chinese Spelling Check",
    "doi": "https://doi.org/10.1145/2826234",
    "publication_date": "2015-11-11",
    "publication_year": 2015,
    "authors": "Kuan‐Yu Chen; Hsin‐Min Wang; Hsin‐Hsi Chen",
    "corresponding_authors": "",
    "abstract": "Chinese spelling check (CSC) is still an unsolved problem today since there are many homonymous or homomorphous characters. Recently, more and more CSC systems have been proposed. To the best of our knowledge, language modeling is one of the major components among these systems because of its simplicity and moderately good predictive power. After deeply analyzing the school of research, we are aware that most of the systems only employ the conventional n -gram language models. The contributions of this article are threefold. First, we propose a novel probabilistic framework for CSC, which naturally combines several important components, such as the substitution model and the language model, to inherit their individual merits as well as to overcome their limitations. Second, we incorporate the topic language models into the CSC system in an unsupervised fashion. The topic language models can capture the long-span semantic information from a word (character) string while the conventional n -gram language models can only preserve the local regularity information. Third, we further integrate Web resources with the proposed framework to enhance the overall performance. Our rigorously empirical experiments demonstrate the consistent and utility performance of the proposed framework in the CSC task.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2267289808",
    "type": "article"
  },
  {
    "title": "A Seed-Based Method for Generating Chinese Confusion Sets",
    "doi": "https://doi.org/10.1145/2933396",
    "publication_date": "2016-07-22",
    "publication_year": 2016,
    "authors": "Liangliang Liu; Cungen Cao",
    "corresponding_authors": "",
    "abstract": "In natural language, people often misuse a word (called a “confused word”) in place of other words (called “confusing words”). In misspelling corrections, many approaches to finding and correcting misspelling errors are based on a simple notion called a “confusion set.” The confusion set of a confused word consists of confusing words. In this article, we propose a new method of building Chinese character confusion sets. Our method is composed of two major phases. In the first phase, we build a list of seed confusion sets for each Chinese character, which is based on measuring similarity in character pinyin or similarity in character shape. In this phase, all confusion sets are constructed manually, and the confusion sets are organized into a graph, called a “seed confusion graph” (SCG), in which vertices denote characters and edges are pairs of characters in the form (confused character, confusing character). In the second phase, we extend the SCG by acquiring more pairs of (confused character, confusing character) from a large Chinese corpus. For this, we use several word patterns (or patterns) to generate new confusion pairs and then verify the pairs before adding them into a SCG. Comprehensive experiments show that our method of extending confusion sets is effective. Also, we shall use the confusion sets in Chinese misspelling corrections to show the utility of our method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2506719862",
    "type": "article"
  },
  {
    "title": "Dependency and Span, Cross-Style Semantic Role Labeling on PropBank and NomBank",
    "doi": "https://doi.org/10.1145/3526214",
    "publication_date": "2022-04-13",
    "publication_year": 2022,
    "authors": "Zuchao Li; Hai Zhao; Junru Zhou; Kevin Parnow; Shexia He",
    "corresponding_authors": "",
    "abstract": "The latest developments in neural semantic role labeling (SRL) have shown great performance improvements with both the dependency and span formalism/styles. Although the two styles share many similarities in linguistic meaning and computation, most previous studies focus on a single style. In this article, we define a new cross-style semantic role label convention and propose a new cross-style joint optimization model designed around the most basic linguistic meaning of a semantic role. Our work provides a solution to make the results of the two styles more comparable and allowing both formalisms of SRL to benefit from their natural connections in both linguistics and computation. Our model learns a general semantic argument structure and is capable of outputting in either style. Additionally, we propose a syntax-aided method to uniformly enhance the learning of both dependency and span representations. Experiments show that the proposed methods are effective on both span and dependency SRL benchmarks.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2988801569",
    "type": "article"
  },
  {
    "title": "Unsupervised Word Segmentation with Bi-directional Neural Language Model",
    "doi": "https://doi.org/10.1145/3529387",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Lihao Wang; Xiaoqing Zheng",
    "corresponding_authors": "",
    "abstract": "We propose an unsupervised word segmentation model, in which for each unlabelled sentence sample, the learning objective is to maximize the generation probability of the sentence given its all possible segmentations. Such a generation probability can be factorized into the likelihood of each possible segment given the context in a recursive way. To capture both the long- and short-term dependencies, we propose to use a bi-directional neural language model to better extract the features of the segment’s context. Two decoding algorithms were also developed to combine the context features from both directions to generate the final segmentation at the inference time, which helps to reconcile word-boundary ambiguities. Experimental results show that our context-sensitive unsupervised segmentation model achieved state-of-the-art at different evaluation settings on various datasets for Chinese, and the comparable result for Thai.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3133631758",
    "type": "article"
  },
  {
    "title": "Combining a Novel Scoring Approach with Arabic Stemming Techniques for Arabic Chatbots Conversation Engine",
    "doi": "https://doi.org/10.1145/3511215",
    "publication_date": "2022-01-20",
    "publication_year": 2022,
    "authors": "Nasser Alshammari; Fawaz Alharbi",
    "corresponding_authors": "",
    "abstract": "Arabic is recognized as one of the main languages around the world. Many attempts and efforts have been done to provide computing solutions to support the language. Developing Arabic chatbots is still an evolving research field and requires extra efforts due to the nature of the language. One of the common tasks of any natural language processing application is the stemming step. It is important for developing chatbots, since it helps with pre-processing the input data and it can be involved with different phases of the chatbot development process. The aim of this article is to combine a scoring approach with Arabic stemming techniques for developing an Arabic chatbot conversation engine. Two experiments are conducted to evaluate the proposed solution. The first experiment is to select which stemmer is more accurate when applying our solution, since our algorithm can support various stemmers. The second experiment was conducted to evaluate our proposed approach against various machine learning models. The results show that the ISRIS stemming algorithm is the best fit for our solution with accuracy 78.06%. The results also indicate that our novel solution achieved an F1 score of 65.5%, while the other machine learning models achieved slightly lower scores. Our study presents a novel technique by combining scoring mechanisms with stemming processes to produce the best answer for every query sent by chatbots users compared to other approaches. This can be helpful for developing Arabic chatbot and can support many domains such as education, business, and health. This technique is among the first techniques that developed purposefully to serve the development of Arabic chatbots conversation engine.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4205900799",
    "type": "article"
  },
  {
    "title": "Improving Chinese-Vietnamese Neural Machine Translation with Linguistic Differences",
    "doi": "https://doi.org/10.1145/3477536",
    "publication_date": "2022-03-25",
    "publication_year": 2022,
    "authors": "Zhiqiang Yu; Zhengtao Yu; Yantuan Xian; Yuxin Huang; Junjun Guo",
    "corresponding_authors": "",
    "abstract": "We present a simple, efficient data augmentation approach for boosting Chinese-Vietnamese neural machine translation performance by leveraging the linguistic difference between the two languages. We first define the formalized representation of modifier symmetry, which is one of the most representative linguistic differences between Chinese and Vietnamese. We then propose and test two data augmentation strategies for leveraging the linguistic difference, which can be integrated naturally with different translation models. Results indicate that both strategies can introduce linguistic rules to boost translation accuracy. Tests on Chinese-Vietnamese benchmarks show significant accuracy improvements. To facilitate studies in this domain, we also release an open-source toolkit 1 with flexible implementation for Chinese-Vietnamese linguistic difference tagging.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4221121101",
    "type": "article"
  },
  {
    "title": "A Study on the Construction of the Evaluation System of the Teaching Ability of Students using Pattern Recognition for Studying Majoring in Badminton in the Mixed Learning Model of Physical Education Majors and Self-Learning System",
    "doi": "https://doi.org/10.1145/3527607",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "Guolei Jiang",
    "corresponding_authors": "Guolei Jiang",
    "abstract": "With my country's ongoing education reform and the continuous development of information technology-enabled education methods, the teaching environment and conditions of various universities have greatly improved, and information technology is changing the learning methods of college students at an alarming speed. This is the advantage of the blended learning model. In badminton as a specialization of physical education, strengthening the professional ability training of college badminton students is an important reform in the direction of achieving for the current talent training goal of our country. In the present teaching environment, the traditional classroom teaching mode and online learning mode coexist and each has their own advantages. Based on the investigation and analysis of the current situation of badminton students' teaching ability, this article proposes methods such as the literature method, expert interview method, questionnaire survey method, derivative Delphi method, and so on, and designs and constructs a future-oriented badminton student teaching ability evaluation system. An in-depth study of the blended learning model has provided a certain theoretical and practical basis. The experimental results of this article show that by selecting two periods of indicators and calculating the weights, it can be seen that in the large-scale badminton teaching ability evaluation system, the badminton teaching organization and management ability (26%), and badminton skills and tactics are satisfactory, and the execution ability is also satisfactory (35%), which is relatively large, and the sum of the two exceeds 60%. The blended learning model exploits the advantages of traditional teaching, combined with online learning to promote the development of students' professional badminton practice ability.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4229023233",
    "type": "article"
  },
  {
    "title": "Forward-backward Transliteration of Punjabi Gurmukhi Script Using N-gram Language Model",
    "doi": "https://doi.org/10.1145/3542924",
    "publication_date": "2022-06-09",
    "publication_year": 2022,
    "authors": "Kapil Dev Goyal; Muhammad Raihan Abbas; Vishal Goyal; Yasir Saleem",
    "corresponding_authors": "",
    "abstract": "Transliterating the text of a language to a foreign script is called forward transliteration and transliterating the text back to the original script is called backward transliteration. In this work, we perform both forward as well as backward transliteration on Punjabi. We transliterate Punjabi person names from Gurmukhi script to English Roman script and from English Roman script back to Gurmukhi script using n-gram language model. We used more than one million parallel entities of person names in Gurmukhi and Roman script as the training corpus. We generated English to Punjabi and Punjabi to English n-grams databases from the corpus. To get better results, we tried to create as long n-grams as possible ranging from bi-gram to 30-gram. Our n-grams database contains more than 10 million n-grams, with each n-gram having multiple mappings of the other script. The most challenging part is to find the mapping for the given n-gram from the parallel name entity while creating n-grams databases. As per the orthography rules, the same combination of letters may have different pronunciation, depending upon its location in the word. Therefore, we categorized n-grams into starting, middle, and ending n-grams and used them accordingly in the transliteration process. The transliteration process works like the merge sort. We start searching the longest possible n-gram in the database and split the string recursively until the match is found. The transliterated strings are merged back to form the final output. In English to Punjabi transliteration, we achieved 96% accuracy using gold standard and 99.14% accuracy using minimum edit distance. In Punjabi to English transliteration, the result showed 96.85% and 99.35% accuracy for the gold standard and minimum edit distance, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4281895511",
    "type": "article"
  },
  {
    "title": "GA-SCS: Graph-Augmented Source Code Summarization",
    "doi": "https://doi.org/10.1145/3554820",
    "publication_date": "2022-08-04",
    "publication_year": 2022,
    "authors": "Mengli Zhang; Gang Zhou; Wanting Yu; Ningbo Huang; Wenfen Liu",
    "corresponding_authors": "",
    "abstract": "Automatic source code summarization system aims to generate a valuable natural language description for a program, which can facilitate software development and maintenance, code categorization, and retrieval. However, previous sequence-based research did not consider the long-distance dependence and highly structured characteristics of source code simultaneously. In this article, we present a Transformer-based Graph-Augmented Source Code Summarization (GA-SCS), which can effectively incorporate inherent structural and textual features of source code to generate an effective code description. Specifically, we develop a graph-based structure feature extraction scheme leveraging abstract syntax tree and graph attention networks to mine global syntactic information. And then, to take full advantage of the lexical and syntactic information of code snippets, we extend the original attention to a syntax-informed self-attention mechanism in our encoder. In the training process, we also adopt a reinforcement learning strategy to enhance the readability and informativity of generated code summaries. We utilize the Java dataset and Python dataset to evaluate the performance of different models. Experimental results demonstrate that our GA-SCS model outperforms all competitive methods on BLEU, METEOR, ROUGE, and human evaluations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4289767201",
    "type": "article"
  },
  {
    "title": "Contrastive Learning between Classical and Modern Chinese for Classical Chinese Machine Reading Comprehension",
    "doi": "https://doi.org/10.1145/3551637",
    "publication_date": "2022-08-05",
    "publication_year": 2022,
    "authors": "Maofu Liu; Junyi Xiang; Xia Xu; Huijun Hu",
    "corresponding_authors": "",
    "abstract": "By leveraging self-supervised tasks, pre-trained language model (PLM) has made significant progress in the field of machine reading comprehension (MRC) . However, in classical Chinese MRC (CCMRC) , the passage is typically in classical style, but the question and options are given in modern style. Existing pre-trained methods seldom model the relationship between classical and modern styles, resulting in overall misunderstanding of the passage. In this paper, we propose a contrastive learning method between classical and modern Chinese in order to reach a deep understanding of the two different styles. In particular, a novel pre-training task and an enhanced co-matching network have been defined: (1) The synonym discrimination (SD) task is used to identify whether modern meaning corresponds to classical Chinese. (2) The enhanced dual co-matching (EDCM) network is employed for a more interactive understanding of the classical passage and the modern options. The experimental results show that our proposed method improves language understanding ability and outperforms existing PLMs on the Haihua, CCLUE, and ChID datasets.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4289878414",
    "type": "article"
  },
  {
    "title": "Modeling and Analysis of Chinese Culture and Communication",
    "doi": "https://doi.org/10.1145/3514238",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "Yingtao Li",
    "corresponding_authors": "Yingtao Li",
    "abstract": "Along with many other Asian countries, China's communication differs from, and at times, conflicts with, the United States, which is considered more collectivist and low-contact than that of the United States. The topic of this article is mental, physical, and behavioral health. This ancient society is one of the world's oldest, with a history spanning many thousands of years. These aspects of Chinese culture have significant weight in the Chinese community and society. Chinese participants care deeply about the cultural context, whether social scientists, humanists, or clinical psychiatrists. Based on several studies, Chinese culture influences various aspects of health, including physical and mental well-being, parent-child interactions and social connections, goals for individuals and groups, and health care delivery models. According to research on the subject, traditional Confucian cultural norms have influenced Chinese communication features. To maintain harmonious ties, the Chinese rely heavily on indirect communication. For them, the way you stand, your attitude, and even the tone of your voice all communicate a lot more than just words. They use imprecise language and may understate the significance of what they say. For the modeling analysis of Chinese culture and communication ( MA-CCC ) model, a brand-new approach has been presented. As long as this Confucian-influenced Chinese communication style persists, it will significantly impact Chinese society and communication between Chinese professionals and their Western counterparts. However, the frequency of attachment terms was lower in Chinese literature; the findings showed a rising tendency for AC ( affectionate communication ) in recent decades. In addition, the frequency of love terms in Chinese novels was linked positively to individuality. As a result of societal shifts, affection sharing becomes more common in individualistic metropolitan settings, as these findings show a performance enhancement of 94.2%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4293085795",
    "type": "article"
  },
  {
    "title": "A Research on University Students’ Behavioral Intention to Use New-generation Information Technology in Intelligent Foreign Language Learning",
    "doi": "https://doi.org/10.1145/3563774",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Shanshan Yu",
    "corresponding_authors": "Shanshan Yu",
    "abstract": "A better understanding of how advancement in science and technology affect students’ learning behavior in an academic setting can help all educators in higher education. With the advancement of science and technology, the new-generation information technology represented by “Cloud Computing, Big Data, Internet of Things, Mobile Network, and Artificial Intelligence” has been profoundly changing the form of education, accelerating the transformation from Teaching 1.0 to Learning 2.0. To investigate the factors influencing university students using a new-generation information technology in intelligent foreign language learning, this study proposed a research model based on Technology Acceptance Model. The sample data were collected from a survey of 237 students at a university. The analysis of structural equation modeling indicated that perceived usefulness, perceived ease of use, construction of foreign language intelligence classroom, and computer self-efficacy all have a positive impact on learners’ behavioral intention. These findings suggest that more emphasis should be laid on students’ learning characteristics, advantages of a new generation of technology, as well as teachers’ intelligent literacy, to further promote the deep integration of the new-generation information technology and intelligent foreign language learning, improving the learning effectiveness ultimately and effectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4297377494",
    "type": "article"
  },
  {
    "title": "Opinion Leader Detection in Asian Social Networks using Modified Spider Monkey Optimization",
    "doi": "https://doi.org/10.1145/3555311",
    "publication_date": "2022-08-10",
    "publication_year": 2022,
    "authors": "Sanjay Kumar; Akshi Kumar; Abhishek Mallik; Sakshi Dhall",
    "corresponding_authors": "",
    "abstract": "The Asian social networks are dominated by the society’s collectivist culture, and this interestingly introduces an influence mechanism aided by word-of-mouth and opinion leaders. An opinion leader can help to generate and shape other people’s opinion and achieve a high information spread on any topic. In this work, a modified spider monkey optimization based opinion leader detection approach is proposed. Firstly, we employ the modified node2vec graph embedding to generate the lower dimensional vectors which act as the initial features for the nodes in a typical Asian social network. Next, the entire population is broken down into several groups using the k-means++ algorithm where the number of clusters is equal to the number of opinion leaders to be selected. The local and global leaders are chosen by using the coordinates of the cluster centres of these clusters. The coordinates of the centroids of the clusters are then used to detect the local and global leaders in the network. The local leaders then form the seed set of opinion leaders for the network. The positions of the nodes in the network, including the local and global leaders, are updated over a number of iterations. At the end of these iterations, the seed set generating the maximum influence forms the set of opinion leaders in the network. We test our proposed approach using the popular information diffusion and cognitive opinion dynamics (COD) models. We perform intensive experiments on several real-life social networks based on various performance metrics. The results obtained reveal that the proposed approach outperforms several existing techniques of opinion leader detection.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4297830965",
    "type": "article"
  },
  {
    "title": "Building a Closed-Domain Question Answering System for a Low-Resource Language",
    "doi": "https://doi.org/10.1145/3566123",
    "publication_date": "2022-10-12",
    "publication_year": 2022,
    "authors": "Phuoc Tran; Dat Nguyen; Huu-Anh Tran; Thien Nguyen; Tram T. Tran",
    "corresponding_authors": "",
    "abstract": "In recent years, the Question Answering System (QAS) has been widely used to develop many systems, such as conversation systems, chatbots, and intelligent search. Depending on the amount of information or knowledge that the system processes, the system can be applied in answering the questions in an open domain or closed domain. There are many approaches to solving the QA problem, but the neural network models have yielded impressive and promising results, especially the Machine Reading Comprehension approach. In this article, we build a closed-domain QAS for a low-resource language, Vietnamese—specifically, “The Postgraduate Admission of Ho Chi Minh City University of Food Industry, Vietnam.” In addition, we have created two datasets to serve our QAS: vi-SQuAD v1.1, which is automatically translated and edited from SQuAD (Stanford University Question Answering Dataset), and HUFI-PostGrad, which is manually collected. We use two main models for the system, including the Intent Classification model and the Machine Reading Comprehension model. Experimental results initially show that our QAS gives encouraging results.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4304693455",
    "type": "article"
  },
  {
    "title": "Mutual Supervised Fusion &amp; Transfer Learning with Interpretable Linguistic Meaning for Social Data Analytics",
    "doi": "https://doi.org/10.1145/3568675",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Yuanpeng Zhang; Yizhang Jiang; Alireza Jolfaei",
    "corresponding_authors": "",
    "abstract": "Social data analytics is often taken as the most commonly used method for community discovery, product recommendations, knowledge graph, and so on. In this study, social data are firstly represented in different feature spaces by using various feature extraction algorithms. Then we build a transfer learning model to leverage knowledge from multiple feature spaces. During modeling, since the assumption that the training and the testing data have the same distribution is always true, we give a theorem and its proof which asserts the necessary and sufficient condition for achieving a minimum testing error. We also theoretically demonstrate that maximizing the classification error consistency across different feature spaces can improve the classification performance. Additionally, the cluster assumption derived from semi-supervised learning is introduced to enhance knowledge transfer. Finally, a Tagaki-Sugeno-Kang (TSK) fuzzy system-based learning algorithm is proposed, which can generate interpretable fuzzy rules. Experimental results not only demonstrate the promising social data classification performance of our proposed approach but also show its interpretability which is missing in many other models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4306873781",
    "type": "article"
  },
  {
    "title": "A Comprehensive Roadmap on Bangla Text-based Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3572783",
    "publication_date": "2022-11-22",
    "publication_year": 2022,
    "authors": "Shumaiya Akter Shammi; Sajal Das; Narayan Ranjan Chakraborty; Sumit Kumar Banshal; Nishu Nath",
    "corresponding_authors": "",
    "abstract": "The effortless expansion of Internet access has eventually transformed the dissemination behavior toward E-Mode. Thus, the usage of online or, more specifically, “Digital” texts has expanded abruptly. “Bangla,” the seventh most spoken language globally, has no different nature. Communication in the Bangla language has also been exposed on the Internet, which describes the feelings of individuals in any specific context. These enormously generated data from diverse sources have drawn the interest of the researchers working in the Natural Language Processing domain. Despite its relatively complicated structure, a lesser amount of annotated data, as well as a limited number of frameworks and approaches, exist. This lacking of resources has kept several stones unturned in this diverse, emotion-rich, and widely spoken language. To bridge the lacking and absence of resources, this article aims to provide a generalized deduced working procedure in this domain. To do so, the existing research work in the domain of sentiment analysis using Bangla text has been collected, evaluated, and summarized. Also, in this article, the techniques used in pre-processing, feature extraction, and eventually used algorithms have been identified and discussed. Considering these facts, this research work sketches a tentative blueprint of sentiment analysis using Bangla text. Additionally, this article discusses existing regional language corpora such as Tamil, Urdu, and Hindi, as well as English and methodologies used to extract emotional essence from Bangla language comparing other languages. That will assist in determining the probable chosen path of exploring Bangla in a deeper aspect. Moreover, this work has deduced and presented a generalized framework that will direct aspiring researchers to decide the pathway of choosing data vis-à-vis methodologies based on their interests.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4309697990",
    "type": "article"
  },
  {
    "title": "Hybridization Based Machine Translations for Low-Resource Language with Language Divergence",
    "doi": "https://doi.org/10.1145/3571742",
    "publication_date": "2022-11-21",
    "publication_year": 2022,
    "authors": "Nandini Sethi; Amita Dev; Poonam Bansal; Deepak Kumar Sharma; Deepak Gupta",
    "corresponding_authors": "",
    "abstract": "A hybridised form of direct and rule-based language processing is used in this paper to present a Machine translation system from Sanskrit to Hindi. The divergence between Sanskrit and Hindi is also discussed in this paper, along with a proposition for how to handle it. Sanskrit-Hindi bilingual dictionaries, Grammatical Sanskrit corpus and a Sanskrit analyses rule base, have all been used in the projected system. The projected system's ability to access data from various data vocabularies and rule bases utilised in the system expansion has been improved by the usage of Elasticsearch technique. Additionally, a novel technique that builds a parse tree from the parsing table is presented in this paper. The system processes the input Sanskrit sentence using the parsing approach and the Context Free Grammar in normal form for Sanskrit language processing. No standard Sanskrit-Hindi Grammatical corpora available for Machine Translation which is designed and developed in the proposed work. The specific language sentence is produced using the Grammatical corpora and bilingual dictionaries. The proposed system achieved a Bilingual Evaluation Understudy (BLEU) score of 51.6 percent after being tested using Python's natural language toolkit API. The proposed system performs better than current systems when compared to cutting-edge systems, according to the comparison.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4309756528",
    "type": "article"
  },
  {
    "title": "Pradvis vac: A socio-demographic dataset for determining the level of hatred severity in a low-resource Hinglish language",
    "doi": "https://doi.org/10.1145/3573199",
    "publication_date": "2022-12-07",
    "publication_year": 2022,
    "authors": "Shankar Biradar; Sunil Saumya; Abhinav Kumar; Ashish Singh",
    "corresponding_authors": "",
    "abstract": "In multilingual societies like India, mixing the native language with English has become common during social media conversations. Further, due to the government’s digitization push, more people from rural India are joining social media platforms, resulting in the exponential growth of native or code-mixed content. The resultant content on social media is available for both positive (also termed as Hope Speech) as well as negative context (also termed as Hate Speech). To keep the social media clean and hate free, it is important to remove the negative content using machine learning filters. Since most of the existing hate content prediction models are trained using high resource language such as English, they fail to work on code-mixed text due to its spelling variance and non-grammatical structure. In addition, the lack of suitable training data could be one reason behind existing models’ poor performance on code-mixed text. To address these issues and promote research in this direction, we developed a manually annotated Hinglish Code-mixed corpus of 9254 comments taken from Twitter handles. We also annotated our data with the target audience and severity level. In each label, we provided a more fine-grained classification with three independent classes, and we built a Multi-label and Multi-class corpus for the severity of hate content prediction in Hinglish code-mixed text. Further, we modeled various supervised classifiers for severity prediction to validate our proposed data. The proposed models employ transformers for feature extraction and different machine learning and RNN (Recurrent neural network) models for classification. According to the experimental results, the target label combined with embeddings from Twitter text using the BiLSTM (a varient of RNN) classifier performed better on severity prediction, attaining an acceptable weighted F1 score.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4311809219",
    "type": "article"
  },
  {
    "title": "Filtering and Extended Vocabulary based Translation for Low-resource Language Pair of Sanskrit-Hindi",
    "doi": "https://doi.org/10.1145/3580495",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Piyush Jha; Rashi Kumar; Vineet Sahula",
    "corresponding_authors": "",
    "abstract": "Neural Machine Translation (NMT) is widely employed for language translation tasks because it performs better than the conventional statistical and phrase-based approaches. However, NMT techniques involve challenges, such as requiring a large and clean corpus of parallel data and the inability to deal with rare words. They need to be faster for real-time applications. More work needs to be done using NMT to address the challenges in translating Sanskrit, one of the oldest and rich languages known to the world, with its morphological richness and limited multilingual parallel corpus. There is usually no similar data between a language pair; hence, no application exists so far that can translate Sanskrit to/from other languages. This study presents an in-depth analysis to address these challenges with the help of a low-resource Sanskrit-Hindi language pair. We employ a novel training corpus filtering with extended vocabulary in a zero-shot transformer architecture. The structure of the Sanskrit language is thoroughly investigated to justify the use of each step. Furthermore, the proposed method is analyzed based on variations in sentence length and also applied to a high-resource language pair in order to demonstrate its efficacy.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4317504322",
    "type": "article"
  },
  {
    "title": "Development of a Benchmark Odia Handwritten Character Database for an Efficient Offline Handwritten Character Recognition with a Chronological Survey",
    "doi": "https://doi.org/10.1145/3583988",
    "publication_date": "2023-02-21",
    "publication_year": 2023,
    "authors": "Raghunath Dey; Rakesh Chandra Balabantaray",
    "corresponding_authors": "",
    "abstract": "A good benchmark dataset is a primary requirement in the offline handwritten character recognition (HCR) process. Only three handwritten numerals and alphabet datasets from Odia are publicly accessible for study, although many writers have used several datasets in their experiments. In this article, two tasks are done to address this issue. Those are the following: First, an extensive survey focused on various datasets is provided with the methodologies used in chronological order. The second factor is a solution to the lack of publicly available handwritten characters and numeral datasets. A new dataset of handwritten Odia characters with numerals has been developed. Anyone can access this dataset by sending an email to the authors of the article. This dataset was created with the help of 150 volunteers of various age groups, races, and qualifications. Some homogeneous experiments are conducted using deep learning models to evaluate the consistency of the dataset. One heterogeneous trial has also been performed to estimate the complexities of the characters present in the dataset by comparing them with the existing benchmark datasets.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4321454741",
    "type": "article"
  },
  {
    "title": "A Context-focused Attention Evolution Model for Aspect-based Sentiment Classification",
    "doi": "https://doi.org/10.1145/3587465",
    "publication_date": "2023-03-20",
    "publication_year": 2023,
    "authors": "Xiaoheng Deng; Dingjie Han; Ping Jiang",
    "corresponding_authors": "",
    "abstract": "Due to their inherent capability in the semantic alignment of aspects and their context words, Attention and Long-Short-Term-Memory (LSTM) mechanisms are widely adopted for Aspect-Based Sentiment Classification (ABSC) tasks. Instead, it is challenging to handle long-range word dependencies on multiple entities due to the deficiency in attention mechanisms. To solve this problem, we propose a Context-Focused Aspect-Based Network to align attention before LSTM, making the model focus more on aspect-related words and ignore irrelevant words, improving the accuracy of final classification. This can either alleviate attention distraction or reinforce the text representation ability. Experiments on two benchmark datasets show that the results achieve respectable performance compared to the state-of-the-art methods available in ABSC. Our approach has the potential to improve classification accuracy by adaptively adjusting the focus on context.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4327950610",
    "type": "article"
  },
  {
    "title": "The Impact of Arabic Diacritization on Word Embeddings",
    "doi": "https://doi.org/10.1145/3592603",
    "publication_date": "2023-04-19",
    "publication_year": 2023,
    "authors": "Mohamed Abbache; Ahmed Abbache; Jingwen Xu; Farid Meziane; Xianbin Wen",
    "corresponding_authors": "",
    "abstract": "Word embedding is used to represent words for text analysis. It plays an essential role in many Natural Language Processing (NLP) studies and has hugely contributed to the extraordinary developments in the field in the last few years. In Arabic, diacritic marks are a vital feature for the readability and understandability of the language. Current Arabic word embeddings are non-diacritized. In this article, we aim to develop and compare word embedding models based on diacritized and non-diacritized corpora to study the impact of Arabic diacritization on word embeddings. We propose evaluating the models in four different ways: clustering of the nearest words; morphological semantic analysis; part-of-speech tagging; and semantic analysis. For a better evaluation, we took the challenge to create three new datasets from scratch for the three downstream tasks. We conducted the downstream tasks with eight machine learning algorithms and two deep learning algorithms. Experimental results show that the diacritized model exhibits a better ability to capture syntactic and semantic relations and in clustering words of similar categories. Overall, the diacritized model outperforms the non-diacritized model. We obtained some more interesting findings. For example, from the morphological semantics analysis, we found that with the increase in the number of target words, the advantages of the diacritized model are also more obvious, and the diacritic marks have more significance in POS tagging than in other tasks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4366390252",
    "type": "article"
  },
  {
    "title": "Automatic Keyword Extraction Algorithm for Chinese Text based on Word Clustering",
    "doi": "https://doi.org/10.1145/3592793",
    "publication_date": "2023-04-22",
    "publication_year": 2023,
    "authors": "Rui Pan",
    "corresponding_authors": "Rui Pan",
    "abstract": "There are some problems in automatic keyword extraction of Chinese text, such as large feature extraction error, low precision of extracted keywords, and poor real-time performance. Therefore, an automatic keyword extraction algorithm for Chinese text based on word clustering is designed. Calculate keyword frequency, document frequency and inverse document frequency features through statistical algorithm, measure the degree of interdependence between keywords with the help of point mutual information, and construct keyword feature item quantification matrix with the help of vector space model corresponding to keywords and feature items to complete keyword feature quantification and realize keyword feature extraction of Chinese text. Calculate the average semantic similarity of keyword words, determine the similarity of keyword features, and eliminate the keyword features with high similarity; Set the comprehensive feature value of the importance of single word words in Chinese text, determine the importance of single word words in the text, remove the single word words with low importance, and use Bayesian framework to reduce the dimension of high-dimensional keyword feature data to realize preprocessing research. The mapping results of keyword vector space model are determined by word clustering algorithm, the text clusters of keyword space clustering results are calculated by clustering algorithm, and the keywords are classified by DBN method. On this basis, the automatic keyword extraction model of Chinese text is designed to realize the automatic keyword extraction of Chinese text. The experimental results show that the design algorithm can effectively reduce the feature extraction error and improve the extraction efficiency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4366769426",
    "type": "article"
  },
  {
    "title": "Application of Intelligent Image Recognition and Digital Media Art in the Inheritance of Black Pottery Intangible Cultural Heritage",
    "doi": "https://doi.org/10.1145/3597430",
    "publication_date": "2023-05-23",
    "publication_year": 2023,
    "authors": "Yin Jie",
    "corresponding_authors": "Yin Jie",
    "abstract": "With the development of science and technology, intelligent image recognition and digital media art are gradually applied to the inheritance of intangible cultural heritage. In the inheritance of black pottery, intelligent image recognition can help identify the authenticity and age of black pottery, and digital media art can inject new vitality into black pottery culture. Digital media, which combines digitalization and mediaization, breaks down the limitations of traditional media and provides ideological and technical guarantees for the reconstruction, dissemination, and promotion of intangible cultural heritage protection. Image recognition technology is no longer limited to the auxiliary detection of images; on the contrary, it has become a recording and collection tool. With the promotion and development of science and technology, it can be more widely shared and disseminated in time and space. In this article, the traditional machine learning algorithm and the intelligent learning algorithm are compared in image recognition, and the image information recognition rate, the number of iterations, the training time, and the information loss rate are compared in four aspects. The results have found that, compared with the traditional learning algorithm, the number of iterations of the intelligent learning algorithm is reduced by 20%, the training time is reduced by 40%, the information loss rate is reduced by 6.3%, and the recognition accuracy rate is improved by 6.1%, which showed that under the current technology, intangible cultural heritage can be better inherited, and Chinese culture can be better promoted, so that more people can understand and join in the protection of intangible cultural heritage.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4377821105",
    "type": "article"
  },
  {
    "title": "Intelligent Multimedia Network Security and PBL Teaching Mode in the Basic Course Teaching of College Design Major",
    "doi": "https://doi.org/10.1145/3597429",
    "publication_date": "2023-05-23",
    "publication_year": 2023,
    "authors": "Yuan Gao; Yani Wu; Jun Qian",
    "corresponding_authors": "",
    "abstract": "At this stage, there are problems of disconnection between theoretical knowledge and practice and lack of coherence and progression in the teaching of basic courses for design majors. The teaching of basic courses for design majors is an important part of design disciplines. Improving the teaching effect of basic courses of design majors is conducive to cultivating students' design thinking and improving students' creative ability. Under the traditional teaching mode, the teaching of the basic courses of design majors excessively pursues the cultivation of skills, ignoring the cultivation of students' imagination and creativity. Students are not very motivated to study the basic courses of design majors. Therefore, this article studied the application of intelligent multimedia Project-based Learning (PBL) teaching mode in the teaching of basic courses of design majors and analyzed the intelligent multimedia PBL teaching mode from five aspects: students' interest in learning, teamwork ability, learning efficiency, academic level, and satisfaction. Research showed that under the intelligent multimedia PBL teaching mode, students' interest in learning has increased by 4.37%, teamwork ability has increased by 5.07%, learning efficiency has increased by 4.65%, academic performance has increased by 5.85%, and student satisfaction has also increased.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4377825126",
    "type": "article"
  },
  {
    "title": "Knowledge Graph Based Recommendation by Adversarial Learning Algorithm in Application of Lifelong Education Data Classification",
    "doi": "https://doi.org/10.1145/3595636",
    "publication_date": "2023-06-09",
    "publication_year": 2023,
    "authors": "Yunlan Xue",
    "corresponding_authors": "Yunlan Xue",
    "abstract": "Students can improve their capacity to learn continuously and work together to achieve a common goal through cooperative and explorative coursework in personalized learning. This article presents an effective method for clustering people by preference and a strategy for developing course suggestions for different organizations. This lets us consider student characteristics and courses in a statistically and semantically clear way. First, this article uses specific word articles and Word2Vec to extract factors efficiently. Optimizing efficiency. After that, a slightly modified K-means algorithm and perceptron adversarial learning method classify students into interest-based study clusters. The knowledge graph is created and saved to achieve this. In conclusion, the opinion-based deep learning algorithm used for subject recommendation system design provides advice for appropriate and high-quality results based on the degree of similarity between recommendation results and expert scoring. To do this, the proposed method is approximated against existing machine learning methods and compared to their prediction performance metrics.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4380088552",
    "type": "article"
  },
  {
    "title": "Automatic Idiom Identification Model for Amharic Language",
    "doi": "https://doi.org/10.1145/3606864",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Anduamlak Abebe Fenta; Seffi Gebeyehu",
    "corresponding_authors": "",
    "abstract": "Idiomatic expressions are important natural parts of all languages and prominent parts of our daily speech. Idioms cannot be interpreted from the words that they are formed with directly and people may not understand the meaning. From past literature, it was noted that idiom affects Natural Language Processing research like machine translation, semantic analysis, and sentiment analysis. Other languages like English, Chinese, and Indian idioms are recognized through different methods in different research. As there is no standard method and research to identify Amharic idioms, this study is aimed to build a model to identify idioms for the Amharic language using a supervised machine learning approach. The study used 800 labeled expressions for training and 200 expressions for testing from Amharic idiom books “የአማ ረኛ ፈሊጦች” and different Amharic documents. To measure the performance of the model, we used accuracy, precision, recall, and F-score. Finally, a 97.5% accuracy result was achieved from the testing dataset showing a promising result. The study contributes to the information systems discourse about improving the awareness and knowledge of researchers on Amharic idioms.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4382938363",
    "type": "article"
  },
  {
    "title": "Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages",
    "doi": "https://doi.org/10.1145/3610773",
    "publication_date": "2023-07-28",
    "publication_year": 2023,
    "authors": "Andargachew Mekonnen Gezmu; Andreas Nürnberger",
    "corresponding_authors": "",
    "abstract": "Neural approaches, which are currently state-of-the-art in many areas, have contributed significantly to the exciting advancements in machine translation. However, Neural Machine Translation (NMT) requires a substantial quantity and good quality parallel training data to train the best model. A large amount of training data, in turn, increases the underlying vocabulary exponentially. Therefore, several proposed methods have been devised for relatively limited vocabulary due to constraints of computing resources such as system memory. Encoding words as sequences of subword units for so-called open-vocabulary translation is an effective strategy for solving this problem. However, the conventional methods for splitting words into subwords focus on statistics-based approaches that mainly conform to agglutinative languages. In these languages, the morphemes have relatively clean boundaries. These methods still need to be thoroughly investigated for their applicability to fusion languages, which is the main focus of this article. Phonological and orthographic processes alter the borders of constituent morphemes of a word in fusion languages. Therefore, it makes it difficult to distinguish the actual morphemes that carry syntactic or semantic information from the word’s surface form, the form of the word as it appears in the text. We, thus, resorted to a word segmentation method that segments words by restoring the altered morphemes. We also compared conventional and morpheme-based NMT subword models. We could prove that morpheme-based models outperform conventional subword models on a benchmark dataset.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385342823",
    "type": "article"
  },
  {
    "title": "Managing Healthcare Infodemic by deep learning in providing healthcare services",
    "doi": "https://doi.org/10.1145/3610290",
    "publication_date": "2023-07-31",
    "publication_year": 2023,
    "authors": "Fahad Ahmad; Muhammad Umer; Saima Sadiq; Rizwan Majeed; Fabio Narducci; Carmen Bisogni",
    "corresponding_authors": "",
    "abstract": "Digital Health care data acquisition and processing is performed by Artificial Intelligence and Internet of Things technologies and digitization of data and information affects the patients’ behavior. News about COVID-19, a global pandemic, is circulating on social media worldwide providing a collection of big data. Awareness about the pandemic is spreading drastically in the form of messages, social media posts, tweets, and videos. It is, therefore, significant to assess the early flow of information on social media during the pandemic to prevent alarmism. This study aims to perform sentiment analysis of social media big data about COVID-19 by deep learning on a dataset provided by IEEE Data Port. The goal is to assist healthcare professionals in developing social media policies that can be used to change public opinion. The Dataset used consists of 11,858 COVID-19-related tweets collected on May 30, 2020. Data are labeled as positive or negative in the first step using TextBlob and VADER. In step II, various machine learning models are compared using three feature extraction techniques in combination with VADER and TextBlob. The results show that Extra Tree Classifier using TF-IDF features outperforms with an accuracy of 0.9474.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385417834",
    "type": "article"
  },
  {
    "title": "GMG-NCDVAE: Guided de novo Molecule Generation using NLP Techniques and Constrained Diverse Variational Autoencoder",
    "doi": "https://doi.org/10.1145/3610533",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Arun Singh Bhadwal; Kamal Kumar; Neeraj Kumar",
    "corresponding_authors": "",
    "abstract": "Text processing techniques in Natural Language Processing (NLP) find applications in many industries such as pharmaceutical, automation, and automotive. Drug design using variational autoencoders is a popular data-assisted technique to design drug molecules with control over molecular properties. It generates continuous latent space, which can be optimized. This paper introduces a constrained variational autoencoder-based molecular generation structure using the SMILES format. The proposal is accompanied by the generation of molecules, filtering them based on scores, and subsequently determining the optimal molecules by using NLP matured techniques. To generate more meaningful latent space, a condition vector of molecular properties is combined with the SMILES representation of molecules. A tunable parameter (diversity,D) is also used to control the diversity in the generated molecules. The proposed architecture is evaluated using standard datasets. Validity, uniqueness, and FCD are evaluation matrices used to access the performance of model. The validity of proposed model is maximum (92.11%) at diversity level 1. As diversity level increases the validity of generated molecules decreases. This is intuitively consistent because increased diversity reduces replicas and improves variety in the generated molecules. Thus proposed model provide control over diversity of generated molecules. The results clearly indicate that the proposed method outperforms other SMILE based methods and gives a new direction for the generation of desired molecules.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385492628",
    "type": "article"
  },
  {
    "title": "Low-Resource Language Information Processing using Dwarf Mongoose Optimization with Deep Learning Based Sentiment Classification",
    "doi": "https://doi.org/10.1145/3607472",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Indresh Kumar Gupta; K. Rana; Vimal Gaur; Kalpna Sagar; D.P. Sharma; Ahmed Alkhayyat",
    "corresponding_authors": "",
    "abstract": "Asian and low-resource language information processing refers to the field of computational linguistics that aims to develop natural language processing (NLP) technologies for languages that have fewer available language resources or are less commonly spoken. This is an important field of study because many languages in Asia and other parts of the world are underrepresented in the field of NLP, which may limit access to information and technology for speakers of these languages. The growing volume of user-generated content on the web has made sentiment analysis (SA) a significant tool for extracting data regarding human emotional states. Twitter sentiment detectors provide a superior solution for assessing the quality of products and services compared to other conventional technologies. The detection performance and classifier accuracy of SA, which can be highly dependent on classifier methods and the quality of input features have been utilised. Deep learning (DL) methods use distinct techniques to extract data from raw data such as tweets or texts and represent them in different forms of models. Therefore, this article presents a Dwarf Mongoose Optimization with Deep Learning-Based Twitter Sentiment Classification (DMODL-TSC) technique to classify sentiments based on tweets. The presented DMODL-TSC technique leverages the concepts of natural language processing (NLP) and DL. Primarily, the raw tweets are preprocessed to transform them into a useful format. Next, the DMODL-TSC technique uses the advanced FastText word embedding technique. Moreover, the bidirectional recurrent neural network (BiRNN) method is utilized for the recognition of sentiments. Finally, the DMO technique is utilized for the optimal hyperparameter optimization of the BiRNN method, which leads to effective classification performance. The comprehensive result examination of the DMODL-TSC system was tested on three datasets, and the obtained outcomes illustrate the supremacy of the DMODL-TSC approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385492630",
    "type": "article"
  },
  {
    "title": "DIEU: A Dynamic Interaction Emotion Unit for Emotion Recognition in Conversation",
    "doi": "https://doi.org/10.1145/3616493",
    "publication_date": "2023-08-17",
    "publication_year": 2023,
    "authors": "Shu Zhao; Weifeng Liu; Jie Chen; Xiao Sun",
    "corresponding_authors": "",
    "abstract": "Emotion recognition in conversation (ERC) is challenging because the conversation takes place in real time and the speakers interact with each other. However, existing methods ignore the dynamic characteristics of interaction between speakers, and the problem of long-range context propagation still exists. In this article, we propose a dynamic interaction emotion unit to solve the preceding problems on the transcription of the conversation. First, we propose a main influence interval search algorithm to provide a dynamic interaction interval for each utterance. Then, we utilize the speaker-aware influence module and the two-stream context module to capture the dynamic interaction and the contextual information from this interval. Furthermore, to obtain the speaker state representation rich in emotional information, we propose a novel dynamic routing algorithm to fuse the preceding information. These well-integrated state representations also enable our model to capture contextual information at a longer distance. Experiments on multiple datasets demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385950848",
    "type": "article"
  },
  {
    "title": "Demographical Based Sentiment Analysis for Detection of Hate Speech Tweets for Low Resource Language",
    "doi": "https://doi.org/10.1145/3616867",
    "publication_date": "2023-08-31",
    "publication_year": 2023,
    "authors": "Kamal Safdar; Shibli Nisar; Waseem Iqbal; Awais Ahmad; Yawar Abbas Bangash",
    "corresponding_authors": "",
    "abstract": "Advancement in IT and communication technology provides the opportunity for social media users to communicate their ideas and thoughts across the globe within no time as well big data promulgated in a result of the communication process itself has immense challenges. Recently, the provision of freedom of speech has witnessed immense promulgation of offensive and hate speech content on the internet aimed the basic human rights violation. The detection of abusive content on social media for rich resource language has become a hot area for researchers in the recent past. However, low-resource languages are underprivileged due to the non-availability of large corpus and its complexity to understand. The proposed methodology mainly has two parts. One is to detect abusive content and the other is to have a demographical analysis of the Indigenously developed dataset. The process starts with the development of a unique unlabeled Urdu dataset of 0.2 M from Twitter through a web scrapper tool named snscraper. The dataset is collected against the 36 districts of Punjab from Pakistan and from the duration 2018- Apr 2022. The dataset is labeled into three target classes Neutral, Offensive, and Hate Speech. After data cleaning, the feature extraction process is achieved with the help of traditional techniques such as Bow and tf-idf with the combination of word and char n-gram and word embedding word2Vec. The dataset is trained on both machine learning algorithms SVM and Logistic regression and deep learning techniques Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNN). The best F score achieved through LSTM on this dataset is 64 and accuracy is 93 through CNN algorithms. A Choropleth map is used for visualization of the dataset distributed among 36 districts of Punjab and a time series plot for time analysis covers five years duration from 2018-Apr to 22.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386307540",
    "type": "article"
  },
  {
    "title": "Effectiveness Analysis of Entrepreneurial Legal Risk Prevention Based on Multi-Modal Deep Learning Model",
    "doi": "https://doi.org/10.1145/3622937",
    "publication_date": "2023-09-07",
    "publication_year": 2023,
    "authors": "Tianhua Li; Shaowei Qu",
    "corresponding_authors": "",
    "abstract": "With the emergence of the upsurge of entrepreneurship, entrepreneurs are increasingly concerned about legal risks. In the process of entrepreneurship, legal risk is the biggest hidden danger of entrepreneurial enterprises. The prevention and avoidance of legal risks is a long and arduous process, and not all risks can be identified and avoided in time. The deep learning method has brought great changes to the fields of speech recognition, image recognition and natural language processing. The tasks in these fields only involve single-mode input, but more recent applications need to involve multi-mode intelligence. Multimodal deep learning mainly includes three aspects: multimodal learning representation, multimodal signal fusion and multimodal application. Through the research on the legal risks in recent years, this paper believed that there are many legal problems in the development of entrepreneurial enterprises, including contract disputes, trade secret disputes, trademark infringement disputes, copyright infringement disputes, and so on. This paper aimed to study the problem of Legal Risks of Entrepreneurship (LRE) that entrepreneurs are concerned about, and proposed a new solution from the perspective of image recognition technology. 486 entrepreneurs were investigated by questionnaire. In the process of entrepreneurship, 87.04% of entrepreneurs encountered legal risks, and they would turn to lawyers for help, which is a better way. However, in most cases, entrepreneurs are exposed to LRE because of their insufficient understanding of economic law, so they solve it through lawyers. However, if a lawyer is hired, the cost would be very high, which would bring great economic pressure to the enterprise. Only 17.90% of entrepreneurs would safeguard their legitimate rights and interests through their own knowledge and legal weapons without resorting to lawyers. It can be seen that entrepreneurs have relatively low practical ability in the use of LRE, and their legal practical ability is obviously insufficient.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386497883",
    "type": "article"
  },
  {
    "title": "GAGPT-2: A Geometric Attention-based GPT-2 Framework for Image Captioning in Hindi",
    "doi": "https://doi.org/10.1145/3622936",
    "publication_date": "2023-09-11",
    "publication_year": 2023,
    "authors": "Santosh Kumar Mishra; Soham Chakraborty; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Image captioning frameworks usually employ an encoder-decoder paradigm, with the encoder receiving abstract image feature vectors as input and the decoder for language modeling. Nowadays, most prominent architectures employ features from region proposals derived from object detection modules. In this work, we propose a novel architecture for image captioning. We employ the object detection module integrated with transformer architecture as an encoder and GPT-2 (Generative Pre-trained Transformer) as a decoder. The encoder utilizes the information of the spatial relationships among detected objects. We introduce a unique methodology for image caption generation in Hindi, which is widely spoken in South Asia and India and is the world’s third most spoken language as well as India’s official language. In terms of BLEU scores, the proposed approach’s performance is comparable to those of other baselines, and the results illustrate that the proposed approach outperforms the other baselines. The efficacy of the proposed approach in generating correct captions is further determined by human assessment in terms of adequacy and fluency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386607626",
    "type": "article"
  },
  {
    "title": "HindiPersonalityNet: Personality Detection in Hindi Conversational Data Using Deep Learning with Static Embedding",
    "doi": "https://doi.org/10.1145/3625228",
    "publication_date": "2023-09-29",
    "publication_year": 2023,
    "authors": "Akshi Kumar; Dipika Jain; Rohit Beniwal",
    "corresponding_authors": "",
    "abstract": "Personality detection along with other behavioral and cognitive assessment can essentially explain why people act the way they do and can be useful to various online applications such as recommender systems, job screening, matchmaking, and counseling. Additionally, psychometric natural language processing relying on textual cues and distinctive markers in writing style within conversational utterances reveals signs of individual personalities. This work demonstrates a text-based deep neural model, HindiPersonalityNet, of classifying conversations into three personality categories (ambivert, extrovert, introvert) for detecting personality in Hindi conversational data. The model utilizes a gated recurrent unit with BioWordVec embeddings for text classification and is trained/tested on a novel dataset, शख्सियत (pronounced as Shakhsiyat) curated using dialogues from an Indian crime-thriller drama series, Aarya . The model achieves an F1-score of 0.701 and shows the potential for leveraging conversational data from various sources to understand and predict a person's personality traits. It exhibits the ability to capture both semantic and long-distance dependencies in conversations and establishes the effectiveness of our dataset as a benchmark for personality detection in Hindi dialogue data. Further, a comprehensive comparison of various static and dynamic word embedding is done on our standardized dataset to ascertain the most suitable embedding method for personality detection.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387168698",
    "type": "article"
  },
  {
    "title": "Reading Scene Text with Aggregated Temporal Convolutional Encoder",
    "doi": "https://doi.org/10.1145/3625822",
    "publication_date": "2023-10-12",
    "publication_year": 2023,
    "authors": "Tianlong Ma; Xiangcheng Du; Xingjiao Wu; Zhao Zhou; Yingbin Zheng; Cheng Jin",
    "corresponding_authors": "",
    "abstract": "Reading scene text in the natural image is of fundamental importance in many real-world problems. Text recognition has a profound effect on information processing by enabling automated extraction and interpretation. Recent scene text recognition methods employ the encoder-decoder framework, which constructs the encoder by obtaining the visual representations based on the last layer of the backbone network and then feeding them into a sequence model. In this article, we propose a novel encoder structure that performs the feature extractor and the sequence modeling within a unified framework. The introduced Aggregated Temporal Convolutional Encoder (ATCE) first incorporates the temporal convolutional layers to consider the long-term temporal relationship in the encoder stage. The aggregation of these temporal convolution modules is designed to utilize visual features from different levels, by augmenting the standard architecture with deeper aggregation to better fuse information across modules. We also study the impact of different attention modules in convolutional blocks for learning accurate text representations. We conduct comparisons on several scene text recognition benchmarks for both Chinese and English; the experiments demonstrate the complementary ability with different decoder variants and the effectiveness of our proposed approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387580232",
    "type": "article"
  },
  {
    "title": "Application of 3D Image Technology in Rural Planning",
    "doi": "https://doi.org/10.1145/3628448",
    "publication_date": "2023-10-20",
    "publication_year": 2023,
    "authors": "Yejun Cao; X YU; Fengling Jiang",
    "corresponding_authors": "",
    "abstract": "The well-being of villages and villagers is directly related to the development of urban-rural relations. Rural development is an important part of poverty alleviation, as well as the main goal and means of rural rejuvenation, because rural planning affects rural economic development and rural revitalization. Electronic imaging can improve the speed of rural area planning, and can also model the planned scheme. However, the current rural planning still lacks top-level design, which cannot improve the overall structural design of rural planning, and the data resources of rural planning are not perfect. Therefore, this article studied the direction of rural planning and design by analyzing the focus, problems, and external environment characteristics of rural planning, and then analyzed the application characteristics in rural planning according to the process of 3D image technology. By reducing the complex design links in rural planning, the office efficiency of planning and the 3D visualization of planning model can be promoted, so as to improve the effect of rural planning and the construction service of rural planning. The simulated annealing algorithm showed that the planning efficiency and average planning speed of 3D image application in rural planning were gradually increasing; the average planning efficiency was about 1.80, and the average planning speed was about 1.05. The planning efficiency increased by 1.20 in the whole process, while the average planning speed increased by 0.33 in the whole process. Through comparison, it can be seen that the planning rationality of rural planning under 3D image was 12.16% higher than the original one, while the measurement error rate of some teachers was 47.28% lower. In a word, 3D imaging and electronic imaging can improve the architectural design and layout planning of rural planning.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387823765",
    "type": "article"
  },
  {
    "title": "AlgBERT: Automatic Construction of Annotated Corpus for Sentiment Analysis in Algerian Dialect",
    "doi": "https://doi.org/10.1145/3632948",
    "publication_date": "2023-11-17",
    "publication_year": 2023,
    "authors": "Khaoula Hamadouche; Kheira Zineb Bousmaha; Mohamed Abdelwaret Bekkoucha; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "Nowadays, sentiment analysis is one of the most crucial research fields of Natural Language Processing (NLP), and it is widely applied in a variety of applications such as marketing and politics. However, the Arabic language still lacks sufficient language resources to enable the tasks of opinion and emotion analysis comparing to other language such as English. Additionally, manual annotation requires a lot of effort and time. In this article, we address this problem and propose a novel automated annotation platform for sentiment analysis called AlgBERT by providing annotated corpus and using deep learning technology that includes many automatic natural language processing algorithms, which is the basis for text classification and opinion analysis. We suggest using BERT model as a method; it is the abbreviation of Bidirectional Encoder Representations from Transformers, as it is one of the most effective technologies in terms of results in different world languages. We used around of 54K comments collected from social networking (Twitter, YouTube) written in Arabic and Algerian dialects. Our AlgBERT system obtained excellent results with an accuracy of 91.04%, and this is considered as one of the best results for opinion analysis in Algerian dialect.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388763435",
    "type": "article"
  },
  {
    "title": "Co-occurrence Weight Selection in Generation of Word Embeddings for Low Resource Languages",
    "doi": "https://doi.org/10.1145/3282443",
    "publication_date": "2019-01-09",
    "publication_year": 2019,
    "authors": "Veysel Yücesoy; Aykut Koç",
    "corresponding_authors": "",
    "abstract": "This study aims to increase the performance of word embeddings by proposing a new weighting scheme for co-occurrence counting. The idea behind this new family of weights is to overcome the disadvantage of distant appearing word pairs, which are indeed semantically close, while representing them in the co-occurrence counting. For high-resource languages, this disadvantage might not be effective due to the high frequency of co-occurrence. However, when there are not enough available resources, such pairs suffer from being distant. To favour such pairs, a weighting scheme based on a polynomial fitting procedure is proposed to shift the weights up for distant words while the weights of nearby words are left almost unchanged. The parameter optimization for new weights and the effects of the weighting scheme are analysed for the English, Italian, and Turkish languages. A small portion of English resources and a quarter of Italian resources are utilized for demonstration purposes, as if these languages are low-resource languages. Performance increase is observed in analogy tests when the proposed weighting scheme is applied to relatively small corpora (i.e., mimicking low-resource languages) of both English and Italian. To show the effectiveness of the proposed scheme in small corpora, it is also shown for a large English corpus that the performance of the proposed weighting scheme cannot outperform the original weights. Since Turkish is relatively a low-resource language, it is demonstrated that the proposed weighting scheme can increase the performance of both analogy and similarity tests when all Turkish Wikipedia pages are utilized as a corpus. The positive effect of the proposed scheme has also been demonstrated in a standard sentiment analysis task for the Turkish language.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2997086126",
    "type": "article"
  },
  {
    "title": "Efficient Low-Resource Neural Machine Translation with Reread and Feedback Mechanism",
    "doi": "https://doi.org/10.1145/3365244",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Zhiqiang Yu; Zhengtao Yu; Junjun Guo; Yuxin Huang; Yonghua Wen",
    "corresponding_authors": "",
    "abstract": "How to utilize information sufficiently is a key problem in neural machine translation (NMT), which is effectively improved in rich-resource NMT by leveraging large-scale bilingual sentence pairs. However, for low-resource NMT, lack of bilingual sentence pairs results in poor translation performance; therefore, taking full advantage of global information in the encoding-decoding process is effective for low-resource NMT. In this article, we propose a novel reread-feedback NMT architecture (RFNMT) for using global information. Our architecture builds upon the improved sequence-to-sequence neural network and consists of a double-deck attention-based encoder-decoder framework. In our proposed architecture, the information generated by the first-pass encoding and decoding process flows to the second-pass encoding process for more sufficient parameters initialization and information use. Specifically, we first propose a “reread” mechanism to transfer the outputs of the first-pass encoder to the second-pass encoder, and then the output is used for the initialization of the second-pass encoder. Second, we propose a “feedback” mechanism that transfers the first-pass decoder’s outputs to a second-pass encoder via an important weight model and an improved gated recurrent unit (GRU). Experiments on multiple datasets show that our approach achieves significant improvements over state-of-the-art NMT systems, especially in low-resource settings.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3014118193",
    "type": "article"
  },
  {
    "title": "Improving Neural Machine Translation with Linear Interpolation of a Short-Path Unit",
    "doi": "https://doi.org/10.1145/3377851",
    "publication_date": "2020-02-07",
    "publication_year": 2020,
    "authors": "Yachao Li; Junhui Li; Min Zhang; Yixin Li; Peng Zou",
    "corresponding_authors": "",
    "abstract": "In neural machine translation (NMT), the source and target words are at the two ends of a large deep neural network, normally mediated by a series of non-linear activations. The problem with such consequent non-linear activations is that they significantly decrease the magnitude of the gradient in a deep neural network, and thus gradually loosen the interaction between source words and their translations. As a result, a source word may be incorrectly translated into a target word out of its translational equivalents. In this article, we propose short-path units (SPUs) to strengthen the association of source and target words by allowing information flow over adjacent layers effectively via linear interpolation. In particular, we enrich three critical NMT components with SPUs: (1) an enriched encoding model with SPU, which interpolates source word embeddings linearly into source annotations; (2) an enriched decoding model with SPU, which enables the source context linearly flow to target-side hidden states; and (3) an enriched output model with SPU, which further allows linear interpolation of target-side hidden states into output states. Experimentation on Chinese-to-English, English-to-German, and low-resource Tibetan-to-Chinese translation tasks demonstrates that the linear interpolation of SPUs significantly improves the overall translation quality by 1.88, 1.43, and 3.75 BLEU, respectively. Moreover, detailed analysis shows that our approaches much strengthen the association of source and target words. From the preceding, we can see that our proposed model is effective both in rich- and low-resource scenarios.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3016012197",
    "type": "article"
  },
  {
    "title": "Extracting Arabic Composite Names Using Genitive Principles of Arabic Grammar",
    "doi": "https://doi.org/10.1145/3382187",
    "publication_date": "2020-06-07",
    "publication_year": 2020,
    "authors": "Hussein Khalil; Taha Osman; Mohammed Miltan",
    "corresponding_authors": "",
    "abstract": "Named Entity Recognition (NER) is a basic prerequisite of using Natural Language Processing (NLP) for information retrieval. Arabic NER is especially challenging as the language is morphologically rich and has short vowels with no capitalisation convention. This article presents a novel rule-based approach that uses linguistic grammar-based techniques to extract Arabic composite names from Arabic text. Our approach uniquely exploits the genitive Arabic grammar rules; in particular, the rules regarding the identification of definite nouns (معرفة) and indefinite nouns (نكرة) to support the process of extracting composite names. Based on domain knowledge and Arabic Genitive Rules (AGR), the developed approach formalises a set of syntactical rules and linguistic patterns that initially use genitive patterns to classify definiteness within phrases and then extracts proper composite names from the unstructured text. The developed novel approach does not place any constraints on the length of the Arabic composite name and our initial experimentation demonstrated high recall and precision results when the NER algorithm was applied to a financial domain corpus.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3034668876",
    "type": "article"
  },
  {
    "title": "Inside Importance Factors of Graph-Based Keyword Extraction on Chinese Short Text",
    "doi": "https://doi.org/10.1145/3388971",
    "publication_date": "2020-06-21",
    "publication_year": 2020,
    "authors": "Junjie Chen; Hongxu Hou; Jing Gao",
    "corresponding_authors": "",
    "abstract": "Keywords are considered to be important words in the text and can provide a concise representation of the text. With the surge of unlabeled short text on the Internet, automatic keyword extraction task has proven useful in other information processing applications. Graph-based approaches are prevalent unsupervised models for this task. However, most of these methods emphasize the importance of the relation between words without considering other importance factors. Furthermore, when measuring the importance of a word in a text, the damping factor is set to 0.85 following PageRank. To the best of our knowledge, there is no existing work investigating the impact of the damping factor on the keyword extraction task. In addition, there are few publicly available labeled Chinese short text datasets for this task. In this article, we investigate the importance parts of words in a given document and propose an improved graph-based method for keyword extraction from short documents. Moreover, we analyze the impact of importance factors on performance. We also provide annotated long and short Chinese datasets for this task. The model is performed on Chinese and English datasets, and results show that our model obtains improvements in performance over the previous unsupervised models on short documents. Comparative experiments show that the damping factor is related to the text length, which is neglected in traditional methods.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3035905614",
    "type": "article"
  },
  {
    "title": "Deep Neural Network--based Machine Translation System Combination",
    "doi": "https://doi.org/10.1145/3389791",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Long Zhou; Jiajun Zhang; Xiaomian Kang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Deep neural networks (DNNs) have provably enhanced the state-of-the-art natural language process (NLP) with their capability of feature learning and representation. As one of the more challenging NLP tasks, neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy and word coverage. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this article, we propose a deep neural network--based system combination framework leveraging both minimum Bayes-risk decoding and multi-source NMT, which take as input the N-best outputs of NMT and SMT systems and produce the final translation. In particular, we apply the proposed model to both RNN and self-attention networks with different segmentation granularity. We verify our approach empirically through a series of experiments on resource-rich Chinese⇒English and low-resource English⇒Vietnamese translation tasks. Experimental results demonstrate the effectiveness and universality of our proposed approach, which significantly outperforms the conventional system combination methods and the best individual system output.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3040757906",
    "type": "article"
  },
  {
    "title": "Hybridization between Neural Computing and Nature-Inspired Algorithms for a Sentence Similarity Model Based on the Attention Mechanism",
    "doi": "https://doi.org/10.1145/3447756",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Peiying Zhang; Xingzhe Huang; Maozhen Li; Yu Xue",
    "corresponding_authors": "",
    "abstract": "Sentence similarity analysis has been applied in many fields, such as machine translation, the question answering system, and voice customer service. As a basic task of natural language processing, sentence similarity analysis plays an important role in many fields. The task of sentence similarity analysis is to establish a sentence similarity scoring model through multi-features. In previous work, researchers proposed a variety of models to deal with the calculation of sentence similarity. But these models do not consider the association information of sentence pairs, but only input sentence pairs into the model. In this article, we propose a sentence feature extraction model based on multi-feature attention. In addition, with the development of deep learning and the application of nature-inspired algorithms, researchers have proposed various hybrid algorithms that combine nature-inspired algorithms with neural networks. The hybrid algorithms not only solve the problem of decision-making based on multiple features but also improve the performance of the model. In the model, we use the attention mechanism to extract sentence features and assign weight. Then, the convolutional neural network is used to reduce the dimension of the matrix. In the training process, we integrate the firefly algorithm in the neural networks. The experimental results show that the accuracy of our model is 74.21%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3133965099",
    "type": "article"
  },
  {
    "title": "A Hybrid Model for Named Entity Recognition on Chinese Electronic Medical Records",
    "doi": "https://doi.org/10.1145/3436819",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Yu Wang; Yining Sun; Zuchang Ma; Lisheng Gao; Yang Xu",
    "corresponding_authors": "",
    "abstract": "Electronic medical records (EMRs) contain valuable information about the patients, such as clinical symptoms, diagnostic results, and medications. Named entity recognition (NER) aims to recognize entities from unstructured text, which is the initial step toward the semantic understanding of the EMRs. Extracting medical information from Chinese EMRs could be a more complicated task because of the difference between English and Chinese. Some researchers have noticed the importance of Chinese NER and used the recurrent neural network or convolutional neural network (CNN) to deal with this task. However, it is interesting to know whether the performance could be improved if the advantages of the RNN and CNN can be both utilized. Moreover, RoBERTa-WWM, as a pre-training model, can generate the embeddings with word-level features, which is more suitable for Chinese NER compared with Word2Vec. In this article, we propose a hybrid model. This model first obtains the entities identified by bidirectional long short-term memory and CNN, respectively, and then uses two hybrid strategies to output the final results relying on these entities. We also conduct experiments on raw medical records from real hospitals. This dataset is provided by the China Conference on Knowledge Graph and Semantic Computing in 2019 (CCKS 2019). Results demonstrate that the hybrid model can improve performance significantly.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3160245402",
    "type": "article"
  },
  {
    "title": "A Neural Joint Model with BERT for Burmese Syllable Segmentation, Word Segmentation, and POS Tagging",
    "doi": "https://doi.org/10.1145/3436818",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Cunli Mao; Zhibo Man; Zhengtao Yu; Shengxiang Gao; Zhenhan Wang; Hongbin Wang",
    "corresponding_authors": "",
    "abstract": "The smallest semantic unit of the Burmese language is called the syllable. In the present study, it is intended to propose the first neural joint learning model for Burmese syllable segmentation, word segmentation, and part-of-speech ( POS ) tagging with the BERT. The proposed model alleviates the error propagation problem of the syllable segmentation. More specifically, it extends the neural joint model for Vietnamese word segmentation, POS tagging, and dependency parsing [28] with the pre-training method of the Burmese character, syllable, and word embedding with BiLSTM-CRF-based neural layers. In order to evaluate the performance of the proposed model, experiments are carried out on Burmese benchmark datasets, and we fine-tune the model of multilingual BERT. Obtained results show that the proposed joint model can result in an excellent performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3164626265",
    "type": "article"
  },
  {
    "title": "Learning Syllables Using Conv-LSTM Model for Swahili Word Representation and Part-of-speech Tagging",
    "doi": "https://doi.org/10.1145/3445975",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Casper Shikali Shivachi; Refuoe Mokhosi; Shijie Zhou; Qihe Liu",
    "corresponding_authors": "",
    "abstract": "The need to capture intra-word information in natural language processing (NLP) tasks has inspired research in learning various word representations at word, character, or morpheme levels, but little attention has been given to syllables from a syllabic alphabet. Motivated by the success of compositional models in morphological languages, we present a Convolutional-long short term memory (Conv-LSTM) model for constructing Swahili word representation vectors from syllables. The unified architecture addresses the word agglutination and polysemous nature of Swahili by extracting high-level syllable features using a convolutional neural network (CNN) and then composes quality word embeddings with a long short term memory (LSTM). The word embeddings are then validated using a syllable-aware language model ( 31.267 ) and a part-of-speech (POS) tagging task ( 98.78 ), both yielding very competitive results to the state-of-art models in their respective domains. We further validate the language model using Xhosa and Shona, which are syllabic-based languages. The novelty of the study is in its capability to construct quality word embeddings from syllables using a hybrid model that does not use max-over-pool common in CNN and then the exploitation of these embeddings in POS tagging. Therefore, the study plays a crucial role in the processing of agglutinative and syllabic-based languages by contributing quality word embeddings from syllable embeddings, a robust Conv–LSTM model that learns syllables for not only language modeling and POS tagging, but also for other downstream NLP tasks.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3165364314",
    "type": "article"
  },
  {
    "title": "TAMIZHİ: Historical Tamil-Brahmi Script Recognition Using CNN and MobileNet",
    "doi": "https://doi.org/10.1145/3402891",
    "publication_date": "2021-05-31",
    "publication_year": 2021,
    "authors": "S. Dhivya; Usha Devi Gandhi",
    "corresponding_authors": "",
    "abstract": "Computational epigraphy is the study of an ancient script where the computer science and mathematical model is relatively built for epigraphy. The Tamil-Brahmi inscriptions are the most ancient of the extant written of the Tamil. The inscriptions furnish valuable information on many aspects of life in the ancient Tamil country from a period anterior to the literary age of Sangam. The recognition of the script and systematic analysis of the script is required. The recognition of this script is complex, containing various curves for a single character and the style of writing overlap with curves and lines. Generating corpus of the script is necessary, since it is the initial step for computational epigraphy. The archaeological department has supported the raw data that helped to develop a corpus of Tamizhi. In this article, we have implemented a convolution neural network in various ways, i.e., (i) Training the CNN model from scratch a Softmax classifier in a sequential model (ii) using MobileNet: Transfer learning paradigm from a pre-trained model on a Tamizhi dataset (iii) Building Model with CNN and SVM (iv) SVM for evaluation of best accuracy to recognize handwritten Brahmi characters. To train the CNN Model an extensive TAMIZHİ handwritten Brahmi Dataset of 1lakh and 90,000 isolated samples for the character has been created and deployed. The designed dataset consists of 9 vowels and 18 consonants and 209 class so researchers can use machine learning. MobileNet outperformed among all the models implemented with the accuracy of 68.3%, whereas other algorithm ranges from 58% to 67% with respect to the Tamizhi dataset. MobileNet model is trained and tested for the dataset of vowels (8 class), consonants (18 class), and consonants vowels (26 class) with the accuracy of 98.1%, 97.7%, 97.5%, respectively.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3181548326",
    "type": "article"
  },
  {
    "title": "SE4ExSum: An Integrated Semantic-aware Neural Approach with Graph Convolutional Network for Extractive Text Summarization",
    "doi": "https://doi.org/10.1145/3464426",
    "publication_date": "2021-09-01",
    "publication_year": 2021,
    "authors": "Tham Vo",
    "corresponding_authors": "Tham Vo",
    "abstract": "Recently, advanced techniques in deep learning such as recurrent neural network (GRU, LSTM and Bi-LSTM) and auto-encoding (attention-based transformer and BERT) have achieved great successes in multiple application domains including text summarization. Recent state-of-the-art encoding-based text summarization models such as BertSum, PreSum and DiscoBert have demonstrated significant improvements on extractive text summarization tasks. However, recent models still encounter common problems related to the language-specific dependency which requires the supports of the external NLP tools. Besides that, recent advanced text representation methods, such as BERT as the sentence-level textual encoder, also fail to fully capture the representation of a full-length document. To address these challenges, in this paper we proposed a novel s emantic-ware e mbedding approach for ex tractive text sum marization , called as: SE4ExSum. Our proposed SE4ExSum is an integration between the use of feature graph-of-words (FGOW) with BERT-based encoder for effectively learning the word/sentence-level representations of a given document. Then, the g raph c onvolutional n etwork (GCN) based encoder is applied to learn the global document's representation which is then used to facilitate the text summarization task. Extensive experiments on benchmark datasets show the effectiveness of our proposed model in comparing with recent state-of-the-art text summarization models.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3196835517",
    "type": "article"
  },
  {
    "title": "Domain-Aware Word Segmentation for Chinese Language: A Document-Level Context-Aware Model",
    "doi": "https://doi.org/10.1145/3481298",
    "publication_date": "2021-11-03",
    "publication_year": 2021,
    "authors": "Kaiyu Huang; Keli Xiao; Fengran Mo; Bo Jin; Zhuang Liu; Degen Huang",
    "corresponding_authors": "",
    "abstract": "Word segmentation is an essential and challenging task in natural language processing, especially for the Chinese language due to its high linguistic complexity. Existing methods for Chinese word segmentation, including statistical machine learning methods and neural network methods, usually have good performance in specific knowledge domains. Given the increasing importance of interdisciplinary and cross-domain studies, one of the challenges in cross-domain word segmentation is to handle the out-of-vocabulary (OOV) words. Existing methods show unsatisfactory performance to meet the practical standard. To this end, we propose a document-level context-aware model that can automatically perceive and identify OOV words from different domains. Our method jointly implements a word-based and a character-based model and then processes the results with a newly proposed reconstruction model. We evaluate the new method by designing and conducting comprehensive experiments on two real-world datasets (e.g., news from different domains). The results demonstrate the superiority of our method over the state-of-the-art models in handling texts from different domains. Importantly, when doing the word segmentation under the cross-domain scenario, our proposed method can improve the performance of OOV words recognition.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3210607400",
    "type": "article"
  },
  {
    "title": "STEMUR: An Automated Word Conflation Algorithm for the Urdu Language",
    "doi": "https://doi.org/10.1145/3476226",
    "publication_date": "2021-11-09",
    "publication_year": 2021,
    "authors": "Tayyaba Fatima; Raees Ul Islam; Muhammad Waqas Anwar; Muhammad Hasan Jamal; Muhammad Tayyab Chaudhry; Zeeshan Gillani",
    "corresponding_authors": "",
    "abstract": "Stemming is a common word conflation method that perceives stems embedded in the words and decreases them to their stem (root) by conflating all the morphologically related terms into a single term, without doing a complete morphological analysis. This article presents STEMUR, an enhanced stemming algorithm for automatic word conflation for Urdu language. In addition to handling words with prefixes and suffixes, STEMUR also handles words with infixes. Rather than using a totally unsupervised approach, we utilized the linguistic knowledge to develop a collection of patterns for Urdu infixes to enhance the accuracy of the stems and affixes acquired during the training process. Additionally, STEMUR also handles English loan words and can handle words with more than one affix. STEMUR is compared with four existing Urdu stemmers including Assas-Band and the template-based stemmer that are also implemented in this study. Results are processed on two corpora containing 89,437 and 30,907 words separately. Results show clear improvements regarding strength and accuracy of STEMUR. The use of maximum possible infix rules boosted our stemmer's accuracy up to 93.1% and helped us achieve a precision of 98.9%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3211718567",
    "type": "article"
  },
  {
    "title": "Neural Arabic Text Diacritization: State-of-the-Art Results and a Novel Approach for Arabic NLP Downstream Tasks",
    "doi": "https://doi.org/10.1145/3470849",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Ali Fadel; Ibraheem Tuffaha; Mahmoud Al‐Ayyoub",
    "corresponding_authors": "",
    "abstract": "In this work, we present several deep learning models for the automatic diacritization of Arabic text. Our models are built using two main approaches, viz. Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN), with several enhancements such as 100-hot encoding, embeddings, Conditional Random Field (CRF), and Block-Normalized Gradient (BNG). The models are tested on the only freely available benchmark dataset and the results show that our models are either better or on par with other models even those requiring human-crafted language-dependent post-processing steps, unlike ours. Moreover, we show how diacritics in Arabic can be used to enhance the models of downstream NLP tasks such as Machine Translation (MT) and Sentiment Analysis (SA) by proposing novel Translation over Diacritization (ToD) and Sentiment over Diacritization (SoD) approaches.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4200304220",
    "type": "article"
  },
  {
    "title": "Corpus-Based Translation Induction in Indian Languages Using Auxiliary Language Corpora from Wikipedia",
    "doi": "https://doi.org/10.1145/3038295",
    "publication_date": "2017-03-17",
    "publication_year": 2017,
    "authors": "Goutham Tholpadi; Chiranjib Bhattacharyya; Shirish Shevade",
    "corresponding_authors": "",
    "abstract": "Identifying translations from comparable corpora is a well-known problem with several applications. Existing methods rely on linguistic tools or high-quality corpora. Absence of such resources, especially in Indian languages, makes this problem hard; for example, state-of-the-art techniques achieve a mean reciprocal rank of 0.66 for English-Italian, and a mere 0.187 for Telugu-Kannada. In this work, we address the problem of comparable corpora-based translation correspondence induction (CC-TCI) when the only resources available are small noisy comparable corpora extracted from Wikipedia. We observe that translations in the source and target languages have many topically related words in common in other “auxiliary” languages. To model this, we define the notion of a translingual theme , a set of topically related words from auxiliary language corpora, and present a probabilistic framework for CC-TCI. Extensive experiments on 35 comparable corpora showed dramatic improvements in performance. We extend these ideas to propose a method for measuring cross-lingual semantic relatedness (CLSR) between words. To stimulate further research in this area, we make publicly available two new high-quality human-annotated datasets for CLSR. Experiments on the CLSR datasets show more than 200% improvement in correlation on the CLSR task. We apply the method to the real-world problem of cross-lingual Wikipedia title suggestion and build the WikiTSu system. A user study on WikiTSu shows a 20% improvement in the quality of titles suggested.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2600077227",
    "type": "article"
  },
  {
    "title": "Inducing a Bilingual Lexicon from Short Parallel Multiword Sequences",
    "doi": "https://doi.org/10.1145/3003726",
    "publication_date": "2017-03-17",
    "publication_year": 2017,
    "authors": "Andrew Finch; Taisuke Harada; Kumiko Tanaka‐Ishii; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "This article proposes a technique for mining bilingual lexicons from pairs of parallel short word sequences. The technique builds a generative model from a corpus of training data consisting of such pairs. The model is a hierarchical nonparametric Bayesian model that directly induces a bilingual lexicon while training. The model learns in an unsupervised manner and is designed to exploit characteristics of the language pairs being mined. The proposed model is capable of utilizing commonly used word-pair frequency information and additionally can employ the internal character alignments within the words themselves. It is thereby capable of mining transliterations and can use reliably aligned transliteration pairs to support the mining of other words in their context. The model is also capable of performing word reordering and word deletion during the alignment process, and it is furthermore capable of operating in the absence of full segmentation information. In this work, we study two mining tasks based on English-Japanese and English-Chinese language pairs, and compare the proposed approach to baselines based on a simpler models that use only word-pair frequency information. Our results show that the proposed method is able to mine bilingual word pairs at higher levels of precision and recall than the baselines.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2603776682",
    "type": "article"
  },
  {
    "title": "Cleaning of Online Bangla Free-form Handwritten Text",
    "doi": "https://doi.org/10.1145/3145538",
    "publication_date": "2017-11-02",
    "publication_year": 2017,
    "authors": "Nilanjana Bhattacharya; Umapada Pal; Partha Pratim Roy",
    "corresponding_authors": "",
    "abstract": "In the normal free-form handwritten text, repetition (repeated writing of the same stroke several times in the same place), over-writing, and crossing out are very common. In this article, we call the presence of these three types of writing as “noise.” Cleaning to extract useful text from such types of noisy text is an important task for robust recognition. To the best of our knowledge, no work has been reported on cleaning of such noise from online text in any scripts and hence, in this article, we propose an automatic text-cleaning approach for online handwriting recognition. Here, at first, crossing out noise with straight strike-through lines is detected using the straightness criteria of online strokes. Next, regions containing repetition, over-writing, and other types of crossing out are located using the positional information of the overlapping strokes. Stroke density, self-intersections of strokes etc. are computed from the strokes of located regions to predict the type of noise and this type of information is used as follows for their cleaning. For cleaning of crossing outs, all strokes of the crossing-out region are removed. For cleaning repetition and over-writing, strokes written earlier are removed, keeping the latest strokes. Finally, delayed strokes are properly arranged and word is passed to online recognizer. Though recognition of free-form handwriting is quite difficult, in this attempt, we obtained up to 70.71% improvement in word-recognition accuracy after noise cleaning.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2765549284",
    "type": "article"
  },
  {
    "title": "Bangla Handwritten Character Segmentation Using Structural Features",
    "doi": "https://doi.org/10.1145/2890497",
    "publication_date": "2016-04-12",
    "publication_year": 2016,
    "authors": "Tapan Kumar Bhowmik; Swapan Kumar Parui; Utpal Roy; Lambert Schomaker",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a new framework for segmentation of Bangla handwritten word images into meaningful individual symbols or pseudo-characters. Existing segmentation algorithms are not usually treated as a classification problem. However, in the present study, the segmentation algorithm is looked upon as a two-class supervised classification problem. The method employs an SVM classifier to select the segmentation points on the word image on the basis of various structural features. For training of the SVM classifier, an unannotated training set is prepared first using candidate segmenting points. The training set is then clustered, and each cluster is labeled manually with minimal manual intervention. A semi-automatic bootstrapping technique is also employed to enlarge the training set from new samples. The overall architecture describes a basic step toward building an annotation system for the segmentation problem, which has not so far been investigated. The experimental results show that our segmentation method is quite efficient in segmenting not only word images but also handwritten texts. As a part of this work, a database of Bangla handwritten word images has also been developed. Considering our data collection method and a statistical analysis of our lexicon set, we claim that the relevant characteristics of an ideal lexicon set are present in our handwritten word image database.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2336568965",
    "type": "article"
  },
  {
    "title": "Improving Unsupervised Dependency Parsing with Knowledge from Query Logs",
    "doi": "https://doi.org/10.1145/2903720",
    "publication_date": "2016-06-22",
    "publication_year": 2016,
    "authors": "Xiuming Qiao; Hailong Cao; Tiejun Zhao",
    "corresponding_authors": "",
    "abstract": "Unsupervised dependency parsing becomes more and more popular in recent years because it does not need expensive annotations, such as treebanks, which are required for supervised and semi-supervised dependency parsing. However, its accuracy is still far below that of supervised dependency parsers, partly due to the fact that their parsing model is insufficient to capture linguistic phenomena underlying texts. The performance for unsupervised dependency parsing can be improved by mining knowledge from the texts and by incorporating it into the model. In this article, syntactic knowledge is acquired from query logs to help estimate better probabilities in dependency models with valence. The proposed method is language independent and obtains an improvement of 4.1% unlabeled accuracy on the Penn Chinese Treebank by utilizing additional dependency relations from the Sogou query logs and Baidu query logs. Morever, experiments show that the proposed model achieves improvements of 8.07% on CoNLL 2007 English using the AOL query logs. We believe query logs are useful sources of syntactic knowledge for many natural language processing (NLP) tasks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2461929917",
    "type": "article"
  },
  {
    "title": "Minimally Supervised Chinese Event Extraction from Multiple Views",
    "doi": "https://doi.org/10.1145/2994600",
    "publication_date": "2016-11-04",
    "publication_year": 2016,
    "authors": "Peifeng Li; Guodong Zhou; Qiaoming Zhu",
    "corresponding_authors": "",
    "abstract": "Although several semi-supervised learning models have been proposed for English event extraction, there are few successful stories in Chinese due to its special characteristics. In this article, we propose a novel minimally supervised model for Chinese event extraction from multiple views. Besides the traditional pattern similarity view (PSV), a semantic relationship view (SRV) is introduced to capture the relevant event mentions from relevant documents. Moreover, a morphological structure view (MSV) is incorporated to both infer more positive patterns and help filter negative patterns via morphological structure similarity. An evaluation of the ACE 2005 Chinese corpus shows that our minimally supervised model significantly outperforms several strong baselines.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2554126788",
    "type": "article"
  },
  {
    "title": "Improving Vector Space Word Representations Via Kernel Canonical Correlation Analysis",
    "doi": "https://doi.org/10.1145/3197566",
    "publication_date": "2018-07-21",
    "publication_year": 2018,
    "authors": "Xuefeng Bai; Hailong Cao; Tiejun Zhao",
    "corresponding_authors": "",
    "abstract": "Cross-lingual word embeddings are representations for vocabularies of two or more languages in one common continuous vector space and are widely used in various natural language processing tasks. A state-of-the-art way to generate cross-lingual word embeddings is to learn a linear mapping, with an assumption that the vector representations of similar words in different languages are related by a linear relationship. However, this assumption does not always hold true, especially for substantially different languages. We therefore propose to use kernel canonical correlation analysis to capture a non-linear relationship between word embeddings of two languages. By extensively evaluating the learned word embeddings on three tasks (word similarity, cross-lingual dictionary induction, and cross-lingual document classification) across five language pairs, we demonstrate that our proposed approach achieves essentially better performances than previous linear methods on all of the three tasks, especially for language pairs with substantial typological difference.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2883309503",
    "type": "article"
  },
  {
    "title": "Input Method for Human Translators",
    "doi": "https://doi.org/10.1145/3230638",
    "publication_date": "2018-11-12",
    "publication_year": 2018,
    "authors": "Guoping Huang; Jiajun Zhang; Zhou Yu; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Computer-aided translation (CAT) systems are the most popular tool for helping human translators efficiently perform language translation. To further improve the translation efficiency, there is an increasing interest in applying machine translation (MT) technology to upgrade CAT. To thoroughly integrate MT into CAT systems, in this article, we propose a novel approach: a new input method that makes full use of the knowledge adopted by MT systems, such as translation rules, decoding hypotheses, and n-best translation lists. The proposed input method contains two parts: a phrase generation model, allowing human translators to type target sentences quickly, and an n-gram prediction model, helping users choose perfect MT fragments smoothly. In addition, to tune the underlying MT system to generate the input method preferable results, we design a new evaluation metric for the MT system. The proposed input method integrates MT effectively and imperceptibly, and it is particularly suitable for many target languages with complex characters, such as Chinese and Japanese. The extensive experiments demonstrate that our method saves more than 23% in time and over 42% in keystrokes, and it also improves the translation quality by more than 5 absolute BLEU scores compared with the strong baseline, i.e., post-editing using Google Pinyin.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2901931512",
    "type": "article"
  },
  {
    "title": "Wikipedia-Based Relatedness Measurements for Multilingual Short Text Clustering",
    "doi": "https://doi.org/10.1145/3276473",
    "publication_date": "2018-12-14",
    "publication_year": 2018,
    "authors": "Tatsuya Nakamura; Masumi Shirakawa; Takahiro Hara; Shojiro Nishio",
    "corresponding_authors": "",
    "abstract": "Throughout the world, people can post information about their local area in their own languages using social networking services. Multilingual short text clustering is an important task to organize such information, and it can be applied to various applications, such as event detection and summarization. However, measuring the relatedness between short texts written in various languages is a challenging problem. In addition to handling multiple languages, the semantic gaps among all languages must be considered. In this article, we propose two Wikipedia-based semantic relatedness measurement methods for multilingual short text clustering. The proposed methods solve the semantic gap problem by incorporating the inter-language links of Wikipedia into Extended Naive Bayes (ENB), a probabilistic method that can be applied to measure semantic relatedness among monolingual short texts. The proposed methods represent a multilingual short text as a vector of the English version of Wikipedia articles (entities). By transferring texts to a unified vector space, the relatedness between texts in different languages with similar meanings can be increased. We also propose an approach that can improve clustering performance and reduce the processing time by eliminating language-specific entities in the unified vector space. Experimental results on multilingual Twitter message clustering revealed that the proposed methods outperformed cross-lingual explicit semantic analysis, a previously proposed method to measure relatedness between texts in different languages. Moreover, the proposed methods were comparable to ENB applied to texts translated into English using a proprietary translation service. The proposed methods enabled relatedness measurements for multilingual short text clustering without requiring machine translation processes.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2904911136",
    "type": "article"
  },
  {
    "title": "Chinese-Catalan",
    "doi": "https://doi.org/10.1145/3312575",
    "publication_date": "2019-04-22",
    "publication_year": 2019,
    "authors": "Marta R. Costa‐jussà; Noé Casas; Carlos Escolano; José A. R. Fonollosa",
    "corresponding_authors": "",
    "abstract": "This article innovatively addresses machine translation from Chinese to Catalan using neural pivot strategies trained without any direct parallel data. The Catalan language is very similar to Spanish from a linguistic point of view, which motivates the use of Spanish as pivot language. Regarding neural architecture, we are using the latest state-of-the-art, which is the Transformer model, only based on attention mechanisms. Additionally, this work provides new resources to the community, which consists of a human-developed gold standard of 4,000 sentences between Catalan and Chinese and all the others United Nations official languages (Arabic, English, French, Russian, and Spanish). Results show that the standard pseudo-corpus or synthetic pivot approach performs better than cascade.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2941983400",
    "type": "article"
  },
  {
    "title": "Order-Sensitive Keywords Based Response Generation in Open-Domain Conversational Systems",
    "doi": "https://doi.org/10.1145/3343258",
    "publication_date": "2019-08-22",
    "publication_year": 2019,
    "authors": "Qingfu Zhu; Weinan Zhang; Lei Cui; Ting Liu",
    "corresponding_authors": "",
    "abstract": "External keywords are crucial for response generation models to address the generic response problems in open-domain conversational systems. The occurrence of keywords in a response depends heavily on the order of the keywords as they are generated sequentially. Meanwhile, the order of keywords also affects the semantics of a response. Previous keywords based methods mainly focus on the composite of keywords, while the order of keywords has not been sufficiently discussed. In this work, we propose an order-sensitive keywords based model to explore the influence of the order of keywords in open-domain response generation. It automatically inferences the most suitable order that is optimized to generate a natural and relevant response, and subsequently generates the response using the ordered keywords as building blocks. We conducted experiments on a public Twitter dataset and the results show that our approach outperforms the state-of-the-art baselines in both automatic and human evaluations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2969379005",
    "type": "article"
  },
  {
    "title": "Word Reordering for Translation into Korean Sign Language Using Syntactically-guided Classification",
    "doi": "https://doi.org/10.1145/3357612",
    "publication_date": "2019-11-23",
    "publication_year": 2019,
    "authors": "Hun-Young Jung; Jong-Hyeok Lee; Eunju Min; Seung‐Hoon Na",
    "corresponding_authors": "",
    "abstract": "Machine translation aims to break the language barrier that prevents communication with others and increase access to information. Deaf people face huge language barriers in their daily lives, including access to digital and spoken information. There are very few digital resources for sign language processing. In this article, we present a transfer-based machine translation system for translating Korean-to-Korean Sign Language (KSL) glosses, mainly composed of (1) dictionary-based lexical transfer and (2) a hybrid syntactic transfer based on a data-driven model. In particular, we formulate complicated word reordering problems in syntactic transfer as multi-class classification tasks and propose “syntactically guided” data-driven syntactic transfer. The core part of our study is a neural classification model for reordering order-important constituent pairs with a reordering task that is newly designed for Korean-to-KSL translation. The experiment results evaluated on news transcript data show that the proposed system achieves a BLEU score of 0.512 and a RIBES score of 0.425, significantly improving upon the baseline system performance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2990344286",
    "type": "article"
  },
  {
    "title": "A Survey of Discourse Representations for Chinese Discourse Annotation",
    "doi": "https://doi.org/10.1145/3293442",
    "publication_date": "2019-01-25",
    "publication_year": 2019,
    "authors": "Xiaomian Kang; Chengqing Zong; Nianwen Xue",
    "corresponding_authors": "",
    "abstract": "A key element in computational discourse analysis is the design of a formal representation for the discourse structure of a text. With machine learning being the dominant method, it is important to identify a discourse representation that can be used to perform large-scale annotation. This survey provides a systematic analysis of existing discourse representation theories to evaluate whether they are suitable for annotation of Chinese text. Specifically, the two properties, expressiveness and practicality, are introduced to compare the representations of theories based on rhetorical relations and the representations of theories based on entity relations. The comparison systematically reveals linguistic and computational characteristics of the theories. After that, we conclude that none of the existing theories are quite suitable for scalable Chinese discourse annotation because they are not both expressive and practical. Therefore, a new discourse representation needs to be proposed, which should balance the expressiveness and practicality, and cover rhetorical relations and entity relations. Inspired by the conclusions, this survey discusses some preliminary proposals on how to represent the discourse structure that are worth pursuing.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2997170739",
    "type": "article"
  },
  {
    "title": "Fusion of Spatio-temporal Information for Indic Word Recognition Combining Online and Offline Text Data",
    "doi": "https://doi.org/10.1145/3364533",
    "publication_date": "2019-11-21",
    "publication_year": 2019,
    "authors": "Subham Mukherjee; Pradeep Kumar; Partha Pratim Roy",
    "corresponding_authors": "",
    "abstract": "We present a novel Indic handwritten word recognition scheme by fusion of spatio-temporal information extracted from handwritten images. The main challenge in Indic word recognition lies in its complexity because of modifiers, touching characters, and compound characters. Hidden Markov Models (HMMs) are being used to model such data due to their ability to learn sequential data, however, the recognition performance is not satisfactory. We propose here a Long Short-Term Memory (LSTM)-based architecture for offline Indic word recognition. Offline recognition methods usually involve spatial data, whereas it has been observed that online recognition schemes show better performance than the offline methodologies. Online information usually refers to the temporal information obtained from the strokes of the pen tip while writing, which is missing in offline word images. In this article, an effort has been made to extract the online temporal information from offline images using stroke recovery and later it is combined with spatial information in LSTM architecture. During recognition, the character models are trained using both offline and extracted pseudo-online handwritten data separately. Finally, a novel fusion scheme has been used to combine them together. From the experiment, it is noted that recognition performance of handwritten Indic words improves considerably due to the fusion scheme of spatial and temporal data.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3002243754",
    "type": "article"
  },
  {
    "title": "A Burmese (Myanmar) Treebank",
    "doi": "https://doi.org/10.1145/3373268",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Chenchen Ding; Sann Su Su Yee; Win Pa Pa; Khin Mar Soe; Masao Utiyama; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "A 20,000-sentence Burmese (Myanmar) treebank on news articles has been released under a CC BY-NC-SA license. Complete phrase structure annotation was developed for each sentence from the morphologically annotated data prepared in previous work of Ding et al. [1]. As the final result of the Burmese component in the Asian Language Treebank Project , this is the first large-scale, open-access treebank for the Burmese language. The annotation details and features of this treebank are presented.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3014864092",
    "type": "article"
  },
  {
    "title": "Machine Normalization",
    "doi": "https://doi.org/10.1145/3378414",
    "publication_date": "2020-04-11",
    "publication_year": 2020,
    "authors": "Randa Zarnoufi; Hamid Jaafar; Mounia Abık",
    "corresponding_authors": "",
    "abstract": "User-generated text in social media communication (SMC) is mainly characterized by non-standard form. It may contain code switching (CS) text, a widespread phenomenon in SMC, in addition to noisy elements used, especially in written conversations (use of abbreviations, symbols, emoticons) or misspelled words. All of these factors constitute a wall in front of text mining applications. Common text mining tools are dedicated to standard use of standard languages but cannot deal with other forms, especially written text in social media. To overcome these problems, in this work we present our solution for the normalization of non-standard use of standard and non-standard languages (dialects) in SMC text with the use of existent resources and tools. The main processing in our solution consists of CS normalization from multiple to one language by the use of a machine translation--like approach. This processing relies on a linguistic approach of CS, which aims at identifying automatically the translation source and target languages (without human intervention). The remaining processing operations concern the normalization of SMC special expressions and spelling correction of out-of-vocabulary words. To preserve the coded-switched sentence meaning across translation, we adopt a knowledge-based approach for word sense translation disambiguation reinforced with a multi-lingual vertical context. All of these processes are embedded in what we refer to as the machine normalization system. Our solution can be used as a front-end of text mining processing, enabling the analysis of SMC noisy text. The conducted experiments show that our system performs better than considered baselines.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3015869721",
    "type": "article"
  },
  {
    "title": "Study on Automated Approach to Recognize Characters for Handwritten and Historical Document",
    "doi": "https://doi.org/10.1145/3396167",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "S. Dhivya; Usha Devi Gandhi",
    "corresponding_authors": "",
    "abstract": "Script recognition is the mechanism of automatic script analysis and recognition whereby intensive study has been carried out and a significant amount of papers on this problem have been released over the past. But there are still a few issues to be solved, particularly in Indian historical manuscripts. This literature examines the Script recognition with reference to multi-script document and different historical scripts such as Kurdish-Latin, Devanagari, Grantha, Arabic handwritten characters, Bangladesh, Devanagari and Gurumukhi, ancient Chinese, Arabic, Nam Character, Greek, Nastalique Urdu, Georgian handwritten, Nandinagari, and Hebrew, which provide the course of study that focuses on the framework for script recognition. This review concentrates on scope of prediction, dataset type, the methods used for data preprocessing, and measures of performance used for analysis. On the basis of this survey, Current research constraints have been recognized and future study specifications are emphasized in the area of modeling historical manuscripts. CCS Concepts:",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3042175449",
    "type": "article"
  },
  {
    "title": "Knowledge Discovery of News Text Based on Artificial Intelligence",
    "doi": "https://doi.org/10.1145/3418062",
    "publication_date": "2020-11-23",
    "publication_year": 2020,
    "authors": "Guangce Ruan; Lei Xia",
    "corresponding_authors": "",
    "abstract": "The explosion of news text and the development of artificial intelligence provide a new opportunity and challenge to provide high-quality media monitoring service. In this article, we propose a semantic analysis approach based on the Latent Dirichlet Allocation (LDA) and Apriori algorithm, and we realize application to improve media monitoring reports by mining large-scale news text. First, we propose to use LDA model to mine news text topic words and reducing news dimensionality. Then, we propose to use Apriori algorithm to discovering the relationship of topic words. Finally, we discovery the relevance of news text topic words and show the intensity and dependency among topic words through drawing. This application can realize to extract the news topics and discover the correlation and dependency among news topics in mass news text. The results show that the method based on LDA and Apriori can help the media monitoring staff to better understand the hidden knowledge in the news text and improve the media analysis report.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3108395396",
    "type": "article"
  },
  {
    "title": "Text-to-Speech Synthesis: Literature Review with an Emphasis on Malayalam Language",
    "doi": "https://doi.org/10.1145/3501397",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "M. P. Jasir; Kannan Balakrishnan",
    "corresponding_authors": "",
    "abstract": "Text-to-Speech Synthesis (TTS) is an active area of research to generate synthetic speech from underlying text. The identified syllables are uttered with proper duration and prosody characteristics to emulate natural speech. It falls under the category of Natural Language Processing (NLP), which aims to bridge the gap in communication between human and machine. So far as Western languages like English are concerned, the research to produce intelligent and natural synthetic speech has advanced considerably. But in a multilingual state like India, many regional languages viz. Malayalam is underexplored when it comes to NLP. In this article, we try to amalgamate the major research works performed in the area of TTS in English and the prominent Indian languages, with a special emphasis on the South Indian language, Malayalam. This review intends to provide right direction to the research activities in the language, in the area of TTS.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4205167257",
    "type": "article"
  },
  {
    "title": "Interactive Gated Decoder for Machine Reading Comprehension",
    "doi": "https://doi.org/10.1145/3501399",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Yiming Cui; Wanxiang Che; Ziqing Yang; Ting Liu; Bing Qin; Shijin Wang; Guoping Hu",
    "corresponding_authors": "",
    "abstract": "Owing to the availability of various large-scale Machine Reading Comprehension ( MRC ) datasets, building an effective model to extract passage spans for question answering has been well studied in previous works. However, in reality, there are some questions that cannot be answered through the passage information, which brings more challenges to this task. In this article, we propose an Interactive Gated Decoder ( IG Decoder ), which focuses on modeling the interactions between the answer span prediction and no-answer prediction with a gating mechanism. We also propose a simple but effective approach for automatically generating pseudo training data, which aims to enrich the training data of the unanswerable questions. Experimental results on popular benchmark SQuAD 2.0 and NewsQA show that the proposed approaches yield consistent improvements over traditional BERT-large and strong ALBERT-xxlarge baseline systems. We also provide detailed ablations of the proposed method and error analysis on hard samples, which could be helpful in future research.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4205168733",
    "type": "article"
  },
  {
    "title": "Find Supports for the Post about Mental Issues: More Than Semantic Matching",
    "doi": "https://doi.org/10.1145/3508373",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Zhao Yun; Dexi Liu; Changxuan Wan; Xiping Liu; Xiangqing Qiu; Jian‐Yun Nie",
    "corresponding_authors": "",
    "abstract": "Mental-health-oriented question-answering (MH-QA) aims at retrieving an appropriate response for a question post involving mental health issues, which will be used to assist counsellors to reply to the support seeker. This task is different from the general QA task because many additional criteria such as emotions are involved. In this paper, we propose the Multi-Feature Graph Convolutional Network model (MF-GCN) to integrate not only semantic features, but also mental health related features and context features, to match question posts and responses. Different types of feature are exploited through multiple graph convolutional networks. A new dataset is constructed for MH-QA to evaluate our model. Experimental results show that our model with mental health features significantly outperforms a wide range of state-of-the-art models without them.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210718628",
    "type": "article"
  },
  {
    "title": "Importance of Combination of Ecology and Literature",
    "doi": "https://doi.org/10.1145/3519597",
    "publication_date": "2022-03-30",
    "publication_year": 2022,
    "authors": "Xuegang Fan; C. B. Sivaparthipan; R.S. Aiswarya",
    "corresponding_authors": "",
    "abstract": "Ecology empowers the environment and is critical to human prosperity and well-being. It offers an innovative approachto the interconnectivity between humans and nature, crucial to food security, clean air and water, and conservation sustainability in a changing world. A selection of literary texts from fiction, poetry, and drama is read and answered in literature. They learn to value words and their strength, journey through the texts into other worlds and ages, understand their society and others, and learn to experience character, joy, and pain.Ecocriticism analyses the relationship between literature and ecology and their synthesis. It is an interdisciplinary study of ecology and literature unique as a synthesis of science and humanism. Machine learning methods are gradually becoming applied science, a significant branch of artificial intelligence.It is suggested that potential machine learning apps in ecocriticism can become an even more appealing platform in the face of \"big data\" and can soon open up new ways of study to further data such as sound and picture.The concept and role of ecocriticism of Artificial Intelligence (AI)are discussed in these reports. By comparison, the AI-Ecocriticism (AI-EC) framework covers the literature-ecological interactions or how literature reflects the physical world. In a period in which automated population and environment tracking produce many data that human beings can no longer analyze, machine learning could become a requirement.The ecology score and the carbon footprint of the particular area are analyzed and reported that ecology score marked 18% average in the last 50 years and carbon footprint reduced remarkably.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4220659668",
    "type": "article"
  },
  {
    "title": "Construction of a Chinese Corpus for Multi-Type Economic Event Relation",
    "doi": "https://doi.org/10.1145/3527240",
    "publication_date": "2022-03-26",
    "publication_year": 2022,
    "authors": "Qizhi Wan; Changxuan Wan; Keli Xiao; Dexi Liu; Qing Liu; Jiangling Deng; Wenkang Luo; Rong Hu",
    "corresponding_authors": "",
    "abstract": "We construct a Chinese Economic Event Treebank (CEETB) , focusing on revealing economic and finance events and their relations. Investigating economic event relations will benefit academic research and practice in not just economics but many other scientific areas. The characteristics of economic-related texts (e.g., abundant longer enterprises names and terms) and the Chinese language speciality (e.g., component ellipsis in long sentences) have resulted in challenges in the event relation extraction task. Existing Chinese corpora containing economic event relations mainly focused on finance areas (e.g., the equity market) and only covered a few event types. To support research that may involve economic text analysis in Chinese, our CEETB is constructed following a carefully designed process. First, based on practical and research requirements, we summarize nine different types of event relations and four types of component ellipses in economic texts. Then, an excellent annotation scheme is presented to hyalinize the model, strategy, and process in annotation, followed by statistical analysis and quality evaluation for the CEETB corpus. Finally, to demonstrate the strengths of the constructed corpus in practical applications, we conduct experiments on five SOTA models for event relation extraction.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4220978177",
    "type": "article"
  },
  {
    "title": "Deep Learning in Computational Linguistics for Chinese Language Translation",
    "doi": "https://doi.org/10.1145/3519386",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Hailin Feng; Shuxuan Xie; Wei Wei; Haibin Lv; Zhihan Lv",
    "corresponding_authors": "",
    "abstract": "Applying artificial intelligence to Chinese language translation in computational linguistics is of practical significance for economic boosts and cultural exchanges. In the present work, the bi-directional long short-term memory (BiLSTM) network is employed to extract Chinese text features regarding the overlapping semantic roles in Chinese language translation and hard-to-converge training of high-dimensional text word vectors in text classification during translation. In addition, AlexNet is optimized to extract the local features of the text and meanwhile update and learn network parameters in the deep network. Then, the attention mechanism is introduced to build a forecasting algorithm of Chinese language translation based on BiLSTM and improved AlexNet. Last, the forecasting algorithm is simulated to validate its performance. Some state-of-the-art algorithms are selected for a comparative experiment, including long short-term memory, regions with convolutional neural network features, AlexNet, and support vector machine. Results demonstrate that the forecasting algorithm proposed here can achieve a feature identification accuracy of 90.55%, at least an improvement of 4.24% over other algorithms. In addition, it provides an area under the curve of above 90%, a training duration of about 54.21 seconds, and a test duration of about 19.07 seconds. Regarding the performance of Chinese language translation, the algorithm proposed here provides a bilingual evaluation understudy (BLEU) value of 28.21 on the training set, with a performance gain ratio reaching 111.55%; on the test set, its BLEU reaches 40.45, with a performance gain ratio of 129.80%. Hence, this forecasting algorithm is notably superior to other algorithms, which can enhance the machine translation performance. Through experiments, the Chinese language translation algorithm constructed here improves translation performance while ensuring a high correct identification rate, providing experimental references for the later intelligent development of Chinese language translation in computational linguistics.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4221013671",
    "type": "article"
  },
  {
    "title": "Exploiting Morpheme and Cross-lingual Knowledge to Enhance Mongolian Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3511098",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Songming Zhang; Ying Zhang; Yufeng Chen; Wu Du; Jinan Xu; Jian Liu",
    "corresponding_authors": "",
    "abstract": "Mongolian named entity recognition (NER) is not only one of the most crucial and fundamental tasks in Mongolian natural language processing, but also an important step to improve the performance of downstream tasks such as information retrieval, machine translation, and dialog system. However, traditional Mongolian NER models heavily rely on the feature engineering. Even worse, the complex morphological structure of Mongolian words makes the data sparser. To alleviate the feature engineering and data sparsity in Mongolian named entity recognition, we propose a novel NER framework with Multi-Knowledge Enhancement (MKE-NER) . Specifically, we introduce both linguistic knowledge through Mongolian morpheme representation and cross-lingual knowledge from Mongolian-Chinese parallel corpus. Furthermore, we design two methods to exploit cross-lingual knowledge sufficiently, i.e., cross-lingual representation and cross-lingual annotation projection. Experimental results demonstrate the effectiveness of our MKE-NER model, which outperforms strong baselines and achieves the best performance (94.04% F1 score) on the traditional Mongolian benchmark. Particularly, extensive experiments with different data scales highlight the superiority of our method in low-resource scenarios.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4225141588",
    "type": "article"
  },
  {
    "title": "A Task-oriented Chatbot Based on LSTM and Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3529649",
    "publication_date": "2022-04-11",
    "publication_year": 2022,
    "authors": "Yu-Ling Hsueh; Tai-Liang Chou",
    "corresponding_authors": "",
    "abstract": "Thanks to the advancements in deep learning, chatbots are widely used in messaging applications. Undoubtedly, a chatbot is a new way of interaction between humans and machines. However, most of the chatbots act as a simple question answering system that responds with formulated answers. Traditional conversational chatbots usually adopt a retrieval-based model that requires a large amount of conversational data for retrieving various intents. Hence, training a chatbot model that uses low-resource conversational data to generate more diverse dialogues is desirable. We propose a method to build a task-oriented chatbot using a sentence generation model that generates sequences based on the generative adversarial network. The architecture of our model contains a generator that generates a diverse sentence and a discriminator that judges the sentences by comparing the generated and the ground-truth sentences. In the generator, we combine the attention model with the sequence-to-sequence model using hierarchical long short-term memory to extract sentence information. For the discriminator, our reward mechanism assigns low rewards for repeated sentences and high rewards for diverse sentences. Extensive experiments are presented to demonstrate the utility of our model that generates more diverse and information-rich sentences than those of the existing approaches.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4226098626",
    "type": "article"
  },
  {
    "title": "Q-Learning for Shift-Reduce Parsing in Indonesian Tree-LSTM-Based Text Generation",
    "doi": "https://doi.org/10.1145/3490501",
    "publication_date": "2022-05-17",
    "publication_year": 2022,
    "authors": "Rochana Prih Hastuti; Yohanes Suyanto; Anny Kartika Sari",
    "corresponding_authors": "",
    "abstract": "Tree-LSTM algorithm accommodates tree structure processing to extract information outside the linear sequence pattern. The use of Tree-LSTM in text generation problems requires the help of an external parser at each generation iteration. Developing a good parser demands the representation of complex features and relies heavily on the grammar of the corpus. The limited corpus results in an insufficient number of vocabs for a grammar-based parser, making it less natural to link the text generation process. This research aims to solve the problem of limited corpus by proposing the use of a Reinforcement Learning algorithm in the formation of constituency trees, which link the sentence generation process given a seed phrase as the input in the Tree-LSTM model. The tree production process is modeled as a Markov’s decision process, where a set of states consists of word embedding vectors, and a set of actions of {Shift, Reduce}. The Deep Q-Network model as an approximator of the Q-Learning algorithm is trained to obtain optimal weights in representing the Q-value function. The test results on perplexity-based evaluation show that the proposed Tree-LSTM and Q-Learning combination model achieves values 9.60 and 4.60 for two kinds of corpus with 205 and 1,000 sentences, respectively, better than the Shift-All model. Human evaluation of Friedman test and posthoc analysis showed that all five respondents tended to give the same assessment for the combination model of Tree-LSTM and Q-Learning, which on average outperforms two other nongrammar models, i.e., Shift-All and Reduce-All.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4280518283",
    "type": "article"
  },
  {
    "title": "Lyrics Analysis of the Arab Singer Abdel ElHalim Hafez",
    "doi": "https://doi.org/10.1145/3544100",
    "publication_date": "2022-06-15",
    "publication_year": 2022,
    "authors": "Walid Gomaa",
    "corresponding_authors": "Walid Gomaa",
    "abstract": "In this work we analyze the lyrics of one of the most famous and influential Arab artists in the twentieth century, namely, (Abdel ElHalim Hafez). Lyrics analysis provides a deep insight into the artist’s career evolution and his interactions with the surrounding environment including the social, political, and economic conditions. In order to perform such analysis we had to collect and compile the lyrics of Abdel ElHalim accompanied with the necessary metadata into an organized and structured form. The data are preprocessed by removing stop words and doing some normalization operations over the songs’ prose. We did not perform any lemmatization or stemming as the original form of the tokens convey much more information than the source words. We performed a lexical analysis in order to study both the lexical density and diversity over the course of Abdel ElHalim’s career life. We have as well studied the most significant words, idioms, and terms played in the songs using tools such as word clouds and more quantitative measures such as term frequency-inverse document frequency. We have divided the career life of Abdel ElHalim into sub-decades of length 5 years and all analyses are done both in a yearly fashion and more coarsely over such sub-decades. We have found a strong correlation between our statistical analysis and the socio-political status in Egypt and the Arab world during that time. This is especially relevant knowing that Abdel ElHalim is very much truly considered the son of the generation of the 1952 revolution in Egypt. The significance of Abdel ElHalim and his lyrics stem essentially from being contemporaneous to radical changes in Egypt across all sectors including political (support of liberation movements across the world, and the conflict with Israel), and socio-economic (especially changing the social class structure in Egypt). We also investigated the potential effectiveness of PoS (Part of Speech) tagging in genre analysis and classification.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4282928492",
    "type": "article"
  },
  {
    "title": "Turkish Data-to-Text Generation Using Sequence-to-Sequence Neural Networks",
    "doi": "https://doi.org/10.1145/3543826",
    "publication_date": "2022-07-08",
    "publication_year": 2022,
    "authors": "Şeniz Demir",
    "corresponding_authors": "Şeniz Demir",
    "abstract": "End-to-end data-driven approaches lead to rapid development of language generation and dialogue systems. Despite the need for large amounts of well-organized data, these approaches jointly learn multiple components of the traditional generation pipeline without requiring costly human intervention. End-to-end approaches also enable the use of loosely aligned parallel datasets in system development by relaxing the degree of semantic correspondences between training data representations and text spans. However, their potential in Turkish language generation has not yet been fully exploited. In this work, we apply sequence-to-sequence (Seq2Seq) neural models to Turkish data-to-text generation where the input data given in the form of a meaning representation is verbalized. We explore encoder-decoder architectures with attention mechanism in unidirectional, bidirectional, and stacked recurrent neural network (RNN) models. Our models generate one-sentence biographies and dining venue descriptions using a crowdsourced dataset where all field value pairs that appear in meaning representations are fully captured in reference sentences. To support this work, we also explore the performances of our models on a more challenging dataset, where the content of a meaning representation is too large to fit into a single sentence, and hence content selection and surface realization need to be learned jointly. This dataset is retrieved by coupling introductory sentences of person-related Turkish Wikipedia articles with their contained infobox tables. Our empirical experiments on both datasets demonstrate that Seq2Seq models are capable of generating coherent and fluent biographies and venue descriptions from field value pairs. We argue that the wealth of knowledge residing in our datasets and the insights obtained from this study hold the potential to give rise to the development of new end-to-end generation approaches for Turkish and other morphologically rich languages.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4284959267",
    "type": "article"
  },
  {
    "title": "Research on Chinese Audio and Text Alignment Algorithm Based on AIC-FCM and Doc2Vec",
    "doi": "https://doi.org/10.1145/3532852",
    "publication_date": "2022-07-18",
    "publication_year": 2022,
    "authors": "Keliang Chen; Jianming Huang; Yansong Cui; Weizheng Ren",
    "corresponding_authors": "",
    "abstract": "‘‘Audiobook” is a multimedia-based reading technology that has emerged in recent years. Realizing the alignment of e-book text and book audio is the most important part of its processing. This article describes an audio and text alignment algorithm using deep learning and neural network technology to improve the efficiency and quality of audiobook production. The algorithm first uses dual-threshold endpoint detection technology to segment long audio into short audio with sentence dimensions and recognizes it as short text. The threshold is calculated by AIC-FCM optimized based on simulated annealing genetic algorithm. Then the algorithm uses Doc2vec optimized by the threshold prediction method based on the average length of the short text to calculate the text similarity. Finally, proofread and output the text sequence and audio segment aligned in the time dimension to meet the needs of audiobook production. Experiments show that compared to traditional audio and text alignment algorithms, the proposed algorithm is closer to the ideal segmentation result in long audio segmentation, and the alignment effect is basically the same as Doc2vec and the time complexity is reduced by about 35%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4285726869",
    "type": "article"
  },
  {
    "title": "Transfer Learning for Low-Resource Multilingual Relation Classification",
    "doi": "https://doi.org/10.1145/3554734",
    "publication_date": "2022-08-08",
    "publication_year": 2022,
    "authors": "Arijit Nag; Bidisha Samanta; Animesh Mukherjee; Niloy Ganguly; Soumen Chakrabarti",
    "corresponding_authors": "",
    "abstract": "Relation classification (sometimes called relation extraction ) requires trustworthy datasets for fine-tuning large language models, as well as for evaluation. Data collection is challenging for Indian languages, because they are syntactically and morphologically diverse, as well as different from resource-rich languages like English. Despite recent interest in deep generative models for Indian languages, relation classification is still not well served by public datasets. In response, we present IndoRE , a dataset with 21K entity- and relation-tagged gold sentences in three Indian languages (Bengali, Hindi, and Telugu), plus English. We start with a multilingual BERT (mBERT)-based system that captures entity span positions and type information, and provides competitive performance on monolingual relation classification. Using this baseline system, we explore transfer mechanisms between languages and the scope to reduce expensive data annotation while achieving reasonable relation extraction performance. Specifically, we (a) study the accuracy-efficiency trade-off between expensive, manually labeled gold instances vs. automatically translated and aligned silver instances to train a relation extractor, (b) device a simple mechanism for budgeted gold data annotation by intelligently converting distant-supervised silver training instances to gold training instances with human annotators using active learning, and finally (c) propose an ensemble model to provide a performance boost over that achieved via limited gold training instances. We release the dataset for future research. 1",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4290648542",
    "type": "article"
  },
  {
    "title": "Script Event Prediction via Multilingual Event Graph Networks",
    "doi": "https://doi.org/10.1145/3557893",
    "publication_date": "2022-08-18",
    "publication_year": 2022,
    "authors": "Bo Zhou; Yubo Chen; Kang Liu; Jun Zhao",
    "corresponding_authors": "",
    "abstract": "Predicting what happens next in text plays a critical role in building NLP applications. Many methods including count-based and neural-network-based have been proposed to tackle the task called script event prediction: predicting the most suitable subsequent event from a candidate list given a chain of narrative events (context). However, two problems including event ambiguity and evidence bias hinder the performance of these monolingual approaches. The former means that some events in the event chain are ambiguous. The latter means that both the wrong and correct candidate events can obtain sufficient support from the event context. In this article, we propose a novel multilingual approach to address two issues simultaneously. Specifically, to alleviate the event ambiguity problem, we project the monolingual event chains to parallel cross-lingual event chains, which can provide complementary information for monolingual event disambiguation. To deal with the evidence bias problem, we construct two monolingual event graphs and a cross-lingual event aligned graph to fully explore connections between events. What’s more, we design a graph attention mechanism to model the confidence of the complement clues, which controls the information integration from various languages. By modeling the events with graphs instead of pairs or chains, the model can compare the candidate subsequent events simultaneously and choose the more suitable subsequent event as the final answer. Extensive experiments were conducted on the widely used New York Times corpus for script event prediction task and experimental results show that our approach outperforms previous models.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4292492189",
    "type": "article"
  },
  {
    "title": "Annotation Projection-based Dependency Parser Development for Nepali",
    "doi": "https://doi.org/10.1145/3542696",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Pooja Rai; Sanjay Chatterji",
    "corresponding_authors": "",
    "abstract": "Building computational resources and tools for the under-resourced languages is strenuous for any Natural Language Processing task. This article presents the first dependency parser for an under-resourced Indian language, Nepali. A prerequisite for developing a parser for a language is a corpus annotated with the desired linguistic representations known as a treebank. With an aim of cross-lingual learning and typological research, we use a Bengali treebank to build a Bengali-Nepali parallel corpus and apply the method of annotation projection from the Bengali treebank to build a treebank for Nepali. With the developed treebank, MaltParser (with all algorithms for projective dependency structures) and a Neural network-based parser have been used to build Nepali parser models. The Neural network-based parser produced state-of-the-art results with 81.2 Unlabeled Attachment Score, 73.2 Label Accuracy, and 66.1 Labeled Attachment Score on the gold test data. The parser models have also been evaluated with the predicted Part-of-speech (POS)-tagged test data. A statistical POS tagger using Conditional Random Field has been developed for predicting the POS tags of the test data.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4293083863",
    "type": "article"
  },
  {
    "title": "Reusable Component Retrieval: A Semantic Search Approach for Low-Resource Languages",
    "doi": "https://doi.org/10.1145/3564604",
    "publication_date": "2022-09-22",
    "publication_year": 2022,
    "authors": "Nazia Bibi; Tauseef Rana; Ayesha Maqbool; Tamim Alkhalifah; Wazir Zada Khan; Ali Kashif Bashir; Yousaf Bin Zikria",
    "corresponding_authors": "",
    "abstract": "A common practice among programmers is to reuse existing code, accomplished by performing natural language queries through search engines. The main aim of code retrieval is to search for the most relevant snippet from a corpus of code snippets. However, code retrieval frameworks for low-resource languages are insufficient. Retrieving the most relevant code snippet efficiently can be accomplished only by eliminating the semantic gap between the code snippets residing in the repository and the user’s query (natural language description). The primary objective of the research is to contribute to this field by providing a code search framework that can be extended for low-resource languages. The secondary objective is to provide a code retrieval mechanism that is semantically relevant to the user query and provide programmers with the ability to locate source code that they want to use when developing new applications. The proposed approach is implemented using a web platform to search for source code. As code retrieval is a sophisticated task, the proposed approach incorporates a semantic search mechanism. This research uses a semantic model for code retrieval, which generates meanings or synonyms of words. The proposed model integrates ontologies and Natural Language Processing. System performance measures and classification accuracy are computed using precision, recall, and F1-score. We also compare the proposed approach with state-of-the-art baseline models. The retrieved results are ranked, showing that our approach significantly outperforms robust code matching. Our evaluation shows that semantic matching leads to improved source code retrieval. This study marks a substantial advancement in integrating programming expertise with code retrieval techniques. Moreover, our system lets users know when and how it is used for successful semantic searching.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4296614492",
    "type": "article"
  },
  {
    "title": "Future Challenges of Accounting Education in China Using Artificial Intelligence Assisted Multimedia Based Smart Accounting System",
    "doi": "https://doi.org/10.1145/3517914",
    "publication_date": "2022-10-27",
    "publication_year": 2022,
    "authors": "Aiyun Zhang; Ying Zhao",
    "corresponding_authors": "",
    "abstract": "The traditional teaching of financial accounting is complicated due to old-age methods of teaching, simplistic methods of education and insufficient participation of students in teaching in China. Those issues affect the effect of teaching. This investigation aims to increase teaching involving students, enhance the impact of learning, and encourage the quality training of financial accounting talents through the Artificial intelligence (AI) assisted Multimedia Supported Two-Path Interactive Teaching (AI-MTWIT) in china. Further, the Modeling of financial accounting courses that include Smart Classroom. Here, the AI-assisted multimedia system for assessing the quality of teaching of universities and colleges has been created to improve the teaching standards of financial accounting in classrooms and recognize the effectiveness of teaching on a timely basis. Centred on interactive theorem, a set of measurement indicators relevant to a multimedia college education level is discussed in this paper. Besides, Digital classes, teaching, and digital lessons, basic teacher information, fundamental student information and performance relations management are examined. Research findings showed that the assessment of the teaching standard had been performed within the context of the teaching assessment tools. Therefore, the program is essential to improve the teaching quality of the classroom.Feedback and reviews will be for students who get at least 80% of their questions answered. Less than 30% of the time, students need to be reminded of the importance of key facts and information. Group debates or classroom discussions may occur if accuracy rates are moderate.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4307366363",
    "type": "article"
  },
  {
    "title": "Study on Machine Translation Teaching Model Based on Translation Parallel Corpus and Exploitation for Multimedia Asian Information Processing",
    "doi": "https://doi.org/10.1145/3523282",
    "publication_date": "2022-11-07",
    "publication_year": 2022,
    "authors": "Yan Gong",
    "corresponding_authors": "Yan Gong",
    "abstract": "Text in one language can be mechanically translated into another language using machine translation (MT). It is possible to anticipate a sequence of words, generally modeling full sentences using machine translation in a single integrated model. Human language's flexibility makes automatic translation an artificial intelligence (AI) challenge of the highest order. A single model rather than a pipeline of fine-tuned models is now the best way to attain state-of-the-art outcomes in machine translation. For example, words having numerous meanings, phrases that use more than one grammatical structure, and other grammar issues make it difficult for a machine to translate; however, many misinterpretations translate to be a breeze. A teacher's job is to assist pupils in overcoming the emotional and cognitive obstacles that stand in the way of developing effective problem-solving abilities. Students will benefit from developing problem-solving abilities since they will apply what they have learned to new circumstances. MT-AI, machine translation technology, and products have been employed in a wide range of applications, including business travel, tourism, and cross-lingual information retrieval. Text translation and phonetic translation are two types of translations that focus on the content of the source language. It is possible to create self-learning systems by injecting machine learning techniques into existing software and then observing the results of such injection. Computer software can translate a massive volume of text in a short period. It takes longer for a human translator to perform the same work as a computer program. The simulation investigation is developed based on correctness and effectiveness, demonstrating the proposed framework's reliability of 95.1%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4308391738",
    "type": "article"
  },
  {
    "title": "Channel Attention TextCNN with Feature Word Extraction for Chinese Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3571716",
    "publication_date": "2022-11-17",
    "publication_year": 2022,
    "authors": "Jiangwei Liu; Zian Yan; Si-Bao Chen; Xiao Sun; Bin Luo",
    "corresponding_authors": "",
    "abstract": "Chinese short text sentiment analysis can help understand society’s views on various hot topics. Many existing sentiment analysis methods are based on sentiment dictionaries. Still, sentiment dictionaries are easily affected by subjective factors. They require a lot of time to build as well as maintenance to prevent obsolescence. For the aim of extracting rich information within texts more effectively, we propose a Channel Attention TextCNN with Feature Word Extraction model (CAT-FWE). The feature word extraction module helps us choose words that affect the sentiment of reviews. Then, these words are integrated with multi-level semantic information to enhance the information of sentences. In addition, the channel attention textCNN module that is a promotion of traditional TextCNN tends to pay more attention to those meaningful features. It eliminates the impacts of features that do not make any sense effectively. We apply our CAT-FWE model to both fine-grained classification and binary classification tasks for Chinese short texts. Experiment results show that it can improve the performance of emotion recognition.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4309617545",
    "type": "article"
  },
  {
    "title": "Document-Level Relation Extraction with Path Reasoning",
    "doi": "https://doi.org/10.1145/3572898",
    "publication_date": "2022-11-30",
    "publication_year": 2022,
    "authors": "Xu Wang; Kehai Chen; Tiejun Zhao",
    "corresponding_authors": "",
    "abstract": "Document-level relation extraction (DocRE) aims to extract relations among entities across multiple sentences within a document by using reasoning skills (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the reasoning paths between two entities. However, most of the advanced DocRE models only attend to the feature representations of two entities to determine their relation, and do not consider one complete reasoning path from one entity to another entity, which may hinder the accuracy of relation extraction. To address this issue, this article proposes a novel method to capture this reasoning path from one entity to another entity, thereby better simulating reasoning skills to classify relation between two entities. Furthermore, we introduce an additional attention layer to summarize multiple reasoning paths for further enhancing the performance of the DocRE model. Experimental results on a large-scale document-level dataset show that the proposed approach achieved a significant performance improvement on a strong heterogeneous graph-based baseline.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4311011759",
    "type": "article"
  },
  {
    "title": "Domain-Invariant Feature Progressive Distillation with Adversarial Adaptive Augmentation for Low-Resource Cross-Domain NER",
    "doi": "https://doi.org/10.1145/3570502",
    "publication_date": "2022-12-14",
    "publication_year": 2022,
    "authors": "Tao Zhang; Congying Xia; Zhiwei Liu; Shu Zhao; Hao Peng; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Considering the expensive annotation in Named Entity Recognition (NER ), Cross-domain NER enables NER in low-resource target domains with few or without labeled data, by transferring the knowledge of high-resource domains. However, the discrepancy between different domains causes the domain shift problem and hampers the performance of cross-domain NER in low-resource scenarios. In this article, we first propose an adversarial adaptive augmentation, where we integrate the adversarial strategy into a multi-task learner to augment and qualify domain adaptive data. We extract domain-invariant features of the adaptive data to bridge the cross-domain gap and alleviate the label-sparsity problem simultaneously. Therefore, another important component in this article is the progressive domain-invariant feature distillation framework. A multi-grained MMD (Maximum Mean Discrepancy) approach in the framework to extract the multi-level domain invariant features and enable knowledge transfer across domains through the adversarial adaptive data. Advanced Knowledge Distillation (KD) schema processes progressively domain adaptation through the powerful pre-trained language models and multi-level domain invariant features. Extensive comparative experiments over four English and two Chinese benchmarks show the importance of adversarial augmentation and effective adaptation from high-resource domains to low-resource target domains. Comparison with two vanilla and four latest baselines indicates the state-of-the-art performance and superiority confronted with both zero-resource and minimal-resource scenarios.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4311491585",
    "type": "article"
  },
  {
    "title": "Multi-granularity Knowledge Sharing in Low-resource Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3639930",
    "publication_date": "2024-01-09",
    "publication_year": 2024,
    "authors": "Chenggang Mi; Shaoliang Xie; Yi Fan",
    "corresponding_authors": "",
    "abstract": "As the rapid development of deep learning methods, neural machine translation (NMT) has attracted more and more attention in recent years. However, lack of bilingual resources decreases the performance of the low-resource NMT model seriously. To overcome this problem, several studies put their efforts on knowledge transfer from high-resource language pairs to low-resource language pairs. However, these methods usually focus on one single granularity of language and the parameter sharing among different granularities in NMT is not well studied. In this article, we propose to improve the parameter sharing in low-resource NMT by introducing multi-granularity knowledge such as word, phrase and sentence. This knowledge can be monolingual and bilingual. We build the knowledge sharing model for low-resource NMT based on a multi-task learning framework, three auxiliary tasks such as syntax parsing, cross-lingual named entity recognition, and natural language generation are selected for the low-resource NMT. Experimental results show that the proposed method consistently outperforms six strong baseline systems on several low-resource language pairs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390725317",
    "type": "article"
  },
  {
    "title": "NLP-enabled Recommendation of Hashtags for Covid based Tweets using Hybrid BERT-LSTM Model",
    "doi": "https://doi.org/10.1145/3640812",
    "publication_date": "2024-01-16",
    "publication_year": 2024,
    "authors": "Kirti Jain; Rajni Jindal",
    "corresponding_authors": "",
    "abstract": "Hashtags have become a new trend to summarize the feelings, sentiments, emotions, swinging moods, food tastes and much more. It also represents various entities like places, families and friends. It is a way to search and categorize various stuff on social media sites. With the increase in the hashtagging, there is a need to automate it, leading to the term “Hashtag Recommendation”. Also, there are plenty of posts on social media sites which remain untagged. These untagged posts get filtered out while searching and categorizing the data using a label. Such posts do not make any contribution to any helpful insight and remain abandoned. But, if the user of such posts is recommended by labels according to his post, then he may choose one or more of them, thus making the posts labelled. For such cases Hashtag recommendation comes into the picture. Although much research work has been done on Hashtag Recommendation using traditional Deep Learning approaches, not much work has been done using NLP based Bert Embedding. In this paper, we have proposed a model, BELHASH, Bert Embedding based LSTM for Hashtag Recommendation. This task is considered as a Multilabel Classification task as the hashtags are one-hot encoded into multiple binary vectors of zeros and ones using MultiLabelBinarizer. This model has been evaluated on Covid 19 tweets. We have achieved 0.72 accuracy, 0.7 Precision, 0.66 Recall and 0.67 F1-Score. This is the first paper of hashtag recommendation to the best of our knowledge combining Bert embedding with LSTM model and achieving the state of the arts results.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390911990",
    "type": "article"
  },
  {
    "title": "PAMR: Persian Abstract Meaning Representation Corpus",
    "doi": "https://doi.org/10.1145/3638288",
    "publication_date": "2024-01-19",
    "publication_year": 2024,
    "authors": "Nasim Tohidi; Chitra Dadkhah; Reza Nouralizadeh Ganji; Ehsan Ghaffari Sadr; Hoda Elmi",
    "corresponding_authors": "",
    "abstract": "One of the most used and well-known semantic representation models is Abstract Meaning Representation (AMR). This representation has had numerous applications in natural language processing tasks in recent years. Currently, for English and Chinese languages, large annotated corpora are available. In addition, in some low-resource languages, related corpora have been generated with less size; although, until now, to the best of our knowledge, there is not any AMR corpus for the Persian/Farsi language. Therefore, the aim of this article is to create a Persian AMR (PAMR) corpus via translating English sentences and adjusting AMR guidelines and to solve the various challenges that are faced in this regard. The result of this research is a corpus, containing 1,020 Persian sentences and their related AMR that can be used in various natural language processing tasks. In this article, to investigate the feasibility of using the corpus, we have applied it to two natural language processing tasks: Sentiment Analysis and Text Summarization.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391026046",
    "type": "article"
  },
  {
    "title": "<b>Fast Recurrent Neural Network with Bi-LSTM for Handwritten Tamil Text Segmentation in NLP</b>",
    "doi": "https://doi.org/10.1145/3643808",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "C. Vinotheni; S. Lakshmana Pandian",
    "corresponding_authors": "",
    "abstract": "Tamil text segmentation is a long-standing test in language comprehension that entails separating a record into adjacent pieces based on its semantic design. Each segment is important in its own way. The segments are organised according to the purpose of the content examination as text groups, sentences, phrases, words, characters or any other data unit. That process has been portioned using rapid tangled neural organisation in this research, which presents content segmentation methods based on deep learning in natural language processing (NLP). This study proposes a bidirectional long short-term memory (Bi-LSTM) neural network prototype in which fast recurrent neural networks (FRNNs) are used to learn Tamil text group embedding and phrases are fragmented using text-oriented data. As a result, this prototype is capable of handling variable measured setting data and gives a vast new dataset for naturally segmenting text in Tamil. In addition, we develop a segmentation prototype and show how well it sums up to unnoticeable regular content using this dataset as a base. With Bi-LSTM, the segmentation precision of FRNN is superior to that of other segmentation approaches; however, it is still inferior to that of certain other techniques. Every content is scaled to the required size in the proposed framework, which is immediately accessible for the preparation. This means, each word in a scaled Tamil text is employed to prepare neural organisation as fragmented content. The results reveal that the proposed framework produces high rates of segmentation for manually authored material that are nearly equivalent to segmentation-based plans.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391614636",
    "type": "article"
  },
  {
    "title": "Boundary-Aware Abstractive Summarization with Entity-Augmented Attention for Enhancing Faithfulness",
    "doi": "https://doi.org/10.1145/3641278",
    "publication_date": "2024-02-13",
    "publication_year": 2024,
    "authors": "Jiuyi Li; Junpeng Liu; Jianjun Ma; Wei Yang; Degen Huang",
    "corresponding_authors": "",
    "abstract": "With the successful application of deep learning, document summarization systems can produce more readable results. However, abstractive summarization still suffers from unfaithful outputs and factual errors, especially in named entities. Current approaches tend to employ external knowledge to improve model performance while neglecting the boundary information and the semantics of the entities. In this article, we propose an entity-augmented method (EAM) to encourage the model to make full use of the entity boundary information and pay more attention to the critical entities. Experimental results on three Chinese and English summarization datasets show that our method outperforms several strong baselines and achieves state-of-the-art performance on the CLTS dataset. Our method can also improve the faithfulness of the summary and generalize well to different pre-trained language models. Moreover, we propose a method to evaluate the integrity of generated entities. Besides, we adapt the data augmentation method in the FactCC model according to the difference between Chinese and English in grammar and train a new evaluation model for factual consistency evaluation in Chinese summarization.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391777555",
    "type": "article"
  },
  {
    "title": "Handling Imbalance and Limited Data in Thyroid Ultrasound and Diabetic Retinopathy Datasets Using Discrete Levy Flights Grey Wolf Optimizer Based Random Forest for Robust Medical Data Classification",
    "doi": "https://doi.org/10.1145/3648363",
    "publication_date": "2024-02-16",
    "publication_year": 2024,
    "authors": "Shobha Aswal; Neelu Jyothi Ahuja; Ritika Mehra",
    "corresponding_authors": "",
    "abstract": "In the field of disease diagnosis, medical image classification faces an inherent challenge due to various factors involving data imbalance, image quality variability, annotation variability, and limited data availability and data representativeness. Such challenges affect the algorithm's classification ability on the medical images in an adverse way, which leads to biased model outcomes and inaccurate interpretations. In this paper, a novel Discrete Levy Flight Grey Wolf Optimizer (DLFGWO) is combined with the Random Forest (RF) classifier to address the above limitations on the biomedical datasets and to achieve better classification rate. The DLFGWO-RF resolves the image quality variability in ultrasound images and limits the inaccuracies on classification using RF by handling the incomplete and noisy data. The sheer focus on the majority class may lead to unequal distribution of classes and thus leads to data imbalance. The DLFGWO balances such distribution by leveraging grey wolves and its exploration and exploitation capabilities are improved using Discrete Levy Flight (DLF). It further optimizes the classifier's performance to achieve balanced classification rate. DLFGWO-RF is designed to perform classification even on limited datasets, thereby the requirement of numerous expert annotations can thus be reduced. In diabetic retinopathy grading, the DLFGWO-RF reduces disagreements in annotation variability using subjective interpretations. However, the representativeness of the diabetic retinopathy dataset fails to capture the entire population diversity, which limits the generalization ability of the proposed DLFGWO-RF. Thus, fine-tuning of RF can robustly adapt to the subgroups in the dataset, enhancing its overall performance. The experiments are conducted on two widely used medical image datasets to test the efficacy of the model. The experimental results show that the DLFGWO-RF classifier achieves improved classification accuracy between 90-95%, which outperforms the existing techniques for various imbalanced datasets.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391883774",
    "type": "article"
  },
  {
    "title": "Consensus-Based Machine Translation for Code-Mixed Texts",
    "doi": "https://doi.org/10.1145/3628427",
    "publication_date": "2024-03-09",
    "publication_year": 2024,
    "authors": "Sainik Kumar Mahata; Dipankar Das; Sivaji Bandyopadhyay",
    "corresponding_authors": "",
    "abstract": "Multilingualism in India is widespread due to its long history of foreign acquaintances. This leads to the presence of an audience familiar with conversing using more than one language. Additionally, due to the social media boom, the usage of multiple languages to communicate has become extensive. Hence, the need for a translation system that can serve the novice and monolingual user is the need of the hour. Such translation systems can be developed by methods such as statistical machine translation and neural machine translation, where each approach has its advantages as well as disadvantages. In addition, the parallel corpus needed to build a translation system, with code-mixed data, is not readily available. In the present work, we present two translation frameworks that can leverage the individual advantages of these pre-existing approaches by building an ensemble model that takes a consensus of the final outputs of the preceding approaches and generates the target output. The developed models were used for translating English-Bengali code-mixed data (written in Roman script) into their equivalent monolingual Bengali instances. A code-mixed to monolingual parallel corpus was also developed to train the preceding systems. Empirical results show improved BLEU and TER scores of 17.23 and 53.18 and 19.12 and 51.29, respectively, for the developed frameworks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392614256",
    "type": "article"
  },
  {
    "title": "Multization: Multi-Modal Summarization Enhanced by Multi-Contextually Relevant and Irrelevant Attention Alignment",
    "doi": "https://doi.org/10.1145/3651983",
    "publication_date": "2024-03-09",
    "publication_year": 2024,
    "authors": "Huan Rong; Zhongfeng Chen; Zhenyu Lu; Fan Xu; Victor S. Sheng",
    "corresponding_authors": "",
    "abstract": "This article focuses on the task of Multi-Modal Summarization with Multi-Modal Output for China JD.COM e-commerce product description containing both source text and source images. In the context learning of multi-modal (text and image) input, there exists a semantic gap between text and image, especially in the cross-modal semantics of text and image. As a result, capturing shared cross-modal semantics earlier becomes crucial for multi-modal summarization. However, when generating the multi-modal summarization, based on the different contributions of input text and images, the relevance and irrelevance of multi-modal contexts to the target summary should be considered, so as to optimize the process of learning cross-modal context to guide the summary generation process and to emphasize the significant semantics within each modality. To address the aforementioned challenges, Multization has been proposed to enhance multi-modal semantic information by multi-contextually relevant and irrelevant attention alignment. Specifically, a Semantic Alignment Enhancement mechanism is employed to capture shared semantics between different modalities (text and image), so as to enhance the importance of crucial multi-modal information in the encoding stage. Additionally, the IR-Relevant Multi-Context Learning mechanism is utilized to observe the summary generation process from both relevant and irrelevant perspectives, so as to form a multi-modal context that incorporates both text and image semantic information. The experimental results in the China JD.COM e-commerce dataset demonstrate that the proposed Multization method effectively captures the shared semantics between the input source text and source images, and highlights essential semantics. It also successfully generates the multi-modal summary (including image and text) that comprehensively considers the semantics information of both text and image.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392619741",
    "type": "article"
  },
  {
    "title": "Supervised Contrast Learning Text Classification Model Based on Data Quality Augmentation",
    "doi": "https://doi.org/10.1145/3653300",
    "publication_date": "2024-03-19",
    "publication_year": 2024,
    "authors": "Liang Wu; Fangfang Zhang; Chao Cheng; Shinan Song",
    "corresponding_authors": "",
    "abstract": "Token-level data augmentation generates text samples by modifying the words of the sentences. However, data that are not easily classified can negatively affect the model. In particular, not considering the role of keywords when performing random augmentation operations on samples may lead to the generation of low-quality supplementary samples. Therefore, we propose a supervised contrast learning text classification model based on data quality augmentation. First, dynamic training is used to screen high-quality datasets containing beneficial information for model training. The selected data is then augmented with data based on important words with tag information. To obtain a better text representation to serve the downstream classification task, we employ a standard supervised contrast loss to train the model. Finally, we conduct experiments on five text classification datasets to validate the effectiveness of our model. In addition, ablation experiments are conducted to verify the impact of each module on classification.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392956390",
    "type": "article"
  },
  {
    "title": "Syntax-aware Offensive Content Detection in Low-resourced Code-mixed Languages with Continual Pre-training",
    "doi": "https://doi.org/10.1145/3653450",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Necva Bölücü; Pelin Canbay",
    "corresponding_authors": "",
    "abstract": "Social media is a widely used platform that includes a vast amount of user-generated content, allowing the extraction of information about users’ thoughts from texts. Individuals freely express their thoughts on these platforms, often without constraints, even if the content is offensive or contains hate speech. The identification and removal of offensive content from social media are imperative to prevent individuals or groups from becoming targets of harmful language. Despite extensive research on offensive content detection, addressing this challenge in code-mixed languages remains unsolved, characterised by issues such as imbalanced datasets and limited data sources. Most previous studies on detecting offensive content in these languages focus on creating datasets and applying deep neural networks, such as Recurrent Neural Networks (RNNs), or pre-trained language models (PLMs) such as BERT and its variations. Given the low-resource nature and imbalanced dataset issues inherent in these languages, this study delves into the efficacy of the syntax-aware BERT model with continual pre-training for the accurate identification of offensive content and proposes a framework called Cont-Syntax-BERT by combining continual learning with continual pre-training. Comprehensive experimental results demonstrate that the proposed Cont-Syntax-BERT framework outperforms state-of-the-art approaches. Notably, this framework addresses the challenges posed by code-mixed languages, as evidenced by its proficiency on the DravidianCodeMix [10,19] and HASOC 2109 [37] datasets. These results demonstrate the adaptability of the proposed framework in effectively addressing the challenges of code-mixed languages.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393202140",
    "type": "article"
  },
  {
    "title": "Application of Hybrid Image Processing Based on Artificial Intelligence in Interactive English Teaching",
    "doi": "https://doi.org/10.1145/3626822",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Xin Dou; Cuiping Shi",
    "corresponding_authors": "",
    "abstract": "Primary school English teaching resources play an important role in primary school English teaching. The information age requires that primary school English teaching should strengthen the use of multimedia resources and gradually realize the diversification of teaching content. Expanded reality innovation is a sort of mixture picture handling innovation, which is one of the significant innovations that would influence the improvement of fundamental schooling in the following five years. It can seamlessly output virtual objects to the real environment, which is convenient for this paper to obtain and absorb information. It can also help students to participate in exploration and cultivate their creativity and imagination. It can strengthen the cooperation between students and teachers and create various learning environments. It has an immeasurable prospect of development in the field of education. The primary school English teaching resources based on augmented reality create a realistic learning situation from two-dimensional plane to three-dimensional three-dimensional display, and enrich the presentation of primary school English teaching content. It can stimulate students’ interest in learning English and promote the transformation of English teaching methods. It is a useful attempt in the field of education. This paper made statistics on the test results of the experimental class and the control class. Most of the scores of the experimental group were between 71 and 100, a total of 27, accounting for 67.5%. The score distribution of the control class was relatively balanced, with the highest number between 61-70, and the number was 10, accounting for 25%. Therefore, it can be seen that hybrid image processing technology is important for interactive English teaching.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393278044",
    "type": "article"
  },
  {
    "title": "Part-of-speech Tagging for Low-resource Languages: Activation Function for Deep Learning Network to Work with Minimal Training Data",
    "doi": "https://doi.org/10.1145/3655023",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Diganta Baishya; Rupam Baruah",
    "corresponding_authors": "",
    "abstract": "Numerous natural language processing (NLP) applications exist today, especially for the most commonly spoken languages such as English, Chinese, and Spanish. Popular traditional methods such as Rule based methods, Naive Bayes classifiers, Hidden Markov models, Conditional Random field-based classifiers, and other stochastic methods have contributed to this improvement in the past. Recently, deep learning has led to exciting breakthroughs in several areas of artificial intelligence, including image processing and natural language processing. It is important to label words as parts of speech to begin developing most of the NLP applications. A deep study in this area reveals that many popular approaches used for this purpose require massive training data. Therefore, these approaches have not been helpful for languages not rich in digital resources. Applying these methods with very little training data prompts the need for innovative problem-solving. This article describes our research, which examines the strengths and weaknesses of well-known approaches, such as conditional random fields and state-of-the-art deep learning models, when applied for part-of-speech tagging using minimal training data for Assamese and English. We also examine the factors affecting them. We discuss our deep learning architecture and the proposed activation function, which shows promise with little training data. The activation function categorizes words belonging to different classes with more confidence by using the outcomes of statistical methods with SMTaylor SoftMax in our deep learning model. With minimal training, our deep learning architecture using the proposed modification of SM-Taylor SoftMax improves accuracy upto 4%, for our small dataset. This technique is a combination of SMTaylor SoftMax and statistical probability distribution of words over tags.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393342141",
    "type": "article"
  },
  {
    "title": "A Novel Pretrained General-purpose Vision Language Model for the Vietnamese Language",
    "doi": "https://doi.org/10.1145/3654796",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Đinh Nhật Vũ; Quang Nhat Minh Pham; Giang Son Tran",
    "corresponding_authors": "",
    "abstract": "Lying in the cross-section of computer vision and natural language processing, vision language models are capable of processing images and text at once. These models are helpful in various tasks: text generation from image and vice versa, image-text retrieval, or visual navigation. Besides building a model trained on a dataset for a task, people also study general-purpose models to utilize many datasets for multitasks. Their two primary applications are image captioning and visual question answering. For English, large datasets and foundation models are already abundant. However, for Vietnamese, they are still limited. To expand the language range, this work proposes a pretrained general-purpose image-text model named VisualRoBERTa. A dataset of 600k images with captions (translated MS COCO 2017 from English to Vietnamese) is introduced to pretrain VisualRoBERTa. The model’s architecture is built using Convolutional Neural Network and Transformer blocks. Fine-tuning VisualRoBERTa shows promising results on the ViVQA dataset with 34.49% accuracy, 0.4173 BLEU 4, and 0.4390 RougeL (in visual question answering task), and best outcomes on the sViIC dataset with 0.6685 BLEU 4, 0.6320 RougeL (in image captioning task).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393342149",
    "type": "article"
  },
  {
    "title": "Student's Emotion Recognition using Multimodality and Deep Learning",
    "doi": "https://doi.org/10.1145/3654797",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "M. Kalaiyarasi; B. V. V. Siva Prasad; Janjhyam Venkata Naga Ramesh; Ravindra Kumar Kushwaha; Ruchi Patel; J. Balajee",
    "corresponding_authors": "",
    "abstract": "The goal of emotion detection is to find and recognise emotions in text, speech, gestures, facial expressions, and more. This paper proposes an effective multimodal emotion recognition system based on facial expressions, sentence-level text, and voice. Using public datasets, we examine face expression image classification and feature extraction. The Tri-modal fusion is used to integrate the findings and to provide the final emotion. The proposed method has been verified in classroom students, and the feelings correlate with their performance. This method categorizes students' expressions into seven emotions: happy, surprise, sad, fear, disgust, anger, and contempt. Compared to the unimodal models, the suggested multimodal network design may reach up to 65% accuracy. The proposed method can detect negative feelings such as boredom or loss of interest in the learning environment.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393387606",
    "type": "article"
  },
  {
    "title": "An adaptive dual graph convolution fusion network for aspect-based sentiment analysis",
    "doi": "https://doi.org/10.1145/3659579",
    "publication_date": "2024-04-17",
    "publication_year": 2024,
    "authors": "Chunmei Wang; Yuan Luo; Chunli Meng; Feiniu Yuan",
    "corresponding_authors": "",
    "abstract": "Aspect-based Sentiment Analysis (ABSA), also known as fine-grained sentiment analysis, aims to predict the sentiment polarity of specific aspect words in the sentence. Some studies have explored the semantic correlation between words in sentences through attention-based methods. Other studies have learned syntactic knowledge by using graph convolution networks to introduce dependency relations. These methods have achieved satisfactory results in the ABSA tasks. However, due to the complexity of language, effectively capturing semantic and syntactic knowledge remains a challenging research question. Therefore, we propose an Adaptive Dual Graph Convolution Fusion Network (AD-GCFN) for aspect-based sentiment analysis. This model uses two graph convolution networks: one for the semantic layer to learn semantic correlations by an attention mechanism, and the other for the syntactic layer to learn syntactic structure by dependency parsing. To reduce the noise caused by the attention mechanism, we designed a module that dynamically updates the graph structure information for adaptively aggregating node information. To effectively fuse semantic and syntactic information, we propose a cross-fusion module that uses the double random similarity matrix to obtain the syntactic features in the semantic space and the semantic features in the syntactic space, respectively. Additionally, we employ two regularizers to further improve the ability to capture semantic correlations. The orthogonal regularizer encourages the semantic layer to learn word semantics without overlap, while the differential regularizer encourages the semantic and syntactic layers to learn different parts. Finally, the experimental results on three benchmark datasets show that the AD-GCFN model is superior to the contrast models in terms of accuracy and macro-F1.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394880565",
    "type": "article"
  },
  {
    "title": "HumourHindiNet: Humour detection in Hindi web series using word embedding and convolutional neural network",
    "doi": "https://doi.org/10.1145/3661306",
    "publication_date": "2024-04-27",
    "publication_year": 2024,
    "authors": "Akshi Kumar; Abhishek Mallik; Sanjay Kumar",
    "corresponding_authors": "",
    "abstract": "Humour is a crucial aspect of human speech, and it is, therefore, imperative to create a system that can offer such detection. While data regarding humour in English speech is plentiful, the same cannot be said for a low-resource language like Hindi. Through this article, we introduce two multimodal datasets for humour detection in the Hindi web series. The dataset was collected from over 500 minutes of conversations amongst the characters of the Hindi web series Kota-Factory and Panchayat . Each dialogue is manually annotated as Humour or Non-Humour. Along with presenting a new Hindi language-based Humour detection dataset, we propose an improved framework for detecting humour in Hindi conversations. We start by preprocessing both datasets to obtain uniformity across the dialogues and datasets. The processed dialogues are then passed through the Skip-gram model for generating Hindi word embedding. The generated Hindi word embedding is then passed onto three convolutional neural network (CNN) architectures simultaneously, each having a different filter size for feature extraction. The extracted features are then passed through stacked Long Short-Term Memory (LSTM) layers for further processing and finally classifying the dialogues as Humour or Non-Humour. We conduct intensive experiments on both proposed Hindi datasets and evaluate several standard performance metrics. The performance of our proposed framework was also compared with several baselines and contemporary algorithms for Humour detection. The results demonstrate the effectiveness of our dataset to be used as a standard dataset for Humour detection in the Hindi web series. The proposed model yields an accuracy of 91.79 and 87.32 while an F1 score of 91.64 and 87.04 in percentage for the Kota-Factory and Panchayat datasets, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4395691389",
    "type": "article"
  },
  {
    "title": "Integrated End-to-End Automatic Speech Recognition for Languages for Agglutinative Languages",
    "doi": "https://doi.org/10.1145/3663568",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "A. Bekarystankyzy; Оrken Mamyrbayev; Tolganay Anarbekova",
    "corresponding_authors": "",
    "abstract": "The relevance of the problem of automatic speech recognition lies in the lack of research for low-resource languages, stemming from limited training data and the necessity for new technologies to enhance efficiency and performance. The purpose of this work was to study the main aspects of integrated end-to-end speech recognition and the use of modern technologies in the natural processing of agglutinative languages, including Kazakh. In this article, the study of language models was carried out using comparative, graphic, statistical, and analytical-synthetic methods, which were used in combination. This article addresses automatic speech recognition (ASR) in agglutinative languages, particularly Kazakh, through a unified neural network model that integrates both acoustic and language modeling. Employing advanced techniques like connectionist temporal classification and attention mechanisms, the study focuses on effective speech-to-text transcription for languages with complex morphologies. Transfer learning from high-resource languages helps mitigate data scarcity in languages such as Kazakh, Kyrgyz, Uzbek, Turkish, and Azerbaijani. The research assesses model performance, underscores ASR challenges, and proposes advancements for these languages. It includes a comparative analysis of phonetic and word-formation features in agglutinative Turkic languages, using statistical data. The findings aid further research in linguistics and technology for enhancing speech recognition and synthesis, contributing to voice identification and automation processes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396624808",
    "type": "article"
  },
  {
    "title": "Quantitative Stylistic Analysis of Middle Chinese Texts Based on the Dissimilarity of Evolutive Core Word Usage",
    "doi": "https://doi.org/10.1145/3665794",
    "publication_date": "2024-05-28",
    "publication_year": 2024,
    "authors": "Bing Qiu; Jiahao Huo",
    "corresponding_authors": "",
    "abstract": "Stylistic analysis enables open-ended and exploratory observation of languages. To fill the gap in the quantitative analysis of the stylistic systems of Middle Chinese, we construct lexical features based on the evolutive core word usage and scheme a Bayesian method for feature parameters estimation. The lexical features are from the Swadesh list, each of which has different word forms along with the language evolution during the Middle Ages. We thus count the varied word of those entries along with the language evolution as the linguistic features. With the Bayesian formulation, the feature parameters are estimated to construct a high-dimensional random feature vector to obtain the pair-wise dissimilarity matrix of all the texts based on different distance measures. Finally, we perform the spectral embedding and clustering to visualize, categorize, and analyze the linguistic styles of Middle Chinese texts. The quantitative result agrees with the existing qualitative conclusions and, furthermore, betters our understanding of the linguistic styles of Middle Chinese from both the inter-category and intra-category aspects. It also helps unveil the special styles induced by the indirect language contact.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399084657",
    "type": "article"
  },
  {
    "title": "X-Phishing-Writer: A Framework for Cross-lingual Phishing E-mail Generation",
    "doi": "https://doi.org/10.1145/3670402",
    "publication_date": "2024-06-03",
    "publication_year": 2024,
    "authors": "Shih-Wei Guo; Yao-Chung Fan",
    "corresponding_authors": "",
    "abstract": "Cybercrime is projected to cause annual business losses of $10.5 trillion by 2025, a significant concern given that a majority of security breaches are due to human errors, especially through phishing attacks. The rapid increase in daily identified phishing sites over the past decade underscores the pressing need to enhance defenses against such attacks. Social Engineering Drills (SEDs) are essential in raising awareness about phishing yet face challenges in creating effective and diverse phishing e-mail content. These challenges are exacerbated by the limited availability of public datasets and concerns over using external language models like ChatGPT for phishing e-mail generation. To address these issues, this article introduces X-Phishing-Writer, a novel cross-lingual Few-shot phishing e-mail generation framework. X-Phishing-Writer allows for the generation of e-mails based on minimal user input, leverages single-language datasets for multilingual e-mail generation, and is designed for internal deployment using a lightweight, open-source language model. Incorporating Adapters into an Encoder–Decoder architecture, X-Phishing-Writer marks a significant advancement in the field, demonstrating superior performance in generating phishing e-mails across 25 languages when compared to baseline models. Experimental results and real-world drills involving 1,682 users showcase a 17.67% e-mail open rate and a 13.33% hyperlink click-through rate, affirming the framework’s effectiveness and practicality in enhancing phishing awareness and defense.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399300747",
    "type": "article"
  },
  {
    "title": "Study on Intelligent Scoring of English Composition Based on Machine Learning from the Perspective of Natural Language Processing",
    "doi": "https://doi.org/10.1145/3625545",
    "publication_date": "2024-06-04",
    "publication_year": 2024,
    "authors": "Jing Tang",
    "corresponding_authors": "Jing Tang",
    "abstract": "Knowledge management is crucial to the teaching and learning process in the current era of digitalization. The idea of \"learning via working together\" is making Natural Language Processing a popular tool to improve the learning process based on the intelligent system for evaluating the composition. English language learning is highly dependent on the composition written by the students under various topics. Teachers are facing huge difficulties in the evaluation of the composition as the level of writing by the students will vary for individual. In this research, Natural Language Processing concept is utilized for getting trained with the student's writing skills and Multiprocessor Learning Algorithm (MLA) combined with Convolutional Neural Network (CNN) (MLA-CNN) for evaluating the composition and declaring the scores for the students. The model's composition scoring rate is validated using a range of learning rate settings. Some theoretical notions for smart teaching are proposed, and it is hoped that this automatic composition scoring model would be used to grade student writing in English classes. When applied to the automatic scoring of students' English composition in schools, the suggested composition scoring system trained by the MLP-CNN has great performance and lays the groundwork for the educational applications of ML inside AI. The study results proved that the proposed model has provided an accuracy of 98%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399321564",
    "type": "article"
  },
  {
    "title": "Systematic Investigation of Recent Pre-trained Language Model for Hate Speech Detection in Arabic Tweets",
    "doi": "https://doi.org/10.1145/3674970",
    "publication_date": "2024-06-25",
    "publication_year": 2024,
    "authors": "Kheir Eddine Daouadi; Yaakoub Boualleg; Oussama Guehairia",
    "corresponding_authors": "",
    "abstract": "Today, hate speech classification from Arabic tweets has gained significant interest among global researchers. Different techniques and systems are harnessed to overcome this classification task. However, two main challenges are confronted, the use of handcrafted features and the fact that their performance rate is still limited. We address the hate speech identification from Arabic tweets while providing a deeper comprehension of the capability of a new technique based on transfer learning. Specifically, the accuracy result of traditional machine learning (ML) models is compared with Pre-trained Language Models (PLMs) as well as Deep Learning (DL) models. Experiments on a benchmark dataset show that (1) the multidialectal PLMs outperform monolingual and multilingual ones; (2) the fine-tuning of recent PLMs enhances the performance results of hate speech classification from Arabic tweets. The major contribution of this work lies in achieving promising accuracy results in the Arabic hate speech classification task.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400017374",
    "type": "article"
  },
  {
    "title": "Augmenting Low-Resource Cross-Lingual Summarization with Progression-Grounded Training and Prompting",
    "doi": "https://doi.org/10.1145/3675167",
    "publication_date": "2024-06-26",
    "publication_year": 2024,
    "authors": "Jiangeng Ma; Yuxin Huang; Linqin Wang; Xiang Huang; Hao Peng; Zhengtao Yu; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Cross-lingual summarization (CLS) , generating summaries in one language from source documents in another language, offers invaluable assistance in enabling global access to information for people worldwide. State-of-the-art neural summarization models typically train or fine-tune language models on large-scale corpora. However, this is difficult to achieve in realistic low-resource scenarios due to the lack of domain-specific annotated data. In this article, we present a novel cross-lingual summarization model that utilizes progressive training with mBART and employs reinforcement learning to optimize discrete prompts, which addresses low-resource cross-lingual summarization through a two-pronged approach. During training, we introduce a progressive approach based on mBART, which allows the pre-trained model to gradually acquire the ability to compress information, develop cross-lingual capabilities, and ultimately adapt to specific summarization tasks. During downstream summarization, we employ a discrete-prompts joint pre-trained model based on reinforcement learning optimization to achieve low-resource cross-lingual summarization. Experimental results on four cross-lingual summarization datasets demonstrate state-of-the-art performance and superiority compared to six baselines in low-resource scenarios.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400046700",
    "type": "article"
  },
  {
    "title": "TinyCheXReport: Compressed deep neural network for Chest X-ray report generation",
    "doi": "https://doi.org/10.1145/3676166",
    "publication_date": "2024-07-03",
    "publication_year": 2024,
    "authors": "Fahd S. Alotaibi; Khaled H. Alyoubi; Ajay Mittal; Vishal Gupta; Navdeep Kaur",
    "corresponding_authors": "",
    "abstract": "Increase in Chest X-ray (CXR) imaging tests has burdened radiologists, thereby posing significant challenges in writing radiological reports on time. Although several deep learning-based automatic report generation methods have been developed, most are over-parameterized. For deployment on edge devices with constrained processing power or limited resources, over-parameterized models are often too large. This article presents a compressed deep learning-based model that is 30% space efficient compared to the non-compressed base model, while both have comparable performance. The model comprising VGG19 and hierarchical long short-term memory equipped with a contextual word embedding layer is used as the base model. The redundant weight parameters are removed from the base model using unstructured one-shot pruning. To overcome the performance degradation, the lightweight pruned model is fine-tuned over publicly available OpenI dataset. The quantitative evaluation metric scores demonstrate that proposed model surpasses the performance of state-of-the-art models. Additionally, the proposed model, being 30% space efficient, is easily deployable in resource-limited settings. Thus, this study serves as baseline for development of compressed models to generate radiological reports from CXR images.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400291918",
    "type": "article"
  },
  {
    "title": "Vi-AbSQA: Multi-task Prompt Instruction Tuning Model for Vietnamese Aspect-based Sentiment Quadruple Analysis",
    "doi": "https://doi.org/10.1145/3676886",
    "publication_date": "2024-07-04",
    "publication_year": 2024,
    "authors": "Dang Van Thin; Duong Ngoc Hao; Ngan Luu-Thuy Nguyen",
    "corresponding_authors": "",
    "abstract": "Aspect-based sentiment analysis (ABSA) has recently received considerable attention within the Natural Language Processing (NLP) community, especially for complex tasks like triplet extraction or quadruplet prediction. However, most existing studies focus on high-resource languages. In this paper, we construct a challenging benchmark dataset for Vietnamese Aspect-based Sentiment Quadruple Analysis (AbSQA), where each sentence can contain explicit and implicit aspects and opinion terms. Moreover, each sample includes at least two aspect categories with different sentiments. We release this dataset for free research purposes, believing it will push forward research in this field. In addition, we present a generative-based approach to address the AbSQA task using a multitask instruction prompt tuning framework. Specifically, we design an effective generation paradigm that leverages instruction prompts to provide more information about the task. Besides, our model leverages relational information by designing separate sub-tasks based on the quadruplet elements and fine-tunes the transformer-based pretrained generative models in a multi-task manner. The experimental results demonstrate that our approach outperforms previously established extraction-based and generative-based methods, as well as the baseline variants.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400323265",
    "type": "article"
  },
  {
    "title": "Analyzing the Effects of Transcription Errors on Summary Generation of Bengali Spoken Documents",
    "doi": "https://doi.org/10.1145/3678005",
    "publication_date": "2024-07-17",
    "publication_year": 2024,
    "authors": "Priyanjana Chowdhury; Nabanika Sarkar; Sanghamitra Nath; Utpal Sharma",
    "corresponding_authors": "",
    "abstract": "Automatic speech recognition (ASR) has become an indispensable part of the AI domain, with various speech technologies reliant on it. The quality of speech recognition depends on the amount of annotated data used to train an ASR system, among other factors. For a low-resourced language, this is a severe constraint and thus ASR quality is often poor. Humans can read through text containing ASR-errors, provided the context of the sentence is preserved. Yet in cases of transcripts generated by ASR systems of low-resource languages, multiple important words are misrecognized and the context is mostly lost; discerning such a text becomes nearly impossible. This article analyzes the types of transcription errors that occur while generating ASR transcripts of spoken documents in Bengali, an under-resourced language predominantly spoken in India and Bangladesh. The transcripts of the Bengali spoken document are generated using the ASR of Google Cloud Speech. The article also explores if there is an effect of such transcription errors in generating speech summaries of these spoken documents. Summarization is carried out extractively; sentences are selected from the ASR-generated text of the spoken document. Speech summaries are created by aggregating the speech-segments from the original speech of the selected sentences. Subjective evaluation shows the “readability” of the spoken summaries are not degraded by ASR errors, but the quality is affected due to the reliance on intermediate text-summary containing transcription errors.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400737577",
    "type": "article"
  },
  {
    "title": "Translation from Tunisian Dialect to Modern Standard Arabic: Exploring Finite-State Transducers and Sequence-to-Sequence Transformer Approaches",
    "doi": "https://doi.org/10.1145/3681788",
    "publication_date": "2024-07-24",
    "publication_year": 2024,
    "authors": "Roua Torjmen; Kais Haddar",
    "corresponding_authors": "",
    "abstract": "Translation from the mother tongue, including the Tunisian dialect, to modern standard Arabic is a highly significant field in natural language processing due to its wide range of applications and associated benefits. Recently, researchers have shown increased interest in the Tunisian dialect, primarily driven by the massive volume of content generated spontaneously by Tunisians on social media following the revolution. This article presents two distinct translators for converting the Tunisian dialect into Modern Standard Arabic. The first translator utilizes a rule-based approach, employing a collection of finite state transducers and a bilingual dictionary derived from the study corpus. On the other hand, the second translator relies on deep learning models, specifically the sequence-to-sequence transformer model and a parallel corpus. To assess, evaluate, and compare the performance of the two translators, we conducted tests using a parallel corpus comprising 8,599 words. The results achieved by both translators are noteworthy. The translator based on finite state transducers achieved a BLEU score of 56.65, while the transformer model-based translator achieved a higher score of 66.07.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400955022",
    "type": "article"
  },
  {
    "title": "TRGCN: A Prediction Model for Information Diffusion Based on Transformer and Relational Graph Convolutional Network",
    "doi": "https://doi.org/10.1145/3672074",
    "publication_date": "2024-07-26",
    "publication_year": 2024,
    "authors": "Jinghua Zhao; X. R. Lyu; Haiying Rong; Jiale Zhao",
    "corresponding_authors": "",
    "abstract": "In order to capture and integrate the structural features and temporal features contained in social graph and diffusion cascade more effectively, an information diffusion prediction model based on the Transformer and Relational Graph Convolutional Network (TRGCN) is proposed. Firstly, a dynamic heterogeneous graph composed of the social network graph and the diffusion cascade graph was constructed, and it was input into the Relational Graph Convolutional Network (RGCN) to extract the structural features of each node. Secondly, the time embedding of each node was reencoded using Bi-directional Long Short-Term Memory (Bi-LSTM). The time decay function was introduced to give different weights to nodes at different time positions, so as to obtain the temporal features of nodes. Finally, structural features and temporal features were input into Transformer and then merged. The spatiotemporal features are obtained for information diffusion prediction. The experimental results on three real datasets of X (formerly known as Twitter), Douban, and Memetracker show that compared with the optimal model in the comparison experiment, the TRGCN model has an average increase of 4.16% in Hits@100 metric and 13.26% in map@100 metric. The validity and rationality of the model are proved.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401029975",
    "type": "article"
  },
  {
    "title": "Improved Arithmetic Optimization Algorithm with Transfer Learning based Arabic Sign Language Identification System",
    "doi": "https://doi.org/10.1145/3686795",
    "publication_date": "2024-08-05",
    "publication_year": 2024,
    "authors": "Muhammad Swaileh A. Alzaidi",
    "corresponding_authors": "Muhammad Swaileh A. Alzaidi",
    "abstract": "The Arabic sign language (ArSL) has witnessed ground-breaking research activities to identify hand gestures and signs through the deep learning (DL) model. SL is a unique communication tool that bridges the gaps between people with hearing impairment and ordinary people. The ArSL recognition system is of immense importance for different groups of people since it enables individuals with hearing impairment to communicate effectively. In SLs, signs are characterized by discrepancies in hand positions, shapes, motions, body parts, and facial expressions, posing a crucial threat to visual recognition in computer vision (CV). An automated sign detection technique needs two primary courses of action: the recognition of specific features and the classification of the input dataset. Previously, several approaches for detecting and classifying SLs had been proposed. In this study, an Improved Metaheuristics with Transfer Learning based Arabic Sign Language Identification System (IMTL-ArSL) method is developed. The primary objective of the IMTL-ArSL method is to detect and classify the existence of various signs in the Arabic language. In the IMTL-ArSL model, the bilateral filtering (BF) model is initially used for preprocessing. Besides, the Residual Network (ResNet50v2) model is applied for extracting features, and an improved arithmetic optimization algorithm (IAOA) model is utilized for hyperparameter tuning. Finally, a gated recurrent unit (GRU) network is exploited to identify sign languages. The empirical analysis of the IMTL-ArSL approach is tested using a benchmark dataset. The experimental values of the IMTL-ArSL approach portrayed a superior accuracy value of 93.87% over other techniques.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401334041",
    "type": "article"
  },
  {
    "title": "Chimp Optimization Algorithm with Deep Learning-Driven Fine-grained Emotion Recognition in Arabic Corpus",
    "doi": "https://doi.org/10.1145/3686796",
    "publication_date": "2024-08-06",
    "publication_year": 2024,
    "authors": "WALA BIN SUBAIT; Iman A. Basheti; Hanan Al Sultan; Muhammad Swaileh A. Alzaidi; Jawhara Aljabri; Mohammed Assiri; Nahla Salih",
    "corresponding_authors": "",
    "abstract": "Recently, emotion analysis and classification of tweets have become a crucial area of research. The Arabic language had experienced difficulties with emotion classification on Twitter(X), needing preprocessing more than other languages. Emotion detection is a major challenge in Natural Language Processing (NLP), which allows machines to ascertain the emotions expressed in the text. The task includes recognizing and identifying human feelings such as fear, anger, sadness, and joy. The discovered sentiments and feelings expressed in tweets have gained much recognition in recent years. The Arab region has played a substantial role in international politics and the global economy needs to scrutinize the emotions and sentiments in the Arabic language. Lexicon-based and machine-learning techniques are two common models that address the problems of emotion classification. This study introduces a Chimp Optimization Algorithm with a Deep Learning-Driven Arabic Fine-grained Emotion Recognition (COADL-AFER) technique. The presented COADL-AFER technique mainly aims to detect several emotions in Arabic tweets. In addition to its academic significance, the COADL-AFER technique has practical applications in various fields, including enhancing applications of E-learning, aiding psychologists in recognising terrorist performance, improving product quality, and enhancing customer service. The COADL-AFER technique applies the long short-term memory (LSTM) model for emotion detection. Finally, the hyperparameter selection of the LSTM method can be accomplished by COA. The experimental validation of the COADL-AFER system, a crucial step in our research, is verified utilizing the Arabic tweets dataset. The simulation results stated the betterment of the COADL-AFER technique, further reinforcing the reliability of our research.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401366747",
    "type": "article"
  },
  {
    "title": "Enhanced BERT-based Multi-Head Self-Attention Transformer for Transformation of Marathi Text to Marathi Sign Language Gloss",
    "doi": "https://doi.org/10.1145/3687304",
    "publication_date": "2024-08-08",
    "publication_year": 2024,
    "authors": "Prachi Waghmare; Ashwini M. Deshpande",
    "corresponding_authors": "",
    "abstract": "One recent advancement in the field of machine learning is the translation of text into sign language gloss, which is a form of natural language for the deaf community. The work presented is a new method to translate Marathi text to Marathi sign language gloss by combining salient features of Bidirectional Encoder Representation from Transformer (BERT) for tokenization and complementing the tokenized frame with Encoder with Attention mechanism and decoding with the LSTM decoder. The work conducted experiments on the created corpora of Marathi text and Marathi sign language gloss sentence pairs. The experiments that employed three models show that the suggested model performs better than the existing approaches. The results show that the translation of Marathi text to sign gloss achieves improved performances with an accuracy of 91.5% and the BLEU scores BLEU-1: 85, BLEU-2: 75, BLEU-3: 65, and BLEU-4: 57.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401427418",
    "type": "article"
  },
  {
    "title": "Learning and Vision-based approach for Human fall detection and classification in naturally occurring scenes using video data",
    "doi": "https://doi.org/10.1145/3687125",
    "publication_date": "2024-08-10",
    "publication_year": 2024,
    "authors": "S. P. Singh; Kumkum Kumari; Ankita Vaish",
    "corresponding_authors": "",
    "abstract": "The advancement of medicine presents challenges for modern cultures, especially with unpredictable elderly falling incidents anywhere due to serious health issues. Delayed rescue for at-risk elders can be dangerous. Traditional elder safety methods like video surveillance or wearable sensors are inefficient and burdensome, wasting human resources and requiring caregivers' constant fall detection monitoring. Thus, a more effective and convenient solution is needed to ensure elderly safety. In this article, a method is presented for detecting human falls in naturally occurring scenes using videos through a traditional Convolutional Neural Network (CNN) model, Inception-v3, VGG-19, and two versions of the You Only Look Once (YOLO) working model. The primary focus of this work is human fall detection through the utilization of deep learning models. Specifically, the YOLO approach is adopted for object detection and tracking in video scenes. By implementing YOLO, human subjects are identified, and bounding boxes are generated around them. The classification of various human activities, including fall detection is accomplished through the analysis of deformation features extracted from these bounding boxes. The traditional CNN model achieves an impressive 99.83% accuracy in human fall detection, surpassing other state-of-the-art methods. However, training time is longer compared to YOLO-v2 and YOLO-v3, but significantly shorter than Inception-v3, taking only around 10% of its total training time.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401482714",
    "type": "article"
  },
  {
    "title": "MMHFND: Fusing Modalities for Multimodal Multiclass Hindi Fake News Detection via Contrastive Learning",
    "doi": "https://doi.org/10.1145/3686797",
    "publication_date": "2024-08-12",
    "publication_year": 2024,
    "authors": "Richa Sharma; Arti Arya",
    "corresponding_authors": "",
    "abstract": "Multimodal content contains more deception than unimodal information, causing significant social and economic impacts. Current techniques often focus on a single modality, neglecting knowledge fusion. While most studies have concentrated on English fake news detection, this study explores multimodality for low-resource languages like Hindi. This work introduces the MMHFND model, based on M-CLIP, which uses late fusion for coarse (Fake vs Real) and fine-grained (World vs India vs Politics vs News vs Fact-Check) configurations. We extract deep representations from image and text using image transformer ResNet-50, a BERT-based L3cube-HindRoberta text transformer handling headlines, content, OCR text, and image captions, paired M-CLIP transformers, and an ELA (Error-Level Analysis) image forensic method incorporating EfficientNet B0 to analyze multimodal news in Hindi language based on Devanagari script. M-CLIP integrates cross-modal similarity mapping of images and texts with retrieved multimodal features. The extracted features undergo redundancy reduction before being channeled into the final classifier. The MAM (Modality Attention Mechanism) is introduced, which generates weights for each modality individually. The MMHFND model uses a computed modality divergence score to identify dissonance between modalities and a modified contrastive loss on the score. We thoroughly analyze the HinFakeNews dataset in a multimodal context, achieving accuracy in coarse- and fine-grained configurations. We also undertake an ablation study to evaluate outcomes and explore alternative fusion processes on three different setups. The results show that the MMHFND model effectively detects fake news in Hindi with an accuracy of 0.986, outperforming other existing multimodal approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401517315",
    "type": "article"
  },
  {
    "title": "A Text-Inception-Based Natural Language Processing Model for Sentiment Analysis of Drug Experiences",
    "doi": "https://doi.org/10.1145/3678470",
    "publication_date": "2024-08-12",
    "publication_year": 2024,
    "authors": "Muhammad Swaileh A. Alzaidi; Alya Alshammari; Manar Almanea; Haneen A. Al-Khawaja; Hanan Al Sultan; Shoayee Dlaim Alotaibi; Wafa Almukadi",
    "corresponding_authors": "",
    "abstract": "The study of sentiment in Natural Language Processing (NLP) is among the most successful research areas because of the availability of millions of user opinions online since the turn of the century. The economic, political, and medical fields are just some of the many that have benefited from studies of sentiment research. While numerous studies have examined more mainstream topics like consumer electronics, movies, and restaurants, relatively few have examined health and medical concerns. Considerable insight into where to direct efforts to improve public health might be gained by a study of how people feel about healthcare as a whole and of individual drug experiences in particular. When it comes to medicine, automatic analysis of online user evaluations paves the way for sifting through massive amounts of user feedback to find information regarding medications' efficacy and side effects that might be used to enhance pharmacovigilance programs. Simple rules-based methods have given way to more complex machine learning approaches like deep learning, which is developing as a technology for many natural language processing jobs. The opensource datasets have been analyzed with models that use word embeddings and term frequency-inverse document frequency (TF-IDF). A feature-enhanced text-inception model for sentiment classification was presented to work in tandem with this approach. The model first employed a cutting-edge text-inception module to glean useful shallow features from the text. K-MaxPooling was subsequently employed to reduce the dimensionality of its shallow and deep includes as well as enhance the generalization of characteristics, and a deep feature extraction module was formed using the bidirectional gated recurrent unit (Bi-GRU) and the capsule neural network to comprehend the text's semantic data. By combining traditional methods with cutting-edge artificial intelligence techniques, this hybrid approach can revolutionize public health initiatives, decision-making, and pharmacovigilance in the healthcare industry. This model achieved an exceptional accuracy rate of 99%, underscoring its effectiveness in sentiment classification and demonstrating its potential to significantly contribute to advancing healthcare and medical research.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401517427",
    "type": "article"
  },
  {
    "title": "Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization",
    "doi": "https://doi.org/10.1145/3689629",
    "publication_date": "2024-08-23",
    "publication_year": 2024,
    "authors": "Songhua Yang; Chenghao Zhang; Chenyuan He; Hongfei Xu; Hongying Zan; Yuxiang Jia",
    "corresponding_authors": "",
    "abstract": "The Biomedical Entity Normalization (BEN) task aims to align raw, unstructured medical entities to standard entities, thus promoting data coherence and facilitating better downstream medical applications. Recently, prompt learning methods have shown promising results in the natural language processing field. However, existing research falls short in tackling the more complex Chinese BEN task, especially in the few-shot scenario with limited medical data, and the vast potential of the external medical knowledge base has not yet been fully exploited. To address these challenges, this article proposes a novel Knowledge-injected Prompt Learning (PL-Knowledge) method. Specifically, the approach consists of five stages: candidate entity matching, knowledge extraction, knowledge encoding, knowledge injection, and prediction output. By effectively encoding the knowledge items contained in medical entities and incorporating them into tailor-made knowledge-injected templates, the additional knowledge enhances the model’s ability to capture latent relationships between medical entities, thus achieving a better match with the standard entities. Comprehensive experiments are conducted on a benchmark dataset in both few-shot and full-scale settings. This method outperforms existing baselines, with an average accuracy improvement of 12.96 percentage points in few-shot and 0.94 percentage points in full-data cases, showcasing its excellence in the BEN task.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401815768",
    "type": "article"
  },
  {
    "title": "K-Means text clustering method based on Decision Grey Wolf Optimization",
    "doi": "https://doi.org/10.1145/3689210",
    "publication_date": "2024-08-20",
    "publication_year": 2024,
    "authors": "Jianwei Wang; chengsheng pan; Jianfeng Shi",
    "corresponding_authors": "",
    "abstract": "Aiming at the problem that the traditional algorithm is easy to fall into local optimum in the process of text clustering, which leads to inaccurate text clustering results, a text clustering method based on decision grey wolf optimization K-Means is proposed to cluster the text data set and the standard UCI data set respectively. Afterword segmentation, stop words removal, feature extraction, and text vectorization of text data, the powerful optimization ability of the Decision Gray Wolf Optimization (DGWO) algorithm is used for global optimization, and the clustering center in K-Means algorithm is replaced by the location of wolves. The position of the wolf group is updated by iterative optimization to obtain the optimal clustering center, to perform text clustering. The experimental results show that compared with the traditional method, the precision, recall, and F-Measure of the text data clustering are improved by 49.22%, 51.15%, and 48.98% respectively. The precision, recall, and F-Measure of UCI data clustering are increased by 23.92%, 25.40%, and 24.70% respectively, and the text clustering results are more reliable.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401976155",
    "type": "article"
  },
  {
    "title": "Improving Cross-lingual Aspect-based Sentiment Analysis with Sememe Bridge",
    "doi": "https://doi.org/10.1145/3691342",
    "publication_date": "2024-09-21",
    "publication_year": 2024,
    "authors": "Yijiang Liu; Fei Li; Donghong Ji",
    "corresponding_authors": "",
    "abstract": "Aspect-based Sentiment Analysis (ABSA) comprises numerous subtasks including aspect term extraction (AE), opinion term extraction (OE), opinion pair extraction (PE), and triplet extraction (TE). Current research in Chinese ABSA primarily concentrates on aspect terms and sentiment polarity, with insufficient emphasis on opinion terms. This article aims to provide a viable solution for the unannotated Chinese ABSA subtasks such as OE, PE, and TE. First, we develop an English-Chinese parallel dataset for ABSA using a semi-automatic process involving machine translation and a word aligner. Second, we examine the efficacy of cross-lingual transfer methods. Third, we propose a plug-and-play transfer method based on sememe knowledge. Sememes are the language-independent smallest semantic units that encapsulate the components, commonalities, and attributes of things extracted from the real world, which can bridge the gap between English and Chinese. Experimental results show that our proposed method brings significant improvements for Chinese ABSA, and achieves a maximum increase of 8% on the F1 metric for model transfer and label transfer on OE, PE and TE.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402697106",
    "type": "article"
  },
  {
    "title": "EiAP-BC: A Novel Emoji Aware Inter-Attention Pair Model for Contextual Spam Comment Detection Based on Posting Text",
    "doi": "https://doi.org/10.1145/3696663",
    "publication_date": "2024-09-25",
    "publication_year": 2024,
    "authors": "Antonius Rachmat Chrismanto; Edi Winarko; Yohanes Suyanto",
    "corresponding_authors": "",
    "abstract": "Detecting spam comments on social media remains a continuously discussed research topic until this day, especially on public figure/celebrity accounts in Indonesia. However, the previous studies only focused on the comments themselves, without considering the context of the posting and the use of emojis in social media. This study proposes a new deep learning model called EiAP-BC (Emoji-aware Inter-Attention Pair BiLSTM CNN) for spam comment detection through a novel approach that considers the contextual information of the posts, enabling the spam comments detection using the relatedness between the comment and its corresponding post that usually discarded. This model can also handle emoji content in comments and posts, which is widely used in social media. The model was tested using the SPAMID-PAIR dataset created from social media in the Indonesian language, achieving the highest accuracy of 88% and performing competitively with existing deep learning models. To assess its generalization capabilities, the EiAP-BC model was also evaluated using similar public datasets and models in sentence-pair classification tasks, and an ablation study was conducted to determine the importance of each layer and its coordination. The EiAP-BC model exhibits several advantages in size, training speed, and parameter count compared to existing state-of-the-art models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402829260",
    "type": "article"
  },
  {
    "title": "Overview of Multimodal Machine Learning",
    "doi": "https://doi.org/10.1145/3701031",
    "publication_date": "2024-10-17",
    "publication_year": 2024,
    "authors": "Aya M. Al‐Zoghby; Esraa Mohamed K. Al-Awadly; Ahmed Ismail Ebada; Wael A. Awad",
    "corresponding_authors": "",
    "abstract": "Human nature is fundamentally driven by the need for interaction and attention, which are fulfilled through various sensory modalities, including hearing, sight, touch, taste, and smell. These senses enable us to perceive, understand, and engage with the world around us. The quality and depth of our interactions change considerably when we use multiple senses simultaneously, highlighting the importance of multimodal interactions in our daily lives. In the realm of technology, multimodal integration offers immense value, as it aims to create systems that can replicate or complement these natural human abilities for enhanced interaction. This paper explores the significance of spatial multimodalities in machine learning, highlighting their role in improving model performance in applications such as autonomous driving, healthcare, and virtual assistants. It addresses challenges like the complexity of fusing diverse sensory data types and proposes solutions such as advanced data fusion techniques, adaptive learning algorithms, and transformer architectures. The goal is to provide an overview of state-of-the-art research and future directions for advancing human-computer interaction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403490486",
    "type": "article"
  },
  {
    "title": "An Extended Pattern Based Comprehensive Stemmer for the Urdu Language",
    "doi": "https://doi.org/10.1145/3701231",
    "publication_date": "2024-10-21",
    "publication_year": 2024,
    "authors": "Mubashir Ali; Anees Baqir; Hafiz Husnain Raza Sherazi; Shehzad Khalid; Phillip Smith; Mark Lee",
    "corresponding_authors": "",
    "abstract": "The Urdu language is used by approximately 200 million people for spoken and written communications on a daily basis. There is a substantial amount of unstructured Urdu textual data that is available worldwide. Data mining techniques can be used to extract meaningful knowledge from such a large, potentially informative source of data. There are many text processing systems available to process unstructured textual data. However, these systems are mostly language specific and developed for a variety of languages such as English, Spanish, Chinese, and so on. Unfortunately, there are not as many language processing resources available for Urdu. Stemming is one of the most important preprocessing steps in the text mining process and its goal is to reduce grammatical words form, e.g., parts of speech, gender, tense, and so on, to their root form. In this work, we have extended the stemming capabilities of our existing pattern-based comprehensive stemming system for Urdu text. In addition to the existing stemming rules in previous work, we introduce novel stemming rules for prefix, and infix stemming. We also optimize the existing suffix removal rules and extend the add character lists for word normalization. These stemming rules are generic and have the ability to generate the stem of Urdu words as well as loan words (words belonging to other languages i.e., Arabic, Persian, Turkish). In the experimental evaluation, we have observed a significant improvement in the overall stemming accuracy of our proposed pattern-based Urud stemmer, which demonstrates the adoptability of the proposed stemming approach for a variety of text-processing applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403608289",
    "type": "article"
  },
  {
    "title": "Arabic Question Generation Using Transformers",
    "doi": "https://doi.org/10.1145/3701559",
    "publication_date": "2024-10-28",
    "publication_year": 2024,
    "authors": "Anwar Alajmi; Haniah Altabaa; Sa’ed Abed; Imtiaz Ahmad",
    "corresponding_authors": "",
    "abstract": "After the increased reliance on online education, online assessment became an essential tool for educators to remotely monitor and evaluate students’ understanding in order to assist them properly. However, the laborious process of creating exam questions is a challenge for most teachers. Thus, automated Question Generation aims to assist teachers by generating questions from given data. Limited research has been conducted to tackle this issue in the Arabic Language due to the complexity of the language and the limited amount of available Arabic data. This paper explores different implementations of the transformer models, that demonstrated their superiority in natural language processing. Three approaches were introduced to tackle this problem with Arabic data using an Arabic-based transformer, an English-based transformer, and a multilingual-based transformer. Each of the fine-tuned models was trained using ARCD, XGLUE, DialectBench, and ArabicQA data sets and evaluated on automatic and manual metrics. Two of the proposed models achieve state-of-the-art results on the Arabic question generation task. The English transformer obtained a ROUGE score of 0.59 on XGLUE, while the Arabic transformer model achieves 0.49 on ARCD. Both of these models demonstrate excellent quality of questions through human-conducted evaluations by achieving low WER and high GC, U, and A scores.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403842092",
    "type": "article"
  },
  {
    "title": "A Survey of Coreference and Zeros Resolution for Arabic",
    "doi": "https://doi.org/10.1145/3702323",
    "publication_date": "2024-10-30",
    "publication_year": 2024,
    "authors": "Abdulrahman Aloraini; Juntao Yu; Wateen Aliady; Massimo Poesio",
    "corresponding_authors": "",
    "abstract": "Coreference resolution is the task of resolving mentions that refer to the same entity into clusters. The area and its tasks are crucial in natural language processing (NLP) applications. Extensive surveys of this task have been conducted for English and Chinese; not too much for Arabic. The few Arabic surveys do not cover recent progress and the challenges for Arabic anaphora; and do not cover zero resolution and comprehensive resolution of zeros and full mentions, or anaphora resolution beyond coreference (e.g., bridging). In this paper, we examine the state-of-the-art for Arabic anaphora resolution, highlighting the challenges and advances in this field. We provide a comprehensive survey of the methods employed for Arabic coreference resolution, as well as an overview of the existing datasets and challenges. The goal is to equip researchers with a thorough understanding of Arabic anaphora resolution and to suggest potential future directions in the field.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403896058",
    "type": "article"
  },
  {
    "title": "A Simple Yet Effective Corpus Construction Framework for Indonesian Grammatical Error Correction",
    "doi": "https://doi.org/10.1145/3704264",
    "publication_date": "2024-11-12",
    "publication_year": 2024,
    "authors": "Nankai Lin; Meiyu Zeng; Wentao Huang; Shengyi Jiang; Lixian Xiao; Aimin Yang",
    "corresponding_authors": "",
    "abstract": "Currently, the majority of research in grammatical error correction (GEC) is concentrated on universal languages, such as English and Chinese. Many low-resource languages lack accessible evaluation corpora. How to efficiently construct high-quality evaluation corpora for GEC in low-resource languages has become a significant challenge. To fill these gaps, in this paper, we present a framework for constructing GEC corpora. Specifically, we focus on Indonesian as our research language and construct an evaluation corpus for Indonesian GEC using the proposed framework, addressing the limitations of existing evaluation corpora in Indonesian. Furthermore, we investigate the feasibility of utilizing existing large language models (LLMs), such as GPT-3.5-Turbo and GPT-4, to streamline corpus annotation efforts in GEC tasks. The results demonstrate significant potential for enhancing the performance of LLMs in low-resource language settings. Our code and corpus can be obtained from https://github.com/GKLMIP/GEC-Construction-Framework.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404279922",
    "type": "article"
  },
  {
    "title": "A Comprehensive Analysis Dashboard for Detecting Similar Saudi Twitter Accounts by Using Stylometric Features",
    "doi": "https://doi.org/10.1145/3705002",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Taghreed Bagies; Rahaf Alsuhaimi; Miada Almasre; Alaa Bafail",
    "corresponding_authors": "",
    "abstract": "Criminals, including terrorists, may use Twitter to communicate and share their ideologies. They often employ multiple accounts for anonymity. While the other accounts hide their identities and use it for different purposes (e.g., communication with unknown criminals), they use one to write tweets revealing their beliefs and spreading evil thoughts (e.g., racism and bullying). Since these multiple accounts will not have the same contents, stakeholders cannot rely on the contents to detect various accounts belonging to the same person. Using stylometric features may help to detect these accounts as they depend on the writing style of a person rather than the content of the words and their meaning. In this paper, we build a model and use stylometric features that differ from the state-of-the-art, such as n-gram for part-of-speech tag, frequency of repeated characters, number of emojis, and more. These features distinguish our approach from existing methods. We evaluated different machine and deep learning classifiers to make comparisons. Our results highlight the effectiveness of our feature set, achieving a remarkable accuracy of 96%. Additionally, our findings indicate that machine learning classifiers exhibit superiority over their deep learning counterparts in the context of this study. Furthermore, we developed a comprehensive dashboard that offers users an in-depth analysis of different Twitter accounts and ranks their similarities. The dashboard serves as a valuable tool for gaining insights into the relationships and similarities among the analyzed Twitter accounts.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404599758",
    "type": "article"
  },
  {
    "title": "Semantically-Informed Graph Neural Networks for Irony Detection in Turkish",
    "doi": "https://doi.org/10.1145/3705610",
    "publication_date": "2024-11-25",
    "publication_year": 2024,
    "authors": "Necva Bölücü; Burcu Can",
    "corresponding_authors": "",
    "abstract": "Social media plays an important role in expressing the thoughts and sentiments of users. Irony is a way of stating a sentiment about something by expressing the opposite of the intended literal meaning. Irony detection is a recent emerging task in low-resource languages, although other tasks related to sentiment, such as sentiment analysis and emotion detection, have been widely tackled. In this study, we investigate Graph Neural Networks (GNNs) for irony detection in Turkish, a low-resource language in sentiment-related tasks. We incorporate semantic information into the GNNs using the Universal Conceptual Cognitive Annotation (UCCA) framework. Extensive experimental results and in-depth analysis show that our models outperform state-of-the-art irony detection models in Turkish. Our UCCA-GAT (UCCA-Graph Attention Network) model achieves an F 1 -score of 94.85% (7.362% gain over the state-of-the-art) on the Turkish-Irony-Dataset and an accuracy of 72.82% (4.39% gain over the state-of-the-art) on the IronyTR Dataset. We also provide a comprehensive analysis of the proposed models to understand their limitations.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404700187",
    "type": "article"
  },
  {
    "title": "Emotion Recognition from Spontaneous Tunisian Dialect Speech",
    "doi": "https://doi.org/10.1145/3708340",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Latifa Iben Nasr; Abir Masmoudi; Lamia Hadrich Belguith",
    "corresponding_authors": "",
    "abstract": "Emotional expressions are a fundamental aspect of human communication, with speech being one of the most natural modes of interaction. Speech Emotion Recognition (SER) is a significant research topic in Natural Language Processing (NLP), aimed at identifying emotions such as satisfaction, frustration, and anger from speech audio using multiple classifiers. This paper presents a method to emotion recognition from spontaneous Tunisian Dialect (TD) speech, marking the first work in the SER field to utilize spontaneous speech for emotion recognition in this dialect. The dataset was created from freely available YouTube videos across multiple domains and labeled with four perceived emotions: anger, satisfaction, frustration, and neutral. To address the data scarcity issue, we implemented data augmentation techniques, specifically Vocal Tract Length Perturbation (VTLP). The preprocessing of the speech signals involved cleaning the data from ambient and unwanted noises. We extracted and selected various spectral features, including mel-frequency cepstral coefficients (MFCC) and Linear Prediction Cepstral Coefficients (LPCC). Subsequently, we applied several classification methods: Support Vector Machine (SVM), Bidirectional Long Short-Term Memory (BiLSTM), Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Random Forest. Our experiments demonstrated that the Random Forest classifier achieved the highest F-score of 58.75%. The results were thoroughly discussed, analyzed, and compared across the five models using different feature extractions. This study provides valuable insights and advancements in the SER field, particularly for the TD, future research directions for improving emotion recognition systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405545612",
    "type": "article"
  },
  {
    "title": "Semantic Feature Graph Consistency with Contrastive Cluster Assignments for Multilingual Document Clustering",
    "doi": "https://doi.org/10.1145/3708887",
    "publication_date": "2024-12-19",
    "publication_year": 2024,
    "authors": "Teng Sun; Zhenqiu Shu; Yuxin Huang; Hongbin Wang; Zhengtao Yu",
    "corresponding_authors": "",
    "abstract": "Multilingual document clustering (MDC) aims to partition multilingual documents into distinct clusters based on topic categories in an unsupervised manner. However, existing MDC methods still suffer from several limitations in practice tasks. Firstly, most of them optimize multiple objectives within the same feature space, thereby leading to the conflict between learning consistently shared semantics and reconstructing inconsistent view-specific information. Secondly, several methods directly integrate information from multilingual documents during the fusion stage, thereby overlooking the semantic differences between different language features. To address the aforementioned problems, we propose a novel multi-view learning method, called Semantic Feature Graph Consistency with Contrastive Cluster Assignments (SFGC 3 A), for multilingual document clustering. Specifically, the proposed SFGC 3 A method implements consistency objective and reconstruction objective in different feature spaces, thus effectively avoiding conflicts between consistency learning and inconsistency reconstruction. Subsequently, we design the semantic feature graph consistency and semantic label consistency modules to further explore consistent semantic information among multilingual documents, thereby reducing the semantic differences among different language views. Extensive experiments on several multilingual document datasets have shown the effectiveness of the proposed SFGC 3 A method in MDC tasks. The source codes for this work will be released later.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405580274",
    "type": "article"
  },
  {
    "title": "Improving Semantic Coherence of Gujarati Text Topic Model Using Inflectional Forms Reduction and Single-letter Words Removal",
    "doi": "https://doi.org/10.1145/3447760",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Uttam Chauhan; Apurva Shah",
    "corresponding_authors": "",
    "abstract": "A topic model is one of the best stochastic models for summarizing an extensive collection of text. It has accomplished an inordinate achievement in text analysis as well as text summarization. It can be employed to the set of documents that are represented as a bag-of-words, without considering grammar and order of the words. We modeled the topics for Gujarati news articles corpus. As the Gujarati language has a diverse morphological structure and inflectionally rich, Gujarati text processing finds more complexity. The size of the vocabulary plays an important role in the inference process and quality of topics. As the vocabulary size increases, the inference process becomes slower and topic semantic coherence decreases. If the vocabulary size is diminished, then the topic inference process can be accelerated. It may also improve the quality of topics. In this work, the list of suffixes has been prepared that encounters too frequently with words in Gujarati text. The inflectional forms have been reduced to the root words concerning the suffixes in the list. Moreover, Gujarati single-letter words have been eliminated for faster inference and better quality of topics. Experimentally, it has been proved that if inflectional forms are reduced to their root words, then vocabulary length is shrunk to a significant extent. It also caused the topic formation process quicker. Moreover, the inflectional forms reduction and single-letter word removal enhanced the interpretability of topics. The interpretability of topics has been assessed on semantic coherence, word length, and topic size. The experimental results showed improvements in the topical semantic coherence score. Also, the topic size grew notably as the number of tokens assigned to the topics increased.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3134798880",
    "type": "article"
  },
  {
    "title": "Multimodal News Feed Evaluation System with Deep Reinforcement Learning Approaches",
    "doi": "https://doi.org/10.1145/3414523",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "S. Rakesh Kumar; S. Muthuramalingam; Fadi Al‐Turjman",
    "corresponding_authors": "",
    "abstract": "Multilingual and multimodal data analysis is the emerging news feed evaluation system. News feed analysis and evaluations are interrelated processes, which are useful in understanding the news factors. The news feed evaluation system can be implemented for single or multilingual language models. Classification techniques used on multilingual news analysis require deep layered learning techniques rather than conventional approaches. In this proposed work, a hierarchical structure of deep learning algorithms is implemented for making an effective complex news evaluation system. Deep learning techniques such as the Deep Cooperative Multilingual Reinforcement Learning Model, the Multidimensional Genetic Algorithm, and the Multilingual Generative Adversarial Network are developed to evaluate a vast number of news feeds. The proposed tech-niques collaborate in a pipeline order to build a deep news feed evaluation system. The implementation details project that the newly proposed system performs 5% to 12% better than the other news evaluation systems.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3135478787",
    "type": "article"
  },
  {
    "title": "Finding Better Subwords for Tibetan Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3448216",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Yachao Li; Jing Jiang; Yangji JIA; Ning Ma",
    "corresponding_authors": "",
    "abstract": "Subword segmentation plays an important role in Tibetan neural machine translation (NMT). The structure of Tibetan words consists of two levels. First, words consist of a sequence of syllables, and then a syllable consists of a sequence of characters. According to this special word structure, we propose two methods for Tibetan subword segmentation, namely syllable-based and character-based methods. The former generates subwords based on the Tibetan syllables, and the latter is based on Tibetan characters. In addition, we carry out experiments with these two subword segmentation methods on low-resource Tibetan-to-Chinese NMT, respectively. The experimental results show that both of them can improve translation performance, in which the subword segmentation based on character sequences can achieve better results. Overall, our proposed character-based subword segmentation is more simple and effective. Moreover, it can achieve better experimental results without paying much attention to the linguistic features of Tibetan.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3139121486",
    "type": "article"
  },
  {
    "title": "A Cascaded Unsupervised Model for PoS Tagging",
    "doi": "https://doi.org/10.1145/3447759",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Necva Bölücü; Burcu Can",
    "corresponding_authors": "",
    "abstract": "Part of speech (PoS) tagging is one of the fundamental syntactic tasks in Natural Language Processing, as it assigns a syntactic category to each word within a given sentence or context (such as noun, verb, adjective, etc.). Those syntactic categories could be used to further analyze the sentence-level syntax (e.g., dependency parsing) and thereby extract the meaning of the sentence (e.g., semantic parsing). Various methods have been proposed for learning PoS tags in an unsupervised setting without using any annotated corpora. One of the widely used methods for the tagging problem is log-linear models. Initialization of the parameters in a log-linear model is very crucial for the inference. Different initialization techniques have been used so far. In this work, we present a log-linear model for PoS tagging that uses another fully unsupervised Bayesian model to initialize the parameters of the model in a cascaded framework. Therefore, we transfer some knowledge between two different unsupervised models to leverage the PoS tagging results, where a log-linear model benefits from a Bayesian model’s expertise. We present results for Turkish as a morphologically rich language and for English as a comparably morphologically poor language in a fully unsupervised framework. The results show that our framework outperforms other unsupervised models proposed for PoS tagging.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3151055720",
    "type": "article"
  },
  {
    "title": "Sanskrit Parsing Following Indian Theories of Verbal Cognition",
    "doi": "https://doi.org/10.1145/3418061",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Amba Kulkarni",
    "corresponding_authors": "Amba Kulkarni",
    "abstract": "Pāṇini’s grammar is an important milestone in the Indian grammatical tradition. Unlike grammars of other languages, it is almost exhaustive and together with the theories of śābdabodha (verbal cognition), this grammar provides a system for language analysis as well as generation. The theories of śābdabodha describe three conditions necessary for verbal cognition. They are ākāṅkṣā (expectancy), yogyatā (meaning congruity), and sannidhi (proximity). We examine them from a computational viewpoint and provide appropriate computational models for their representation. Next, we describe the design of a parser following the theories of śābdabodha and present three algorithms for solving the constraints imposed by the theories of śābdabodha . The first algorithm is modeled as a constraint satisfaction problem, the second one as a vertex-centric graph traversal, and the third one as an edge-centric binary join, each one being an improvement over the previous one.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3156705778",
    "type": "article"
  },
  {
    "title": "Cross-lingual Adaptation Using Universal Dependencies",
    "doi": "https://doi.org/10.1145/3448251",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Nasrin Taghizadeh; Heshaam Faili",
    "corresponding_authors": "",
    "abstract": "We describe a cross-lingual adaptation method based on syntactic parse trees obtained from the Universal Dependencies (UD), which are consistent across languages, to develop classifiers in low-resource languages. The idea of UD parsing is to capture similarities as well as idiosyncrasies among typologically different languages. In this article, we show that models trained using UD parse trees for complex NLP tasks can characterize very different languages. We study two tasks of paraphrase identification and relation extraction as case studies. Based on UD parse trees, we develop several models using tree kernels and show that these models trained on the English dataset can correctly classify data of other languages, e.g., French, Farsi, and Arabic. The proposed approach opens up avenues for exploiting UD parsing in solving similar cross-lingual tasks, which is very useful for languages for which no labeled data is available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3165582353",
    "type": "article"
  },
  {
    "title": "The Research on Rejoining of the Oracle Bone Rubbings Based on Curve Matching",
    "doi": "https://doi.org/10.1145/3460393",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Yaolin Tian; Weize Gao; Xuxing Liu; Shanxiong Chen; Bofeng Mo",
    "corresponding_authors": "",
    "abstract": "The rejoining of oracle bone rubbings is a fundamental topic for oracle research. However, it is a tough task to reassemble severely broken oracle bone rubbings because of detail loss in manual labeling, the great time consumption of rejoining, and the low accuracy of results. To overcome the challenges, we introduce a novel CFDA&amp;CAP algorithm that consists of the Curve Fitting Degree Analysis (CFDA) algorithm and the Correlation Analysis of Pearson (CAP) algorithm. First, the orthogonalization system is constructed to extract local features based on the curve features analysis. Second, the global feature descriptor is depicted by using coordinate points sequences. Third, we screen candidate curves based on the features as well as the CFDA algorithm, so the search range of the candidates is narrowed down. Finally, image recommendation libraries for target curves are generated by adopting the CAP algorithm, and the rank for each target matching curve generates simultaneously for result evaluation. With experiments, the proposed method shows a good effect in rejoining oracle bone rubbings automatically: (1) it improves the average accuracy rate of curve matching up to 84%, and (2) for a low-resource task, the accuracy of our method has 25% higher accuracy than that of other methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3193784082",
    "type": "article"
  },
  {
    "title": "Event Graph Neural Network for Opinion Target Classification of Microblog Comments",
    "doi": "https://doi.org/10.1145/3469725",
    "publication_date": "2021-11-02",
    "publication_year": 2021,
    "authors": "Yan Xiang; Zhengtao Yu; Junjun Guo; Yuxin Huang; Yantuan Xian",
    "corresponding_authors": "",
    "abstract": "Opinion target classification of microblog comments is one of the most important tasks for public opinion analysis about an event. Due to the high cost of manual labeling, opinion target classification is generally considered as a weak-supervised task. This article attempts to address the opinion target classification of microblog comments through an event graph convolution network (EventGCN) in a weak-supervised manner. Specifically, we take microblog contents and comments as document nodes, and construct an event graph with three typical relationships of event microblogs, including the co-occurrence relationship of event keywords extracted from microblogs, the reply relationship of comments, and the document similarity. Finally, under the supervision of a small number of labels, both word features and comment features can be represented well to complete the classification. The experimental results on two event microblog datasets show that EventGCN can significantly improve the classification performance compared with other baseline models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3211125622",
    "type": "article"
  },
  {
    "title": "Deep Level Analysis of Legitimacy in Bengali News Sentences",
    "doi": "https://doi.org/10.1145/3459928",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Soma Das; Pooja Rai; Sanjay Chatterji",
    "corresponding_authors": "",
    "abstract": "The tremendous increase in the growth of misinformation in news articles has the potential threat for the adverse effects on society. Hence, the detection of misinformation in news data has become an appealing research area. The task of annotating and detecting distorted news article sentences is the immediate need in this research direction. Therefore, an attempt has been made to formulate the legitimacy annotation guideline followed by annotation and detection of the legitimacy in Bengali e-papers. The sentence-level manual annotation of Bengali news has been carried out in two levels, namely “Level-1 Shallow Level Classification” and “Level-2 Deep Level Classification” based on semantic properties of Bengali sentences. The tagging of 1,300 anonymous Bengali e-paper sentences has been done using the formulated guideline-based tags for both levels. The validation of the annotation guideline has been done by applying benchmark supervised machine learning algorithms using the lexical feature, syntactic feature, domain-specific feature, and Level-2 specific feature in both levels. Performance evaluation of these classifiers is done in terms of Accuracy, Precision, Recall, and F-Measure. In both levels, Support Vector Machine outperforms other benchmark classifiers with an accuracy of 72% and 65% in Level-1 and Level-2, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3216880711",
    "type": "article"
  },
  {
    "title": "Developing a Cross-lingual Semantic Word Similarity Corpus for English–Urdu Language Pair",
    "doi": "https://doi.org/10.1145/3472618",
    "publication_date": "2021-11-18",
    "publication_year": 2021,
    "authors": "Ghazeefa Fatima; Rao Muhammad Adeel Nawab; Muhammad Salman Khan; Ali Saeed",
    "corresponding_authors": "",
    "abstract": "Semantic word similarity is a quantitative measure of how much two words are contextually similar. Evaluation of semantic word similarity models requires a benchmark corpus. However, despite the millions of speakers and the large digital text of the Urdu language on the Internet, there is a lack of benchmark corpus for the Cross-lingual Semantic Word Similarity task for the Urdu language. This article reports our efforts in developing such a corpus. The newly developed corpus is based on the SemEval-2017 task 2 English dataset, and it contains 1,945 cross-lingual English–Urdu word pairs. For each of these pairs of words, semantic similarity scores were assigned by 11 native Urdu speakers. In addition to corpus generation, this article also reports the evaluation results of a baseline approach, namely “Translation Plus Monolingual Analysis” for automated identification of semantic similarity between English–Urdu word pairs. The results showed that the path length similarity measure performs better for the Google and Bing translated words. The newly created corpus and evaluation results are freely available online for further research and development.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3217488387",
    "type": "article"
  },
  {
    "title": "Enhancing Lexical Translation Consistency for Document-Level Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3485469",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Xiaomian Kang; Yang Zhao; Jiajun Zhang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Document-level neural machine translation (DocNMT) has yielded attractive improvements. In this article, we systematically analyze the discourse phenomena in Chinese-to-English translation, and focus on the most obvious ones, namely lexical translation consistency. To alleviate the lexical inconsistency, we propose an effective approach that is aware of the words which need to be translated consistently and constrains the model to produce more consistent translations. Specifically, we first introduce a global context extractor to extract the document context and consistency context, respectively. Then, the two types of global context are integrated into a encoder enhancer and a decoder enhancer to improve the lexical translation consistency. We create a test set to evaluate the lexical consistency automatically. Experiments demonstrate that our approach can significantly alleviate the lexical translation inconsistency. In addition, our approach can also substantially improve the translation quality compared to sentence-level Transformer.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200087490",
    "type": "article"
  },
  {
    "title": "Light Diacritic Restoration to Disambiguate Homographs in Modern Arabic Texts",
    "doi": "https://doi.org/10.1145/3486675",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Aqil M. Azmi; Rehab M. Alnefaie; Hatim Aboalsamh",
    "corresponding_authors": "",
    "abstract": "Diacritic restoration (also known as diacritization or vowelization) is the process of inserting the correct diacritical markings into a text. Modern Arabic is typically written without diacritics, e.g., newspapers. This lack of diacritical markings often causes ambiguity, and though natives are adept at resolving, there are times they may fail. Diacritic restoration is a classical problem in computer science. Still, as most of the works tackle the full (heavy) diacritization of text, we, however, are interested in diacritizing the text using a fewer number of diacritics. Studies have shown that a fully diacritized text is visually displeasing and slows down the reading. This article proposes a system to diacritize homographs using the least number of diacritics, thus the name “light.” There is a large class of words that fall under the homograph category, and we will be dealing with the class of words that share the spelling but not the meaning. With fewer diacritics, we do not expect any effect on reading speed, while eye strain is reduced. The system contains morphological analyzer and context similarities. The morphological analyzer is used to generate all word candidates for diacritics. Then, through a statistical approach and context similarities, we resolve the homographs. Experimentally, the system shows very promising results, and our best accuracy is 85.6%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200130398",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on Misinformation, Fake News and Rumor Detection in Low-Resource Languages",
    "doi": "https://doi.org/10.1145/3505588",
    "publication_date": "2021-12-24",
    "publication_year": 2021,
    "authors": "Akshi Kumar; Christian Esposito; D.A. Karras",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to Special Issue on Misinformation, Fake News and Rumor Detection in Low-Resource Languages Authors: Akshi Kumar View Profile , Christian Esposito View Profile , Dimitrios A. Karras View Profile Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingVolume 21Issue 1January 2022 Article No.: 1epp 1–3https://doi.org/10.1145/3505588Online:24 December 2021Publication History 0citation247DownloadsMetricsTotal Citations0Total Downloads247Last 12 Months247Last 6 weeks39 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200367444",
    "type": "article"
  },
  {
    "title": "Arabic Handwritten Word Recognition Based on Stationary Wavelet Transform Technique using Machine Learning",
    "doi": "https://doi.org/10.1145/3474391",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Atallah Al-Shatnawi; Faisal Al-Saqqar; Alireza Souri",
    "corresponding_authors": "",
    "abstract": "This paper is aimed at improving the performance of the word recognition system (WRS) of handwritten Arabic text by extracting features in the frequency domain using the Stationary Wavelet Transform (SWT) method using machine learning, which is a wavelet transform approach created to compensate for the absence of translation invariance in the Discrete Wavelets Transform (DWT) method. The proposed SWT-WRS of Arabic handwritten text consists of three main processes: word normalization, feature extraction based on SWT, and recognition. The proposed SWT-WRS based on the SWT method is evaluated on the IFN/ENIT database applying the Gaussian, linear, and polynomial support vector machine, the k-nearest neighbors, and ANN classifiers. ANN performance was assessed by applying the Bayesian Regularization (BR) and Levenberg-Marquardt (LM) training methods. Numerous wavelet transform (WT) families are applied, and the results prove that level 19 of the Daubechies family is the best WT family for the proposed SWT-WRS. The results also confirm the effectiveness of the proposed SWT-WRS in improving the performance of handwritten Arabic word recognition using machine learning. Therefore, the suggested SWT-WRS overcomes the lack of translation invariance in the DWT method by eliminating the up-and-down samplers from the proposed machine learning method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200583170",
    "type": "article"
  },
  {
    "title": "Adversarial Separation Network for Text Style Transfer",
    "doi": "https://doi.org/10.1145/3472621",
    "publication_date": "2021-12-30",
    "publication_year": 2021,
    "authors": "Haitong Yang; Guangyou Zhou; Tingting He",
    "corresponding_authors": "",
    "abstract": "This article considers the task of text style transfer: transforming a specific style of sentence into another while preserving its style-independent content. A dominate approach to text style transfer is to learn a good content factor of text, define a fixed vector for every style and recombine them to generate text in the required style. In fact, there are a large number of different words to convey the same style from different aspects. Thus, using a fixed vector to represent one style is very inefficient, which causes the weak representation power of the style vector and limits text diversity of the same style. To address this problem, we propose a novel neural generative model called Adversarial Separation Network (ASN), which can learn the content and style vector jointly and the learnt vectors have strong representation power and good interpretabilities. In our method, adversarial learning is implemented to enhance our model’s capability of disentangling the two factors. To evaluate our method, we conduct experiments on two benchmark datasets. Experimental results show our method can perform style transfer better than strong comparison systems. We also demonstrate the strong interpretability of the learnt latent vectors.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4205592637",
    "type": "article"
  },
  {
    "title": "Role of Morphology Injection in SMT",
    "doi": "https://doi.org/10.1145/3129208",
    "publication_date": "2017-09-15",
    "publication_year": 2017,
    "authors": "S Sreelekha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Phrase-based Statistical models are more commonly used as they perform optimally in terms of both, translation quality and complexity of the system. Hindi and in general all Indian languages are morphologically richer than English. Hence, even though Phrase-based systems perform very well for the less divergent language pairs, for English to Indian language translation, we need more linguistic information (such as morphology, parse tree, parts of speech tags, etc.) on the source side. Factored models seem to be useful in this case, as Factored models consider word as a vector of factors. These factors can contain any information about the surface word and use it while translating. Hence, the objective of this work is to handle morphological inflections in Hindi and Marathi using Factored translation models while translating from English. SMT approaches face the problem of data sparsity while translating into a morphologically rich language. It is very unlikely for a parallel corpus to contain all morphological forms of words. We propose a solution to generate these unseen morphological forms and inject them into original training corpora. In this paper, we study factored models and the problem of sparseness in context of translation to morphologically rich languages. We propose a simple and effective solution which is based on enriching the input with various morphological forms of words. We observe that morphology injection improves the quality of translation in terms of both adequacy and fluency. We verify this with the experiments on two morphologically rich languages: Hindi and Marathi, while translating from English.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4300334900",
    "type": "article"
  },
  {
    "title": "Modeling Monolingual Character Alignment for Automatic Evaluation of Chinese Translation",
    "doi": "https://doi.org/10.1145/2815619",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Maoxi Li; Mingwen Wang; Hanxi Li; Fan Xu",
    "corresponding_authors": "",
    "abstract": "Automatic evaluation of machine translations is an important task. Most existing evaluation metrics rely on matching the same word or letter n -grams. This strategy leads to poor results on Chinese translations because one has to rely merely on matching identical characters. In this article, we propose a new evaluation metric that allows different characters with the same or similar meaning to match. An Indirect Hidden Markov Model (IHMM) is proposed to align the Chinese translation with human references at the character level. In the model, the emission probabilities are estimated by character similarity, including character semantic similarity and character surface similarity, and transition probabilities are estimated by a heuristic distance-based distortion model. When evaluating the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks, the experimental results indicate that the proposed metric has a significantly better correlation with human evaluation than the state-of-the-art machine translation metrics (i.e., BLEU, Meteor Universal, and TESLA-CELAB). This study shows that it is important to allow different characters to match in the evaluation of Chinese translations and that the IHMM is a reasonable approach for the alignment of Chinese characters.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2271795500",
    "type": "article"
  },
  {
    "title": "From Image to Translation",
    "doi": "https://doi.org/10.1145/2857052",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "Tongtao Zhang; Aritra Chowdhury; Nimit Dhulekar; Jinjing Xia; Kevin Knight; Heng Ji; Bülent Yener; Li-Ming Zhao",
    "corresponding_authors": "",
    "abstract": "The lack of computational support has significantly slowed down automatic understanding of endangered languages. In this paper, we take Nyushu (simplified Chinese: 女书; literally: “women’s writing”) as a case study to present the first computational approach that combines Computer Vision and Natural Language Processing techniques to deeply understand an endangered language. We developed an end-to-end system to read a scanned hand-written Nyushu article, segment it into characters, link them to standard characters, and then translate the article into Mandarin Chinese. We propose several novel methods to address the new challenges introduced by noisy input and low resources, including Nyushu-specific feature selection for character segmentation and linking, and character linking lattice based Machine Translation. The end-to-end system performance indicates that the system is a promising approach and can serve as a standard benchmark.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2405776910",
    "type": "article"
  },
  {
    "title": "Pairwise Comparative Classification for Translator Stylometric Analysis",
    "doi": "https://doi.org/10.1145/2898997",
    "publication_date": "2016-06-27",
    "publication_year": 2016,
    "authors": "Heba El-Fiqi; Eleni Petraki; Hussein A. Abbass",
    "corresponding_authors": "",
    "abstract": "In this article, we present a new type of classification problem, which we call Comparative Classification Problem (CCP), where we use the term data record to refer to a block of instances. Given a single data record with n instances for n classes, the CCP problem is to map each instance to a unique class. This problem occurs in a wide range of applications where the independent and identically distributed assumption is broken down. The primary difference between CCP and classical classification is that in the latter, the assignment of a translator to one record is independent of the assignment of a translator to a different record. In CCP, however, the assignment of a translator to one record within a block excludes this translator from further assignments to any other record in that block. The interdependency in the data poses challenges for techniques relying on the independent and identically distributed (iid) assumption. In the Pairwise CCP (PWCCP), a pair of records is grouped together. The key difference between PWCCP and classical binary classification problems is that hidden patterns can only be unmasked by comparing the instances as pairs. In this article, we introduce a new algorithm, PWC4.5, which is based on C4.5, to manage PWCCP. We first show that a simple transformation—that we call Gradient-Based Transformation (GBT)—can fix the problem of iid in C4.5. We then evaluate PWC4.5 using two real-world corpora to distinguish between translators on Arabic-English and French-English translations. While the traditional C4.5 failed to distinguish between different translators, GBT demonstrated better performance. Meanwhile, PWC4.5 consistently provided the best results over C4.5 and GBT.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2462992868",
    "type": "article"
  },
  {
    "title": "Improving Semantic Parsing with Enriched Synchronous Context-Free Grammars in Statistical Machine Translation",
    "doi": "https://doi.org/10.1145/2963099",
    "publication_date": "2016-11-03",
    "publication_year": 2016,
    "authors": "Junhui Li; Muhua Zhu; Wei Lu; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Semantic parsing maps a sentence in natural language into a structured meaning representation. Previous studies show that semantic parsing with synchronous context-free grammars (SCFGs) achieves favorable performance over most other alternatives. Motivated by the observation that the performance of semantic parsing with SCFGs is closely tied to the translation rules, this article explores to extend translation rules with high quality and increased coverage in three ways. First, we examine the difference between word alignments for semantic parsing and statistical machine translation (SMT) to better adapt word alignment in SMT to semantic parsing. Second, we introduce both structure and syntax informed nonterminals, better guiding the parsing in favor of well-formed structure, instead of using a uninformed nonterminal in SCFGs. Third, we address the unknown word translation issue via synthetic translation rules. Last but not least, we use a filtering approach to improve performance via predicting answer type. Evaluation on the standard GeoQuery benchmark dataset shows that our approach greatly outperforms the state of the art across various languages, including English, Chinese, Thai, German, and Greek.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2547305264",
    "type": "article"
  },
  {
    "title": "User-based Hierarchical Network of Sina Weibo Emotion Analysis",
    "doi": "https://doi.org/10.1145/3579048",
    "publication_date": "2023-01-06",
    "publication_year": 2023,
    "authors": "Qian Chen; Xiao Sun; Jiamin Wang; Meng Wang",
    "corresponding_authors": "",
    "abstract": "Emotion analysis on Sina Weibo has a great impetus for government agencies to survey public opinion and enterprises to track market demand. Most of the existing emotion analysis work on Sina Weibo focuses on mining the information contained in a single Weibo, ignoring the problem of inaccurate information extraction caused by the lack of contextual information in Weibo texts. Inspired by humans judging user emotional states from Weibo texts, this article creates a Weibo text five-category emotion classification dataset based on active users and proposes a user-based hierarchical network for Weibo emotion analysis. First, use the multi-head attention mechanism and convolutional neural network set in the information extraction module to analyze a single Weibo text to fully extract the emotional information contained in the text; at the same time, through the moving window set in the relevant information capture module, obtain other Weibo texts posted by the same user within a period, and capture the effective correlation information between Weibo texts; then, the dual text representation obtained above is concatenated, and through the information interaction layer, the relevant information is retrieved again, and the text representation is updated; finally, the classifier output the five-category emotion labels corresponding to each Weibo text. We demonstrate the model’s effectiveness through experiments and analysis in the results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4313649684",
    "type": "article"
  },
  {
    "title": "Hyper Parameter Optimization of CRNN for Printed Devanagari Script Recognition using Taguchi's Method",
    "doi": "https://doi.org/10.1145/3578549",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Shaheera Saba Mohd Naseem Akhter; Priti P. Rege",
    "corresponding_authors": "",
    "abstract": "The Devanagari script is one of the most widely used scripts worldwide. The existing deep learning-based optical character recognition system for printed Devanagari scripts using Convolutional Neural Network – Recurrent Neural Network , or CRNN is not robust enough to recognize any randomly printed Devanagari scanned document. At present, the hyper-parameters of the CRNN system are selected randomly either with the trial-and-error or grid search methods. Moreover, there is no optimized way to choose the hyper-parameters of the CRNN, which improves the recognition accuracy for Devanagari documents. Furthermore, the lack of standard Devanagari script datasets has hampered the development of word recognizers. In this paper, the hyper-parameter of the CRNN system is optimized using Taguchi's method of optimization. The performance of the hyper-parameters optimized CRNN system is compared with the current state-of-the-art text recognition CRNN network. The results reveal that the CRNN optimized with Taguchi's method performs better than the CRNN-based systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4317502697",
    "type": "article"
  },
  {
    "title": "Feature Fusion Pyramid Network for End-to-end Scene Text Detection",
    "doi": "https://doi.org/10.1145/3582003",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "Yirui Wu; Lilai Zhang; Hao Li; Yunfei Zhang; Shaohua Wan",
    "corresponding_authors": "",
    "abstract": "How to properly involve text characteristics like multi-scale, arbitrary direction, length aspect ratio, into detection network design has become a hot topic in computer vision. Feature Pyramid Network (FPN) is a typical method to achieve robust text detection, where its low-level and high-level feature map retains spatial structure and global semantic information, respectively. However, its strict hierarchical structure fails to fuse low-level and high-level information to improve distinguish ability of feature map. To address this problem, we propose a novel feature fusion pyramid network for end-to-end scene text detection by fusing multi-modal information. By diving pyramid structure into high-level and low-level layers, channel and spatial attention modules are adopted to enhance high-level and low-level feature representation by encoding channel and spatial -wise context information, respectively. In order to reduce information loss by layer transmission, a special residual network is designed to achieve short-cut between high-level and low-level features, so as to realize multi-modal feature fusion. Experiments show the precision and recall of the propose method on ICDAR2015, ICDAR2017-MLT and MSRA-TD500 datasets reach 88.7%/82.1%, 77.0%/60.3% and 85.3%/74.8%, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4318618551",
    "type": "article"
  },
  {
    "title": "Think More Ambiguity Less: A Novel Dual Interactive Model with Local and Global Semantics for Chinese Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3583685",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Yue Jia; Wei Fang; Hengyang Lu",
    "corresponding_authors": "",
    "abstract": "Chinese is a representative East Asian language. Chinese Named Entity Recognition (CNER) aims to recognize various entities. It is significant for other NLP tasks to utilize CNER. Recent research to develop CNER systems has been dedicated to either considering word enhancement or capturing global information to strengthen local composition and alleviate word ambiguity in the meanings of words. However, information on words acquired from external lexicons is often confused, and this has led to incorrect judgments regarding the boundaries of words. Moreover, relevant studies typically use excessively complex models to capture the global semantics of sentences. To solve these two problems, we incorporate a global representation into the procedure of local word enhancement. We propose an intuitive and effective dual-module interactive network that can enhance the boundaries of words and extract the global semantics by using a rethinking mechanism to refine the importance of local composition and global information. The results of experiments on four CNER datasets showed that the proposed model can outperform other baselines in terms of the F1 score.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4319964821",
    "type": "article"
  },
  {
    "title": "TAM GAN: Tamil Text to Naturalistic Image Synthesis Using Conventional Deep Adversarial Networks",
    "doi": "https://doi.org/10.1145/3584019",
    "publication_date": "2023-02-16",
    "publication_year": 2023,
    "authors": "M. Diviya; Karmel Arockiasamy",
    "corresponding_authors": "",
    "abstract": "Text-to-image synthesis has advanced recently as a prospective area for improvement in computer vision applications. The image synthesis model follows significant neural network architectures such as Generative Adversarial Networks (GANs). The flourishing text-to-image generation approaches can nominally reflect the meaning of the text in generated images. Still, they need the prospect of providing the necessary details and eloquent object features. Intelligent systems are trained in text-to-image synthesis applications for various languages. However, their contribution to regional languages is yet to be explored. Autoencoders prompt the synthesis of images, but they result in blurriness, which results in clear output and essential features of the picture. Based on textual descriptions, The GAN model is capable of producing realistic images of a high quality that can be used in various applications, like fashion design, photo editing, computer-aided design, and educational platforms. The proposed method uses two-stage processing to create a language model using a BERT model called TAM-BERT and an existing MuRIL BERT, followed by image synthesis using a GAN. The work was conducted using the Oxford-102 dataset, and the model's efficiency was evaluated using the F1-Score measure.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4321089621",
    "type": "article"
  },
  {
    "title": "Matrix Quantization and LPC Vocoder Based Linear Predictive for Low-Resource Speech Recognition system",
    "doi": "https://doi.org/10.1145/3585313",
    "publication_date": "2023-02-23",
    "publication_year": 2023,
    "authors": "Surbhi Bhatia; Ankit Kumar; T. Janaki Rami Reddy; Neeraj Varshney; Shakila Basheer",
    "corresponding_authors": "",
    "abstract": "Over the last ten years, there has been significant progress in the use of low-rate speech coders in voice applications for computers, military communications, and civil communications. This advancement has been made possible by the development of new speech coders that can generate high-quality speech at low data rates. The majority of existing coders include spectral representation of speech, speech waveform matching, and ”optimization” of the coder’s performance for human hearing. The goal of this paper is to provide a thorough evaluation of voice coding methods for educational purposes, with a particular emphasis on the algorithms used in low-rate cellular communication standards. The algorithm we developed using a voice-excited LPC vocoder produces clear, low-distortion results. Ordinary LPCs, on the other hand, fall short of vocoders because they can handle signals other than speech, such as music. To improve quality, additional bandwidth is used to reduce the bit rate. To improve the quality, we tried two approaches. The first was to increase the number of bits required to quantize the DCT coefficients. This coefficient would outperform the inverse DCT in closer error rearrangements. The second possibility is to increase the total number of quantized coefficients. As a result, error array rearrangements would be more accurate. The goal is to identify the point at which a method improvement outperforms the previous, better result. Other coding methods become more complex, but this vocoder suffices.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4321605879",
    "type": "article"
  },
  {
    "title": "Contrastive Adversarial Training for Multi-Modal Machine Translation",
    "doi": "https://doi.org/10.1145/3587267",
    "publication_date": "2023-03-14",
    "publication_year": 2023,
    "authors": "Xin Huang; Jiajun Zhang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "The multi-modal machine translation task is to improve translation quality with the help of additional visual input. It is expected to disambiguate or complement semantics while there are ambiguous words or incomplete expressions in the sentences. Existing methods have tried many ways to fuse visual information into text representations. However, only a minority of sentences need extra visual information as complementary. Without guidance, models tend to learn text-only translation from the major well-aligned translation pairs. In this article, we propose a contrastive adversarial training approach to enhance visual participation in semantic representation learning. By contrasting multi-modal input with the adversarial samples, the model learns to identify the most informed sample that is coupled with a congruent image and several visual objects extracted from it. This approach can prevent the visual information from being ignored and further fuse cross-modal information. We examine our method in three multi-modal language pairs. Experimental results show that our model is capable of improving translation accuracy. Further analysis shows that our model is more sensitive to visual information.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4324142619",
    "type": "article"
  },
  {
    "title": "Integrating Reconstructor and Post-Editor into Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3588766",
    "publication_date": "2023-03-24",
    "publication_year": 2023,
    "authors": "Nian Yi; Chenze Shao; Aishan Wumaier",
    "corresponding_authors": "",
    "abstract": "Neural machine translation (NMT) mainly comprises the encoder and decoder. The encoder is mainly used to extract the feature vector of the source language sentence. The decoder predicts the next token according to the feature vector extracted by the encoder and the information of the current moment. In this process, there is no guarantee that the features extracted by the encoder are indistinguishable from the meaning of the sentences in the source language. There is also no guarantee that the decoder can accurately predict the corresponding character. These issues can lead to over-translation and under-translation issues in the translated results. Previous researchers alleviated this problem by calculating the gap between the reconstructed source-language sentences and the source-language sentences. Inspired by this method, we propose to integrate a reconstructor and a post-editor into NMT during the training. The reconstructor takes the translation of NMT as input to reconstruct the source sentence, and the post-editor takes the translation as input and post-edits it to predict the target sentence. Through the training of the reconstructor and the post-editor, the semantics of the translation are forced to follow the source sentence and the target sentence. Experimental results show that our approach can effectively improve the performance of NMT on multiple translation tasks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4360851369",
    "type": "article"
  },
  {
    "title": "Robust Multi-task Learning-based Korean POS Tagging to Overcome Word Spacing Errors",
    "doi": "https://doi.org/10.1145/3591206",
    "publication_date": "2023-04-05",
    "publication_year": 2023,
    "authors": "Cheoneum Park; Juae Kim",
    "corresponding_authors": "",
    "abstract": "End-to-end neural network-based approaches have recently demonstrated significant improvements in natural language processing (NLP). However, in the NLP application such as assistant systems, NLP components are still processed to extract results using a pipeline paradigm. The pipeline-based concept has issues with error propagation. In Korean, morphological analysis and part-of-speech (POS) tagging step, incorrectly analyzing POS tags for a sentence containing spacing errors negatively affects other modules behind the POS module. Hence, we present a multi-task learning-based POS tagging neural model for Korean with word spacing challenges. When we apply this model to the Korean morphological analysis and POS tagging, we get findings that are robust to word spacing errors. We adopt syllable-level input and output formats, as well as a simple structure for ELECTRA and RNN-CRF models for multi-task learning, and we achieve a good performance 98.30 of F1, better than previous studies on the Sejong corpus test set.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4362639752",
    "type": "article"
  },
  {
    "title": "Metadial: A Meta-learning Approach for Arabic Dialogue Generation",
    "doi": "https://doi.org/10.1145/3590960",
    "publication_date": "2023-04-10",
    "publication_year": 2023,
    "authors": "Mohsen Shamas; Wassim El‐Hajj; Hazem Hajj; Khaled Shaban",
    "corresponding_authors": "",
    "abstract": "Dialogue generation is the automatic generation of a text response, given a user’s input. Dialogue generation for low-resource languages has been a challenging tasks for researchers. However, the advancements in deep learning models have made developing conversational agents that perform the tasks of dialogue generation not only possible, but also effective and helpful in many applications spanning a variety of domains. Nevertheless, work on conversational bots for low-resource languages such as the Arabic language is still limited due to various challenges, including the language structure, vocabulary, and the scarcity of its data resources. Meta-learning has been introduced before in the natural language processing (NLP) realm and showed significant improvements in many tasks; however, it has rarely been used in natural language generation (NLG) tasks and never in Arabic NLG. In this work, we propose a meta-learning approach for Arabic dialogue generation for fast adaptation on low-resource domains, namely, Arabic. We start by using existing pre-trained models; we then meta-learn the initial parameters on high-resource dataset before finetuning the parameters on the target tasks. We prove that the proposed model that employs meta-learning techniques improves generalization and enables fast adaptation of the transformer model on low-resource NLG tasks. We report gains in the BLEU-4 and improvements in Semantic textual Similarity (STS) metrics when compared to the existing state-of-the-art approach. We also do a further study on the effectiveness of the meta-learning algorithms on the response generation of the models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4363677476",
    "type": "article"
  },
  {
    "title": "<b><i>Komala</i>and<i>Kaṭhora</i>: A Novel Approach Towards Classification of Hindi Poetry</b>",
    "doi": "https://doi.org/10.1145/3589249",
    "publication_date": "2023-04-11",
    "publication_year": 2023,
    "authors": "Niraj Kumar Singh; Komal Naaz; Soubhik Chakraborty",
    "corresponding_authors": "",
    "abstract": "Literary compositions are very often analyzed using various constituent units like words, phrases, sentences, and paragraphs. Unlike the conventional research that focuses on the aforementioned constituent units, our task is a statistical effort carried out on the most fundamental unit of any literary composition called varna , or character, followed by automated classification using learning algorithms. This article is a case study on the Hindi adaptations of two significant literary pieces, namely, Jana-Gaṇa-Mana and Vande-Mātaram , and acknowledging that the two songs being studied belong to different classes based on their bhava, i.e., the inherent emotion of the poem. The present task is the first of its kind that uses the concept of komala and kaṭhora varna to establish diversity between the two. The two-proportion Z-test is successfully applied to statistical data pertaining to the candidate songs, thereby reestablishing the theoretical assertions by investigating real pieces of literature. Taking the statistical verification as ground, a learning-based classification system is designed to yield the best accuracy of 85%, which further compliments the theory reestablished statistically.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4364361801",
    "type": "article"
  },
  {
    "title": "Intelligent English Language Translation And Grammar Learning Based On Internet Of Things Technology",
    "doi": "https://doi.org/10.1145/3588769",
    "publication_date": "2023-04-11",
    "publication_year": 2023,
    "authors": "Yuanyuan Chen",
    "corresponding_authors": "Yuanyuan Chen",
    "abstract": "research-article Free Access Share on Intelligent English Language Translation And Grammar Learning Based On Internet Of Things TechnologyJust Accepted Author: Yuanyuan Chen Office of Science and Technology Administration, Zhengzhou University of Economics and Business, Zhengzhou, He Nan, 451191, China Intelligent English Translation; Grammar Learning; Internet of Things Technology; Artificial Intelligence Office of Science and Technology Administration, Zhengzhou University of Economics and Business, Zhengzhou, He Nan, 451191, China Intelligent English Translation; Grammar Learning; Internet of Things Technology; Artificial Intelligence 0009-0003-1237-4982Search about this author Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingAccepted on April 2023https://doi.org/10.1145/3588769Published:11 April 2023Publication History 0citation50DownloadsMetricsTotal Citations0Total Downloads50Last 12 Months50Last 6 weeks12 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4364376887",
    "type": "article"
  },
  {
    "title": "Analysis of Cursive Text Recognition Systems: A Systematic Literature Review",
    "doi": "https://doi.org/10.1145/3592600",
    "publication_date": "2023-04-13",
    "publication_year": 2023,
    "authors": "Sulaiman Khan; Shah Nazir; Habib Ullah Khan",
    "corresponding_authors": "",
    "abstract": "Regional and cultural diversities around the world have given birth to a large number of writing systems and scripts, which consist of varying character sets. Developing an optimal character recognition for such a varying and large character set is a challenging task. Unlimited variations in handwritten text due to mood swings, varying writing styles, changes in medium of writing, and many more puzzle the research community. To overcome this problem, researchers have proposed various techniques for the automatic recognition of cursive languages like Urdu, Pashto, and Arabic. With the passage of time, the field of text recognition matured, and the number of publications exponentially increased in the targeted field. It is very difficult to find all the techniques developed, calculate the time and resource consumptions, and understand the cost–benefit tradeoffs among these techniques. These tradeoffs resist making this technology able for practical use. To address these tradeoffs, this article systematic analysis to identify gaps in the literature and suggest new enhanced solution accordingly. A total of 153 of the most relevant articles from 2008 to 2022 are analyzed in this systematic literature review (SLR) work. This systematic review process shows (1) the list of techniques suggested for cursive text recognition purposes and its capabilities, (2) set of feature extraction techniques proposed, and (3) implementation tools used to design and simulate the empirical studies in this specialized field. We have also discussed the emerging trends and described their implications for the research community in this specialized domain. This systematic assessment will ultimately help researchers to perform an overview of the existing character/text recognition approaches, recognition capabilities, and time consumption and subsequently identify the areas that requires a significant attention in the near future.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4365447147",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue of Recent Advances in Computational Linguistics for Asian Languages",
    "doi": "https://doi.org/10.1145/3588316",
    "publication_date": "2023-03-31",
    "publication_year": 2023,
    "authors": "Jerry Chun‐Wei Lin; Vicente García‐Díaz; Juan Antonio Morente-Molinera",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to the Special Issue of Recent Advances in Computational Linguistics for Asian Languages Authors: Jerry Chun-Wei Lin Western Norway University of Applied Sciences, Bergen, Norway Western Norway University of Applied Sciences, Bergen, Norway 0000-0001-8768-9709View Profile , Vicente GarcÍa DÍaz University of Oviedo, Spain University of Oviedo, Spain 0000-0003-2037-8548View Profile , Juan Antonio Morente Molinera University of Granada, Spain University of Granada, Spain 0000-0002-2729-6900View Profile Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingVolume 22Issue 3Article No.: 62pp 1–5https://doi.org/10.1145/3588316Published:14 April 2023Publication History 0citation29DownloadsMetricsTotal Citations0Total Downloads29Last 12 Months29Last 6 weeks29 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4365518381",
    "type": "article"
  },
  {
    "title": "Sequence Generation Model Integrating Domain Ontology for Mathematical question tagging",
    "doi": "https://doi.org/10.1145/3593804",
    "publication_date": "2023-04-24",
    "publication_year": 2023,
    "authors": "Tao Huang; Shengze Hu; Keke Lin; Huali Yang; Hao Zhang; Houbing Song; Zhihan Lv",
    "corresponding_authors": "",
    "abstract": "In online learning systems, tagging knowledge points for questions is a fundamental task. Automatic tagging technology uses intelligent algorithms to automatically tag knowledge points for questions to reduce manpower and time costs. However, the current knowledge point tagging technology cannot satisfy the situation that mathematics questions often involve a variable number of knowledge points, lacks the consideration of the characteristics of the mathematics field, and ignores the internal connection between knowledge points. To address the above issues, we propose a Sequence Generation Model Integrating Domain Ontology for Mathematical question tagging (SOMPT). SOMPT performs data augmentation for text and then obtains intermediate text based on domain ontology replacement to facilitate deep learning model to understand mathematical question text. SOMPT is able to obtain dynamic word vector embedding to optimize the textual representation for math questions. What’s more, our model can capture the relationship between tags to generate knowledge points more accurately in the way of sequence generation. The comparative experimental results show that our proposed model has an excellent tagging ability for mathematical questions. Moreover, the sequence generation module in SOMPT can be applied on other multi-label classification tasks and be on par with the state-of-the-art performance models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4366830208",
    "type": "article"
  },
  {
    "title": "Multilingual BERT-based Word Alignment By Incorporating Common Chinese Characters",
    "doi": "https://doi.org/10.1145/3594634",
    "publication_date": "2023-04-26",
    "publication_year": 2023,
    "authors": "Zezhong Li; Xiao Sun; Fuji Ren; Jianjun Ma; Degen Huang; Piao Shi",
    "corresponding_authors": "",
    "abstract": "Word alignment is an important task of detecting translation equivalents between a sentence pair. Although word alignment is no longer necessarily needed for neural machine translation, it’s still useful in a wealth of applications, e.g., bilingual lexicon induction, constraint decoding, and so on. However, the most well-known word aligners are still Giza++ and fastAlign, both of which are implementations of traditional IBM models. To keep pace with the advance in NMT, there has been a surge of interest in replacing the IBM models with neural models. We follow this trend but aim to boost performance of word alignment between Japanese and Chinese, which share a large portion of Chinese characters. Our key idea is to leverage these common Chinese characters in both languages as an indicator for inferring alignment; i.e., the source and target words with the common Chinese characters should be most likely aligned. Following this idea, we propose three methods that leverage common Chinese characters to boost the mBERT-based word alignment, including reward factor, representation alignment, and contrastive training. Furthermore, we annotate and release a golden dataset for Japanese-Chinese word alignment. Experiments on the dataset show that our methods outperform several strong baselines in terms of AER score and verify the effectiveness of exploiting common Chinese characters.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4367055929",
    "type": "article"
  },
  {
    "title": "Enhancing RDF Verbalization with Descriptive and Relational Knowledge",
    "doi": "https://doi.org/10.1145/3595293",
    "publication_date": "2023-05-01",
    "publication_year": 2023,
    "authors": "Fan Zhang; Meishan Zhang; Shuang Liu; Yueheng Sun; Nan Duan",
    "corresponding_authors": "",
    "abstract": "RDF verbalization has received increasing interest, which aims to generate a natural language description of the knowledge base. Sequence-to-sequence models based on Transformer are able to obtain strong performance equipped with pre-trained language models such as BART and T5. However, in spite of the general performance gain introduced by the pre-trained models, the performance of the task is still limited by the small scale of the training dataset. To address the problem, we propose two orthogonal strategies to enhance the representation learning of RDF triples. Concretely, two types of knowledge are introduced, i.e., descriptive knowledge and relational knowledge, respectively. The descriptive knowledge indicates the semantic information of self definition, and the relational knowledge indicates the semantic information learned from the structural context. We further combine the descriptive and relational knowledge together to enhance the representation learning. Experimental results on the WebNLG and SemEval-2010 datasets show that the two types of knowledge can both enhance the model performance, and their combination is able to obtain further improvements in most cases, providing new state-of-the-art results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4367604604",
    "type": "article"
  },
  {
    "title": "Research on the Application of Neural Network Classification Model in English Grammar Error Correction",
    "doi": "https://doi.org/10.1145/3596492",
    "publication_date": "2023-05-08",
    "publication_year": 2023,
    "authors": "Yang Xi",
    "corresponding_authors": "Yang Xi",
    "abstract": "The field of English grammar error correction has developed rapidly in recent years, and many excellent research results have been obtained. However, errors are generally viewed as a whole for revision, which cannot be subdivided to identify the specific error types of sentences, lacks interpretability, and requires a lot of manpower to label the data. To address this, the study uses the CoreNLP tool to output the input text as a basic form of each word, and encodes and decodes the linguistic text in the corpus using the CNN-based Seq2Seq model. And word embedding and left-right GRU operations are performed on left-right text and target words in turn, and the merged new vector is obtained using two attention mechanism operations. It is fed into the MLP classifier for classification and error correction. To further improve the model performance, two model optimizers, stochastic gradient descent (SGD) and Adam, are used to tune the parameters of the classification prediction model. The study thus constructs an English grammar classification and error correction model based on a neural network classification method. The experimental analysis shows that the average accuracy of the model constructed in the study is 96.83%, the average recall rate is 78.93%, and the average overall evaluation F0.5 is 92.67%, which can effectively and accurately classify and correct the grammatical errors that occur in the process of English language learning.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4375861559",
    "type": "article"
  },
  {
    "title": "A Novel Multi-Party Authentication Scheme for FCN-based MIoT Systems in Natural Language Processing Environment",
    "doi": "https://doi.org/10.1145/3590149",
    "publication_date": "2023-05-11",
    "publication_year": 2023,
    "authors": "Xiangwei Meng; Ce Yang; Yiren Qi; Wei Liang; Zisang Xu; Kuan‐Ching Li; Hai Deng",
    "corresponding_authors": "",
    "abstract": "Natural language processing (NLP) assists to increase the efficiency of human and Multimedia Internet of Things (MIoT) interaction. Notably, large-scale NLP tasks can be offloaded from a cloud server to fog nodes closer to a mobile terminal device for lower response latency. But communication security is ongoing issues that need to be addressed. Effective mutual authentication among multiple entities is essential to ensure the security of MIoT systems based on a dynamic Fog Computing Network (FCN). However, the existing schemes are unsuitable for the dynamic FCN due to the security vulnerabilities such as the linkable sessions. To solve this problem, an Anonymous Multi-Party Authentication (AMPA) scheme is proposed to address the challenges of secure FCN-based MIoT communications in this paper. The proposed scheme uses a bilinear pairing operation to realize the authentication between the fog nodes and cloud server and to establish the group key. Besides, the scheme allows cloud-authenticated terminal devices to be added to the FCN and reduces the need for the resource-limited terminal device to perform many authentication protocols. The security analysis is carried out to demonstrate that AMPA scheme can meet various safety requirements. Performance evaluations shown that the proposed AMPA scheme achieves satisfactory performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4376149579",
    "type": "article"
  },
  {
    "title": "Attention-based Stacked Bidirectional Long Short-term Memory Model for Word Sense Disambiguation",
    "doi": "https://doi.org/10.1145/3594780",
    "publication_date": "2023-05-18",
    "publication_year": 2023,
    "authors": "Yujia Sun; Jan Platoš",
    "corresponding_authors": "",
    "abstract": "Word sense disambiguation is a basic task in Natural Language Processing which aims to identify the most appropriate sense of ambiguous words in different contexts by applying algorithm models. In this work, we propose a model that uses a stacked bidirectional Long Short-Term Memory neural network and attention mechanism to determine the sense of ambiguous words. First, the stacked bidirectional Long Short-Term Memory is employed for deep embedding-based representation of sentences containing ambiguous words. Then, we utilize the self-attention mechanism to highlight the contextual features of ambiguous words, and then construct the overall semantic representation of sentences. Finally, the sentence semantic representation is applied to the multilayer perception classifier to generate the appropriate category of the ambiguous word sense items. This model is tested on the Semeval-2007 task-17: English lexical samples dataset and using examples of ambiguous words sourced from Oxford, Cambridge, and Collins dictionaries as extra test datasets. The effectiveness of the proposed approach is demonstrated via comparison with existing word sense disambiguation approaches. Our experimental results show that the proposed model outperforms other word sense disambiguation methods in terms of the evaluation metrics (Average Accuracy, Micro F1-Score, Kappa, and Matthews Correlation Coefficient), and exhibits strong interpretability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4377028551",
    "type": "article"
  },
  {
    "title": "An Effective Language Convention Model Based on Deep Structured Learning and Natural Language Processing for Higher Education",
    "doi": "https://doi.org/10.1145/3490502",
    "publication_date": "2023-05-20",
    "publication_year": 2023,
    "authors": "Yang Ya-lan; Wei Tang",
    "corresponding_authors": "",
    "abstract": "Language translation is highly needed in the increasingly networked world. The writing of English essays is an essential competency for higher education students. For non-English students in China, this is especially important. They often represent large numbers of Chinese English as second language students. Therefore this paper introduced Deep Structured Learning with Natural language processing (DSL-NLP) has been proposed to convert the Chinese language to the English language for student learning. An automatic input tool for writing assistance will be beneficial in building the proposed DSL-NLP method. The relationship between textual attributes and input from human teachers and provide characteristics feedback system. This study explored natural language processing (NLP) to determine the interpretative actions required for a successful understanding of the literary text. NLP attempts to assist in achieving consistency in applied linguistics, improving classroom communication, optimizing learner attitudes and motivation, raising personality, facilitating personal development, and even changing students' perspectives toward life. Thus the experimental results show the DSL-NLP improves student learning in the English language, enhances student performance in education, and effectively increases the student career",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4377137815",
    "type": "article"
  },
  {
    "title": "Query Context Expansion for Open-Domain Question Answering",
    "doi": "https://doi.org/10.1145/3603498",
    "publication_date": "2023-06-05",
    "publication_year": 2023,
    "authors": "Wenhao Zhu; Xiaoyu Zhang; Ye Liang; Qiuhong Zhai",
    "corresponding_authors": "",
    "abstract": "Humans are accustomed to autonomously associating prior knowledge with the text in a query when answering questions. However, for machines lacking cognition and common sense, a query is merely a combination of some words. Although we can enrich the semantic information of the given query through language representation or query expansion (QE) , the information contained in the query is still insufficient. In this paper, we propose an effective passage retrieval method named query context expansion-based retrieval (QCER) for open-domain question answering (OpenQA) . QCER associates a query with domain information by adding contextual association information based on the pseudo-relevance feedback (PRF) . QCER uses a dense reader to select top-n expansion terms for QE. We implement QCER by appending reader predictions, theoretically present in candidate passages, as contextual information to the initial query to form the new query. QCER with sparse representations (BM25) can improve retrieval efficiency and accelerate query convergence so that the reader can find the desired answer using fewer relevant passages, e.g., 10 passages, as soon as possible. Moreover, QCER can be easily combined with dense passage retrieval (DPR) to achieve even better performance, as sparse and dense representations are often complementary. Remarkably, we demonstrate that QCER achieves state-of-the-art performance in three tasks, passage retrieval, passage reading, and passage reranking, on the Natural Questions (NQ) and TriviaQA (Trivia) datasets under an extractive QA setup.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4379388346",
    "type": "article"
  },
  {
    "title": "Experiments of Supervised Learning and Semi-Supervised Learning in Thai Financial News Sentiment: A Comparative Study",
    "doi": "https://doi.org/10.1145/3603499",
    "publication_date": "2023-06-08",
    "publication_year": 2023,
    "authors": "Suntarin Sangsavate; Sukree Sinthupinyo; Achara Chandrachai",
    "corresponding_authors": "",
    "abstract": "Sentiment classification is an instrument of natural language processing tasks in text analysis to measure customer feedback from given documents such as product reviews, news, and texts. This research aims to experiment with Thai financial news sentiment classification and evaluate sentiment classification performance. In this research, we show financial news sentiment classification experimental results when comparing supervised and semi-supervised methods. In the research methodology, we use PyThaiNLP to tokenize and remove stopwords and split datasets into 85% of the training set and 15% of the testing set. Next, we classify sentiment using machine learning and deep learning approaches with feature extraction such as bag-of-words, term frequency–inverse document frequency, and word embedding (Word2Vec and Bidirectional Encoder Representations from Transformers (BERT)) in given texts. The results show that support vector machine with the BERT model yields the best performance at 83.38%; in contrast, the random forest classifier with bag-of-words yields the worst performance at 54.10% in the machine learning approach. Another experiment reveals that long short-term memory with the BERT model yields the best performance at 84.07% in contrast to the convolutional neural network with bag-of-words, which yields the worst performance at 69.80% in the deep learning approach. The results imply that support vector machine, convolutional neural network, and long short-term memory are suitable for classifying sentiment in complex structure language. From this study, we observe the importance of sentiment classification tools between supervised and semi-supervised learning, and we look forward to furthering this work.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4379876250",
    "type": "article"
  },
  {
    "title": "Fuzzy Influenced Process to Generate Comparable to Parallel Corpora",
    "doi": "https://doi.org/10.1145/3599235",
    "publication_date": "2023-06-17",
    "publication_year": 2023,
    "authors": "Debajyoty Banik; Asif Ekbal; Suresh Chandra Satapathy",
    "corresponding_authors": "",
    "abstract": "Data-driven supervised approaches rely on the parallel corpus. Due to lack of data and resources availability, it has become more difficult to achieve accurate outputs. In addition, the efficiency of the machine translation system depends on the quality of the used corpora. Hindi still lacks good quality parallel corpora and needs more resources for accurate machine translation. Comparable corpora are easily available compared to parallel corpora, but they cannot be used directly in machine translation. In our present research, we propose an algorithm to mine these comparable corpora from the web, and generate the parallel corpora automatically. Machine translation systems, system combination approach, and IR-based technique join their hands together to choose the set of sentence pairs. Then the sentence pairs having the best score are chosen to prepare the final parallel corpora. The primary modules of this architecture are fuzzy logic-based evaluation metric, information retrieval module, statistical machine translation system, Google neural machine translation system, Microsoft machine translation system, and system combination module for machine translation. For case study, we prepare the Hindi-English parallel corpora of (30825 + 51235) = 82060 sentence pairs. Evaluation results show that the F-Score measurement varies from 95.73 to 96.98 for various data sets. The source code and prepared dataset (comparable and parallel corpus) can be found at https://github.com/debajyoty/Comparable-partallel-Algo2.git.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4381054130",
    "type": "article"
  },
  {
    "title": "So2al-wa-Gwab: A New Arabic Question-Answering Dataset Trained on Answer Extraction Models",
    "doi": "https://doi.org/10.1145/3605550",
    "publication_date": "2023-06-22",
    "publication_year": 2023,
    "authors": "Hani Al-Omari; Rehab Duwairi",
    "corresponding_authors": "",
    "abstract": "Question answering (QA) is the task of responding to questions posed by users automatically. A question-answering system is divided into three main components: question analysis, information retrieval, and answer extraction. This paper has focused only on the answer extraction part. In the past couple of years, many QA systems have been developed and become mature and ready for use in different languages. Nevertheless, the advancement of Arabic QA systems still faces different obstacles and a lack of relevant resources and tools for researchers. This paper presents the So2al-wa-Gwab dataset since the publicly available datasets include various faults, such as the use of machine translation to build the data, a short context size, and a small number of question-answer pairings. Thus, this new dataset avoids the aforementioned drawbacks. Furthermore, in this paper, we have trained three deep learning models, namely, Bi-Directional flow network (BiDAF), QA Network (QANet), and BERT model, and tested them on seven different datasets, thus providing a comprehensive comparison between existing Arabic QA datasets. The obtained results emphasize that machine-translated datasets fall back when compared with human-annotated data. Also, the QA task becomes harder as the context, from which to extract the answer, becomes larger.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4381687034",
    "type": "article"
  },
  {
    "title": "SER: Performance Evaluation of CNN Model Along with an Overview of Available Indic Speech Datasets, and Transition of Classifiers From Traditional to Modern Era",
    "doi": "https://doi.org/10.1145/3605778",
    "publication_date": "2023-06-26",
    "publication_year": 2023,
    "authors": "Surbhi Khurana; Amita Dev; Poonam Bansal",
    "corresponding_authors": "",
    "abstract": "Speech emotion recognition (SER) is a rapidly evolving field in affective computing and human-computer interaction. In general, a SER system extracts and classifies prominent elements called features from a pre-processed speech signal to target the presence of speaker's certain emotion. This paper explores the utilization of deep learning classifiers in SER and surveys available datasets in both Indic and international languages. The paper highlights the significance of SER in enhancing human-computer interaction and presents deep learning as an effective approach to handle the complexity of speech signals. Various deep learning architectures, including Convolution Neural Networks (CNNs), Recurrent Neural Network (RNNs), and hybrid models, are analysed in terms of training methodology, and performance on benchmark datasets. Additionally, the paper conducts a comprehensive survey of publicly available datasets for speech emotion recognition, considering emotional categories, language diversity, recording conditions, and sample sizes. Challenges in adapting deep learning models to these datasets, such as data augmentation and cross-lingual transfer learning, are discussed. Moreover, the CNN based model is analysed on accuracy, precision, recall and F-1 score on Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset with the value 84%, 85%, 84% and 84% resp. The review concludes with key findings, emphasizing the strengths and limitations of deep learning classifiers for SER. It identifies the need for standardized evaluation protocols, exploration of transfer learning across languages, and development of robust and culturally diverse datasets as future research directions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4382024628",
    "type": "article"
  },
  {
    "title": "The Contribution of Selected Linguistic Markers for Unsupervised Arabic Verb Sense Disambiguation",
    "doi": "https://doi.org/10.1145/3605777",
    "publication_date": "2023-07-11",
    "publication_year": 2023,
    "authors": "Asma Djaidri; Hassina Aliane; Hamid Azzoune",
    "corresponding_authors": "",
    "abstract": "Word sense disambiguation (WSD) is the task of automatically determining the meaning of a polysemous word in a specific context. Word sense induction is the unsupervised clustering of word usages in a different context to distinguish senses and perform unsupervised WSD. Most studies consider function words as stop words and delete them in the pre-processing step. However, function words can encode meaningful information that can help to improve the performance of WSD approaches. We propose in this work a novel approach to solve Arabic verb sense disambiguation that is based on a preposition-based classification that is used in an automatic word sense induction step to build sense inventories to disambiguate Arabic verbs. However, in the wake of the success of neural language models, recent works obtained encouraging results using BERT pre-trained models for English-language WSD approaches. Hence, we use contextualized word embeddings for an unsupervised Arabic WSD that is based on linguistic markers and uses sentence-BERT Transformer pre-trained models, which yields encouraging results that outperform other existing unsupervised neural AWSD approaches.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4383878805",
    "type": "article"
  },
  {
    "title": "Evolutionary Algorithm with Graph Neural Network Driven Cyberbullying Detection on Low Resource Asian Languages",
    "doi": "https://doi.org/10.1145/3609799",
    "publication_date": "2023-07-18",
    "publication_year": 2023,
    "authors": "Hussein Ali Rasool; Firas Al-Dolaimy; Forat Falih Hasan; Ali Alsalamy; Munqith Saleem; Ahmed Alkhayyat; Moolchand Sharma",
    "corresponding_authors": "",
    "abstract": "ICT is widely adopted by Asian youth and is utilized by people of all ages across the continent. Despite its many advantages, unethical ICT usage can lead to many complications. A harmful application of ICT for social communication and engagement is cyberbullying. Simply adhering to the generally accepted norms and guidelines for cybersecurity will not protect you from cybercrime. Even well-known social media stages like Twitter are safe from this attack. Natural language processing (NLP) research on cyberbullying detection has become popular recently. Even though old-style NLP procedures have become highly cyberbullying, there are still hurdles to overcome. These include the limited character count allowed by social media platforms, an imbalance among comments, ambiguity, and unnecessary use of slang. Models based on (CNNs), Multilayer Perceptrons (MLPs), and (RNNs), have recently shown encouraging results in a variety of NLP tasks. With this motivation, this research develops an African vulture optimization algorithm with a graph neural network-based cyberbullying detection and classification (AVOAGNN-CBDC) model. The proposed AVOAGNN-CBDC technique mainly intends to detect and classify cyberbullying. The AVOAGNN-CBDC technique undergoes data preprocessing in different stages and a FastText-based word embedding process to achieve this. Besides, the AVOAGNN-CBDC technique employs the GNN model for cyberbullying detection and classification. Finally, the AVOA is used for the optimal parameter selection of the GNN model, which helps achieve improved classification performance. The experimental result investigation of the AVOAGNN-CBDC technique is tested on the cyberbullying dataset, and the outcomes highlighted the supremacy of the AVOAGNN-CBDC technique in terms of several measures.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4384697979",
    "type": "article"
  },
  {
    "title": "A Systematic Literature Review on Vietnamese Aspect-based Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3610226",
    "publication_date": "2023-07-19",
    "publication_year": 2023,
    "authors": "Dang Van Thin; Duong Ngoc Hao; Ngan Luu-Thuy Nguyen",
    "corresponding_authors": "",
    "abstract": "Aspect-based sentiment analysis (ABSA) is one of the principal tasks in the automatic deep understanding of texts, widely applied in a broad range of real-world applications. Many studies have been performed on different tasks and datasets for other languages (e.g., English, Chinese) to address this topic. For Vietnamese language, this topic has been attracting considerable interest in recent years. However, we found that many studies tend to repeat the research instead of inheriting and extending the previous works. Moreover, previous studies’ methods of comparison or evaluation metrics have not shown consistency and connection. This might restrict the development of future studies on this research topic. To the best of our knowledge, no research has been conducted to overview the existing studies for the ABSA research in Vietnamese language. The primary objective of this study is to provide a systematic and comprehensive review of the current Vietnamese ABSA research. More specifically, we analyze the early approaches, evaluation metrics, and available published benchmark datasets used in the Vietnamese ABSA task. We also discuss the challenge and recommend potential future directions for Vietnamese ABSA. This work is expected to provide readers with a wealth of knowledge, the research gap, and the challenges in the Vietnamese ABSA field.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4384828153",
    "type": "article"
  },
  {
    "title": "DZ-SMS: An Authentic Corpus of Algerian SMS",
    "doi": "https://doi.org/10.1145/3610522",
    "publication_date": "2023-07-27",
    "publication_year": 2023,
    "authors": "Brahim Dahou; Leila Falek; Mourad Abbas; Slimane Mekaoui; Mohamed Lichouri; Aicha Zitouni",
    "corresponding_authors": "",
    "abstract": "In this article, a complete methodology of a corpus realization of authentic Short Message Service (SMS) from Algerian dialect and which are transcribed in Latin characters or symbols is presented. A linguistic material constituted by 6,000 SMS coming from the different geographical regions of Algeria (Middle, East, and West) corresponding to 42 administrative and geographical departments, have been collected. The coexistence of several dialects through these three regions simultaneously has obliged us to consider and operate a classification of the data for each dialect. This data classification has yielded three extracted regional dialectic corpora, each of them covering a specific number of administrative departments. These treatments are based on the so-called Data-n-gram tokenization targeting the suppression of the stop words, the stemming and the imbalance of the classes linked to the nature of the SMS. Consequently, three text classifiers based on three linear classifiers, namely, Stochastic Gradient Descent (SGD), The Ridge Regression (RDG), and Linear Support Vector Machines, to find out the number of significant corpora to extract from the collected data. A deep analysis of the results has shown that the 5-grams data representation is more representative whereas the stop-words removal and stemming process has generated an information loss that has subsequently inferred an alteration of the recognition rate of about 2%. The emerging problem of classes imbalance has been treated by using three techniques: Random Oversampling, Synthetic Minorities Oversampling Technique (SMOTE), and Adaptive Synthetic (ADASYN). This treatment produced interesting results and enhancements; particularly, the classification by region with the oversampling process SMOTE by using the RDG technique has reached a better percentage of 55.93% whereas the classification by department with the oversampling process ADASYN associated with the SGD has only yielded a maximum score of about 17.11%. The results, which undoubtedly are in favor of the classification by region, have compelled us to create three Subdialectal regional corpora, each, covering a certain number of Algerian departments.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4385363235",
    "type": "article"
  },
  {
    "title": "Comprehending the Gossips: Meme Explanation in Time-Sync Video Comment via Multimodal Cues",
    "doi": "https://doi.org/10.1145/3612920",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Zheyong Xie; Weidong He; Tong Xu; Shiwei Wu; Chen Zhu; Ping Yang; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed the booming of online social media platforms with embracing the popular service called “Time-Sync Comment”, which supports the viewers to share their time-sync opinions along with video content. In this way, we observe that numerous semantically-altered terms, or “Memes”, were created by niche users to express their unique ideas and emotions, and further attracted a large group of viewers with better activity and enthusiasm. Unfortunately, since the memes were created based on domain-specific knowledge and semantically varied depending on the multimodal context in videos, newcomers may fail to comprehend the semantic connotation of memes, which may severely impair their user-experiences. To deal with this issue, in this article, we propose a novel meme explanation framework, called ProMDE, to automatically capture and comprehend the memes in time-sync comments, which could further benefit the viewers with meme explanation service. Specifically, we first iteratively reconstruct the original time-sync comments compared with visual embedding to detect the semantically-altered terms as meme candidates. Afterward, based on the guides from the domain-specific corpus, visual and textual features will be fused to represent the context-aware multimodal cues. Moreover, to accurately describe the commonly-seen homophones in memes, i.e., they have the same pronunciation but different word-spelling expressions, we integrate the phonetic symbols as an additional modality to enhance the framework. Finally, we utilize a Transformer-based decoder to generate the natural language explanation for captured memes. Extensive experiments on a large real-world dataset prove that our framework could significantly outperform several state-of-the-art baseline methods, demonstrating the efficacy of modeling multimodal context and pronunciation for meme detection and explanation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4385497271",
    "type": "article"
  },
  {
    "title": "Improving Sequence-to-sequence Tibetan Speech Synthesis with Prosodic Information",
    "doi": "https://doi.org/10.1145/3616012",
    "publication_date": "2023-08-14",
    "publication_year": 2023,
    "authors": "Weizhao Zhang; Hongwu Yang",
    "corresponding_authors": "",
    "abstract": "There are about 6,000 languages worldwide, most of which are low-resource languages. Although the current speech synthesis (or text-to-speech, TTS) for major languages (e.g., Mandarin, English, French) has achieved good results, the voice quality of TTS for low-resource languages (e.g., Tibetan) still needs to be further improved. Because prosody plays a significant role in natural speech, the article proposes two sequence-to-sequence (seq2seq) Tibetan TTS models with prosodic information fusion to improve the voice quality of synthesized Tibetan speech. We first constructed a large-scale Tibetan corpus for seq2seq TTS. Then we designed a prosody generator to extract prosodic information from the Tibetan sentences. Finally, we trained two seq2seq Tibetan TTS models by fusing prosodic information, including feature-level and model-level prosodic information fusion. The experimental results showed that the proposed two seq2seq Tibetan TTS models, which fuse prosodic information, could effectively improve the voice quality of synthesized speech. Furthermore, the model-level prosodic information fusion only needs 60% ~ 70% of the training data to synthesize a voice similar to the baseline seq2seq Tibetan TTS. Therefore, the proposed prosodic information fusion methods can improve the voice quality of synthesized speech for low-resource languages.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4385809948",
    "type": "article"
  },
  {
    "title": "Methods of Improving Japanese-Chinese Machine Translation System through Machine Learning and Human-Computer Interaction",
    "doi": "https://doi.org/10.1145/3610774",
    "publication_date": "2023-08-19",
    "publication_year": 2023,
    "authors": "Yuwei Ma; Qian Yu",
    "corresponding_authors": "",
    "abstract": "With globalization, the exchange of people between different countries is becoming more frequent. Due to different languages, there are serious obstacles to personnel exchanges. To a large extent, they hinder the growth of industries such as economy, culture and tourism in each country. The emergence of Machine Translation (MT) has effectively improved the problem of language barriers, and greatly reduced the workload of translators in text translation. However, MT does not have the same flexible flexibility as human translation. It just translates the text word by word, which is often difficult to meet people's higher needs. This paper proposed to build a Japanese-Chinese MT system and integrate machine learning and Human-Computer Interaction (HCI) technology into the system. To further enhance the efficiency of the system, enhancement algorithms were also applied to the system to optimize the performance of the system. From the experimental results, in terms of BLEU (Bilingual Evaluation Understudy) index, the average BLEU index of the algorithm in this paper was 8.59, and that of the traditional algorithm was 6.55. In terms of translation precision, the average precision of the algorithm in this paper was 91.53%, while that of the traditional algorithm was 87.28%. In terms of translation readability, the average readability of the algorithm in this paper was 93.32%, while that of the traditional algorithm was 89.22%. By comparison, the average BLEU index of the algorithm in this paper has increased by 2.04; the average accuracy of translation increased by 4.25%; the average readability increased by 4.1%. From the above data, it was evident that the enhancement algorithm can optimize the performance of the Japanese-Chinese MT system well.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386001667",
    "type": "article"
  },
  {
    "title": "Part-of-speech Tagger for Assamese Using Ensembling Approach",
    "doi": "https://doi.org/10.1145/3617653",
    "publication_date": "2023-09-01",
    "publication_year": 2023,
    "authors": "Dhrubajyoti Pathak; Sukumar Nandi; Priyankoo Sarmah",
    "corresponding_authors": "",
    "abstract": "Ensemble system for part-of-speech (POS) tagging is beneficial for many resource-poor languages that do not have enough annotated training data to train Deep Learning (DL, also named Deep Neural Network)-based POS taggers. An Ensemble system is a better choice to incorporate the linguistic features of a language and leverage the benefits of various types of POS taggers. In this work, we present our experiment of developing an ensemble tagger for Assamese, a low-resource, morphologically rich scheduled language of India, spoken by more than 15 million people. Despite the success of modern neural-network-based models in sequence tagging tasks, it has yet to receive attention in developing tasks such as POS in a resource-poor language such as Assamese. We develop a POS tagging model based on the BiLSTM-CRF architecture with a corpus of 404k tokens. We cover several word embeddings during training. Among all the experiments, the top two POS tagging models achieve tagging F1 scores of 0.746 and 0.745. We observe that the DL-based taggers are not able to achieve decent accuracy. It may be due to the inability to capture the linguistic features of the language or due to comparatively less annotated data. So, we build another POS tagger using a rule-based approach considering several morphological phenomena of the language and get an F1 score of 0.85. Subsequently, we integrate the top two DL-based taggers with the rule-based ones and develop a new POS tagger using an ensemble approach, of which we get an improved F1 score of 0.925. Performance improvement of our new ensemble POS taggers over the baseline taggers suggests that integration of the taggers combines the qualities of all taggers in the new tagger. Therefore, this study also states ensemble taggers are more suitable for highly inflectional, morphologically rich resource-poor languages.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386378676",
    "type": "article"
  },
  {
    "title": "MHG-ERC: Multi-hypergraph Feature Aggregation Network for Emotion Recognition in Conversations",
    "doi": "https://doi.org/10.1145/3622935",
    "publication_date": "2023-09-06",
    "publication_year": 2023,
    "authors": "Zheng Cheng; Haojie Xu; Xiao Sun",
    "corresponding_authors": "",
    "abstract": "The modeling of conversational context is an essential step in Emotion Recognition in Conversations (ERC). To maintain high performance and a low GPU memory consumption, this article proposes a new idea of using multiple hypergraphs to model the conversational context and designs a multi-hypergraph feature aggregation network for ERC. We use context window, speaker information, position information between utterances, and specific step size to construct different hyperedges. Then, various hypergraphs generated by different hyperedges are used to aggregate local and remote context information in turn. Experiments on two dialogue emotion datasets, IEMOCAP and MELD, demonstrate the effectiveness and superiority of this new model. In addition, our model requires only relatively low GPU memory consumption.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386482871",
    "type": "article"
  },
  {
    "title": "Research on the Application of Translation Parallel Corpus in Interpretation Teaching",
    "doi": "https://doi.org/10.1145/3623270",
    "publication_date": "2023-09-29",
    "publication_year": 2023,
    "authors": "Yan Gong; Li Cheng",
    "corresponding_authors": "",
    "abstract": "Large and organized sets of translated texts between languages are called parallel translation corpora (PTLs). Even though data-driven learning can generate insights from massive datasets and create more tailored learning experiences, it has gained in popularity. There are however a few problems with this strategy, such as poor data quality, privacy concerns, inability to scale, a lack of clear explanations, and high costs. To provide high-quality output, machine translation algorithms are generally trained utilizing parallel corpora generated by human translators. The written word can be deciphered from one language to another through translation. The spoken word can be conveyed from one language to another through translation. Based on the study of real samples, machine translation from corpus linguistics uses its translations to create its translations. Statistical approaches are only one of the many ways a corpus may be used. Translated texts from two or more languages are called parallel corpora. With the emergence of data-driven learning (DDL) in translation training and language instruction, they are becoming increasingly popular in translation and contrastive research. While working as a professional translator, you're likely to encounter a wide range of challenges. These are lexical-semantic, grammar, syntactic, rhetorical, practical, and cultural difficulties. There are limitations to the number of possible translations that dictionaries can provide and difficulty in doing a thorough search. When it comes to translating, contemporary technologies bring up a whole new world of possibilities thanks to the sheer volume of data and the speed at which it is available. Students of translation can benefit from this study's innovative way of employing PTL-DDL, which can help them improve the quality and speed of their translations. In addition, the parallel corpora's sample sentences are readily available, making it easier to choose the best translations from a pool of translation candidates. Because of these characteristics, the approach is well-suited to creating active or encoding dictionaries.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387188013",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis of Turkish Drug Reviews with Bidirectional Encoder Representations from Transformers",
    "doi": "https://doi.org/10.1145/3626523",
    "publication_date": "2023-10-09",
    "publication_year": 2023,
    "authors": "Mehmet Bozuyla",
    "corresponding_authors": "Mehmet Bozuyla",
    "abstract": "Sentiment analysis of user generated product or service reviews is significant to enhance quality. Healthcare related computational linguistics studies, particularly analysis of drug based user criticisms, have principal importance above all. Sentiment analysis of healthcare reviews reveal the relations between patients, doctors and healthcare services. More specifically, sentiment analysis of drug reviews may be used to acquire relations such as adverse drug reactions (ADRs), diagnosis-treatment assist, and personalized therapy recommendations. Most of the drug review sentiment studies are in English. Though Turkish is a widely spoken language, there is limited research conducted on medical domain and there is particularly no study related to drug review sentiment analysis. In this study, we generated a Turkish drug review dataset and we evaluated the generated dataset in detail against (i) traditional machine learning algorithms with language pre-processing steps, stemming and feature selection, (ii) deep learning algorithms with word2vec embedding language model, and (iii) various bidirectional encoder representations from transformers (BERT) models in terms of sentiment analysis. The experiments show that neural transformers are promising in Turkish drug review sentiment identification. In particular, Turkish dedicated BERT (BERTurk) resulted in 95.1% weighted-F1 score as the best drug review sentiment prediction performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387460638",
    "type": "article"
  },
  {
    "title": "Automatic graph construction and Exploring different types of LSTMs for Asian Hindi languages for Medical review Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3626196",
    "publication_date": "2023-10-11",
    "publication_year": 2023,
    "authors": "Rani kumari; Sunil Kumar; Korhan Cengiz; Nikola Ivković; Prasanalakshmi Balaji",
    "corresponding_authors": "",
    "abstract": "Sentiment Analysis (SA) of medical reviews is crucial for improving healthcare outcomes. However, analyzing sentiment in low-resource languages such as Asian Hindi presents significant challenges. In this study, we propose an automatic graph construction approach to extract relevant features from medical reviews in Asian Hindi languages. We explore different types of Long Short-Term Memory (LSTMs), including traditional LSTMs, bidirectional LSTMs, and attention-based LSTMs, to classify the sentiment of medical reviews. Our proposed approach uses attention-based LSTM architecture and pre-trained Word2Vec embeddings to achieve high accuracy. We compare the proposed approach with existing models using various evaluation metrics, including accuracy, precision, recall, and F1-score. The results demonstrate that our proposed approach outperforms all existing models in terms of accuracy, achieving an accuracy score of 81%. These findings could have implications for improving healthcare outcomes by enabling better monitoring of patient feedback and identifying areas for improvement in medical services.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387540784",
    "type": "article"
  },
  {
    "title": "English Listening Teaching Mode under Artificial Intelligence Speech Synthesis Technology",
    "doi": "https://doi.org/10.1145/3615866",
    "publication_date": "2023-10-30",
    "publication_year": 2023,
    "authors": "Cui Yu; Lili Wu; J. Li; Shuang Li",
    "corresponding_authors": "",
    "abstract": "To discuss the application of artificial intelligence (AI) speech synthesis technology and wireless network technology in English listening teaching, firstly, the present situation of English listening teaching is analyzed. Secondly, a questionnaire is designed to investigate the feasibility of AI speech synthesis technology in English listening teaching. Finally, the control group (a traditional mode of English listening teaching) and experimental group (AI speech synthesis technology) are set up, compared, and analyzed. The survey shows that for natural fluency, speech synthesis has the lowest score. The textbook audio score is the highest, with an average score of 4.23. According to the mean opinion score (MOS), the AI speech synthesis technology belongs to high-quality speech coding. Therefore, the English audio generated by combining AI speech synthesis technology under wireless network technology also has high quality. Meanwhile, 113 people in the survey consider that the clear articulation of the synthesized audio is full marks, which is clearer than the English audio matching the textbook, indicating that students think the English audio of the supporting textbook is poor in the clear articulation, which affects students’ listening. Moreover, the English listening scores of the experimental group are notably better than those of the control group. It is hoped that the traditional teaching mode can be better integrated with wireless networks and AI technology, thereby improving the clarity of English listening, providing mobile learning, and advancing students' English performance and learning efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388097497",
    "type": "article"
  },
  {
    "title": "Dependency-based BERT for Chinese Event Argument Extraction",
    "doi": "https://doi.org/10.1145/3633306",
    "publication_date": "2023-11-21",
    "publication_year": 2023,
    "authors": "Daiyi Li; Li Yan; Zongmin Ma",
    "corresponding_authors": "",
    "abstract": "Existing event extraction methods independently identify and classify each argument role separately, ignoring the interdependence between different parameter roles. Further, these methods rely on simple vectors to represent word embeddings. By embedding explicit syntactic constraints in the attention mechanism, we address these shortcomings by using dependency syntax to guide the text modeling. Specifically, we use dependency syntax to guide the BERT model for Chinese event argument role extraction, which mainly consists of three stages. First, the self-attention method guided by a dependency syntactic parsing tree is embedded in the Transformer computing framework of the BERT model. In addition to obtaining a deep two-way linguistic representation of a word according to its context information, this method also expresses the long-distance syntactic dependency relationships between words based on context information. Secondly, the designed conditional layer normalization method is applied to the event argument extraction model in order to integrate the semantic information of trigger words into the text, thereby improving the accuracy of the argument role extraction. Lastly, conditional random fields (CRFs) are used to determine the optimal sequence of labels at the sentence level based on the dependence relationships between adjacent labels. According to the experimental results, the constructed model outperforms several strong baselines in the Chinese event argument extraction task on the ACE2005 dataset and the iFLYTEKA.I.2020 dataset.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388863148",
    "type": "article"
  },
  {
    "title": "A Machine Learning–Based Readability Model for Gujarati Texts",
    "doi": "https://doi.org/10.1145/3637826",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Chandrakant Bhogayata",
    "corresponding_authors": "Chandrakant Bhogayata",
    "abstract": "This study aims to develop a machine learning–based model to predict the readability of Gujarati texts. The dataset was 50 prose passages from Gujarati literature. Fourteen lexical and syntactic readability text features were extracted from the dataset using a machine learning algorithm of the unigram parts of speech tagger and three Python programming scripts. Two samples of native Gujarati speaking secondary and higher education students rated the Gujarati texts for readability judgment on a 10-point scale of “easy” to “difficult” with the interrater agreement. After dimensionality reduction, seven text features as the independent variables and the mean readability rating as the dependent variable were used to train the readability model. As the students' level of education and gender were related to their readability rating, four readability models for school students, university students, male students, and female students were trained with a backward stepwise multiple linear regression algorithm of supervised machine learning. The trained model is comparable across the raters’ groups. The best model is the university students’ readability rating model. The model is cross-validated. It explains 91% and 88% of the variance in readability ratings at training and cross-validation, respectively, and its effect size and power are large and high.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390051442",
    "type": "article"
  },
  {
    "title": "Leveraging Hierarchical Deep Semantics to Classify Implicit Discourse Relations via a Mutual Learning Method",
    "doi": "https://doi.org/10.1145/3178456",
    "publication_date": "2018-02-13",
    "publication_year": 2018,
    "authors": "Xiaohan She; Ping Jian; Pengcheng Zhang; Heyan Huang",
    "corresponding_authors": "",
    "abstract": "This article presents a mutual learning method using hierarchical deep semantics for the classification of implicit discourse relations in English. With the absence of explicit discourse markers, traditional discourse techniques mainly concentrate on discrete linguistic features in this task, which always leads to a data sparseness problem. To relieve this problem, we propose a mutual learning neural model that makes use of multilevel semantic information together, including the distribution of implicit discourse relations, the semantics of arguments, and the co-occurrence of phrases and words. During the training process, the predicting targets of the model, which are the probability of the discourse relation type and the distributed representation of semantic components, are learned jointly and optimized mutually. The experimental results show that this method outperforms the previous works, especially in multiclass identification attributed to the hierarchical semantic representations and the mutual learning strategy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2557473920",
    "type": "article"
  },
  {
    "title": "Word Segmentation for Burmese Based on Dual-Layer CRFs",
    "doi": "https://doi.org/10.1145/3232537",
    "publication_date": "2018-11-12",
    "publication_year": 2018,
    "authors": "Shaoning Zhang; Cunli Mao; Zhengtao Yu; Hongbin Wang; Zhongwei Li; Jiafu Zhang",
    "corresponding_authors": "",
    "abstract": "Burmese is an isolated language, in which the syllable is the smallest unit. Syllable segmentation methods based on matching lead to performance subject to the syllable segmentation effect. This article proposes a word segmentation method with fusion conditions of double syllable features. It combines word segmentation and segmentation of syllables into one process, thus reducing the impact of errors on the syllable segmentation of Burmese. In the first layer of the conditional random fields (CRF) model, Burmese characters as atomic features are integrated into the Burma section of the Barkis Speech Paradigm (Backus normal form) features to realize the Burma syllable sequence tags. In the second layer of the CRFs model, with the syllable marked as input, it realizes the sequence markers through building a feature template with syllables as atomic features. The experimental results show that the proposed method has a better effect compared with the method based on the matching of syllables.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2900613036",
    "type": "article"
  },
  {
    "title": "Optimizing Automatic Evaluation of Machine Translation with the ListMLE Approach",
    "doi": "https://doi.org/10.1145/3226045",
    "publication_date": "2018-11-12",
    "publication_year": 2018,
    "authors": "Maoxi Li; Mingwen Wang",
    "corresponding_authors": "",
    "abstract": "Automatic evaluation of machine translation is critical for the evaluation and development of machine translation systems. In this study, we propose a new model for automatic evaluation of machine translation. The proposed model combines standard n-gram precision features and sentence semantic mapping features with neural features, including neural language model probabilities and the embedding distances between translation outputs and their reference translations. We optimize the model with a representative list-wise learning to rank approach, ListMLE, in terms of human ranking assessments. The experimental results on WMT’2015 Metrics task indicated that the proposed approach yields significantly better correlations with human assessments than several state-of-the-art baseline approaches. In particular, the results confirmed that the proposed list-wise learning to rank approach is useful and powerful for optimizing automatic evaluation metrics in terms of human ranking assessments. Deep analysis also demonstrated that optimizing automatic metrics with the ListMLE approach is a reasonable method and adding the neural features can gain considerable improvements compared with the traditional features.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2900955675",
    "type": "article"
  },
  {
    "title": "Adversarial Evaluation of Robust Neural Sequential Tagging Methods for Thai Language",
    "doi": "https://doi.org/10.1145/3383201",
    "publication_date": "2020-05-18",
    "publication_year": 2020,
    "authors": "Can Udomcharoenchaikit; Prachya Boonkwan; Peerapon Vateekul",
    "corresponding_authors": "",
    "abstract": "Sequential tagging tasks, such as Part-Of-Speech (POS) tagging and Named-Entity Recognition, are the building blocks of many natural language processing applications. Although prior works have reported promising results in standard settings, they often underperform on non-standard text, such as microblogs and social media. In this article, we introduce an adversarial evaluation scheme for the Thai language by creating adversarial examples based on known spelling errors. Furthermore, we propose novel methods including UNK masking, condition initialization with affixation embeddings, and untied-directional self-attention mechanism to enhance robustness and interpretability of the neural networks. We conducted experiments on two Thai corpora: BEST2010 and ORCHID. Our adversarial evaluation schemes reveal that bidirectional LSTM (BiLSTM) do not perform well on adversarial examples. Our best methods match the performance of the BiLSTM baseline model and outperform it on adversarial examples.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3034636606",
    "type": "article"
  },
  {
    "title": "Neural Co-training for Sentiment Classification with Product Attributes",
    "doi": "https://doi.org/10.1145/3394113",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ruirui Bai; Zhongqing Wang; Fang Kong; Shoushan Li; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Sentiment classification aims to detect polarity from a piece of text. The polarity is usually positive or negative, and the text genre is usually product review. The challenges of sentiment classification are that it is hard to capture semantic of reviews, and the labeled data is hard to annotate. Therefore, we propose neural co-training to learn the semantic representation of each review using the neural network model, and learn the information from unlabeled data using a co-training framework. In particular, we use the attention-based bi-directional Gated Recurrent Unit (Att-BiGRU) to model the semantic content of each review and regard different categories of the target product as different views. We then use a co-training framework to learn and predict the unlabeled reviews with different views. Experiment results with the Yelp dataset demonstrate the effectiveness of our approach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3041173970",
    "type": "article"
  },
  {
    "title": "Semi-supervised Thai Sentence Segmentation Using Local and Distant Word Representations",
    "doi": "https://doi.org/10.1145/3389037",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "Chanatip Saetia; Tawunrat Chalothorn; Ekapol Chuangsuwanich; Peerapon Vateekul",
    "corresponding_authors": "",
    "abstract": "A sentence is typically treated as the minimal syntactic unit used for extracting valuable information from a longer piece of text. However, in written Thai, there are no explicit sentence markers. We proposed a deep learning model for the task of sentence segmentation that includes three main contributions. First, we integrate n-gram embedding as a local representation to capture word groups near sentence boundaries. Second, to focus on the keywords of dependent clauses, we combine the model with a distant representation obtained from self-attention modules. Finally, due to the scarcity of labeled data, for which annotation is difficult and time-consuming, we also investigate and adapt Cross-View Training (CVT) as a semi-supervised learning technique, allowing us to utilize unlabeled data to improve the model representations. In the Thai sentence segmentation experiments, our model reduced the relative error by 7.4% and 10.5% compared with the baseline models on the Orchid and UGWC datasets, respectively. We also applied our model to the task of pronunciation recovery on the IWSLT English dataset. Our model outperformed the prior sequence tagging models, achieving a relative error reduction of 2.5%. Ablation studies revealed that utilizing n-gram presentations was the main contributing factor for Thai, while the semi-supervised training helped the most for English.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3041843014",
    "type": "article"
  },
  {
    "title": "Detecting Entities of Works for Chinese Chatbot",
    "doi": "https://doi.org/10.1145/3414901",
    "publication_date": "2020-09-27",
    "publication_year": 2020,
    "authors": "Chuhan Wu; Fangzhao Wu; Tao Qi; Junxin Liu; Yongfeng Huang; Xing Xie",
    "corresponding_authors": "",
    "abstract": "Chatbots such as Xiaoice have gained huge popularity in recent years. Users frequently mention their favorite works such as songs and movies in conversations with chatbots. Detecting these entities can help design better chat strategies and improve user experience. Existing named entity recognition methods are mainly designed for formal texts, and their performance on the informal chatbot conversation texts may not be optimal. In addition, these methods rely on massive manually annotated data for model training. In this article, we propose a neural approach to detect entities of works for Chinese chatbot. Our approach is based on a language model (LM) long-short term memory (LSTM) convolutional neural network (CNN) conditional random value (CRF), or LM-LSTM-CNN-CRF, framework, which contains a language model to generate context-aware character embeddings, a Bi-LSTM network to learn contextual character representations from global contexts, a CNN to learn character representations from local contexts, and a CRF layer to jointly decode the character label sequence. In addition, we propose an automatic text annotation method via quote marks to reduce the effort of manual annotation. Besides, we propose an iterative data purification method to improve the quality of the automatically constructed labeled data. Massive experiments on a real-world dataset validate that our approach can achieve good performance on entity detection for Chinese chatbots.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3088120390",
    "type": "article"
  },
  {
    "title": "TransBERT",
    "doi": "https://doi.org/10.1145/3427669",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Zhongyang Li; Xiao Ding; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Recent advances, such as GPT, BERT, and RoBERTa, have shown success in incorporating a pre-trained transformer language model and fine-tuning operations to improve downstream NLP systems. However, this framework still has some fundamental problems in effectively incorporating supervised knowledge from other related tasks. In this study, we investigate a transferable BERT (TransBERT) training framework, which can transfer not only general language knowledge from large-scale unlabeled data but also specific kinds of knowledge from various semantically related supervised tasks, for a target task. Particularly, we propose utilizing three kinds of transfer tasks, including natural language inference, sentiment classification, and next action prediction, to further train BERT based on a pre-trained model. This enables the model to get a better initialization for the target task. We take story-ending prediction as the target task to conduct experiments. The final results of 96.0% and 95.0% accuracy on two versions of Story Cloze Test datasets dramatically outperform previous state-of-the-art baseline methods. Several comparative experiments give some helpful suggestions on how to select transfer tasks to improve BERT. Furthermore, experiments on six English and three Chinese datasets show that TransBERT generalizes well to other tasks, languages, and pre-trained models.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3133892054",
    "type": "article"
  },
  {
    "title": "Robust Cross-lingual Task-oriented Dialogue",
    "doi": "https://doi.org/10.1145/3457571",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Lu Xiang; Junnan Zhu; Yang Zhao; Yu Zhou; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Cross-lingual dialogue systems are increasingly important in e-commerce and customer service due to the rapid progress of globalization. In real-world system deployment, machine translation (MT) services are often used before and after the dialogue system to bridge different languages. However, noises and errors introduced in the MT process will result in the dialogue system's low robustness, making the system's performance far from satisfactory. In this article, we propose a novel MT-oriented noise enhanced framework that exploits multi-granularity MT noises and injects such noises into the dialogue system to improve the dialogue system's robustness. Specifically, we first design a method to automatically construct multi-granularity MT-oriented noises and multi-granularity adversarial examples, which contain abundant noise knowledge oriented to MT. Then, we propose two strategies to incorporate the noise knowledge: (i) Utterance-level adversarial learning and (ii) Knowledge-level guided method. The former adopts adversarial learning to learn a perturbation-invariant encoder, guiding the dialogue system to learn noise-independent hidden representations. The latter explicitly incorporates the multi-granularity noises, which contain the noise tokens and their possible correct forms, into the training and inference process, thus improving the dialogue system's robustness. Experimental results on three dialogue models, two dialogue datasets, and two language pairs have shown that the proposed framework significantly improves the performance of the cross-lingual dialogue system.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3193807484",
    "type": "article"
  },
  {
    "title": "Neural Unsupervised Semantic Role Labeling",
    "doi": "https://doi.org/10.1145/3461613",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Kashif Munir; Hai Zhao; Zuchao Li",
    "corresponding_authors": "",
    "abstract": "The task of semantic role labeling ( SRL ) is dedicated to finding the predicate-argument structure. Previous works on SRL are mostly supervised and do not consider the difficulty in labeling each example which can be very expensive and time-consuming. In this article, we present the first neural unsupervised model for SRL. To decompose the task as two argument related subtasks, identification and clustering, we propose a pipeline that correspondingly consists of two neural modules. First, we train a neural model on two syntax-aware statistically developed rules. The neural model gets the relevance signal for each token in a sentence, to feed into a BiLSTM, and then an adversarial layer for noise-adding and classifying simultaneously, thus enabling the model to learn the semantic structure of a sentence. Then we propose another neural model for argument role clustering, which is done through clustering the learned argument embeddings biased toward their dependency relations. Experiments on the CoNLL-2009 English dataset demonstrate that our model outperforms the previous state-of-the-art baseline in terms of non-neural models for argument identification and classification.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3194355832",
    "type": "article"
  },
  {
    "title": "Low-Resource Language Discrimination toward Chinese Dialects with Transfer Learning and Data Augmentation",
    "doi": "https://doi.org/10.1145/3473499",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Fan Xu; Yangjie Dan; Keyu Yan; Yong Ma; Mingwen Wang",
    "corresponding_authors": "",
    "abstract": "Chinese dialects discrimination is a challenging natural language processing task due to scarce annotation resource. In this article, we develop a novel Chinese dialects discrimination framework with transfer learning and data augmentation (CDDTLDA) in order to overcome the shortage of resources. To be more specific, we first use a relatively larger Chinese dialects corpus to train a source-side automatic speech recognition (ASR) model. Then, we adopt a simple but effective data augmentation method (i.e., speed, pitch, and noise disturbance) to augment the target-side low-resource Chinese dialects, and fine-tune another target ASR model based on the previous source-side ASR model. Meanwhile, the potential common semantic features between source-side and target-side ASR models can be captured by using self-attention mechanism. Finally, we extract the hidden semantic representation in the target ASR model to conduct Chinese dialects discrimination. Our extensive experimental results demonstrate that our model significantly outperforms state-of-the-art methods on two benchmark Chinese dialects corpora.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3208277898",
    "type": "article"
  },
  {
    "title": "Enriching Conventional Ensemble Learner with Deep Contextual Semantics to Detect Fake News in Urdu",
    "doi": "https://doi.org/10.1145/3461614",
    "publication_date": "2021-11-10",
    "publication_year": 2021,
    "authors": "Ramsha Saeed; Hammad Afzal; Haider Abbas; Maheen Fatima",
    "corresponding_authors": "",
    "abstract": "Increased connectivity has contributed greatly in facilitating rapid access to information and reliable communication. However, the uncontrolled information dissemination has also resulted in the spread of fake news. Fake news might be spread by a group of people or organizations to serve ulterior motives such as political or financial gains or to damage a country’s public image. Given the importance of timely detection of fake news, the research area has intrigued researchers from all over the world. Most of the work for detecting fake news focuses on the English language. However, automated detection of fake news is important irrespective of the language used for spreading false information. Recognizing the importance of boosting research on fake news detection for low resource languages, this work proposes a novel semantically enriched technique to effectively detect fake news in Urdu—a low resource language. A model based on deep contextual semantics learned from the convolutional neural network is proposed. The features learned from the convolutional neural network are combined with other n-gram-based features and are fed to a conventional majority voting ensemble classifier fitted with three base learners: Adaptive Boosting, Gradient Boosting, and Multi-Layer Perceptron. Experiments are performed with different models, and results show that enriching the traditional ensemble learner with deep contextual semantics along with other standard features shows the best results and outperforms the state-of-the-art Urdu fake news detection model.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3214231390",
    "type": "article"
  },
  {
    "title": "Combining Self-supervised Learning and Active Learning for Disfluency Detection",
    "doi": "https://doi.org/10.1145/3487290",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Shaolei Wang; Zhongyuan Wang; Wanxiang Che; Sendong Zhao; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Spoken language is fundamentally different from the written language in that it contains frequent disfluencies or parts of an utterance that are corrected by the speaker. Disfluency detection (removing these disfluencies) is desirable to clean the input for use in downstream NLP tasks. Most existing approaches to disfluency detection heavily rely on human-annotated data, which is scarce and expensive to obtain in practice. To tackle the training data bottleneck, in this work, we investigate methods for combining self-supervised learning and active learning for disfluency detection. First, we construct large-scale pseudo training data by randomly adding or deleting words from unlabeled data and propose two self-supervised pre-training tasks: (i) a tagging task to detect the added noisy words and (ii) sentence classification to distinguish original sentences from grammatically incorrect sentences. We then combine these two tasks to jointly pre-train a neural network. The pre-trained neural network is then fine-tuned using human-annotated disfluency detection training data. The self-supervised learning method can capture task-special knowledge for disfluency detection and achieve better performance when fine-tuning on a small annotated dataset compared to other supervised methods. However, limited in that the pseudo training data are generated based on simple heuristics and cannot fully cover all the disfluency patterns, there is still a performance gap compared to the supervised models trained on the full training dataset. We further explore how to bridge the performance gap by integrating active learning during the fine-tuning process. Active learning strives to reduce annotation costs by choosing the most critical examples to label and can address the weakness of self-supervised learning with a small annotated dataset. We show that by combining self-supervised learning with active learning, our model is able to match state-of-the-art performance with just about 10% of the original training data on both the commonly used English Switchboard test set and a set of in-house annotated Chinese data.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4200184345",
    "type": "article"
  },
  {
    "title": "Toxic Comment Classification Based on Bidirectional Gated Recurrent Unit and Convolutional Neural Network",
    "doi": "https://doi.org/10.1145/3488366",
    "publication_date": "2021-12-21",
    "publication_year": 2021,
    "authors": "Zhongguo Wang; Zhang Bao",
    "corresponding_authors": "",
    "abstract": "For English toxic comment classification, this paper presents the model that combines Bi-GRU and CNN optimized by global average pooling (BG-GCNN) based on the bidirectional gated recurrent unit (Bi-GRU) and global pooling optimized convolution neural network (CNN) . The model treats each type of toxic comment as a binary classification. First, Bi-GRU is used to extract the time-series features of the comment and then the dimensionality is reduced through global pooling optimized convolution neural network. Finally, the classification result is output by Sigmoid function. Comparative experiments show the BG-GCNN model has a better classification effect than Text-CNN, LSTM, Bi-GRU, and other models. The Macro-F1 value of the toxic comment dataset on the Kaggle competition platform is 0.62. The F1 values of the three toxic label classification results (toxic, obscene, and insult label) are 0.81, 0.84, and 0.74, respectively, which are the highest values in the comparative experiment.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4200192288",
    "type": "article"
  },
  {
    "title": "A Position-Aware Language Modeling Framework for Extractive Broadcast News Speech Summarization",
    "doi": "https://doi.org/10.1145/3099472",
    "publication_date": "2017-08-16",
    "publication_year": 2017,
    "authors": "Shih‐Hung Liu; Kuan-Yu Chen; Yu‐Lun Hsieh; Berlin Chen; Hsin‐Min Wang; Hsu‐Chun Yen; Wen−Lian Hsu",
    "corresponding_authors": "",
    "abstract": "Extractive summarization, a process that automatically picks exemplary sentences from a text (or spoken) document with the goal of concisely conveying key information therein, has seen a surge of attention from scholars and practitioners recently. Using a language modeling (LM) approach for sentence selection has been proven effective for performing unsupervised extractive summarization. However, one of the major difficulties facing the LM approach is to model sentences and estimate their parameters more accurately for each text (or spoken) document. We extend this line of research and make the following contributions in this work. First, we propose a position-aware language modeling framework using various granularities of position-specific information to better estimate the sentence models involved in the summarization process. Second, we explore disparate ways to integrate the positional cues into relevance models through a pseudo-relevance feedback procedure. Third, we extensively evaluate various models originated from our proposed framework and several well-established unsupervised methods. Empirical evaluation conducted on a broadcast news summarization task further demonstrates performance merits of the proposed summarization methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2747573959",
    "type": "article"
  },
  {
    "title": "Automatically Building VoIP Speech Parallel Corpora for Arabic Dialects",
    "doi": "https://doi.org/10.1145/3132708",
    "publication_date": "2017-10-05",
    "publication_year": 2017,
    "authors": "Khalid Almeman",
    "corresponding_authors": "Khalid Almeman",
    "abstract": "This article discusses the process of automatically building Arabic multi-dialect speech corpora using Voice over Internet Protocol (VoIP). The Asterisk framework was adopted to act as the main connection between the parties, for which two virtual machines were created: a sender and a receiver. The sender makes a VoIP call to the receiver using the Asterisk framework, while the receiver records the call automatically, a process that is repeated for all the audio files involved in the corpora. In this work, more than 67,000 automatic calls were made between the sender and receiver machines, generating VoIP Arabic corpora for four Arabic dialects. The resulting corpora can be considered the first Arabic VoIP parallel speech corpora and will be made freely available to researchers in Arabic NLP and speech recognition research.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2761960002",
    "type": "article"
  },
  {
    "title": "Advancing Chinese Event Detection via Revisiting Character Information",
    "doi": "https://doi.org/10.1145/3502197",
    "publication_date": "2022-02-11",
    "publication_year": 2022,
    "authors": "Yanxia Qin; Zhongqing Wang; Yue Zhang; Kehai Chen; Min Zhang",
    "corresponding_authors": "",
    "abstract": "Recently, character information has been successfully introduced into the encoder-decoder event detection model to relieve the trigger-word mismatch problem, thus achieving impressive results in the languages without natural delimiters (i.e., Chinese). However, it is introduced into the encoder or the decoder separately, which makes the advantage of character information not be captured and represented adequately for event detection. In this article, we proposed a novel method to model character information in both the encoding and decoding stages to advance the neural event detection model. In particular, the proposed method can encode both words and characters and predict their event types jointly and further leverage interactions between word and its characters to optimize the inference. Experimental results show that the proposed model outperforms previous event detection methods on the ACE2005 Chinese benchmark. We release our code at Github. 1",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4211233277",
    "type": "article"
  },
  {
    "title": "Critical Analysis of Existing Punjabi Grammar Checker and a Proposed Hybrid Framework Involving Machine Learning and Rule-Base Criteria",
    "doi": "https://doi.org/10.1145/3514237",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "Vikas Verma; Sanjeev Kumar Sharma",
    "corresponding_authors": "",
    "abstract": "An important area of research involving Artificial Intelligence (AI) is Natural Language Processing (NLP). The objective of training a machine is to imitate and manipulate text and speech of humans. Progressive research is undertaken to find connections between humans and their usage of language commonly used being referred as Natural Language. Various tools for different languages have been developed for operating the natural languages widely used by public. NLP integrates various disciplines and works cohesively for processing text, Information Retrieval, AI and so on. One such tool used for checking the accuracy of a given sentence in any language is referred to as a Grammar Checker. So a Grammar checker of a particular language explores grammatical errors (if any) and provides remedial suggestions for correction of the same. Such feature is imbibed by virtue of Natural Language Processing using Computational Linguistics. We have justified the need of an emerging Machine Learning technique by critically evaluating the existing Punjabi Grammar checker that was developed earlier in light of certain real-time cases. This process is accomplished by critically evaluating the output of each phase and identifying the component accountable for generating maximum errors and false alarms. Based on this analysis, we have proposed a hybrid framework as an efficient way of analyzing correction in sentences. This is attainable through the said booming technique of Machine Learning explicitly using Deep Neural Networks in combination with the existing rule-based approach. It's a novel approach as no work using machine learning has been done earlier in Punjabi Grammar Checker.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4220860652",
    "type": "article"
  },
  {
    "title": "Token Relation Aware Chinese Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3531534",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Zeyu Huang; Wenge Rong; Xiaofeng Zhang; Yuanxin Ouyang; Chenghua Lin; Zhang Xiong",
    "corresponding_authors": "",
    "abstract": "Due to the lack of natural delimiters, most Chinese Named Entity Recognition (NER) approaches are character-based and utilize an external lexicon to leverage the word-level information. Although they have achieved promising results, the latent words they introduced are still non-contextualized. In this paper, we investigate three relations, i.e., adjacent relation between characters, character co-occurrence relation between latent words, and dependency relation among tokens, to address this issue. Specifically, we first establish the local context for latent words and then propose a masked self-attention mechanism to incorporate such local contextual information. Besides, since introducing external knowledge such as lexicon and dependency relation inevitably brings in some noises, we propose a gated information controller to handle this problem. Extensive experimental results show that the proposed approach surpasses most similar methods on public datasets and demonstrates its promising potential.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4225127258",
    "type": "article"
  },
  {
    "title": "Target-Oriented Knowledge Distillation with Language-Family-Based Grouping for Multilingual NMT",
    "doi": "https://doi.org/10.1145/3546067",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Heejin Do; Gary Geunbae Lee",
    "corresponding_authors": "",
    "abstract": "Multilingual NMT has developed rapidly, but still has performance degradation caused by language diversity and model capacity constraints. To achieve the competitive accuracy of multilingual translation despite such limitations, knowledge distillation, which improves the student network by matching the teacher network’s output, has been applied and shown enhancement by focusing on the important parts of the teacher distribution. However, existing knowledge distillation methods for multilingual NMT rarely consider the knowledge, which has an important function as the student model’s target, in the process. In this article, we propose two distillation strategies that effectively use the knowledge to improve the accuracy of multilingual NMT. First, we introduce a language-family-based approach, guiding to select appropriate knowledge for each language pair. By distilling the knowledge of multilingual teachers that each processes a group of languages classified by language families, the multilingual model overcomes accuracy degradation caused by linguistic diversity. Second, we propose target-oriented knowledge distillation, which intensively focuses on the ground-truth target of knowledge with a penalty strategy. Our method provides a sensible distillation by penalizing samples without actual targets, while additionally targeting the ground-truth targets. Experiments using TED Talk datasets demonstrate the effectiveness of our method with BLEU scores increment. Discussions of distilled knowledge and further observations of the methods also validate our results.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4283762912",
    "type": "article"
  },
  {
    "title": "A Decision Model for Ranking Asian Higher Education Institutes Using an NLP-Based Text Analysis Approach",
    "doi": "https://doi.org/10.1145/3534562",
    "publication_date": "2022-07-14",
    "publication_year": 2022,
    "authors": "B. Prabadevi; N. Deepa; K. Ganesan; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "Identification of the best institute for higher education has become one of the most challenging issues in the present education system. It has become more complicated as more institutes exist with extraordinary infrastructural facilities. Therefore, a decision model is required to identify the best institute for higher education based on multiple criteria. This article proposes a Natural Language Processing (NLP) -based decision model for the identification of the best higher education institute using MCDM methods. The existing decision models for the selection of the best higher education institutions consider a limited number of criteria for decision-making. In this proposed model, 17 criteria and 15 institute datasets have been identified for the development of the decision model through extensive research and experts opinion. The NLP-based text analysis approach is applied to extract the relevant information and convert it to a suitable format. As the relative importance of the criteria plays a crucial role in decision-making, CRITIC and Rank centroid methods are applied for the calculation of relative weights of criteria. TOPSIS method is used to generate the ranking grades of alternatives for each criterion. An objective function is defined to calculate the evaluation scores and select the best institute for higher education. It has been observed that the ranks obtained from the developed model match pretty well with the ranks obtained from other MCDM methods and the experts.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4285387971",
    "type": "article"
  },
  {
    "title": "Toward Explainable Dialogue System Using Two-stage Response Generation",
    "doi": "https://doi.org/10.1145/3551869",
    "publication_date": "2022-08-18",
    "publication_year": 2022,
    "authors": "Shaobo Li; Chengjie Sun; Zhen Xu; Prayag Tiwari; Bingquan Liu; Deepak Gupta; K. Shankar; Zhenzhou Ji; Mingjiang Wang",
    "corresponding_authors": "",
    "abstract": "In recent years, neural networks have achieved impressive performance on dialogue response generation. However, most of these models still suffer from some shortcomings, such as yielding uninformative responses and lacking explainable ability. This article proposes a Two-stage Dialogue Response Generation model (TSRG), which specifies a method to generate diverse and informative responses based on an interpretable procedure between stages. TSRG involves a two-stage framework that generates a candidate response first and then instantiates it as the final response. The positional information and a resident token are injected into the candidate response to stabilize the multi-stage framework, alleviating the shortcomings in the multi-stage framework. Additionally, TSRG allows adjusting and interpreting the interaction pattern between the two generation stages, making the generation response somewhat explainable and controllable. We evaluate the proposed model on three dialogue datasets that contain millions of single-turn message-response pairs between web users. The results show that, compared with the previous multi-stage dialogue generation models, TSRG can produce more diverse and informative responses and maintain fluency and relevance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293067326",
    "type": "article"
  },
  {
    "title": "Multi-Turn and Multi-Granularity Reader for Document-Level Event Extraction",
    "doi": "https://doi.org/10.1145/3542925",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Hang Yang; Yubo Chen; Kang Liu; Jun Zhao; Zuyu Zhao; Weijian Sun",
    "corresponding_authors": "",
    "abstract": "Most existing event extraction works mainly focus on extracting events from one sentence. However, in real-world applications, arguments of one event may scatter across sentences and multiple events may co-occur in one document. Thus, these scenarios require document-level event extraction (DEE), which aims to extract events and their arguments across sentences from a document. Previous works cast DEE as a two-step paradigm: sentence-level event extraction (SEE) to document-level event fusion. However, this paradigm lacks integrating document-level information for SEE and suffers from the inherent limitations of error propagation. In this article, we propose a multi-turn and multi-granularity reader for DEE that can extract events from the document directly without the stage of preliminary SEE. Specifically, we propose a new paradigm of DEE by formulating it as a machine reading comprehension task (i.e., the extraction of event arguments is transformed to identify the answer span from the document). Beyond the framework of machine reading comprehension, we introduce a multi-turn and multi-granularity reader to capture the dependencies between arguments explicitly and model long texts effectively. The empirical results demonstrate that our method achieves superior performance on the MUC-4 and the ChFinAnn datasets.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293083838",
    "type": "article"
  },
  {
    "title": "Character-based Joint Word Segmentation and Part-of-Speech Tagging for Tibetan Based on Deep Learning",
    "doi": "https://doi.org/10.1145/3511600",
    "publication_date": "2022-08-31",
    "publication_year": 2022,
    "authors": "Li Yan; Xiaomin Li; Yiru Wang; Hui Lv; Fenfang Li; La Duo",
    "corresponding_authors": "",
    "abstract": "Tibetan word segmentation and POS tagging are the primary tasks of Tibetan natural language processing. Most of existing methods of Tibetan word segmentation and POS tagging are based on rules and statistics, which need manual construction of features. In addition, the joint mode has shown stronger capabilities for word segmentation and POS tagging and have received great interests. In this paper, we propose Bi-LSTM+IDCNN+CRF structures, a simple yet effective end-to-end neural network model, for joint Tibetan word segmentation and POS tagging. We conduct step-by-step and joint experiments on the Tibetan datasets. The results demonstrate that the performance of the Bi-LSTM+IDCNN+CRF model is the best regardless of the step-by-step or joint mode. We obtain state-of-the-art performance in the joint tagging mode. The F1 score of the word segmentation task reached 92.31%, and the F1 score of the POS tagging task reached 81.26%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293762739",
    "type": "article"
  },
  {
    "title": "Meta-ED: Cross-lingual Event Detection Using Meta-learning for Indian Languages",
    "doi": "https://doi.org/10.1145/3555340",
    "publication_date": "2022-08-09",
    "publication_year": 2022,
    "authors": "Aniruddha Roy; Isha Sharma; Sudeshna Sarkar; Pawan Goyal",
    "corresponding_authors": "",
    "abstract": "Lack of annotated data is a major concern in Event Detection (ED) tasks for low-resource languages. Cross-lingual ED seeks to address this issue by transferring information across various languages to improve overall performance. In this article, we propose a method for cross-lingual ED with a few training instances. We present a model agnostic meta-learning approach for few-shot cross-lingual ED that is able to find good parameter initialization and enables fast adaptation to new low-resource languages. We evaluate our model on four Indian languages. The results show that our approach significantly outperforms the base model.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4297816471",
    "type": "article"
  },
  {
    "title": "Context-aware Urdu Information Retrieval System",
    "doi": "https://doi.org/10.1145/3502854",
    "publication_date": "2022-10-14",
    "publication_year": 2022,
    "authors": "Umar Shoaib; Laiba Fiaz; Chinmay Chakraborty; Hafiz Tayyab Rauf",
    "corresponding_authors": "",
    "abstract": "World Wide Web (WWW) is playing a vital role for sharing dynamic knowledge in every field of life. The information on web comprises a huge amount of data in different forms such as structured, semi structured, or few is totally in unstructured format. Due to huge size of information, searching from larger textual data about the specific topic or getting precise information is a challenging task. All this leads to the problem of word sense ambiguity (WSA). Urdu language-based information retrieval system using different techniques related to Web Semantic Search Engine architecture is proposed to efficiently retrieve the relevant information and solve the problem of WSA. The proposed system has average precision ratio 96% as compared to average precision ratio of 74% and 75% average precision Google for single word query. For the long text queries, our system outperforms the existing famous search engines with 92% accuracy such as Bing and Google having 16.50% and 16% accuracy, respectively. Similarly, the proposed system for single word query, the recall ratio is 32.25% as compared to 25% and 25% of Bing and Google. The results of recall ratio for long text query are improved as well, showing 6.38% as compared to 6.20% and 4.8% of Bing and Google, respectively. The results showed that the proposed system gives better and efficient results as compared to the existing systems for Urdu language.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4306174515",
    "type": "article"
  },
  {
    "title": "Research of College English Online Course Based on Cloud Computing and Exploitation for Multimedia Asian Information Processing",
    "doi": "https://doi.org/10.1145/3524112",
    "publication_date": "2022-11-08",
    "publication_year": 2022,
    "authors": "Hongfang Xiao; Sathishkumar Veerappampalayam Easwaramoorthy; Adhiyaman Manickam",
    "corresponding_authors": "",
    "abstract": "In today's techno world,teachers worldwide can learn the world-famous masterclasses and access all sorts of courses for school teachers. Learners have more learning opportunities based on the excellent sharing of resources. Technical support is provided in developing the Fuzzy Integrated Cloud Computing Framework (FICCF) to develop Online Courses. This paper explores the educational capabilities of cloud computing, the real-time contact between teachers and students, studying, and heterogeneous terminal access to track evaluation-based fuzzy variables. It addresses the benefits of open-ended modes, creating an English course design system in a cloud-based environment, modifying lecture mode, and reforming the English course design mode. The experimental result shows that the proposed cloud computing framework has better efficiency in developing and promoting English online courses.The self-learning systems make it possible to provide the best possible learning environment. It ensures that sensitive data generated by an application is deleted, and this feature ensures accuracy.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4308595829",
    "type": "article"
  },
  {
    "title": "Punctuation Prediction in Bangla Text",
    "doi": "https://doi.org/10.1145/3575804",
    "publication_date": "2022-12-14",
    "publication_year": 2022,
    "authors": "Habibur Rahman; Md. Rezwan Shahrior Rahin; Araf Mohammad Mahbub; Md. Adnanul Islam; Md. Saddam Hossain Mukta; Md. Mahbubur Rahman",
    "corresponding_authors": "",
    "abstract": "Punctuation prediction is critical as it can enhance the readability of machine-transcribed speeches or texts significantly by adding appropriate punctuation. Furthermore, systems like Automatic Speech Recognizer (ASR) produce texts that are unpunctuated, making the readability difficult for humans and also hampers the performance of various natural language processing (NLP) tasks. Such NLP related tasks have been investigated thoroughly for English; however, very limited work is done for punctuation prediction in the Bangla language. In this study, we train a bidirectional recurrent neural network (BRNN) along with Attention model with a plausibly large Bangla dataset. Afterwards, we apply extensive postprocessing techniques for predicting punctuation more accurately with the employed model. Initially, we perform experimentation with a relatively imbalanced dataset, and our model shows promising results F1=56.9 for Period) in punctuation prediction. Later, we also investigate the model’s performance using a balanced Bangla dataset to achieve higher performance scores ( F1=62.2 for Question). Thus, the goal of this study is to propose an efficient approach that can predict punctuation in Bangla texts effectively. Our study also includes investigation on how our postprocessing techniques affect the prediction performance. Being an early attempt for the punctuation prediction in Bangla text, our work is expected to significantly contribute in the NLP field for the Bangla language, and will pave the way for future work with the Bangla language in this direction.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4311476482",
    "type": "article"
  },
  {
    "title": "Fusion Pre-trained Emoji Feature Enhancement for Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3578582",
    "publication_date": "2022-12-29",
    "publication_year": 2022,
    "authors": "Jie Chen; Zhiqiang Yao; Shu Zhao; Yanping Zhang",
    "corresponding_authors": "",
    "abstract": "Emoji are often used in social media to enrich users’ emotions, and they play an important role in the task of social media sentiment analysis. In practice, researchers are more likely to consider emoji as special symbols and treat them separately from the text. Some existing methods use emoji as a dictionary for matching or converting emoji into text. However, these methods disregard the relationship between emoji and context, blue and they do not reflect the emotions that users are expected to express. It is challenging to incorporate the original emotions of emoji in social media sentiment analysis. In this article, we propose the EPE model: Emoji Pre-trained feature Enhanced sentiment analysis. Specifically, we collected 8 million tweets and selected 5 million tweets with pre-trained emoji with context using the BERT model. We labeled 20,000 tweets as a three-category dataset and used Bi-LSTM with an attention layer to extract text features. Emoji were retained as key emotion information and combined with text features in the final layer as a connected vector for final prediction. Experimental results with our dataset showed that the proposed EPE model achieved better performance than other baseline models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4313254885",
    "type": "article"
  },
  {
    "title": "Chinese Zero Pronoun Resolution",
    "doi": "https://doi.org/10.1145/3325884",
    "publication_date": "2019-06-05",
    "publication_year": 2019,
    "authors": "Qingyu Yin; Weinan Zhang; Yu Zhang; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Semantic information that has been proven to be necessary to the resolution of common noun phrases is typically ignored by most existing Chinese zero pronoun resolvers. This is because that zero pronouns convey no descriptive information, which makes it almost impossible to calculate semantic similarities between the zero pronoun and its candidate antecedents. Moreover, most of traditional approaches are based on the single-candidate model, which considers the candidate antecedents of a zero pronoun in isolation and thus overlooks their reciprocities. To address these problems, we first propose a neural-network-based zero pronoun resolver ( NZR ) that is capable of generating vector-space semantics of zero pronouns and candidate antecedents. On the basis of NZR , we develop the collaborative filtering-based framework for Chinese zero pronoun resolution task, exploring the reciprocities between the candidate antecedents of a zero pronoun to more rationally re-estimate their importance. Experimental results on the Chinese portion of the OntoNotes 5.0 corpus are encouraging: Our proposed model substantially surpasses the Chinese zero pronoun resolution baseline systems.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2952562689",
    "type": "article"
  },
  {
    "title": "μ-Forcing",
    "doi": "https://doi.org/10.1145/3341110",
    "publication_date": "2019-07-13",
    "publication_year": 2019,
    "authors": "Dayiheng Liu; Xue Yang; Feng He; Yuanyuan Chen; Jiancheng Lv",
    "corresponding_authors": "",
    "abstract": "It has been previously observed that training Variational Recurrent Autoencoders (VRAE) for text generation suffers from serious uninformative latent variables problem. The model would collapse into a plain language model that totally ignore the latent variables and can only generate repeating and dull samples. In this paper, we explore the reason behind this issue and propose an effective regularizer based approach to address it. The proposed method directly injects extra constraints on the posteriors of latent variables into the learning process of VRAE, which can flexibly and stably control the trade-off between the KL term and the reconstruction term, making the model learn dense and meaningful latent representations. The experimental results show that the proposed method outperforms several strong baselines and can make the model learn interpretable latent variables and generate diverse meaningful sentences. Furthermore, the proposed method can perform well without using other strategies, such as KL annealing.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2956843879",
    "type": "article"
  },
  {
    "title": "Importance of Signal Processing Cues in Transcription Correction for Low-Resource Indian Languages",
    "doi": "https://doi.org/10.1145/3342352",
    "publication_date": "2019-08-10",
    "publication_year": 2019,
    "authors": "Jeena J. Prakash; Golda Brunet Rajan; Hema A. Murthy",
    "corresponding_authors": "",
    "abstract": "Accurate phonetic transcriptions are crucial for building robust acoustic models for speech recognition as well as speech synthesis applications. Phonetic transcriptions are not usually provided with speech corpora. A lexicon is used to generate phone-level transcriptions of speech corpora with sentence-level transcriptions. When lexical entries are not available, letter-to-sound (LTS) rules are used. Whether it is a lexicon or LTS, the rules for pronunciation are generic and may not match the spoken utterance. This can lead to transcription errors. The objective of this study is to address the issue of mismatch between the transcription and its acoustic realisation. In particular, the issue of vowel deletions is studied. Group-delay-based segmentation is used to determine insertion/deletion of vowels in the speech utterance. The transcriptions are corrected in the training data based on this. The corrected data are used in automatic speech recognition (ASR) and text to speech synthesis (TTS) systems. ASR and TTS systems built with the corrected transcriptions show improvements in the performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2968423016",
    "type": "article"
  },
  {
    "title": "Wasf-Vec",
    "doi": "https://doi.org/10.1145/3345517",
    "publication_date": "2019-12-12",
    "publication_year": 2019,
    "authors": "Tiba Zaki Abdulhameed; Imed Zitouni; Ikhlas Abdel‐Qader",
    "corresponding_authors": "",
    "abstract": "Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word’s topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word’s nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3007179947",
    "type": "article"
  },
  {
    "title": "Learning and Modeling Unit Embeddings Using Deep Neural Networks for Unit-Selection-Based Mandarin Speech Synthesis",
    "doi": "https://doi.org/10.1145/3372244",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Zhou Xiao; Zhen-Hua Ling; Li-Rong Dai",
    "corresponding_authors": "",
    "abstract": "A method of learning and modeling unit embeddings using deep neutral networks (DNNs) is presented in this article for unit-selection-based Mandarin speech synthesis. Here, a unit embedding is defined as a fixed-length embedding vector for a phone-sized unit candidate in a corpus. Modeling phone-sized embedding vectors instead of frame-sized acoustic features can better measure the long-term dependencies among consecutive units in an utterance. First, a DNN with an embedding layer is built to learn the embedding vectors of all unit candidates in the corpus from scratch. In order to enable the extracted embedding vectors to carry both acoustic and linguistic information of unit candidates, a multitarget learning strategy is designed for the DNN. Its optional prediction targets include frame-level acoustic features, unit durations, monophone and tone identifiers, and context classes. Then, another two DNNs are constructed to map linguistic features toward the extracted embedding vectors. One of them employs the unit vectors of preceding phones besides the linguistic features of current phone as its input. At synthesis time, the distances between the unit vectors predicted by these two DNNs and the ones derived from unit candidates are used as a part of the target cost and a part of the concatenation cost, respectively. Our experiments on a Mandarin speech synthesis corpus demonstrate that learning and modeling unit embeddings improve the naturalness of hidden Markov model (HMM)-based unit selection speech synthesis. Furthermore, integrating multiple targets for learning unit embeddings achieves better performance than using only acoustic targets according to our subjective evaluation results.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3008540909",
    "type": "article"
  },
  {
    "title": "Speech-Driven End-to-End Language Discrimination toward Chinese Dialects",
    "doi": "https://doi.org/10.1145/3389021",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Fan Xu; Luo Jian; Mingwen Wang; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "Language discrimination among similar languages, varieties, and dialects is a challenging natural language processing task. The traditional text-driven focus leads to poor results. In this article, we explore the effectiveness of speech-driven features toward language discrimination among Chinese dialects. First, we systematically explore the appropriateness of speech-driven MFCC features toward CNN-based language discrimination. Then, we design an end-to-end speech recognition model based on HMM-DNN to predict Chinese dialect words. We adopt attention mechanism to extract the discriminative words related to different Chinese dialects. Finally, through a CNN, we combine the word-level embedding and the MFCC-based features. Evaluation of two benchmark Chinese dialect corpora shows the appropriateness and effectiveness of the proposed speech-driven approach to fine-grained Chinese dialect discrimination compared to the state-of-the-art methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3033952891",
    "type": "article"
  },
  {
    "title": "CESS-A System to Categorize Bangla Web Text Documents",
    "doi": "https://doi.org/10.1145/3398070",
    "publication_date": "2020-06-18",
    "publication_year": 2020,
    "authors": "Ankita Dhar; Himadri Mukherjee; Niladri Sekhar Dash; Kaushik Roy",
    "corresponding_authors": "",
    "abstract": "Technology has evolved remarkably, which has led to an exponential increase in the availability of digital text documents of disparate domains over the Internet. This makes the retrieval of the information a very much time- and resource-consuming task. Thus, a system that can categorize such documents based on their domains can truly help the users in obtaining the required information with relative ease and also reduce the workload of the search engines. This article presents a text categorization system (CESS) that categorizes text document using newly proposed hybrid features that combines term frequency-inverse document frequency-inverse class frequency and modified chi-square methods. Experiments were performed on real-world Bangla documents from eight domains comprises of 24,29,857 tokens, and the highest accuracy of 99.91% has been obtained with multilayer perceptron-based classification. Also, the experiments were tested on Reuters-21578 and 20 Newsgroups datasets and obtained accuracies of 97.29% and 94.67%, respectively, to show the language-independent nature of the system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3036149167",
    "type": "article"
  },
  {
    "title": "Grading Tibetan Children’s Literature",
    "doi": "https://doi.org/10.1145/3392046",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "dirk schmidt",
    "corresponding_authors": "dirk schmidt",
    "abstract": "Worldwide, literacy is on the rise. This historically unprecedented surge—especially over the past 200 years—has changed nearly everything about the ancient technology of reading. Who reads is changing: Literacy is no longer just for elite, professional readers, but for anyone and everyone. What and why we read is changing: We do not just read difficult texts for academic, religious, legal, or record-keeping purposes; we also read easy texts to be entertained, to access information, and to communicate with each other on a daily basis. And how we read is changing: Memorization, recitation, and oral performance has given way to a rapid, silent, individual activity. Many of these democratizing changes have been made possible by technology. This has included advances in methods and materials that have made reading and writing easy, cheap, and widely available—like paper, the printing press, and the digital revolution. But perhaps the biggest reason literacy has become so widespread has been its ability to reach people in their own natural languages . More recently, this progress has been enhanced by NLP tools, like readability editors, that have helped authors, journalists, and other writing professionals make simple, clear content suitable for both beginning readers and widespread audiences. To that end, this article introduces a new readability tool, “Dakje,” alongside a specific use case, and demonstrates how it can help benefit literacy in the Tibetan languages. This NLP software works by word-splitting Tibetan text and analyzing those words using level lists that are based on frequency analysis from corpora. Users then have instant access to statistics on the readability of their word choices so they can make edits for easy-to-read text. In our test-case, Dakje helped us reduce sentence complexity by 34%, total word count by 10%, and non-level vocabulary use from 16% to 1% when compared to an original English-to-Tibetan translation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3041931536",
    "type": "article"
  },
  {
    "title": "Condition-Transforming Variational Autoencoder for Generating Diverse Short Text Conversations",
    "doi": "https://doi.org/10.1145/3402884",
    "publication_date": "2020-10-13",
    "publication_year": 2020,
    "authors": "Yu-Ping Ruan; Zhen-Hua Ling; Xiaodan Zhu",
    "corresponding_authors": "",
    "abstract": "In this article, conditional-transforming variational autoencoders (CTVAEs) are proposed for generating diverse short text conversations. In conditional variational autoencoders (CVAEs), the prior distribution of latent variable z follows a multivariate Gaussian distribution with mean and variance modulated by the input conditions. Previous work found that this distribution tended to become condition-independent in practical applications. Thus, this article designs CTVAEs to enhance the influence of conditions in CVAEs. In a CTVAE model, the latent variable z is sampled by performing a non-linear transformation on the combination of the input conditions and the samples from a condition-independent prior distribution N (0, I). In our experiments using a Chinese Sina Weibo dataset, the CTVAE model derives z samples for decoding with better condition-dependency than that of the CVAE model. The earth mover’s distance (EMD) between the distributions of the latent variable z at the training stage, and the testing stage is also reduced by using the CTVAE model. In subjective preference tests, our proposed CTVAE model performs significantly better than CVAE and sequence-to-sequence (Seq2Seq) models on generating diverse, informative, and topic-relevant responses.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3096542808",
    "type": "article"
  },
  {
    "title": "On the Construction of Web NER Model Training Tool based on Distant Supervision",
    "doi": "https://doi.org/10.1145/3422817",
    "publication_date": "2020-11-15",
    "publication_year": 2020,
    "authors": "Chien-Lung Chou; Chia‐Hui Chang; Yuan-Hao Lin; Kuo-Chun Chien",
    "corresponding_authors": "",
    "abstract": "Named entity recognition (NER) is an important task in natural language understanding, as it extracts the key entities (person, organization, location, date, number, etc.) and objects (product, song, movie, activity name, etc.) mentioned in texts. However, existing natural language processing (NLP) tools (such as Stanford NER) recognize only general named entities or require annotated training examples and feature engineering for supervised model construction. Since not all languages or entities have public NER support, constructing a tool for NER model training is essential for low-resource language or entity information extraction. In this article, we study the problem of developing a tool to prepare training corpus from the Web with known seed entities for custom NER model training via distant supervision. The major challenge of automatic labeling lies in the long labeling time due to large corpus and seed entities as well as the concern to avoid false positive and false negative examples due to short and long seeds. To solve this problem, we adopt locality-sensitive hashing (LSH) for various length of seed entities. We conduct experiments on five types of entity recognition tasks, including Chinese person names, food names, locations, points of interest (POIs), and activity names to demonstrate the improvements with the proposed Web NER model construction tool. Because the training corpus is obtained by automatic labeling of the seed entity–related sentences, one could use either the entire corpus or the positive only sentences for model training. Based on the experimental results, we found the decision should depend on whether traditional linear chained conditional random fields (CRF) or deep neural network–based CRF is used for model training as well as the completeness of the provided seed list.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3103318804",
    "type": "article"
  },
  {
    "title": "Model Generation of Accented Speech using Model Transformation and Verification for Bilingual Speech Recognition",
    "doi": "https://doi.org/10.1145/2661637",
    "publication_date": "2015-04-20",
    "publication_year": 2015,
    "authors": "Han-Ping Shen; Chung‐Hsien Wu; Pei-Shan Tsai",
    "corresponding_authors": "",
    "abstract": "Nowadays, bilingual or multilingual speech recognition is confronted with the accent-related problem caused by non-native speech in a variety of real-world applications. Accent modeling of non-native speech is definitely challenging, because the acoustic properties in highly-accented speech pronounced by non-native speakers are quite divergent. The aim of this study is to generate highly Mandarin-accented English models for speakers whose mother tongue is Mandarin. First, a two-stage, state-based verification method is proposed to extract the state-level, highly-accented speech segments automatically. Acoustic features and articulatory features are successively used for robust verification of the extracted speech segments. Second, Gaussian components of the highly-accented speech models are generated from the corresponding Gaussian components of the native speech models using a linear transformation function. A decision tree is constructed to categorize the transformation functions and used for transformation function retrieval to deal with the data sparseness problem. Third, a discrimination function is further applied to verify the generated accented acoustic models. Finally, the successfully verified accented English models are integrated into the native bilingual phone model set for Mandarin-English bilingual speech recognition. Experimental results show that the proposed approach can effectively alleviate recognition performance degradation due to accents and can obtain absolute improvements of 4.1%, 1.8%, and 2.7% in word accuracy for bilingual speech recognition compared to that using traditional ASR approaches, MAP-adapted, and MLLR-adapted ASR methods, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2212316480",
    "type": "article"
  },
  {
    "title": "Acoustic Features for Hidden Conditional Random Fields--Based Thai Tone Classification",
    "doi": "https://doi.org/10.1145/2833088",
    "publication_date": "2015-12-11",
    "publication_year": 2015,
    "authors": "Natthawut Kertkeidkachorn; Proadpran Punyabukkana; Atiwong Suchato",
    "corresponding_authors": "",
    "abstract": "In the Thai language, tone information is necessary for Thai speech recognition systems. Previous studies show that many acoustic cues are attributed to shapes of tones. Nevertheless, most Thai tone classification studies mainly adopted F 0 values and their derivatives without considering other acoustic features. In this article, other acoustic features for Thai tone classification are investigated. In the experiment, energy values and spectral information represented by three spectral-based features including the LPC-based feature, PLP-based feature, and MFCC-based feature are applied to the HCRF-based Thai tone classification, which was reported as the best approach for Thai tone classification. The energy values provide an error rate reduction of 22.40% in the isolated word scenario, while there are slight improvements in the continuous speech scenario. On the contrary, spectral-based features greatly contribute to Thai tone classification in the continuous-speech scenario, whereas spectral-based features slightly degrade performances in the isolated-word scenario. The best achievement in the continuous-speech scenario is obtained from the PLP-based feature, which yields an error rate reduction of 13.90%. Therefore, findings in this article are that energy values and spectral-based features, especially the PLP-based feature, are the main contributors to the improvement of the performances of Thai tone classification in the isolated-word scenario and the continuous-speech scenario, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2268485963",
    "type": "article"
  },
  {
    "title": "Enhancing Shift-Reduce Constituent Parsing with Action N-Gram Model",
    "doi": "https://doi.org/10.1145/2820902",
    "publication_date": "2016-02-17",
    "publication_year": 2016,
    "authors": "Hao Zhou; Shujian Huang; Junsheng Zhou; Yue Zhang; Huadong Chen; Xinyu Dai; Chuan Cheng; Jiajun Chen",
    "corresponding_authors": "",
    "abstract": "Current shift-reduce parsers “understand” the context by embodying a large number of binary indicator features with a discriminative model. In this article, we propose the action n-gram model, which utilizes the action sequence to help parsing disambiguation. The action n-gram model is trained on action sequences produced by parsers with the n-gram estimation method, which gives a smoothed maximum likelihood estimation of the action probability given a specific action history. We show that incorporating action n-gram models into a state-of-the-art parsing framework could achieve parsing accuracy improvements on three datasets across two languages.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2278483508",
    "type": "article"
  },
  {
    "title": "Speech Act Identification Using Semantic Dependency Graphs with Probabilistic Context-Free Grammars",
    "doi": "https://doi.org/10.1145/2786978",
    "publication_date": "2016-01-07",
    "publication_year": 2016,
    "authors": "Jui‐Feng Yeh",
    "corresponding_authors": "Jui‐Feng Yeh",
    "abstract": "We propose an approach for identifying the speech acts of speakers’ utterances in conversational spoken dialogue that involves using semantic dependency graphs with probabilistic context-free grammars (PCFGs). The semantic dependency graph based on the HowNet knowledge base is adopted to model the relationships between words in an utterance parsed by PCFG. Dependency relationships between words within the utterance are extracted by decomposing the semantic dependency graph according to predefined events. The corresponding values of semantic slots are subsequently extracted from the speaker's utterances according to the corresponding identified speech act. The experimental results obtained when using the proposed approach indicated that the accuracy rates of speech act detection and task completion were 95.6% and 77.4% for human-generated transcription (REF) and speech-to-text recognition output (STT), respectively, and the average numbers of turns of each dialogue were 8.3 and 11.8 for REF and STT, respectively. Compared with Bayes classifier, partial pattern tree, and Bayesian-network-based approaches, we obtained 14.1%, 9.2%, and 3% improvements in the accuracy of speech act identification, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2293305934",
    "type": "article"
  },
  {
    "title": "Towards Machine Translation in Semantic Vector Space",
    "doi": "https://doi.org/10.1145/2699927",
    "publication_date": "2015-04-20",
    "publication_year": 2015,
    "authors": "Jiajun Zhang; Shujie Liu; Mu Li; Ming Zhou; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Measuring the quality of the translation rules and their composition is an essential issue in the conventional statistical machine translation (SMT) framework. To express the translation quality, the previous lexical and phrasal probabilities are calculated only according to the co-occurrence statistics in the bilingual corpus and may be not reliable due to the data sparseness problem. To address this issue, we propose measuring the quality of the translation rules and their composition in the semantic vector embedding space (VES). We present a recursive neural network (RNN)-based translation framework, which includes two submodels. One is the bilingually-constrained recursive auto-encoder, which is proposed to convert the lexical translation rules into compact real-valued vectors in the semantic VES. The other is a type-dependent recursive neural network, which is proposed to perform the decoding process by minimizing the semantic gap (meaning distance) between the source language string and its translation candidates at each state in a bottom-up structure. The RNN-based translation model is trained using a max-margin objective function that maximizes the margin between the reference translation and the n-best translations in forced decoding. In the experiments, we first show that the proposed vector representations for the translation rules are very reliable for application in translation modeling. We further show that the proposed type-dependent, RNN-based model can significantly improve the translation quality in the large-scale, end-to-end Chinese-to-English translation evaluation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2395672222",
    "type": "article"
  },
  {
    "title": "Query Expansion in Resource-Scarce Languages",
    "doi": "https://doi.org/10.1145/2997643",
    "publication_date": "2016-11-18",
    "publication_year": 2016,
    "authors": "Arjun Atreya; Ashish Kankaria; Pushpak Bhattacharyya; Ganesh Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Retrievals in response to queries to search engines in resource-scarce languages often produce no results, which annoys the user. In such cases, at least partially relevant documents must be retrieved. We propose a novel multilingual framework, MultiStructPRF , which expands the query with related terms by (i) using a resource-rich assisting language and (ii) giving varied importance to the expansion terms depending on their position of occurrence in the document. Our system uses the help of an assisting language to expand the query in order to improve system recall. We propose a systematic expansion model for weighting the expansion terms coming from different parts of the document. To combine the expansion terms from query language and assisting language, we propose a heuristics-based fusion model . Our experimental results show an improvement over other PRF techniques in both precision and recall for multiple resource-scarce languages like Marathi, Bengali, Odia, Finnish, and the like. We study the effect of different assisting languages on precision and recall for multiple query languages. Our experiments reveal an interesting fact: Precision is positively correlated with the typological closeness of query language and assisting language, whereas recall is positively correlated with the resource richness of the assisting language.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2551816470",
    "type": "article"
  },
  {
    "title": "Special Issue on Deep Structured Learning for Natural Language Processing",
    "doi": "https://doi.org/10.1145/3436206",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Gunasekaran Manogaran; Hassan Qudrat‐Ullah; Qin Xin",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3157664510",
    "type": "article"
  },
  {
    "title": "Heuristic Bilingual Graph Corpus Network to Improve English Instruction Methodology Based on Statistical Translation Approach",
    "doi": "https://doi.org/10.1145/3406205",
    "publication_date": "2021-05-05",
    "publication_year": 2021,
    "authors": "Hui Fang; Hongmei Shi; Jiuzhou Zhang",
    "corresponding_authors": "",
    "abstract": "The number of sentence pairs in the bilingual corpus is a key to translation accuracy in computational machine translations. However, if the amount goes beyond a certain degree, the increasing number of cases has less impact on the translation while the construction of translation systems requires a considerable amount of time and energy, thus preventing the development of a statistical translation by the computer. This article offers a number of classifications for measuring the amount of information for each pair of sentences, using the Heuristic Bilingual Graph Corpus Network (HBGCN) to form an improved method of corpus selection that takes the difference between the first amount of information between the pairs of sentences into account. Using a graphic-based selector method as a training set, they achieve a close translation result through our experiments with the whole body and achieve better results than basic results for the following based on the Document Inverse Frequency (DIF) ranking approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3158350807",
    "type": "article"
  },
  {
    "title": "A Hierarchical Sequence-to-Sequence Model for Korean POS Tagging",
    "doi": "https://doi.org/10.1145/3421762",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Guozhe Jin; Zhezhou Yu",
    "corresponding_authors": "",
    "abstract": "Part-of-speech (POS) tagging is a fundamental task in natural language processing. Korean POS tagging consists of two subtasks: morphological analysis and POS tagging. In recent years, scholars have tended to use the seq2seq model to solve this problem. The full context of a sentence is considered in these seq2seq-based Korean POS tagging methods. However, Korean morphological analysis relies more on local contextual information, and in many cases, there exists one-to-one matching between morpheme surface form and base form. To make better use of these characteristics, we propose a hierarchical seq2seq model. In our model, the low-level Bi-LSTM encodes the syllable sequence, whereas the high-level Bi-LSTM models the context information of the whole sentence, and the decoder generates the morpheme base form syllables as well as the POS tags. To improve the accuracy of the morpheme base form recovery, we introduced the convolution layer and the attention mechanism to our model. The experimental results on the Sejong corpus show that our model outperforms strong baseline systems in both morpheme-level F1-score and eojeol-level accuracy, achieving state-of-the-art performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3160111908",
    "type": "article"
  },
  {
    "title": "Recent Developments in Tibetan NLP",
    "doi": "https://doi.org/10.1145/3453692",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Congjun Long; Nathan W. Hill",
    "corresponding_authors": "",
    "abstract": "research-article Share on Recent Developments in Tibetan NLP Authors: Long Congjun Chinese Academy of Social Sciences Chinese Academy of Social SciencesView Profile , Nathan W. Hill Trinity College Dublin Trinity College DublinView Profile Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingVolume 20Issue 2March 2021 Article No.: 19pp 1–3https://doi.org/10.1145/3453692Online:23 April 2021Publication History 0citation84DownloadsMetricsTotal Citations0Total Downloads84Last 12 Months69Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3160807235",
    "type": "article"
  },
  {
    "title": "Intermodal Sentiment Analysis for Images with Text Captions Using the VGGNET Technique",
    "doi": "https://doi.org/10.1145/3450971",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Rachana Sathish; P. Ezhumalai",
    "corresponding_authors": "",
    "abstract": "More individuals actively express their opinions and attitudes in social media through advanced improvements such as visual content and text captions. Sentiment analysis for visuals such as images, video, and GIFs has become an emerging research trend in understanding social involvement and opinion prediction. Numerous individual researchers have obtained good progress in outcomes for text sentiment analysis and image sentiment analysis. The combination of image sentiment analysis with text caption analysis needs more research. This article presents a VGG Network-based Intermodal Sentiment Analysis Model (VGGNET-ISAM) for transferring the connection between texts to images. A mapping process is developed using the VGG Network for gathering the opinion information as numerical vectors. The Active Deep Learning (ADL) classifier is used for opinion prediction from the obtained information vectors. Simulation experiments are carried out to evaluate the proposed approach. The findings show that the model outperforms and gives better solutions with high accuracy, precision with low delay, and low error rate.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3177369324",
    "type": "article"
  },
  {
    "title": "Approaches for Multilingual Phone Recognition in Code-switched and Non-code-switched Scenarios Using Indian Languages",
    "doi": "https://doi.org/10.1145/3437256",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "K. E. Manjunath; Srinivasa Raghavan K. M.; K. Sreenivasa Rao; Dinesh Babu Jayagopi; V. Ramasubramanian",
    "corresponding_authors": "",
    "abstract": "In this study, we evaluate and compare two different approaches for multilingual phone recognition in code-switched and non-code-switched scenarios. First approach is a front-end Language Identification (LID)-switched to a monolingual phone recognizer (LID-Mono), trained individually on each of the languages present in multilingual dataset. In the second approach, a common multilingual phone-set derived from the International Phonetic Alphabet (IPA) transcription of the multilingual dataset is used to develop a Multilingual Phone Recognition System (Multi-PRS). The bilingual code-switching experiments are conducted using Kannada and Urdu languages. In the first approach, LID is performed using the state-of-the-art i-vectors. Both monolingual and multilingual phone recognition systems are trained using Deep Neural Networks. The performance of LID-Mono and Multi-PRS approaches are compared and analysed in detail. It is found that the performance of Multi-PRS approach is superior compared to more conventional LID-Mono approach in both code-switched and non-code-switched scenarios. For code-switched speech, the effect of length of segments (that are used to perform LID) on the performance of LID-Mono system is studied by varying the window size from 500 ms to 5.0 s, and full utterance. The LID-Mono approach heavily depends on the accuracy of the LID system and the LID errors cannot be recovered. But, the Multi-PRS system by virtue of not having to do a front-end LID switching and designed based on the common multilingual phone-set derived from several languages, is not constrained by the accuracy of the LID system, and hence performs effectively on code-switched and non-code-switched speech, offering low Phone Error Rates than the LID-Mono system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3192425623",
    "type": "article"
  },
  {
    "title": "Exploration of Effective Attention Strategies for Neural Automatic Post-editing with Transformer",
    "doi": "https://doi.org/10.1145/3465383",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Jaehun Shin; WonKee Lee; Byung-Hyun Go; Baikjin Jung; Youngkil Kim; Jong-Hyeok Lee",
    "corresponding_authors": "",
    "abstract": "Automatic post-editing (APE) is the study of correcting translation errors in the output of an unknown machine translation (MT) system and has been considered as a method of improving translation quality without any modification to conventional MT systems. Recently, several variants of Transformer that take both the MT output and its corresponding source sentence as inputs have been proposed for APE; and models introducing an additional attention layer into the encoder to jointly encode the MT output with its source sentence recorded a high-rank in the WMT19 APE shared task. We examine the effectiveness of such joint-encoding strategy in a controlled environment and compare four types of decoder multi-source attention strategies that have been introduced into previous APE models. The experimental results indicate that the joint-encoding strategy is effective and that taking the final encoded representation of the source sentence is the more proper strategy than taking such representation within the same encoder stack. Furthermore, among the multi-source attention strategies combined with the joint-encoding, the strategy that applies attention to the concatenated input representation and the strategy that adds up the individual attention to each input improve the quality of APE results over the strategy using the joint-encoding only.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3196167364",
    "type": "article"
  },
  {
    "title": "Normalization of Transliterated Mongolian Words Using Seq2Seq Model with Limited Data",
    "doi": "https://doi.org/10.1145/3464361",
    "publication_date": "2021-09-01",
    "publication_year": 2021,
    "authors": "Zolzaya Byambadorj; Ryota Nishimura; Altangerel Ayush; Norihide Kitaoka",
    "corresponding_authors": "",
    "abstract": "The huge increase in social media use in recent years has resulted in new forms of social interaction, changing our daily lives. Due to increasing contact between people from different cultures as a result of globalization, there has also been an increase in the use of the Latin alphabet, and as a result a large amount of transliterated text is being used on social media. In this study, we propose a variety of character level sequence-to-sequence (seq2seq) models for normalizing noisy, transliterated text written in Latin script into Mongolian Cyrillic script, for scenarios in which there is a limited amount of training data available. We applied performance enhancement methods, which included various beam search strategies, N-gram-based context adoption, edit distance-based correction and dictionary-based checking, in novel ways to two basic seq2seq models. We experimentally evaluated these two basic models as well as fourteen enhanced seq2seq models, and compared their noisy text normalization performance with that of a transliteration model and a conventional statistical machine translation (SMT) model. The proposed seq2seq models improved the robustness of the basic seq2seq models for normalizing out-of-vocabulary (OOV) words, and most of our models achieved higher normalization performance than the conventional method. When using test data during our text normalization experiment, our proposed method which included checking each hypothesis during the inference period achieved the lowest word error rate (WER = 13.41%), which was 4.51% fewer errors than when using the conventional SMT method.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3198419977",
    "type": "article"
  },
  {
    "title": "An Unsupervised and Robust Line and Word Segmentation Method for Handwritten and Degraded Printed Document",
    "doi": "https://doi.org/10.1145/3474118",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Jayati Mukherjee; Swapan K. Parui; Utpal Roy",
    "corresponding_authors": "",
    "abstract": "Segmentation of text lines and words in an unconstrained handwritten or a machine-printed degraded document is a challenging document analysis problem due to the heterogeneity in the document structure. Often there is un-even skew between the lines and also broken words in a document. In this article, the contribution lies in segmentation of a document page image into lines and words. We have proposed an unsupervised, robust, and simple statistical method to segment a document image that is either handwritten or machine-printed (degraded or otherwise). In our proposed method, the segmentation is treated as a two-class classification problem. The classification is done by considering the distribution of gap size (between lines and between words) in a binary page image. Our method is very simple and easy to implement. Other than the binarization of the input image, no pre-processing is necessary. There is no need of high computational resources. The proposed method is unsupervised in the sense that no annotated document page images are necessary. Thus, the issue of a training database does not arise. In fact, given a document page image, the parameters that are needed for segmentation of text lines and words are learned in an unsupervised manner. We have applied our proposed method on several popular publicly available handwritten and machine-printed datasets (ISIDDI, IAM-Hist, IAM, PBOK) of different Indian and other languages containing different fonts. Several experimental results are presented to show the effectiveness and robustness of our method. We have experimented on ICDAR-2013 handwriting segmentation contest dataset and our method outperforms the winning method. In addition to this, we have suggested a quantitative measure to compute the level of degradation of a document page image.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3210108814",
    "type": "article"
  },
  {
    "title": "Recurrent Neural Hidden Markov Model for High-order Transition",
    "doi": "https://doi.org/10.1145/3476511",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Tatsuya Hiraoka; Sho Takase; Kei Uchiumi; Atsushi Keyaki; Naoaki Okazaki",
    "corresponding_authors": "",
    "abstract": "We propose a method to pay attention to high-order relations among latent states to improve the conventional HMMs that focus only on the latest latent state, since they assume Markov property. To address the high-order relations, we apply an RNN to each sequence of latent states, because the RNN can represent the information of an arbitrary-length sequence with their cell: a fixed-size vector. However, the simplest way, which provides all latent sequences explicitly for the RNN, is intractable due to the combinatorial explosion of the search space of latent states. Thus, we modify the RNN to represent the history of latent states from the beginning of the sequence to the current state with a fixed number of RNN cells whose number is equal to the number of possible states. We conduct experiments on unsupervised POS tagging and synthetic datasets. Experimental results show that the proposed method achieves better performance than previous methods. In addition, the results on the synthetic dataset indicate that the proposed method can capture the high-order relations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3210534801",
    "type": "article"
  },
  {
    "title": "One-Shot Relation Learning for Knowledge Graphs via Neighborhood Aggregation and Paths Encoding",
    "doi": "https://doi.org/10.1145/3484729",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Jian Sun; Yu Zhou; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "The relation learning between two entities is an essential task in knowledge graph (KG) completion that has received much attention recently. Previous work almost exclusively focused on relations widely seen in the original KGs, which means that enough training data are available for modeling. However, long-tail relations that only show in a few triples are actually much more common in practical KGs. Without sufficiently large training data, the performance of existing models on predicting long-tail relations drops impressively. This work aims to predict the relation under a challenging setting where only one instance is available for training. We propose a path-based one-shot relation prediction framework, which can extract neighborhood information of an entity based on the relation query attention mechanism to learn transferable knowledge among the same relation. Simultaneously, to reduce the impact of long-tail entities on relation prediction, we selectively fuse path information between entity pairs as auxiliary information of relation features. Experiments in three one-shot relation learning datasets show that our proposed framework substantially outperforms existing models on one-shot link prediction and relation prediction.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4200161632",
    "type": "article"
  },
  {
    "title": "Dual-View Conditional Variational Auto-Encoder for Emotional Dialogue Generation",
    "doi": "https://doi.org/10.1145/3481890",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Mei Li; Jiajun Zhang; Lu Xiang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Emotional dialogue generation aims to generate appropriate responses that are content relevant with the query and emotion consistent with the given emotion tag. Previous work mainly focuses on incorporating emotion information into the sequence to sequence or conditional variational auto-encoder (CVAE) models, and they usually utilize the given emotion tag as a conditional feature to influence the response generation process. However, emotion tag as a feature cannot well guarantee the emotion consistency between the response and the given emotion tag. In this article, we propose a novel Dual-View CVAE model to explicitly model the content relevance and emotion consistency jointly. These two views gather the emotional information and the content-relevant information from the latent distribution of responses, respectively. We jointly model the dual-view via VAE to get richer and complementary information. Extensive experiments on both English and Chinese emotion dialogue datasets demonstrate the effectiveness of our proposed Dual-View CVAE model, which significantly outperforms the strong baseline models in both aspects of content relevance and emotion consistency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4200241081",
    "type": "article"
  },
  {
    "title": "Evaluating the Content of LMF Standardized Dictionaries",
    "doi": "https://doi.org/10.1145/3047406",
    "publication_date": "2017-05-19",
    "publication_year": 2017,
    "authors": "Wafa Wali; Bilel Gargouri; Adelmajid Ben Hamadou",
    "corresponding_authors": "",
    "abstract": "Since the age of paper versions, dictionaries are often published with anomalies in their content resulting from lexicographer’s mistakes or from the lack of efficiency of automatic enrichment systems. Many of these anomalies are expensive to manually detect and difficult to automatically control, notably with lightly structured models of dictionaries. In this article, we take advantage of the fine structure proposed by the Lexical Markup Framework (LMF) norm to investigate the detection of anomalies in the content of LMF normalized dictionaries. First, we give a theoretical study on the plausible anomalies, such as inconsistency, incoherence, redundancy, and incompleteness. Second, we detail the approach that we propose for the automatic detection of such anomalies. Finally, we report on an experiment carried out on an available normalized dictionary of the Arabic language. The experiment has shown that the proposed approach gives reasonable results in terms of precision and recall.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2615715744",
    "type": "article"
  },
  {
    "title": "The Contribution of Stemming and Semantics in Arabic Topic Segmentation",
    "doi": "https://doi.org/10.1145/3152464",
    "publication_date": "2018-01-11",
    "publication_year": 2018,
    "authors": "Marwa Naili; Anja Habacha Chaïbi; Henda Hajjami Ben Ghézala",
    "corresponding_authors": "",
    "abstract": "Topic Segmentation is one of the pillars of Natural Language Processing. Yet there is a remarkable research gap in this field, as far as the Arabic language is concerned. The purpose of this article is to improve Arabic Topic Segmentation (ATS) by inquiring into two segmenters: ArabC99 and ArabTextTiling. This study is carried out on two independent levels: the pre-processing level and the segmentation level. These levels represent the basic steps of topic segmentation. On the pre-processing level, we examine the effect of using different Arabic stemming algorithms on ATS. We find out that Light10 is more appropriate for the pre-processing step. Based on this conclusion, we proceed to the second level by proposing two Arabic segmenters called ArabC99-LS-LSA and ArabTextTiling-LS-LSA. These latter use external semantic knowledge related to the Latent Semantic Analysis (LSA). Based on the evaluation results, we notice that LSA provides improvements in this field. Hence, the main outcome of this article emphasizes the multilevel improvement of ATS based on Light10 and LSA.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2783378256",
    "type": "article"
  },
  {
    "title": "Integrating Shallow Syntactic Labels in the Phrase-Boundary Translation Model",
    "doi": "https://doi.org/10.1145/3178460",
    "publication_date": "2018-02-14",
    "publication_year": 2018,
    "authors": "Shahram Salami; Mehrnoush Shamsfard",
    "corresponding_authors": "",
    "abstract": "Using a novel rule labeling method, this article proposes a hierarchical model for statistical machine translation. The proposed model labels translation rules by matching the boundaries of target side phrases with the shallow syntactic labels including POS tags and chunk labels on the target side of the training corpus. The boundary labels are concatenated if there is no label for the whole target span. Labeling with the classes of boundary words on the target side phrases has been previously proposed as a phrase-boundary model which can be considered as the base form of our model. In the extended model, the labeler uses a POS tag if there is no chunk label in one boundary. Using chunks as phrase labels, the proposed model generalizes the rules to decrease the model sparseness. The sparseness is a more important issue in the language pairs with a lot of differences in the word order because they have less number of aligned phrase pairs for extraction of rules. The extended phrase-boundary model is also applicable for low-resource languages having no syntactic parser. Some experiments are performed with the proposed model, the base phrase-boundary model, and variants of Syntax Augmented Machine Translation (SAMT) in translation from Persian and German to English as source and target languages with different word orders. According to the results, the proposed model improves the translation performance in the quality and decoding time aspects. Using BLEU as our metric, the proposed model has achieved a statistically significant improvement of about 0.5 point over the base phrase-boundary model.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2788594087",
    "type": "article"
  },
  {
    "title": "A Dependency Parser for Spontaneous Chinese Spoken Language",
    "doi": "https://doi.org/10.1145/3196278",
    "publication_date": "2018-07-25",
    "publication_year": 2018,
    "authors": "Ruifang He; Yaru Wang; Dawei Song; Peng Zhang; Yuan Jia; Aijun Li",
    "corresponding_authors": "",
    "abstract": "Dependency analysis is vital for spoken language understanding in spoken dialogue systems. However, existing research has mainly focused on western spoken languages, Japanese, and so on. Little research has been done for spoken Chinese in terms of dependency parsing. Therefore, the new spoken corpus, D-ESCSC (Dependency-Expressive Speech Corpus of Standard Chinese) is built by adding new dependency relations special to spoken Chinese based on a written Chinese annotation scheme. Since spoken Chinese contains typical ill-grammatical phenomena, e.g., translocation, repetition, duplication, and omission, the new atom feature related to punctuation and three feature templates are proposed to improve a graph-based dependency parser. Experimental results on spoken Chinese corpus show that the atom feature and three templates really work and the new parser outperforms the baseline parser. To our best knowledge, it is the first work to report dependency parsing results of spoken Chinese.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2883549073",
    "type": "article"
  },
  {
    "title": "Transition-Based Korean Dependency Parsing Using Hybrid Word Representations of Syllables and Morphemes with LSTMs",
    "doi": "https://doi.org/10.1145/3241745",
    "publication_date": "2018-12-14",
    "publication_year": 2018,
    "authors": "Seung‐Hoon Na; Jianri Li; Jonghoon Shin; Kangil Kim",
    "corresponding_authors": "",
    "abstract": "Recently, neural approaches for transition-based dependency parsing have become one of the state-of-the art methods for performing dependency parsing tasks in many languages. In neural transition-based parsing, a parser state representation is first computed from the configuration of a stack and a buffer, which is then fed into a feed-forward neural network model that predicts the next transition action. Given that words are basic elements of a stack and buffer, a parser state representation is considerably affected by how a word representation is defined. In particular, word representation issues become more critical in morphologically rich languages such as Korean, as the set of potential words is not bound but introduce the second-order vocabulary complexity, called the phrase vocabulary complexity due to the agglutinative characteristics of the language. In this article, we propose a hybrid word representation that combines two compositional word representations, each of which is derived from representations of syllables and morphemes , respectively. Our underlying assumption for this hybrid word representation is that, because both syllables and morphemes are two common ways of decomposing Korean words, it is expected that their effects in inducing word representation are complementary to one another. Experimental results carried on Sejong and SPMRL 2014 datasets show that our proposed hybrid word representation leads to the state-of-the-art performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2904870873",
    "type": "article"
  },
  {
    "title": "A Supplementary Feature Set for Sentiment Analysis in Japanese Dialogues",
    "doi": "https://doi.org/10.1145/3310283",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Peter Lajos Ihasz; Mate Kovacs; Ian Piumarta; Victor V. Kryssanov",
    "corresponding_authors": "",
    "abstract": "Recently, real-time affect-awareness has been applied in several commercial systems, such as dialogue systems and computer games. Real-time recognition of affective states, however, requires the application of costly feature extraction methods and/or labor-intensive annotation of large datasets, especially in the case of Asian languages where large annotated datasets are seldom available. To improve recognition accuracy, we propose the use of cognitive context in the form of “emotion-sensitive” intentions. Intentions are often represented through dialogue acts and, as an emotion-sensitive model of dialogue acts, a tagset of interpersonal-relations-directing interpersonal acts (the IA model) is proposed. The model's adequacy is assessed using a sentiment classification task in comparison with two well-known dialogue act models, the SWBD-DAMSL and the DIT++. For the assessment, five Japanese in-game dialogues were annotated with labels of sentiments and the tags of all three dialogue act models which were used to enhance a baseline sentiment classifier system. The adequacy of the IA tagset is demonstrated by a 9% improvement to the baseline sentiment classifier's recognition accuracy, outperforming the other two models by more than 5%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2944504303",
    "type": "article"
  },
  {
    "title": "Syntax-Based Chinese-Vietnamese Tree-to-Tree Statistical Machine Translation with Bilingual Features",
    "doi": "https://doi.org/10.1145/3314938",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Shengxiang Gao; Jihao Huang; Mingya Xue; Zhengtao Yu; Zhuo Wang; Yang Zhang",
    "corresponding_authors": "",
    "abstract": "Because of the scarcity of bilingual corpora, current Chinese--Vietnamese machine translation is far from satisfactory. Considering the differences between Chinese and Vietnamese, we investigate whether linguistic differences can be used to supervise machine translation and propose a method of syntax-based Chinese--Vietnamese tree-to-tree statistical machine translation with bilingual features. Analyzing the syntax differences between Chinese and Vietnamese, we define some linguistic difference-based rules, such as attributive position, time adverbial position, and locative adverbial position, and create rewards for similar rules. These rewards are integrated into the extraction of tree-to-tree translation rules, and we optimize the pruning of the search space during the decoding phase. The experiments on Chinese--Vietnamese bilingual sentence translation show that the proposed method performs better than several compared methods. Further, the results show that syntactic difference features, with search pruning, can improve the accuracy of machine translation without degrading the efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2948296906",
    "type": "article"
  },
  {
    "title": "NeuMorph",
    "doi": "https://doi.org/10.1145/3342354",
    "publication_date": "2019-08-10",
    "publication_year": 2019,
    "authors": "Abhisek Chakrabarty; Akshay Chaturvedi; Utpal Garain",
    "corresponding_authors": "",
    "abstract": "This article deals with morphological tagging for low-resource languages. For this purpose, five Indic languages are taken as reference. In addition, two severely resource-poor languages, Coptic and Kurmanji, are also considered. The task entails prediction of the morphological tag (case, degree, gender, etc.) of an in-context word. We hypothesize that to predict the tag of a word, considering its longer context such as the entire sentence is not always necessary. In this light, the usefulness of convolution operation is studied resulting in a convolutional neural network (CNN) based morphological tagger. Our proposed model (BLSTM-CNN) achieves insightful results in comparison to the present state-of-the-art. Following the recent trend, the task is carried out under three different settings: single language, across languages, and across keys. Whereas the previous models used only character-level features, we show that the addition of word vectors along with character-level embedding significantly improves the performance of all the models. Since obtaining high-quality word vectors for resource-poor languages remains a challenge, in that scenario, the proposed character-level BLSTM-CNN proves to be most effective. 1",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2968515498",
    "type": "article"
  },
  {
    "title": "Layer-Wise De-Training and Re-Training for ConvS2S Machine Translation",
    "doi": "https://doi.org/10.1145/3358414",
    "publication_date": "2019-11-01",
    "publication_year": 2019,
    "authors": "Hongfei Yu; Xiaoqing Zhou; Xiangyu Duan; Min Zhang",
    "corresponding_authors": "",
    "abstract": "The convolutional sequence-to-sequence (ConvS2S) machine translation system is one of the typical neural machine translation (NMT) systems. Training the ConvS2S model tends to get stuck in a local optimum in our pre-studies. To overcome this inferior behavior, we propose to de-train a trained ConvS2S model in a mild way and retrain to find a better solution globally. In particular, the trained parameters of one layer of the NMT network are abandoned by re-initialization while other layers’ parameters are kept at the same time to kick off re-optimization from a new start point and safeguard the new start point not too far from the previous optimum. This procedure is executed layer by layer until all layers of the ConvS2S model are explored. Experiments show that when compared to various measures for escaping from the local optimum, including initialization with random seeds, adding perturbations to the baseline parameters, and continuing training (con-training) with the baseline models, our method consistently improves the ConvS2S translation quality across various language pairs and achieves better performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2984121220",
    "type": "article"
  },
  {
    "title": "Toward a Sustainable Handling of Interlinear-Glossed Text in Language Documentation",
    "doi": "https://doi.org/10.1145/3389010",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Johann‐Mattis List; Nathaniel A. Sims; Robert Forkel",
    "corresponding_authors": "",
    "abstract": "While the amount of digitally available data on the worlds’ languages is steadily increasing, with more and more languages being documented, only a small proportion of the language resources produced are sustainable. Data reuse is often difficult due to idiosyncratic formats and a negligence of standards that could help to increase the comparability of linguistic data. The sustainability problem is nicely reflected in the current practice of handling interlinear-glossed text, one of the crucial resources produced in language documentation. Although large collections of glossed texts have been produced so far, the current practice of data handling makes data reuse difficult. In order to address this problem, we propose a first framework for the computer-assisted, sustainable handling of interlinear-glossed text resources. Building on recent standardization proposals for word lists and structural datasets, combined with state-of-the-art methods for automated sequence comparison in historical linguistics, we show how our workflow can be used to lift a collection of interlinear-glossed Qiang texts (an endangered language spoken in Sichuan, China), and how the lifted data can assist linguists in their research.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2992462302",
    "type": "article"
  },
  {
    "title": "Semantic Role Labeling System for Persian Language",
    "doi": "https://doi.org/10.1145/3372246",
    "publication_date": "2020-01-09",
    "publication_year": 2020,
    "authors": "Azadeh Mirzaei; Fatemeh Sedghi; Pegah Safari",
    "corresponding_authors": "",
    "abstract": "In this article, we present an automatic semantic role labeling system in Persian consisting of two modules: argument identification for specifying argument spans and argument classification for categorizing their semantic roles. Our modules have been trained on Persian Proposition Bank in which predicate-argument information is manually added as a layer on top of Persian Dependency Treebank with about 30,000 sentences. Therefore, our system was trained on 216,871 verbal predicates and 42,386 nonverbal ones consisting of 40,813 nouns and 1,573 adjectives with 33 semantic classes. As a supervised method, we used maximum entropy for building an argument identifier that results in human-level accuracy of 99% and support vector machine for an argument classifier with an F1 of 84. Regarding both verbal and nonverbal predicates with an expanded role set, we achieved reasonable results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3014327316",
    "type": "article"
  },
  {
    "title": "Extracting Polarity Shifting Patterns from Any Corpus Based on Natural Annotation",
    "doi": "https://doi.org/10.1145/3345518",
    "publication_date": "2020-01-10",
    "publication_year": 2020,
    "authors": "Ge Xu; Xiaoyan Yang; Yuanzheng Cai; Zhiqiang Ruan; Tao Wang; Xiangwen Liao",
    "corresponding_authors": "",
    "abstract": "In recent years, online sentiment texts are generated by users in various domains and in different languages. Binary polarity classification (positive or negative) on business sentiment texts can help both companies and customers to evaluate products or services. Sometimes, the polarity of sentiment texts can be modified, making the polarity classification difficult. In sentiment analysis, such modification of polarity is termed as polarity shifting , which shifts the polarity of a sentiment clue (emotion, evaluation, etc.). It is well known that detection of polarity shifting can help improve sentiment analysis in texts. However, to detect polarity shifting in corpora is challenging: (1) polarity shifting is normally sparse in texts, making human annotation difficult; (2) corpora with dense polarity shifting are few; we may need polarity shifting patterns from various corpora. In this article, an approach is presented to extract polarity shifting patterns from any text corpus. For the first time, we proposed to select texts rich in polarity shifting by the idea of natural annotation , which is used to replace human annotation. With a sequence mining algorithm, the selected texts are used to generate polarity shifting pattern candidates, and then we rank them by C-value before human annotation. The approach is tested on different corpora and different languages. The results show that our approach can capture various types of polarity shifting patterns, and some patterns are unique to specific corpora. Therefore, for better performance, it is reasonable to construct polarity shifting patterns directly from the given corpus.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3016841394",
    "type": "article"
  },
  {
    "title": "Learning Word-vector Quantization",
    "doi": "https://doi.org/10.1145/3397967",
    "publication_date": "2020-06-18",
    "publication_year": 2020,
    "authors": "Umut Orhan; Enıs Arslan",
    "corresponding_authors": "",
    "abstract": "We introduced a new classifier named Learning Word-vector Quantization (LWQ) to solve morphological ambiguities in Turkish, which is an agglutinative language. First, a new and morphologically annotated corpus, and then its datasets are prepared with a series of processes. According to datasets, LWQ finds optimal word-vectors positions by moving them in the Euclidean space. LWQ does morphological disambiguation in two steps: First, it defines all solution candidates of an ambiguous word using a morphological analyzer; second, it chooses the best candidate according to its total distances to neighbor words that are not ambiguous. To show LWQ's performance, we have conducted many tests on the corpus by considering the consistency of classification. In the experiments, we achieve 98.4% correct classification ratio to choose correct parse output, which is an excellent level for the literature.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3036455613",
    "type": "article"
  },
  {
    "title": "Applying Text Analytics to the Mind-section Literature of the Tibetan Tradition of the Great Perfection",
    "doi": "https://doi.org/10.1145/3392047",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ravi Krishna; Norman Mu; Kurt Keutzer",
    "corresponding_authors": "",
    "abstract": "Over the past decade, through a mixture of optical character recognition and manual input, there is now a growing corpus of Tibetan literature available as e-texts in Unicode format. With the creation of such a corpus, the techniques of text analytics that have been applied in the analysis of English and other modern languages may now be applied to Tibetan. In this work, we narrow our focus to examine a modest portion of that literature, the Mind-section portion of the literature of the Tibetan tradition of the Great Perfection. Here, we will use the lens of text analytics tools based on machine learning techniques to investigate a number of questions of interest to scholars of this and related traditions of the Great Perfection. It has been necessary for us to participate in all portions of this process: corpora identification and text edition selection, rendering the text as e-texts in Unicode using both Optical Character Recognition and manual entry, data cleaning and transformation, implementation of software for text analysis, and interpretation of results. For this reason, we hope this study can serve as a model for other low-resource languages that are just beginning to approach the problem of providing text analytics for their language.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3041422642",
    "type": "article"
  },
  {
    "title": "Improved Heuristic Data Management and Protection Algorithm for Digital China Cultural Datasets",
    "doi": "https://doi.org/10.1145/3394114",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Rui Shang; Xia Li",
    "corresponding_authors": "",
    "abstract": "In the present scenario sustainable management and protection of digital cultural datasets are considered as a significant area of research. In the recent past, the protection and management of cultural data are facing several new challenges and opportunities. Though several researchers explored their work on managing and protecting cultural data, efficiently and reliability of the present data management algorithm seems to be more complicated due to its incompetence in managing data in an optimized manner. This work presents an improved heuristic big data management algorithm for cultural datasets which is considered as a new discipline of digital cultural heritage specially established for strengthening strategic and interdisciplinary research. The scientific operation and management mechanism of digital protection of cultural heritage is experimentally validated and results show promising outcomes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3041984804",
    "type": "article"
  },
  {
    "title": "Emotion Recognition with Conversational Generation Transfer",
    "doi": "https://doi.org/10.1145/3494532",
    "publication_date": "2022-01-19",
    "publication_year": 2022,
    "authors": "Hongchao Ma; Zhongqing Wang; Xiabing Zhou; Guodong Zhou; Qinglei Zhou",
    "corresponding_authors": "",
    "abstract": "Emotion recognition in conversation is one of the essential tasks of natural language processing. However, this task’s annotation data is insufficient since such data is hard to collect and annotate. Meanwhile, there is large-scale data for conversational generation, and this data does not need annotation manually. But, whether the vector space between different datasets is similar will be a problem. Therefore, we utilize a same dataset to train the conversational generator and the classifier, and transfer knowledge between them. In particular, we propose an Emotion Recognition with Conversational Generation Transfer (ERCGT) framework to model the interaction among utterances by transfer learning. First, we train a conversational generator. In the second step, a transfer learning model is used to transfer the knowledge of generator to the emotion recognition model. Empirical studies illustrate the effectiveness of the proposed framework over several strong baselines on three benchmark emotion classification datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4206121569",
    "type": "article"
  },
  {
    "title": "Handwritten New Tai Lue Character Recognition Using Convolutional Prior Features and Deep Variationally Sparse Gaussian Process Modeling",
    "doi": "https://doi.org/10.1145/3506700",
    "publication_date": "2022-01-20",
    "publication_year": 2022,
    "authors": "Hai Guo; Na Dong; Jun Zhao; Yuchao Liu",
    "corresponding_authors": "",
    "abstract": "New Tai Lue is widely used in Southwest China and Southeast Asia. Hence, it is important to study related handwritten character recognition. Considering the many similar characters in handwritten New Tai Lue, this paper proposes an offline handwritten New Tai Lue character recognition method based on convolutional prior features and deep variationally sparse Gaussian process (DVSGP) modeling. An offline handwritten database is constructed, a convolutional neural network is trained to extract the convolutional features of New Tai Lue character images as prior features, and a DVSGP model is built. The extracted features are input into the DVSGP model to construct a recognition model. The experimental results show that the accuracy of the model is 97.67% and that the precision, recall, and F1-score are 0.9769, 0.9767, and 0.9767, respectively, which are better than those of other methods. The proposed method also achieves high accuracy on the MNIST recognition task, verifying its universal applicability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4210775601",
    "type": "article"
  },
  {
    "title": "Contextual-Aware Information Extractor with Adaptive Objective for Chinese Medical Dialogues",
    "doi": "https://doi.org/10.1145/3511602",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Gangqiang Hu; Shengfei Lyu; Xingyu Wu; Jinlong Li; Huanhuan Chen",
    "corresponding_authors": "",
    "abstract": "Electronic Medical Records ( EMRs ) are the foundation of modern medical information systems. Despite the benefits of EMRs, the exhausting process of constructing EMRs decreases the efficiency of medical consultation. Therefore, it becomes an emerging research field to automatically extract EMRs from medical dialogues. In Chinese medical dialogues, the phenomena of omission and reference are extremely common, leading to strong contextual relevance among utterances. However, recent studies on converting Chinese medical dialogues to EMRs lack a reliable mechanism to effectively exploit the contextual relevance information among utterances. Moreover, they neglect the frequency imbalance of different items and treat these items indiscriminately, which eventually degrade the overall system performance. In this article, we proposed a Contextual-Aware Information Extractor ( CANE ), which employs a local-to-global mechanism over utterances to model the contextual relevance among utterances. Furthermore, an adaptive objective is introduced to alleviate the frequency imbalance of items by dynamically assigning weights to each sample. Experimental results indicate that CANE outperforms previous state-of-the-arts with considerable improvements (+6.11% and +3.39% on F1-score).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4210819377",
    "type": "article"
  },
  {
    "title": "Deep Understanding Based Multi-Document Machine Reading Comprehension",
    "doi": "https://doi.org/10.1145/3519296",
    "publication_date": "2022-02-24",
    "publication_year": 2022,
    "authors": "Feiliang Ren; Yongkang Liu; Bochao Li; Zhibo Wang; Yu Guo; Shilei Liu; Huimin Wu; Jiaqi Wang; Chunchao Liu; Bingchao Wang",
    "corresponding_authors": "",
    "abstract": "Most existing multi-document machine reading comprehension models mainly focus on understanding the interactions between the input question and documents, but ignore following two kinds of understandings. First, to understand the semantic meaning of words in the input question and documents from the perspective of each other. Second, to understand the supporting cues for a correct answer from the perspective of intra-document and inter-documents. Ignoring these two kinds of important understandings would make the models oversee some important information that may be helpful for inding correct answers. To overcome this deiciency, we propose a deep understanding based model for multi-document machine reading comprehension. It has three cascaded deep understanding modules which are designed to understand the accurate semantic meaning of words, the interactions between the input question and documents, and the supporting cues for the correct answer. We evaluate our model on two large scale benchmark datasets, namely TriviaQA Web and DuReader. Extensive experiments show that our model achieves state-of-the-art results on both datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4213459054",
    "type": "article"
  },
  {
    "title": "Sentence Boundary Disambiguation for Tibetan Based on Attention Mechanism at the Syllable Level",
    "doi": "https://doi.org/10.1145/3527663",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Fenfang Li; Hui Lv; La Duo; Binbin Yong; Qingguo Zhou",
    "corresponding_authors": "",
    "abstract": "Tibetan is a low-resource language with few existing electronic reference materials. The goal of Tibetan sentence boundary disambiguation (SBD) is to segment long text into sentences, and it is the foundation for downstream tasks corpora building. This study implemented the Tibetan SBD at the syllable level to avoid word segmentation (WS) errors affecting the accuracy of SBD. Specifically, the attention mechanism is introduced based on a recurrent neural network (RNN) to study Tibetan SBD. The primary objective is to determine, using a trained model, whether the shad contained in Tibetan text is the ending of the sentence, and implement experiments on syllable embedding and component embedding to measure the model's performance. The highest accuracy for Tibetan syllable embedding and component embedding is 96.23% and 95.40 %, respectively, and the F1 score reaches 96.23% and 95.37%, respectively. The experimental results demonstrate that the proposed method can achieve better results than the established rule-based and statistical methods without considering various syntactic and part-of-speech (POS) tagging rules. German and English data from the Europarl corpus and Thai data from the IWSLT2015 corpus are validated to prove the models’ reliability and generalizability. The results demonstrate that this method is efficient not only for low-resource languages but also for high-resource languages. More importantly, we can formally apply the experimental results of this study to the research of downstream tasks, such as machine translation and automatic summarization.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220679055",
    "type": "article"
  },
  {
    "title": "Research and Implementation of Automatic Indexing Method of PDF for Digital Publishing",
    "doi": "https://doi.org/10.1145/3501400",
    "publication_date": "2022-03-30",
    "publication_year": 2022,
    "authors": "Keliang Chen; Jianming Huang; Qi Zhang; Yansong Cui",
    "corresponding_authors": "",
    "abstract": "With the rapid development of mobile Internet technology and artificial intelligence technology, the digital publishing industry is in urgent need of using intelligent technology to change the current way of content production and service. Most of the e-book resources owned by publishing enterprises are in PDF format, which is not suitable for reading on mobile devices, and it is not convenient to directly extract key information and construct knowledge graph. With this in mind, this article designs a PDF automatic indexing scheme that can identify all the element information in PDF and output structured data automatically and then extract all the key information in it to generate a keyword library with tag weights. The scheme mainly involves two key technical points: parsing PDF based on text features and grammar rules and extracting keywords based on tag weights. The former visualizes the text block in PDF into a rectangular area, divides the elements by clustering algorithm, and, finally, outputs structured data containing all the information. The latter combines the tags and their weights in the structured data and extracts the keywords in it by the inter-word relation algorithm. The structured data and keywords database produced by this scheme can be used to produce intelligent e-book and build knowledge graph, thus helping publishing enterprises to transform from a content service provider to an intelligent knowledge service provider. This transformation can deeply excavate the core value of the content held by the publishing industry and promote the digitization and intelligentization process of the whole industry.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220971275",
    "type": "article"
  },
  {
    "title": "One Type Context Is Not Enough: Global Context-aware Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3526215",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Linqing Chen; Junhui Li; Zhengxian Gong; M. Zhang; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "How to effectively model global context has been a critical challenge for document-level neural machine translation (NMT). Both preceding and global context have been carefully explored in the sequence-to-sequence (seq2seq) framework. However, previous studies generally map global context into one vector, which is not enough to well represent the entire document since this largely ignores the hierarchy between sentences and words within. In this article, we propose to model global context for source language from both sentence level and word level. Specifically at sentence level, we extract useful global context for the current sentence, while at word level, we compute global context against words within the current sentence. On this basis, both kinds of global context can be appropriately fused before being incorporated into the state-of-the-art seq2seq model, i.e., Transformer . Detailed experimentation on various document-level translation tasks shows that global context at both sentence level and word level significantly improve translation performance. More encouraging, both kinds of global context are complementary. This leads to more improvement when both kinds of global context are used.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220999604",
    "type": "article"
  },
  {
    "title": "Data Mining Techniques and Machine Learning Algorithms in the Multimedia System to Enhance Engineering Education",
    "doi": "https://doi.org/10.1145/3517805",
    "publication_date": "2022-03-30",
    "publication_year": 2022,
    "authors": "Cai Yinying; Juan Li; Bo Wang",
    "corresponding_authors": "",
    "abstract": "In the current digital era, engineering education worldwide faces a massive challenge in education and career development. By authorizing educators and administrators to migrate to the actions, cloud services technology has transformed into the educational environment. A Multimedia assisted smart learning system (MSLS) has been suggested in this paper where universities/colleges will advocate future development and begin skill-set enhancement courses by e-learning. To classify their employment prospects at the early stage of graduation, this proposed system measures learners' academic/skill data. Machine learning and Data mining are advanced research fields whose accelerated advancement is attributable to developments in data processing research, database industry growth, and business requirements for methods capable of extracting useful information from massive data stores. In addition, for skill set evaluation, a practical algorithm is suggested to find different groups of students that lack the appropriate skill set. The anticipated student groups can be provided with opportunities by e-learning to enhance their required skill set. The findings suggest that more critical choices can boost employment prospects and overall educational development by implementing the new engineering education system.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4221119233",
    "type": "article"
  },
  {
    "title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT",
    "doi": "https://doi.org/10.1145/3530691",
    "publication_date": "2022-04-22",
    "publication_year": 2022,
    "authors": "Aisha Khatun; Anisur Rahman; Md. Saiful Islam; Hemayet Ahmed Chowdhury; Ayesha Tasnim",
    "corresponding_authors": "",
    "abstract": "Authorship Attribution is the task of creating an appropriate characterization of text that captures the authors’ writing style to identify the original author of a given piece of text. With increased anonymity on the internet, this task has become increasingly crucial in various security and plagiarism detection fields. Despite significant advancements in other languages such as English, Spanish, and Chinese, Bangla lacks comprehensive research in this field due to its complex linguistic feature and sentence structure. Moreover, existing systems are not scalable with the increasing number of authors, and performance drops with the small number of samples per author. In this paper, we propose the use of Average-Stochastic Gradient Descent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an effective transfer learning approach that addresses the problem of complex linguistic features extraction and scalability for authorship attribution in Bangla Literature (AABL). We analyze the effect of different tokenization, such as word, sub-word, and character level tokenization, and demonstrate the effectiveness of these tokenizations in the proposed model. Moreover, we introduce the publicly available Bangla Authorship Attribution Dataset of 16 authors (BAAD16) containing 17,966 sample texts and 13.4+ million words to solve the standard dataset scarcity problem and release six variations of pre-trained language models for use in any Bangla NLP downstream task. For evaluation, we used our developed BAAD16 dataset as well as other publicly available datasets. Empirically, our proposed model outperformed state-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset. Furthermore, we showed that the proposed system scales much better with the increasing number of authors, and performance remains steady even with few training samples.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4224249876",
    "type": "article"
  },
  {
    "title": "Text Implicates Prosodic Ambiguity: A Corpus for Intention Identification of the Korean Spoken Language",
    "doi": "https://doi.org/10.1145/3529648",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "Won Ik Cho; Hyeon Seung Lee; Ji Won Yoon; Seok‐min Kim; Nam Soo Kim",
    "corresponding_authors": "",
    "abstract": "For a large portion of real-life utterances, the intention cannot be solely decided by either their semantic or syntactic characteristics. Although not all the sociolinguistic and pragmatic information can be digitized, at least phonetic features are indispensable in understanding the spoken language. Especially in head-final languages such as Korean, sentence-final prosody has great importance in identifying the speaker's intention. This paper suggests a system which identifies the inherent intention of a spoken utterance given its transcript, in some cases using auxiliary acoustic features. The main point here is a separate distinction for cases where discrimination of intention requires an acoustic cue. Thus, the proposed classification system decides whether the given utterance is a fragment, statement, question, command, or a rhetorical question/command, utilizing the intonation-dependency coming from the head-finality. Based on an intuitive understanding of the Korean language that is engaged in the data annotation, we construct a network which identifies the intention of a speech, and validate its utility with the test sentences. The system, if combined with up-to-date speech recognizers, is expected to be flexibly inserted into various language understanding modules.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4224286872",
    "type": "article"
  },
  {
    "title": "Event Detection via Context Understanding Based on Multi-task Learning",
    "doi": "https://doi.org/10.1145/3529388",
    "publication_date": "2022-04-07",
    "publication_year": 2022,
    "authors": "Jing Xia; Xiaolong Li; Yongbin Tan; Wu Zhang; Dajun Li; Zhengkun Xiong",
    "corresponding_authors": "",
    "abstract": "Event detection (ED) aims to identify events of interest described in the text. With the current explosive growth of text data on the internet, ED is increasingly practical and has gained many researchers’ attention. The existing works usually design ED as a token-level multi-class classification task. In this setting, given a sentence, ED models’ prediction for each token is relatively independent and thus cannot fully utilize sentence-level information and the association relations between multiple events in this sentence. To handle these situations, this paper proposes a multi-task learning based event detection model, which introduces an event type oriented text classification as an auxiliary task to improve the model’s understanding of sentence-level information. In addition, this model utilizes a Conditional Random Field (CRF) to explore the correlations between various event types and constrain the model’s output space. Experimental comparisons with state-of-the-art baselines on DuEE dataset demonstrate the model’s effectiveness.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4226412871",
    "type": "article"
  },
  {
    "title": "Exploiting Japanese–Chinese Cognates with Shared Private Representations for NMT",
    "doi": "https://doi.org/10.1145/3533429",
    "publication_date": "2022-05-05",
    "publication_year": 2022,
    "authors": "Zezhong Li; Fuji Ren; Xiao Sun; Degen Huang; Piao Shi",
    "corresponding_authors": "",
    "abstract": "Neural machine translation has achieved remarkable progress over the past several years; however, little attention has been paid to machine translation (MT) between Japanese and Chinese, which share a large proportion of cognate words that can be utilized as additional linguistic knowledge to enhance translation performance. In this article, we seek to strengthen the semantic correlation between Japanese and Chinese by leveraging cognate words that share common Chinese characters. Specifically, we experiment with three strategies: (1) a shared vocabulary with cognate lexicon induction, which models the commonality between source and target cognates; (2) a shared private representation with a dynamic gating mechanism, which models the language-specific features on the source side; and (3) an embedding shortcut, which enables the decoder to access the shared private representation with shortest distance and aids the training process. The experiments and analysis presented in this article demonstrate that our proposed approaches can significantly improve the performance of both Japanese-to-Chinese and Chinese-to-Japanese translations and verify the effectiveness of exploiting Japanese–Chinese cognates for MT.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4229056828",
    "type": "article"
  },
  {
    "title": "A Preliminary Analysis on the Correlates of Stress and Tones in Mizo",
    "doi": "https://doi.org/10.1145/3546950",
    "publication_date": "2022-07-06",
    "publication_year": 2022,
    "authors": "Esther Ramdinmawii; Sanghamitra Nath",
    "corresponding_authors": "",
    "abstract": "Stress is the property of a language to exhibit prominence or distinction in one or more syllables in a given domain. The existence of word stress has not been suitably explored in previous acoustic studies of the Mizo language, which is a tonal language of the Kuki-Chin sub-category in Tibeto-Burman language families. In this study, we attempt to analyze word stress on disyllabic target words, specifically in three lexical categories— adjectives, nouns, and verbs . Utterances of the target words are recorded in isolated setting (out of focus) and in sentence frames (in focus). First, averages of features, namely— duration, intensity, F0, formants , and spectral tilt , are extracted and investigated for identification of stressed and unstressed syllables on a total of 2,880 samples. Next, the interaction of word stress on the four tones of Mizo is investigated. While it is found that H-tone is generally stressed, inferences are made that stressed syllables are not unique to a specific tone. Third, significance of the selected features are validated using a two-tailed paired sample t -test. Our analysis indicates that the mean differences in duration, intensity, and F0 of the stressed and unstressed syllables are significant across the lexical categories at p &lt; 0.05. Next, validations on the significance of the mean differences are carried out using Cohen’s d effect size and Pearson’s Correlation Coefficient ( r ). Finally, three machine learning models—Support Vector Machines (SVM), Naive Baye’s, and Ensemble learning methods (AdaBoost and Boosted Aggregation), are used to identify stressed and unstressed syllables associated with tones in Mizo. Discriminating differences, especially in disyllabic verbs , are observed between stressed vs. unstressed syllables. Conclusions are drawn that duration is a strong and robust cue for acoustic correlates of stress, while intensity is a medium cue for stress and F0 a weak cue for stress.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4286630721",
    "type": "article"
  },
  {
    "title": "Composing Word Embeddings for Compound Words Using Linguistic Knowledge",
    "doi": "https://doi.org/10.1145/3561299",
    "publication_date": "2022-09-07",
    "publication_year": 2022,
    "authors": "Kanako Komiya; Shinji Kono; Takumi Seito; Teruo Hirabayashi",
    "corresponding_authors": "",
    "abstract": "In recent years, the use of distributed representations has been a fundamental technology for natural language processing. However, Japanese has multiple compound words, and often we must compare the meanings of a word and a compound word. Moreover, word boundaries in Japanese are unspecific because Japanese does not have delimiters between words, e.g., “ぶどう狩り” (grape picking) is one word according to one dictionary, whereas “ぶどう” and “狩り” are different words according to another dictionary. This study describes an attempt to compose word embeddings of a compound word from its constituent words in Japanese. We used “short unit” and “long unit,” both of which are the units of terms in UniDic—a Japanese dictionary compiled by the National Institute for Japanese Language and Linguistics—for constituent and compound words, respectively. Furthermore, we composed a word embedding of a compound word from the word embeddings of two constituent words using a neural network. The training data for the word embedding of compound words was created using a corpus generated by concatenating the corpora divided by constituent and compound words. We propose using linguistic knowledge for compositing word embedding to demonstrate how it improves the composition performance. We compared cosine similarity between composed and correct word embeddings of compound words to assess models with and without linguistic knowledge. Furthermore, we evaluated our methods by the ranking of synonyms using a thesaurus. We compared several frameworks and algorithms that use three types of linguistic knowledge—semantic patterns, parts of speech patterns, and compositionality score—and then investigated which linguistic knowledge improves the composition performance. The experiments demonstrated that the multitask models with the classification task of the parts of speech patterns and the estimation task of compositionality scores achieved high performances.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4294875713",
    "type": "article"
  },
  {
    "title": "Impact of Similarity Measures in Graph-based Automatic Text Summarization of Konkani Texts",
    "doi": "https://doi.org/10.1145/3554943",
    "publication_date": "2022-09-13",
    "publication_year": 2022,
    "authors": "Jovi D’Silva; Uzzal Sharma",
    "corresponding_authors": "",
    "abstract": "Automatic text summarization is a popular area in Natural Language Processing and Machine Learning. In this work, we adopt a graph-based text summarization approach, using PageRank algorithm, for automatically summarizing Konkani text documents. Konkani is an Indo-Aryan language spoken primarily in the state of Goa, which is on the west coast of India. It is a low-resource language with limited language processing tools. Such tools are readily available in other popular languages of choice for automatic text summarization, like English. The Konkani language dataset used for this purpose is based on Konkani folktales. We examine the impact of various language-independent and language-dependent similarity measures on the construction of the graph. The language-dependent similarity measures use pre-trained fastText word embeddings. A fully connected undirected graph is constructed for each document with the sentences represented as the graph's vertices. The vertices are connected to each other based on how strongly they are related to one another. Thereafter, PageRank algorithm is used for ranking the scores of the vertices. The top-ranking sentences are used to generate the summary. ROUGE toolkit was used for evaluating the quality of these system-generated summaries, and the performance was evaluated against human generated “gold-standard” abstracts and also compared with baselines and benchmark systems. The experimental results show that language-independent similarity measures performed well compared to language-dependent similarity measures despite not using language-specific tools, such as stop-words list, stemming, and word embeddings.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4296592066",
    "type": "article"
  },
  {
    "title": "Analyzing Variations of Everyday Japanese Conversations Based on Semantic Labels of Functional Expressions",
    "doi": "https://doi.org/10.1145/3552310",
    "publication_date": "2022-08-09",
    "publication_year": 2022,
    "authors": "Yuya Chiba; Ryuichiro Higashinaka",
    "corresponding_authors": "",
    "abstract": "To achieve effective dialogue processing, the kinds of daily conversations people have must be clarified. Unfortunately, the characteristics of everyday conversations remain insufficiently investigated. In recent years, the Corpus of Everyday Japanese Conversation (CEJC) was developed, which is a large-scale corpus constructed by recording everyday Japanese conversations. In this article, we investigate the linguistic variations of everyday conversations in a multitude of situations using CEJC. We conducted factor analysis of it using the semantic categories of functional expressions that represent such subjective information as modality, thoughts, and communicative intention in addition to various tenses and facts. Our analysis identified seven factors that characterize everyday conversations and suggests that they are expressed by a combination of a dialogue’s purpose (e.g., “Explanation” and “Suggestion”) and its manners (e.g., “Politeness” and “Involvement”). We also analyzed the BTSJ–Japanese natural conversation corpus with transcripts and recordings and the Nagoya University conversational corpus and confirmed the generalizability of these factors.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4297816466",
    "type": "article"
  },
  {
    "title": "BSML: Bidirectional Sampling Aggregation-based Metric Learning for Low-resource Uyghur Few-shot Speaker Verification",
    "doi": "https://doi.org/10.1145/3564782",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Yunfei Zi; Shengwu Xiong",
    "corresponding_authors": "",
    "abstract": "In recent years, text-independent speaker verification has remained a hot research topic, especially for the limited enrollment and/or test data. At the same time, due to the lack of sufficient training data, the study of low-resource few-shot speaker verification makes the models prone to overfitting and low accuracy of recognition. Therefore, a bidirectional sampling aggregation-based meta-metric learning method is proposed to solve the low-accuracy problem of speaker recognition in a low-resource environment with limited data, termed bidirectional sampling multi-scale Fisher feature fusion (BSML). First, the BSML method was used for effective feature enhancement in the feature extraction stage; second, a large number of similar and disjoint tasks were used to train the models to learn how to compare sample similarity; finally, new tasks were used to identify unknown samples by calculating the similarity of the samples. Extensive experiments are conducted on a short-duration text-independent speaker verification dataset generated from the THUYG-20 low-resource Uyghur with limited data, which comprised speech samples of diverse lengths. The experimental result has shown that the metric learning approach is effective in avoiding model overfitting and improving model generalization, with significant results in the identification of short-duration speaker verification in low-resource Uyghur with few-shot. It also demonstrates that BSML outperforms the state-of-the-art deep-embedding speaker recognition architectures and recent metric learning approach by at least 18%–67% in the few-shot test set. The ablation experiments further illustrate that our proposed approaches can achieve substantial improvement over prior methods and achieves better performance and generalization ability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4298005128",
    "type": "article"
  },
  {
    "title": "Surface Realization Architecture for Low-resourced African Languages",
    "doi": "https://doi.org/10.1145/3567594",
    "publication_date": "2022-10-12",
    "publication_year": 2022,
    "authors": "Zola Mahlaza; C. Maria Keet",
    "corresponding_authors": "",
    "abstract": "There has been growing interest in building surface realization systems to support the automatic generation of text in African languages. Such tools focus on converting abstract representations of meaning to a text. Since African languages are low-resourced, economical use of resources and general maintainability are key considerations. However, there is no existing surface realizer architecture that possesses most of the maintainability characteristics (e.g., modularity, reusability, and analyzability) that will lead to maintainable software that can be used for the languages. Moreover, there is no consensus surface realization architecture created for other languages that can be adapted for the languages in question. In this work, we solve this by creating a novel surface realizer architecture suitable for low-resourced African languages that abides by the features of maintainable software. Its design comes after a granular analysis, classification, and comparison of the architectures used by 77 existing NLG systems. We compare our architecture to existing architectures and show that it supports the most features of a maintainable software product.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4304693673",
    "type": "article"
  },
  {
    "title": "Automatically Temporal Labeled Data Generation Using Positional Lexicon Expansion for Focus Time Estimation of News Articles",
    "doi": "https://doi.org/10.1145/3568164",
    "publication_date": "2022-10-19",
    "publication_year": 2022,
    "authors": "Usman Ahmed; Jerry Chun‐Wei Lin; Vicente García‐Díaz",
    "corresponding_authors": "",
    "abstract": "Many facts change over time, which is a fundamental aspect of our physical environment. In the case of pandemic articles, the user is not interested in the creation date of the document but in the facts and the cause of the last pandemic. Fake news can be better combated by having a document with a temporal focus. Currently, neither the sequence of events nor the temporal focus is considered when obtaining news documents. Despite the limited number of temporal aspects in the available datasets, it is difficult to test and evaluate the temporal conclusions of the model. The goal of this work is to develop a temporal focus news article retrieval model based on co-training to advance research in semi-supervised learning. A mapping of the dataset is performed using (1) the evolving focus time of news articles and (2) the semi-supervised method based on coincidence contexts for learning low-dimensional continuous vectors for learning neural contrast embedding models generating focus time-based query in sequential news articles to facilitate temporal understanding by learning low-dimensional continuous vectors. A diverse dataset of news articles is used to evaluate the effectiveness of the proposed method. With semi-supervised learning and lexicon expansion, the result of the developed model can achieve 89%. The method performed better than previous baselines and traditional machine learning models with improvements of 12.65% and 4.7%, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4306790013",
    "type": "article"
  },
  {
    "title": "A Span-based Target-aware Relation Model for Frame-semantic Parsing",
    "doi": "https://doi.org/10.1145/3569581",
    "publication_date": "2022-10-27",
    "publication_year": 2022,
    "authors": "Xuefeng Su; Ru Li; Xiaoli Li; Baobao Chang; Zhiwei Hu; Xiaoqi Han; Zhichao Yan",
    "corresponding_authors": "",
    "abstract": "Frame-semantic Parsing (FSP) is a challenging and critical task in Natural Language Processing (NLP). Most of the existing studies decompose the FSP task into frame identification (FI) and frame semantic role labeling (FSRL) subtasks, and adopt a pipeline model architecture that clearly causes error propagation problem. However, recent jointly learning models aim to address the above problem and generally treat FSP as a span-level structured prediction task, which, unfortunately, leads to cascading error propagation problem between roles and less-efficient solutions due to huge search space of roles. To address these problems, we reformulate the FSRL task into a target-aware relation classification task and propose a novel and lightweight jointly learning framework that simultaneously processes three subtasks of FSP, including frame identification, argument identification, and role classification. The novel task formulation and jointly learning with interaction mechanisms among subtasks can help improve the overall system performance and reduce the search space and time complexity, compared with existing methods. Extensive experimental results demonstrate that our proposed model significantly outperforms 10 state-of-the-art models in terms of F1 score across two benchmark datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4307366247",
    "type": "article"
  },
  {
    "title": "Morphologically Motivated Input Variations and Data Augmentation in Turkish-English Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3571073",
    "publication_date": "2022-11-11",
    "publication_year": 2022,
    "authors": "Zeynep Yi̇rmi̇beşoğlu; Tunga Güngör",
    "corresponding_authors": "",
    "abstract": "Success of neural networks in natural language processing has paved the way for neural machine translation (NMT), which rapidly became the mainstream approach in machine translation. Significant improvement in translation performance has been achieved with breakthroughs such as encoder-decoder networks, attention mechanism, and Transformer architecture. However, the necessity of large amounts of parallel data for training an NMT system and rare words in translation corpora are issues yet to be overcome. In this article, we approach NMT of the low-resource Turkish-English language pair. We employ state-of-the-art NMT architectures and data augmentation methods that exploit monolingual corpora. We point out the importance of input representation for the morphologically rich Turkish language and make a comprehensive analysis of linguistically and non-linguistically motivated input segmentation approaches. We prove the effectiveness of morphologically motivated input segmentation for the Turkish language. Moreover, we show the superiority of the Transformer architecture over attentional encoder-decoder models for the Turkish-English language pair. Among the employed data augmentation approaches, we observe back-translation to be the most effective and confirm the benefit of increasing the amount of parallel data on translation quality. This research demonstrates a comprehensive analysis on NMT architectures with different hyperparameters, data augmentation methods, and input representation techniques, and proposes ways of tackling the low-resource setting of Turkish-English NMT.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4308804398",
    "type": "article"
  },
  {
    "title": "Neural Variational Gaussian Mixture Topic Model",
    "doi": "https://doi.org/10.1145/3578583",
    "publication_date": "2022-12-29",
    "publication_year": 2022,
    "authors": "Yi-Kun Tang; Heyan Huang; Xuewen Shi; Xian-Ling Mao",
    "corresponding_authors": "",
    "abstract": "Neural variational inference-based topic modeling has gained great success in mining abstract topics from documents. However, these topic models usually mainly focus on optimizing the topic proportions for documents, while the quality and the internal construction of topics are usually neglected. Specifically, these models lack the guarantee that semantically related words are supposed to be assigned to the same topic and are difficult to ensure the interpretability of topics. Moreover, many topical words recur frequently in the top words of different topics, which makes the learned topics semantically redundant and similar, and of little significance for further study. To solve the above problems, we propose a novel neural topic model called Neural Variational Gaussian Mixture Topic Model (NVGMTM). We use Gaussian distribution to depict the semantic relevance between words in the topics. Each topic in NVGMTM is considered as a multivariate Gaussian distribution over words in the word-embedding space. Thus, semantically related words share similar probabilities in each topic, which makes the topics more coherent and interpretable. Experimental results on two public corpora show the proposed model outperforms the state-of-the-art baselines.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4313254486",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Chinese Spell Checking",
    "doi": "https://doi.org/10.1145/2818354",
    "publication_date": "2015-11-11",
    "publication_year": 2015,
    "authors": "Lung‐Hao Lee; Gina‐Anne Levow; Shih-Hung Wu; Chao-Lin Liu",
    "corresponding_authors": "",
    "abstract": "This special issue contains four articles based on and expanded from systems presented at the SIGHAN-7 Chinese Spelling Check Bakeoff. We provide an overview of the approaches and designs for Chinese spelling checkers presented in these articles. We conclude this introductory article with a summary of possible future directions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2294016735",
    "type": "article"
  },
  {
    "title": "Constructing Complex Search Tasks with Coherent Subtask Search Goals",
    "doi": "https://doi.org/10.1145/2742547",
    "publication_date": "2015-12-11",
    "publication_year": 2015,
    "authors": "Ting-Xuan Wang; Wen‐Hsiang Lu",
    "corresponding_authors": "",
    "abstract": "Nowadays, due to the explosive growth of web content and usage, users deal with their complex search tasks by web search engines. However, conventional search engines consider a search query corresponding only to a simple search task. In order to accomplish a complex search task, which consists of multiple subtask search goals, users usually have to issue a series of queries. For example, the complex search task “travel to Dubai” may involve several subtask search goals, including reserving hotel room, surveying Dubai landmarks, booking flights, and so forth. Therefore, a user can efficiently accomplish his or her complex search task if search engines can predict the complex search task with a variety of subtask search goals. In this work, we propose a complex search task model (CSTM) to deal with this problem. The CSTM first groups queries into complex search task clusters, and then generates subtask search goals from each complex search task cluster. To raise the performance of CSTM, we exploit four web resources including community question answering, query logs, search engine result pages, and clicked pages. Experimental results show that our CSTM is effective in identifying the comprehensive subtask search goals of a complex search task.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2296985027",
    "type": "article"
  },
  {
    "title": "Adaptation of Language Models for SMT Using Neural Networks with Topic Information",
    "doi": "https://doi.org/10.1145/2816816",
    "publication_date": "2016-01-22",
    "publication_year": 2016,
    "authors": "Yinggong Zhao; Shujian Huang; Xinyu Dai; Jiajun Chen",
    "corresponding_authors": "",
    "abstract": "Neural network language models (LMs) are shown to be effective in improving the performance of statistical machine translation (SMT) systems. However, state-of-the-art neural network LMs usually use words before the current position as context and neglect global topic information, which can help machine translation (MT) systems to select better translation candidates from a higher perspective. In this work, we propose improvement of the state-of-the-art feedforward neural language model with topic information. Two main issues need to be tackled when adding topics into neural network LMs for SMT: one is how to incorporate topics to the neural network; the other is how to get target-side topic distribution before translation. We incorporate topics by appending topic distribution to the input layer of a feedforward LM. We adopt a multinomial logistic-regression (MLR) model to predict the target-side topic distribution based on source side information. Moreover, we propose a feedforward neural network model to learn joint representations on the source side for topic prediction. LM experiments demonstrate that the perplexity on validation set can be greatly reduced by the topic-enhanced feedforward LM, and the prediction of target-side topics can be improved dramatically with the MLR model equipped with the joint source representations. A final MT experiment, conducted on a large-scale Chinese--English dataset, shows that our feedforward LM with predicted topics improves the translation performance against a strong baseline.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2332031589",
    "type": "article"
  },
  {
    "title": "A “Suggested” Picture of Web Search in Turkish",
    "doi": "https://doi.org/10.1145/2891105",
    "publication_date": "2016-05-16",
    "publication_year": 2016,
    "authors": "Erdem Sarigil; Oğuz Yılmaz; İsmail Sengör Altıngövde; Rifat Ozcan; Özgür Ulusoy",
    "corresponding_authors": "",
    "abstract": "Although query log analysis provides crucial insights about Web users’ search interests, conducting such analyses is almost impossible for some languages, as large-scale and public query logs are quite scarce. In this study, we first survey the existing query collections in Turkish and discuss their limitations. Next, we adopt a novel strategy to obtain a set of Turkish queries using the query autocompletion services from the four major search engines and provide the first large-scale analysis of Web queries and their results in Turkish.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2394919552",
    "type": "article"
  },
  {
    "title": "Understanding Document Semantics from Summaries",
    "doi": "https://doi.org/10.1145/2956236",
    "publication_date": "2016-11-18",
    "publication_year": 2016,
    "authors": "Karthik Krishnamurthi; Vijayapal Reddy Panuganti; Vishnu Vardhan Bulusu",
    "corresponding_authors": "",
    "abstract": "Summary of a document contains words that actually contribute to the semantics of the document. Latent Semantic Analysis (LSA) is a mathematical model that is used to understand document semantics by deriving a semantic structure based on patterns of word correlations in the document. When using LSA to capture semantics from summaries, it is observed that LSA performs quite well despite being completely independent of any external sources of semantics. However, LSA can be remodeled to enhance its capability to analyze correlations within texts. By taking advantage of the model being language independent, this article presents two stages of LSA remodeling to understand document semantics in the Indian context, specifically from Hindi text summaries. One stage of remodeling is done by providing supplementary information, such as document category and domain information. The second stage of remodeling is done by using a supervised term weighting measure in the process. The remodeled LSA’s performance is empirically evaluated in a document classification application by comparing the accuracies of classification to plain LSA. An improvement in the performance of LSA in the range of 4.7% to 6.2% is achieved from the remodel when compared to the plain model. The results suggest that summaries of documents efficiently capture the semantic structure of documents and is an alternative to full-length documents for understanding document semantics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2552693086",
    "type": "article"
  },
  {
    "title": "Bravely Say I Don’t Know: Relational Question-Schema Graph for Text-to-SQL Answerability Classification",
    "doi": "https://doi.org/10.1145/3579030",
    "publication_date": "2023-01-04",
    "publication_year": 2023,
    "authors": "Wei Yu; Haiyan Yang; Mengzhu Wang; Xiaodong Wang",
    "corresponding_authors": "",
    "abstract": "Recently, the Text-to-SQL task has received much attention. Many sophisticated neural models have been invented that achieve significant results. Most current work assumes that all the inputs are legal and the model should generate an SQL query for any input. However, in the real scenario, users are allowed to enter the arbitrary text that may not be answered by an SQL query. In this article, we focus on the issue–answerability classification for the Text-to-SQL system, which aims to distinguish the answerability of the question according to the given database schema. Existing methods concatenate the question and the database schema into a sentence, then fine-tune the pre-trained language model on the answerability classification task. In this way, the database schema is regarded as sequence text that may ignore the intrinsic structure relationship of the schema data, and the attention that represents the correlation between the question token and the database schema items is not well designed. To this end, we propose a relational Question-Schema graph framework that can effectively model the attention and relation between question and schema. In addition, a conditional layer normalization mechanism is employed to modulate the pre-trained language model to generate better question representation. Experiments demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state of the art on the benchmark TRIAGESQL. Specifically, the model attains 88.41%, 78.24%, and 75.98% in Precision, Recall, and F1, respectively. Additionally, it outperforms the baseline by approximately 4.05% in Precision, 6.96% in Recall, and 6.01% in F1.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4313591655",
    "type": "article"
  },
  {
    "title": "Bidirectional Sentence Ordering with Interactive Decoding",
    "doi": "https://doi.org/10.1145/3561510",
    "publication_date": "2023-01-19",
    "publication_year": 2023,
    "authors": "Guirong Bai; Shizhu He; Kang Liu; Jun Zhao",
    "corresponding_authors": "",
    "abstract": "Sentence ordering aims at restoring orders of shuffled sentences in a paragraph. Previous methods usually predict orders in a single direction, i.e., from head to tail. However, unidirectional prediction inevitably causes error accumulation, which restricts performance. In this article, we propose a bidirectional ordering method, which predicts orders in both head-to-tail and tail-to-head directions at the same time. In our bidirectional ordering method, two directions can interact with each other and help alleviate the error accumulation problem of ordering. Experiments demonstrate that our method can effectively improve performance of previous models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4317503531",
    "type": "article"
  },
  {
    "title": "User Interest Identification with Social Media Information using Natural Language and Meta-Heuristic Technique",
    "doi": "https://doi.org/10.1145/3579165",
    "publication_date": "2023-02-02",
    "publication_year": 2023,
    "authors": "Jiangbo Zheng; Ying Liang",
    "corresponding_authors": "",
    "abstract": "As the number of Internet users and social networking apps has grown in recent years, interest-based recommendation systems have been more commonly used in practice. Given the vast quantity of data available from LinkedIn and Twitter, as well as the expanding number of users, it was critical to create a real-time framework for recommending and monitoring relevant tweets or posts based on the user's interests. Using association rules, the interests of social network users can be uncovered. A considerable number of association rules extracted from social networks were found to be mostly dependent on coverage requirements. After finding patterns of frequent and non-frequent patterns, a large number of non-frequent terms were eliminated in association rule mining. In order to reduce the complexity of the association rule mining process, the more relevant terms are selected by the Hybridized Competitive Swarm Optimizer and Gravitational Search Algorithm (CSO-GSA), which is utilized for association rule generation and classification using deep learning techniques. In this research, numerous relevant rules for human interest are identified. The numerical outcome of the proposed strategy is compared with existing state-of-the-art techniques. The proposed CSO with GSA outperforms the existing techniques.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4318990744",
    "type": "article"
  },
  {
    "title": "Modelling of Speech Parameters of Punjabi by Pre-trained Deep Neural Network Using Stacked Denoising Autoencoders",
    "doi": "https://doi.org/10.1145/3568308",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Navdeep Kaur; Parminder Singh",
    "corresponding_authors": "",
    "abstract": "Statistical parametric speech synthesis techniques such as deep neural network (DNN) and hidden Markov model (HMM) have grown in popularity since last decade over the concatenative speech synthesis approaches by modelling excitation and spectral parameters of speech to synthesize the waveforms from the written text. Due to inappropriate acoustic modelling, speech synthesized using HMM-based speech synthesis sounds muffled. DNN tried to improve the acoustic model by replacing decision trees in HMM with powerful regression model. Further, the performance of a deep neural network is greatly enhanced using pre-learning either restricted Boltzmann machines (RBM) or autoencoders. RBMs are capable to map multi-modal property of speech but result in spectral distortion of synthesized speech waveforms as non-consideration of reconstruction error. This article proposed the model of deep neural network, which is pre-trained using stacked denoising autoencoders to map speech parameters of the Punjabi language. Denoising autoencoders work by adding noise in the training data and then reconstructing the original measurements to reduce the reconstruction error. The synthesized voice using the proposed model showed the VARN of 0.82, F0 RMSE (Hz) 9.03, and V/UV error rate of 4.04% have been observed.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4319964304",
    "type": "article"
  },
  {
    "title": "Better Localness for Non-Autoregressive Transformer",
    "doi": "https://doi.org/10.1145/3587266",
    "publication_date": "2023-03-11",
    "publication_year": 2023,
    "authors": "Shuheng Wang; Heyan Huang; Shumin Shi",
    "corresponding_authors": "",
    "abstract": "The Non-Autoregressive Transformer, due to its low inference latency, has attracted much attention from researchers. Although, the performance of the non-autoregressive transformer has been significantly improved in recent years, there is still a gap between the non-autoregressive transformer and the autoregressive transformer. Considering the success of localness on the autoregressive transformer, in this work, we consider incorporating localness into the non-autoregressive transformer. Specifically, we design a dynamic mask matrix according to the query tokens, key tokens, and relative distance, and unify the localness module for self-attention and cross-attention module. We conduct experiments on several benchmark tasks, and the results show that our model can significantly improve the performance of the non-autoregressive transformer.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4323923967",
    "type": "article"
  },
  {
    "title": "BayesKGR: Bayesian Few-Shot Learning for Knowledge Graph Reasoning",
    "doi": "https://doi.org/10.1145/3589183",
    "publication_date": "2023-03-27",
    "publication_year": 2023,
    "authors": "Feng Zhao; Cheng Yan; Hai Jin; Lifang He",
    "corresponding_authors": "",
    "abstract": "Reasoning over knowledge graphs (KGs) has received increasing attention recently due to its promising applications in many areas, such as semantic search and recommendation systems. Subsequently, most reasoning models are inherently transductive and ignore uncertainties of KGs, making it difficult to generalize to unseen entities. Moreover, existing approaches usually require each entity in the KG to have sufficient training samples, which leads to the overfitting of the entity having few instances. In fact, long-tail distributions are quite widespread in KGs, and newly emerging entities will tend to have only a few related triples. In this work, we aim at studying knowledge graph reasoning under a challenging setting where only limited training samples are available. Specifically, we propose a Bayesian inductive reasoning method and incorporate meta-learning techniques in few-shot learning to solve data deficiency and uncertainties. We design a Bayesian graph neural network as a meta-learner to achieve Bayesian inference, which can extrapolate meta-knowledge from observed KG to emerging entities. We conduct extensive experiments on two large-scale benchmark datasets, and the results demonstrate considerable performance improvement with the proposed approach over other baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4361003467",
    "type": "article"
  },
  {
    "title": "Semi-Supervised Semantic Role Labeling with Bidirectional Language Models",
    "doi": "https://doi.org/10.1145/3587160",
    "publication_date": "2023-04-01",
    "publication_year": 2023,
    "authors": "Kashif Munir; Hai Zhao; Zuchao Li",
    "corresponding_authors": "",
    "abstract": "The recent success of neural networks in NLP applications has provided a strong impetus to develop supervised models for semantic role labeling (SRL) that forego the requirement for extensive feature engineering. Recent state-of-the-art approaches require high-quality annotated datasets that are costly to obtain and almost unavailable for low-resource languages. We present a semi-supervised approach that utilizes both labeled and unlabeled data to provide performance improvement over a mere supervised SRL model. We show that our proposed semi-supervised SRL model provides larger improvement over a supervised model in the scenario where labeled training data size is small. Our SRL system leverages unlabeled data under the language modeling paradigm. We demonstrate that the incorporation of a self pre-trained bidirectional language model (S-PrLM) into a SRL system can help in SRL performance improvement by learning composition functions from the unlabeled data. Previous researches have concluded that syntax information is very useful for high-performing SRL systems, so we incorporate syntax information by employing an unsupervised approach to leverage dependency path information to connect argument candidates in vector space, which helps in distinguishing arguments with similar contexts but different syntactic functions. The basic idea is to connect predicate ( w p ) with argument candidate ( w a ) with the dependency path ( r ) between them in the embedding space. Experiments on the CoNLL-2008 and CoNLL-2009 datasets confirm that our full SRL model outperforms previous best models in terms of F 1 score.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4362456320",
    "type": "article"
  },
  {
    "title": "Research on Korean Translation in the Context of Epidemic Prevention and Control",
    "doi": "https://doi.org/10.1145/3589640",
    "publication_date": "2023-03-31",
    "publication_year": 2023,
    "authors": "Zhiguo Wang; Chunxiao Ma",
    "corresponding_authors": "",
    "abstract": "An emergency like COVID-19 requires a theoretical framework for policy implementation that involves public and private sector collaborations. After policy failures, new institutions have formed that trigger PPP's later, allowing the incumbent administration to continue in office longer. It focuses on novel approaches to dealing with pandemics. The present administration put these rules in place to keep COVID-19 under control. When it comes to Real Time - polymerase chain reaction (RT-PCR) testing, South Korea's government and corporations partnered to swiftly raise the quantity of testing in the country. Models of policy change are shown to be dynamic, cyclical, and recursive. During the COVID-19 outbreak in South Korea, an empirical content research was conducted. Even though South Korea's leader was at risk of losing public support to the point where impeachment was mentioned as a possible option, he dramatically reversed public mood to win general elections by a wide margin in April 2020, while the pandemic scenario persisted. To win reelection, democratic administrations are under more pressure to effectively perform crisis management when faced with a crisis. As a result, they are under even more pressure to immediately mobilize public and private resources. The emergency use authorization (EUA) protocol for test kits is an example of \"leapfrogging actors\" – up-and-coming innovators – who helped turn a pandemic tragedy into a possibility for sustained leadership and for them. The results based on infected premises culling rate ratio is 82.3%, number of measles cases report is 86.4%, spread and epidemic ratio is 84.2%, important of epidemiology is 89.35%, transmission potential of COVID-19 is 91.24% and illustration of epidemic control is 92.45. The results based on infected premises culling rate ratio is 82.3%, number of measles cases report is 86.4%, spread and epidemic ratio is 84.2%, important of epidemiology is 89.35%, transmission potential of COVID-19 is 91.24% and illustration of epidemic control is 92.45.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4362470106",
    "type": "article"
  },
  {
    "title": "The Influence of Chinese Characters on Chinese Sign Language",
    "doi": "https://doi.org/10.1145/3591465",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Tianyu Ren; Dengfeng Yao; Chaoran Yang; Xinchen Kang",
    "corresponding_authors": "",
    "abstract": "Chinese Sign Language (CSL) and Chinese are languages used in the Chinese mainland. As a dominant language, Chinese has great influence on all levels of CSL. CSL, as a visual sign language, is fundamentally different from Chinese in linguistic structure. Unlike English, Chinese, as a pictograph, has influence on Chinese and CSL. This study explains in detail the influence of Chinese characters on CSL at the lexical level, including many elements from Chinese, such as “仿字 fangzi” (form imitating Chinese characters), “书空 shukong” (writing in the air with the index finger), loan translation, finger spelling, and mouthing patterns. This influence is not a simple borrowing of Chinese characters, but a creative imitation and adaptation according to the needs of sign language to express meaning. After a long period of evolution, the characteristics of Chinese characters are naturally integrated into CSL loanwords, which makes the relationship between sign language and Chinese characters closer. CSL borrows a large number of Chinese words, most of which are signs to express non-core concepts. These borrowed signs are an indispensable part of the CSL sign language family, enrich sign language vocabulary, improve the accuracy of sign language expression, and play a positive role in promoting the learning, work, and lives of deaf people.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4362720757",
    "type": "article"
  },
  {
    "title": "Semi-Supervised Lexicon-Aware Embedding for News Article Time Estimation",
    "doi": "https://doi.org/10.1145/3592604",
    "publication_date": "2023-04-13",
    "publication_year": 2023,
    "authors": "Usman Ahmed; Jerry Chun‐Wei Lin; Gautam Srivastava; Unil Yun",
    "corresponding_authors": "",
    "abstract": "In the information retrieval community, Temporal Information Retrieval (TIR) has become increasingly popular. Documents focused on the time surrounding their publication are more likely to be accurate and contain information relevant to the reader. In this study, we explore the inverted pyramid paradigm by extracting temporal expressions from news documents, standardizing their values, and evaluating them based on their position within the text. We present a lexicon expansion method that employs WordNet as input. This approach enhances the lexicon by grouping words with similar meanings, potentially improving the accuracy of event detection algorithms. Additionally, this process can introduce new words and phrases to the lexicon, expanding the vocabulary. Using each tagged dataset, a classifier is trained with a pre-trained network. A pool of unlabeled data are processed, and high-confidence pseudo-labels are assigned. Pseudo-labels are generated by leveraging the partially trained model and the original labelled data. As the classifier predicts the correct label for a data sample, the pseudo-labels of other data samples are updated, and vice versa. At the end of this process, the predictions from different matching classifiers are combined. It takes several rounds to label the unlabeled inputs using this method. To evaluate the proposed solutions, we conducted experiments on 4,500 online news articles relevant to temporal retrieval. LSTM, BiLSTM, and BERT models with and without lexicon expansion were assessed based on log loss and relative divergence of entropy. A jointly trained semi-supervised learning model achieved a mean KL divergence of 0.89, an F1 score of 0.74 for temporal events, and 0.63 for non-temporal events. Besides alleviating data sparsity issues and enabling the training of more complex networks, this technique can also serve as an alternative to data augmentation methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4365451534",
    "type": "article"
  },
  {
    "title": "Text Polishing with Chinese Idiom: Task, Datasets and Pre-trained Baselines",
    "doi": "https://doi.org/10.1145/3593806",
    "publication_date": "2023-04-21",
    "publication_year": 2023,
    "authors": "Junwei Liao; Shuai Cheng; Minghuan Tan",
    "corresponding_authors": "",
    "abstract": "This work presents the task of text polishing, which generates a sentence that is more graceful than the input sentence while retaining its semantic meaning. Text polishing has great value in real usage and is an important component in modern writing assistance systems. However, the task is still not well studied in the literature. Further research in this important direction requires more formal task definitions, benchmark datasets, and powerful baseline models. In this work, we formulate the task as a context-dependent text generation problem and conduct a case study on the text polishing with Chinese idiom. To circumvent the difficulties of task data annotation, we propose a semi-automatic data construction pipeline based on human-machine collaboration, and establish a large-scale text polishing dataset consisting of 1.5 million instances. We propose two types of task-specific pre-training objectives for the text polishing task and implement a series of Transformer-based models pre-trained on a massive Chinese corpus as baselines. We conduct extensive experiments with the baseline models on the constructed text polishing datasets and have some major findings. The human evaluation further reveals the polishing ability of the final system.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4366683619",
    "type": "article"
  },
  {
    "title": "MSA Speech Rhythm Pattern in a Multilingual Setting",
    "doi": "https://doi.org/10.1145/3593295",
    "publication_date": "2023-04-22",
    "publication_year": 2023,
    "authors": "Ghania Droua-Hamdani",
    "corresponding_authors": "Ghania Droua-Hamdani",
    "abstract": "This study examines variation in rhythm metrics in a multilingual setting by focusing on between-speaker differences. The investigation analyzes speech rhythm patterns of segmental durations in the speech of 77 Algerian speakers belonging to three educational background classes and three age groups. The experiment focuses on speech rhythm variability according to the level of educational background of the speakers and the language used in daily life. The gender and age of speakers are also analyzed. Results show that five vocalic rhythm metrics reflect the contrast between long and short vowels that was observed from the acoustic measurements. The statistical analysis reveals that rhythm metrics are sensitive to differences between groups of speakers, such as age and educational background. The outcomes also show that the lack of practice of Modern Standard Arabic by some speaker groups considerably affects vowel quantity.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4366768526",
    "type": "article"
  },
  {
    "title": "Editorial for the Special Issue on Computational Linguistics Processing in Low-Resource Indigenous Languages",
    "doi": "https://doi.org/10.1145/3591208",
    "publication_date": "2023-05-09",
    "publication_year": 2023,
    "authors": "Gautam Srivastava; Jerry Chun‐Wei Lin; Prof. Yu-Dong Zhang",
    "corresponding_authors": "",
    "abstract": "introduction Share on Editorial for the Special Issue on Computational Linguistics Processing in Low-Resource Indigenous Languages Editors: Gautam Srivastava Brandon University (Canada) Brandon University (Canada) 0000-0001-9851-4103Search about this author , Jerry Chun-Wei Lin Western Norway University of Applied Sciences (Norway) Western Norway University of Applied Sciences (Norway) 0000-0001-8768-9709Search about this author , Prof. Yu-Dong Zhang University of Leicester, UK University of Leicester, UK 0000-0002-4870-1493Search about this author Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingVolume 22Issue 5Article No.: 130pp 1–3https://doi.org/10.1145/3591208Published:09 May 2023Publication History 0citation29DownloadsMetricsTotal Citations0Total Downloads29Last 12 Months29Last 6 weeks29 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4375949401",
    "type": "article"
  },
  {
    "title": "Editorial: Ontology-based Knowledge Presentation and Computational Linguistics for Semantic Big Social Data Analytics in Asian Social Networks",
    "doi": "https://doi.org/10.1145/3594719",
    "publication_date": "2023-05-09",
    "publication_year": 2023,
    "authors": "Chinmay Chakraborty; Shaohua Wan; Mohammad R. Khosravi",
    "corresponding_authors": "",
    "abstract": "Data-driven ontology-based knowledge (OK) presentation and computational linguistics for evolving semantic Asian social networks (ASNs) can make one of the most important platforms that provide robust and real-time data mapping in massive access across the heterogeneous big data sources in the web that is named OK-ASN. It benefits from computational intelligence, web-of-things (WoT) architecture, semantic features, statistical learning and pattern recognition, database management, computer vision, cyber-security, and language processing. OK-ASN is a critical strategy for WoT big data mining and enterprises from social media to medical and industrial sectors.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4375949445",
    "type": "editorial"
  },
  {
    "title": "Low-Resource Language Based Cognitive Psychology in English Language Analysis: A State-of-art for China's Higher Vocational Colleges",
    "doi": "https://doi.org/10.1145/3588768",
    "publication_date": "2023-05-11",
    "publication_year": 2023,
    "authors": "Ai-e CAO; Fuhan Chen",
    "corresponding_authors": "",
    "abstract": "The present study aims at identifying the academic and professional needs of vocational students to enhance their English communication skills that are considered as the prerequisites for employment in China. In view of the demands of globalization, course designers and academicians design the curriculum to meet the challenges of the current scenario. This study aims at finding out the gap between student's needs and the existing syllabus to provide suggestions for the better implementation of the English communication skills course for vocational students by analysing the prescribed syllabus of Woyingzhichang and Zibo Vocational Institute in China. The data for analysis were collected from both students and teachers using a structured questionnaire. It is identified that student need fewer changes in syllabus and English teaching skills in the schools of China.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4376149826",
    "type": "article"
  },
  {
    "title": "Exploring Graph-based Transformer Encoder for Low-Resource Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3599969",
    "publication_date": "2023-05-25",
    "publication_year": 2023,
    "authors": "Long Nguyen; Binh P. Nguyen; Binh M. Le; Điền Đinh",
    "corresponding_authors": "Binh M. Le",
    "abstract": "The Transformer is commonly used in Neural Machine Translation (NMT), but it faces issues with over-parameterization in low-resource settings. This means that simply increasing the model parameters significantly will not lead to improved performance. In this study, we propose a graph-based approach that slightly increases the parameters while significantly outperforming the scaled version of the Transformer. We accomplish this by utilizing Graph Neural Networks to encode Universal Conceptual Cognitive Annotation (UCCA), allowing the linguistic features of UCCA to be incorporated into the word embeddings. This improves the performance of the NMT system since the word embedding is now more capable and informative. Experimental results demonstrate that the proposed method outperforms the scaled Transformer model by +0.4, +0.41, and +0.33 BLEU, respectively, in English-Vietnamese/French/Czech datasets. Furthermore, this method reduces the number of parameters by 47% when compared to the scaled Transformer. A thorough analysis of error patterns reveals that the proposed method provides structural awareness to translation systems. Our code is available at: https://github.com/nqbinh17/UCCA_GNN.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4378218513",
    "type": "article"
  },
  {
    "title": "Application of Psychological Effects in Intelligent Digital English Teaching Based on Multimodal Low Resource Language Information Processing from a Psychological Perspective",
    "doi": "https://doi.org/10.1145/3599726",
    "publication_date": "2023-05-25",
    "publication_year": 2023,
    "authors": "Lili Yang; Yue Zhang",
    "corresponding_authors": "",
    "abstract": "With the development of information technology, intelligent digital education is gradually emerging. At the same time, with the closer economic and trade relations between regions, the demand for English talents is becoming more and more urgent. Applying the concept and method of intelligent digital education to English Teaching (ET) is of great significance for improving students’ learning efficiency and academic level, and enhancing students’ English application ability and cross-cultural communication ability. However, due to the limitations of the traditional teaching model, such as the single and outdated model, the low interest of students in learning, and the lack of emphasis on cultivating students’ English practical ability, it cannot effectively meet the requirements of improving students’ learning effect. Therefore, this article studies the intelligent digital ET mode based on multimodal low-resource language information processing, proposes the construction method of intelligent digital ET mode, and conducts experimental research on it. The research showed that the positive emotional input level index of Group X students was 8.47% higher than that of Group T students; the efficiency index of Group X students was 5.02% higher than that of Group T students; the autonomous learning ability and English learning level of Group X students were higher than those of Group T; the students in Group X had higher recognition of ET mode than those in Group T. The intelligent digital ET mode can effectively improve the learning effect.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4378218582",
    "type": "article"
  },
  {
    "title": "Retrospective Multi-granularity Fusion Network for Chinese Idiom Cloze-style Reading Comprehension",
    "doi": "https://doi.org/10.1145/3603370",
    "publication_date": "2023-06-09",
    "publication_year": 2023,
    "authors": "Jianyu Yue; Yiwen Sun; Xiaojun Bi; Zheng Chen; Yu Zhang",
    "corresponding_authors": "",
    "abstract": "Chinese idiom cloze-style reading comprehension task is of great significance for improving the machine’s ability to understand Chinese idioms, which is one of the essential application requirements in advanced artificial intelligence. Existing methods suffer from an insufficient deep semantic understanding of the text. To solve this problem, this paper proposes a novel Retrospective Multi-granularity Fusion Network (RMFNet) for Chinese idiom cloze-style reading comprehension. Our RMFNet is equipped with two novel modules to model deeper contextual information of passage and Chinese idioms, respectively. First, we propose a novel Multi-granularity Passage Fusion (MgPF) module, which enhances the passage representation by integrating different semantic perspectives. Second, we propose a Retrospective Reading (Re \\(^2\\) ) module that implements a back-and-forth reading mechanism to concentrate on critical Chinese idioms, thereby generating an ultimate memory for the whole text. Notably, the intuition of the MgPF module and the Re \\(^2\\) module is based on human reading strategies in the real world. The strategies in these modules are similar to how humans perceive the text. Extensive experiments are conducted on Chinese benchmark datasets to evaluate the effectiveness and superiority of the proposed method. Our RMFNet achieves state-of-the-art performance and in-depth analysis verifies its capability for understanding the deep semantics of the text.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4380085693",
    "type": "article"
  },
  {
    "title": "IIITH-CSTD Corpus: Crowdsourced Strategies for the Collection of a Large-scale Telugu Speech Corpus",
    "doi": "https://doi.org/10.1145/3600228",
    "publication_date": "2023-06-12",
    "publication_year": 2023,
    "authors": "Ganesh S. Mirishkar; Vishnu Vidyadhara Raju; Meher Dinesh Naroju; Sudhamay Maity; Prakash Yalla; Anil Kumar Vuppala",
    "corresponding_authors": "",
    "abstract": "Due to the lack of a large annotated speech corpus, many low-resource Indian languages struggle to utilize recent advancements in deep neural network architectures for Automatic Speech Recognition (ASR) tasks. Collecting large-scale databases is an expensive and time-consuming task. Current approaches lack extensive traditional expert-based data acquisition guidelines, as they are tedious and complex. In this work, we present the International Institute of Information Technology Hyderabad-Crowd Sourced Telugu Database (IIITH-CSTD), a Telugu corpus collected through crowdsourcing. In particular, our main objective is to mitigate the low-resource problem for Telugu. We also present the sources, crowdsourcing pipeline, and the protocols used to collect the corpus for a low-resource language, namely, Telugu. Data of approximately 2,000 hours of transcribed audio is presented and released in this article, covering three major regional dialects of the Telugu language in three different (i.e., read, conversational and spontaneous) speaking styles on topics such as politics, sports, and arts, science, and so on. 1 We also present the experimental results of the collected corpus on ASR tasks. We hope this work will motivate researchers to curate large-scale annotated speech data for other low-resource Indic languages.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4380324324",
    "type": "article"
  },
  {
    "title": "A Systematic Review of Stemmers of Indian and Non-Indian Vernacular Languages",
    "doi": "https://doi.org/10.1145/3604612",
    "publication_date": "2023-06-14",
    "publication_year": 2023,
    "authors": "Nakul R. Dave; Mayuri A. Mehta; Ketan Kotecha",
    "corresponding_authors": "",
    "abstract": "The stemming process is crucial and significant in the pre-processing step of natural language processing. The stemmer oversees the stemming process. It facilitates the extraction of morphological variants of a root or base word from the provided word. Over the period, several stemmers for various vernacular languages have been proposed. However, very few research studies have comprehensively investigated these available stemmers. This article makes multifold contributions. First, we discuss the various stemmers of 15 Indian and 17 non-Indian languages describing their key points, benefits, and drawbacks. All the Indian languages for which stemmers have been built are covered in this study. For the non-Indian languages, stemmers of commonly spoken languages have been covered. Second, we present a language-wise comparative analysis of stemmers based on our identified parameters. Third, we discuss the wordnets and dictionaries available for different languages. Fourth, we provide details of the datasets available for various languages. Fifth, we also provide challenges in existing stemmers and future directions for future researchers. The study presented in this article reveals that significant research has been carried out for the stemmers of influential languages such as English, Arabic, and Urdu. On the other hand, languages with d resources, such as Farsi, Polish, Odia, Amharic, and others, have received the least attention for research. Moreover, rigorous analysis reveals that most of the stemmers suffer from over-stemming errors. With a complete catalogue of available stemmers, this study aims at assisting the researchers and professionals working in the areas such as information retrieval, semantic annotation, word meaning disambiguation, and ontology learning.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4380609126",
    "type": "review"
  },
  {
    "title": "A Bayesian Convolutional Neural Network Model with Uncertainty for Multi-label Text Classification on Mechanisms of Action (MoA) Prediction",
    "doi": "https://doi.org/10.1145/3604428",
    "publication_date": "2023-06-18",
    "publication_year": 2023,
    "authors": "Xuming Tong; Zhisheng Zhao; Junhua Liang; Lihua Ding; Caijun Jia; Yanhong Yuan",
    "corresponding_authors": "",
    "abstract": "With the development of scientific research techniques, drug discovery has shifted from the serendipitous approach of the past to more targeted models based on an understanding of the underlying biological mechanisms of disease. However, there are hundreds or more of mechanism of action (MoA) data in the known drugs, which makes this process faced with complicated multi-label classification of text data. Traditional multi-label text classification algorithms will increase the complexity of the model and reduce the accuracy as the number of labels increases. Although deep learning algorithms can solve the problem of model complexity, they are currently only suitable for processing image format data. To overcome these problems, this study proposes a multi-label classification method based on Bayesian deep learning, which can convert non-image data format into image data, making it suitable for Convolutional neural network algorithm requirements. Then in the PyTorch environment, the Bayesian deep learning algorithm and the EfficientNet convolutional neural network are perfectly combined using the BLiTZ library to construct the Bayesian convolutional neural network model which named BCNNM. Not only improves the classification efficiency, this method also solves the problem of imbalanced classification of multi-label data, and fully considers the uncertainty in the neural network. In the process of drug development, this method has important practical significance for processing the multi-label classification of MoA data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4381123980",
    "type": "article"
  },
  {
    "title": "Improving Indic code-mixed to monolingual translation using Mixed Script Augmentation, Generation &amp; Transfer Learning",
    "doi": "https://doi.org/10.1145/3606695",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Rajat Subhra Bhowmick; Isha Ganguli; Ananya Paul; Jayanta Paul; Jaya Sil",
    "corresponding_authors": "",
    "abstract": "The use of code-mixed languages (written in Roman character) on social media platforms is prevalent in multilingual nations. Translation from code-mixed to monolingual is necessary for social media analysis, content filtering, and targeted advertising. Training translation models from scratch is difficult due to the scarcity of available code-mixed resources and the extremely noisy nature of real-time code-mixed sentences. At the moment, multilingual state-of-the-art language models are routinely used for multilingual applications. However, multilingual models are ineffective in handling code-mixed sentences as it is usually written in Roman script but contain words from at least two languages. In the paper, two data augmentation techniques are proposed to improve code-mixed to monolingual translation, one based on script augmentation and the other on code-mixed sentence generation. The proposed approach converts the code-mixed sentences into ‘Mixed Script form’ that restore the native language words in the sentences with corresponding native language scripts. The novelty of the work is that the multilingual language models include each language’s linguistic competence, preserving context in the monolingual sentences, not possible in the earlier models. Using an mT5 model, denoising and mixed-script switching are performed, followed by monolingual translation with another mT5 model. Code-mixed sentences are generated by employing a simple code-mixed sentence generating technique using monolingual parallel inputs. Two different Indic language sets, namely Hindi-English and Bengali-English are applied and in each case, the proposed approach outperforms straight uni-script (Roman) code-mixed to monolingual translation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4383067474",
    "type": "article"
  },
  {
    "title": "Query-by-Example Spoken Term Detection for Zero-Resource Languages Using Heuristic Search",
    "doi": "https://doi.org/10.1145/3609505",
    "publication_date": "2023-07-15",
    "publication_year": 2023,
    "authors": "P. Sudhakar; Sreenivasa Rao K; Pabitra Mitra",
    "corresponding_authors": "",
    "abstract": "Query-by-Example spoken content retrieval is a demanding and challenging task when a large volume of spoken content is piled up in the repositories without annotation. In the absence of annotation, spoken content retrieval is achieved by capturing the similarities between the query and spoken terms from the acoustic feature representation itself. Dynamic Time Warping (DTW) centric techniques identify the optimal alignment between the acoustic feature representations and capture the similarities between query and spoken terms. Despite feasibility, the DTW-centric techniques produce a lot of false alarms due to the variabilities that exist in natural speech and degrade the performance. In the proposed approach, the variability challenges are addressed in two stages. At first, the speaker-independent acoustic feature representation was obtained from the deep convolutional neural networks that reduce the speaker variabilities. In the second stage, the similarities between the query and spoken term were captured using the heuristic search method. The proposed approach was compared with other state-of-the-art methods using Microsoft Low-Resource Language speech corpus. A 3% improvement and 32% reduction in the hit and false alarm ratio were achieved across languages.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4384407239",
    "type": "article"
  },
  {
    "title": "North Korean Neural Machine Translation through South Korean Resources",
    "doi": "https://doi.org/10.1145/3608947",
    "publication_date": "2023-07-19",
    "publication_year": 2023,
    "authors": "Hwichan Kim; Tosho Hirasawa; Sangwhan Moon; Naoaki Okazaki; Mamoru Komachi",
    "corresponding_authors": "",
    "abstract": "South and North Korea both use the Korean language. However, Korean natural language processing (NLP) research has mostly focused on South Korean language. Therefore, existing NLP systems in the Korean language, such as neural machine translation (NMT) systems, cannot properly process North Korean inputs. Training a model using North Korean data is the most straightforward approach to solving this problem, but the data to train NMT models are insufficient. To solve this problem, we constructed a parallel corpus to develop a North Korean NMT model using a comparable corpus. We manually aligned parallel sentences to create evaluation data and automatically aligned the remaining sentences to create training data. We trained a North Korean NMT model using our North Korean parallel data and improved North Korean translation quality using South Korean resources such as parallel data and a pre-trained model. In addition, we propose Korean-specific pre-processing methods, character tokenization, and phoneme decomposition to use the South Korean resources more efficiently. We demonstrate that the phoneme decomposition consistently improves the North Korean translation accuracy compared to other pre-processing methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4384828005",
    "type": "article"
  },
  {
    "title": "Exploiting Functional Discourse Grammar to Enhance Complex Arabic Relation Extraction using a Hybrid Semantic Knowledge Base - Machine Learning Approach",
    "doi": "https://doi.org/10.1145/3610581",
    "publication_date": "2023-07-25",
    "publication_year": 2023,
    "authors": "Taha Osman; Hussein Khalil; Mohammed Miltan; Khaled Shaalan; Rowida Alfrjani",
    "corresponding_authors": "",
    "abstract": "Relation extraction from unstructured Arabic text is especially challenging due to the Arabic language complex morphology and the variation in word semantics and lexical categories. The research documented in this paper presents a hybrid Semantic Knowledge base - Machine Learning (SKML) approach for extracting complex Arabic relations from unstructured Arabic documents; the proposed approach exploits the principles of Functional Discourse Grammar (FDG) to emphasise the semantic and pragmatic properties of the language and facilitate the identification of relation elements. At the initial phase, the novel FDG-SKML relation extraction approach deploys a lexical-based mechanism that utilises a purposely built domain-specific Semantic Knowledge to encode the semantic association between the identified relations’ elements. The evaluation of the initial stage evidenced improved accuracy for extracting most complex Arabic relations. The initial relation extraction mechanism was further extended by integrating its output into a Machine Learning classifier that facilitated extracting especially complex relations with significant disparity in the relation elements’ presence, order, and correlation. Using Economics as the problem domain, experimental evaluation evidenced the high accuracy of our FDG-SKML approach in complex Arabic relation extraction task and demonstrated its further improvement upon integration with machine learning classifiers.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385253967",
    "type": "article"
  },
  {
    "title": "Speech Feature Enhancement based on Time-frequency Analysis",
    "doi": "https://doi.org/10.1145/3605549",
    "publication_date": "2023-08-23",
    "publication_year": 2023,
    "authors": "Hao D.; Thanh-Duc Chau; Trần Thái Sơn",
    "corresponding_authors": "",
    "abstract": "Time-frequency analysis (TFA) is a powerful method to exploit the hidden information of signals, including speech signals. Many techniques in this group were invented and developed to capture the most crucial stationary feature. However, human speech is not stable, and it contains some non-stationary elements. This work aims to design a new algorithm via the TFA technique to extract the trends and changes inside the speech signal in the time-frequency (TF) plane. We design a new algorithm to create a set of atoms for the signal transform, which can analyze the signal in many different view directions via Poly-Linear Chirplet Transform (PLCT). After processing the signal, the proposed method returns a multichannel output in which each channel results from a particular Linear Chirplet Transform (LCT). The feature then is combined with the MFCC feature to form the final representation. Although the size for speech representation rises, our extracted feature contains rich-meaning information to improve the recognition results compared to other features in gender recognition, dialect recognition, and speaker recognition.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386098783",
    "type": "article"
  },
  {
    "title": "Automatic Resource Augmentation for Machine Translation in Low Resource Language: <tt>EnIndic Corpus</tt>",
    "doi": "https://doi.org/10.1145/3617371",
    "publication_date": "2023-08-31",
    "publication_year": 2023,
    "authors": "Anasua Banerjee; Vinay Kumar; Achyut Shankar; Rutvij H. Jhaveri; Debajyoty Banik",
    "corresponding_authors": "",
    "abstract": "Parallel corpus is the primary ingredient of machine translation. It is required to train the statistical machine translation (SMT) and neural machine translation (NMT) systems. There is a lack of good quality parallel corpus for Hindi to English. Comparable corpora for a given language pair are comparatively easy to find, but this cannot be used directly in SMT or NMT systems. As a result, we generate a parallel corpus from the comparable corpus. For this purpose, the sentences (which are translations of each other) are mined from the comparable corpus to prepare the parallel corpus. The proposed algorithm uses the length of the sentence and word translation model to align sentence pairs that are translations of each other. Then, the sentence pairs that are poor translations of each other (measured by a similarity score based on IBM model 1 translation probability) are filtered out. We apply this algorithm to comparable corpora, which are crawled from speeches of the President and Vice-President of India, and mined parallel corpora out of them. The prepared parallel corpus contains good quality aligned sentences (with 96.338% f-score). Subsequently, incorrect sentence pairs are filtered out manually to make the corpus in qualitative practical use. Finally, we gather various sentences from different sources to prepare the EnIndic corpus, which comprises 1,656,207 English-Hindi sentence pairs (miscellaneous domain). We have deployed this prepared largest English-Hindi parallel corpus at https://github.com/debajyoty/EnIndic.git and the source code at https://github.com/debajyoty/EnIndicSourceCode.git.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386307686",
    "type": "article"
  },
  {
    "title": "Stacking of BERT and CNN Models for Arabic Word Sense Disambiguation",
    "doi": "https://doi.org/10.1145/3623379",
    "publication_date": "2023-09-07",
    "publication_year": 2023,
    "authors": "Rakia Saidi; Fethi Jarray",
    "corresponding_authors": "",
    "abstract": "We propose a new approach for Arabic Word Sense Disambiguation (AWSD) by hybridization of single-layer Convolutional Neural Network (CNN) with contextual representation (BERT). WSD is the task of automatically detecting the correct meaning of a word used in a given context. WSD can be performed as a classification task, and the context is generally a short sentence. Kim [ 26 ] proved that combining a CNN with an RNN (recurrent neural network) provides a good result for text classification. Here, we use a concatenation of BERT models as a word embedding to get simultaneously the target and context representation. Our approach improves the performance of WSD in Arabic languages. The experimental results show that our model outperforms the state-of-the-art approaches and improves the accuracy of 96.42% on the Arabic WordNet dataset.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386527432",
    "type": "article"
  },
  {
    "title": "An Arabic Probabilistic Parser Based on a Property Grammar",
    "doi": "https://doi.org/10.1145/3612921",
    "publication_date": "2023-09-19",
    "publication_year": 2023,
    "authors": "Raja Bensalem; Kais Haddar; Philippe Blache",
    "corresponding_authors": "",
    "abstract": "The specificities of Arabic parsing, such as agglutination, vocalization, and the relatively order-free words in Arabic sentences, remain major issues to consider. To promote its robustness, such parseing should define different types of constraints. Property Grammar (PG) formalism verifies the satisfiability of the constraints directly on the units of the structure, thanks to its properties (or relations). In this context, we propose to build a probabilistic parser with syntactic properties, using a PG, and we measure the production rules in terms of different implicit information and in particular the syntactic properties. We experimented with our parser on the treebank ATB, using the parsing algorithm CYK, and we obtained encouraging results. Our method is also automatic for implementation of most property types. Its generalization for other languages or corpus domains (using treebanks) could be a good perspective. Its combination with pre-trained models of BERT may also make our parser faster.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386845621",
    "type": "article"
  },
  {
    "title": "Language Model Method for Collocation Rules of Parts of Speech in Machine Translation System",
    "doi": "https://doi.org/10.1145/3625095",
    "publication_date": "2023-09-21",
    "publication_year": 2023,
    "authors": "Jinhui Liu; Zhang Feng",
    "corresponding_authors": "",
    "abstract": "With the development of the times, modern society has now entered the Internet of Things (IoT) information age and Machine Translation (MT) plays an important role in increasingly frequent cross-language communication. In recent years, China's artificial intelligence industry has been in a stage of rapid construction, and the scale of its core industries has grown explosively, and a large number of artificial intelligence companies, including issuers, have emerged. Part of speech has always been a major problem in MT. One of the reasons is that there are a large number of multi-category words in Chinese and a large number of polysemy words in English, so part of speech collocation problems account for a large proportion of MT errors, which to some extent affects the credibility and accuracy of the translation. To reduce the error problem in MT of part of speech collocation, this paper used Machine Learning (ML) methods to study the Language Model (LM) of part of speech collocation based on recurrent neural network (NN) and compared it with the traditional statistical LM. In terms of the accuracy rate of the two LMS in the automatic evaluation index of machine translation, the experimental results show that the recursive NN LM established by the ML method had an accuracy rate of 80.42% and 83.57%, respectively, for the part-of-speech matching rules of the IoT machine translation system in the dialogue between Chinese and English and the translation of articles. The accuracy of traditional statistical LM evaluation was 71.29% and 69.52%, respectively. Compared to traditional statistical LM, the accuracy of translation was higher. This showed that the recurrent NN LM reduced the number of errors in the collocation of parts of speech in MT and improved the accuracy and credibility of MT.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386928898",
    "type": "article"
  },
  {
    "title": "Task-based Meta Focal Loss for Multilingual Low-resource Speech Recognition",
    "doi": "https://doi.org/10.1145/3626187",
    "publication_date": "2023-09-29",
    "publication_year": 2023,
    "authors": "Yaqi Chen; Wenlin Zhang; Hao Zhang; Dan Qu; Xukui Yang",
    "corresponding_authors": "",
    "abstract": "Low-resource automatic speech recognition is a challenging task due to a lack of labeled training data. To resolve this issue, multilingual meta-learning learns a better model initialization from many source language tasks for fast adaptation to unseen target languages. However, for diverse source languages, the quantity and difficulty vary greatly because of their different data scales and phonological systems. These differences lead to task-quantity and task-difficulty imbalance issues and thus a failure of multilingual meta-learning ASR. In this work, we propose a task-based meta focal loss (TMFL) approach to address this tough challenge. Specifically, we introduce a hard-task moderator and update the meta-parameters using gradients from both the support set and query set. Our proposed approach focuses more on hard tasks and makes full use of the data from hard tasks. Moreover, we analyze the significance of the hard task moderator and interpret its significance at the sample level. Experiment results show that the proposed method, TMFL, significantly outperforms the state-of-the-art multilingual meta-learning on all target languages for the IARPA BABEL and OpenSLR datasets, especially under very-low-resource conditions. In particular, it can reduce character error rate from 72% to 60% by fine-tuning the pre-trained model with about 22 hours of Vietnamese data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387187790",
    "type": "article"
  },
  {
    "title": "Gender Classification System Based on the Behavioral Biometric Modality: Application of Handwritten Text",
    "doi": "https://doi.org/10.1145/3626236",
    "publication_date": "2023-10-02",
    "publication_year": 2023,
    "authors": "Shaveta Dargan; Munish Kumar",
    "corresponding_authors": "",
    "abstract": "Forensic Science is a branch of science that deals with the discovery, examination, and analysis of strong elements or evidence involved in the criminal justice system. It involves the use of scientific methods to investigate crimes. The Gender Classification System is closely linked to forensic studies, specifically investigating individuals through their handwriting, known as Behavioral Biometrics. Biometric systems rely on behavioral and physiological traits such as brain-prints, fingerprints, handwritten text, speech, facial attributes, gait information, palm vein patterns, hand geometry, electrocardiograms (ECGs), and more. Gender classification is an intriguing and important aspect within the field of pattern recognition and machine learning. It involves a binary problem of classifying individuals as either male or female. Analyzing the differences in femininity and masculinity behaviors can contribute to the evaluation of biometric-based identification systems. Gender classification has numerous forensic applications, including crime identification, demographic research, forgery detection, security, and surveillance. The main objective of this article is to present the latest survey findings on the gender classification system based on handwritten text, specifically the behavioral biometric modality. It includes an overview of the state-of-the-art work, the general framework, approaches, biometric modalities, and critical analysis. The article concludes with a critical analysis, discussion of open issues, concluding remarks, and future perspectives.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387264571",
    "type": "article"
  },
  {
    "title": "A Hybrid Deep Ranking Weighted Multi-Hashing Recommender System",
    "doi": "https://doi.org/10.1145/3626195",
    "publication_date": "2023-10-05",
    "publication_year": 2023,
    "authors": "Suresh Kumar; Jyoti Prakash Singh; Surya Kant; Neha Jain",
    "corresponding_authors": "",
    "abstract": "In countries where there is a low availability of resources for language, businesses face the challenge of overcoming language barriers to reach their customers. One possible solution is to use collaborative filtering-based recommendation systems in their native languages. These systems employ algorithms that understand the customers’ preferences and suggest products or services in their native language. Collaborative filtering (CF) is a popular recommendation technique that simulates word-of-mouth phenomena. However, the accuracy of a CF recommendation can be affected by sparse data. In this research article, we present a novel hybrid weighted multi-deep ranking supervised hashing (HWMDRH) approach. Our method leverages both user-based and item-based CF by merging the item-based deep ranking weighted multi-hash recommender system prediction with the user-based deep ranking weighted multi-hash recommender system prediction to generate Top-N prediction. We conducted extensive experiments on the MovieLens 1M dataset, and our results show that the proposed HWMDRH model outperforms existing models and achieves state-of-the-art performance across recall, precision, RMSE, and F1-score metrics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387377690",
    "type": "article"
  },
  {
    "title": "How a Deep Contextualized Representation and Attention Mechanism Justifies Explainable Cross-Lingual Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3626094",
    "publication_date": "2023-10-09",
    "publication_year": 2023,
    "authors": "Rouzbeh Ghasemi; Saeedeh Momtazi",
    "corresponding_authors": "",
    "abstract": "The number of applications in sentiment analysis is growing daily, and research in this field is increasing. Despite the rapid growth of data sources in English, low-resource languages suffer from a lack of data for accurate training models. Moreover, users cannot trust such systems without explaining the output. In this study, we propose a cross-lingual deep neural model to improve the accuracy of sentiment analysis for low-resource languages while providing an explainable description of the predictions. The proposed model contains a word representation model where we use XLM-RoBERTa, a pre-trained contextualized transformer-based cross-lingual language model, and a long short-term memory network together with an attention mechanism that helps improve the explainability of the model and detect the informative words that impact text polarity. Our experiments show the superiority of the proposed model compared to the state-of-the-art mono-lingual techniques and cross-lingual models. The results show 0.55% improvement compared to the cross-lingual sentiment analysis proposed by Ghasemi et al. and 15.08% improvement compared to the mono-lingual contextualized sentiment analysis. Moreover, we achieve 0.54% further improvement when using attention mechanisms for enhancing the model with explainability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387453348",
    "type": "article"
  },
  {
    "title": "Building a Framework for Identifying Arabic Dialects Using Deep Learning Techniques",
    "doi": "https://doi.org/10.1145/3630632",
    "publication_date": "2023-10-28",
    "publication_year": 2023,
    "authors": "Mohammed Q. Shatnawi; Muneer Bani Yassein; Aseel Abu Huq",
    "corresponding_authors": "",
    "abstract": "Statista statistics show that social media usage has rapidly increased over the past ten years, and that by 2021, there will be approximately 4 billion 871 million Internet users worldwide. This is because mobile phone platforms allow users to express their sentiments and opinions in a variety of languages, including Arabic. One of the most widely used social media sites, Twitter offers a conducive environment for users to voice their thoughts. It is used by magazines and government websites to publish official statements and decisions, and while they write in Modern Standard Arabic, consumers interact with it in their native languages. To tackle issues with natural language processing, such as sentiment analysis and translation, a huge volume of data in many languages must be interpreted and processed. We combine a dataset from Katherine that covers the dialects of 8 countries with a dataset from the NADI shared tasks that was gathered via Twitter and includes the dialects of 21 countries. Three deep learning models with various word embeddings were used. The best results were obtained by CNN/BiLSTM with FastText (51.4",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387999601",
    "type": "article"
  },
  {
    "title": "Factorized Recurrent Neural Network with Attention for Language Identification and Content Detection",
    "doi": "https://doi.org/10.1145/3630607",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Birhanu Hailu Belay; Gebeyehu Belay Gebremeskel; Belete Biazen Bezabih; Seffi Gebeyehu",
    "corresponding_authors": "",
    "abstract": "Language identification and content detection are essential for ensuring effective digital communication, and content moderation. While extensive research has primarily focused on well-known and widely spoken languages, challenges persist when dealing with indigenous and resource-limited languages, especially between closely similar languages such as Ethiopian languages. This article aims to simultaneously identify the language of a given text and detect its content, and to achieve this, we propose a novel attention-based recurrent neural network framework. The proposed method has an attention-embedded Bidirectional-LSTM architecture with two classifiers that identify the language of a given text and content within the text. The two classifiers share a common feature space before they branched at their task-specific layers where both layers are assisted by attention mechanism. We use five different topics in Six Ethiopian Languages the dataset consists of nearly 22,624 sentences. We compared our result with the classical NLP techniques, the proposed method shortened the data prepossessing steps. We evaluated the model performance using the accuracy metric, achieving results of 98.88% for language identification and 96.5% for text content detection. The dataset, source code, and pretrained model are available at https://github.com/bdu-birhanu/LID_TCD .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388210342",
    "type": "article"
  },
  {
    "title": "CCCS: Contrastive Cross-Language Code Search Using Code Graph Information",
    "doi": "https://doi.org/10.1145/3628429",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Li Kuang; Yi Cheng; Honghao Gao",
    "corresponding_authors": "",
    "abstract": "Developers often search and reuse existing code snippets to improve software development efficiency during software development. Currently, researchers have proposed many code search methods. However, the search intent of existing methods is basically a natural language query. In order to support code migration and code refactoring, it is necessary to search relevant code snippets of another programming language with code snippets of one programming language. In this paper, we propose a C ontrastive C ross-language C ode S earch method using code graph information, called CCCS . CCCS first converts code snippets into high-dimensional vectors using pre-trained CodeBERT to extract the sequence features of code snippets. Next, the structural features of code snippets are extracted using a graph convolutional neural network. Finally, the model is trained using the contrastive learning method to optimize the vector representation of cross-language code snippets, enabling the model to distinguish code snippets from different programming languages with the same functionality. To evaluate the effectiveness of our method, we conducted comparison experiments and ablation experiments on a small-scale dataset and a large-scale dataset, respectively. The experimental results show that our method far outperforms the state-of-the-art baseline model in terms of MRR metrics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388412708",
    "type": "article"
  },
  {
    "title": "Adaptive Learning and Correlative Assessment of Differential Usage Patterns for Students with-or-without Learning Disabilities via Learning Analytics",
    "doi": "https://doi.org/10.1145/3632365",
    "publication_date": "2023-11-17",
    "publication_year": 2023,
    "authors": "Masooda Modak; Prachi Gharpure; M Sasikumar",
    "corresponding_authors": "",
    "abstract": "Learning Disabilities (LD) can be categorized into logical, analytical, grammatical, vocabulary, sequential, and inference disabilities. Analysis of such disabilities assists students to identify and strengthen their weak areas. A wide variety of analysis models is proposed by researchers to perform such tasks, but most of these models are highly complex and cannot be scaled for multimodal parameter sets. To overcome these issues, this text proposes a model for correlative assessment of differential usage patterns in students with-or-without learning disabilities via multimodal analysis. The proposed model initially collects real-time inference sets for students with Learning Disabilities (LD) and without LDs. These sets consist of question-specific recorded responses for \"Addition,\" \"Carry Propagation,\" \"Basic to Advanced Grammar,\" \"Direct, Inference and Vocabulary Comprehension,\" \"Finding odd-man-out,\" \"Sequencing,\" and \"Pseudo and Sight Spelling\" for different question sets. Answers to these questions and their metadata were processed via a correlative engine that assisted in evaluation of correctness, time needed per question per category, number of skips, number of revisits, and unanswered ratio for different students. This evaluation was combined with temporal analysis to identify per-category progress of students. Based on this progress, students were either upgraded to next level or given lower-level questions, which assisted them to incrementally improve their grades. The model proved that the performance of LD students is 55% less than the non-LD students and an average of 18 LD students have achieved an average of 33% of improvement after having multiple attempts of the adaptive lessons. The model uses a correlation function, which enables to identify answering patterns of LD and non-LD students with 98.4% accuracy, thus can be used for clinical scenarios.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388763324",
    "type": "article"
  },
  {
    "title": "Few-shot Incremental Event Detection",
    "doi": "https://doi.org/10.1145/3634747",
    "publication_date": "2023-12-02",
    "publication_year": 2023,
    "authors": "Hao Wang; Hanwen Shi; Jianyong Duan",
    "corresponding_authors": "",
    "abstract": "Event detection tasks can enable the quick detection of events from texts and provide powerful support for downstream natural language processing tasks. Most such methods can only detect a fixed set of predefined event classes. To extend them to detect a new class without losing the ability to detect old classes requires costly retraining of the model from scratch. Incremental learning can effectively solve this problem, but it requires abundant data of new classes. In practice, however, the lack of high-quality labeled data of new event classes makes it difficult to obtain enough data for model training. To address the above mentioned issues, we define a new task, few-shot incremental event detection, which focuses on learning to detect a new event class with limited data, while retaining the ability to detect old classes to the extent possible. We created a benchmark dataset IFSED for the few-shot incremental event detection task based on FewEvent and propose two benchmarks, IFSED-K and IFSED-KP. Experimental results show that our approach has a higher F1-score than baseline methods and is more stable.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389269043",
    "type": "article"
  },
  {
    "title": "Dual-Branch Multitask Fusion Network for Offline Chinese Writer Identification",
    "doi": "https://doi.org/10.1145/3638554",
    "publication_date": "2023-12-26",
    "publication_year": 2023,
    "authors": "Haixia Wang; Yingyu Mao; Qingran Miao; Qun Xiao; Yilong Zhang",
    "corresponding_authors": "",
    "abstract": "Chinese characters are complex and contain discriminative information, meaning that their writers have the potential to be recognized using less text. In this study, offline Chinese writer identification based on a single character was investigated. To extract comprehensive features to model Chinese characters, explicit and implicit information as well as global and local features are of interest. A dual-branch multitask fusion network is proposed that contains two branches for global and local feature extraction simultaneously, and introduces auxiliary tasks to help the main task. Content recognition, stroke number estimation, and stroke recognition are considered as three auxiliary tasks for explicit information. The main task extracts implicit information of writer identity. The experimental results validated the positive influences of auxiliary tasks on the writer identification task, with the stroke number estimation task being most helpful. In-depth research was conducted to investigate the influencing factors in Chinese writer identification, with respect to character complexity, stroke importance, and character number, which provides a systematic reference for the actual application of neural networks in Chinese writer identification.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390225420",
    "type": "article"
  },
  {
    "title": "Hypergraph Neural Network for Emotion Recognition in Conversations",
    "doi": "https://doi.org/10.1145/3638760",
    "publication_date": "2023-12-27",
    "publication_year": 2023,
    "authors": "Zheng Cheng; Haojie Xu; Xiao Sun",
    "corresponding_authors": "",
    "abstract": "Modeling conversational context is an essential step for emotion recognition in conversations. Existing works still suffer from insufficient utilization of local context information and remote context information. This article designs a hypergraph neural network, namely HNN-ERC, to better utilize local and remote contextual information. HNN-ERC combines the recurrent neural network with the conventional hypergraph neural network to strengthen connections between utterances and make each utterance receive information from other utterances better. The proposed model has empirically achieved state-of-the-art results on three benchmark datasets, demonstrating the effectiveness and superiority of the new model.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390263698",
    "type": "article"
  },
  {
    "title": "Improving the Detection of Multilingual South African Abusive Language via Skip-gram Using Joint Multilevel Domain Adaptation",
    "doi": "https://doi.org/10.1145/3638759",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Oluwafemi Oriola; Eduan Kotzé",
    "corresponding_authors": "",
    "abstract": "The distinctiveness and sparsity of low-resource multilingual South African abusive language necessitate the development of a novel solution to automatically detect different classes of abusive language instances using machine learning. Skip-gram has been used to address sparsity in machine learning classification problems but is inadequate in detecting South African abusive language due to the considerable amount of rare features and class imbalance. Joint Domain Adaptation has been used to enlarge features of a low-resource target domain for improved classification outcomes by jointly learning from the target domain and large-resource source domain. This article, therefore, builds a Skip-gram model based on Joint Domain Adaptation to improve the detection of multilingual South African abusive language. Contrary to the existing Joint Domain Adaptation approaches, a Joint Multilevel Domain Adaptation model involving adaptation of monolingual source domain instances and multilingual target domain instances with high frequency of rare features was executed at the first level and adaptation of target-domain features and first-level features at the next level. Both surface-level and embedding word features were used to evaluate the proposed model. In the evaluation of surface-level features, the Joint Multilevel Domain Adaptation model outperformed the state-of-the-art models with accuracy of 0.92 and F1-score of 0.68. In the evaluation of embedding features, the proposed model outperformed the state-of-the-art models with accuracy of 0.88 and F1-score of 0.64. The Joint Multilevel Domain Adaptation model significantly improved the average information gain of the rare features in different language categories and reduced class imbalance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390317701",
    "type": "article"
  },
  {
    "title": "Ibn-Ginni: An Improved Morphological Analyzer for Arabic",
    "doi": "https://doi.org/10.1145/3639050",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Waleed Nazih; Amany Fashwan; Amr El-Gendy; Yasser Hifny",
    "corresponding_authors": "",
    "abstract": "Arabic is a morphologically rich language, which means that the Arabic language has a complicated system of word formation and structure. The affixes in the Arabic language (i.e., prefixes and suffixes) can be added to root words to generate different meanings and grammatical functions. These affixes can indicate aspects such as tense, gender, number, case, person, and more. In addition, the meaning and function of words can be modified in Arabic using an internal structure known as morphological patterns. Computational morphological analyzers of Arabic are vital to developing Arabic language processing toolkits. In this article, we introduce a new morphological analyzer (Ibn-Ginni) that inherits the speed and quality of the Buckwalter Arabic Morphological Analyzer (BAMA). The BAMA has poor coverage of the classical Arabic language. Hence, the coverage of classical Arabic is improved by using the Alkhalil analyzer. Although it is slow, it was used to generate a huge number of solutions for 3 million unique Arabic words collected from different resources. These word form-based solutions were converted to stem-based solutions, refined manually, and added to the database of BAMA, resulting in substantial improvements in the quality of the analysis. Hence, Ibn-Ginni is a hybrid system between BAMA and Alkhalil analyzers and may be considered an efficient large-scale analyzer. The Ibn-Ginni analyzer analyzed 0.6 million more words than the BAMA analyzer. Therefore, our analyzer significantly improves the coverage of the Arabic language. Besides, the Ibn-Ginni analyzer is high speed at providing solutions, the average time to analyze a word is 0.3 ms. Using a corpus designed for benchmarking Arabic morphological analyzers, our analyzer was able to find all solutions for 72.72% of the words. Moreover, the analyzer did not provide all possible morphological solutions for 24.24% of the words. The analyzer and its morphological database are publicly available on GitHub. 1",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390317717",
    "type": "article"
  },
  {
    "title": "Explanation Guided Knowledge Distillation for Pre-trained Language Model Compression",
    "doi": "https://doi.org/10.1145/3639364",
    "publication_date": "2023-12-29",
    "publication_year": 2023,
    "authors": "Zhao Yang; Yuanzhe Zhang; Dianbo Sui; Yiming Ju; Jun Zhao; Kang Liu",
    "corresponding_authors": "",
    "abstract": "Knowledge distillation is widely used in pre-trained language model compression, which can transfer knowledge from a cumbersome model to a lightweight one. Though knowledge distillation based model compression has achieved promising performance, we observe that explanations between the teacher model and the student model are not consistent. We argue that the student model should study not only the predictions of the teacher model but also the internal reasoning process. To this end, we propose Explanation Guided Knowledge Distillation (EGKD) in this article, which utilizes explanations to represent the thinking process and improve knowledge distillation. To obtain explanations in our distillation framework, we select three typical explanation methods rooted in different mechanisms, namely gradient-based , perturbation-based , and feature selection methods. Then, to improve computational efficiency, we propose different optimization strategies to utilize the explanations obtained by these three different explanation methods, which could provide the student model with better learning guidance. Experimental results on GLUE demonstrate that leveraging explanations can improve the performance of the student model. Moreover, our EGKD could also be applied to model compression with different architectures.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390393202",
    "type": "article"
  },
  {
    "title": "DSISA: A New Neural Machine Translation Combining Dependency Weight and Neighbors",
    "doi": "https://doi.org/10.1145/3638762",
    "publication_date": "2023-12-29",
    "publication_year": 2023,
    "authors": "Lingfang Li; A. Lin Zhang; M. X. Luo",
    "corresponding_authors": "",
    "abstract": "Most of the previous neural machine translations (NMT) rely on parallel corpus. Integrating explicitly prior syntactic structure information can improve the neural machine translation. In this article, we propose a Syntax Induced Self-Attention (SISA) which explores the influence of dependence relation between words through the attention mechanism and fine-tunes the attention allocation of the sentence through the obtained dependency weight. We present a new model, Double Syntax Induced Self-Attention (DSISA), which fuses the features extracted by SISA and a compact convolution neural network (CNN). SISA can alleviate long dependency in sentence, while CNN captures the limited context based on neighbors. DSISA utilizes two different neural networks to extract different features for richer semantic representation and replaces the first layer of Transformer encoder. DSISA not only makes use of the global feature of tokens in sentences but also the local feature formed with adjacent tokens. Finally, we perform simulation experiments that verify the performance of the new model on standard corpora.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390393222",
    "type": "article"
  },
  {
    "title": "Real-time Assistive Reader Pen for Arabic Language",
    "doi": "https://doi.org/10.1145/3423133",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Mohammad A. Alzubaidi; Mwaffaq Otoom; Nouran S. Ahmad",
    "corresponding_authors": "",
    "abstract": "Disability is an impairment affecting an individual's livelihood and independence. Assistive technology enables the disabled cohort of the community to break the barriers to learning, access information, contribute to the community, and live independently. This article proposes an assistive device to enable people with visual disabilities and learning disabilities to access printed Arabic material in real-time, and to help them participate in the education system and the professional workforce. This proposed assistive device employs Optical Character Recognition (OCR) and Text To Speech (TTS) conversion, using concatenation synthesis. OCR is achieved using image processing, character extraction, and classification, while Arabic speech synthesis is achieved through concatenation synthesis, followed by Multi Band Re-synthesis Overlap-Add (MBROLA). Waveform generation in the second phase produces vocal output for the disabled user to hear. OCR character and word accuracy tests were conducted for nine Arabic fonts. The results show that six fonts were recognized with over 60% character accuracy and two fonts were recognized with over 88% accuracy. A Mean Opinion Score (MOS) test for speech quality was conducted. The results showed an overall MOS score of 3.53/5 and indicated that users were able to understand the speech. A real-time usability testing was conducted with 10 subjects. The results showed an overall average of agreements scores of 3.9/5 and indicated that the proposed Arabic reader pen meets the real-time constraints and is pleasant and satisfying to use and can contribute to make printed Arabic material accessible to visually impaired persons and people with learning disabilities.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3150409344",
    "type": "article"
  },
  {
    "title": "Exploring Topic-language Preferences in Multilingual Swahili Information Retrieval in Tanzania",
    "doi": "https://doi.org/10.1145/3458671",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Joseph P. Telemala; Hussein Suleman",
    "corresponding_authors": "",
    "abstract": "Habitual switching of languages is a common behaviour among polyglots when searching for information on the Web. Studies in information retrieval (IR) and multilingual information retrieval (MLIR) suggest that part of the reason for such regular switching of languages is the topic of search. Unlike survey-based studies, this study uses query and click-through logs. It exploits the querying and results selection behaviour of Swahili MLIR system users to explore how topic of search (query) is associated with language preferences—topic-language preferences. This article is based on a carefully controlled study using Swahili-speaking Web users in Tanzania who interacted with a guided multilingual search engine. From the statistical analysis of queries and click-through logs, it was revealed that language preferences may be associated with the topics of search. The results also suggest that language preferences are not static; they vary along the course of Web search from query to results selection. In most of the topics, users either had significantly no language preference or preferred to query in Kiswahili and changed their preference to either English or no preference for language when selecting/clicking on the results. The findings of this study might provide researchers with more insights in developing better MLIR systems that support certain types of users and in certain scenarios.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3194051295",
    "type": "article"
  },
  {
    "title": "BYANJON: A Ground Truth Preparation System for Online Handwritten Bangla Documents",
    "doi": "https://doi.org/10.1145/3464379",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Shibaprasad Sen; Ankan Bhattacharyya; Ram Sarkar; Kaushik Roy",
    "corresponding_authors": "",
    "abstract": "The work reported in this article deals with the ground truth generation scheme for online handwritten Bangla documents at text-line, word, and stroke levels. The aim of the proposed scheme is twofold: firstly, to build a document level database so that future researchers can use the database to do research in this field. Secondly, the ground truth information will help other researchers to evaluate the performance of their algorithms developed for text-line extraction, word extraction, word segmentation, stroke recognition, and word recognition. The reported ground truth generation scheme starts with text-line extraction from the online handwritten Bangla documents, then words extraction from the text-lines, and finally segmentation of those words into basic strokes. After word segmentation, the basic strokes are assigned appropriate class labels by using modified distance-based feature extraction procedure and the MLP ( Multi-layer Perceptron ) classifier. The Unicode for the words are then generated from the sequence of stroke labels. XML files are used to store the stroke, word, and text-line levels ground truth information for the corresponding documents. The proposed system is semi-automatic and each step such as text-line extraction, word extraction, word segmentation, and stroke recognition has been implemented by using different algorithms. Thus, the proposed ground truth generation procedure minimizes huge manual intervention by reducing the number of mouse clicks required to extract text-lines, words from the document, and segment the words into basic strokes. The integrated stroke recognition module also helps to minimize the manual labor needed to assign appropriate stroke labels. The freely available and can be accessed at https://byanjon.herokuapp.com/ .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3195210859",
    "type": "article"
  },
  {
    "title": "Construction of a Corpus of Rhetorical Devices in Slogans and Structural Analysis of Antitheses",
    "doi": "https://doi.org/10.1145/3465218",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Ayana Niwa; Naoaki Okazaki; Kohei Wakimoto; Keisuke Nishiguchi; Masataka Mouri",
    "corresponding_authors": "",
    "abstract": "An advertising slogan is a sentence that expresses a product or a work of art in a straightforward manner and is used for advertising and publicity. Moving the consumer's mind and attracting their interest can significantly influence sales. Although rhetorical techniques in a slogan are known to improve the effectiveness of advertising, not much attention has been devoted to analyze or automatically generate sentences with the techniques. Therefore, we constructed a large corpus of slogans and revealed the linguistic characteristics of the basic statistics and rhetorical devices. Another point of focus was antitheses, of which the usage rates are relatively high and which have a specific sentence structure and lexical constraints. The generation of a slogan that contains an antithesis necessitates the structure of sentences, known as templates, to be extracted and also requires knowledge of word pairs with semantic contrast. Thus, the next step involved analysis of the structure to extract the sentence structure and lexical knowledge about the antithesis. Despite its simple architecture, the proposed method exceeds the prediction accuracy and efficiency of a comparable method. Lexical knowledge that is not available in existing dictionaries was also extracted.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3195410827",
    "type": "article"
  },
  {
    "title": "Query Expansion for Transliterated Text Retrieval",
    "doi": "https://doi.org/10.1145/3447649",
    "publication_date": "2021-07-20",
    "publication_year": 2021,
    "authors": "Dinesh Kumar Prabhakar; Sukomal Pal; Chiranjeev Kumar",
    "corresponding_authors": "",
    "abstract": "With Web 2.0, there has been exponential growth in the number of Web users and the volume of Web content. Most of these users are not only consumers of the information but also generators of it. People express themselves here in colloquial languages, but using Roman script (transliteration). These texts are mostly informal and casual, and therefore seldom follow grammar rules. Also, there does not exist any prescribed set of spelling rules in transliterated text. This freedom leads to large-scale spelling variations, which is a major challenge in mixed script information processing. This article studies different existing phonetic algorithms to handle the issue of spelling variation, points out the limitations of them, and proposes a novel phonetic encoding approach with two different flavors in the light of Hindi transliteration. Experiments performed over Hindi song lyrics retrieval in mixed script domain with three different retrieval models show that proposed approaches outperform the existing techniques in a majority of the cases (sometimes statistically significantly) for a number of metrics like nDCG@1, nDCG@5, nDCG@10, MAP, MRR, and Recall.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3197236654",
    "type": "article"
  },
  {
    "title": "A Unified Dialogue Management Strategy for Multi-intent Dialogue Conversations in Multiple Languages",
    "doi": "https://doi.org/10.1145/3461763",
    "publication_date": "2021-09-20",
    "publication_year": 2021,
    "authors": "Tulika Saha; Dhawal Gupta; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Building Virtual Agents capable of carrying out complex queries of the user involving multiple intents of a domain is quite a challenge, because it demands that the agent manages several subtasks simultaneously. This article presents a universal Deep Reinforcement Learning framework that can synthesize dialogue managers capable of working in a task-oriented dialogue system encompassing various intents pertaining to a domain. The conversation between agent and user is broken down into hierarchies, to segregate subtasks pertinent to different intents. The concept of Hierarchical Reinforcement Learning, particularly options , is used to learn policies in different hierarchies that operates in distinct time steps to fulfill the user query successfully. The dialogue manager comprises top-level intent meta-policy to select among subtasks or options and a low-level controller policy to pick primitive actions to communicate with the user to complete the subtask provided to it by the top-level policy in varying intents of a domain. The proposed dialogue management module has been trained in a way such that it can be reused for any language for which it has been developed with little to no supervision. The developed system has been demonstrated for “Air Travel” and “Restaurant” domain in English and Hindi languages. Empirical results determine the robustness and efficacy of the learned dialogue policy as it outperforms several baselines and a state-of-the-art system.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3201065191",
    "type": "article"
  },
  {
    "title": "Synonymy Expansion Using Link Prediction Methods: A Case Study of Assamese WordNet",
    "doi": "https://doi.org/10.1145/3467966",
    "publication_date": "2021-11-02",
    "publication_year": 2021,
    "authors": "Bornali Phukon; Akash Anil; Sanasam Ranbir Singh; Priyankoo Sarmah",
    "corresponding_authors": "",
    "abstract": "WordNets built for low-resource languages, such as Assamese, often use the expansion methodology. This may result in missing lexical entries and missing synonymy relations. As the Assamese WordNet is also built using the expansion method, using the Hindi WordNet, it also has missing synonymy relations. As WordNets can be visualized as a network of unique words connected by synonymy relations, link prediction in complex network analysis is an effective way of predicting missing relations in a network. Hence, to predict the missing synonyms in the Assamese WordNet, link prediction methods were used in the current work that proved effective. It is also observed that for discovering missing relations in the Assamese WordNet, simple local proximity-based methods might be more effective as compared to global and complex supervised models using network embedding. Further, it is noticed that though a set of retrieved words are not synonyms per se, they are semantically related to the target word and may be categorized as semantic cohorts.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3210975719",
    "type": "article"
  },
  {
    "title": "A Case Study on Handwritten <i>Indic</i> Script Classification: Benchmarking of the Results at Page, Block, Text-line, and Word Levels",
    "doi": "https://doi.org/10.1145/3476102",
    "publication_date": "2021-11-03",
    "publication_year": 2021,
    "authors": "Pawan Kumar Singh; Ram Sarkar; Ajith Abraham; Mita Nasipuri",
    "corresponding_authors": "",
    "abstract": "Handwritten script classification is still considered as a challenging research problem in the domain of document image analysis. Although some research attempts have been made by the researchers for solving the challenging issues, a comprehensive solution is yet to be achieved. The case study, undertaken here, analyzes the performances of various state-of-the art handwritten script classification methods for Indian scripts where features, needed for the script classification task, are extracted from the script images at four different granularity levels, i.e., page, block, text line, or word. The results of handwritten script classification at each level have been obtained and compared using eight different feature sets and six different state-of-the-art classifiers. Based on the classification results, an ideal level for performing the handwritten script classification task is suggested among these four classification levels. The results have also been improved by using two feature dimensionality reduction methods. All these experiments are done on two different handwritten Indic script databases, of which one is an in-house developed dataset and the other one is a freely available dataset. Finally, some future research directions that may be undertaken by the researchers as an application of the handwritten Indic script classification problem are also highlighted. The work presented here provides a basic foundation for the construction of a comprehensive handwritten script classification method for official Indian scripts.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3211196679",
    "type": "article"
  },
  {
    "title": "Using Pre-trained Language Model to Enhance Active Learning for Sentence Matching",
    "doi": "https://doi.org/10.1145/3480937",
    "publication_date": "2021-12-30",
    "publication_year": 2021,
    "authors": "Guirong Bai; Shizhu He; Kang Liu; Jun Zhao",
    "corresponding_authors": "",
    "abstract": "Active learning is an effective method to substantially alleviate the problem of expensive annotation cost for data-driven models. Recently, pre-trained language models have been demonstrated to be powerful for learning language representations. In this article, we demonstrate that the pre-trained language model can also utilize its learned textual characteristics to enrich criteria of active learning. Specifically, we provide extra textual criteria with the pre-trained language model to measure instances, including noise, coverage, and diversity. With these extra textual criteria, we can select more efficient instances for annotation and obtain better results. We conduct experiments on both English and Chinese sentence matching datasets. The experimental results show that the proposed active learning approach can be enhanced by the pre-trained language model and obtain better performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200207086",
    "type": "article"
  },
  {
    "title": "Joined Type Length Encoding for Nested Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3487057",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Mohammad Sadegh Sheikhaei; Hasan Zafari; Yuan Tian",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a new encoding scheme for named entity recognition (NER) called Joined Type-Length encoding (JoinedTL). Unlike most existing named entity encoding schemes, which focus on flat entities, JoinedTL can label nested named entities in a single sequence. JoinedTL uses a packed encoding to represent both type and span of a named entity, which not only results in less tagged tokens compared to existing encoding schemes, but also enables it to support nested NER. We evaluate the effectiveness of JoinedTL for nested NER on three nested NER datasets: GENIA in English, GermEval in German, and PerNest, our newly created nested NER dataset in Persian. We apply CharLSTM+WordLSTM+CRF, a three-layer sequence tagging model on three datasets encoded using JoinedTL and two existing nested NE encoding schemes, i.e., JoinedBIO and JoinedBILOU. Our experiment results show that CharLSTM+WordLSTM+CRF trained with JoinedTL encoded datasets can achieve competitive F1 scores as the ones trained with datasets encoded by two other encodings, but with 27%–48% less tagged tokens. To leverage the power of three different encodings, i.e., JoinedTL, JoinedBIO, and JoinedBILOU, we propose an encoding-based ensemble method for nested NER. Evaluation results show that the ensemble method achieves higher F1 scores on all datasets than the three models each trained using one of the three encodings. By using nested NE encodings including JoinedTL with CharLSTM+WordLSTM+CRF, we establish new state-of-the-art performance with an F1 score of 83.7 on PerNest, 74.9 on GENIA, and 70.5 on GermEval, surpassing two recent neural models specially designed for nested NER.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200314022",
    "type": "article"
  },
  {
    "title": "Towards Developing Uniform Lexicon Based Sorting Algorithm for Three Prominent Indo-Aryan Languages",
    "doi": "https://doi.org/10.1145/3488371",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Mir Ragib Ishraq; Nitesh Khadka; Asif Mohammed Samir; Mohammad Shahidur Rahman",
    "corresponding_authors": "",
    "abstract": "Three different Indic/Indo-Aryan languages - Bengali, Hindi and Nepali have been explored here in character level to find out similarities and dissimilarities. Having shared the same root, the Sanskrit, Indic languages bear common characteristics. That is why computer and language scientists can take the opportunity to develop common Natural Language Processing (NLP) techniques or algorithms. Bearing the concept in mind, we compare and analyze these three languages character by character. As an application of the hypothesis, we also developed a uniform sorting algorithm in two steps, first for the Bengali and Nepali languages only and then extended it for Hindi in the second step. Our thorough investigation with more than 30,000 words from each language suggests that, the algorithm maintains total accuracy as set by the local language authorities of the respective languages and good efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200442491",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing, Part 3",
    "doi": "https://doi.org/10.1145/3476464",
    "publication_date": "2021-09-27",
    "publication_year": 2021,
    "authors": "Gunasekaran Manogaran; Hassan Qudrat‐Ullah; Qin Xin",
    "corresponding_authors": "",
    "abstract": "introduction Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing, Part 3 Share on Editors: Gunasekaran Manogaran View Profile , Hassan Qudrat-Ullah View Profile , Qin Xin View Profile Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingVolume 20Issue 5September 2021 Article No.: 72epp 1–3https://doi.org/10.1145/3476464Published:31 August 2021 0citation11DownloadsMetricsTotal Citations0Total Downloads11Last 12 Months11Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4248823859",
    "type": "article"
  },
  {
    "title": "SCBG: Semantic-Constrained Bidirectional Generation for Emotional Support Conversation",
    "doi": "https://doi.org/10.1145/3666090",
    "publication_date": "2024-05-27",
    "publication_year": 2024,
    "authors": "Yangyang Xu; Zhuoer Zhao; Xiao Sun",
    "corresponding_authors": "",
    "abstract": "The Emotional Support Conversation (ESC) task aims to deliver consolation, encouragement, and advice to individuals undergoing emotional distress, thereby assisting them in overcoming difficulties. In the context of emotional support dialogue systems, it is of utmost importance to generate user-relevant and diverse responses. However, previous methods failed to take into account these crucial aspects, resulting in a tendency to produce universal and safe responses (e.g., “I do not know” and “I am sorry to hear that”). To tackle this challenge, a semantic-constrained bidirectional generation (SCBG) framework is utilized for generating more diverse and user-relevant responses. Specifically, we commence by selecting keywords that encapsulate the ongoing dialogue topics based on the context. Subsequently, a bidirectional generator generates responses incorporating these keywords. Two distinct methodologies, namely, statistics-based and prompt-based methods, are employed for keyword extraction. Experimental results on the ESConv dataset demonstrate that the proposed SCBG framework improves response diversity and user relevance while ensuring response quality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399045859",
    "type": "article"
  },
  {
    "title": "Novel Character Identification Utilizing Semantic Relation with Animate Nouns in Korean",
    "doi": "https://doi.org/10.1145/3197657",
    "publication_date": "2018-07-21",
    "publication_year": 2018,
    "authors": "Taekeun Park; Seung-Hoon Kim",
    "corresponding_authors": "",
    "abstract": "For identifying speakers of quoted speech or extracting social networks from literature, it is indispensable to extract character names and nominals. However, detecting proper nouns in the novels translated into or written in Korean is harder than in English because Korean does not have a capitalization feature. In addition, it is almost impossible for any proper noun dictionary to include all kinds of character names that have been created or will be created by authors. Fortunately, a previous study shows that utilizing postpositions for animate nouns is a simple and effective tool for character identification in Korean novels without a proper noun dictionary and a training corpus. In this article, we propose a character identification method utilizing the semantic relation with known animate nouns. For 80 novels in Korean, the proposed method increases the micro- and macro-average recall by 13.68% and 11.86%, respectively, while decreasing the micro-average precision by 0.28% and increasing the macro-average precision by 0.07% compared to the previous study. If we focus on characters that are responsible for more than 1% of the character name mentions in each novel, the micro- and macro-average F-measure of the proposed method are 96.98% and 97.32%, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2883510700",
    "type": "article"
  },
  {
    "title": "Tempo-HindiWordNet",
    "doi": "https://doi.org/10.1145/3277504",
    "publication_date": "2018-12-14",
    "publication_year": 2018,
    "authors": "Sabyasachi Kamila; Mohammed Hasanuzzaman; Asif Ekbal; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Temporality has significantly contributed to various Natural Language Processing and Information Retrieval applications. In this article, we first create a lexical knowledge-base in Hindi by identifying the temporal orientation of word senses based on their definition and then use this resource to detect underlying temporal orientation of the sentences. To create the resource, we propose a semi-supervised learning framework, where each synset of the Hindi WordNet is classified into one of the five categories, namely, past , present , future , neutral , and atemporal . The algorithm initiates learning with a set of seed synsets and then iterates following different expansion strategies, viz. probabilistic expansion based on classifier’s confidence and semantic distance based measures. We manifest the usefulness of the resource that we build on an external task, viz. sentence-level temporal classification. The underlying idea is that a temporal knowledge-base can help in classifying the sentences according to their inherent temporal properties. Experiments on two different domains, viz. general and Twitter, show interesting results.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2904820527",
    "type": "article"
  },
  {
    "title": "Transform, Combine, and Transfer",
    "doi": "https://doi.org/10.1145/3325886",
    "publication_date": "2019-06-05",
    "publication_year": 2019,
    "authors": "Ayan Kumar Das; Sudeshna Sarkar",
    "corresponding_authors": "",
    "abstract": "Transfer parsing has been used for developing dependency parsers for languages with no treebank by using transfer from treebanks of other languages (source languages). In delexicalized transfer, parsed words are replaced by their part-of-speech tags. Transfer parsing may not work well if a language does not follow uniform syntactic structure with respect to its different constituent patterns. Earlier work has used information derived from linguistic databases to transform a source language treebank to reduce the syntactic differences between the source and the target languages. We propose a transformation method where a source language pattern is transformed stochastically to one of the multiple possible patterns followed in the target language. The transformed source language treebank can be used to train a delexicalized parser in the target language. We show that this method significantly improves the average performance of single-source delexicalized transfer parsers. We also show that, in the multi-source settings, parsers trained using a concatenation of transformed source language treebanks work better when a subset of the source language treebanks is used rather than concatenating all of them or only one. However, the problem of selecting the subset of treebanks whose combination gives the best-performing parser from the set of all the available treebanks is hard. We propose a greedy selection heuristic based on the labelled attachment scores of the corresponding single-source parsers trained using the treebanks after transformation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2949109718",
    "type": "article"
  },
  {
    "title": "Chinese Syntax Parsing Based on Sliding Match of Semantic String",
    "doi": "https://doi.org/10.1145/3329707",
    "publication_date": "2019-07-25",
    "publication_year": 2019,
    "authors": "Wei Wang; Degen Huang; Jingxiang Cao",
    "corresponding_authors": "",
    "abstract": "Different from the current syntax parsing based on deep learning, we present a novel Chinese parsing method, which is based on Sliding Match of Semantic String (SMOSS). (1) Training stage: In a treebank, headwords of tree nodes are represented by semantic codes given in the Synonym Dictionary (Tongyici Cilin). N-gram semantic templates are extracted from every layer of a syntax tree by means of sliding window to establish one N-gram semantic template library. (2) Parsing stage: Words of a sentence, including headwords of chunks, are represented by the semantic codes from Tongyici Cilin. With the sliding window method, N-gram semantic code strings are extracted to match with the templates in the N-gram semantic template library; subsequently, the mapping information of the matched templates is employed to guide the chunking of semantic code strings. The Chinese syntax parsing is completed through continuous matching and chunking. On the same training scale, N-gram semantic template can create favorable conditions for flexible matching and improve the syntax parsing performance. With train and test sets from the Tsinghua Chinese Treebank (TCT), the results are F1-score 99.71% (closed test) and F1-score 70.43% (open test), respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2963169439",
    "type": "article"
  },
  {
    "title": "An Automatic and a Machine-assisted Method to Clean Bilingual Corpus",
    "doi": "https://doi.org/10.1145/3342351",
    "publication_date": "2019-10-09",
    "publication_year": 2019,
    "authors": "Jyoti Srivastava; Sudip Sanyal; Ashish Kumar Srivastava",
    "corresponding_authors": "",
    "abstract": "Two different methods of corpus cleaning are presented in this article. One is a machine-assisted technique, which is good to clean small-sized parallel corpus, and the other is an automatic method, which is suitable for cleaning large-sized parallel corpus. A baseline SMT (MOSES) system is used to evaluate these methods. The machine-assisted technique used two features: word alignment and length of the source and target language sentence. These features are used to detect mistranslations in the corpus, which are then handled by a human translator. Experiments of this method are conducted on the English-to-Indian Language Machine Translation (EILMT) corpus (English-Hindi). The Bilingual Evaluation Understudy (BLEU) score is improved by 0.47% for the clean corpus. Automatic method of corpus cleaning uses a combination of two features. One feature is length of source and target language sentence and the second feature is Viterbi alignment score generated by Hidden Markov Model for each sentence pair. Two different threshold values are used for these two features. These values are decided by using a small-sized manually annotated parallel corpus of 206 sentence pairs. Experiments of this method are conducted on the HindEnCorp corpus, released in the workshop of the Association of Computational Linguistics (ACL 2014). The BLEU score is improved by 0.6% on clean corpus. A comparison of the two methods is also presented on EILMT corpus.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2979419423",
    "type": "article"
  },
  {
    "title": "SEEUNRS: Semantically Enriched Entity-Based Urdu News Recommendation System",
    "doi": "https://doi.org/10.1145/3639049",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Safia Kanwal; Muhammad Kamran Malik; Zubair Nawaz; Khawar Mehmood",
    "corresponding_authors": "",
    "abstract": "The advancement in the production, distribution, and consumption of news has fostered easy access to the news with fair challenges. The main challenge is to present the right news to the right audience. The news recommendation system is one of the technological solutions to this problem. Much work has been done on news recommendation systems for the major languages of the world, but trivial work has been done for resource-poor languages like Urdu. Another significant hurdle in the development of an efficient news recommendation system is the scarcity of an accessible and suitable Urdu dataset. To this end, an Urdu news mobile application was used to collect the news data and user feedback for 1 month. After refinement, the first-ever Urdu dataset of 100 users and 23,250 news was curated for the Urdu news recommendation system. In addition, SEEUNRS , a semantically enriched entity-based Urdu news recommendation system, is proposed. The proposed scheme exploits the hidden features of a news article and entities to suggest the right article to the right audience. Results have shown that the presented model has an improvement of 6.9% in the F1 measure from traditional recommendation system techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390751989",
    "type": "article"
  },
  {
    "title": "Modeling a Novel Approach for Emotion Recognition Using Learning and Natural Language Processing",
    "doi": "https://doi.org/10.1145/3641851",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Lakshmi Lalitha V.; Dinesh Kumar Anguraj",
    "corresponding_authors": "",
    "abstract": "Various facts, including politics, entertainment, industry, and research fields, are connected to analyzing the audience's emotions. Sentiment Analysis (SA) is a Natural Language Processing (NLP) concept that uses statistical and lexical forms as well as learning techniques to forecast how different types of content in social media will express the audience's neutral, positive, and negative emotions. There is lack of an adequate tool to quantify the characteristics and independent text for assessing the primary audience emotion from the available online social media dataset. The focus of this research is on modeling a cutting-edge method for decoding the connectivity among social media texts and assessing audience emotions. Here, a novel dense layer graph model (DLG-TF) for textual feature analysis is used to analyze the relevant connectedness inside the complex media environment to forecast emotions. The information from the social media dataset is extracted using some popular convolution network models, and the predictions are made by examining the textual properties. The experimental results show that, when compared to different standard emotions, the proposed DLG-TF model accurately predicts a greater number of possible emotions. The macro-average of baseline is 58%, the affective is 55%, the crawl is 55%, and the ultra-dense is 59%, respectively. The feature analysis comparison of baseline, affective, crawl, ultra-dense and DLG-TF using the unsupervised model based on EmoTweet gives the precision, recall, and F1-score of the anticipated model are explained. The micro- and macro-average based on these parameters are compared and analyzed. The macro-average of baseline is 47%, the affective is 46%, the crawl is 50%, and the ultra-dense is 85%, respectively. It makes precise predictions using the social media dataset that is readily available. A few criteria, including accuracy, recall, precision, and F-measure, are assessed and contrasted with alternative methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391110986",
    "type": "article"
  },
  {
    "title": "Automatic Construction of Interval-Valued Fuzzy Hindi WordNet using Lexico-Syntactic Patterns and Word Embeddings",
    "doi": "https://doi.org/10.1145/3643132",
    "publication_date": "2024-02-02",
    "publication_year": 2024,
    "authors": "Minni Jain; Rajni Jindal; Amita Jain",
    "corresponding_authors": "",
    "abstract": "A computational lexicon is the backbone of any language processing system. It helps computers to understand the language complexity as a human does by inculcating words and their semantic associations. Manually constructed famous Hindi WordNet (HWN) consists of various classical semantic relations (crisp relations). To handle uncertainty and represent Hindi WordNet more semantically, Type- 1 fuzzy graphs are applied to relations of Hindi WordNet. But uncertainty in the crisp membership degree is not considered in Type 1 fuzzy set (T1FS). Also collecting billions (5,55,69,51,753 relations in HWN) of membership values from experts (humans) is not feasible. This paper applied the concept of Interval-Valued Fuzzy graphs and proposed Interval- Valued Fuzzy Hindi WordNet (IVFHWN). IVFHWN automatically identifies Interval- Valued Fuzzy relations between words and their degree of membership using word embeddings and lexico-syntactic patterns. The experimental results for the word sense disambiguation problem show better outcomes when IVFHWN is being used in place of Type 1 Fuzzy Hindi WordNet and classical Hindi WordNet.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391473774",
    "type": "article"
  },
  {
    "title": "Improved BIO-Based Chinese Automatic Abstract-Generation Model",
    "doi": "https://doi.org/10.1145/3643695",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Qing Li; Weibin Wan; Yuming Zhao; Xiaoyan Jiang",
    "corresponding_authors": "",
    "abstract": "With its unique information-filtering function, text summarization technology has become a significant aspect of search engines and question-and-answer systems. However, existing models that include the copy mechanism often lack the ability to extract important fragments, resulting in generated content that suffers from thematic deviation and insufficient generalization. Specifically, Chinese automatic summarization using traditional generation methods often loses semantics because of its reliance on word lists. To address these issues, we proposed the novel BioCopy mechanism for the summarization task. By training the tags of predictive words and reducing the probability distribution range on the glossary, we enhanced the ability to generate continuous segments, which effectively solves the above problems. Additionally, we applied reinforced canonicality to the inputs to obtain better model results, making the model share the sub-network weight parameters and sparsing the model output to reduce the search space for model prediction. To further improve the model’s performance, we calculated the bilingual evaluation understudy (BLEU) score on the English dataset CNN/DailyMail to filter the thresholds and reduce the difficulty of word separation and the dependence of the output on the word list. We fully fine-tuned the model using the LCSTS dataset for the Chinese summarization task and conducted small-sample experiments using the CSL dataset. We also conducted ablation experiments on the Chinese dataset. The experimental results demonstrate that the optimized model can learn the semantic representation of the original text better than other models and performs well with small sample sizes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391568802",
    "type": "article"
  },
  {
    "title": "Disambiguation of Isolated Manipuri Tonal Contrast Word Pairs Using Acoustic Features",
    "doi": "https://doi.org/10.1145/3643830",
    "publication_date": "2024-02-12",
    "publication_year": 2024,
    "authors": "Thiyam Susma Devi; Pradip K. Das",
    "corresponding_authors": "",
    "abstract": "Manipuri is a low-resource, Tibeto-Burman tonal language spoken mainly in Manipur, a northeastern state of India. Tone identification is crucial to speech comprehension for tonal languages, where tone defines the word’s meaning. Automatic Speech Recognition for those languages can perform better by including tonal information from a powerful tone detection system. While significant research has been conducted on tonal languages like Mandarin, Thai, Cantonese, and Vietnamese, a notable gap exists in exploring Manipuri within this context. To address this gap, this work expands our previously developed handcrafted speech corpus, ManiTo, which comprises isolated Manipuri tonal contrast word pairs to study the tones of Manipuri. This extension includes contributions from 20 native speakers. Preliminary findings have confirmed that Manipuri has two unique tones, Falling and Level. The study then conducts a comprehensive acoustic feature analysis. Two sets of features based on Pitch contours, Jitter, and Shimmer measurements are investigated to distinguish the two tones of Manipuri. Support Vector Machine, Long Short-term Memory, Random Forest, and k-Nearest Neighbors are the classifiers adopted to validate the selected feature sets. The results indicate that the second set of features consistently outperformed the first set, demonstrating higher accuracy, particularly when utilizing the Random Forest classifier, which provides valuable insights for further advancements in speech recognition technology for low-resource tonal language Manipuri.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391746718",
    "type": "article"
  },
  {
    "title": "Exploration on Advanced Intelligent Algorithms of Artificial Intelligence for Verb Recognition in Machine Translation",
    "doi": "https://doi.org/10.1145/3649891",
    "publication_date": "2024-02-28",
    "publication_year": 2024,
    "authors": "Qinghua Ai; Qingyan Ai; J. L. Wang",
    "corresponding_authors": "",
    "abstract": "This article aimed to address the problems of word order confusion, context dependency, and ambiguity in traditional machine translation (MT) methods for verb recognition. By applying advanced intelligent algorithms of artificial intelligence, verb recognition can be better processed and the quality and accuracy of MT can be improved. Based on Neural machine translation (NMT), basic attention mechanisms, historical attention information, dynamically obtain information related to the generated words, and constraint mechanisms were introduced to embed semantic information, represent polysemy, and annotate semantic roles of verbs. This article used the Workshop on MT (WMT), British National Corpus (BNC), Gutenberg, Reuters Corpus, and OpenSubtitles corpus, and enhanced the data in the corpora. The improved NMT model was compared with traditional NMT models, Rule-Based MT (RBMT), and Statistical MT (SMT). The experimental results showed that the average verb semantic matching degree of the improved NMT model in five corpora was 0.85, and the average Bilingual Evaluation Understudy (BLEU) score in five corpora was 0.90. The improved NMT model in this article can effectively improve the accuracy of verb recognition in MT, providing new methods for verb recognition in MT.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392240509",
    "type": "article"
  },
  {
    "title": "NPEL: Neural Paired Entity Linking in Web Tables",
    "doi": "https://doi.org/10.1145/3652511",
    "publication_date": "2024-03-19",
    "publication_year": 2024,
    "authors": "Tianxing Wu; Lin Li; Huan Gao; Guilin Qi; Yuxiang Wang; Yuehua Li",
    "corresponding_authors": "",
    "abstract": "This paper studies entity linking (EL) in Web tables, which aims to link the string mentions in table cells to their referent entities in a knowledge base. Two main problems exist in previous studies: 1) contextual information is not well utilized in mention-entity similarity computation; 2) the assumption on entity coherence that all entities in the same row or column are highly related to each other is not always correct. In this paper, we propose NPEL , a new N eural P aired E ntity L inking framework, to overcome the above problems. In NPEL, we design a deep learning model with different neural networks and an attention mechanism, to model different kinds of contextual information of mentions and entities, for mention-entity similarity computation in Web tables. NPEL also relaxes the above assumption on entity coherence by a new paired entity linking algorithm, which iteratively selects two mentions with the highest confidence for EL. Experiments on real-world datasets exhibit that NPEL has the best performance compared with state-of-the-art baselines in different evaluation metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392956402",
    "type": "article"
  },
  {
    "title": "A Context-enhanced Adaptive Graph Network for Time-sensitive Question Answering",
    "doi": "https://doi.org/10.1145/3653674",
    "publication_date": "2024-03-22",
    "publication_year": 2024,
    "authors": "Jitong Li; Shaojuan Wu; Xiaowang Zhang; Zhiyong Feng",
    "corresponding_authors": "",
    "abstract": "Time-sensitive question answering is to answer questions limited to certain timestamps based on the given long document, which mixes abundant temporal events with an explicit or implicit timestamp. While existing models make great progress in answering time-sensitive questions, their performance degrades dramatically when a long distance separates the correct answer from the timestamp mentioned in the question. In this paper, we propose a Context-enhanced Adaptive Graph network (CoAG) to capture long-distance dependencies between sentences within the extracted question-related episodes. Specifically, we propose a time-aware episode extraction module that obtains question-related context based on timestamps in the question and document. As the involvement of episodes confuses sentences with adjacent timestamps, an adaptive message passing mechanism is designed to capture and transfer inter-sentence differences. In addition, we present a hybrid text encoder to highlight question-related context built on global information. Experimental results show that CoAG significantly improves compared to state-of-the-art models on five benchmarks. Moreover, our model has a noticeable advantage in solving long-distance time-sensitive questions, improving the EM scores by 2.03% to 6.04% on TimeQA-Hard.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393086040",
    "type": "article"
  },
  {
    "title": "Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts",
    "doi": "https://doi.org/10.1145/3654811",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Queenie Luo; Yung-Sung Chuang",
    "corresponding_authors": "",
    "abstract": "Scholars in the humanities heavily rely on ancient manuscripts to study history, religion, and socio-political structures of the past. Significant efforts have been devoted to digitizing these precious manuscripts using OCR technology. However, most manuscripts have been blemished over the centuries, making it unrealistic for OCR programs to accurately capture faded characters. This work presents the Transformer + Confidence Score mechanism architecture for post-processing Google’s Tibetan OCR-ed outputs. According to the Loss and Character Error Rate metrics, our Transformer + Confidence Score mechanism architecture proves superior to the Transformer, LSTM-to-LSTM, and GRU-to-GRU architectures. Our method can be adapted to any language dealing with post-processing OCR outputs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393342143",
    "type": "article"
  },
  {
    "title": "MRMI-TTS: Multi-reference audios and Mutual Information Driven Zero-shot Voice cloning",
    "doi": "https://doi.org/10.1145/3649501",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Yi-Ting Chen; Wanting Li; Buzhou Tang",
    "corresponding_authors": "",
    "abstract": "Voice cloning in text-to-speech (TTS) is the process of replicating the voice of a target speaker with limited data. Among various voice cloning techniques, this article focuses on zero-shot voice cloning. Although existing TTS models can generate high-quality speech for seen speakers, cloning the voice of an unseen speaker remains a challenging task. The key aspect of zero-shot voice cloning is to obtain a speaker embedding from the target speaker. Previous works have used a speaker encoder to obtain a fixed-size speaker embedding from a single reference audio unsupervised, but they suffer from insufficient speaker information and content information leakage in speaker embedding. To address these issues, this article proposes MRMI-TTS, a FastSpeech2-based framework that uses speaker embedding as a conditioning variable to provide speaker information. The MRMI-TTS extracts speaker embedding and content embedding from multi-reference audios using a speaker encoder and a content encoder. To obtain sufficient speaker information, multi-reference audios are selected based on sentence similarity. The proposed model applies mutual information minimization on the two embeddings to remove entangled information within each embedding. Experiments on the public English dataset VCTK show that our method can improve synthesized speech in terms of both similarity and naturalness, even for unseen speakers. Compared to state-of-the-art reference embedding learned methods, our method achieves the best performance on the zero-shot voice cloning task. Furthermore, we demonstrate that the proposed method has a better capability of maintaining the speaker embedding in different languages. Sample outputs are available on the demo page. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393342192",
    "type": "article"
  },
  {
    "title": "Graph4IUR: Incomplete Utterance Rewriting with Semantic Graph",
    "doi": "https://doi.org/10.1145/3653301",
    "publication_date": "2024-04-04",
    "publication_year": 2024,
    "authors": "Z. Gao; Jinke Wang; Tong Xu; Zhefeng Wang; Yu Yang; Jia Su; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Utterance rewriting aims to identify and supply the omitted information in human conversation, which further enables the downstream task to understand conversations more comprehensively. Recently, sequence edit methods, which leverage the overlap between two sentences, have been widely applied to narrow the search space confronted by the previous linear generation methods. However, these methods ignore the relationship between linguistic elements in the conversation, which reflects how the knowledge and thoughts are organized in human communication. In this case, although most of the content in rewritten sentences can be found in the context, we found that some connecting words expressing relationships are often missing, which results in the out-of-context problem for the previous sentence edit method. To that end, in this paper, we propose a new semantic Graph-based Incomplete Utterance Rewriting (Graph4IUR) framework, which takes the semantic graph to depict the relationship between linguistic elements and captures out-of-context words. Specifically, we adopt the Abstract Meaning Representation (AMR) [4] graph as the basic sentence-to-graph method to depict the dialogue from the graph perspective, which could well represent the high-level semantics relationships of sentences. Along this line, we further adapt the sentence editing models to rewrite without changing the sentence architecture, which brings a restriction to exploring the overlap part of the current and rewritten sentences in the IUR task. Extensive experimental results indicate that our Graph4IUR framework can effectively alleviate the out-of-context problem and improve the performance of the previous edit-based methods in the IUR task.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393934436",
    "type": "article"
  },
  {
    "title": "Performance of Binarization Algorithms on Tamizhi Inscription Images: An Analysis",
    "doi": "https://doi.org/10.1145/3656583",
    "publication_date": "2024-04-08",
    "publication_year": 2024,
    "authors": "Monisha Munivel; V. S. Felix Enigo",
    "corresponding_authors": "",
    "abstract": "Binarization of Tamizhi (Tamil-Brahmi) inscription images are highly challenging, as it is captured from very old stone inscriptions that exists around 3rd century BCE in India. The difficulty is due to the degradation of these inscriptions by environmental factors and human negligence over ages. Though many works have been carried out in the binarization of inscription images, very little research was performed for inscription images and no work has been reported for binarization of inscriptions inscribed on irregular medium. The findings of the analysis hold true to all writings that are carved in irregular background. This article reviews the performance of various binarization techniques on Tamizhi inscription images. Since no previous work was performed, we have applied the existing binarization algorithms on Tamizhi inscription images and analyzed the performance of these algorithms with proper reasoning. In the future, we believe that this reasoning on the results will help a new researcher to adapt or combine or devise new binarization techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394563633",
    "type": "article"
  },
  {
    "title": "Personalized Explainable Recommendations for Self-Attention Collaboration",
    "doi": "https://doi.org/10.1145/3657636",
    "publication_date": "2024-04-10",
    "publication_year": 2024,
    "authors": "Yongfu Zha; Xuanxuan Che; Lina Sun; Yumin Dong",
    "corresponding_authors": "",
    "abstract": "In recommender systems, providing reasonable explanations can enhance users’ comprehension of recommended results. Template-based explainable recommendation heavily relies on pre-defined templates, constraining the expressiveness of generated sentences and resulting in low-quality explanations. Recently, a novel approach was introduced, utilizing embedding representations of items and comments to address the issue of user IDs and item IDs not residing in the same semantic space as words, thus attributing linguistic meaning to IDs. However, these models often fail to fully exploit collaborative information within the data. In personalized recommendation and explanation processes, understanding the user’s emotional feedback and feature preferences is paramount. To address this, we propose a personalized explainable recommendation model based on self-attention collaboration. Initially, the model employs an attention network to amalgamate the user’s historical interaction feature preferences with their user ID information, while simultaneously integrating all feature information of the item with its item ID to enhance semantic ID representation. Subsequently, the model incorporates the user’s comment feature rhetoric and sentiment feedback to generate more personalized recommendation explanations utilizing a self-attention network. Experimental evaluations conducted on two datasets of varying scales demonstrate the superiority of our model over current state-of-the-art approaches, validating its effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394691924",
    "type": "article"
  },
  {
    "title": "Complexity Analysis of Chinese Text Based on the Construction Grammar Theory and Deep Learning",
    "doi": "https://doi.org/10.1145/3625390",
    "publication_date": "2024-04-10",
    "publication_year": 2024,
    "authors": "Changlin Wu; Chang-an Wu",
    "corresponding_authors": "",
    "abstract": "Due to the complexity of Chinese and the differences between Chinese and English, the application of Chinese text in the digital field has a certain complexity. Taking Chinese text in Open Relation Extraction (ORE) as the research object, the complexity of Chinese text is analyzed. An extraction system of word vectors based on construction grammar theory and Deep Learning (DL) is constructed to achieve smooth extraction of Chinese text. The work of this paper mainly includes the following aspects. To study the application of DL in the complexity analysis of Chinese text based on construction grammar, firstly, the connotation of construction grammar and its role in Chinese text analysis are explored. Secondly, from the perspective of the ORE of word vectors in language analysis, an ORE model based on word vectors is implemented. Moreover, an extraction method based on the distance of word vectors is proposed. The test results show that the F1 value of the proposed algorithm is 67% on the public WEB-500 and NYT-500 datasets, which is superior to other similar text extraction algorithms. When the recall rate is more than 30%, the accuracy of the proposed method is higher than several other latest language analysis systems. This indicates that the proposed Chinese text extraction system based on the DL algorithm and construction grammar theory has advantages in complexity analysis and can provide a new research idea for Chinese text analysis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394692203",
    "type": "article"
  },
  {
    "title": "An Interaction-Design Method Based upon a Modified Algorithm of Newton's Second Law of Motion",
    "doi": "https://doi.org/10.1145/3657634",
    "publication_date": "2024-04-20",
    "publication_year": 2024,
    "authors": "Qiao Feng; Tian Huang",
    "corresponding_authors": "",
    "abstract": "Newton's Second Law of Motion algorithm is crucial to interactive visual effects and interactive behavior in interface design. Designers can only utilize simple algorithm templates in interface design since they lack organized mathematical science, especially programming. Directly using Newton's Second Law of Motion algorithm introduces two interface design issues. First, the created picture has a simplistic impact, laborious interaction, too few interactive parts, and boring visual effects. Second, using this novel approach directly to interface design reduces creativity, originality, and cognitive inertia. This study suggests a Newton's Second Law–based algorithm modification. It provides a novel algorithm application idea and a design strategy based on algorithm change to enable new interface design. Algorithm design gives interface design a new viewpoint and improves content production. In the arithmetic process of Newton's Second Law of Motion algorithm, the introduction of repulsive force, reset force, shape, color, and other attributes of interactive objects, and the integration of other algorithms to transform its basic arithmetic logic, is conducive to the improvement of the visual effect of interaction design. It also improves users’ interaction experiences, sentiments, and desire to participate with design work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394975829",
    "type": "article"
  },
  {
    "title": "Share What You Already Know: Cross-Language-Script Transfer and Alignment for Sentiment Detection in Code-Mixed Data",
    "doi": "https://doi.org/10.1145/3661307",
    "publication_date": "2024-04-27",
    "publication_year": 2024,
    "authors": "Niraj Pahari; Kazutaka Shimada",
    "corresponding_authors": "",
    "abstract": "Code-switching entails mixing multiple languages. It is an increasingly occurring phenomenon in social media texts. Usually, code-mixed texts are written in a single script, even though the languages involved have different scripts. Pre-trained multilingual models primarily utilize the data in the native script of the language. In existing studies, the code-switched texts are utilized as they are. However, using the native script for each language can generate better representations of the text owing to the pre-trained knowledge. Therefore, a cross-language-script knowledge-sharing architecture utilizing the cross-attention and alignment of the representations of text in individual language scripts was proposed in this study. Experimental results on two different datasets containing Nepali-English and Hindi-English code-switched texts, demonstrate the effectiveness of the proposed method. The interpretation of the model using the model explainability technique illustrates the sharing of language-specific knowledge between language-specific representations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4395690874",
    "type": "article"
  },
  {
    "title": "Learning Domain Specific Sub-layer Latent Variable for Multi-Domain Adaptation Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3661305",
    "publication_date": "2024-04-29",
    "publication_year": 2024,
    "authors": "Shuanghong Huang; Chong Feng; Ge Shi; Zhengjun Li; Xuan Zhao; Xinyan Li; Xiaomei Wang",
    "corresponding_authors": "",
    "abstract": "Domain adaptation proves to be an effective solution for addressing inadequate translation performance within specific domains. However, the straightforward approach of mixing data from multiple domains to obtain the multi-domain neural machine translation (NMT) model can give rise to the parameter interference between domains problem, resulting in a degradation of overall performance. To address this, we introduce a multi-domain adaptive NMT method aimed at learning domain specific sub-layer latent variable and employ the Gumbel-Softmax reparameterization technique to concurrently train both model parameters and domain specific sub-layer latent variable. This approach facilitates learning private domain-specific knowledge while sharing common domain-invariant knowledge, effectively mitigating the parameter interference problem. The experimental results show that our proposed method significantly improved by up to 7.68 and 3.71 BLEU compared with the baseline model in English-German and Chinese-English public multi-domain datasets, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396229240",
    "type": "article"
  },
  {
    "title": "Emotion Detection System for Malayalam Text using Deep Learning and Transformers",
    "doi": "https://doi.org/10.1145/3663475",
    "publication_date": "2024-05-01",
    "publication_year": 2024,
    "authors": "K Anuja; P. C. Reghu Raj; K. R. Remesh Babu",
    "corresponding_authors": "",
    "abstract": "Recent advances in Natural Language Processing (NLP) have improved the performance of the systems that perform tasks, such as Emotion Detection (ED), Information Retrieval, Translation, etc., in resource-rich languages like English and Chinese. But similar advancements have not been made in Malayalam due to the dearth of annotated datasets. Because of its rich morphology, free word order and agglutinative character, data preparation in Malayalam is highly challenging. In this paper, we employ traditional Machine Learning (ML) techniques such as support vector machines (SVM) and multilayer perceptrons (MLP), and recent deep learning methods such as Recurrent Neural Networks (RNN) and advanced transformer-based methodologies to train an emotion detection system. This work stands out since all the previous attempts to extract emotions from Malayalam text have relied on lexicons, which are inappropriate for handling large amounts of data. By tweaking the hyperparameters, we enhanced the transformer-based model known as MuRIL to obtain an accuracy of 79%, which is then compared with the only state-of-the-art (SOTA) model. We found that the proposed techniques surpass the SOTA system available for detecting emotions in Malayalam reported so far.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396561412",
    "type": "article"
  },
  {
    "title": "Multi-Lingual Representation of Natural Language Processing for Low Resource Asian Language Processing Systems",
    "doi": "https://doi.org/10.1145/3603169",
    "publication_date": "2024-05-10",
    "publication_year": 2024,
    "authors": "Elena Verdú; Yuri Vanessa Nieto; Nasir Saleem",
    "corresponding_authors": "",
    "abstract": "Natural language processing (NLP) is a vibrant field of interdisciplinary Computer Science research. Ultimately, NLP seeks to build intelligence into software so that software will be able to process a natural language as skillfully and artfully as ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396813249",
    "type": "article"
  },
  {
    "title": "Multi Task Learning Based Shallow Parsing for Indian Languages",
    "doi": "https://doi.org/10.1145/3664620",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Pruthwik Mishra; Vandan Mujadia; Dipti Misra Sharma",
    "corresponding_authors": "",
    "abstract": "Shallow Parsing is an important step for many Natural Language Processing tasks. Although shallow parsing has a rich history for resource rich languages, it is not the case for most Indian languages. Shallow Parsing consists of POS Tagging and Chunking. Our study focuses on developing shallow parsers for Indian languages. As part of shallow parsing, we included morph analysis as well. For the study, we first consolidated available shallow parsing corpora for seven Indian Languages (Hindi, Kannada, Bangla, Malayalam, Marathi, Urdu, Telugu) for which treebanks are publicly available. We then trained models to achieve state-of-the-art performance for shallow parsing in these languages for multiple domains. Since analyzing the performance of model predictions at sentence level is more realistic, we report the performance of these shallow parsers not only at the token level, but also at the sentence level. We also present machine learning techniques for multi-task shallow parsing. Our experiments show that fine-tuned contextual embedding with multi-task learning improves the performance of multiple as well as individual shallow parsing tasks across different domains. We show the transfer learning capability of these models by creating shallow parsers (only with POS and Chunk) for Gujarati, Odia, and Punjabi for which no treebanks are available. As a part of this work, we will be releasing the Indian Languages Shallow Linguistic (ILSL) benchmarks for 10 Indian languages, including both the major language families Indo-Aryan and Dravidian as common building blocks that can be used to evaluate and understand various linguistic phenomena found in Indian languages and how well newer approaches can tackle them.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396834323",
    "type": "article"
  },
  {
    "title": "Marathi to Indian Sign Language Machine Translation",
    "doi": "https://doi.org/10.1145/3664609",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Suvarna R. Bhagwat; R. P. Bhavsar; B. V. Pawar",
    "corresponding_authors": "",
    "abstract": "Machine translation has been a prominent field of research, contributing significantly to human life enhancement. Sign language machine translation, a subfield, focuses on translating spoken language content into sign language and vice versa, thereby facilitating communication between the normal hearing and hard-of-hearing communities, promoting inclusivity. This study presents the development of a ‘sign language machine translation system’ converting simple Marathi sentences into Indian Sign Language (ISL) glosses and animation. Given the low-resource nature of both languages, a phrase-level rule-based approach was employed for the translation. Initial encoding of translation rules relied on basic linguistic knowledge of Marathi and ISL, with subsequent incorporation of rules to address 'simultaneous morphological' features in ISL. These rules were applied during the ‘generation phase’ of translation to dynamically adjust phonological sign parameters, resulting in improved target sentence fluency. The paper provides a detailed description of the system architecture, translation rules, and comprehensive experimentation. Rigorous evaluation efforts were undertaken, encompassing various linguistic features, and the findings are discussed herein. The web-based version of the system serves as an interpreter for brief communications and can support the teaching and learning of sign language and its grammar in schools for hard-of-hearing students.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396861009",
    "type": "article"
  },
  {
    "title": "Abusive Language Detection in Khasi Social Media Comments",
    "doi": "https://doi.org/10.1145/3664285",
    "publication_date": "2024-05-14",
    "publication_year": 2024,
    "authors": "Arup Baruah; Lakhamti Wahlang; Firstbornson Jyrwa; Floriginia Shadap; Ferdous Ahmed Barbhuiya; Kuntal Dey",
    "corresponding_authors": "",
    "abstract": "This paper describes the work performed for automated abusive language detection in the Khasi language, a low-resource language spoken primarily in the state of Meghalaya, India. A dataset named Khasi Abusive Language Dataset (KALD) was created which consists of 4,573 human-annotated Khasi YouTube and Facebook comments. A corpus of Khasi text was built and it was used to create Khasi word2vec and fastText word embeddings. Deep learning, traditional machine learning, and ensemble models were used in the study. Experiments were performed using word2vec, fastText, and topic vectors obtained using LDA. Experiments were also performed to check if zero-shot cross-lingual nature of language models such as LaBSE and LASER can be utilized for abusive language detection in the Khasi language. The best F1 score of 0.90725 was obtained by an XGBoost classifier. After feature selection and rebalancing of the dataset, F1 score of 0.91828 and 0.91945 were obtained by an SVM based classifiers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396903880",
    "type": "article"
  },
  {
    "title": "Scoring Multi-hop Question Decomposition Using Masked Language Models",
    "doi": "https://doi.org/10.1145/3665140",
    "publication_date": "2024-05-15",
    "publication_year": 2024,
    "authors": "Abdellah Hamouda Sidhoum; M’hamed Mataoui; Faouzi Sebbak; Adil Imad Eddine Hosni; Kamel Smaı̈li",
    "corresponding_authors": "",
    "abstract": "Question answering (QA) is a sub-field of Natural Language Processing (NLP) that focuses on developing systems capable of answering natural language queries. Within this domain, multi-hop question answering represents an advanced QA task that requires gathering and reasoning over multiple pieces of information from diverse sources or passages. To handle the complexity of multi-hop questions, question decomposition has been proven to be a valuable approach. This technique involves breaking down complex questions into simpler sub-questions, reducing the complexity of the problem. However, it’s worth noting that existing question decomposition methods often rely on training data, which may not always be readily available for low-resource languages or specialized domains. To address this issue, we propose a novel approach that utilizes pre-trained masked language models to score decomposition candidates in a zero-shot manner. The method involves generating decomposition candidates, scoring them using a pseudo-log likelihood estimation, and ranking them based on their scores. To evaluate the efficacy of the decomposition process, we conducted experiments on two datasets annotated on decomposition in two different languages, Arabic and English. Subsequently, we integrated our approach into a complete QA system and conducted a reading comprehension performance evaluation on the HotpotQA dataset. The obtained results emphasize that while the system exhibited a small drop in performance, it still maintained a significant advance compared to the baseline model. The proposed approach highlights the efficiency of the language model scoring technique in complex reasoning tasks such as multi-hop question decomposition.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396927733",
    "type": "article"
  },
  {
    "title": "Towards Better Quantity Representations for Solving Math Word Problems",
    "doi": "https://doi.org/10.1145/3665644",
    "publication_date": "2024-05-18",
    "publication_year": 2024,
    "authors": "Runxin Sun; Shizhu He; Jun Zhao; Kang Liu",
    "corresponding_authors": "",
    "abstract": "Solving a math word problem requires selecting quantities in it and performing appropriate arithmetic operations to obtain the answer. For deep learning-based methods, it is vital to obtain good quantity representations, i.e., to selectively and emphatically aggregate information in the context of quantities. However, existing works have not paid much attention to this aspect. Many works simply encode quantities as ordinary tokens, or use some implicit or rule-based methods to select information in their context. This leads to poor results when dealing with linguistic variations and confounding quantities. This article proposes a novel method to identify question-related distinguishing features of quantities by contrasting their context with the question and the context of other quantities, thereby enhancing the representation of quantities. Our method not only considers the contrastive relationship between quantities but also considers multiple relationships jointly. Besides, we propose two auxiliary tasks to further guide the representation learning of quantities: (1) predicting whether a quantity is used in the question and (2) predicting the relations (operators) between quantities given the question. Experimental results show that our method outperforms previous methods on SVAMP and ASDiv-A under similar settings, even some newly released strong baselines. Supplementary experiments further confirm that our method indeed improves the performance of quantity selection by improving the representation of both quantities and questions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4397033259",
    "type": "article"
  },
  {
    "title": "Document-Level Relation Extraction Based on Machine Reading Comprehension and Hybrid Pointer-sequence Labeling",
    "doi": "https://doi.org/10.1145/3666042",
    "publication_date": "2024-06-01",
    "publication_year": 2024,
    "authors": "Xiaoyi Wang; Jie Liu; Jiong Wang; Jianyong Duan; Guixia Guan; Qing Zhang; Jianshe Zhou",
    "corresponding_authors": "",
    "abstract": "Document-level relational extraction requires reading, memorization, and reasoning to discover relevant factual information in multiple sentences. It is difficult for the current hierarchical network and graph network methods to fully capture the structural information behind the document and make natural reasoning from the context. Different from the previous methods, this article reconstructs the relation extraction task into a machine reading comprehension task. Each pair of entities and relationships is characterized by a question template, and the extraction of entities and relationships is translated into identifying answers from the context. To enhance the context comprehension ability of the extraction model and achieve more precise extraction, we introduce large language models (LLMs) during question construction, enabling the generation of exemplary answers. Besides, to solve the multi-label and multi-entity problems in documents, we propose a new answer extraction model based on hybrid pointer-sequence labeling, which improves the reasoning ability of the model and realizes the extraction of zero or multiple answers in documents. Extensive experiments on three public datasets show that the proposed method is effective.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399262432",
    "type": "article"
  },
  {
    "title": "FedREAS: A Robust Efficient Aggregation and Selection Framework for Federated Learning",
    "doi": "https://doi.org/10.1145/3670689",
    "publication_date": "2024-06-04",
    "publication_year": 2024,
    "authors": "Shuming Fan; Chuanjia Wang; Xinyu Ruan; Hongjian Shi; Ruhui Ma; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "In the field of Natural Language Processing (NLP), Deep Learning (DL) and Neural Network (NN) technologies have been widely applied to machine translation and sentiment analysis and have demonstrated outstanding performance. In recent years, NLP applications have also combined multimodal data, such as visual and audio, continuously improving language processing performance. At the same time, the size of Neural Network models is increasing, and many models cannot be deployed on devices with limited computing resources. Deploying models on cloud platforms has become a trend. However, deploying models in the cloud introduces new privacy risks for endpoint data, despite overcoming computational limitations. Federated Learning (FL) methods protect local data by keeping the data on the client side and only sending local updates to the central server. However, the FL architecture still has problems, such as vulnerability to adversarial attacks and non-IID data distribution. In this work, we propose a Federated Learning aggregation method called FedREAS. The server uses a benchmark dataset to train a global model and obtains benchmark updates in this method. Before aggregating local updates, the server adjusts the local updates using the benchmark updates and then returns the adjusted benchmark updates. Then, based on the similarity between the adjusted local updates and the adjusted benchmark updates, the server aggregates these local updates to obtain a more robust update. This method also improves the client selection process. FedREAS selects suitable clients for training at the beginning of each round based on specific strategies, the similarity of the previous round’s updates, and the submitted data. We conduct experiments on different datasets and compare FedREAS with other Federated Learning methods. The results show that FedREAS outperforms other methods regarding model performance and resistance to attacks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399320990",
    "type": "article"
  },
  {
    "title": "AraSpot: Arabic Spoken Command Spotting",
    "doi": "https://doi.org/10.1145/3674968",
    "publication_date": "2024-06-26",
    "publication_year": 2024,
    "authors": "Mahmoud Salhab; Haidar Harmanani",
    "corresponding_authors": "",
    "abstract": "Spoken keyword spotting is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a state-of-the-art 99.59% result outperforming previous approaches. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400047229",
    "type": "article"
  },
  {
    "title": "Travel Agency Task Dialogue Corpus: A Multimodal Dataset with Age-Diverse Speakers",
    "doi": "https://doi.org/10.1145/3675166",
    "publication_date": "2024-06-26",
    "publication_year": 2024,
    "authors": "Michimasa Inaba; Yuya Chiba; Zhiyang Qi; Ryuichiro Higashinaka; Kazunori Komatani; Yusuke Miyao; Takayuki Nagai",
    "corresponding_authors": "",
    "abstract": "When individuals communicate, they use different vocabularies, speaking speeds, facial expressions, and gestural languages, depending on those with whom they are speaking. This study focuses on the age of the speaker as a factor that affects the style of communication. We collected a multimodal dialogue corpus with various speaker ages. We used travel as the topic, as it interests people of all ages, and we set up a task based on a tourism consultation between an operator and a customer at a travel agency. This article presents the details of the dialogue task, collection procedures and annotations, and analysis of the characteristics of the dialogues and facial expressions, focusing on the age of the speakers. The results of the analysis suggest that the adult speakers have more independent opinions, the older speakers express their opinions more frequently than other age groups, and those in the operator role smile more frequently at minors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400047235",
    "type": "article"
  },
  {
    "title": "BERT-Inspired Progressive Stacking to Enhance Spelling Correction in Bengali Text",
    "doi": "https://doi.org/10.1145/3669941",
    "publication_date": "2024-07-05",
    "publication_year": 2024,
    "authors": "Debajyoty Banik; Saneyika Das; Sheshikala Martha; Achyut Shankar",
    "corresponding_authors": "",
    "abstract": "Common spelling checks in the current digital era have trouble reading languages such as Bengali, which employ English letters differently. In response, we have created a better Bidirectional Encoder Representations from Transformers (BERT)–based spell checker that makes use of a convolutional neural network (CNN) sub-model (Semantic Network). Our novelty, which we term progressive stacking , concentrates on improving BERT model training while expediting the corrective process. We discovered that, when comparing shallow and deep versions, deeper models could require less training time. There is potential for improving spelling corrections with this technique. We categorized and utilized as a test set a 6,300-word dataset that Nayadiganta Mohiuddin supplied, some of which had spelling errors. The most popular terms were the same as those found in the Prothom-Alo artificial error dataset.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400360658",
    "type": "article"
  },
  {
    "title": "CoMix: Confronting with Noisy Label Learning with Co-training Strategies on Textual Mislabeling",
    "doi": "https://doi.org/10.1145/3678175",
    "publication_date": "2024-07-15",
    "publication_year": 2024,
    "authors": "Shu Zhao; Zhuoer Zhao; Yangyang Xu; Xiao Sun",
    "corresponding_authors": "",
    "abstract": "The existence of noisy labels is inevitable in real-world large-scale corpora. As deep neural networks are notably vulnerable to overfitting on noisy samples, this highlights the importance of the ability of language models to resist noise for efficient training. However, little attention has been paid to alleviating the influence of label noise in natural language processing. To address this problem, we present CoMix, a robust Noise-against training strategy taking advantage of Co-training that deals with textual annotation errors in text classification tasks. In our proposed framework, the original training set is first split into labeled and unlabeled subsets according to a sample partition criteria and then applies label refurbishment on the unlabeled subsets. We implement textual interpolation in hidden space between samples on the updated subsets. Meanwhile, we employ peer diverged networks simultaneously leveraging co-training strategies to avoid the accumulation of confirm bias. Experimental results on three popular text classification benchmarks demonstrate the effectiveness of CoMix in bolstering the network’s resistance to label mislabeling under various noise types and ratios, which also outperforms the state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400656927",
    "type": "article"
  },
  {
    "title": "Exploring the Correlation between Emojis and Mood Expression in Thai Twitter Discourse",
    "doi": "https://doi.org/10.1145/3680543",
    "publication_date": "2024-07-24",
    "publication_year": 2024,
    "authors": "Attapol Rutherford; Pawitsapak Akarajaradwong",
    "corresponding_authors": "",
    "abstract": "Mood, a long-lasting affective state detached from specific stimuli, plays an important role in behavior. Although sentiment analysis and emotion classification have garnered attention, research on mood classification remains in its early stages. This study adopts a two-dimensional structure of affect, comprising “pleasantness” and “activation,” to classify mood patterns. Emojis, graphic symbols representing emotions and concepts, are widely used in computer-mediated communication. Unlike previous studies that consider emojis as direct labels for emotion or sentiment, this work uses a pre-trained large language model which integrates both text and emojis to develop a mood classification model. Our contributions are three-fold. First, we annotate 10,000 Thai tweets with mood to train the models and release the dataset to the public. Second, we show that emojis contribute to determining mood to a lesser extent than text, far from mapping directly to mood. Third, through the application of the trained model, we observe the correlation of moods during the Thai political turmoil of 2019–2020 on Thai X (formerly known as Twitter) and find a significant correlation. These moods closely reflect the news events and reveal one side of Thai public opinion during the turmoil.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400955226",
    "type": "article"
  },
  {
    "title": "Word Sense Disambiguation Combining Knowledge Graph And Text Hierarchical Structure",
    "doi": "https://doi.org/10.1145/3677524",
    "publication_date": "2024-07-25",
    "publication_year": 2024,
    "authors": "Yukun Cao; Chengkun Jin; Yijia Tang; Ziyue Wei",
    "corresponding_authors": "",
    "abstract": "Current supervised word sense disambiguation models have obtained high disambiguation results using annotated information of different word senses and pre-trained language models. However, the semantic data of the supervised word sense disambiguation models are in the form of short texts, and much of the corpus information is not rich enough to distinguish the semantics in different scenarios. This article proposes a bi-encoder word sense disambiguation method combining a knowledge graph and text hierarchy structure, by introducing structured knowledge from the knowledge graph to supplement more extended semantic information, using the hierarchy of contextual input text to describe the meaning of words and phrases, and constructing a BERT-based bi-encoder, introducing a graph attention network to reduce the noise information in the contextual input text, so as to improve the disambiguation accuracy of the target words in phrase form and ultimately improve the disambiguation effectiveness of the method. By comparing the method with the latest nine comparison algorithms in five test datasets, the disambiguation accuracy of the method mostly outperformed the comparison algorithms and achieved better results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400984306",
    "type": "article"
  },
  {
    "title": "Adaptive Semantic Information Extraction of Tibetan Opera Mask with Recall Loss",
    "doi": "https://doi.org/10.1145/3666041",
    "publication_date": "2024-07-26",
    "publication_year": 2024,
    "authors": "Wen Yao; Jie Li; Donghong Cai; Zhicheng Dong; Fangkai Cai; Ping Lan; Quan Zhou",
    "corresponding_authors": "",
    "abstract": "With the development of artificial intelligence, natural language processing enables us to better understand and utilize semantic information. However, traditional object detection algorithms cannot get an effective performance, when dealed with Tibetan opera mask datasets which have the properties of limited samples, symmetrical patterns and high inter-class distances. In order to solve this issue, we propose a novel feature representation model with recall loss function for detecting different marks. In the model, we develop an adaptive feature extraction network with fused layers to extract features. Furthermore, a lightweight efficient attention mechanism is designed to enhance the significance of key features. Additionally, a recall loss function is proposed to increase the differences among classes. Finally, experimental results on the dataset of Tibetan opera mask demonstrate that our proposed model outperforms compared models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401029819",
    "type": "article"
  },
  {
    "title": "Can Rhyme Consistency Score be used as a Feature in Stylistics? A Statistical Endeavour with Hindi Poetry",
    "doi": "https://doi.org/10.1145/3681789",
    "publication_date": "2024-07-29",
    "publication_year": 2024,
    "authors": "Niraj Kumar Singh; Komal Naaz; Raj Aryan",
    "corresponding_authors": "",
    "abstract": "Stylistics is the study and analysis of linguistic style using the tools of language. It is an investigation of language elements employed in a spoken or written piece under examination. Rhyme is one such element that can help to inspect style in an artistic piece of writing. It is a literary device that adds melody and aesthetics to any poetic composition. Whether intentionally or unintentionally, the authors’ skill in employing this literary trick will reveal something about their personality in their work. The rhyming quality of a poem has been proven to be a feature in studying literary pieces in the state-of-the-art work that analyses Doha composition using rhyme quality. However, the consistency of rhyme quality throughout a writer's work makes more sense when thinking from a stylistic viewpoint. This article focuses on studying and analyzing poetry to identify the similarities and differences across writers' styles while utilizing the extent of rhyme consistency as the key feature. The research examines the works of five prominent Hindi poets, namely, Ramdhari Singh Dinkar, Jayshankar Prasad, Sumitranandan Pant, Mahadevi Verma, and Harivansh Rai Bachchan, while using their poems as its case study. It provides the rhyme consistency score as a novel stylistic function verified using relevant statistical tests. In the process of achieving the mentioned, this article contributes a structured dataset, ARCSet , and well-posed style conclusions about the five renowned authors of Hindi literature.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401090504",
    "type": "article"
  },
  {
    "title": "Deep Neural Network with a Characteristic Analysis for Seal Stroke Recognition",
    "doi": "https://doi.org/10.1145/3676883",
    "publication_date": "2024-07-30",
    "publication_year": 2024,
    "authors": "Xingyu Cui; Yong Li; Lili Xu",
    "corresponding_authors": "",
    "abstract": "Seal characters are derived from ancient Chinese pictographs, naturally inheriting pictographic characteristics and complex structures. As the essential components of seal characters, seal strokes play a vital role in seal character recognition, composition and writing, so accurate recognition of seal strokes can greatly promote the investigation of seal characters. Inspired by curve fitting, we propose a new model called the characteristic analysis neural network (CANN) for seal stroke recognition. Instead of indiscriminate grasping of feature information in regular neural networks, we design an efficient approximation technique based on the piecewise Bezier curves that can effectively facilitate structural compression and lossless feature extraction. The feature extraction capability of Bezier approximation helps the methodology achieve impressive recognition accuracy not only on the seal strokes but also on any curve-based symbols. Furthermore, the hierarchical structure of the deep learning strategy is inherited and improved for better performance with high generalisation. Experiments conducted on different types of strokes verify that CANN obtains superior performance on both seal strokes and other smooth symbols. The robustness and the effectiveness of CANN are also demonstrated with minimal learning cost compared to other state-of-art models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401127089",
    "type": "article"
  },
  {
    "title": "Improved Tibetan Word Vectors Models Based on Position Information Fusion",
    "doi": "https://doi.org/10.1145/3681787",
    "publication_date": "2024-08-01",
    "publication_year": 2024,
    "authors": "Hui Lv; H. Y. Lv; Yang Liu; Jun Shen; La Duo; Li Yan; Qingguo Zhou; Binbin Yong",
    "corresponding_authors": "",
    "abstract": "Tibetan language processing is crucial for preserving its rich cultural heritage and reducing communication barriers between different languages. However, as a low-resource language, the development of Tibetan natural language processing has lagged behind. To address the unique and complex structural information of Tibetan, this article improves the embedding model based on fundamental Tibetan Component-and-Character-and-Word-based Embedding (TCCWE) to enhance the effectiveness of word vector representation. We incorporate position information into the training of Tibetan word vectors, developing models based on components, characters, and their integration. Furthermore, to evaluate the effectiveness of these word vectors, we propose an intrinsic evaluation set, wordsimT, based on k -means clustering. Experimental results demonstrate that the character-based positional vector integration model achieves a Spearman's rank correlation coefficient of 79.99% on the wordsimT benchmark, outperforming the baseline TCCWE model by 1.51%. Additionally, we validate the proposed models in downstream text classification tasks. These findings underscore the importance of incorporating positional information in Tibetan word vectors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401204023",
    "type": "article"
  },
  {
    "title": "Enhancing Lyrics Rewriting with Weak Supervision from Grammatical Error Correction Pre-training and Reference Knowledge Fusion",
    "doi": "https://doi.org/10.1145/3687126",
    "publication_date": "2024-08-06",
    "publication_year": 2024,
    "authors": "Jiajia Li; Ping Wang; Zuchao Li; Kevin Parnow; Hai Zhao; Weiping Ding",
    "corresponding_authors": "",
    "abstract": "Lyric rewriting involves taking the original lyrics of a song and creatively rephrasing them while preserving their core meaning and emotional essence. Sequence-to-sequence methods often face the problem of lack of annotated corpus and difficulty in understanding lyrics when dealing with the lyric rewriting task. Inspired by the language rewriting technique, grammatical error correction (GEC) and sequence-to-sequence generation techniques, and neural machine translation (NMT) methods, we propose novel self-supervised learning methods that can effectively solve the problem of the lack of a lyric rewriting corpus. In addition, we also propose a new pretrained DAE Transformer model with data prior knowledge fusion to enhance the lyric rewriting ability. The reference-as-context model (RaC-Large) constructed by us based on these two methods achieves the best results in comparison with the baseline including large language models, fully verifying the effectiveness of the new method. We also validate the effectiveness of our approach on GEC and NMT tasks, further demonstrating the potential of our approach on a broad range of sequence-to-sequence tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401367291",
    "type": "article"
  },
  {
    "title": "A DENSE SPATIAL NETWORK MODEL FOR EMOTION RECOGNITION USING LEARNING APPROACHES",
    "doi": "https://doi.org/10.1145/3688000",
    "publication_date": "2024-08-10",
    "publication_year": 2024,
    "authors": "Lakshmi Lalitha; Dinesh Kumar Anguraj",
    "corresponding_authors": "",
    "abstract": "Researchers are increasingly eager to develop techniques to extract emotional data from new sources due to the exponential growth of subjective information on Web 2.0. One of the most challenging aspects of textual emotion detection is the collection of data with emotion labels, given the subjectivity involved in labeling emotions. To address this significant issue, our research aims to aid in the development of effective solutions. We propose a Deep Convolutional Belief-based Spatial Network Model (DCB-SNM) as a semi-automated technique to tackle this challenge. This model involves two basic phases of analysis: text and video. In this process, pre-trained annotators identify the dominant emotion. Our work evaluates the impact of this automatic pre-annotation approach on manual emotion annotation from the perspectives of annotation time and agreement. The data on annotation time indicates an increase of roughly 20% when the pre-annotation procedure is utilized, without negatively affecting the annotators' skill. This demonstrates the benefits of pre-annotation approaches. Additionally, pre-annotation proves to be particularly advantageous for contributors with low prediction accuracy, enhancing overall annotation efficiency and reliability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401483842",
    "type": "article"
  },
  {
    "title": "XLIT: A Method to Bridge Task Discrepancy in Machine Translation Pre-training",
    "doi": "https://doi.org/10.1145/3689630",
    "publication_date": "2024-08-23",
    "publication_year": 2024,
    "authors": "Khang Pham; Long Nguyen; Điền Đinh",
    "corresponding_authors": "",
    "abstract": "Transfer learning from pre-trained language models to encoder-decoder translation models faces a challenge due to the mismatch between the tasks of pre-training and fine-tuning. Pre-trained models are not explicitly trained to understand the semantic interactions between different languages. To address this issue, a cross-lingual embedding space is used as an interface during the pre-training phase. This approach enables the decoder inputs to attend to the encoder outputs, similar to the fine-tuning process. However, the effectiveness of this transfer heavily relies on the quality of the pre-trained unsupervised cross-lingual embeddings, which introduces complexity and reduces reproducibility. In this study, we propose a pre-training method called Cross-lingual Interaction Transfer (XLIT), which does not depend on other embedding techniques. XLIT effectively reconciles the task discrepancy in machine translation fine-tuning. We conducted extensive experiments involving four low-resource and six very low-resource translation directions. The results of our experiments demonstrate that our method surpasses randomly initialized models and previous pre-training techniques by up to 9.4 BLEU. Furthermore, we demonstrate that our method achieves comparable performance when pre-trained with large-scale monolingual data from various languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401815194",
    "type": "article"
  },
  {
    "title": "An Unsupervised Domain-Adaptive Framework for Chinese Spelling Checking",
    "doi": "https://doi.org/10.1145/3689821",
    "publication_date": "2024-08-27",
    "publication_year": 2024,
    "authors": "Xi Wang; Ruoqing Zhao; Jing Li; Piji Li",
    "corresponding_authors": "",
    "abstract": "Chinese Spelling Check (CSC) is a meaningful task in the area of natural language processing, which aims at detecting spelling errors in Chinese texts and then correcting these errors. Current typical CSC models have shown impressive performance in general datasets with the help of pretrained language models such as BERT, but they suffer great performance loss in downstream tasks with domain-specific terms because they are primarily trained on general corpora. To verify the cross-domain adaptation ability of these models, we build three new datasets with abundant domain-specific terms on financial, medical, and legal domains and conduct empirical investigations on them in the corresponding domain-specific test datasets to verify the cross-domain adaptation ability. In response to the poor performance of the existing models, we propose a framework named uChecker, which utilizes an unsupervised method in spelling error detection and correction. Experimental results prove that uChecker can perform well in domain-specific test datasets while not losing its performance in the general domain.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401915981",
    "type": "article"
  },
  {
    "title": "Improving Tone Recognition Performance using Wav2vec 2.0-Based Learned Representation in Yoruba, a Low-Resourced Language",
    "doi": "https://doi.org/10.1145/3690384",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Saint Germes Bienvenu Bengono Obiang; Norbert Tsopzé; Paulin Melatagia Yonta; Jean-François Bonastre; T. Jimenez",
    "corresponding_authors": "",
    "abstract": "Many sub-Saharan African languages are categorized as tone languages and for the most part, they are classified as low resource languages due to the limited resources and tools available to process these languages. Identifying the tone associated with a syllable is therefore a key challenge for speech recognition in these languages. We propose models that automate the recognition of tones in continuous speech that can easily be incorporated into a speech recognition pipeline for these languages. We have investigated different neural architectures as well as several features extraction algorithms in speech (Filter banks, LEAF, Cestrogram, MFCC). In the context of low-resource languages, we also evaluated Wav2vec models for this task. In this work, we use a public speech recognition dataset on Yoruba. As for the results, using the combination of features obtained from CS (Cestrogram) and FB (Filters Bank), we obtain a minimum TER (Tone Error Rate) of 19.54% while the evaluations of the models using Wav2vec 2.0, we have a TER of 17.72% demonstrating that the use of Wav2vec 2.0 provides better performance than the models used in the literature for tone identification on low-resource languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402057031",
    "type": "article"
  },
  {
    "title": "Cross-linguistic rhythm analysis of Mising and Assamese",
    "doi": "https://doi.org/10.1145/3694785",
    "publication_date": "2024-09-04",
    "publication_year": 2024,
    "authors": "Parismita Gogoi; Priyankoo Sarmah; S. R. Mahadeva Prasanna",
    "corresponding_authors": "",
    "abstract": "The objective of the current study is to explore a quantitative frequency domain technique to evaluate rhythm in spontaneous speech data of 19 native speakers of Mising and Assamese, two low-resourced languages spoken in Assam, North-East India. The concept of analyzing speech rhythm using amplitude modulation (AM) low-frequency (LF) spectrum, also known as rhythm formant analysis (RFA), is initially put forth by Gibbon and Li [ 17 ]. We propose three features from rhythm formants of the LF spectrum and also explore discrete cosine transform (DCT)–based characterization of the retrieved LF spectrum. We aim to distinguish the rhythm of Assamese and two Mising dialects, namely Pagro and Delu, with the aid of machine learning techniques fed with the derived features as input. We have observed that the features are efficient in classifying Assamese vs Pagro and Assamese vs Delu with an accuracy of 92.73% and 91.15%, respectively. The experimental analysis further reveals that Assamese is rhythmically closer to Delu than Pagro.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402229417",
    "type": "article"
  },
  {
    "title": "Unveiling Transformative Insights via Cross-Modal Learning and Natural Language Processing for Enhanced Supply Chain Intelligence",
    "doi": "https://doi.org/10.1145/3687306",
    "publication_date": "2024-09-12",
    "publication_year": 2024,
    "authors": "Xiaobing Wu",
    "corresponding_authors": "Xiaobing Wu",
    "abstract": "This day's quickly developing business landscape, supply chains have become more globalized, intricate, and multi-covering, making them crucial for companies to navigate through disruptions and unpredictability. The major which are addressed in the supply chain process are lack of transparency and visibility of the supply chain network and that's leads to delay and inefficiency in the process. In order to overcome those drawbacks in the supply chain process, in this article an enhanced supply chain intelligence is developed which performs Unveiling Transformative Insights using the learning process like Cross-Modal Learning (CML) and Natural Language Processing (NLP). The implementation of these techniques is carried out in the software Python. This analysis consists of certain calculation called enhanced supply chain analysis, sales revenue Vs SKU analysis, various modes cost analysis, Lead time vs different supplier and location. The comparative analysis is performed among the technique like RF regression, SARIMA-LSTM-BP and BiLSTM model. The parameters which are involved in this performance analysis are MAE, MSE, RMSE and R^2.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402530277",
    "type": "article"
  },
  {
    "title": "Knowledge Graph Guided Neural Machine Translation with Dynamic Reinforce-selected Triples",
    "doi": "https://doi.org/10.1145/3696664",
    "publication_date": "2024-09-24",
    "publication_year": 2024,
    "authors": "Yang Zhao; Xiaomian Kang; Yaping Zhang; Jiajun Zhang; Yu Zhou; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Previous methods incorporating knowledge graphs (KGs) into neural machine translation (NMT) adopt a static knowledge utilization strategy, that introduces many useless knowledge triples and makes the useful triples difficult be utilized by NMT. To address this problem, we propose a KG guided NMT model with dynamic reinforce-selected triples. The proposed methods could dynamically select the different useful knowledge triples for different source sentences. Specifically, the proposed model contains two components: 1) knowledge selector, that dynamically selects useful knowledge triples for a source sentence, and 2) knowledge guided NMT (KgNMT), that utilizes the selected triples to guide the translation of NMT. Meanwhile, to overcome the non-differentiable problem and guide the training procedure, we propose a policy gradient strategy to encourage the model to select useful triples and improve the generation probability of gold target sentence. Various experimental results show that the proposed method can significantly outperform the baseline models in both translation quality and handling the entities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402776818",
    "type": "article"
  },
  {
    "title": "Social-sum-Mal: A Dataset for Abstractive Text Summarization in Malayalam",
    "doi": "https://doi.org/10.1145/3696107",
    "publication_date": "2024-09-16",
    "publication_year": 2024,
    "authors": "Rahul Raj M; Dhanya S Pankaj",
    "corresponding_authors": "",
    "abstract": "Abstractive text summarization techniques for Malayalam language is still in its infancy. The lack of benchmarked datasets for this task is one of the constraints in developing and testing good models. Malayalam has seven nominal case forms, two nominal number forms, and three gender forms. It is subjected to extreme agglutination and inflection. Due to this, the translation of other text summarization datasets to Malayalam may not capture these case forms effectively. Therefore curation of datasets from scratch is highly demanded for specific text-processing applications in Malayalam. This paper introduces a novel dataset designed specifically for advancing the field of automatic abstractive text summarization in Malayalam language. The dataset is curated to address the unique linguistic characteristics of the Malayalam language. It is named as Social-sum-Mal dataset, capable of addressing three different types of summarization tasks- long, extreme, and query-based summarizations. In addition, Social-sum-Mal can be extended for other applications like text classification, multi-document summarization, and question answering. To enhance the dataset transparency, a datasheet is created for Social-sum-Mal. Data accuracy and annotator biases are evaluated using proper testing strategies including Jaccard, cosine, and overlap similarities. The correctness of the dataset is further evaluated by comparing it with a deep-learning-based text summarization model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402806427",
    "type": "article"
  },
  {
    "title": "Analysing Psychological Sentiment Prediction Across Modalities: Harnessing Emotion Datasets within Natural Language Processing (NLP)",
    "doi": "https://doi.org/10.1145/3687305",
    "publication_date": "2024-09-16",
    "publication_year": 2024,
    "authors": "N Li; Rong Kong",
    "corresponding_authors": "",
    "abstract": "Studying human emotions and feelings is a crucial element in the field of psychology, having significant implications such as evaluating mental health and improving human-computer interactions. Recently, there has been a rise in interest in examining how psychological sentiment can be predicted through various mediums, including text, audio, video, and physiological signals. By utilizing advancements in Natural Language Processing (NLP) and analysing multimodal data, this research delves into incorporating emotion datasets into NLP frameworks to improve psychological sentiment prediction. This presents present an applicability of some of the NLP techniques to predicts the Psychological Sentiment such as Deep Generating adversarial networks (D-GANs), Long short-term memory (LSTM) and gated recurrent unit (GRU). The sentimental analysis performed by the considered algorithms are implemented in python and the parameters which are considered for the results evaluation are model loss, confusion matrix, accuracy, precision, recall and f1-score. Our goal with this analysis is to offer a deeper understanding of the existing methodologies and future scope of growth in psychological sentiment prediction of research in the field of NLP.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402821253",
    "type": "article"
  },
  {
    "title": "An Emotional Dialogue System Using Conditional Generative Adversarial Networks with a Sequence-to-Sequence Transformer Encoder",
    "doi": "https://doi.org/10.1145/3698394",
    "publication_date": "2024-10-02",
    "publication_year": 2024,
    "authors": "Wen-Chieh Huang; Yu-Ling Hsueh",
    "corresponding_authors": "",
    "abstract": "Understanding the expression of emotion and generating appropriate responses are key steps toward constructing emotional, conversational agents. In this paper, we propose a framework for single-turn emotional conversation generation, and there are three main components in our model, namely, a sequence-to-sequence model with stacked encoders, a conditional variational autoencoder, and conditional generative adversarial networks. For the sequence-to-sequence model with stacked encoders, we designed a two-layer encoder by combining Transformer with gated recurrent units-based neural networks. Because of the flexibility of the sequence-to-sequence model, we adopted a conditional variational autoencoder in our framework, which uses latent variables to learn a distribution over potential responses and generates diverse responses. Furthermore, we regard a conditional variational autoencoder-based, sequence-to-sequence model as the generative model, and the training of the generative model is assisted by both a content discriminator and an emotion classifier, which assists our model in promoting content information and emotion expression. We use automated evaluation and human evaluation to evaluate our model and baselines on the NTCIR short text conversation task (STC-3) Chinese emotional conversation generation (CECG) Subtask dataset [44], and the experimental results demonstrate that our proposed framework can generate semantically reasonable and emotionally appropriate responses.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403073908",
    "type": "article"
  },
  {
    "title": "C <scp>anton</scp> MT: Investigating Back-Translation and Model-Switch Mechanisms for Cantonese-English Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3698236",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Kung Yin Hong; Lifeng Han; Riza Batista-Navarro; Goran Nenadić",
    "corresponding_authors": "",
    "abstract": "This paper investigates the development and evaluation of machine translation models from Cantonese to English (and backward), where we propose a novel approach to tackle low-resource language translations. Despite recent improvements in Neural Machine Translation (NMT) models with Transformer-based architectures, Cantonese, a language with over 80 million native speakers, has below-par State-of-the-art commercial translation models due to a lack of resources. The main objectives of the study are to develop a model that can effectively translate Cantonese to English and evaluate it against state-of-the-art commercial models. To achieve this, a new parallel corpus has been created by combining different available corpora online with preprocessing and cleaning. In addition, a monolingual Cantonese dataset has been created through web scraping to aid the synthetic parallel corpus generation. Following the data collection process, several approaches, including fine-tuning models, back-translation , and model switch , have been used. The translation quality of models has been evaluated with multiple quality metrics, including lexicon-based metrics (SacreBLEU and hLEPOR) and embedding-space metrics (COMET and BERTscore). Based on the automatic metrics, the best model is selected and compared against the 2 best commercial translators using a new human evaluation framework HOPES . The best model proposed in this investigation (NLLB-mBART) with model switch mechanisms has reached comparable and even better automatic evaluation scores against State-of-the-art commercial models (Bing and Baidu Translators), with a SacreBLEU score of 16.8 on our test set. Furthermore, an open-source web application has been developed to allow users to translate between Cantonese and English, with the different trained models available for effective comparisons between models from this investigation and users. CantonMT is available at https://github.com/kenrickkung/CantoneseTranslation",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403194074",
    "type": "article"
  },
  {
    "title": "Empowering Digital Civility with an NLP Approach for Detecting Twitter Cyberbullying through Boosted Ensembles",
    "doi": "https://doi.org/10.1145/3695251",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Senthil Prabakaran; Navaneetha Krishnan Muthunambu; Nagarajan Jeyaraman",
    "corresponding_authors": "",
    "abstract": "As the number of social networking sites grows, so do cyber dangers. Cyberbullying is harmful behavior that uses technology to intimidate, harass, or harm someone, often on social media platforms like 𝕏 (formerly known as Twitter). Machine learning is the optimal approach for cyberbullying detection on 𝕏 to process large amounts of data, identify patterns of offensive behavior, and automate the detection process for corpus of tweets. To identify cyber threats using a trained model, the boosted ensemble (BE) technique is assessed with various machine learning algorithms such as the convolutional neural network (CNN), long short-term memory (LSTM), naive Bayes (NB), decision tree (DT), support vector machine (SVM), bidirectional LSTM (BILSTM), recurrent neural network LSTM (RNN-LSTM), multi-modal cyberbullying detection (MMCD), and random forest (RF). These classifiers are trained on the vectorized data to classify the tweets to identify cyberbullying threats. The proposed framework can detect cyberbullying cases precisely on tweets. The significance of the work lies in detecting and mitigating cyber threats in real time, and it impacts in enhancing the safety and well-being of social media users by reducing instances of cyberbullying and other cyber threats. The comparative analysis is done using metrics like accuracy, precision, recall, and F1-score, and the comparison results show that the BE technique outperforms other compared algorithms with its overall performance. Respectively, the accuracy rates of CNN, LSTM, NB, DT, SVM, RF, BILSTM, and BE are 92.5%, 93.5%, 84.6%, 88%, 89.3%, 92%, 93.75%, and 96%; precision rates of CNN, LSTM, NB, DT, SVM, RF, RNN-LSTM, and BE are 90.2%, 91.3%, 88%, 85%, 86%, 91.6%, 92.1%, and 94%; recall rates of CNN, LSTM, NB, DT, SVM, RF, BILSTM, and BE are 89.8%, 90.7%, 90%, 82%, 88.67%, 89%, 91.04%, and 93.7%; and F1-scores of CNN, LSTM, NB, DT, SVM, RF, MMCD, and BE are 90.6%, 91.8%, 85%, 84.56% 87.2%, 90%, 84.6%, and 94.89%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403194260",
    "type": "article"
  },
  {
    "title": "SIAT: Document-level Event Extraction via Spatiality-Augmented Interaction Model with Adaptive Thresholding",
    "doi": "https://doi.org/10.1145/3698261",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Zekun Tao; Changjian Wang; Zhiliang Tian; Kele Xu; Yong Guo; Shanshan Li; Yanru Bai; 大作 千葉",
    "corresponding_authors": "",
    "abstract": "Document-level event extraction endeavors to automatically extract structural events from a given document. Many existing approaches focus on modeling entity interactions and decoding these interactions into events, assigning each entity as an event argument. However, these approaches encounter two primary limitations: they exclusively capture semantic dependencies to model entity interactions, overlooking the indication of the spatial distribution features of entities; they decode interactions imprecisely with a hard binary-classification boundary, potentially failing to calibrate micro differences in interactions. To overcome these limitations, we introduce a novel approach termed the S patiality-augmented I nteraction Model with A daptive T hresholding (SIAT). Our method addresses the first limitation by calculating the relative position encoding of entities to represent spatial interaction features. These features are then integrated with multi-granularity semantic interactions, enhancing the modeling of entity interactions for each entity pair. Furthermore, we introduce an adaptive event decoding mechanism, which establishes a more flexible decision boundary for different entity interactions. Additionally, an adaptive loss function for threshold learning is designed to further refine the model. Experimental results demonstrate that our proposed method achieves competitive performance compared to state-of-the-art methods on two public event extraction datasets while maintaining considerable training efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403195287",
    "type": "article"
  },
  {
    "title": "RPEPL: Tibetan Sentiment Analysis Based on Relative Position Encoding and Prompt Learning",
    "doi": "https://doi.org/10.1145/3698575",
    "publication_date": "2024-10-09",
    "publication_year": 2024,
    "authors": "Chunhong Kong; Xueqiang Lv; L Zhang; Haixing Zhao; Zangtai Cai; Yuzhong Chen",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis is a critical task for natural language processing. Much research has been done for high-resource languages such as English and Chinese. However, Tibetan is an extremely low resource language with less reference information. According to the practical demands, this article proposes RPEPL, a Tibetan sentiment analysis method based on relative position encoding and prompt learning. First, word information is introduced to syllable sequences by converting the directed acyclic lattice into a squashed structure. Second, a relative position encoding is used to encode the position information of syllables and words. Third, the association relations and semantic information of tokens are identified by leveraging the multi-attention. Finally, the sentiment category of the Tibetan sentence is obtained through a prompt learning framework. Experimental results demonstrate that RPEPL significantly outperforms the baseline methods on the TUSA dataset and TNEC (Tibetan News Event Comments) dataset. Additionally, traditional recurrent neural networks cannot perform large-scale parallel computation and convolutional neural networks have difficulty modeling long-distance dependencies in Tibetan text, both of which are resolved using RPEPL. Furthermore, the use of multi-attention not only enriches the association relations between syllables and words but also enhances the understanding of sentence semantic and syntactic structure information, and improves the performance of Tibetan sentiment analysis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403251418",
    "type": "article"
  },
  {
    "title": "SMDDH: Singleton Mention Detection using Deep Learning in Hindi Text",
    "doi": "https://doi.org/10.1145/3700821",
    "publication_date": "2024-10-22",
    "publication_year": 2024,
    "authors": "Kusum Lata; Pardeep Singh; Kamlesh Dutta",
    "corresponding_authors": "",
    "abstract": "Mention detection is an important component of the Coreference Resolution (CR) system, where mentions such as name, nominal, and pronominals are identified. These mentions can be purely coreferential mentions or singleton mentions (non-coreferential mentions). Coreferential mentions are those mentions in a text that refer to the same entities in the real world. Whereas, singleton mentions are mentioned only once in the text and do not participate in the coreference as they are not mentioned again in the following text. Filtering of these singleton mentions can substantially improve the performance of a CR process. This paper proposes a singleton mention detection module based on a Fully Connected Network (FCN) and a Long Short-Term Memory for Hindi text and model identifying singleton mentions so that these mentions can be filtered out to reduce the search space for CR. A CR system can look for the previous reference of that mention in the text and if these mentions are removed from the list of mentions, then it reduces the searching time and also space time. This model utilizes a few hand-crafted features, context information, and embedding for words from word2vec and a multilingual Bidirectional Encoder Representations from Transformers (mBERT) language model. The coreference annotated Hindi dataset comprising 3.6K sentences, and 78K tokens are used for the task. The singleton mention detection model is analyzed extensively by experimenting with various lengths of context windows for each mention. The performance of the model is significant with two window sizes of context as compared to other various window sizes of contexts such as 2,3,4,5, etc., and all previous and all next words of each mention. The Precision, Recall, and F-measure of the LSTM-FCN model with mBERT (Word + Context + Syntactic) with two window sizes of context for identifying the singleton mentions are 63%, 71%, and 67% respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403619646",
    "type": "article"
  },
  {
    "title": "Distilling Knowledge in Machine Translation of Agglutinative Languages with Backward and Morphological Decoders",
    "doi": "https://doi.org/10.1145/3703455",
    "publication_date": "2024-11-07",
    "publication_year": 2024,
    "authors": "Telem Joyson Singh; Sanasam Ranbir Singh; Priyankoo Sarmah",
    "corresponding_authors": "",
    "abstract": "Agglutinative languages often have morphologically complex words(MCWs) composed of multiple morphemes arranged in a hierarchical structure, posing significant challenges in translation tasks. We present a novel Knowledge Distillation approach tailored for improving the translation of such languages. Our method involves an encoder, a forward decoder, and two auxiliary decoders: a backward decoder and a morphological decoder. The forward decoder generates target morphemes autoregressively and is augmented by distilling knowledge from the auxiliary decoders. The backward decoder incorporates future context, while the morphological decoder integrates target-side morphological information. We have also designed a reliability estimation method to selectively distill only the reliable knowledge from these auxiliary decoders. Our approach relies on morphological word segmentation. We show that the word segmentation method based on unsupervised morphology learning outperforms the commonly used Byte Pair Encoding method on highly agglutinative languages in translation tasks. Our experiments conducted on English-Tamil, English-Manipuri, and English-Marathi datasets show that our proposed approach achieves significant improvements over strong Transformer-based NMT baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404122129",
    "type": "article"
  },
  {
    "title": "Multimodal Sentiment Analysis for the Malay Language: New Corpus using CNN-based Framework",
    "doi": "https://doi.org/10.1145/3703445",
    "publication_date": "2024-11-11",
    "publication_year": 2024,
    "authors": "Stewart J. Taylor; Fariza Fauzi",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis in the Malay language has traditionally focused on text-based data. Malay is the native language of Malaysia and other surrounding countries. While text-based sentiment analysis has shown good performance, it often lacks accuracy due to the absence of the speaker's affective state and intentions, which can lead to misinterpretations. Multimodal sentiment analysis addresses these shortcomings and has demonstrated improved performance in various prediction tasks. Unfortunately, there has been little research in this area for the Malay language, due to a lack of corpus and baseline studies. This paper introduces a new Malay Multimodal Sentiment Corpus, ‘MyMSC’, with annotations at both the multimodal and unimodal text levels. It contains 1208 segments covering political and social topics. The corpus development processes are described in detail, along with the necessary guidelines and considerations. This paper proposes a CNN-based framework with a late fusion method as the baseline model. Experiments with the proposed model demonstrate that the multimodal approach (F1 score = 0.77) outperforms the unimodal approach (F1 score = 0.68), validating the contribution of multimodality to classification performance. The differences between the two types of annotations and their impact are further elaborated. The full corpus is available at https://github.com/serenaroseaini/MyMSC .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404246571",
    "type": "article"
  },
  {
    "title": "PromptCNER: A Segmentation-based Method for Few-shot Chinese NER with Prompt-tuning",
    "doi": "https://doi.org/10.1145/3705314",
    "publication_date": "2024-11-20",
    "publication_year": 2024,
    "authors": "Chengcheng Mai; Yu Chen; Ziyu Gong; Hanxiang Wang; Mengchuan Qiu; Chunfeng Yuan; Yihua Huang",
    "corresponding_authors": "",
    "abstract": "Recognizing Chinese entities in low-resource settings is a challenging but promising task, which extracts structured pre-defined entities and corresponding types from unstructured text. Compared with the prosperous Named Entity Recognition (NER) methods for Indo-European languages, such as English, the research on Chinese NER is still in its infancy. The main obstacles to the development of Chinese NER methods include the ambiguity of Chinese entity boundary recognition and limited data resources. To address these issues, in this paper, a word-segmentation-based model is present for few-shot Chinese NER. First, we enumerate all possible candidate entity spans on the character level for accurate entity boundary identification with the proposed word segmentation and combination strategy. Then, one kind of question-answer-based prompt template loaded with the candidate entity spans is proposed to cast entity extraction into the masked token prediction task, for dealing with the low-data problem by taking full advantage of the generality and transferability of the pre-trained language model. The extensive experimental results show that our method outperforms the state-of-the-art baselines in low-data settings and also achieves comparable performance in full-data settings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404542582",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Cognitive-Inspired Multimedia Information Processing and Applications for Low-Resource Languages",
    "doi": "https://doi.org/10.1145/3676150",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Chi Lin; Chang Wu Yu; Ning Wang",
    "corresponding_authors": "",
    "abstract": "Public health-related issues (e.g., illness, pollution, and healthcare) are important social issues for governments and researchers. Many of these issues have strong spatial and/or spatio-temporal components, for example, epidemics of infectious ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404589514",
    "type": "article"
  },
  {
    "title": "UIE-Based Relational Extraction Task for Mine Hoist Fault Data",
    "doi": "https://doi.org/10.1145/3705313",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Xiaochao Dang; Guozhen Ding; Xiaohui Dong; Fenfang Li; Shiwei Gao; Wang Yue",
    "corresponding_authors": "",
    "abstract": "Information extraction is pivotal in natural language processing, where the goal is to convert unstructured text into structured information. A significant challenge in this domain is the diversity and specific needs of various processing tasks. Traditional approaches typically utilize separate frameworks for different information extraction tasks, such as named entity recognition and relationship extraction, which hampers their uniformity and scalability. In this study, this study introduce a Universal Information Extraction (UIE) framework combined with a cue learning strategy, significantly improving the efficiency and accuracy of extracting mine hoist fault data. Initially, domain-specific data is manually labeled to fine-tune the model, and the accuracy is further enhanced by constructing negative examples during this fine-tuning process. The model then focuses on faults using the Structured Extraction Language (SEL) and a schema-based prompt syntax, the Structural Schema Instructor (SSI), which targets and extracts key information from the fault data to meet specific domain requirements. Experimental results show that UIE substantially improves the processing efficiency and the F1 accuracy of the extracted mine hoist fault data, with the fine-tuned F1 score increasing from 23.59% to 92.51%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404599542",
    "type": "article"
  },
  {
    "title": "Subword Dictionary Learning and Segmentation for Expanding the Vocabulary of Automatic Speech Recognition in Tamil and Kannada",
    "doi": "https://doi.org/10.1145/3705312",
    "publication_date": "2024-11-23",
    "publication_year": 2024,
    "authors": "Bharathi Pilar; A Madhavaraj; A. G. Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "We present automatic speech recognition (ASR) systems for Tamil and Kannada based on subword modeling to effectively handle unlimited vocabulary arising due to the highly agglutinative nature of the languages. We propose a variant of the byte pair encoding (BPE) algorithm named extended-BPE, and Morfessor tool to segment each word into its subwords. We have effectively incorporated maximum likelihood and Viterbi estimation techniques with weighted finite state transducers framework in these algorithms to learn the subword dictionary from a large text corpus. Using the learned subword dictionary, the words in the transcriptions of the training data are segmented into subwords. We train deep neural network ASR systems that recognize the subword sequence for any given test speech utterance. The output subword sequence is then post-processed using deterministic rules to get the final word sequence. Because of this subword design, the number of words that can be recognized is much larger than the number of words in the training corpus. For Tamil ASR, we use 152 hours of data for training and 65 hours for testing, whereas for Kannada ASR, we use 275 hours for training and 72 hours for testing. Upon experimenting with different combinations of segmentation and estimation techniques, we find that the word error rate (WER) reduces drastically when compared to the baseline word-level ASR, achieving a maximum absolute WER reduction of 6.24% and 6.63% for Tamil and Kannada, respectively. Further, comparing our results with that of an end-to-end ASR model available on Github, we have seen that our subword language models can perform comparable to or exceed that of recent end-to-end ASR models for Kannada and Tamil languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404650819",
    "type": "article"
  },
  {
    "title": "Visual Story Generation Model Guided by Multi Granularity Image Information",
    "doi": "https://doi.org/10.1145/3708886",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Yuanlong Wang; Qiang Ma; Ru Li; Zhang Hu",
    "corresponding_authors": "",
    "abstract": "Visual story generation, which involves generating short stories from sequential images, has become a core task at the intersection of computer vision and natural language processing. However, existing methods suffer from a bias in the concept predicates predicted, leading to a semantic gap between the generated stories and the images. This paper proposes a novel visual story generation model that utilizes muliti granularity image information to guide the generation process and correct the bias in concept predicates, resulting in more image-consistent stories. The proposed model consists of two stages: In the first stage, a set of concepts predicates is predicted from the image and enriched with external knowledge, and the most suitable concepts for story generation are selected. In the second stage, fine-grained image information are utilized to integrate image information into the story generation module, improving the bias in concept predicates. The image theme information and the generated results of previous moments are used as prompts to guide the story generation module. Experimental results show that the proposed model outperforms baseline models in all evaluation metrics. Specifically, the BLEU-1, BLEU-2, BLEU-3, and BLEU-4 metrics are improved by 4.0, 3.8, 3.02, and 1.98 percentage points, respectively, and the METEOR metric is improved by 1.4 percentage points. The generated stories are more consistent with the image content, maintain a consistent theme, and enhance coherence between contexts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405545554",
    "type": "article"
  },
  {
    "title": "Arabic App Reviews: Analysis and Classification",
    "doi": "https://doi.org/10.1145/3708987",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Othman Aljeezani; Dorieh M. Alomari; Irfan Ahmad",
    "corresponding_authors": "",
    "abstract": "User opinions and feedback on mobile applications are crucial for application developers, offering insights into issues like bugs, popular features, and enhancement requests. Given the vast number of feedback for each app, it is impractical for developers to manually extract valuable information. To better understand and analyze user opinions, developers can benefit from automatic sentiment analysis and classification of app reviews. Existing research has primarily focused on reviews written in English, with some studies addressing sentiment analysis of Arabic reviews but overlooking the classification task. Given the widespread use and complexity of Arabic compared to English, our work investigates both sentiment analysis and classification of Arabic app reviews. We introduce the AURA ( A pp U ser R eview in A rabic) dataset. AURA dataset has two versions: AURA-Sentiment with 29,700 labeled reviews for sentiment analysis, and AURA-Classification with 2,900 labeled reviews for classification. Using these datasets, we applied deep learning (DL) and natural language processing (NLP) techniques for analyzing and classifying Arabic app reviews. Leveraging the MarBert model, we achieved an F1-score of 0.89 for sentiment analysis and an F1-score of 0.62 for a four-class classification problem. Our findings provide valuable insights and suggest directions for future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405700096",
    "type": "article"
  },
  {
    "title": "Uniformly Interpolated Balancing for Robust Prediction in Translation Quality Estimation",
    "doi": "https://doi.org/10.1145/3365916",
    "publication_date": "2020-01-19",
    "publication_year": 2020,
    "authors": "Hyun Kim; Seung‐Hoon Na",
    "corresponding_authors": "",
    "abstract": "There has been growing interest among researchers in quality estimation (QE), which attempts to automatically predict the quality of machine translation (MT) outputs. Most existing works on QE are based on supervised approaches using quality-annotated training data. However, QE training data quality scores readily become imbalanced or skewed : QE data are mostly composed of high translation quality sentence pairs but the data lack low translation quality sentence pairs. The use of imbalanced data with an induced quality estimator tends to produce biased translation quality scores with “high” translation quality scores assigned even to poorly translated sentences. To address the data imbalance, this article proposes a simple, efficient procedure called uniformly interpolated balancing to construct more balanced QE training data by inserting greater uniformness to training data. The proposed uniformly interpolated balancing procedure is based on the preparation of two different types of manually annotated QE data: (1) default skewed data and (2) near-uniform data . First, we obtain default skewed data in a naive manner without considering the imbalance by manually annotating qualities on MT outputs. Second, we obtain near-uniform data in a selective manner by manually annotating a subset only, which is selected from the automatically quality-estimated sentence pairs. Finally, we create uniformly interpolated balanced data by combining these two types of data, where one half originates from the default skewed data and the other half originates from the near-uniform data. We expect that uniformly interpolated balancing reflects the intrinsic skewness of the true quality distribution and manages the imbalance problem. Experimental results on an English-Korean quality estimation task show that the proposed uniformly interpolated balancing leads to robustness on both skewed and uniformly distributed quality test sets when compared to the test sets of other non-balanced datasets.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3015017038",
    "type": "article"
  },
  {
    "title": "Structurally Comparative Hinge Loss for Dependency-Based Neural Text Representation",
    "doi": "https://doi.org/10.1145/3387633",
    "publication_date": "2020-05-18",
    "publication_year": 2020,
    "authors": "Kexin Wang; Yu Zhou; Jiajun Zhang; Shaonan Wang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Dependency-based graph convolutional networks (DepGCNs) are proven helpful for text representation to handle many natural language tasks. Almost all previous models are trained with cross-entropy (CE) loss, which maximizes the posterior likelihood directly. However, the contribution of dependency structures is not well considered by CE loss. As a result, the performance improvement gained by using the structure information can be narrow due to the failure in learning to rely on this structure information. To face the challenge, we propose the novel structurally comparative hinge (SCH) loss function for DepGCNs. SCH loss aims at enlarging the margin gained by structural representations over non-structural ones. From the perspective of information theory, this is equivalent to improving the conditional mutual information of model decision and structure information given text. Our experimental results on both English and Chinese datasets show that by substituting SCH loss for CE loss on various tasks, for both induced structures and structures from an external parser, performance is improved without additional learnable parameters. Furthermore, the extent to which certain types of examples rely on the dependency structure can be measured directly by the learned margin, which results in better interpretability. In addition, through detailed analysis, we show that this structure margin has a positive correlation with task performance and structure induction of DepGCNs, and SCH loss can help model focus more on the shortest dependency path between entities. We achieve the new state-of-the-art results on TACRED, IMDB, and Zh. Literature datasets, even compared with ensemble and BERT baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3026239495",
    "type": "article"
  },
  {
    "title": "ROLE OF ADVANCED WEB BASED CONTENT MANAGEMENT SYSTEM AND ITS SIGNIFICANCE IN LIBRARIES MANAGEMENT SYSTEM",
    "doi": "https://doi.org/10.1145/3394115",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "Bin Liu; Yao Lü",
    "corresponding_authors": "",
    "abstract": "Libraries are vaults of learning and the enormous development in computerized assets has constrained library experts to utilize different data innovation tools to oversee and render management to the clients in Chinese education and research sector. To accomplish more noteworthy proficiency in the quickly evolving economic condition, libraries are progressively searching for new standards to convey management to clients in their work areas to innovate their knowledge. Library gateways assume a significant job by making a web domain where clients can without much of a stretch access data. It offers benefits as well as improving insightful correspondence and research among the supporters of the library. This paper discuses the entry of Advanced Web based Content Management System (AW-CMS) using linked list based iterations for database management has been made here to clarify the different supervisionclients, so it mayimprove the successful data searchers and assess accurate data proficiently and reliability in an effective and efficient manner. It likewise features, how it attempts to meet client desires for data on interest in library management and its experimental validation in accordance with existing software systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3040981953",
    "type": "article"
  },
  {
    "title": "Design and Development of Heuristic Utility Management Algorithm for Chinese Library Management System",
    "doi": "https://doi.org/10.1145/3397968",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Xiaodong Yang; Xiaoxia Lin",
    "corresponding_authors": "",
    "abstract": "Utility Management in a library is the programmatic tool with the synthetic mental program ability, along with Artificial Intelligence capacities, headed to manage a high volume of books, articles, and assignments, which help to ease the manual significance of librarians. This computerized machine code helps librarians to deal with various databases of the library management system. This framework keeps the records of all the resource details in an optimized manner. It uses a utility management software code with an optimized search classifier that helps to deal with the resource of the library. In this work, the Heuristic Utility Management Algorithm (HUMA) has been used to keep track of resources in the library using mathematical modeling and standardized programmatic computation on tags, which relates the decode scanner to parse the input information. HUMA helps to reduce the manual routine work done by the librarians, and it has been analyzed in this research with prominent survey outcomes based on experimental validation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3041908022",
    "type": "article"
  },
  {
    "title": "An Extensible Framework of Leveraging Syntactic Skeleton for Semantic Relation Classification",
    "doi": "https://doi.org/10.1145/3402885",
    "publication_date": "2020-09-27",
    "publication_year": 2020,
    "authors": "Hao Wang; Qiongxing Tao; Siyuan Du; Xiangfeng Luo",
    "corresponding_authors": "",
    "abstract": "Relation classification is one of the most fundamental upstream tasks in natural language processing and information extraction. State-of-the-art approaches make use of various deep neural networks (DNNs) to extract higher-level features directly. They can easily access to accurate classification results by taking advantage of both local entity features and global sentential features. Recent works on relation classification devote efforts to modify these neural networks, but less attention has been paid to the feature design concerning syntax. However, from a linguistic perspective, syntactic features are essential for relation classification. In this article, we present a novel linguistically motivated approach that enhances relation classification by imposing additional syntactic constraints. We investigate to leverage syntactic skeletons along with the sentential contexts to identify hidden relation types. The syntactic skeletons are extracted under the guidance of prior syntax knowledge. During extraction, the input sentences are recursively decomposed into syntactically shorter and simpler chunks. Experimental results on the SemEval-2010 Task 8 benchmark show that incorporating syntactic skeletons into current DNN models enhances the task of relation classification. Our systems significantly surpass two strong baseline systems. One of the substantial advantages of our proposal is that this framework is extensible for most current DNN models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3089182715",
    "type": "article"
  },
  {
    "title": "Adversarial Cross-domain Community Question Retrieval",
    "doi": "https://doi.org/10.1145/3487291",
    "publication_date": "2022-01-10",
    "publication_year": 2022,
    "authors": "Aibo Guo; Xinyi Li; Ning Pang; Xiang Zhao",
    "corresponding_authors": "",
    "abstract": "Community Q&amp;A forum is a special type of social media that provides a platform to raise questions and to answer them (both by forum participants), to facilitate online information sharing. Currently, community Q&amp;A forums in professional domains have attracted a large number of users by offering professional knowledge. To support information access and save users’ efforts of raising new questions, they usually come with a question retrieval function, which retrieves similar existing questions (and their answers) to a user’s query. However, it can be difficult for community Q&amp;A forums to cover all domains, especially those emerging lately with little labeled data but great discrepancy from existing domains. We refer to this scenario as cross-domain question retrieval. To handle the unique challenges of cross-domain question retrieval, we design a model based on adversarial training, namely, X-QR , which consists of two modules—a domain discriminator and a sentence matcher. The domain discriminator aims at aligning the source and target data distributions and unifying the feature space by domain-adversarial training. With the assistance of the domain discriminator, the sentence matcher is able to learn domain-consistent knowledge for the final matching prediction. To the best of our knowledge, this work is among the first to investigate the domain adaption problem of sentence matching for community Q&amp;A forums question retrieval. The experiment results suggest that the proposed X-QR model offers better performance than conventional sentence matching methods in accomplishing cross-domain community Q&amp;A tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4206458772",
    "type": "article"
  },
  {
    "title": "A Twitter Framework to assess the Effectiveness of Indian Government Campaign",
    "doi": "https://doi.org/10.1145/3490503",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Aarzoo Dhiman; Durga Toshniwal",
    "corresponding_authors": "",
    "abstract": "Many government and private agencies introduce social, governmental, educational, commercial and public-health based campaigns to refine various sections of the society. Traditionally, citizen feedback and on-field surveys are performed to assess the effectiveness of such campaigns. However, there is limited availability of the social media tools that assess the impact of different government campaigns automatically. In this research work, a framework has been proposed which uses the social media data i.e. Twitter data pertaining to an Indian Cleanliness Campaign Swachh Bharat Abhiyan (SBA) to perform the effectiveness assessment. This research work has been performed in two parts. First, Twitter data has been processed to predict the perceptions of citizens using Word Embeddings -based Tweet Pooling and Subjectivity score -based Sentiment Analysis and second, the performance of the cities has been predicted using the demographic features based predictor models. The experimentation shows a 0.77 correlation between the proposed framework and the government surveys while predicting the citizens’ perspectives and 80(+/-15)% accuracy while predicting the performance of cities using Random Forest Regression. Furthermore, the cities have been clustered using Twitter and the demographic data to find out the interesting patterns and behaviors. This research work provides better insights to the potential of social media data in the interventional studies of the developing countries such as India.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4210635544",
    "type": "article"
  },
  {
    "title": "Acoustic Analysis of Vowels in Konkani",
    "doi": "https://doi.org/10.1145/3474358",
    "publication_date": "2022-02-03",
    "publication_year": 2022,
    "authors": "Swapnil Fadte; Edna Vaz Fernandes; Ramdas Karmali; Jyoti D. Pawar",
    "corresponding_authors": "",
    "abstract": "Konkani is an under-resourced language mainly spoken on the west coast of India. Although linguistic analyses of vowel sounds in various dialects of Konkani have been done in the past, more accurate analysis of Konkani vowels, especially an acoustic-phonetic analysis, was never carried out. In this article, we present a detailed analysis of nine Konkani vowels, namely /i/, /e/, /ε/, /u/, /o/, /ɔ/, /a/, /ə/, and /ɨ̞/ . The dataset used for the analysis was created from audio recordings of 28 native speakers of Goan Konkani. Based on the experimental results, we propose a vowel chart for Konkani. We also observed a partial loss of Konkani vowel / ɨ̞ / in the regular speech of native speakers. This change is also evident in the substitution analysis of vowel phonemes that was carried out by us as a part of this study.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4210708233",
    "type": "article"
  },
  {
    "title": "Generating Factoid Questions with Question Type Enhanced Representation and Attention-based Copy Mechanism",
    "doi": "https://doi.org/10.1145/3474555",
    "publication_date": "2022-01-28",
    "publication_year": 2022,
    "authors": "Yue Hu; Haitong Yang; Guangyou Zhou; Jimmy Xiangji Huang",
    "corresponding_authors": "",
    "abstract": "Question generation over knowledge bases is an important research topic. How to deal with rare and low-frequency words in traditional generation models is a key challenge for question generation. Although the copy mechanism provides significant performance improvements, the original copy mechanism weakens the focus on aspect generation in the overall representations. In this article, we present a novel method to improve question generation with a question type enhanced representation and attention-based copy mechanism. The proposed method exploits the advantages of the generate mode in the copy mechanism and replaces objects in the factual triples with question types, which attempts to improve the output quality in the generate mode and effectively generate questions with proper interrogative words. We evaluate the proposed method on two standard benchmark datasets. The experimental results demonstrate that our proposed method can produce higher-quality questions than these of the Encoder-Decoder-based and CopyNet-based methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4210778341",
    "type": "article"
  },
  {
    "title": "Neural Topic Model Training with the REBAR Gradient Estimator",
    "doi": "https://doi.org/10.1145/3517336",
    "publication_date": "2022-02-23",
    "publication_year": 2022,
    "authors": "Amit Kumar; Nazanin Esmaili; Massimo Piccardi",
    "corresponding_authors": "",
    "abstract": "Topic modelling is an important approach of unsupervised machine learning that allows automatically extracting the main “topics” from large collections of documents. In addition, topic modelling is able to identify the topic proportions of each individual document, which can be helpful for organizing the collections. Many topic modelling algorithms have been proposed to date, including several that leverage advanced techniques such as variational inference and deep autoencoders. However, to date topic modelling has made limited use of reinforcement learning, a framework that has obtained vast success in many other unsupervised learning tasks. For this reason, in this article we propose training a neural topic model using a reinforcement learning objective and minimizing the objective with the recently-proposed REBAR gradient estimator. Experiments performed over two probing datasets have shown that the proposed model has achieved improvements over all the compared models in terms of both model perplexity and topic coherence, and produced topics that appear qualitatively informative and consistent.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4213450365",
    "type": "article"
  },
  {
    "title": "Hypernymy Detection for Low-resource Languages: A Study for Hindi, Bengali, and Amharic",
    "doi": "https://doi.org/10.1145/3490389",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Abhik Jana; Gopalakrishnan Venkatesh; Seid Muhie Yimam; Chris Biemann",
    "corresponding_authors": "",
    "abstract": "Numerous attempts for hypernymy relation (e.g., dog “is-a” animal) detection have been made for resourceful languages like English, whereas efforts made for low-resource languages are scarce primarily due to lack of gold-standard datasets and suitable distributional models. Therefore, we introduce four gold-standard datasets for hypernymy detection for each of the two languages, namely, Hindi and Bengali, and two gold-standard datasets for Amharic. Another major contribution of this work is to prepare distributional thesaurus (DT) embeddings for all three languages using three different network embedding methods (DeepWalk, role2vec, and M-NMF) for the first time on these languages and to show their utility for hypernymy detection. Posing this problem as a binary classification task, we experiment with supervised classifiers like Support Vector Machine, Random Forest, and so on, and we show that these classifiers fed with DT embeddings can obtain promising results while evaluated against proposed gold-standard datasets, specifically in an experimental setup that counteracts lexical memorization. We further incorporate DT embeddings and pre-trained fastText embeddings together using two different hybrid approaches, both of which produce an excellent performance. Additionally, we validate our methodology on gold-standard English datasets as well, where we reach a comparable performance to state-of-the-art models for hypernymy detection.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4214875890",
    "type": "article"
  },
  {
    "title": "Konkani WordNet: Corpus-Based Enhancement using Crowdsourcing",
    "doi": "https://doi.org/10.1145/3503156",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Sanjana Manerkar; Kavita Asnani; Preeti Ravindranath Khorjuvenkar; Shilpa J. Desai; Jyoti D. Pawar",
    "corresponding_authors": "",
    "abstract": "Konkani is one of the languages included in the eighth schedule of the Indian constitution. It is the official language of Goa and is spoken mainly in Goa and some places in Karnataka and Kerala. Konkani WordNet or Konkani Shabdamalem (kōṁkanī śabdamālēṁ) as it has been referred to, was developed under the Indradhanush WordNet Project Consortium during the period from August 2010 to October 2013. This project was funded by Technology Development for Indian Languages (TDIL), Department of Electronics &amp; Information Technology (Deity), and Ministry of Communication and Information Technology (MCIT). The work on Konkani WordNet has halted since the end of the project. Currently, the Konkani WordNet contains around 32,370 synsets. However, to make it a powerful resource for NLP applications in the Konkani language, a need is felt for research work toward enhancement of the Konkani WordNet via community involvement. Crowdsourcing is a technique in which the knowledge of the crowd is utilized to accomplish a particular task. In this article, we have presented the details of the crowdsourcing platform named “Konkani Shabdarth” (kōṁkanī śabdārth). Konkani Shabdarth attempts to use the knowledge of Konkani speaking people for creating new synsets and perform the quantitative enhancement of the wordnet. It also intends to work toward enhancing the overall quality of the Konkani WordNet by validating the existing synsets, and adding the missing words to the existing synsets. A text corpus named “Konkani Shabdarth Corpus”, has been created from the Konkani literature while implementing the Konkani Shabdarth tool. Using this corpus, 572 root words that are missing from the Konkani WordNet have been identified which are given as input to Konkani Shabdarth. As of now, total 94 users have registered on the platform, out of which 25 users have actually played the game. Currently, 71 new synsets have been obtained for 21 words. For some of the words, multiple entries for the concept definition have been received. This overlap is essential for automating the process of validating the synsets. Due to the pandemic period, it has been difficult to train and get players to actually play the game and contribute. We studied the impact of adding missing words from other existing Konkani text corpus on the coverage of Konkani WordNet. The expected increase in the percentage coverage of Konkani WordNet has been found to be in the range 20–27 after adding the missing words from the Konkani Shabdarth corpus in comparison to the other corpora for which the increase is in the range 1–10.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4214899524",
    "type": "article"
  },
  {
    "title": "Improving Neural Machine Translation by Transferring Knowledge from Syntactic Constituent Alignment Learning",
    "doi": "https://doi.org/10.1145/3510580",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "Chao Su; Heyan Huang; Shumin Shi; Ping Jian",
    "corresponding_authors": "",
    "abstract": "Statistical machine translation (SMT) models rely on word-, phrase-, and syntax-level alignments. But neural machine translation (NMT) models rarely explicitly learn the phrase- and syntax-level alignments. In this article, we propose to improve NMT by explicitly learning the bilingual syntactic constituent alignments. Specifically, we first utilize syntactic parsers to induce syntactic structures of sentences, and then we propose two ways to utilize the syntactic constituents in a perceptual (not adversarial) generator-discriminator training framework. One way is to use them to measure the alignment score of sentence-level training examples, and the other is to directly score the alignments of constituent-level examples generated with an algorithm based on word-level alignments from SMT. In our generator-discriminator framework, the discriminator is pre-trained to learn constituent alignments and distinguish the ground-truth translation from the fake ones, while the generative translation model is fine-tuned to receive the alignment knowledge and to generate translations that best approximate the true ones. Experiments and analysis show that the learned constituent alignments can help improve the translation results.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220668513",
    "type": "article"
  },
  {
    "title": "AIP: A Named Entity Recognition Method Combining Glyphs and Sounds",
    "doi": "https://doi.org/10.1145/3522736",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Bo Liu; Zhuo Su; Guangzhi Qu",
    "corresponding_authors": "",
    "abstract": "In recent years, a large number of Chinese electronic texts have been produced in the process of information construction in various fields. Identifying specific entities in these electronic texts has become a major research focus. Most existing research methods use radicals to extract the glyph features of Chinese characters but have seen its limitation. This paper extracts the features of Chinese characters from three aspects: glyph features, phonetic features, and character features, and improves conventional feature extraction methods for each kind of feature. A new named entity recognition method (AIP) is proposed by transforming Chinese characters into corresponding images for glyph feature extraction, dividing pinyin into initials, vowels, and tones for phonetic feature extraction, and fine-tuning the A Lite Bert model for character feature extraction to improve the performance of the model. This paper compares the performance of the AIP model and mainstream neural network models on Chinese named entity recognition tasks on commonly used data sets and the data sets in specific domains. The results showed that AIP achieved better results than the related work. The F1 values on the two data sets are 94.4% and 80.5%, respectively, which validates the model's versatility.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220738390",
    "type": "article"
  },
  {
    "title": "Image Semantic Analysis in Visual Media Art Using Machine Learning and Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3522576",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Weiran Cao",
    "corresponding_authors": "Weiran Cao",
    "abstract": "The current archaeological work in China has the problems of high cost, large material consumption, more attention on human detection and long time-consuming. It is urgent to use modern high-precision detection technology for auxiliary work. This exploration will also use the semantic recognition system based on deep learning and neural network for the recognition of oracle bone inscriptions in archaeology. Therefore, combined with the concept of multimedia semantic recognition and analysis, a unified real-time target detection semantic analysis model named You Only Look Once (YOLOv2) is established based on the deep convolutional neural network under deep learning in the field of machine learning, to test the semantic analysis of oracle bone inscriptions. Moreover, Faster Region-Convolutional Neural Network (Faster R-CNN) and traditional YOLO models are selected to conduct the controlled experiments. A YOLOv2 recognition system is established based on Diffusion-Convolutional Neural Networks (DCNN) under deep learning. First, the concept and performance of DCNN are studied. Next, the basic information of oracle bone inscriptions is analyzed. A recognition system based on DCNN is established. On the premise that the three models can identify different directions of the same oracle bone inscriptions sample, the detection accuracy and detection loss value of YOLOv2 are better than those of the other two models, the detection accuracy is as high as 0.90, and the loss value is less than 0.10. Therefore, it is considered that this YOLOv2 semantic analysis model can be applied in oracle bone inscriptions and other archaeological work, which improves the work efficiency and simplifies the human work items for the domestic archaeological work. This semantic analysis model is of great help to the pattern recognition of cultural relics in archaeological work, and can help professionals analyze the meaning of patterns faster when there are massive similar oracle bone inscriptions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220751495",
    "type": "article"
  },
  {
    "title": "Construction of Marketing Curriculum System Based on Blending Learning “3+2” Joint Training of Higher Vocational and Undergraduate Education Using NLP for Marketing Document Management and Information Retrieval",
    "doi": "https://doi.org/10.1145/3524113",
    "publication_date": "2022-03-30",
    "publication_year": 2022,
    "authors": "Zhiqin Li",
    "corresponding_authors": "Zhiqin Li",
    "abstract": "The blended learning system provides educational tools to a student in accordance with the student's expressed educational interests, and it is a mix of online training and assignments, giving them more control over the learning and other developmental facets, which it has a profound impact on current higher education. This article analyzes the “3+2” segmented training mode of higher vocational education and undergraduate education, discusses the problems existing in the curriculum system of marketing major in higher vocational education under this training mode and the transformative potential of blended learning in the context of the challenges facing higher education, with the perspective of Natural language Processing (NLP) Assistance on Digital library management, and in NLP the human language is divided into segments, so that the grammatical structure and the actual meaning of the words can be analyzed and understood that puts forward corresponding measures to promote the development of the integration of segmented training courses of higher vocational education and undergraduate education by smart technologies and improve the quality of marketing talents jointly trained under higher vocational education and undergraduate education. The observational results predict the research of some scholars and, this article puts forward the opinions of curriculum construction, and designs the integrated curriculum system diagram, to have a certain reference for the “3+2” joint training mode with the compounding of the NLP Assistance on Digital Management attains an effective and accurate outcome with the construction of marketing curriculum system based on blending learning.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220761024",
    "type": "article"
  },
  {
    "title": "A Novel Social Interaction Assistive Device for Arab Deaf People",
    "doi": "https://doi.org/10.1145/3508374",
    "publication_date": "2022-03-22",
    "publication_year": 2022,
    "authors": "Mwaffaq Otoom; Mohammad A. Alzubaidi; Lina Nasr Abd Al-Raziq",
    "corresponding_authors": "",
    "abstract": "Many deaf people worldwide face problems with integrating into society and interacting with people who do not understand sign language. This can lead to isolation and difficulty in expressing feelings. In this research, our primary goal is to help deaf people communicate, express their feelings, and socialize with others. Toward that end, 40 Arabic words that are commonly used in social interactions were used to build a dataset of hand movements used by deaf people to express these words. These movements were recorded using a Leap Motion Controller ( LMC ). The resulting dataset consists of 1,579 instances and 112 features, recorded with the help of five deaf persons. Feature reduction and oversampling techniques were applied to analyze the dataset. Machine learning algorithms were then used to build a model that is able to classify any given hand posture or gesture into one of those 40 words. This work compared the performance of nine classification algorithms: Random Forest, Decision Table, Classification via Regression, K-Nearest Neighbor (KNN), Simple Logistic, Input Mapped Classifier, Random Tree, J48, and Bayes network. Results show that the Random Forest model achieved the highest accuracy with over 90%, outperforming the other eight models. Subsequently, a usability study was conducted by 10 deaf people to test the effectiveness of the proposed assistive device. The results suggest that the proposed device is useful for facilitating social communication with deaf people. It also suggests that the device was preferred, when compared with other relevant devices.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4221117178",
    "type": "article"
  },
  {
    "title": "IsiXhosa Named Entity Recognition Resources",
    "doi": "https://doi.org/10.1145/3531478",
    "publication_date": "2022-06-02",
    "publication_year": 2022,
    "authors": "Roald Eiselen; Andiswa Bukula",
    "corresponding_authors": "",
    "abstract": "Named entity recognition has been one of the most widely researched natural language processing technologies over the past two decades. For the South African languages, however, relatively little research and development work has been done. This changed with the release of the NCHLT named entity annotated resources, a collection of named entity annotated data and Conditional Random Field-based named entity recognisers for ten of the official languages. In this work, we provide a detailed description and linguistic analysis of the named entity (NE) annotated data for the agglutinative isiXhosa language, by analysing the morphosyntactic features relevant to the three main types of NE, viz. person, location, and organisation. From the data, we identify suffix and capitalisation features that may be good predictors of the different NE types. Based on these features, we describe the named entity recogniser and feature set developed as part of the NCHLT release. The recogniser has high precision, 0.9713 overall, but relatively low recall, 0.7409, especially for person names, 0.5963, resulting in an overall F-score of 0.8406. Although there are various avenues to improve the named entity recogniser, this is a significant release for a historically under-resourced language.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4281721917",
    "type": "article"
  },
  {
    "title": "An Understanding-oriented Robust Machine Reading Comprehension Model",
    "doi": "https://doi.org/10.1145/3546190",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Feiliang Ren; Yongkang Liu; Bochao Li; Shilei Liu; Bingchao Wang; J.K. Wang; Chunchao Liu; 啓介 喜馬",
    "corresponding_authors": "",
    "abstract": "Although existing machine reading comprehension models are making rapid progress on many datasets, they are far from robust. In this paper, we propose an understanding-oriented machine reading comprehension model to address three kinds of robustness issues, which are over sensitivity, over stability and generalization. Specifically, we first use a natural language inference module to help the model understand the accurate semantic meanings of input questions so as to address the issues of over sensitivity and over stability. Then in the machine reading comprehension module, we propose a memory-guided multi-head attention method that can further well understand the semantic meanings of input questions and passages. Third, we propose a multilanguage learning mechanism to address the issue of generalization. Finally, these modules are integrated with a multi-task learning based method. We evaluate our model on three benchmark datasets that are designed to measure models robustness, including DuReader (robust) and two SQuAD-related datasets. Extensive experiments show that our model can well address the mentioned three kinds of robustness issues. And it achieves much better results than the compared state-of-the-art models on all these datasets under different evaluation metrics, even under some extreme and unfair evaluations. The source code of our work is available at: https://github.com/neukg/RobustMRC.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4283753560",
    "type": "article"
  },
  {
    "title": "Analysis of the Teaching Design Process Mode of College English Application Oriented Curriculum with NLP Voice Data Authentication",
    "doi": "https://doi.org/10.1145/3529392",
    "publication_date": "2022-07-28",
    "publication_year": 2022,
    "authors": "Dai Linyi",
    "corresponding_authors": "Dai Linyi",
    "abstract": "The purpose is to introduce game-based learning into college English communication courses, improve the competence of college students’ oral English expression and reduce their anxieties in speaking English. Four games are designed and an experiment of 20 sophomores’ performances in college oral communication classes is conducted for a semester. According to the classroom activities of oral English, the content of the games is reciting and delivering the sentences in order, guessing and understanding the characteristics of the single word, guessing the word by asking questions, and describing the pictures in English. This can be implemented with the Natural language Processing Assistance on Digital Library Management which presents the requirements for handling the documents in digital libraries.The research tools include an oral English test, FLCAS (Foreign Language Classroom Anxiety Scale), classroom observation and a game-based learning recordwith the efficientresults are as follows: 1. The introduction of the four games into the oral English communication courses is found feasible. But the student's basic knowledge of English affects their enthusiasm for the games. 2.The skill of students' oral English expression is improved significantly after the introduction of the games, and they can speak complete English sentences to answer questions and describe pictures. 3. The students’ test anxiety decreases significantly, but they still feel anxious about the negative comments on their oral English performances. 4. According to the reflection and revision of the whole research process, the teaching mode design of the college English application course is developed in three aspects, including game design, game-course matching, and game integration into teaching.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4288084792",
    "type": "article"
  },
  {
    "title": "Photo Realistic Generation from Arabic Text Description Based on Generative Adversarial Networks",
    "doi": "https://doi.org/10.1145/3490504",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "Sara Maher Mathematics; Mohamed Loey",
    "corresponding_authors": "",
    "abstract": "Generating accurate high-resolution images from text representations is a difficult problem in computer vision that has a wide range of functional applications. Text-to-image conversion is not dissimilar to the difficulties inherent in language processing. For example, each text meaning can be encoded in two distinct human languages, while photographs and text are two distinct encoding languages for similar data. However, these are two distinct issues, since text-to-image or image-to-text conversions are extremely multimodal in nature. The proposed model for creating 256 × 256 realistic images from Arabic text descriptions is discussed in this article. The relationship between an Arabic word in a sentence and its component in a picture as introduced in this paper using the DAMSM model. This model teaches two neural networks how to map the Arabic picture and word sub-regions of a full sentence to a shared semantic model. It performs well as an Arabic-text encoder and a picture encoder. We start with the Modified-Arabic dataset and train the model from scratch. The proposed model establishes a new standard for the conversion of Arabic text to realistic pictures. A mutation happens when Arabic is used as the primary language for converting Arabic texts to real images. The inception score of the newly introduced model reported by 3.42 ± .05 on the CUB database.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4293085966",
    "type": "article"
  },
  {
    "title": "BERIS: An mBERT-based Emotion Recognition Algorithm from Indian Speech",
    "doi": "https://doi.org/10.1145/3517195",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "Pramod Mehra; Shashi Kant Verma",
    "corresponding_authors": "",
    "abstract": "Emotions, the building blocks of the human intellect, play a vital role in Artificial Intelligence (AI). For a robust AI-based machine, it is important that the machine understands human emotions. COVID-19 has introduced the world to no-touch intelligent systems. With an influx of users, it is critical to create devices that can communicate in a local dialect. A multilingual system is required in countries like India, which has a large population and a diverse range of languages. Given the importance of multilingual emotion recognition, this research introduces BERIS, an Indian language emotion detection system. From the Indian sound recording, BERIS estimates both acoustic and textual characteristics. To extract the textual features, we used Multilingual Bidirectional Encoder Representations from Transformers. For acoustics, BERIS computes the Mel Frequency Cepstral Coefficients and Linear Prediction coefficients, and Pitch. The features extracted are merged in a linear array. Since the dialogues are of varied lengths, the data are normalized to have arrays of equal length. Finally, we split the data into training and validated set to construct a predictive model. The model can predict emotions from the new input. On all the datasets presented, quantitative and qualitative evaluations show that the proposed algorithm outperforms state-of-the-art approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4293793243",
    "type": "article"
  },
  {
    "title": "Clustering-based Sequence to Sequence Model for Generative Question Answering in a Low-resource Language",
    "doi": "https://doi.org/10.1145/3563036",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Amir Jalaly Bidgoly; Hossein Amirkhani; Razieh Baradaran",
    "corresponding_authors": "",
    "abstract": "Despite the impressive success of sequence to sequence models for generative question answering, they need a vast amount of question-answer pairs during training, which is hard and expensive to obtain, especially for low-resource languages. In this article, we present a framework that exploits the semantic clusters among the question-answer pairs to compensate for the lack of enough training data. In the training phase, the question-answer pairs are clustered, and a cluster predictor is trained to identify the cluster each question belongs to. Then, a sequence to sequence model is trained, where there is a different generator for each cluster in the decoder component. During the test phase, the cluster of the input question is first identified using the trained cluster predictor, and the appropriate decoder is exploited. Our experiments on a Persian religious dataset show that the proposed method outperforms the standard sequence to sequence model by a large margin in terms of ROUGE and BLEU scores. This is traced back to the lower number of words in each cluster, leading to a reduction in the number of effective parameters each generator needs to learn, which help the model learn from fewer training data with less overfitting.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4296130355",
    "type": "article"
  },
  {
    "title": "Generation of Voice Signal Tone Sandhi and Melody Based on Convolutional Neural Network",
    "doi": "https://doi.org/10.1145/3545569",
    "publication_date": "2022-09-19",
    "publication_year": 2022,
    "authors": "Wei Jiang; Mengqi Li; Mohammad Shabaz; Ashutosh Sharma; Mohd Anul Haq",
    "corresponding_authors": "",
    "abstract": "There is a need to prevent the use of modulated voice signals to conduct criminal activities. Voice signal change detection based on convolutional neural networks is proposed. We use three commonly used voice processing software (Audacity, CoolEdit, and RTISI) to change tones in voice libraries. The research further raises each voice by five semitones and are recorded at different levels (+4, +5, +6, +7, and +8, respectively). Simultaneously, every voice is lowered by five halftones, represented as –4, –5, –6, –7, and –8, respectively. The convolution neural network corresponding to network b-3 is determined as the final classifier in this article through experiments. The average accuracy A1 of its three categories has reached more than 97%, the detection accuracy A2 of electronic tone sandhi speech has reached more than 97%, and the false alarm rate of the original speech is less than 1.9%. The outcomes obtained shows that the detection algorithm in this article is effective, and it has good generalization ability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4296449081",
    "type": "article"
  },
  {
    "title": "Topic-Based Unsupervised and Supervised Dictionary Induction",
    "doi": "https://doi.org/10.1145/3564698",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Yuzhi Liu; Massimo Piccardi",
    "corresponding_authors": "",
    "abstract": "Word translation is a natural language processing task that provides translation between the words of a source and a target language. As a task, it reduces to the induction of a bilingual dictionary, which is typically performed by aligning word embeddings of the source language to word embeddings of the target language. To date, all the existing approaches have focused on performing a single, global alignment in word embedding space. However, semantic differences between the various languages, in addition to differences in the content of the corpora used for training the word embeddings, can hinder the effectiveness of such a global alignment. For this reason, in this article we propose conducting the alignment between the source and target embedding spaces by multiple mappings at topic level. The experimental results show that our approach has been able to achieve an average accuracy improvement of +3.30 percentage points over a state-of-the-art approach in unsupervised dictionary induction from languages as diverse as German, French, Italian, Spanish, Finnish, Turkish, and Chinese to English, and +3.95 points average improvement in supervised dictionary induction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4297999782",
    "type": "article"
  },
  {
    "title": "Research on Teaching Reform of Innovative and Entrepreneurial Talents Training Course in Local Undergraduate Universities Based on Intelligent Blended Learning",
    "doi": "https://doi.org/10.1145/3529393",
    "publication_date": "2022-10-19",
    "publication_year": 2022,
    "authors": "Yanhong Ding; Na Yang",
    "corresponding_authors": "",
    "abstract": "In order to make college development better accord with the development demand of regional society, the innovation and entrepreneurship (IE) training course reform in local undergraduate colleges is studied. To begin with, the research background as well as necessity of the teaching reform of basic courses in local undergraduate colleges for IE are analyzed. Moreover, the current situation of IE education (IEE) in the local undergraduate colleges in Shaanxi Province is investigated to understand the implementation status of IEE in local undergraduate colleges in Shaanxi Province. Then, the adverse factors affecting the IEE implementation process are studied. Finally, the adverse factors and the causes of the problems are analyzed. From the four aspects of educational philosophy, curriculum structure, curriculum content as well as teaching staff, the suggestions and programs of basic courses reform for IEE are provided. The survey results show that more than 50% of the students have their own unique understanding of IEE, and think that IEE is quite necessary and helpful for their own development. The school has also attached importance to IEE, but the effect is mediocre at present. It suggests that students’ demand for IEE cannot be satisfied now. The methods and survey results proposed give guidance as well as data support to the IEE reform in local undergraduate colleges.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4306845364",
    "type": "article"
  },
  {
    "title": "Multilingual News Feed Analysis using Intelligent Linguistic Particle Filtering Techniques",
    "doi": "https://doi.org/10.1145/3569899",
    "publication_date": "2022-11-18",
    "publication_year": 2022,
    "authors": "S. Rakesh Kumar; Gayathri Nagasubramanian; S. Muthuramalingam; Fadi Al‐Turjman",
    "corresponding_authors": "",
    "abstract": "Analyzing real-time news feeds and their impacts in the real world is a complex task in the social networking arena. Particularly, countries with a multilingual environment have various patterns and perceptions of news reports considering the diversity of the people. Multilingual and multimodal news analysis is an emerging trend for evaluating news source neutralities. Therefore, in this work, four new deep news particle filtering techniques were developed, including generic news analysis, sequential importance re-sampling (SIR) -based news particle filtering analysis, reinforcement learning (RL) -based multimodal news analysis, and deep Convolution neural network (DCNN) -based multi-news filtering approach, for news classification. Results indicate that these techniques, which primarily employ particle filtering with multilevel sampling strategies, produce 15% to 20% better performance than conventional news analysis techniques.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4309467331",
    "type": "article"
  },
  {
    "title": "CodeFed: Federated Speech Recognition for Low-Resource Code-Switching Detection",
    "doi": "https://doi.org/10.1145/3571732",
    "publication_date": "2022-11-17",
    "publication_year": 2022,
    "authors": "Chetan Madan; Harshita Diddee; Deepika Kumar; Mamta Mittal",
    "corresponding_authors": "",
    "abstract": "One common constraint in the practical application of speech recognition is Code Switching. The issue of code-switched languages is especially aggravated in the context of Indian languages – since most massively multilingual models are trained on corpora that are not representative of the diverse set of Indian languages. An associated constraint with such systems is the privacy-intrusive nature of the applications that aim to collate such representative data. To collectively mitigate both problems, this work presents CodeFed: A federated learning-based code-switching detection model that can be deployed to collaboratively be trained by leveraging private data from multiple users, without compromising their privacy. Using a representative low-resource Indic dataset, we demonstrate the superior performance of a collaboratively trained global model that is trained using federated learning on three low-resource Indic languages – Gujarati, Tamil and Telugu and draw a comparison of the model with respect to the most current work in the field. Finally, to evaluate the practical realizability of the proposed system, CodeFed also discusses the system overview of the label generation architecture which may accompany CodeFed’s possible real-time deployment.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4309620154",
    "type": "article"
  },
  {
    "title": "Improving the Robustness of Loanword Identification in Social Media Texts",
    "doi": "https://doi.org/10.1145/3572773",
    "publication_date": "2022-11-23",
    "publication_year": 2022,
    "authors": "Chenggang Mi",
    "corresponding_authors": "Chenggang Mi",
    "abstract": "As a potential bilingual resource, loanwords play a very important role in many natural language processing tasks. If loanwords in a low-resource language can be identified effectively, the generated donor-receipt word pairs will benefit many cross-lingual natural language processing tasks. However, most studies on loanword identification mainly focus on formal texts such as news and government documents. Loanword identification in social media texts is still an under-studied field. Since it faces many challenges and can be widely used in several downstream tasks, more efforts should be put on loanword identification in social media texts. In this study, we present a multi-task learning architecture with deep bi-directional recurrent neural networks for loanword identification in social media texts, where different task supervision can happen at different layers. The multi-task neural network architecture learns higher-order feature representations from word and character sequences along with basic spell error checking, part-of-speech tagging, and named entity recognition information. Experimental results on Uyghur loanword identification in social media texts in five donor languages (Chinese, Arabic, Russian, Turkish, and Farsi) show that our method achieves the best performance compared with several strong baseline systems. We also combine the loanword detection results into the training data of neural machine translation for low-resource language pairs. Experiments show that models trained on the extended datasets achieve significant improvements compared with the baseline models in all language pairs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4309765392",
    "type": "article"
  },
  {
    "title": "<i>WH</i> <sup>2</sup> <i>D</i> <sup>2</sup> <i>N</i> <sup>2</sup> : Distributed AI-enabled OK-ASN Service for Web of Things",
    "doi": "https://doi.org/10.1145/3564242",
    "publication_date": "2022-12-16",
    "publication_year": 2022,
    "authors": "Kun Liang; Ruhui Ma; Hua Yang; Hao Wang; Ningxin Hu; Tao Song; Honghao Gao; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "Model data-driven ontology and knowledge presentation for evolving semantic Asian social networks (OK-ASN) is a critical strategy for web of things (WoT) services. Meanwhile, Deep Neural Network (DNN)-based OK-ASN service in WoT is growing rapidly. However, most DNN-based services cannot utilize the potential of WoT fully, as heterogeneity exists in WoT. Therefore, this article proposes a novel framework called Web-based Heterogeneous Hierarchical Distributed Deep Neural Network ( WH 2 D 2 N 2 ) to deploy the DNNs for OK-ASN services on WoT, overcoming the heterogeneity. The architecture of the system and the designed Edge-Cloud-Joint execute scheme utilize heterogeneous devices to make DNN inference ubiquitous and output two types of results to meet various requirements. To bring robustness to OK-ASN services, a global scheduling is designed to arrange the workflow dynamically. The results of our experiments prove the efficiency of the execute scheme and the global scheduling in the system.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4311604301",
    "type": "article"
  },
  {
    "title": "Building Dialogue Understanding Models for Low-resource Language Indonesian from Scratch",
    "doi": "https://doi.org/10.1145/3575803",
    "publication_date": "2022-12-16",
    "publication_year": 2022,
    "authors": "Donglin Di; Xianyang Song; Weinan Zhang; Yue Zhang; Fanglin Wang",
    "corresponding_authors": "",
    "abstract": "Using off-the-shelf resources from resource-rich languages to transfer knowledge to low-resource languages has received a lot of attention. The requirements of enabling the model to achieve the reliable performance, including the scale of required annotated data and the effective framework, are not well guided. To address the first question, we empirically investigate the cost-effectiveness of several methods for training intent classification and slot-filling models from scratch in Indonesia (ID) using English data. Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), which consists of “BiCF Mixing”, “Latent Space Refinement” and “Joint Decoder”, respectively, to overcome the lack of low-resource language dialogue data. BiCF Mixing based on the word-level alignment strategy generates code-mixed data by utilizing the importance-frequency and translating-confidence. Moreover, Latent Space Refinement trains a new dialogue understanding model using code-mixed data and word embedding models. Joint Decoder based on Bidirectional LSTM (BiLSTM) and Conditional Random Field (CRF) is used to obtain experimental results of intent classification and slot-filling. We also release a large-scale fine-labeled Indonesia dialogue dataset (ID-WOZ 1 ) and ID-BERT for experiments. BiCF achieves 93.56% and 85.17% (F1 score) on intent classification and slot filling, respectively. Extensive experiments demonstrate that our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4311605657",
    "type": "article"
  },
  {
    "title": "Neurocomputer System of Semantic Analysis of the Text in the Kazakh Language",
    "doi": "https://doi.org/10.1145/3652159",
    "publication_date": "2024-03-13",
    "publication_year": 2024,
    "authors": "Akerke Аkanova; Aisulu Ismailova; Zhanar Oralbekova; Zhanat Kenzhebayeva; Galiya Anarbekova",
    "corresponding_authors": "",
    "abstract": "The purpose of the study is to solve an extreme mathematical problem—semantic analysis of natural language, which can be used in various fields, including marketing research, online translators, and search engines. When training the neural network, data training methods based on the latent Dirichlet allocation model and vector representation of words were used. This study presents the development of a neurocomputer system used for the purpose of semantic analysis of the text in the Kazakh language, based on machine learning and the use of the latent Dirichlet allocation model. In the course of the study, the stages of system development were considered, regarding the text recognition algorithm. The Python programming language was used as a tool using libraries that greatly simplify the process of creating neural networks, including the Keras library. An experiment was conducted with the involvement of experts to test the effectiveness of the system, the results of which confirmed the reliability of the data provided by the system. The papers of modern computer linguists dealing with the problems of natural language processing using various technologies and methods are considered.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392750043",
    "type": "article"
  },
  {
    "title": "Online English Resource Integration Algorithm based on high-dimensional Mixed Attribute Data Mining",
    "doi": "https://doi.org/10.1145/3657289",
    "publication_date": "2024-04-16",
    "publication_year": 2024,
    "authors": "Zhiyu Zhou",
    "corresponding_authors": "Zhiyu Zhou",
    "abstract": "To improve the scalability of resources and ensure the effective sharing and utilization of online English resources, an online English resource integration algorithm based on high-dimensional mixed-attribute data mining is proposed. First, an integration structure based on high-dimensional mixed-attribute data mining is constructed. According to this structure, the characteristics of online English resources are extracted, and historical data mining is carried out in combination with the spatial distribution characteristics of resources. In this way, the spatial mapping function of features is established, and the optimal clustering center is designed according to the clustering and fusion structure of online English resources. At this node, the clustering and fusion of online English resources are carried out. According to the fusion results, the distribution structure model of online English resources is constructed, and the optimization research of the integration algorithm of online English resources is carried out. The experimental results show that the integration optimization efficiency of the proposed algorithm is 89%, and the packet loss rate is 0.19%. It has good integration performance, and can realize the integration of multi-channel and various forms of online English resources.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394844604",
    "type": "article"
  },
  {
    "title": "Abusive Comment Detection in Tamil Code-Mixed Data by Adjusting Class Weights and Refining Features",
    "doi": "https://doi.org/10.1145/3664619",
    "publication_date": "2024-05-18",
    "publication_year": 2024,
    "authors": "G L Gayathri; Krithika Swaminathan; Divyasri Krishnakumar; D. Thenmozhi; B. Bharathi",
    "corresponding_authors": "",
    "abstract": "In recent years, a significant portion of the content on various platforms on the internet has been found to be offensive or abusive. Abusive comment detection can go a long way in preventing internet users from facing the adverse effects of coming in contact with abusive language. This problem is particularly challenging when the comments are found in low-resource languages like Tamil or Tamil-English code-mixed text. So far, there has not been any substantial work on abusive comment detection using imbalanced datasets. Furthermore, significant work has not been performed, especially for Tamil code-mixed data, that involves analysing the dataset for classification and accordingly creating a custom vocabulary for preprocessing. This paper proposes a novel approach to classify abusive comments from an imbalanced dataset using a customised training vocabulary and a combination of statistical feature selection with language-agnostic feature selection while making use of explainable AI for feature refinement. Our model achieved an accuracy of 74% and a macro F1-score of 0.46.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4397033164",
    "type": "article"
  },
  {
    "title": "In-memory database load balancing optimization for massive information processing of the Internet of Things",
    "doi": "https://doi.org/10.1145/3670996",
    "publication_date": "2024-06-10",
    "publication_year": 2024,
    "authors": "Huixiang Xu",
    "corresponding_authors": "Huixiang Xu",
    "abstract": "In order to improve the operation effect of the in-memory database for massive information processing of the Internet of Things, this paper combines the load balancing signal processing algorithm to carry out the load balancing optimization analysis of the in-memory database. According to the local transformation characteristics of non-stationary multi-component signals, an adaptive FSST algorithm is proposed in this paper. According to the signal separability condition, this paper uses the local Rayleigh entropy to estimate the window function parameters of the adaptive FSST and the adaptive FSST2. In addition, this paper adopts an adaptive window function to automatically match the local changes of the signal, so that the signal has the optimal energy aggregation in any part. The results show that when the number of concurrent users is the same, the time consumption, throughput and bandwidth of the proposed method are always higher than the method proposed in reference [10]. When the number of concurrent books is 97, the time of the proposed method is 45000ms, the time of the proposed method is 40000ms, the highest throughput of the proposed method is 2.30 MB/s, the highest bandwidth is 11.9MB/s, the highest throughput of the method proposed in reference [10] is 2.2 MB/s, and the highest bandwidth is 11.8MB/s. The load balancing optimization algorithm of the memory database for massive information processing of the Internet of Things has good results.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399498866",
    "type": "article"
  },
  {
    "title": "INTERPRETING AND TRANSLATING THE KOREAN LANGUAGE BASED ON THE MACHINE TRANSLATION MODEL FOR COLLEGE STUDENTS",
    "doi": "https://doi.org/10.1145/3674969",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Wei Fang",
    "corresponding_authors": "Wei Fang",
    "abstract": "Korean languages (KL) have the first, middle, and final consonant sounds. Because the basic consonants were produced in imitation of the human pronunciation organs, mimicking the forms of the organ of articulation when they are uttered makes it stand out. Research into text identification and translation for display boards has dominated contemporary machine vision work. When it comes to automated text translation, two examples come to mind: tour guide software and hotel room service bots. The fundamental issue is that the two languages have distinct pronunciations and grammatical structures. Many English sounds have no equivalent in the Korean language, making learning the language more difficult for Korean speakers. KL-MTM is gaining popularity in the classroom, although assessment is difficult for human raters, such as language instructors, because of the time and effort required to evaluate informational equivalency between the source-language message and its translations in the target language. Overall, students utilize them for various purposes to supplement their language education, whether at home or in the classroom. Results show a wide range of student dependencies and values for these tools, with some students relying heavily on them while others are less reliant on them. The students evaluated prominent KL-MTM tools, indicating difficulty critically analyzing their outcomes. Considerations for teaching and learning are addressed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400089410",
    "type": "article"
  },
  {
    "title": "Predictive Modeling for Arabic Fake News Detection: Leveraging Language Model Embeddings and Stacked Ensemble",
    "doi": "https://doi.org/10.1145/3677016",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Muhammad Umer; Arwa A. Jamjoom; Shtwai Alsubai; Aisha Ahmed Alarfaj; Ebtisam Alabdulqader; Imran Ashraf",
    "corresponding_authors": "",
    "abstract": "The proliferation of fake news poses a substantial threat to information integrity, prompting the need for robust detection mechanisms. This study advances the research on Arabic fake news detection and overcomes the limitation of lower accuracy for fake news detection. This research addresses Arabic fake news detection using word embedding and a powerful stacking classifier. The proposed model combines bagging, boosting, and baseline classifiers, harnessing the strengths of each to create a robust ensemble. Extensive experiments are carried out to evaluate the proposed approach indicating remarkable results, with recall, F1 score, accuracy, and precision reaching 99%. The utilization of advanced stacking techniques, coupled with appropriate textual feature extraction, empowers the model to effectively detect Arabic fake news. Study results make a valuable contribution to fake news detection, particularly in the Arabic context, providing a valuable tool for enhancing information veracity and fostering a more informed public discourse. Furthermore, the proposed model’s accuracy is compared with other cutting-edge models from the existing literature to showcase its superior performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400422367",
    "type": "article"
  },
  {
    "title": "Recognition of Counterfactual Statements in Turkish",
    "doi": "https://doi.org/10.1145/3706105",
    "publication_date": "2024-11-27",
    "publication_year": 2024,
    "authors": "Ali Acar; Selma Tekır",
    "corresponding_authors": "",
    "abstract": "Counterfactual statements are examples of causal reasoning as they describe events that did not happen and, optionally, those events’ consequences if they happened. SemEval-2020 introduces the counterfactual detection (CFD) task and shares an English dataset. Since then, a set of datasets has been released in English, German, and Japanese as part of Amazon product reviews. This work releases the first Turkish corpus of counterfactuals (TRCD). The data collection process is driven by a clue phrase list of counterfactuals, mainly in the form of verb inflections in Turkish. We use clue phrase-based filtering to collect sentences from the Turkish National Corpus (TNC). On the other hand, half of the collection is subject to random word filtering to avoid selection bias due to clue phrases. After the human annotation process with an Inter Annotator Agreement of 0.65, we have 5000 sentences, of which \\(12.8\\% \\) contain counterfactual statements. Furthermore, we provide a comprehensive baseline of transformer-based models by testing the effect of clue phrases, cross-lingual performance comparisons using the available CFD datasets, and zero-shot cross-lingual classification experiments using fine-tuning on the different combinations of the existing datasets. The results confirm that TRCD is compatible with the other CFD datasets. Moreover, fine-tuning a Turkish-specific model (BERTurk) performs better than the multilingual alternatives (mBERT and XLM-R). BERTurk is more robust to clue phrase masking. This result emphasizes the importance of a language-specific tokenizer for contextual understanding, especially for low-resource languages. Finally, our qualitative analysis gives insights into errors by different models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404766634",
    "type": "article"
  },
  {
    "title": "UPON: Urdu Poetry Generation Using Deep Learning: A Novel Approach and Evaluation",
    "doi": "https://doi.org/10.1145/3708535",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Muhammad Rauf Tabassam; Hajra Waheed; Iqra Safder; Raheem Sarwar; Naif Radi Aljohani; Raheel Nawaz; Saeed‐Ul Hassan; Farooq Zaman; Muhammad Ahtazaz Ahsan",
    "corresponding_authors": "",
    "abstract": "Poetry represents the oldest and most esteemed literary form, allowing poets to convey ideas while carefully attending to elements such as meaning, coherence, poetic quality, and fluency. Notably, the creation of good poetry entails considerations of rhyme and meter. With the advent of artificial intelligence (AI), significant advancements have been made in automatic text generation, primarily within languages such as English and Chinese. However, the generation of Urdu poetry presents a unique challenge due to the language’s inherent ambiguity, cultural and historical nuances, and the demand for creativity. The existing body of literature has only marginally explored Urdu prose and has almost entirely overlooked the domain of Urdu poetry generation, primarily due to the scarcity of comprehensive training data. In response to this deficiency, this research endeavor addresses this challenge. It begins by introducing a specialized Urdu poetry dataset adhering to a specific meter, ’behr-e-khafeef,’ which incorporates approximately 20,000 couplets from the Rekhta repository. Subsequently, a character-based encoding methodology is proposed to transform these couplets into a numerical representation, assigning a distinct identifier to each character. The generation process initiates with the creation of the first verse through a character-level LSTM, followed by the application of a machine translation technique, specifically sequence-to-sequence learning, to formulate the second verse based on the first. The generated poetry is subjected to evaluation based on metrics, including BLEU scores. Additionally, an expert panel of Urdu poets is engaged to conduct a human assessment of the generated couplets, with the evaluation encompassing critical dimensions such as meaning, coherence, poetic quality, and fluency. Our findings are juxtaposed with existing poetry generation systems, demonstrating a notable advancement in the state-of-the-art, as evidenced by a BLEU score of 0.23. The research culminates with the presentation of prospective avenues for further exploration, aimed at inspiring the scholarly community to enhance the domain of poetry generation and augment existing contributions in this field.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405545540",
    "type": "article"
  },
  {
    "title": "Collective Web-Based Parenthetical Translation Extraction Using Markov Logic Networks",
    "doi": "https://doi.org/10.1145/2794399",
    "publication_date": "2015-12-18",
    "publication_year": 2015,
    "authors": "Richard Tzong‐Han Tsai",
    "corresponding_authors": "Richard Tzong‐Han Tsai",
    "abstract": "Parenthetical translations are translations of terms in otherwise monolingual text that appear inside parentheses. Parenthetical translations extraction (PTE) is the task of extracting parenthetical translations from natural language documents. One of the main difficulties in PTE is to detect the left boundary of the translated term in preparenthetical text. In this article, we propose a collective approach that employs Markov logic to model multiple constraints used in the PTE task. We show how various constraints can be formulated and combined in a Markov logic network (MLN). Our experimental results show that the proposed collective PTE approach significantly outperforms a current state-of-the-art method, improving the average F-measure up to 27.11% compared to the previous word alignment approach. It also outperforms an individual MLN-based system by 8.2% and a system based on conditional random fields by 5.9%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2198274028",
    "type": "article"
  },
  {
    "title": "TALLIP Perspectives: Editorial Commentary",
    "doi": "https://doi.org/10.1145/2710043",
    "publication_date": "2015-01-30",
    "publication_year": 2015,
    "authors": "Richard Sproat",
    "corresponding_authors": "Richard Sproat",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2267040095",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2791399",
    "publication_date": "2015-06-12",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "There has been recent interest in statistical approaches to Korean morphological analysis. However, previous studies have been based mostly on generative models, including a hidden Markov model (HMM), without utilizing discriminative models such as a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229621607",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2856425",
    "publication_date": "2016-02-01",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Nowadays, due to the explosive growth of web content and usage, users deal with their complex search tasks by web search engines. However, conventional search engines consider a search query corresponding only to a simple search task. In order to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230695957",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2915955",
    "publication_date": "2016-06-02",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Document Image Understanding (DIU) and Electronic Document Management are active fields of research involving image understanding, interpretation, efficient handling, and routing of documents as well as their retrieval. Research on most of the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232189453",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2764912",
    "publication_date": "2015-04-20",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Nowadays, bilingual or multilingual speech recognition is confronted with the accent-related problem caused by non-native speech in a variety of real-world applications. Accent modeling of non-native speech is definitely challenging, because the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238292097",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2730923",
    "publication_date": "2015-01-30",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A novel method to induce wide-coverage Combinatory Categorial Grammar (CCG) resources for Japanese is proposed in this article. For some languages including English, the availability of large annotated corpora and the development of data-based induction ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238379414",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2847552",
    "publication_date": "2016-01-22",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Two of the most popular Machine Translation (MT) paradigms are rule based (RBMT) and corpus based, which include the statistical systems (SMT). When scarce parallel corpus is available, RBMT becomes particularly attractive. This is the case of the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245060439",
    "type": "paratext"
  },
  {
    "title": "TALLIP Perspectives: Editorial Commentary",
    "doi": "https://doi.org/10.1145/2823512",
    "publication_date": "2015-11-11",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on TALLIP Perspectives: Editorial Commentary: The State of the JournalACM Transactions on Asian and Low-Resource Language Information ProcessingVolume 14Issue 4Article No.: 19pp 1–3https://doi.org/10.1145/2823512Published:11 November 2015Publication History 0citation178DownloadsMetricsTotal Citations0Total Downloads178Last 12 Months10Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245498700",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2845556",
    "publication_date": "2015-11-11",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This special issue contains four articles based on and expanded from systems presented at the SIGHAN-7 Chinese Spelling Check Bakeoff. We provide an overview of the approaches and designs for Chinese spelling checkers presented in these articles. We ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246289458",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2961867",
    "publication_date": "2016-12-06",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The online handwriting data are an integral part of data analysis and classification research, as collected handwritten data offers many challenges to group handwritten stroke classes. The present work has been done for grouping handwritten strokes from ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248762337",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3008658",
    "publication_date": "2016-12-14",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Morphological analysis, which includes analysis of part-of-speech (POS) tagging, stemming, and morpheme segmentation, is one of the key components in natural language processing (NLP), particularly for agglutinative languages. In this article, we ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254383165",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2876004",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Language Model (LM) is an essential component of Statistical Machine Translation (SMT). In this article, we focus on developing efficient methods for LM construction. Our main contribution is that we propose a Natural N-grams based Converting (NNGC) ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254807728",
    "type": "paratext"
  },
  {
    "title": "Deep Neural Network Based Noised Asian Speech Enhancement and Its Implementation on a Hearing Aid App",
    "doi": "https://doi.org/10.1145/3439797",
    "publication_date": "2021-07-21",
    "publication_year": 2021,
    "authors": "Xiaoqian Fan; Bowen Yang; Wenzhi Chen; Quanfang Fan",
    "corresponding_authors": "",
    "abstract": "This article studies noised Asian speech enhancement based on the deep neural network (DNN) and its implementation on an app. We use the THCHS-30 speech dataset and the common noise dataset in daily life as training and testing data of the DNN. To stack the frequency data of multiple audio frames to improve the effect of speech enhancement, the system compares the best number of stacked frames during training and testing. At the same time, the influence of training rounds on the PESQ is compared, and the best number of rounds is obtained. On this basis, the best model is implemented on the hearing aid app, and the real-time performance of the device is tested. The experiment shows that based on the DNN, using an appropriate number of rounds for training and using an appropriate number of audio frames stacking to improve the speech enhancement effect, and transplanting this speech enhancement model to the hearing aid app, can effectively improve speech clarity and intelligibility within a reasonable time delay range.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3186489282",
    "type": "article"
  },
  {
    "title": "Improved Word Sense Determination in Malayalam using Latent Dirichlet Allocation and Semantic Features",
    "doi": "https://doi.org/10.1145/3476978",
    "publication_date": "2021-11-03",
    "publication_year": 2021,
    "authors": "S Sruthi; B. Kannan; Binu Paul",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed phenomenal developments worldwide in the field of NLP . But developments in Indian regional languages are very few compared to them. This work is a step towards the construction of a target word sense disambiguation system in Malayalam, which is the regional language of the state of Kerala, India. Word Sense Disambiguation/Determination refers to the task of correctly identifying the sense of an ambiguous word from its context. This is considered an AI-Complete problem in the field of Natural Language Processing . For this purpose, an exclusive corpus of 1,147 contexts of target ambiguous words has been created, which to the best of our knowledge is the first attempt in Malayalam. This work describes how the performance of an unsupervised LDA-based approach towards WSD could be improved using semantic features like synonyms and co-occurrence information.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3210565949",
    "type": "article"
  },
  {
    "title": "Development of Automatic Rule-based Semantic Tagger and Karaka Analyzer for Hindi",
    "doi": "https://doi.org/10.1145/3479155",
    "publication_date": "2021-11-18",
    "publication_year": 2021,
    "authors": "Pragya Katyayan; Nisheeth Joshi",
    "corresponding_authors": "",
    "abstract": "Hindi is the third most-spoken language in the world (615 million speakers) and has the fourth highest native speakers (341 million). It is an inflectionally rich and relatively free word-order language with an immense vocabulary set. Despite being such a celebrated language across the globe, very few Natural Language Processing (NLP) applications and tools have been developed to support it computationally. Moreover, most of the existing ones are not efficient enough due to the lack of semantic information (or contextual knowledge). Hindi grammar is based on Paninian grammar and derives most of its rules from it. Paninian grammar very aggressively highlights the role of karaka theory in free-word order languages. In this article, we present an application that extracts all possible karakas from simple Hindi sentences with an accuracy of 84.2% and an F1 score of 88.5%. We consider features such as Parts of Speech tags, post-position markers (vibhaktis), semantic tags for nouns and syntactic structure to grab the context in different-sized word windows within a sentence. With the help of these features, we built a rule-based inference engine to extract karakas from a sentence. The application takes in a text file with clean (without punctuation) simple Hindi sentences and gives back karaka tagged sentences in a separate text file as output.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3216061704",
    "type": "article"
  },
  {
    "title": "Fusion Based AER System Using Deep Learning Approach for Amplitude and Frequency Analysis",
    "doi": "https://doi.org/10.1145/3488369",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "A. Pramod Reddy; V. Vijayarajan",
    "corresponding_authors": "",
    "abstract": "Automatic emotion recognition from Speech (AERS) systems based on acoustical analysis reveal that some emotional classes persist with ambiguity. This study employed an alternative method aimed at providing deep understanding into the amplitude–frequency, impacts of various emotions in order to aid in the advancement of near term, more effectively in classifying AER approaches. The study was undertaken by converting narrow 20 ms frames of speech into RGB or grey-scale spectrogram images. The features have been used to fine-tune a feature selection system that had previously been trained to recognise emotions. Two different Linear and Mel spectral scales are used to demonstrate a spectrogram. An inductive approach for in sighting the amplitude and frequency features of various emotional classes. We propose a two-channel profound combination of deep fusion network model for the efficient categorization of images. Linear and Mel- spectrogram is acquired from Speech-signal, which is prepared in the recurrence area to input Deep Neural Network. The proposed model Alex-Net with five convolutional layers and two fully connected layers acquire most vital features form spectrogram images plotted on the amplitude-frequency scale. The state-of-the-art is compared with benchmark dataset (EMO-DB). RGB and saliency images are fed to pre-trained Alex-Net tested both EMO-DB and Telugu dataset with an accuracy of 72.18% and fused image features less computations reaching to an accuracy 75.12%. The proposed model show that Transfer learning predict efficiently than Fine-tune network. When tested on Emo-DB dataset, the propȯsed system adequately learns discriminant features from speech spectrȯgrams and outperforms many stȧte-of-the-art techniques.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4200308572",
    "type": "article"
  },
  {
    "title": "Tri-training for Dependency Parsing Domain Adaptation",
    "doi": "https://doi.org/10.1145/3488367",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Shu Jiang; Zuchao Li; Hai Zhao; Bao‐Liang Lu; Rui Wang",
    "corresponding_authors": "",
    "abstract": "In recent years, the research on dependency parsing focuses on improving the accuracy of the domain-specific (in-domain) test datasets and has made remarkable progress. However, there are innumerable scenarios in the real world that are not covered by the dataset, namely, the out-of-domain dataset. As a result, parsers that perform well on the in-domain data usually suffer from significant performance degradation on the out-of-domain data. Therefore, to adapt the existing in-domain parsers with high performance to a new domain scenario, cross-domain transfer learning methods are essential to solve the domain problem in parsing. This paper examines two scenarios for cross-domain transfer learning: semi-supervised and unsupervised cross-domain transfer learning. Specifically, we adopt a pre-trained language model BERT for training on the source domain (in-domain) data at the subword level and introduce self-training methods varied from tri-training for these two scenarios. The evaluation results on the NLPCC-2019 shared task and universal dependency parsing task indicate the effectiveness of the adopted approaches on cross-domain transfer learning and show the potential of self-learning to cross-lingual transfer learning.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4200461937",
    "type": "article"
  },
  {
    "title": "Prose2Poem: The Blessing of Transformers in Translating Prose to Persian Poetry",
    "doi": "https://doi.org/10.1145/3592791",
    "publication_date": "2023-04-14",
    "publication_year": 2023,
    "authors": "Reza Khanmohammadi; Mitra Sadat Mirshafiee; Yazdan Rezaee Jouryabi; Seyed Abolghasem Mirroshandel",
    "corresponding_authors": "",
    "abstract": "Persian poetry has consistently expressed its philosophy, wisdom, speech, and rationale based on its couplets, making it an enigmatic language on its own to both native and non-native speakers. Nevertheless, the noticeable gap between Persian prose and poems has left the two pieces of literature mediumless. Having curated a parallel corpus of prose and their equivalent poems, we introduce a novel Neural Machine Translation approach for translating prose to ancient Persian poetry using transformer-based language models in an exceptionally low-resource setting. Translating input prose into ancient Persian poetry presents two primary challenges: In addition to being reasonable in conveying the same context as the input prose, the translation must also satisfy poetic standards. Hence, we designed our method consisting of three stages. First, we trained a transformer model from scratch to obtain an initial translations of the input prose. Next, we designed a set of heuristics to leverage contextually rich initial translations and produced a poetic masked template. In the last stage, we pretrained different variations of BERT on a poetry corpus to use the masked language modelling technique to obtain final translations. During the evaluation process, we considered both automatic and human assessment. The final results demonstrate the eligibility and creativity of our novel heuristically aided approach among Literature professionals and non-professionals in generating novel Persian poems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3202020111",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3041821",
    "publication_date": "2017-04-06",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article proposes a technique for mining bilingual lexicons from pairs of parallel short word sequences. The technique builds a generative model from a corpus of training data consisting of such pairs. The model is a hierarchical nonparametric ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235233203",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3141228",
    "publication_date": "2017-11-16",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Phrase-based Statistical Machine Translation (PBSMT) is commonly used for automatic translation. However, PBSMT runs into difficulty when either or both of the source and target languages are morphologically rich. Factored models are found to be useful ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237549516",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3097269",
    "publication_date": "2017-09-20",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Since the age of paper versions, dictionaries are often published with anomalies in their content resulting from lexicographer’s mistakes or from the lack of efficiency of automatic enrichment systems. Many of these anomalies are expensive to manually ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250794778",
    "type": "paratext"
  },
  {
    "title": "Approximating to the Real Translation Quality for Neural Machine Translation via Causal Motivated Methods",
    "doi": "https://doi.org/10.1145/3583684",
    "publication_date": "2023-02-13",
    "publication_year": 2023,
    "authors": "Xuewen Shi; Heyan Huang; Ping Jian; Yi-Kun Tang",
    "corresponding_authors": "",
    "abstract": "It is hard to evaluate translations objectively and accurately, which limits the applications of machine translation. In this article, we assume that the above phenomenon is caused by noise interference during translation evaluation, and we handle the problem through a perspective of causal inference. We assume that the observable translation score is affected by the unobservable true translation quality and some noise simultaneously. If there is a variable that is related to the noise and independent to the true translation quality, the related noise can be eliminated by removing the effect of that variable from the observed score. Based on the above causality hypothesis, this article studies the length bias problem of beam search for neural machine translation (NMT) and the input related noise problem of translation quality estimation (QE). For the NMT length bias problem, we conduct the experiments on four typical NMT tasks (Uyghur–Chinese, Chinese–English, English–German, and English–French) with different scales of datasets. Comparing with previous approaches, the proposed causal motivated method is model-agnostic and does not require supervised training. For QE tasks, we conduct the experiments on the WMT’20 submissions. Experimental results show that the denoised QE results gain better Pearson’s correlation scores with human assessed scores compared to the original submissions. Further analyses on the NMT and QE tasks also demonstrate the rationality of the empirical assumptions made on our methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4320497798",
    "type": "article"
  },
  {
    "title": "Learning Category Distribution for Text Classification",
    "doi": "https://doi.org/10.1145/3585279",
    "publication_date": "2023-02-27",
    "publication_year": 2023,
    "authors": "Xiangyu Wang; Chengqing Zong",
    "corresponding_authors": "",
    "abstract": "Label smoothing has a wide range of applications in the machine learning field. Nonetheless, label smoothing only softens the targets by adding a uniform distribution into a one-hot vector, which cannot truthfully reflect the underlying relations among categories. However, learning category relations is of vital importance in many fields such as emotion taxonomy and open set recognition. In this work, we propose a method to obtain the label distribution for each category (category distribution) to reveal category relations. Furthermore, based on the learned category distribution, we calculate new soft targets to improve the performance of model classification. Compared with existing methods, our algorithm can improve neural network models without any side information or additional neural network module by considering category relations. Extensive experiments have been conducted on four original datasets and 10 constructed noisy datasets with three basic neural network models to validate our algorithm. The results demonstrate the effectiveness of our algorithm on the classification task. In addition, three experiments (arrangement, clustering, and similarity) are also conducted to validate the intrinsic quality of the learned category distribution. The results indicate that the learned category distribution can well express underlying relations among categories.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4322495777",
    "type": "article"
  },
  {
    "title": "Transferred Low-rank Discriminative Sub-dictionary Learning Method for Information Identification and Understanding in IoT-based Natural Language Processing",
    "doi": "https://doi.org/10.1145/3586994",
    "publication_date": "2023-03-06",
    "publication_year": 2023,
    "authors": "Tongguang Ni; Xiaoqing Gu; Yizhang Jiang; Yi Gu",
    "corresponding_authors": "",
    "abstract": "As Internet of Things (IoT) technology progresses rapidly, there is an increasing demand for automatic identification and understanding of natural language data. However, data labeling requires large amounts of effort and cost. Most intelligent algorithms rely on the assumption of uniform distribution of data, which brings great challenge to IoT-based natural language processing. To solve this problem, this study develops a transferred low-rank discriminative sub-dictionary learning (TLDSL) method. The TLDSL method learns a shared subspace through the maximum mean discrepancy (MMD) strategy that minimizes the distribution difference of sparse coefficients between the source and target domains. By learning the common sub-dictionary of the two domains, TLDSL reveals the intrinsic connection and establishes a bridge between the two domains, thus completing the knowledge transfer. By introducing the sub-dictionary incoherence, TLDSL can avoid the atomic correlation between different sub-dictionaries. In addition, the sparse coefficients are constrained in low rank representation, which can improve the model discrimination ability while preserving the global data structure. Experiments show that the TLDSL method can be effectively performed on cross-domain text classification and handwritten digit recognition.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4323312434",
    "type": "article"
  },
  {
    "title": "Enhancing Translation Competence via Flipping the EFL Translation Classroom in China",
    "doi": "https://doi.org/10.1145/3586995",
    "publication_date": "2023-03-14",
    "publication_year": 2023,
    "authors": "Chengche Qiao; Xiaodong Huang",
    "corresponding_authors": "",
    "abstract": "The ultimate goal of translation teaching is to cultivate the students’ translation competence. Inspired by the PACTE (PACTE, 2003) about the definition of translation competence, we further break down the translation competences into five sub-competences with more explicit teaching goals: lexical variety, grammatical correctness, stylistic appropriateness, fluency and coherence competence, which is vital for the cultivation of translation competence of engineering students. Then, an experimental method is used to compare the outcomes of students enrolled in a flipped and a traditional version of a EFL (English as a Foreign Language) Translation Course at a university in China. This empirical study aims to determine the effectiveness of flipped classroom model (FCM) in translation class. 60 undergraduates majoring in science and technology are involved in this research. Pre- and post- tests are used as instruments and data are analyzed using descriptive and inferential statistics. Quantitative data analysis reveals that comparing with traditional classroom flipped classroom can promote the cultivation of lexical variety, stylistic appropriateness, fluency and coherence competence. Considering the positive results obtained in the study, the article points out the use of the flipped approach as an example of good practice for cultivating the translation competence in EFL translation course.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4324157370",
    "type": "article"
  },
  {
    "title": "Cognitive Psychology Based Text Analysis Using Feature Extraction and Classification by Deep Learning Architectures for English Language Analysis",
    "doi": "https://doi.org/10.1145/3590150",
    "publication_date": "2023-04-12",
    "publication_year": 2023,
    "authors": "Shan Chen",
    "corresponding_authors": "Shan Chen",
    "abstract": "research-article Free Access Share on Cognitive Psychology Based Text Analysis Using Feature Extraction and Classification by Deep Learning Architectures for English Language AnalysisJust Accepted Author: Shan Chen University of Perpetual Help System Dalta, Manila, 1740, Philippines Changde Vocational Technical College, Changde, 415000, China Corresponding University of Perpetual Help System Dalta, Manila, 1740, Philippines Changde Vocational Technical College, Changde, 415000, China Corresponding 0009-0005-0218-0660View Profile Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingAccepted on February 2023https://doi.org/10.1145/3590150Published:12 April 2023Publication History 0citation84DownloadsMetricsTotal Citations0Total Downloads84Last 12 Months84Last 6 weeks19 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4365144903",
    "type": "article"
  },
  {
    "title": "Stock Price Trends Prediction Based on the Classical Models with Key Information Fusion of Ontologies",
    "doi": "https://doi.org/10.1145/3592599",
    "publication_date": "2023-04-13",
    "publication_year": 2023,
    "authors": "Dawei Jin; Yiyi Hu; Jingyu Chen; Mengran Xia",
    "corresponding_authors": "",
    "abstract": "An ontology of the financial field can support effective association and integration of financial knowledge. Based on behavioral finance, social media is increasingly applied as one of the data sources for information fusion in stock forecasting to approximate the patterns of market changes. By predicting Tesla (TSLA) stock price trends, this study finds that satisfactory forecasting results can be achieved using classical models and incorporating key information features from the technical indicator ontology class and the investor behavior ontology class, even in the face of the impact of the COVID-19 epidemic. In the post-epidemic period, the back propagation neural network (BPNN) model is used to predict the price trend of TSLA for the next five trading days with an accuracy of up to 91.34%, an F1 score of 0.91, and a return of up to 268.42% obtained from simulated trading. This study extends the research on stock forecasting using fused information in the ontology of the financial field, providing a new basis for general investors in the selection of fusion information and the application of trading strategies and providing effective support for organizations to make intelligent financial decisions under uncertainty.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4365451496",
    "type": "article"
  },
  {
    "title": "Information Processing for Low Resource Processing Based Cognitive Psychology for Second Language Teaching by Opinion Mining using Deep Learning Architecture",
    "doi": "https://doi.org/10.1145/3590151",
    "publication_date": "2023-04-13",
    "publication_year": 2023,
    "authors": "Juntao Li",
    "corresponding_authors": "Juntao Li",
    "abstract": "Foreign language instruction is crucial and difficult in every nation. Effective teachers must consider students' attitudes, motivation, and knowledge. Quality teaching determines student success. This study presents an AI-based deep learning method for second language and English instruction. This dataset was collected from students' second language and English teaching preferences. Dimensionality reduction and missing value removal were done on the dataset. Fuzzy set-based clustering with stochastic gradient residual neural network (ResNet) architecture classified this processed data. Students' second language and English teaching opinions were collected using fuzzy rules. Fuzzy clustering and stochastic gradient ResNet architecture classified this data. Student opinion mining was used for experimental study of various datasets and the parametric analysis yielded 96% accuracy, 90% sensitivity, 92% specificity, 82% F-1 score, 72% Mean squared error (MSE), and 88% Area Under the Curve (AUC).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4365451500",
    "type": "article"
  },
  {
    "title": "A Chinese Short Text Classification Method for Tax Audit Reports based on Word Importance and Syntactic Enhancement BERT",
    "doi": "https://doi.org/10.1145/3594635",
    "publication_date": "2023-04-25",
    "publication_year": 2023,
    "authors": "Yaning Shi; Lukun Wang; Chunpeng Tian; Rujia Wang; Jiaming Pei; Amir Hussian; Ali Kashif Bashir",
    "corresponding_authors": "",
    "abstract": "Tax audit is an important part of the tax collection and management system, which directly affects the economic interests of the country and taxpayers. Therefore, reducing the enforcement risk in tax audit is crucial to continuously improve the tax collection and management system. Recently, the research of using deep learning to classify Chinese tax audit data to achieve this goal has attracted much attention. Inspired by BERT, this paper proposes a syntactic enhancement BERT (SE-BERT). It can improve BERT’s text understanding ability by learning input features and grammatical structure of text from text content and location embeddings. In addition, we weight the word importance calculated by TF-IDF with SE-BERT to improve the ability of recognizing local salient features. Through comparative experiments on our Chinese tax audit dataset, our method achieves better performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4366985284",
    "type": "article"
  },
  {
    "title": "Research on the Implementation of Advertising Design Teaching Based on Unity3D Development Platform and Web3D Technology",
    "doi": "https://doi.org/10.1145/3595294",
    "publication_date": "2023-05-05",
    "publication_year": 2023,
    "authors": "Zhengwu Zhong",
    "corresponding_authors": "Zhengwu Zhong",
    "abstract": "In this work, the Unity3D development platform and Web3D technology are integrated into the teaching method of advertising design to get rid of the issues due to lack of communication and efficacy through design of digital advertisement. Based on this, an approach for ingenious product design from nature is proposed, with an emphasis on attaining a functional interaction of aesthetic intent and geometric features and investigating the relationships among natural systems and designers in product design from nature. The ponderings and research findings for the methodologies associated with the proposed approach are presented. This methodology is thought to significantly bring down the delivery time of ground-breaking design and development of products, both economically and technologically. The findings in comprehensive experiments demonstrates that interactive virtual technology can significantly enhance the efficacy and interaction of the whole system in the process of digital advertising design.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4372348768",
    "type": "article"
  },
  {
    "title": "A Study on the Strategic Application of Semantic Segmentation Based on Improved Convolutional Neural Networks on English-Chinese Interaction",
    "doi": "https://doi.org/10.1145/3596493",
    "publication_date": "2023-05-08",
    "publication_year": 2023,
    "authors": "Yang Xi; Bei Yang",
    "corresponding_authors": "",
    "abstract": "Teachers need to provide numerous examples sentences for students to translate in the process of teaching English, but the number of sentences given by teachers to practice subjectively is not enough. Therefore, the study constructs a text generation model using an improved convolutional neural network semantic segmentation method, where the corpus utterances are keyword extracted and new shorter utterances are generated based on the keywords for language learners to practice translation. The research first uses the textRank algorithm to extract semantic keywords to obtain a dataset, and then uses CNN to construct an encoder to achieve semantic encoding of the keyword dataset. However, during the research process, it was found that traditional CNN models are relatively sensitive to the location of input data. Therefore, the research introduces the idea of Decomposition Machine (FM) to improve the encoder. In order to control text generation, research has introduced a weighted additive attention mechanism in the decoding process to associate the meaning of the generated text sequence with the meaning of the keyword set. Based on this, a text generation model for generating a translation related corpus in English teaching is constructed. This results in a text generation model that can be used to generate a translation-linked corpus for English language teaching. The average BLEU value of model 1 is 0.924, Inform value is 98.40, the Success value is 97.64, and the Combine value is 101.24, which can achieve high-quality text generation by the keyword lexical meaning and provide technical guarantee for the establishment of the corpus in educating in English.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4375865504",
    "type": "article"
  },
  {
    "title": "Sentiment Analysis and Corpus: Cognitive Perspective and Overhead-accuracy Tradeoff",
    "doi": "https://doi.org/10.1145/3594537",
    "publication_date": "2023-05-26",
    "publication_year": 2023,
    "authors": "Jie Wang; Jiangjun Yuan; Weinan Liu",
    "corresponding_authors": "",
    "abstract": "Human logical thinking is in the form of natural language. With the development of computer science techniques, it becomes easier and more convenient for natural language processing. Therefore, various natural language processing applications have emerged. Sentiment analysis is one of these novel applications and has been applied in many areas. In Amazon.com, there are a large number of user comments and product discussions, which can help a person to decide whether to buy a product or not without asking the opinions from friends and family members. Therefore, sentiment analysis on user comments and product discussions, such as Amazon reviews, becomes increasingly useful and important. In this paper, the effect of corpus on sentiment analysis of the Amazon review dataset with the aid of support vector machine is studied. We generate eight different size datasets from the Amazon review dataset filtered by different word frequency in Corpus of Contemporary American and conduct some experiments on these eight datasets. According to the experimental result, we make some conclusions and give some suggestions to facilitate researchers to make a trade-off between accuracy and experimental cost.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4378376523",
    "type": "article"
  },
  {
    "title": "Collaborative Optimization of English Online Teaching Informatization Based on Intelligent Multimedia Image Technology",
    "doi": "https://doi.org/10.1145/3599725",
    "publication_date": "2023-05-26",
    "publication_year": 2023,
    "authors": "Haihua Tu",
    "corresponding_authors": "Haihua Tu",
    "abstract": "With the rapid development of computer technology and the deepening of educational concepts, the traditional English teaching methods have also changed. As a new teaching method, online teaching has received extensive attention. Online teaching has introduced intelligent multimedia image technology in English teaching, which has transformed the essence of English teaching by organically integrating traditional teaching and multimedia teaching. With the rapid growth of Internet and intelligent multimedia image technology, the number of digital images is growing exponentially. How to extract the required image from the large amount of multimedia image data has been an important topic at present. In this case, intelligent multimedia image technology emerges as the times require, which extracts the features of the image from the content of the image itself, and then retrieves it through the features. This article proposed a retrieval and classification method based on intelligent multimedia image technology. Among them, Support Vector Machine (SVM) was used to classify multimedia resources, which had a good effect. The results of this experiment showed that before the experiment, the listening score of the multimedia group was 65.5, and the speaking score was 59.7. After the experiment, the listening score of the multimedia group was 79.8, and the speaking score was 80.6. It can be found that the performance of the multimedia group has been greatly improved after the experiment, which shows that online English teaching based on intelligent multimedia image technology is more conducive to the improvement of students’ academic performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4378376656",
    "type": "article"
  },
  {
    "title": "Computational Technologies for Construction of Business Korean Translation Corpus Based on Association Rules Mining",
    "doi": "https://doi.org/10.1145/3603500",
    "publication_date": "2023-06-06",
    "publication_year": 2023,
    "authors": "Zhen Zhang",
    "corresponding_authors": "Zhen Zhang",
    "abstract": "With the implementation of China's reform and opening up and China's accession to the World Trade Organization, Chinese business Korean translation plays an increasingly important role in international trade activities. Although the study of business Korean has aroused the strong research interest of scholars, its research focus is limited to word selection, grammatical transformation and various specific translation techniques, which is far from systematic. Good business translation should also involve factors such as the translator's role, cross-culture, and the client of translation. However, how to combine corpus with language teaching, make corpus enter the classroom and get practical application in daily language teaching is still in the exploratory stage. Most of the existing translation theories are tailored for literary translation, and business translation is a branch of non-literary translation, so these theories are difficult to guide business translation. Combined with the research results of translation studies and examples in business Korean, this paper discusses the characteristics and translation of business Korean as a special Korean, so as to provide reference for translation or related learners, and also want to attract more scholars to pay attention to business Korean and its translation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4379513939",
    "type": "article"
  },
  {
    "title": "Virtual Sound Image Reconstruction Method for Multi-objective Optimization of Folk Music Based on Evolutionary Algorithm",
    "doi": "https://doi.org/10.1145/3604614",
    "publication_date": "2023-06-20",
    "publication_year": 2023,
    "authors": "Yifang Wang",
    "corresponding_authors": "Yifang Wang",
    "abstract": "At present, the exchanges in various fields such as culture, economy, and politics are becoming more and more close in the world. From the perspective of the relationship between national music and cultural development, the development of national music has also received more and more attention, and it has become an inevitable trend in the development of today's era. The purpose of this paper was to perform virtual reconstruction of the sound image of folk music through multiple objective optimizations, and to model the virtual sound image of folk music as a multi-objective optimization problem. According to the research on sound image positioning, the relevant noise factors were removed, so as to achieve the playback of ethnic music that enhanced the surround effect and visual enjoyment. The evolutionary algorithm in this paper was mainly based on the multi-objective optimized sound image localization technology. For the music virtual sound image, an improved FCM algorithm and an evolutionary multi-objective optimization algorithm combining local and non-local information of the sound image were proposed, respectively. Through the analysis of the traditional sound image algorithm method, the accuracy of the sound image localization on the horizontal plane could be effectively improved. After the conversion, the audience could feel the better stereo image and surround feeling of the folk music. Through experimental analysis, it can be seen that the system can not only perform virtual conversion of audio-visual signals of different frequencies, but also provide data for different audio playback systems. The feature point registration error is low, and the reconstruction effect is good, especially the random delay processing in the range of 0∼20m, and the performance is better than the traditional method. Finally, virtual left and right surround sound image signals were obtained, which effectively improved the three-dimensional surround feeling of folk music.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4381334423",
    "type": "article"
  },
  {
    "title": "Multi-modal Chinese Text Emotion Metaphor Computation Based on Mutual Information and Information Entropy",
    "doi": "https://doi.org/10.1145/3605211",
    "publication_date": "2023-06-23",
    "publication_year": 2023,
    "authors": "Zhifa Zeng; Yuhang Li",
    "corresponding_authors": "",
    "abstract": "Metaphor is to express another thing through one thing, it is not only a rhetorical means, but also embodies a kind of analogical cognition and way of thinking of people. In recent years, metaphors have made more and more progress in Chinese language recognition, so their status has gradually risen. Chinese language sentiment analysis is a relatively difficult problem in language processing. Chinese language semantics is ambiguous and complex, so it is a great challenge to construct Chinese Emotion structure. In this paper, Chinese nouns and Chinese verbs are used as test samples to conduct metaphor test calculation. The test results of mutual information and information entropy show that for the sample metaphor calculation, the correct rate of individual words is less than 70%. The ratio between the macro-average of gerund words and the baseline fluctuates relatively large. The metaphor recognition of nouns is about 20% higher than the original test results and about 10% higher for verbs. It shows that under the action of mutual information and information entropy, the metaphor recognition performance of the basic structure of Chinese Emotion constructed from the perspective of metaphor is improved, and further illustrates the feasibility of mutual information and information entropy algorithm for building the basic structure.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4381740977",
    "type": "article"
  },
  {
    "title": "Athlete Training Sensor Data Detection Research Based on Optimized Convolutional Neural Network",
    "doi": "https://doi.org/10.1145/3605890",
    "publication_date": "2023-06-26",
    "publication_year": 2023,
    "authors": "Qiang Qian; Yuanyuan Gao; Xuefeng Sun; Chao Qi; Yutao Yuan",
    "corresponding_authors": "",
    "abstract": "The traditional convolutional neural network has matured in image detection and recognition, but maintaining its high accuracy requires a large memory for computation, which increases the volume of sensors and puts additional strain on athletes. Therefore, the research uses local error calculation to replace global error, and uses layer-by-layer residual unit structure to replace gradient calculation, to apply it to athlete training sensor data detection. Through the simulation experiment and the example experiment of the data set, the optimal residual unit structure and the local error joint weight of the residual network human motion detection are obtained. At the same time, in the simulation experiment and example experiment analysis of the school physical education curriculum, the accuracy of the algorithm model proposed in the study is higher than the first five CNN network structures, and its training time is the shortest. The experiment demonstrated that the local error method combined with the residual network structure had improved precise performance in motion detection, and the computational load was low, making it appropriate for wearable sensors during training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4382024102",
    "type": "article"
  },
  {
    "title": "A Study on Corpus-based Stopword Lists in Indian Language IR",
    "doi": "https://doi.org/10.1145/3606262",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Siba Sankar Sahu; Sukomal Pal",
    "corresponding_authors": "",
    "abstract": "We explore and evaluate the effect of different stopword lists (non-corpus-based and corpus-based) in the information retrieval (IR) tasks with different Indian languages such as Bengali, Marathi, Gujarati, Hindi, and English. The issue was investigated from three viewpoints. Is there any performance difference between non-corpus-based and corpus-based stopword removal in chosen Indian languages? Can corpus-based stopword lists improve performance in Indian languages IR? If yes, to what extent? Among the different corpus-based stopword lists, which stopword list provides the best IR performance? Does the length of a corpus-based stopword list affect the retrieval performance in Indian languages? If yes, to what extent? It was observed that a corpus-based stopword list provides better retrieval performance than a non-corpus-based stopword list in different Indian languages. Among the different corpus-based stopword lists generated and experimented with, Zipf’s law-based stopword list (idf-based one) provides the best retrieval performance in various Indian languages. The aggregation1-based stopword list provides better retrieval than the aggregation2-based list in Indian languages, but in English, the aggregation2-based stopword list performs better than the aggregation1-based list. The best performing idf-based stopword list improves MAP score by 5.43% in Bengali, 1.91% in Marathi, 5.4% in Gujarati, 1.5% in Hindi, and 2.12% in English, respectively, over their baseline counterparts. The probabilistic retrieval models (BM25 and TF-IDF) perform best in different Indian languages. A smaller length of corpus-based stopword lists performs better than a larger length of non-corpus-based stopword lists for all the Indian languages considered. The proposed schemes demonstrate that a stopword list can be heuristically generated in a language-independent statistical method and effectively used for IR tasks with performance comparable, to or even better than non-corpus-based stopword lists.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4383104757",
    "type": "article"
  },
  {
    "title": "Chinese Speech Enhancement and Adaptive Recognition Technology for Complex Language Environments",
    "doi": "https://doi.org/10.1145/3608950",
    "publication_date": "2023-07-12",
    "publication_year": 2023,
    "authors": "Ziqi Gao",
    "corresponding_authors": "Ziqi Gao",
    "abstract": "The development of intelligent technology has also made rapid progress in relevant speech fields. In order to increase the application scenarios of speech recognition systems, the research has improved the traditional Speech enhancement algorithm, namely the Ideal Binary Mask (IBM) algorithm, and combined it with the unimproved IBM algorithm to propose an adaptive IBM algorithm. Based on this algorithm, the research has built a new speech recognition system, The system uses an FIR filter to realize pre-emphasis processing and uses Berouti spectral subtraction to preprocess speech. The Speech enhancement model is built using a deep learning network model. The results showed that the IBM algorithm had the highest score in the Perceptual Evaluation of Speech Quality (PESQ) at 3.5596, followed by the Ideal Ratio Mask (IRM) algorithm at 3.3429. The improvement of the IBM algorithm was feasible when the noise intensity coefficient was greater than 0.008. When the noise intensity coefficient was greater than 0.08, the average score of the improved IBM algorithm was 2.1079, and the average score of the unimproved IBM algorithm was 1.9418. The proposed adaptive IBM algorithm has higher performance in complex speech environments compared to the original system.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4384030349",
    "type": "article"
  },
  {
    "title": "OLFF-Net: Robust Registration of 3D Point Cloud based on Overlapped Local Feature Fusion",
    "doi": "https://doi.org/10.1145/3609332",
    "publication_date": "2023-07-14",
    "publication_year": 2023,
    "authors": "Yanqi Li; Hui Li",
    "corresponding_authors": "",
    "abstract": "Recent advance in high-accuracy sensors has made point cloud become the main data format to characterize the three-dimensional world. Since the sensor can only scan and capture the 3D data within a limited field of view, an alignment algorithm is needed to generate the complete 3D scene. Point cloud registration is the solution for alignment problem that aims to estimate the transformation matrix between two frames of different point cloud sets. In this paper, we propose a neural network called OLFF-Net to achieve robust registration of 3D point clouds based on overlapped local feature fusion, which focuses on extracting rotational-invariant local features while providing enough information to achieve accurate alignment. Extensive experiments on representative datasets indicate that the framework can largely outperform competing methods with an average improvement of 16.82% in the metrics over the compared methods. More importantly, it shows significant generalization capability and can be widely applied to point cloud data with multiple complex structures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4384300934",
    "type": "article"
  },
  {
    "title": "Research on the Turning of Graphic Design Information Communication from Static Presentation to Interactive",
    "doi": "https://doi.org/10.1145/3606863",
    "publication_date": "2023-07-13",
    "publication_year": 2023,
    "authors": "Huiling Zhang",
    "corresponding_authors": "Huiling Zhang",
    "abstract": "Aiming at the problem that the static presentation of graphic design information cannot meet the needs of current designers, a curve fitting algorithm based on inflection point detection is proposed. Combine it with the electronic whiteboard system to build an interactive whiteboard system for railway graphic design, and realize the transformation of graphic design information from static to dynamic in this way. The performance comparison experiment of the curve fitting algorithm proposed in the research shows that the accuracy rate of the algorithm is 88.2%, which is higher than that of similar comparison algorithms. The results show that the proposed curve fitting algorithm has better fitting performance. In the empirical analysis of the interactive electronic whiteboard system of railway plane information, it is found that the happy and excited emotional scores of users using the system are increased by 1.8 and 1.3 points respectively. In addition, the Timeliness and Convenience index scores of the transformed graphic design information transmission mode have increased by 1.7 and 1.3 points respectively. The result shows that the transformation of graphic design information transmission mode into dynamic interactive mode through the curve fitting algorithm can effectively improve its user experience, thus improving the designer's graphic design efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4384383240",
    "type": "article"
  },
  {
    "title": "ICON: A Linguistically-Motivated Large-Scale Benchmark Indonesian Constituency Treebank",
    "doi": "https://doi.org/10.1145/3609798",
    "publication_date": "2023-07-25",
    "publication_year": 2023,
    "authors": "Ee Suan Lim; Wei Qi Leong; Thanh Ngan Nguyen; Wei Ming Kng; William Chandra Tjhi; Dea Adhista; Ayu Purwarianti",
    "corresponding_authors": "",
    "abstract": "Constituency parsing is an important task of informing how words are combined to form sentences. While constituency parsing in English has seen significant progress in the last few years, tools for constituency parsing in Indonesian remain few and far between. In this work, we publish ICON (Indonesian CONstituency treebank), the hitherto largest publicly available manually-annotated benchmark Indonesian constituency treebank with a size of 10,000 sentences and approximately 124,000 constituents and 182,000 tokens, which can support the training of state-of-the-art transformer-based models. As part of the process of building the treebank, we review and revamp the constituent and POS tagsets in use in existing treebanks to ensure that the labels are relevant and suitable for the grammatical features of Indonesian. We establish strong baselines on the ICON dataset using the Berkeley Neural Parser with transformer-based pre-trained embeddings, with the best performance of 88.85% F1 score coming from our own version of SpanBERT (IndoSpanBERT). We further analyze the predictions made by our best-performing model to reveal certain idiosyncrasies in Indonesian that pose challenges for constituency parsing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385239309",
    "type": "article"
  },
  {
    "title": "Research on Product Advertising Design Combining Feature Extraction Technology and Web3D Technology",
    "doi": "https://doi.org/10.1145/3608948",
    "publication_date": "2023-07-19",
    "publication_year": 2023,
    "authors": "Zhengwu Zhong",
    "corresponding_authors": "Zhengwu Zhong",
    "abstract": "This work, built on the Unity3D development platform, presents a way for merging feature extraction technology and Web3D technology into advertising design to effectively address the issues of poor efficiency and distortion in the field. Using the candidate text layout generating technique of visual salience, we first build the vector function set based on the three main colors, then we produce the visual communication partition model of advertising design. Next, the number of feature parameters of the shape advertising design is obtained via the establishment of coding coefficient constraint features and the use of an upgraded neural network technique to extract local feature parameter information about the product. Finally, the product design model is brought to life using Web3D technology to boost advertising design's productivity and accuracy. The experiments show that this method not only results in a high rate of correct product identification but also offers a fresh viewpoint on the visual communication of product advertising design by merging the two disciplines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385611023",
    "type": "article"
  },
  {
    "title": "Low Resource Language Analysis Using Deep Learning Algorithm for Gender Classification",
    "doi": "https://doi.org/10.1145/3614427",
    "publication_date": "2023-08-09",
    "publication_year": 2023,
    "authors": "Abhishek Singhal; Devendra Kumar Sharma",
    "corresponding_authors": "",
    "abstract": "Voice signals are the essential input source for applications based on human and computer interaction technology. Gender identification through voice signals is one of the most challenging tasks. For voice signal based analysis, deep learning algorithms provide an alternative to traditional and conventional algorithms for classification. To identify the gender through voice signals of female, male and ‘first-time’ transgender, the deep learning algorithm is used to improve the robustness of the identification model with the Mel Frequency Cepstrum Coefficients (MFCC) as a feature of the voice signals. This article presents the identification accuracy of gender with the help of recorded live voice signals. The voice samples of the third gender are recorded in the Hindi language. These Hindi language voice samples of transgender are very low resources and are unavailable at any recognized sources. The simulation results do not depend on the duration of the signals and are text independent. The recurrent neural network – Bidirectional Long Short-term Memory (RNN – BiLSTM) algorithm has been simulated on the recorded voice signals. The simulation outcome is compared with the earlier reported results in the literature. The gender-wise average accuracy of the proposed model is achieved as 91.44%, 94.94%, and 96.11% for males, females, and transgender, respectively, using voice signals. The identification accuracy of transgender is high in comparison to other genders. On the other hand, the average accuracy of the proposed model is obtained as 94.16%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385696377",
    "type": "article"
  },
  {
    "title": "Improving Generative Adversarial Network-based Vocoding through Multi-scale Convolution",
    "doi": "https://doi.org/10.1145/3610532",
    "publication_date": "2023-08-16",
    "publication_year": 2023,
    "authors": "Wanting Li; Yi-Ting Chen; Buzhou Tang",
    "corresponding_authors": "",
    "abstract": "Vocoding is a sub-process of text-to-speech task, which aims at generating audios from intermediate representations between text and audio. Several recent works have shown that generative adversarial network– (GAN) based vocoders can generate audios with high quality. While GAN-based neural vocoders have shown higher efficiency in generating speed than autoregressive vocoders, the audio fidelity still cannot compete with ground-truth samples. One major cause of the degradation in audio quality and spectrogram vague comes from the average pooling layers in discriminator. As the multi-scale discriminator commonly used by recent GAN-based vocoders applies several average pooling layers to capture different-frequency bands, we believe it is crucial to prevent the high-frequency information from leakage in the average pooling process. This article proposes MSCGAN, which solves the above-mentioned problem and achieves higher-fidelity speech synthesis. We demonstrate that substituting the average pooling process with a multi-scale convolution architecture effectively retains high-frequency features and thus forces the generator to recover audio details in time and frequency domain. Compared with other state-of-the-art GAN-based vocoders, MSCGAN can produce competitive audio with a higher spectrogram clarity and mean opinion score score in subjective human evaluation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385876966",
    "type": "article"
  },
  {
    "title": "Deep Neural Network-based Mixed Speech Recognition Technology for Chinese and English",
    "doi": "https://doi.org/10.1145/3616373",
    "publication_date": "2023-08-17",
    "publication_year": 2023,
    "authors": "Lei Han",
    "corresponding_authors": "Lei Han",
    "abstract": "In the field of human-computer interaction, the current more advanced speech recognition systems are all single speech recognition, and it is urgent to adopt new in-depth learning technology to improve the existing speech recognition system. In this context, this research is based on DNN and investigates mixed speech recognition techniques for both Chinese and English. A single speech recognition algorithm based on DNN is first investigated, and then a new hybrid Chinese and English speech recognition model is constructed by fusing the attention mechanism and CTC loss function. In the construction of the hybrid speech recognition model, the end-to-end model and Transformer framework are used to combine the monotonic alignment property of the CTC loss function, which allows complex sound units to be transformed into characters for easy extraction and recognition. The performance of the constructed models was tested on Chinese speech dataset, English speech dataset and mixed Chinese and English speech dataset to determine the recognition accuracy and speed of the models. The results show that the proposed recognition model achieves 81.2% recognition accuracy and 100 recognition speed/minute on the Chinese-English mixed speech dataset, which is much better than the other three models. This study successfully addresses the need for improved speech recognition systems by introducing a novel hybrid model for mixed Chinese-English speech recognition. The experimental results confirm the superiority of the proposed model, achieving high accuracy and rapid recognition speed. The developed model holds promising potential for enhancing human-computer interaction and enabling efficient communication between Chinese and English speakers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385954324",
    "type": "article"
  },
  {
    "title": "Can Same-right-and-different-left Gestures Be Recognized with Only Right-hand Signals?",
    "doi": "https://doi.org/10.1145/3617370",
    "publication_date": "2023-08-25",
    "publication_year": 2023,
    "authors": "Yidan Cao; Qingshan Wang; Qi Wang; Peng Liu",
    "corresponding_authors": "",
    "abstract": "Sign language serves as a bridge between the hearing-impaired and other people. Existing sensor-based approaches tend to only collect data from the dominant hand. Does this signal collection method affect the accuracy of gesture recognition, especially gestures where the dominant hand has the same movement while the non-dominant hand has different movements? The specific gestures are called same-right-and-different-left (SRDL) where the right hand is dominant. This article is the first to propose an SRDL-aware sign language recognition system. First, an SRDL discriminator based on an autoencoder and range classifier is designed to determine whether the gesture is SRDL. Second, an SRDL feature selector based on clustering relationship is presented. Multivariate variational mode decomposition and fast fourier transform are used to obtain the feature expression. Moreover, a clustering relationship algorithm is proposed to dynamically select features for every group of SRDL gestures in the feature expression. Finally, the experimental results show that the average word error rate is 14.3% and decreases by 8.5% and 12.1% compared with Signspeaker and MyoSign, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386162193",
    "type": "article"
  },
  {
    "title": "A Comparative Study on Selecting Acoustic Modeling Units for WFST-based Mongolian Speech Recognition",
    "doi": "https://doi.org/10.1145/3617830",
    "publication_date": "2023-08-29",
    "publication_year": 2023,
    "authors": "Yonghe Wang; Feilong Bao; Gaunglai Gao",
    "corresponding_authors": "",
    "abstract": "Traditional weighted finite-state transducer– (WFST) based Mongolian automatic speech recognition (ASR) systems use phonemes as pronunciation lexicon modeling units. However, Mongolian is an agglutinative, low-resource language, and building an ASR system based on the phoneme pronunciation lexicon remains a challenge for various reasons. First, the phoneme pronunciation lexicon manually constructed by Mongolian linguists is finite, which is usually used to build a grapheme-to-phoneme conversion (G2P) model to frequently expand new words. However, the data sparsity decreases the robustness of the G2P model and affects the performance of the final ASR system. Second, homophones and polysyllabic words are common in Mongolian, which has a certain impact on the construction of the Mongolian acoustic model. To address these problems, in this work, we first propose a grapheme-to-phoneme alignment model to obtain the mapping relationship between phonemes and subword units. Then, we construct an acoustic subword segmentation set to segment words directly instead of using the traditional G2P method to predict phoneme sequences to expand the pronunciation lexicon. Further, by analyzing the Mongolian encoding form, we also propose an acoustic subword modeling units construction method that removes control characters. Finally, we investigate various acoustic subword modeling units for pronunciation lexicon construction for the Mongolian ASR system. Experiments on a Mongolian dataset with 325 hours of training show that the pronunciation lexicon based on the acoustic subword modeling unit can effectively construct the WFST-based Mongolian ASR system. Further, removing the control characters when building the acoustic subword modeling unit can further improve the ASR system performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386245989",
    "type": "article"
  },
  {
    "title": "Research on the Diversification of Language Ability in Applied Linguistics and Foreign Linguistics Based on New Media",
    "doi": "https://doi.org/10.1145/3617993",
    "publication_date": "2023-09-04",
    "publication_year": 2023,
    "authors": "Jin Xu",
    "corresponding_authors": "Jin Xu",
    "abstract": "In order to improve the research effect of the diversification of language ability in applied linguistics and foreign linguistics, this paper conducts research on the diversification of language ability in applied linguistics and foreign linguistics based on new media and intelligent data processing technology. In the process of language learning data processing, combining the adjacency degree and the network structure entropy, this paper proposes the adjacency structure entropy based on the super network to identify the key nodes in the super network. Moreover, this paper applies this indicator to the competitive hypernetwork, and accurately obtains the key nodes of the hypernetwork. In addition, this paper constructs a scientific research cooperation hypernetwork, and applies the adjacency structure entropy in the hypernetwork to this dataset. From the experimental research, it can be seen that the algorithm proposed in this paper can play a certain role in the diversification analysis of linguistic ability in linguistics and foreign linguistics. At the same time, the model proposed in this paper has a certain effect on the diversification of language ability in applied linguistics and foreign linguistics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386421900",
    "type": "article"
  },
  {
    "title": "Evaluation on Network Social Media Named Entity Recognition Model Based on Active Learning",
    "doi": "https://doi.org/10.1145/3600055",
    "publication_date": "2023-09-05",
    "publication_year": 2023,
    "authors": "Guijiao He; Yunfeng Zhou; Yaodong Zheng",
    "corresponding_authors": "",
    "abstract": "The medical security privacy and named entity recognition (NER) technology under the blockchain technology has been a hot topic in all walks of life. As a typical representative of medical security risks and NER, the NER model of online social media based on active learning has attracted worldwide attention. NER is an important part of natural language processing. Traditional recognition technology usually requires a lot of external information, and through the manual identification of its features, which costs a lot of time and energy. In order to solve the shortcomings of traditional recognition algorithms and the lack of feature extraction in network media NER, a new active learning model was introduced in this paper. In the information age, people are increasingly demanding a large amount of text information, and NER technology came into being. Its main function is to accurately identify important information from text and provide useful information for high-level work. The initial design of NER system is mainly based on the recognition of rules, so as to realize the recognition of named entities. However, in a complex network environment, it takes a lot of time and energy to establish rules without conflicts, and it has poor mobility. In recent years, with the continuous development of computer technology, the use of machine learning to actively learn the unknown information in the target area reduces the workload of manual annotation, thus realizing the active learning of large amounts of data. The research showed that the recognition accuracy under the traditional NER was low, and the information processing speed was slow; the accuracy rate of NER based on active learning was as high as 97%, and the speed of information processing had also been greatly improved, which had solved many problems under the traditional mode. User satisfaction could be as high as 95%, which showed that the latter had broad prospects. The progress of the new era cannot be separated from the support of new technologies. The research of this article has important guiding significance for medical security privacy and the application of NER under blockchain technology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386443034",
    "type": "article"
  },
  {
    "title": "Chinese Event Discourse Deixis Resolution: Design of the Dataset and Model",
    "doi": "https://doi.org/10.1145/3618109",
    "publication_date": "2023-09-06",
    "publication_year": 2023,
    "authors": "Dongdong Xie; Fei Li; Bobo Li; Chong Teng; Donghong Ji; Meishan Zhang",
    "corresponding_authors": "",
    "abstract": "Anaphora resolution is a traditional task in the natural language processing community, defined as a cohesion phenomenon where one entity points back to a previous entity. Event discourse deixis (EDD) is a kind of more complex anaphora in which the anaphors refer to event descriptions such as sentences or clauses. Event discourse deixis resolution (EDDR) is able to help machines understand the richer linguistic and semantic information in the discourse. However, compared to anaphora resolution, EDDR has received relatively less research attention. In this work, we investigate the EDDR task by designing the corresponding dataset and model. First, we manually construct a high-quality Chinese corpus for EDDR, including 4,417 documents and 5,929 event chains that consist of event antecedents and anaphors. Second, we propose a deep neural network model for EDDR, which formulates the task into two subtasks, namely event anaphor recognition and event antecedent recognition. Our model is trained under the two subtasks jointly so that the EDDR task can be performed end to end. Besides our final model, we also build seven pipeline and joint models as baselines to build comprehensive benchmarks for follow-up research. Experimental results on our EDDR dataset show that our model outperforms all the baselines and achieves about 53%, 44%, 53%, and 63% F1s using standard anaphora resolution metrics such as CoNLL, MUC, B3, and Ceafe. The performances show that EDDR is a challenging task and worth researching in the future. Our dataset and model will be released to facilitate follow-up research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386477220",
    "type": "article"
  },
  {
    "title": "Construction of Practical Teaching Mode of Law Course Based on Multi-Mode and Low-Resource Language Learning",
    "doi": "https://doi.org/10.1145/3622938",
    "publication_date": "2023-09-07",
    "publication_year": 2023,
    "authors": "Jing Han",
    "corresponding_authors": "Jing Han",
    "abstract": "China's legal profession is a new profession emerging in the process of China's rule of law. Its development and refinement are increasing in accordance with the rapid social, economic and political development. However, as a lawyer, people need to learn real kung fu. For complex and changeable legal practice problems, people should use the simplest and most effective way to solve them. Therefore, it is necessary to study the curriculum of law specialty to achieve its purpose. It is also necessary to reform the traditional education methods, introduce modern scientific and technological means, and improve their effectiveness, so as to construct a practical teaching mode of Professional Courses in Law (PCL). For practical legal courses, it can be conducted through multi-mode low-resource language learning method. This paper aimed to study how to study the ability of students to obtain information, process information and analyze and process information based on intelligent multi-image feature fusion. Based on the investigation of teaching objectives and teachers' teaching requirements, and the analysis and arrangement of the investigation results, a multi-task learning system model based on image intelligent features was established. In this experiment, 494 valid questionnaires were used to analyze the current situation of PCL practical teaching. Among them, 10.32% and 65.59% of the students were very satisfied with the results of PCL practical teaching, and 18.02% and 59.31% of the students liked PCL practical teaching very much. In the survey on the influencing factors of PCL practical teaching activities, 61.34% of the students thought that the students did not know enough and were not enthusiastic enough. On the whole, the students are satisfied with the practical teaching arrangement of PCL, but from the actual situation, the effect of practical teaching is not very good, and the expected goal has not been achieved.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386527925",
    "type": "article"
  },
  {
    "title": "Semi-Automatic Building and Learning of a Multilingual Ontology",
    "doi": "https://doi.org/10.1145/3615864",
    "publication_date": "2023-09-11",
    "publication_year": 2023,
    "authors": "Fatma Ben Mesmia; Malek Mouhoub",
    "corresponding_authors": "",
    "abstract": "Most online platforms, applications, and Websites use a massive amount of heterogeneous evolving data. These data must be structured and normalized before integration to improve the search and increase the relevance of results. An ontology can address this critical task by efficiently managing data and providing structured formats through techniques such as the Web Ontology Language (OWL). However, building an ontology can be costly, primarily if conducted manually. In this context, we propose a new methodology for automatically building and learning a multilingual ontology using Arabic as the base language via a corpus collected from Wikipedia. Our proposed methodology relies on Finite-state transducers (FSTs). FSTs are regrouped into a cascade to reduce errors and minimize ambiguity. The produced ontology is extended to English and French and independent language images via a translator we developed using APIs. The rationale for starting with the Arabic corpus to extract terms is that entity linking is more convenient from Arabic to other languages. In addition, many Wikipedia articles in English and French (for instance) do not have associated Arabic articles, but the opposite is true. In addition, dealing with Arabic terms permits us to enrich the Arabic module of the free linguistic platform we use in dictionaries and graphs. To assess the efficiency of our proposed methodology, we conducted performance metrics. The reported results are encouraging and promising.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386607605",
    "type": "article"
  },
  {
    "title": "Study on Logistic Service Management of Colleges and Universities Based on Data Mining Algorithms",
    "doi": "https://doi.org/10.1145/3590961",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Zhicheng Zhang",
    "corresponding_authors": "Zhicheng Zhang",
    "abstract": "Construction of a large logistics service (LS) that can adapt to the new situation is necessary for improving the self-development capability of university logistics in the reform process of socialization, and the measures are as follows: with support from the government sector, to create an external environment; with resource integration as a goal, to create an organizational structure; with market mechanism as a promoter, to the Independent college is a significant innovation of the higher education system, whose method of operation achieves the partnership between resources and social forces in higher education. There are several references in the text for further logistic reform in universities via data mining (DM) algorithms concerning logistic entities and autonomous colleges, which examine the market features and interaction between them. The logistics service data mining (LS-DM) approach plays a critical role in advancing logistic management science while boosting the economy's overall benefits when used with other measures. As a result of the rapid popularization of higher education, new features and models place an even greater demand on logistics management in colleges and universities. Refined management must be advocated and implemented in the new scenario. To apply refined management, you must alter your management philosophy, fine-tune your rules and regulations, enhance performance capabilities, and put mechanisms for monitoring and assessing progress. As a result, logistics management can be continuously improved, students and teachers receive better and more gratifying services, and the scientific growth of colleges and universities may be laid solidly.. The proposed LS-DM system with logistics service, data mining, and machine learning model demonstrates simulation outcomes with an accuracy of 89.7% and a precision of 87.8%, which is greater than the accuracy and precision exhibited by the existing models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386710347",
    "type": "article"
  },
  {
    "title": "Research on Recognition Method of Social Robot Based on T-A-GCNIIT in the Metaverse",
    "doi": "https://doi.org/10.1145/3624014",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Huaben Wang; J. Tang",
    "corresponding_authors": "",
    "abstract": "Social robots are used in intelligent customer service, intelligent chat, intelligent shopping guides, and more because of emotion recognition studies in cognitive psychology. However, determining the user's purpose quickly and precisely has proved challenging. Domestic researchers proposed the A-GCNII model to address missing feature information; however, it needs a lot of math. This research offers a social robot recognition approach using the T-A-GCNIIT model and cognitive psychology to optimize computing complexity and performance. The T-A-GCNIIT algorithm processes social network data, and the Viola–Jones algorithm improves social robot intelligence to represent social robots in the meta-universe. The model performs well in node classification, link prediction, community discovery, and other tasks, with enhanced accuracy, recall, F1 score value, and other metrics. The model can also better comprehend the user's emotional state using cognitive psychology to better recognize their purpose and propose a fresh notion for enhancing social robots' cognitive psychology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386889808",
    "type": "article"
  },
  {
    "title": "Handwritten Odia Digit Recognition using Learning Systems: A Comparison of Neural Networks and Support Vector Machine Models",
    "doi": "https://doi.org/10.1145/3626524",
    "publication_date": "2023-10-09",
    "publication_year": 2023,
    "authors": "U.C. Sharma; Rajat Bansal; Pradeepta Kumar Sarangi; Deepali Gupta; Shalli Rani; Fazlullah Khan; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "The Odia language is one of the many regional languages spoken in India. It is the official language of Odisha, a State in eastern India. The Odia language carries a 1500-year-old history and worldwide is spoken by more than 50 million people. The Odia digits are complex due to the presence of many curves in each character. Handwritten scripts are even more complex due to free-style writing. However, the development of an innovative machine learning model is essential because Odia scripts consist of a huge number of historical documents of more than 1000 years old. A robust automation method will help in converting historical documents into digital form and will help to preserve the documents. This will solve a big problem in society. This work experiments with handwritten Odia numerals by implementing two different classifiers. The first one is the implementation of a Convolutional Neural Network (CNN) and the second experiment implements a Support Vector Machine (SVM). Finally, results from both experiments have been compared. The dataset has been generated through software by writing the digits on MS Paint. Both CNN and SVM models have been implemented through Python programming to recognize the inputs into a particular class. Both training and testing of the models have been done using this dataset. The accuracy from the CNN Model is obtained to be 94.999% which is ≈95% and for SVM, the model accuracy is 86%. Comparing both results, it is concluded that the CNN model is comparatively better than the SVM classifier in the case of the proposed work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387460482",
    "type": "article"
  },
  {
    "title": "Binary Semantic Pattern Rules for Chinese-English Machine Translation Based on Machine Learning Algorithms",
    "doi": "https://doi.org/10.1145/3626095",
    "publication_date": "2023-10-10",
    "publication_year": 2023,
    "authors": "Juan Diego Zamudio Padilla; Liuqin Wang",
    "corresponding_authors": "",
    "abstract": "With the increase of internationalization and the exponential growth of intercultural communication, the importance of interlingual translation has become increasingly prominent. Machine translation has been a booming area of research as technology has advanced. Due to the complexity of language ability and limited understanding of language laws, there are challenges for machine translation. This paper focused on how to construct and apply binary semantic pattern rules through machine learning to improve the translation effect in Chinese-English machine translation. The research results of this paper would contribute to the further development and improvement of Chinese-English machine translation technology. In order to produce high-quality translation results, research in machine translation has recognized the need to analyze and understand the semantics of natural language. To address the important issue of lexical and syntactic ambiguity, representations of binary semantic pattern rules have been developed to formally describe these rules. Based on this, this paper designed and implemented a corpus-based binary semantic rule extraction and optimization algorithm, which used machine learning algorithms to automatically detect the semantic rules of two or more than two phrases in the Chinese corpus, and then automatically optimized and converted them according to the statistical results, and realized the design of Chinese-English machine translation system. The article evaluated the quality of machine translation to test the effectiveness of machine translation binary semantic pattern rules based on machine learning algorithms. The study found that compared with the rule set A, the rule sets B and C obtained automatically by the rule mining algorithm had significantly improved accuracy, both reaching more than 90%. This showed that the binary semantic pattern rule mining algorithm and optimization algorithm proposed in this paper were reasonable.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387496112",
    "type": "article"
  },
  {
    "title": "The Development and Teaching Application of Japanese Peripheral Language Phenomenon Based on Big Data Corpus",
    "doi": "https://doi.org/10.1145/3626525",
    "publication_date": "2023-10-13",
    "publication_year": 2023,
    "authors": "Wei Yi",
    "corresponding_authors": "Wei Yi",
    "abstract": "Since the introduction of the concept of big data corpus into language analysis, many scholars have conducted research on Japanese big data corpus, especially the subject, from different language views and perspectives, and have drawn many valuable conclusions. However, due to scholars' different theories and different thinking modes, their understanding of the Japanese big data corpus is still quite different. The seminar teaching method, as a teaching mode for the cultivation of innovative talents in Japanese peripheral languages, has distinctive features such as interactivity, democracy, motivation, and extension. It has been widely used in postgraduate and undergraduate courses at home and abroad and applied to A wide range of disciplines. Especially in Japan, each university has Seminar courses. Because of its strong practicality, it is conducive to cultivating students' independent learning ability, organization and coordination ability, and communication ability, so whether it can be applied and promoted in the teaching of Japanese peripheral languages, this research is carried out. Although experts have always been committed to the innovation of Japanese teaching, the advancement of teaching materials, and the continuous innovation of design, they also need the teaching practice and experimentation of front-line teachers. After years of practice in basic Japanese teaching by generations of teachers, we still encounter this problem: students cannot use the language fluently, they can only memorize words, grammar, and the usage of some fixed terms by rote; Some students with strong writing ability are not strong in conversation and expression. According to these problems in teaching, the author found that students could not use language well, probably because they did not have the awareness and desire to communicate. Sometimes the single teaching method and the boring teaching content make students uninterested in learning Japanese, and the large number of pseudonyms, Chinese characters, and complex grammatical structures make students dazzled. The experimental results show that the new teaching method proposed by the experimental model is suitable for the learning of Japanese peripheral languages, which improves the student's learning and thinking ability, which proves that the optimized NN-SLVM model is good.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387614610",
    "type": "article"
  },
  {
    "title": "Semantic Template-based Convolutional Neural Network for Text Classification",
    "doi": "https://doi.org/10.1145/3627820",
    "publication_date": "2023-10-16",
    "publication_year": 2023,
    "authors": "Yung‐Chun Chang; Siu Hin Ng; Jung-Peng Chen; Yu-Chi Liang; Wen−Lian Hsu",
    "corresponding_authors": "",
    "abstract": "We propose a semantic template-based distributed representation for the convolutional neural network called Semantic Template-based Convolutional Neural Network (STCNN) for text categorization that imitates the perceptual behavior of human comprehension. STCNN is a highly automatic approach that learns semantic templates that characterize a domain from raw text and recognizes categories of documents using a semantic-infused convolutional neural network that allows a template to be partially matched through a statistical scoring system. Our experiment results show that STCNN effectively classifies documents in about 140,000 Chinese news articles into predefined categories by capturing the most prominent and expressive patterns and achieves the best performance among all compared methods for Chinese topic classification. Finally, the same knowledge can be directly used to perform a semantic analysis task.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387664602",
    "type": "article"
  },
  {
    "title": "DESPP-DETR: A Dense Connection Efficient Spatial Pooling DEtection TRansformer for Vehicle Detection",
    "doi": "https://doi.org/10.1145/3628426",
    "publication_date": "2023-10-18",
    "publication_year": 2023,
    "authors": "S. P. Krishnendhu; Prabu Mohandas",
    "corresponding_authors": "",
    "abstract": "Real-time vehicle detection is a challenging and vital task in intelligent transportation systems. The key requirements for a vehicle detection model are speed and accuracy. However, existing real-time vehicle detection models often sacrifice one of these qualities in favor of the other. This trade-off makes them unfit for real-time deployment, where both speed and accuracy are equally important. Additionally, occlusion, which refers to the obstruction or partial covering of vehicles, further complicates detection and affects the system’s accuracy. In this study, we propose DESPP-DETR, a one-stage detection network for real-time vehicle detection. It is based on bipartite matching and a transformer encoder-decoder architecture, with the addition of a dense connection block and enhanced spatial pyramid pooling. The presence of dense connection block strengthens feature extraction. The enhanced spatial pyramid pooling eliminates the fixed-size constraint and increases the network’s learning capacity. When compared to existing models, DESPP-DETR achieves greater accuracy in real-time vehicle detection. On the MS COCO 2017 dataset, the proposed model achieves an improved mean average precision (mAP) of 75.53%, making it a promising solution for intelligent transportation systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387735339",
    "type": "article"
  },
  {
    "title": "Fine-Grained Domain Adaptation for Chinese Syntactic Processing",
    "doi": "https://doi.org/10.1145/3629519",
    "publication_date": "2023-10-20",
    "publication_year": 2023,
    "authors": "Meishan Zhang; Peiming Guo; Peijie Jiang; Dingkun Long; Yueheng Sun; Guangwei Xu; Pengjun Xie; Min Zhang",
    "corresponding_authors": "",
    "abstract": "Syntactic processing is fundamental to natural language processing. It provides rich and comprehensive syntax information in sentences that could be potentially beneficial for downstream tasks. Recently, pretrained language models have shown great success in Chinese syntactic processing, which typically involves word segmentation, POS tagging, and dependency parsing. However, the on-going research never ends since performance would be degraded drastically when tested on a highly-discrepant domain. This problem is widely accepted as domain adaptation, where the test domain differs from the training domain in supervised learning. Self-training is one promising solution for it, and straightforward source-to-target adaptation has already shown remarkable effectiveness in previous work. While this strategy ignores the fact that sentences of the target domain sentences may have very different gaps from the source training domain. More specifically, sentences with large gaps might fail by direct self-training adaptation. To this end, we propose fine-grained domain adaptation for Chinese syntactic processing in this work, aiming to model the gaps between the source and the target domains accurately and progressively. The key idea is to divide the target domain into fine-grained subdomains by using a specified domain distance metric, and then perform gradual self-training on the subdomains. We further offer an intuitive theoretical illustration based on the theory of Kumar et al. (2020) approximately. In addition, a novel representation learning framework is proposed to encode fine-grained subdomains effectively, aiming to utilize the above idea fully. Experimental results on benchmark datasets show that our method can achieve significant improvements over a variety of baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387823821",
    "type": "article"
  },
  {
    "title": "The Application Research of Distributed Interface Coordination Based on Deep Reinforcement Learning in English MOOC Platform",
    "doi": "https://doi.org/10.1145/3614428",
    "publication_date": "2023-10-25",
    "publication_year": 2023,
    "authors": "Xueyan Ma",
    "corresponding_authors": "Xueyan Ma",
    "abstract": "Developing information technology and other business management firms is not the same as technological advancement. The only way we can declare that complete technical progress is in place and operating well in the nation is if the education system has been built. The educational system is set up to instruct pupils via wireless technologies and online classes. Advancement in online classes has resulted in the Massive Open Online Courses (MOOC) Platform. The advantage of the MOOC platform is that it is a platform that provides free online courses for anyone to register and continue the course at their own pace. On this MOOC platform, implementing Deep Reinforcement Learning (DRL) aids in choosing the course and learning through an intelligent mechanism. The transmission between students and teachers will significantly rise because of this teaching strategy through the Distributed Interface Coordination (DIC) mechanism where the resources are distributed in the wireless networking environment. Additionally, the registered students should possess privileges to access the resources. This combination of DIC with DRL can be termed as DIC-DRL model for the proposed system. The data was collected using a low packet rate and outdated wireless connection technologies. The suggested model has a 97.67 percent accuracy rate in comparison with the existing model's support vector machine (SVM) mechanism.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387933461",
    "type": "article"
  },
  {
    "title": "Mountain Gazelle Optimizer with Deep Learning Driven Satirical News Classification on Low-resource Language Corpus",
    "doi": "https://doi.org/10.1145/3629975",
    "publication_date": "2023-10-28",
    "publication_year": 2023,
    "authors": "Badriyya B. Al-onazi; Faiz Abdullah Alotaibi; Abdulkhaleq Q. A. Hassan; Bhawani Sankar Panigrahi; K. Kartheeban; Tariq Hussain Sheikh; Sonali Vyas; Victor Hugo C. de Albuquerque",
    "corresponding_authors": "",
    "abstract": "The development of satirical and fake news on digital platforms has source of major concern about the spread of misinformation and its control on society. As part of the Arabic language, fake news detection (FND) shows particular problems because of language difficulties and the scarcity of labeled data. FND on Arabic corpus utilizing deep learning (DL) contains leveraging advanced neural network (NN) techniques and methods to automatically recognize and classify deceptive data in the Arabic language text. This procedure is vital in combating the spread of disinformation and misinformation, promoting media literacy, and make sure the credibility of data sources for the Arabic-speaking community. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) are common selections for FND because of their capability for learning hierarchical features and model sequential data from the text. In this view, this study develops a Mountain Gazelle Optimizer with Deep Learning-Driven Fake News Classification on Arabic Corpus (MGODL-FNCAC) technique. The presented MGODL-FNCAC approach aims to increase the performance of the fake news classification on the Arabic corpus. Primarily, the MGODL-FNCAC technique involves different stages of pre-processing to make the input data compatible for classification. For fake news detection, the MGODL-FNCAC technique applies the deep belief network (DBN) model. At last, the MGO approach can be used for the better hyperparameter tuning of the DBN approach, which supports in enhancing the overall training process and detection rate. The simulation outcomes of the MGODL-FNCAC technique can be examined on Arabic corpus data. The extensive outcomes exhibit the importance of the MGODL-FNCAC system over other methodologies with maximum accuracy of 97.68% and 95.14% on Covid19Fakes and Satirical dataset, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388182224",
    "type": "article"
  },
  {
    "title": "Enhancing Asian Indigenous Language Processing through Deep Learning-based Handwriting Recognition and Optimization Techniques",
    "doi": "https://doi.org/10.1145/3632173",
    "publication_date": "2023-11-10",
    "publication_year": 2023,
    "authors": "A. Manimaran; Mohammad Haider Syed; M. Siva Kumar; S. Selvanayaki; Gurram Sunitha; Asmita Manna",
    "corresponding_authors": "",
    "abstract": "Asian indigenous language or autochthonous language is a language which is native to a region and spoken by indigenous people in Asia. This language is a linguistically different community created in the region. Recently, researchers in handwriting detection studies comparing with indigenous languages have attained important internet amongst the research community. A new development of artificial intelligence (AI), natural language processing (NLP), cognitive analytics, and computational linguistics (CL) find it helpful in the analysis of regional low-resource languages. It can be obvious in the obtainability of effectual machine detection methods and open access handwritten databases. Tamil is the most ancient Indian language that is mostly exploited in the Southern part of India, Sri Lanka, and Malaysia. Tamil handwritten Character Recognition (HCR) is a critical procedure in optical character detection. Therefore, this study designs a Henry Gas Solubility Optimization with Deep Learning-based Handwriting Recognition Model (HGSODL-HRM) for Asian Indigenous Language Processing. The proposed HGSODL-HRM technique relies on computer vision and DL concepts for automated handwriting recognition in the Tamil language, which is one of the popular indigenous languages in Asia. To accomplish this, the HGSODL-HRM technique employs a capsule network (CapsNet) model for feature vector generation with the HGSO algorithm as a hyperparameter optimizer. For the recognition of handwritten characters, wavelet neural network (WNN) model is exploited. Finally, the WNN parameters can be optimally chosen by sail fish optimizer (SFO) algorithm. To demonstrate the promising results of the HGSODL-HRM system, an extensive range of simulations can be implemented. The simulation outcomes stated the betterment of the HGSODL-HRM system compared to recent DL models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388575168",
    "type": "article"
  },
  {
    "title": "Machine Learning and Natural Language Processing Algorithms in the Remote Mobile Medical Diagnosis System of Internet Hospitals",
    "doi": "https://doi.org/10.1145/3632172",
    "publication_date": "2023-11-17",
    "publication_year": 2023,
    "authors": "Huang Yong; Lu Zhang",
    "corresponding_authors": "",
    "abstract": "In order to alleviate the contradiction between supply and demand of professional pharmacists, integrate medical resources, and ensure the safety of patients' medication, the telemedicine diagnosis system has played a great role. Today, in this \"Internet +\" era, all walks of life have begun to integrate with Internet technology. The purpose of this paper was to discuss the practical utility of machine learning and natural language processing algorithms in the remote mobile medical diagnosis system of Internet hospitals, for which this paper conducted in-depth discussion. This paper first introduced the basic concepts, development and characteristics of machine learning and natural language processing algorithms in detail, and carefully studied and analyzed the development and culture of traditional offline medical diagnosis models. Based on machine learning and natural language processing algorithms, a remote mobile medical diagnosis is designed. By combining with the medical diagnosis system of traditional hospitals, a new type of remote mobile medical diagnosis system for Internet hospitals was designed and developed, and the combination of traditional medical industry and Internet technology was deeply studied. According to each functional requirement, the image module, heart rate measurement module and user setting module are designed respectively. Compared to traditional medical diagnosis systems, the accuracy of the remote mobile medical diagnosis system based on machine learning applied in internet hospital diagnosis in this article reached 80% or even higher. At the same time, it was found through experiments that when the evolution number was 3, the maximum fit value and average fit value were the same, both of which were 0.6. This indicates that the system can accommodate more than 10000 people at the same time, and patients can receive good treatment plans, with a very broad application prospect.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388763326",
    "type": "article"
  },
  {
    "title": "Named Entity Recognition in Persian Language based on Self-attention Mechanism with Weighted Relational Position Encoding",
    "doi": "https://doi.org/10.1145/3633513",
    "publication_date": "2023-11-22",
    "publication_year": 2023,
    "authors": "Ebrahim Ganjalipour; A. H. Refahi Sheikhani; Sohrab Kordrostami; Ali Hosseinzadeh",
    "corresponding_authors": "",
    "abstract": "Named-entity Recognition (NER) is challenging for languages with low digital resources. The main difficulties arise from the scarcity of annotated corpora and the consequent problematic training of an effective NER Model. We propose a customized model based on linguistic properties to compensate for this lack of resources in low-resource languages like Persian. According to pronoun-dropping and subject-object-verb word order specifications of Persian, we propose new weighted relative positional encoding in the self-attention mechanism. Using the pointwise mutual information factor, we inject co-occurrence information into context representation. We trained and tested our model on three different datasets: Arman, Peyma, and ParsTwiNER, and our method achieved 94.16%, 93.36%, and 84.49% word-level F1 scores, respectively. The experiments showed that our proposed model performs better than other Persian NER models. Ablation Study and Case Study also showed that our method can converge faster and is less prone to overfitting.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388903721",
    "type": "article"
  },
  {
    "title": "A Novel Knowledge Augmented Model Customization Approach for Arabic Offensive Language Detection",
    "doi": "https://doi.org/10.1145/3634702",
    "publication_date": "2023-11-28",
    "publication_year": 2023,
    "authors": "Fatemah Husain",
    "corresponding_authors": "Fatemah Husain",
    "abstract": "Multiple attempts to develop systems for detecting online Arabic offensive language have been explored in previous studies. However, most of these attempts do not consider the variation of Arabic dialects, cultures, and offensive phrases. In contrast, this study aims to extract knowledge from multiple offensive language datasets to build a cross-dialect and culture knowledge-based repository. This knowledge-based repository is utilized to develop novel system architecture based on customizing the AraBERT model in a unique method to preserve dialectal knowledge and offensive cultural knowledge within the contextual word embedding of BERT architecture. Performance evaluation procedures consist of statistical evaluation metrics and a behavioral checklist. Results report more effective predictions by the customized model than the uncustomized one, particularly for offensive text. The customization process allows the model to gain more knowledge of informal text in general, and a better understanding of dialectal Arabic.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389098629",
    "type": "article"
  },
  {
    "title": "A Relation Embedding Assistance Networks for Multi-hop Question Answering",
    "doi": "https://doi.org/10.1145/3635114",
    "publication_date": "2023-12-16",
    "publication_year": 2023,
    "authors": "Songlin Jiao; Zhenfang Zhu; Jiangtao Qi; Fuyong Xu; Hongli Pei; Wen-Ling Wang; Ze Song; Peiyu Liu",
    "corresponding_authors": "",
    "abstract": "Multi-hop Knowledge Graph Question Answering aims at finding an entity to answer natural language questions from knowledge graphs. When humans perform multi-hop reasoning, people tend to focus on specific relations across different hops and confirm the next entity. Therefore, most algorithms choose the wrong specific relation, which makes the system deviate from the correct reasoning path. The specific relation at each hop plays an important role in multi-hop question answering. Existing work mainly relies on the question representation as relation information, which cannot accurately calculate the specific relation distribution. In this article, we propose an interpretable assistance framework that fully utilizes the relation embeddings to assist in calculating relation distributions at each hop. Moreover, we employ the fusion attention mechanism to ensure the integrity of relation information and hence to enrich the relation embeddings. The experimental results on three English datasets and one Chinese dataset demonstrate that our method significantly outperforms all baselines. The source code of REAN will be available at https://github.com/2399240664/REAN",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389829637",
    "type": "article"
  },
  {
    "title": "Investigation of Visual Language Landscape of Tourist Attractions from Multimodal Perspective",
    "doi": "https://doi.org/10.1145/3638049",
    "publication_date": "2023-12-20",
    "publication_year": 2023,
    "authors": "Liwei Fu; Qiang Li",
    "corresponding_authors": "",
    "abstract": "With the development of economic globalization, the tourism industry has been welcomed by the public. The visual language landscape of tourist attractions can not only assist tourists to play and watch the project, but if it is properly planned, the language landscape can also become a major feature and highlight of the scenic spot. Therefore, how to set up and construct the visual language landscape of tourist attractions is a problem that needs to be considered in each region. In response to the above problems, on the basis of understanding the concept types of the visual language landscape of tourist attractions, this article conducts in-depth research and investigation on the visual language landscape of tourist attractions, combining the evaluation dataset in the multimodal perspective and the Convolutional Neural Network (CNN) –Recurrent Neural Network (RNN) model based on semantic regularization. This article conducted a comparative experiment on each model on the NUS-WIDE dataset and the MS-COCO dataset. The experimental results showed that it was crucial to give full play to the expressive power of the CNN. Compared to the NUS-WIDE dataset, the MS-COCO dataset brought less additional boost by leveraging social media tags. The CIDEr score of the CNN-RNN model based on semantic regularization was improved by 11.4%, which placed the foundation for the investigation and analysis of the linguistic landscape of tourist attractions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390003346",
    "type": "article"
  },
  {
    "title": "An Ensemble Strategy with Gradient Conflict for Multi-Domain Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3638248",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Zhibo Man; Yujie Zhang; Li Yu; Yuanmeng Chen; Yufeng Chen; Jinan Xu",
    "corresponding_authors": "",
    "abstract": "Multi-domain neural machine translation aims to construct a unified neural machine translation model to translate sentences across various domains. Nevertheless, previous studies have one limitation is the incapacity to acquire both domain-general and domain-specific representations concurrently. To this end, we propose an ensemble strategy with gradient conflict for multi-domain neural machine translation that automatically learns model parameters by identifying both domain-shared and domain-specific features. Specifically, our approach consists of (1) a parameter-sharing framework, where the parameters of all the layers are originally shared and equivalent to each domain, and (2) ensemble strategy, in which we design an Extra Ensemble strategy via a piecewise condition function to learn direction and distance-based gradient conflict. In addition, we give a detailed theoretical analysis of the gradient conflict to further validate the effectiveness of our approach. Experimental results on two multi-domain datasets show the superior performance of our proposed model compared to previous work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390051525",
    "type": "article"
  },
  {
    "title": "The Computational Method for Supporting Thai VerbNet Construction",
    "doi": "https://doi.org/10.1145/3638533",
    "publication_date": "2023-12-26",
    "publication_year": 2023,
    "authors": "Krittanut Chungnoi; Rachada Kongkachandra; Sarun Gulyanon",
    "corresponding_authors": "",
    "abstract": "VerbNet is a lexical resource for verbs that has many applications in natural language processing tasks, especially ones that require information about both the syntactic behavior and the semantics of verbs. This article presents an attempt to construct the first version of a Thai VerbNet corpus via data enrichment of the existing lexical resource. This corpus contains the annotation at both the syntactic and semantic levels, where verbs are tagged with frames within the verb class hierarchy and their arguments are labeled with the semantic role. We discuss the technical aspect of the construction process of Thai VerbNet and survey different semantic role labeling methods to make this process fully automatic. We also investigate the linguistic aspect of the computed verb classes and the results show the potential in assisting semantic classification and analysis. At the current stage, we have built the verb class hierarchy consisting of 28 verb classes from 112 unique concept frames over 490 unique verbs using our association rule learning method on Thai verbs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390237488",
    "type": "article"
  },
  {
    "title": "Leveraging Dual Gloss Encoders in Chinese Biomedical Entity Linking",
    "doi": "https://doi.org/10.1145/3638555",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Tzu-Mi Lin; Man-Chen Hung; Lung‐Hao Lee",
    "corresponding_authors": "",
    "abstract": "Entity linking is the task of assigning a unique identity to named entities mentioned in a text, a sort of word sense disambiguation that focuses on automatically determining a pre-defined sense for a target entity to be disambiguated. This study proposes the DGE (Dual Gloss Encoders) model for Chinese entity linking in the biomedical domain. We separately model a dual encoder architecture, comprising a context-aware gloss encoder and a lexical gloss encoder, for contextualized embedding representations. DGE are then jointly optimized to assign the nearest gloss with the highest score for target entity disambiguation. The experimental datasets consist of a total of 10,218 sentences that were manually annotated with glosses defined in the BabelNet 5.0 across 40 distinct biomedical entities. Experimental results show that the DGE model achieved an F1-score of 97.81, outperforming other existing methods. A series of model analyses indicate that the proposed approach is effective for Chinese biomedical entity linking.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390317696",
    "type": "article"
  },
  {
    "title": "Expanding Paraphrase Lexicons by Exploiting Generalities",
    "doi": "https://doi.org/10.1145/3160488",
    "publication_date": "2018-01-30",
    "publication_year": 2018,
    "authors": "Atsushi Fujita; Pierre Isabelle",
    "corresponding_authors": "",
    "abstract": "Techniques for generating and recognizing paraphrases, i.e., semantically equivalent expressions, play an important role in a wide range of natural language processing tasks. In the last decade, the task of automatic acquisition of subsentential paraphrases, i.e., words and phrases with (approximately) the same meaning, has been drawing much attention in the research community. The core problem is to obtain paraphrases of high quality in large quantity. This article presents a method for tackling this issue by systematically expanding an initial seed lexicon made up of high-quality paraphrases. This involves automatically capturing morpho-semantic and syntactic generalizations within the lexicon and using them to leverage the power of large-scale monolingual data. Given an input set of paraphrases, our method starts by inducing paraphrase patterns that constitute generalizations over corresponding pairs of lexical variants, such as “amending” and “amendment,” in a fully empirical way. It then searches large-scale monolingual data for new paraphrases matching those patterns. The results of our experiments on English, French, and Japanese demonstrate that our method manages to expand seed lexicons by a large multiple. Human evaluation based on paraphrase substitution tests reveals that the automatically acquired paraphrases are also of high quality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2792210162",
    "type": "article"
  },
  {
    "title": "Multitask Pointer Network for Korean Dependency Parsing",
    "doi": "https://doi.org/10.1145/3282442",
    "publication_date": "2019-02-08",
    "publication_year": 2019,
    "authors": "Sangkeun Jung; Cheoneum Park; Changki Lee",
    "corresponding_authors": "",
    "abstract": "Dependency parsing is a fundamental problem in natural language processing. We introduce a novel dependency-parsing framework called head-pointing--based dependency parsing . In this framework, we cast the Korean dependency parsing problem as a statistical head-pointing and arc-labeling problem. To address this problem, a novel neural network called the multitask pointer network is devised for a neural sequential head-pointing and type-labeling architecture. Our approach does not require any handcrafted features or language-specific rules to parse dependency. Furthermore, it achieves state-of-the-art performance for Korean dependency parsing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2912608215",
    "type": "article"
  },
  {
    "title": "Converting Dependency Structure Into Persian Phrase Structure",
    "doi": "https://doi.org/10.1145/3314937",
    "publication_date": "2019-05-07",
    "publication_year": 2019,
    "authors": "Mohammad Hossein Dehghan; Heshaam Faili",
    "corresponding_authors": "",
    "abstract": "Treebank is one of the important and useful resources in natural language processing represented in two different annotated schemas: phrase and dependency structures. There are many works that convert a phrase structure into a dependency structure and vice versa. Most of them are based that exploit the handcrafted head percolation table and argument table in predefined deterministic ways. In this article, we propose a method to convert a dependency structure into a phrase structure by enriching a trainable model of former hybrid strategy approach. By adding a classifier to the algorithm and using postprocessing modification, the quality of conversion is increased. We evaluate our method in two different languages, English and Persian, and then analyze the errors. The results of our experiments show a 46.01% reduction of error rate in English and 76.50% for Persian compared to our baseline. We build a new phrase structure treebank by converting 10,000 sentences of Persian dependency treebank into corresponding phrase structures and correcting them manually.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2944000677",
    "type": "article"
  },
  {
    "title": "Matching Graph, a Method for Extracting Parallel Information from Comparable Corpora",
    "doi": "https://doi.org/10.1145/3329713",
    "publication_date": "2019-07-25",
    "publication_year": 2019,
    "authors": "Somayeh Bakhshaei; Reza Safabakhsh; Shahram Khadivi",
    "corresponding_authors": "",
    "abstract": "Comparable corpora are valuable alternatives for the expensive parallel corpora. They comprise informative parallel fragments that are useful resources for different natural language processing tasks. In this work, a generative model is proposed for efficient extraction of parallel fragments from a pair of comparable documents. The core of the proposed model is a graph called the Matching Graph. The ability of the Matching Graph to be trained on a small initial seed makes it a proper model for language pairs suffering from the scarce resource problem. Experiments show that the Matching Graph performs significantly better than other recently published models. According to the experiments on English-Persian and Arabic-Persian language pairs, the extracted parallel fragments can be used instead of parallel data for training statistical machine translation systems. Results reveal that the extracted fragments in the best case are able to retrieve about 90% of the information of a statistical machine translation system that is trained on a parallel corpus. Moreover, it is shown that using the extracted fragments as additional information for training statistical machine translation systems leads to an improvement of about 2% for English-Persian and about 1% for Arabic-Persian translation on BLEU score.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2963587420",
    "type": "article"
  },
  {
    "title": "中国語談話注釈のための談話表現の調査【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Kang Xiaomian; Zong Chengqing; Xue Nianwen",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3177333746",
    "type": "article"
  },
  {
    "title": "知的エージェントのための経験ベース因果学習【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Liu Yang; Wang Shaonan; Zhang Jiajun; Zong Chengqing",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3190192423",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3229525",
    "publication_date": "2018-08-20",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Lexicon-based sentiment analysis (SA) aims to address the problem of extracting people’s opinions from their comments on the Web using a predefined lexicon of opinionated words. In contrast to the machine learning (ML) approach, lexicon-based methods ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230221997",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3338846",
    "publication_date": "2019-10-09",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Singlish can be interesting to the computational linguistics community both linguistically, as a major low-resource creole based on English, and computationally, for information extraction and sentiment analysis of regional social media. In our ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232276503",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3292011",
    "publication_date": "2019-01-08",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We investigate the use of word embeddings for query translation to improve precision in cross-language information retrieval (CLIR). Word vectors represent words in a distributional space such that syntactically or semantically similar words are close ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233051924",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3305347",
    "publication_date": "2019-07-24",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Nowadays, social media is used by many people to express their opinions about a variety of topics. Opinion Mining or Sentiment Analysis techniques extract opinions from user generated contents. Over the years, a multitude of Sentiment Analysis studies ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233896156",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3300146",
    "publication_date": "2019-02-10",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Existing supervised solutions for Named Entity Recognition (NER) typically rely on a large annotated corpus. Collecting large amounts of NER annotated corpus is time-consuming and requires considerable human effort. However, collecting small amounts of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239080915",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3184403",
    "publication_date": "2018-05-10",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of learning compositional Chinese sentence representations, which represent the meaning of a sentence by composing the meanings of its constituent words. In contrast to English, a Chinese word is composed of characters,...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244214313",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3327969",
    "publication_date": "2019-08-17",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In computational linguistics, sentiment analysis refers to the classification of opinions in a positive class or a negative class. There exist a lot of different methods for sentiment analysis of the English language, but the literature lacks the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248494278",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3358605",
    "publication_date": "2019-12-12",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "External keywords are crucial for response generation models to address the generic response problems in open-domain conversational systems. The occurrence of keywords in a response depends heavily on the order of the keywords as they are generated ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250128393",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3160862",
    "publication_date": "2018-02-05",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The lack or absence of parallel and comparable corpora makes bilingual lexicon extraction a difficult task for low-resource languages. The pivot language and cognate recognition approaches have been proven useful for inducing bilingual lexicons for such ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250673703",
    "type": "paratext"
  },
  {
    "title": "Outline Extraction with Question-Specific Memory Cells",
    "doi": "https://doi.org/10.1145/3377707",
    "publication_date": "2020-03-29",
    "publication_year": 2020,
    "authors": "Jingxuan Yang; Haotian Cui; Si Li; Sheng Gao; Jun Guo; Zhengdong Lu",
    "corresponding_authors": "",
    "abstract": "Outline extraction has been widely applied in online consultation to help experts quickly understand individual cases. Given a specific case described as unstructured plain text, outline extraction aims to make a summary for this case by answering a set of questions, which in fact is a new type of machine reading comprehension task. Inspired by a recently popular memory network, we propose a novel question-specific memory cell network (QSMCN) to extract information related to multiple questions on-the-fly as it reads texts. QSMCN constructs a specific memory cell for each question, which is sequentially expanded in recurrent neural network style. Each cell contains three specific vectors to first identify whether current input is related to corresponding question and then update question-specific case representation. We add a penalization term in the loss function to make extracted knowledge more reasonable and interpretable. To support this study, we construct a new outline extraction corpus, InjuryCase, 1 which is composed of 3,995 real Chinese occupational injury cases. Experimental results show that our method makes a significant improvement. We further apply the proposed framework on two multi-aspect extraction tasks and find that the proposed model also remarkably outperforms existing state-of-the-art methods of the aspect extraction task.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3015749523",
    "type": "article"
  },
  {
    "title": "Editorial from the New Editor-in-Chief",
    "doi": "https://doi.org/10.1145/3397501",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Imed Zitouni",
    "corresponding_authors": "Imed Zitouni",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3040305107",
    "type": "article"
  },
  {
    "title": "An Analysis for Elements of Affecting the Establishment and Promotion of Micro-business Trust in C2C Model under WeChat Circumstance",
    "doi": "https://doi.org/10.1145/3398011",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ailing Wang; Jie Sun; LI Lei-ming",
    "corresponding_authors": "",
    "abstract": "The core of micro-business and consumer transactions is trust. Based on the Theory of Reasoned Action and Technology Acceptance Model, this article discusses the factors of the establishment and promotion of micro-business trust from the trust orientation of consumer, the trust of WeChat businesses, and the trust of WeChat platform. Data were obtained by questionnaire, and SPSS software was used for data reliability and multiple regression analysis. It is concluded that all three levels have a significant positive impact on the establishment and promotion of C2C mode micro-business trust. The trust of WeChat businesses and the trust of WeChat platform have a greater influence on the establishment of micro-business trust. The trust orientation of consumer and the trust of WeChat businesses have a greater impact on the promotion of micro-business trust. Among them, the WeChat business trust level is the most important factor.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3040754729",
    "type": "article"
  },
  {
    "title": "Attention Mechanism for Uyghur Personal Pronouns Resolution",
    "doi": "https://doi.org/10.1145/3412323",
    "publication_date": "2020-10-13",
    "publication_year": 2020,
    "authors": "Qimeng Yang; Long Yu; Shengwei Tian; Jinmiao Song",
    "corresponding_authors": "",
    "abstract": "Deep neural network models for Uyghur personal pronoun resolution learn semantic information for personal pronoun and antecedents, but tend to be short-sighted—they ignore the importance of each feature. In this article, we propose a Uyghur personal pronoun resolution model based on Attention mechanism, Convolutional neural networks and Gated recurrent unit (ATCG). Our model studies the grammatical structure and semantic features of Uyghur, and extracts 11 key features for Uyghur resolution task. Attention mechanism can focus on the importance of words in sentences. Gated Recurrent Unit (GRU) is applied in this model to achieve the interdependent features with long distance. The ATCG model effectively makes up for the shortcomings of relying only on the features of the content level and achieves better classification performance. Experimental results on Uyghur resolution dataset show that our model surpasses the state-of-the-art models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3096290474",
    "type": "article"
  },
  {
    "title": "中国語方言に対する音声駆動エンドツーエンド言語識別【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Xu Fan; Luo Jian; Wang Mingwen; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3182916046",
    "type": "article"
  },
  {
    "title": "Response Generation via Structure-Aware Constraints",
    "doi": "https://doi.org/10.1145/3526216",
    "publication_date": "2022-03-26",
    "publication_year": 2022,
    "authors": "Mengyu Guan; Zhongqing Wang; Guodong Zhou",
    "corresponding_authors": "",
    "abstract": "End-to-end neural modeling with the encoder-decoder architecture has shown great promise in response generation. However, it often generates dull and generic responses due to its failure to effectively perceive various kinds of act, sentiment, and topic information. To address these challenges, we propose a response-generation model with structure-aware constraints to capture the structure of dialog and generate a better response with various constraints of the act, sentiment, and topic. In particular, given an utterance sequence, we first learn the representation of each utterance in the encoding stage. We then learn the turn, speaker, and dialog representation from the utterance representations and construct the structure of dialog. Third, we employ an attention mechanism to extract the constraints of act, sentiment, and topic based on the structure of the dialog. Finally, we utilize these structure-aware constraints to control the response-generation process in decoding stage. Extensive experimental results validate the superiority of our proposed model against the state-of-the-art baselines. In addition, the results also show that the proposed model can generate responses with more appropriate content based on the structure-aware constraints.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220704535",
    "type": "article"
  },
  {
    "title": "Icon Art Design Language Combined with Real-time Intelligent Image Processing under Internet of Things",
    "doi": "https://doi.org/10.1145/3522574",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Yang Gao",
    "corresponding_authors": "Yang Gao",
    "abstract": "While providing massive information, the intelligent media Internet of Things (IoT) also poses challenges to the overall environment and the development of modern market economy. The employment of enterprises and people is still facing great difficulties, and the world economic situation is still complicated and severe. In addition, there are many design resources for icon art design on the Internet, and with different design styles, the demand for icon design is also increasing. The biggest difference between icons and ordinary pictures is that icons can convey the characteristics and meaning of pictures faster. The Generative Adversarial Network (GAN) technology in intelligent image processing and the TensorFlow learning framework are used to build and improve the icon generation network to simplify the icon design process. Computers are used in place of designers for icon art design. Firstly, the related technical background of icon generation network implementation is drawn through the introduction of related concepts of intelligent image processing. Secondly, Python is used to process the established icon dataset. Finally, the icon generation network is improved. The model training results show that the icon generation network has a peak feature loss value of 9.0 and an average error of 8.0. After the color label is added, the effect is significantly improved. The improved icon generation network has a peak feature loss of 7.0 and an average error of 6.0. The results show that after the color labels are added, the improved GAN model has a very high recognition rate for artistic icons. The improved network model also distinguishes the newly generated icons from the original ones. The comprehensive application effect of the model is good. This provides specific application and reference value for the intelligent development of the IoT.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220798024",
    "type": "article"
  },
  {
    "title": "Design of Intelligent Decision System via Computer Aided Translation Software Evaluation under the Background of Internet of Things",
    "doi": "https://doi.org/10.1145/3517196",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "Yueting Xiang; Chengang Zeng; Kaiyang Sun; Kunzhi Tang",
    "corresponding_authors": "",
    "abstract": "The identification and classification of professional terms of machine translation are studied in this work, to improve the accuracy and professionalism of computer aided translation (CAT) software. Firstly, the current situation and related fields of machine translation are analyzed to summarize the difficulties and shortcomings in machine translation. Secondly, the concept of term is introduced to conduct targeted research on the imbalance problem of terminology classification and recognition in machine translation. Thirdly, a term recognition model based on integrated recognition method is proposed. Finally, the classification accuracy and recall rate of the model are verified using the method of confusion matrix in experiments. The results demonstrate that in comparison of the recall rate, classification accuracy, and f value in different fields, the classification accuracy of network terms by the hybrid method combining the over-sampling method and under-sampling method is the highest of 77%, that of sports terms is the lowest of 71%, and that of economic terms is 74%. Among the recall rate, accuracy rate and f value, the recall rate is the highest, reaching more than 80%, especially for economic terms of 91%. The combination of over-sampling and under-sampling performs better than the under-sampling with playback and under-sampling without playback in terms of term recognition and classification in different fields. Through the classification results before and after integration, it is obvious that the integration of each base classifier not only effectively improves the classification accuracy of terms, but also greatly improves the recall rate. This term recognition model can help CAT software in improving the recognition accuracy of term translation, which has certain practical effects and provides reference for research in related fields.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220903709",
    "type": "article"
  },
  {
    "title": "Coarse-to-Fine Output Predictions for Efficient Decoding in Neural Machine Translation",
    "doi": "https://doi.org/10.1145/3527664",
    "publication_date": "2022-04-07",
    "publication_year": 2022,
    "authors": "Qi Chen; Oi Yee Kwong; Yinqiao Li; Tong Xiao; Jingbo Zhu",
    "corresponding_authors": "",
    "abstract": "Neural Machine Translation (NMT) systems are undesirably slow as the decoder often has to compute probability distributions over large target vocabularies. In this work, we propose a coarse-to-fine approach to reduce the complexity of the decoding process, using only the information of the weight matrix in the Softmax layer. The large target vocabulary is first trimmed to a small candidate set in the coarse-grained phase, and from this candidate set the final top- k results are generated in the fine-grained phase. Tested on an RNN-based NMT system and a Transformer-based NMT system separately, our GPU-friendly method achieved a significant speed-up without harming the translation quality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4225511789",
    "type": "article"
  },
  {
    "title": "Radial Basis Function Attention for Named Entity Recognition",
    "doi": "https://doi.org/10.1145/3539014",
    "publication_date": "2022-05-31",
    "publication_year": 2022,
    "authors": "Jiusheng Chen; Xingkai Xu; Xiaoyu Zhang",
    "corresponding_authors": "",
    "abstract": "Attention mechanism is an increasingly important approach in the field of natural language processing (NLP). In the attention-based named entity recognition (NER) model, most attention mechanisms can calculate attention coefficient to express the importance of sentence semantic information but cannot adjust the position distribution of contextual feature vectors in the semantic space. To address this issue, a radial basis function attention (RBF-attention) layer is proposed to adaptively regulate the position distribution of sequence contextual feature vectors, which can minimize the relative distance of within-category named entities and maximize the relative distance of between-category named entities in the semantic space. The experimental results on CoNLL2003 English and MSRA Chinese NER datasets indicate that the proposed model performs better than other baseline approaches without relying on any external feature engineering.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4281683424",
    "type": "article"
  },
  {
    "title": "TAM: A Front-End to an Auto-Parallelizing Compiler",
    "doi": "https://doi.org/10.1145/3543510",
    "publication_date": "2022-06-15",
    "publication_year": 2022,
    "authors": "Ashish Sharma; Mayank Badjatiya; Aayush Sahay; Aryan Verma; Ayush Agarwal; Gaurav Singal; Deepak Garg; Deepak Kumar Jain; Ketan Kotecha",
    "corresponding_authors": "",
    "abstract": "The multi-core architecture has revolutionized the parallel computing. Despite this, the modern age compilers have a long way to achieve auto-parallelization. Through this paper, we introduce a language that encouraging the auto-parallelization. We are also introducing Front-End for our auto-parallelizing compiler. Later, we examined our compiler employing a different number of core and verify results based on different metrics based on total compilation time, memory utilization, power utilization and CPU utilization. At last, we learned that parallelizing multiple files engage more CPU resources, memory and energy, but it finishes the task at hand in less time. In this paper, we have proposed a loop code generation technique that makes the generation of nested loop IR code faster by dividing the blocks into some extra code blocks using a modular approach. Our TAM compiler technique speedup by 7.506, 5.283 and 2.509 against sequential compilation when we utilized 8, 4 and 2 cores respectively. We observed that the CPU utilization of the TAM compiler reaches the maximum permissible limit when an optimal parallelizable instance is compiled.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4282919686",
    "type": "article"
  },
  {
    "title": "Research on Blended Learning Evaluation Method of Overseas Chinese Education in the Post-epidemic Era",
    "doi": "https://doi.org/10.1145/3542926",
    "publication_date": "2022-06-28",
    "publication_year": 2022,
    "authors": "Wenxia Jiang; Qingna Lin; Wei Qin",
    "corresponding_authors": "",
    "abstract": "The spread of COVID-19 in the world has changed the way of life, economy, society, learning, and work around the world. Under the background of normalization of epidemic prevention and control, how to carry out overseas Chinese education and how to affect overseas Chinese education is an important issue with scholars in China and abroad and overseas Chinese. How to effectively deal with the international situation after the outbreak of the epidemic and how to determine the future developmental direction of overseas Chinese education is an important goal and direction that researchers need to study in depth. Based on the perspective of teaching effect evaluation of blended learning mode under the background of artificial intelligence, this article takes ASEAN Phuket Thai Chinese Schools as an example, collects relevant research data through a grounded survey method, adopts a quantitative analysis method, comprehensively judges the impact of overseas Chinese education after the outbreak of the epidemic, and puts forward systematic, comprehensive, and scientific suggestions. It is hoped that the decision-making mechanism and method innovation of overseas Chinese education in response to COVID-19 will be effectively promoted, and the effective development of overseas Chinese education will be actively promoted.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4283658509",
    "type": "article"
  },
  {
    "title": "A Case Study for Sports Characteristic Town: Enlightenment to the Sport Industry",
    "doi": "https://doi.org/10.1145/3517915",
    "publication_date": "2022-07-29",
    "publication_year": 2022,
    "authors": "Deren Zhong",
    "corresponding_authors": "Deren Zhong",
    "abstract": "The characteristic sports town is proposed to promote urban and rural integration by implementing a national-level fitness strategy. It plays a crucial role in the sports industry's promotion, launch, and supply. The characteristic town for sports creates a form of interaction between new urbanization and the sports industry. They play a unique role in fostering the systemic reform of sports, the convergence of urban and rural areas, the legacy and creativity of popular sports culture, and addressing urban–rural people's fitness needs. The growth of the sports sector has been elevated to the strategic height of national economic building. This article considers creating a characteristic sports town as a research object, using various research techniques, such as the literature review, case analysis, and expert interviews. “Building a Characteristic Sports Town” is the latest economic growth hub for advancing sports. It supports the spiritual civilization of the people, respects economic development, and improves people's happiness. The characteristic sports town lowers the country's unemployment rate. Finally, the characteristic sports town promotes a country's health and economic development at higher rates. The experimental results show that the proposed National-level Fitness Strategy enhances outcomes of a successful sports industry curriculum development ratio of 96.2%, precision ratio of 92.1%, probability ratio of 93.5%, cost-effectiveness ratio of 94.4%, recall ratio of 91.2%, self-esteem ratio of 91.9%, interaction ratio of 98.7%, and efficiency ratio of 94.2% when compared to other methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4288702959",
    "type": "article"
  },
  {
    "title": "Low-Power Feature-Attention Chinese Keyword Spotting Framework with Distillation Learning",
    "doi": "https://doi.org/10.1145/3558002",
    "publication_date": "2022-08-17",
    "publication_year": 2022,
    "authors": "Lei Lei; Yuan Guo-shun; Tianle Zhang; Hongjiang Yu",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose a novel Low-Power Feature-Attention Chinese Keyword Spotting Framework based on a depthwise separable convolution neural network (DSCNN) with distillation learning to recognize speech signals of Chinese wake-up words. The framework consists of a low-power feature-attention acoustic model and its learning methods. Different from the existing model, the proposed acoustic model based on connectionist temporal classification (CTC) focuses on the reduction of power consumption by reducing model network parameters and multiply-accumulate (MAC) operations through our designed feature-attention network and DSCNN. In particular, the feature-attention network is specially designed to extract effective syllable features from a large number of MFCC features. This could refine MFCC features by selectively focusing on important speech signal features and removing invalid speech signal features to reduce the number of speech signal features, which helps to significantly reduce the parameters and MAC operations of the whole acoustic model. Moreover, DSCNN with fewer parameters and MAC operations compared with traditional convolution neural networks is adopted to extract effective high-dimensional features from syllable features. Furthermore, we apply a distillation learning algorithm to efficiently train the proposed low-power acoustic model by utilizing the knowledge of the trained large acoustic model. Experimental results thoroughly verify the effectiveness of our model and show that the proposed acoustic model still has better accuracy than other acoustic models with the lowest power consumption and smallest latency measured by NVIDIA JETSON TX2. It has only 14.524 KB parameters and consumes only 0.141 J energy per query and 17.9 ms latency on the platform, which is hardware-friendly.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4292092851",
    "type": "article"
  },
  {
    "title": "Automatic Labeling of Clusters for a Low-Resource Urdu Language",
    "doi": "https://doi.org/10.1145/3511097",
    "publication_date": "2022-03-21",
    "publication_year": 2022,
    "authors": "Zarmeen Nasim; Sajjad Haider",
    "corresponding_authors": "",
    "abstract": "Document clustering techniques often produce clusters that require human intervention to interpret the meaning of such clusters. Automatic cluster labeling refers to the process of assigning a meaningful phrase to a cluster as a label. This article proposes an unsupervised method for cluster labeling that is based on noun phrase chunking. The proposed method is compared with four other statistical-based methods, including Z-Order, M-Order, T-Order, and YAKE. In addition to the statistical measures based labeling schemes, the approach is also compared with two graph-based techniques: TextRank and PositionRank. The experiments were performed on the low-resource Urdu language corpus of News Headlines. The proposed approach's effectiveness was evaluated using cosine similarity, the Jaccard index, and feedback received from human evaluators. The results show that the proposed method outperforms other methods. It was found that the labels produced were more relevant and semantically rich in contrast to other approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4293793261",
    "type": "article"
  },
  {
    "title": "Research on UAV Teaching Application and Technological Innovation with 5G Technology and Development of High-pole Throwing Hydrangea",
    "doi": "https://doi.org/10.1145/3529391",
    "publication_date": "2022-09-02",
    "publication_year": 2022,
    "authors": "Xu Shijun; YunTian Li; Zeng Cong; Jin Yong Tang",
    "corresponding_authors": "",
    "abstract": "The technical innovation of high pole throwing Hydrangea is increasing and this study compares with the existing projection technology theory from two aspects of teaching and competition, promotes the corresponding improvement of competition rules and referee law enforcement. This paper studies (1) The history and culture of Ethnic Zhuangs' embroidered ball and the development process of modern throwing embroidered ball by using the method of literature review, (2) Studies the technical characteristics of high-pole throwing Hydrangea by using the method of mathematical analysis, (3) Compares the landing points of two main Hydrangea throwing techniques after passing through the circle by using UAV aerial photography of Hydrangea flying tracks, (4) The Self-learning Systems and Pattern Recognition and Exploitation for Multimedia Asian Information Processing in which the disputes existing in the process of competition rules and referee enforcement. The research shows that: the competition rules and the referee's decision are controversial, the existing projection teaching technology lacks practical guidance for high-level competition, innovative projection technology is divided into three matching situations, the best shot angle range is 64°&lt;α &lt;72°; and the shot speed range is 13.04m/s &lt; V0 &lt; 13.70m/s. Suggestions: According to the observation and analysis of the competition scene using UAV and Internet and other science and technology, improve the teaching and competitive level of Hydrangea throwing, improve the competition rules, enhance the referee's law enforcement ability so as to improve the competitive level of national sports, and take the standardized and international road to inherit the excellent national sports culture, and provide new development ideas for the popularization and promotion of national traditional sports and the inheritance of excellent national traditional sports culture.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4294238424",
    "type": "article"
  },
  {
    "title": "Identification and Extraction of Features from Malayalam Poems for Analyzing Syllable Duration Patterns",
    "doi": "https://doi.org/10.1145/3561298",
    "publication_date": "2022-09-07",
    "publication_year": 2022,
    "authors": "M. P. Jasir; Kannan Balakrishnan",
    "corresponding_authors": "",
    "abstract": "Text-to-speech (TTS) synthesis is an active area of research to generate synthetic speech from the underlying text. Compared to English and many European languages, TTS is yet to mature in Malayalam, the principal language of the South Indian state of Kerala. A syllable has to be uttered with proper durational and prosodic characteristics to emulate natural speech. When it comes to poems in Malayalam, many of them have an inherent rhythm attached to them. In Malayalam, this property is characterized by the Vruta [ 28 ] in which the poem is written. Vruta decides the meter of narration of the poem. Therefore, it is only consequential that Vruta can give away vital cues about the durational and prosodic characteristics of the poem verses recited. This study intends to identify the features that determine the durational characteristics of a poem written in a particular Vruta and develop an algorithm to extract those features required to build a dataset to model the duration of syllable utterances for tuneful TTS in Malayalam. Poems written in three Vrutas, namely Kakali, Manjari, and Keka, are considered in this study. Nineteen extractible features from the orthographic representation of a poem are identified for this purpose. A standard dataset is built using these extracted features. Later, support vector machine and feed forward neural network based estimators are proposed to model the duration of Malayalam poem syllables for tuneful speech synthesis. The hyperparameters are optimized using the GridsearchCV algorithm from the Scikit-learn machine learning library [ 15 ].",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4294875682",
    "type": "article"
  },
  {
    "title": "Khmer Sentiment Lexicon Based on PU Learning and Label Propagation Algorithm",
    "doi": "https://doi.org/10.1145/3564697",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Chao Li; Xin Yan; Guangyi XU; Zhongying Deng; Yuanyuan Mo",
    "corresponding_authors": "",
    "abstract": "The sentiment lexicon is an important tool for natural language processing tasks. In addition to being able to determine the sentiment polarity of words or phrases, it can assist attribute-level, sentence-level, and text-level sentiment analysis tasks. In light of the fact that tagging data and corpora for the Khmer language are scarce, where most resources related to sentiment lexicons are for English, this paper proposes a method for constructing a sentiment lexicon for Khmer based on Positive-Unlabeled learning (PU Learning) and the label propagation algorithm. Sentiment words are first extracted from a corpus using the Spy technique of PU learning method. The main idea is to purify the set of N-class examples, train the MLP classifier, and continuously delete spy words and increase the number of P-class words in the iterative process. Following this, the sentiment polarity of the candidate words is determined. By considering the problem of determining the sentiment polarity of the candidate words as one of calculating its probability distribution, a small number of labeled sentiment words and candidate words are used to construct a graph model. The contextual information of the candidate words is used to construct a simple supplementary graph model of the set of sentiment words through word co-occurrence and triangulation, where this enhances the correlation between data items. The sentiment polarity of the candidate words is then determined through the label propagation algorithm. The results of experiments show that the proposed method can be used to construct a Khmer sentiment lexicon with a small number of labeled data and a small corpus without requiring excessive manual labeling.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4298000226",
    "type": "article"
  },
  {
    "title": "On the Effectiveness of Images in Multi-modal Text Classification: An Annotation Study",
    "doi": "https://doi.org/10.1145/3565572",
    "publication_date": "2022-10-07",
    "publication_year": 2022,
    "authors": "Chunpeng Ma; Aili Shen; Hiyori Yoshikawa; Tomoya Iwakura; Daniel Beck; Timothy Baldwin",
    "corresponding_authors": "",
    "abstract": "Combining different input modalities beyond text is a key challenge for natural language processing. Previous work has been inconclusive as to the true utility of images as a supplementary information source for text classification tasks, motivating this large-scale human study of labelling performance given text-only, images-only, or both text and images. To this end, we create a new dataset accompanied with a novel annotation method—Japanese Entity Labeling with Dynamic Annotation—to deepen our understanding of the effectiveness of images for multi-modal text classification. By performing careful comparative analysis of human performance and the performance of state-of-the-art multi-modal text classification models, we gain valuable insights into differences between human and model performance, and the conditions under which images are beneficial for text classification.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4303427394",
    "type": "article"
  },
  {
    "title": "Evaluation of Classroom Teaching Quality Based on Film and Multimedia Mixed Learning",
    "doi": "https://doi.org/10.1145/3536422",
    "publication_date": "2022-11-05",
    "publication_year": 2022,
    "authors": "Yuanyuan Ji",
    "corresponding_authors": "Yuanyuan Ji",
    "abstract": "Nowadays, the computer information management system has been integrated into people's life and work. Because of the rapid development of film and television industry, multimedia classroom teaching has become a trend of film and television teaching. More and more subjects have developed blended teaching models, and how to evaluate the learning effects of blended teaching has also attracted more and more attention from the educational circles. Therefore, based on the background of video multimedia classroom teaching and the theory of fuzzy synthetic valuation, this paper designs and implements a set of video multimedia classroom teaching quality valuation system. Firstly, based on the fuzzy synthetic valuation, the corresponding valuation indexes are designed. Secondly, the system design, according to the specific user application requirements, design the relevant functions; finally, the B / S mode structure system, using JSP / servlet web page development technology to achieve the system. In order to test the performance of the system designed in this paper, a small-scale teaching quality valuation system is constructed. Through the analysis of the application of the system, it is found that the teaching quality valuation system designed in this paper can well evaluate the teaching quality of teachers, promote the development of film and television teaching, and improve the teaching quality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4308262145",
    "type": "article"
  },
  {
    "title": "Exploration of the Requirements of College English Follow-Up Teaching Courses Through Mobile Web Application",
    "doi": "https://doi.org/10.1145/3529390",
    "publication_date": "2022-11-05",
    "publication_year": 2022,
    "authors": "Tan Juan",
    "corresponding_authors": "Tan Juan",
    "abstract": "The purpose is to analyze the implementation status and students' requirements of English follow-up teaching courses in universities and provide a basis for improving the quality of English teaching in universities. Here, a detailed analysis is conducted for college students and their FL (Foreign Language) requirements through basic theories and RA (Requirements Analysis) model. Then, the implementation status of college English follow-up courses is analyzed, including its advantages and disadvantages. A total of 150 English majors and 150 non-English majors (international economy and trade) in Xi'an International Studies University are selected as subjects. Afterward, 300 QSs (Questionnaire Survey) are distributed, and 280 valid QSs are collected. The data are analyzed with STAT (Statistical) SW (Software). The results show that both English majors and international economic and trade majors prefer the original English textbooks over school-teachers-written textbooks, and students love humorous teachers more. Students from both majors like the curriculum arrangement of integration of English skills, intercultural knowledge, and humanistic literacy the best. Meanwhile, the requirement of English majors is stronger for the integrated curriculum. The teaching mode and teaching assessment method of English majors are quite different from those of international economic and trade majors. English majors are more inclined to combine online teaching, classroom teaching, and thematic lectures, while classroom teaching is the last thing they want. At the same time, they assess English follow-up teaching through curriculum papers rather than examinations. It shows that different majors have different requirements for follow-up teaching courses. Universities provide a customized and localized follow-up teaching mechanism for students.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4308318511",
    "type": "article"
  },
  {
    "title": "Comparative Study on the Teaching of Japanese Listening and Speaking in the Language Room of Teachers' Intelligent Multimedia Systems in Chinese and Foreign Universities",
    "doi": "https://doi.org/10.1145/3533315",
    "publication_date": "2022-11-11",
    "publication_year": 2022,
    "authors": "Hao Yu",
    "corresponding_authors": "Hao Yu",
    "abstract": "With the popularization of computers and various mobile intelligent terminals, intelligent multimedia teaching systems with learners as the main body are more and more favored by learners, which affects and changes the methods and modes of Japanese teaching to a large extent. Native language teachers are considered the authority of the language. Fluent oral expression and profound knowledge of language and culture make them regarded as ideal foreign language teachers, especially in oral teaching, they are regarded as having incomparable advantages. This research aims to explore the differences between Chinese and foreign teachers in listening and speaking teaching under the teaching conditions of the intelligent multimedia language room, and to try to analyze the reasons for the differences. This paper interviewed 180 sophomores of Japanese majors, 4 Chinese Japanese teachers and 2 Japanese foreign teachers at foreign language universities through a questionnaire survey, and analyzed the statistical data through SPSS 21.0. The 180 students were divided into groups of 30, with 4 groups from non-Japanese teachers' classes and 2 from Japanese teachers' classes. For a long time, they have used the same Japanese listening and speaking textbooks in the intelligent multimedia language room. In order to better improve the quality of teaching, Chinese and foreign teachers should strengthen teaching cooperation and exchanges, rationally improve teaching content, optimize the use of teaching software, and achieve win-win cooperation. In the NJST class, 29.2% of students believe that they spend more time practicing listening in class than speaking. In the NNJST class, close to 39.1% of students believe that teachers spend more time on listening. Fully consider the needs of learners in learning a second language other than Chinese, and through research on Japanese learning strategies and teaching theories, to design and develop an intelligent Japanese teaching system with comprehensive functions, strong applicability, and support for individualized, autonomous and collaborative learning.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4308804045",
    "type": "article"
  },
  {
    "title": "Requirement Analysis and Teaching Strategies for Advanced English Writing Course Based on Blended Learning Algorithm",
    "doi": "https://doi.org/10.1145/3533317",
    "publication_date": "2022-11-16",
    "publication_year": 2022,
    "authors": "Tian Zhang",
    "corresponding_authors": "Tian Zhang",
    "abstract": "To make advanced English writing course in colleges and universities better service economic development, the teaching status quo of advanced English writing in several colleges and universities in Xi'an is studied through the questionnaire survey, and suggestions from students are collected and analyzed based on the theory of requirement analysis. The questionnaire survey results show that 60% of students take advanced English writing course to be more competitive in their professional fields. However, teaching in advanced English writing at this stage cannot meet their requirements for academic paper writing, and 50% of the students are less satisfied with the current advanced English writing course. They hope that academic English can be incorporated into comprehensive English, so that they can improve their ability to use academic English. The requirement analysis on advanced English writing reveals that students hope to have a more comprehensive teaching mode for English writing, and put forward different requirements for teachers' qualifications, class size, teaching mode, and teaching content. In response to these requirements, teaching strategies on advanced English writing are discussed, so as to establish a more complete teaching system for advanced English writing. This study is of great significance for research on advanced English writing course in China.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4309618237",
    "type": "article"
  },
  {
    "title": "Research on Strategies of Synchronous Interaction Hybrid Basic Sports and University Education Interaction Model Based on SERVQUAL Model",
    "doi": "https://doi.org/10.1145/3533316",
    "publication_date": "2022-12-02",
    "publication_year": 2022,
    "authors": "Zhili Chang",
    "corresponding_authors": "Zhili Chang",
    "abstract": "Promoting the balanced development of regional basic education and finally realizing education equity is an important task for my country's current educational development. To this end, the balanced development of informatization and basic education has carried out experiments and practical explorations of informatization to promote the balanced development of regional basic education. Through synchronized interactive hybrid classrooms, the sharing of high-quality urban teachers to rural teaching sites has helped them to implement national regulations. Course. Using the SERVQUAL model for data analysis, synchronous interactive hybrid classrooms, as a new form of high-quality teacher sharing, are of great significance to promoting the balanced development of basic education in my country. It has attracted great attention from education researchers and is considered to be a solution area. Effective means for the problem of imbalanced sex education resources. Therefore, the VR teaching experience, the use of wearable sports equipment, the construction of a physical education network platform, and the four basic sports professional courses of physical education and university education interactive strategies are proposed, and the effects of the interactive strategies are analyzed. The experimental results show that the students' evaluation of the traditional basic physical education teaching mode is low, and the score is basically below 5 points. After the interaction between basic physical education and university education when applying the SERVQUAL model, the VR teaching experience strategy has a significant impact on basic physical education. VR teaching experience strategy basic physical education students achieve a score of 10, and their interest has increased by 34%. Sports performance under this interactive strategy increased by 51%. The strategy of interaction between basic physical education and university education has increased students' interest in basic physical education courses, improved students' performance in basic physical education courses, and promoted the development of physical education to a certain extent.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4311119212",
    "type": "article"
  },
  {
    "title": "Neural Dependency Parser for Tibetan Sentences",
    "doi": "https://doi.org/10.1145/3429456",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Bo An; Congjun Long",
    "corresponding_authors": "",
    "abstract": "The research of Tibetan dependency analysis is mainly limited to two challenges: lack of a dataset and reliance on expert knowledge. To resolve the preceding challenges, we first introduce a new Tibetan dependency analysis dataset, and then propose a neural-based framework that resolves the reliance on the expert knowledge issue by automatically extracting feature vectors of words and predicts their head words and type of dependency arcs. Specifically, we convert the words in the sentence into distributional vectors and employ a sequence to vector network to extract feature words. Furthermore, we introduce a head classifier and type classifier to predict the head word and type of dependency arc, respectively. Experiments demonstrate that our model achieves promising performance on the Tibetan dependency analysis task.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3138162744",
    "type": "article"
  },
  {
    "title": "A Joint Model for Representation Learning of Tibetan Knowledge Graph Based on Encyclopedia",
    "doi": "https://doi.org/10.1145/3447248",
    "publication_date": "2021-03-30",
    "publication_year": 2021,
    "authors": "Yuan Sun; Andong Chen; Chaofan Chen; Tianci Xia; Xiaobing Zhao",
    "corresponding_authors": "",
    "abstract": "Learning the representation of a knowledge graph is critical to the field of natural language processing. There is a lot of research for English knowledge graph representation. However, for the low-resource languages, such as Tibetan, how to represent sparse knowledge graphs is a key problem. In this article, aiming at scarcity of Tibetan knowledge graphs, we extend the Tibetan knowledge graph by using the triples of the high-resource language knowledge graphs and Point of Information map information. To improve the representation learning of the Tibetan knowledge graph, we propose a joint model to merge structure and entity description information based on the Translating Embeddings and Convolution Neural Networks models. In addition, to solve the segmentation errors, we use character and word embedding to learn more complex information in Tibetan. Finally, the experimental results show that our model can make a better representation of the Tibetan knowledge graph than the baseline.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3141398089",
    "type": "article"
  },
  {
    "title": "Recognition of Tibetan Maximal-length Noun Phrases Based on Syntax Tree",
    "doi": "https://doi.org/10.1145/3423324",
    "publication_date": "2021-03-30",
    "publication_year": 2021,
    "authors": "Congjun Long; Xuewen Zhou; Maoke Zhou",
    "corresponding_authors": "",
    "abstract": "Frequently corresponding to syntactic components, the Maximal-length Noun Phrase (MNP) possesses abundant syntactic and semantic information and acts a certain semantic role in sentences. Recognition of MNP plays an important role in Natural Language Processing and lays the foundation for analyzing and understanding sentence structure and semantics. By comparing the essence of different MNPs, this article defines the MNP in the Tibetan language from the perspective of syntax tree. A total of 6,038 sentences are extracted from the syntax tree corpus, the structure type, boundary feature, and frequency of MNPs are analyzed, and the MNPs are recognized by applying the sequence tagging model and the syntactic analysis model. The accuracy, recall, and F1 score of the recognition results of applying sequence tagging model are 87.14%, 84.72%, and 85.92%, respectively. The accuracy, recall, and F1 score of the recognition results of applying syntactic analysis model are 87.66%, 87.63%, and 87.65%, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3142957285",
    "type": "article"
  },
  {
    "title": "Multi-level Chunk-based Constituent-to-Dependency Treebank Transformation for Tibetan Dependency Parsing",
    "doi": "https://doi.org/10.1145/3424247",
    "publication_date": "2021-03-30",
    "publication_year": 2021,
    "authors": "Shumin Shi; Dan Luo; Xing Wu; Congjun Long; Heyan Huang",
    "corresponding_authors": "",
    "abstract": "Dependency parsing is an important task for Natural Language Processing (NLP). However, a mature parser requires a large treebank for training, which is still extremely costly to create. Tibetan is a kind of extremely low-resource language for NLP, there is no available Tibetan dependency treebank, which is currently obtained by manual annotation. Furthermore, there are few related kinds of research on the construction of treebank. We propose a novel method of multi-level chunk-based syntactic parsing to complete constituent-to-dependency treebank conversion for Tibetan under scarce conditions. Our method mines more dependencies of Tibetan sentences, builds a high-quality Tibetan dependency tree corpus, and makes fuller use of the inherent laws of the language itself. We train the dependency parsing models on the dependency treebank obtained by the preliminary transformation. The model achieves 86.5% accuracy, 96% LAS, and 97.85% UAS, which exceeds the optimal results of existing conversion methods. The experimental results show that our method has the potential to use a low-resource setting, which means we not only solve the problem of scarce Tibetan dependency treebank but also avoid needless manual annotation. The method embodies the regularity of strong knowledge-guided linguistic analysis methods, which is of great significance to promote the research of Tibetan information processing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3148674130",
    "type": "article"
  },
  {
    "title": "Taming the Wild Etext: Managing, Annotating, and Sharing Tibetan Corpora in Open Spaces",
    "doi": "https://doi.org/10.1145/3418060",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Ngawang Trinley; Tenzin; dirk schmidt; Helios Hildt; Tenzin Kaldan",
    "corresponding_authors": "",
    "abstract": "Digital text is quickly becoming essential to modern daily life. The article you are reading right now is born digital; unlike texts of the not-so-distant past, it may never be printed at all. Worldwide, the trend is clear: Digital text is on the way in, and print is on its way out. Year-by-year, more and more readers are turning to ebooks, internet news, and other forms of ereading, while generation by generation, print is becoming less and less relevant. 1 1 Pew research shows 50% of Americans have a dedicated ereading device, with yearly gains in ereadership [1]; industry research, too, shows a definite trend toward ereading and non-traditional publishing, with ebooks making up 50% of fiction reading in 2016 [2], while journalism is also trending online [3]. These trends are not unique to English—to meet the demands and expectations of today's readers, Tibetan texts, too, are being digitized by many organizations and institutions with a shared appreciation for the Tibetan literary heritage. They include a variety of secular publishers, monastic institutions, and Buddhist foundations, among others. But while these organizations share common goals for common texts, their work is all too frequently completely disconnected from the community at large. This situation negatively impacts what is already a minoritized and under-resourced language. While competition—from other languages, as well as other publishers in the Tibetan etext world—has been a driver of innovation in the adoption of ereading technology, we believe that a rich, shared data source is not only in everyone's best interest but also the only practical way forward when we consider the time, effort, expertise, and money that quality digitization takes. That is why we have designed OpenPecha to be a public, open platform for collaborative etext curation and annotation sharing. Its aim is providing a wide range of users with the latest version of the exact “view” of any text needed, while maintaining the integrity of the text and its annotations and simultaneously allowing for community improvements and additions. In this article, we explore the details of how the project came to be, what it is, and how it works, while also presenting a few common use cases.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3163064939",
    "type": "article"
  },
  {
    "title": "Developing the Persian Wordnet of Verbs Using Supervised Learning",
    "doi": "https://doi.org/10.1145/3450969",
    "publication_date": "2021-05-26",
    "publication_year": 2021,
    "authors": "Zahra Mousavi; Heshaam Faili",
    "corresponding_authors": "",
    "abstract": "Nowadays, wordnets are extensively used as a major resource in natural language processing and information retrieval tasks. Therefore, the accuracy of wordnets has a direct influence on the performance of the involved applications. This paper presents a fully-automated method for extending a previously developed Persian wordnet to cover more comprehensive and accurate verbal entries. At first, by using a bilingual dictionary, some Persian verbs are linked to Princeton WordNet synsets. A feature set related to the semantic behavior of compound verbs as the majority of Persian verbs is proposed. This feature set is employed in a supervised classification system to select the proper links for inclusion in the wordnet. We also benefit from a pre-existing Persian wordnet, FarsNet, and a similarity-based method to produce a training set. This is the largest automatically developed Persian wordnet with more than 27,000 words, 28,000 PWN synsets and 67,000 word-sense pairs that substantially outperforms the previous Persian wordnet with about 16,000 words, 22,000 PWN synsets and 38,000 word-sense pairs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3164114211",
    "type": "article"
  },
  {
    "title": "An OT-ET Analysis of Polish Singular-Plural Pairs",
    "doi": "https://doi.org/10.1145/3434238",
    "publication_date": "2021-05-25",
    "publication_year": 2021,
    "authors": "Xiaowen Ji; Jincheng Ni",
    "corresponding_authors": "",
    "abstract": "Optimality Theory (OT) and Exemplar Theory (ET) are two enchanting theories to many scholars, but each still faces criticism and remaining persistent problems. Application of both theories to areas in linguistics where conflicts may arise has been attempted, but still the suitability of combining the two theories to resolve contradictions awaits further analysis and verification. This article takes Polish singular-plural pairs as the object of study and argues in favor of an OT-ET combined model of analyzing the linguistic phenomenon. First, an underlying representation is identified to be the input in an OT analysis. Then two main changes are recognized between the input and output, and are regarded as instances of positional neutralization, and their relevant constraints and constraint hierarchies are presented. Following this, challenges are posed to OT despite its merits. It turns out that the combined OT-ET model works well, with historical development, underspecification, constraint hierarchy, and resemblance to existing word clouds, among others, all playing relevant parts. The current study adds to the extensiveness of language data analyzed for or against combining OT and ET, and sketches the analysis pattern of thus doing, with a view to offering more real-life language materials for an OT-ET combined model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3164902692",
    "type": "article"
  },
  {
    "title": "Sequence Alignment with Q-Learning Based on the Actor-Critic Model",
    "doi": "https://doi.org/10.1145/3433540",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "LI Yarong",
    "corresponding_authors": "LI Yarong",
    "abstract": "Multiple sequence alignment methods refer to a series of algorithmic solutions for the alignment of evolutionary-related sequences while taking into account evolutionary events such as mutations, insertions, deletions, and rearrangements under certain conditions. In this article, we propose a method with Q-learning based on the Actor-Critic model for sequence alignment. We transform the sequence alignment problem into an agent's autonomous learning process. In this process, the reward of the possible next action taken is calculated, and the cumulative reward of the entire process is calculated. The results show that the method we propose is better than the gene algorithm and the dynamic programming method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3173401393",
    "type": "article"
  },
  {
    "title": "Configurational Path to Chinese Reading Stickiness of Digital Library",
    "doi": "https://doi.org/10.1145/3459092",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Yongliang Deng; Hua Zhang",
    "corresponding_authors": "",
    "abstract": "Attracting and retaining readers in an increasingly competitive environment is an urgent problem for digital libraries of original literature. However, few empirical studies address online reading stickiness, particularly the factors affecting the promotion of online reading stickiness, in what combinations or paths these effects exist, and whether there are complementary, alternative, and inhibitory relationships among the factors. To solve the practical problems and fill the theoretical gap, we use a fuzzy-set qualitative comparative analysis to study the interaction effects of the flow experience (feeling of immersion and perceived pleasure), technology acceptance model (perceived usefulness and perceived ease of use), and customer participation (information sharing and interpersonal interaction) to identify the critical configurations leading to a high level of stickiness in online reading and to verify the complementarity, substitution, and inhibition relationships among these variables. The findings provide implications for further research on complexity theory in digital libraries of original literature, and for managers to view and redesign online reading stickiness as configurations of IT and psychological capabilities. This study enriches and develops the existing theories and expands the application of the qualitative comparative analysis method in the field of digital libraries.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3174940377",
    "type": "article"
  },
  {
    "title": "A Computer Corpus-Based Study of Chinese EFL Learners’ Use of Adverbial Connectors and Its Implications for Building a Language-Based Learning Environment",
    "doi": "https://doi.org/10.1145/3457987",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Fei Deng; Timothy V. Rasinski",
    "corresponding_authors": "",
    "abstract": "This research adopts the methodology of corpus-based analysis and contrastive interlanguage analysis (CIA), using three corpora as the data source to analyze the adverbial connectors used by Chinese EFL (English as a foreign language) learners (i.e., university students in Guangzhou, China) in their written English. Major findings show that Chinese EFL learners have displayed a general tendency to overuse English adverbial connectors in terms of total tokens when compared with native speakers of English, and Chinese EFL learners deviate notably from the native speakers of English in the use of some individual English adverbial connectors. The research explores that Chinese EFL learners’ use of English adverbial connectors might be influenced by L1 transfer, writing handbooks’ and teachers’ instruction, learners’ lack of audience awareness, and lack of stylistic awareness. The research has some implications for language learning: a large collection of learner corpora, a target language's native speakers corpus, a learner's mother language corpus, and corpus software AntConc can complement textbooks in language learners’ deep learning process, constituting a language-based learning environment for human languages with reduced perplexity and increased accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3176744667",
    "type": "article"
  },
  {
    "title": "Mining Domain Terminologies Using Search Engine's Query Log",
    "doi": "https://doi.org/10.1145/3462327",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Weijian Ni; Tong Liu; Qingtian Zeng; Nengfu Xie",
    "corresponding_authors": "",
    "abstract": "Domain terminologies are a basic resource for various natural language processing tasks. To automatically discover terminologies for a domain of interest, most traditional approaches mostly rely on a domain-specific corpus given in advance; thus, the performance of traditional approaches can only be guaranteed when collecting a high-quality domain-specific corpus, which requires extensive human involvement and domain expertise. In this article, we propose a novel approach that is capable of automatically mining domain terminologies using search engine's query log—a type of domain-independent corpus of higher availability, coverage, and timeliness than a manually collected domain-specific corpus. In particular, we represent query log as a heterogeneous network and formulate the task of mining domain terminology as transductive learning on the heterogeneous network. In the proposed approach, the manifold structure of domain-specificity inherent in query log is captured by using a novel network embedding algorithm and further exploited to reduce the need for the manual annotation efforts for domain terminology classification. We select Agriculture and Healthcare as the target domains and experiment using a real query log from a commercial search engine. Experimental results show that the proposed approach outperforms several state-of-the-art approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3195262486",
    "type": "article"
  },
  {
    "title": "Source-side Reordering to Improve Machine Translation between Languages with Distinct Word Orders",
    "doi": "https://doi.org/10.1145/3448252",
    "publication_date": "2021-07-31",
    "publication_year": 2021,
    "authors": "Karunesh Arora; Shyam Sunder Agrawal",
    "corresponding_authors": "",
    "abstract": "English and Hindi have significantly different word orders. English follows the subject-verb-object (SVO) order, while Hindi primarily follows the subject-object-verb (SOV) order. This difference poses challenges to modeling this pair of languages for translation. In phrase-based translation systems, word reordering is governed by the language model, the phrase table, and reordering models. Reordering in such systems is generally achieved during decoding by transposing words within a defined window. These systems can handle local reorderings, and while some phrase-level reorderings are carried out during the formation of phrases, they are weak in learning long-distance reorderings. To overcome this weakness, researchers have used reordering as a step in pre-processing to render the reordered source sentence closer to the target language in terms of word order. Such approaches focus on using parts-of-speech (POS) tag sequences and reordering the syntax tree by using grammatical rules, or through head finalization. This study shows that mere head finalization is not sufficient for the reordering of sentences in the English-Hindi language pair. It describes various grammatical constructs and presents a comparative evaluation of reorderings with the original and the head-finalized representations. The impact of the reordering on the quality of translation is measured through the BLEU score in phrase-based statistical systems and neural machine translation systems. A significant gain in BLEU score was noted for reorderings in different grammatical constructs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3195322482",
    "type": "article"
  },
  {
    "title": "Coherent Dialog Generation with Query Graph",
    "doi": "https://doi.org/10.1145/3462551",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Jun Xu; Zeyang Lei; Haifeng Wang; Zheng-Yu Niu; Hua Wu; Wanxiang Che; Jizhou Huang; Ting Liu",
    "corresponding_authors": "",
    "abstract": "Learning to generate coherent and informative dialogs is an enduring challenge for open-domain conversation generation. Previous work leverage knowledge graph or documents to facilitate informative dialog generation, with little attention on dialog coherence. In this article, to enhance multi-turn open-domain dialog coherence, we propose to leverage a new knowledge source, web search session data, to facilitate hierarchical knowledge sequence planning, which determines a sketch of a multi-turn dialog. Specifically, we formulate knowledge sequence planning or dialog policy learning as a graph grounded Reinforcement Learning (RL) problem. To this end, we first build a two-level query graph with queries as utterance-level vertices and their topics (entities in queries) as topic-level vertices. We then present a two-level dialog policy model that plans a high-level topic sequence and a low-level query sequence over the query graph to guide a knowledge aware response generator. In particular, to foster forward-looking knowledge planning decisions for better dialog coherence, we devise a heterogeneous graph neural network to incorporate neighbouring vertex information, or possible future RL action information, into each vertex (as an RL action) representation. Experiment results on two benchmark dialog datasets demonstrate that our framework can outperform strong baselines in terms of dialog coherence, informativeness, and engagingness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3195866373",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing",
    "doi": "https://doi.org/10.1145/3474087",
    "publication_date": "2021-05-31",
    "publication_year": 2021,
    "authors": "Gunasekaran Manogaran; Hassan Qudrat‐Ullah; Qin Xin",
    "corresponding_authors": "",
    "abstract": "research-article Share on Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing Authors: Gunasekaran Manogaran Big Data Scientist, University of California, Davis, USA Big Data Scientist, University of California, Davis, USAView Profile , Hassan Qudrat-Ullah Professor of Decision Sciences, School of Administrative Studies, York University, Toronto, Canada Professor of Decision Sciences, School of Administrative Studies, York University, Toronto, CanadaView Profile , Qin Xin Full Professor of Computer Science, Faculty of Science and Technology, University of the Faroe Islands, Faroe Islands. Full Professor of Computer Science, Faculty of Science and Technology, University of the Faroe Islands, Faroe Islands.View Profile Authors Info & Claims ACM Transactions on Asian and Low-Resource Language Information ProcessingVolume 20Issue 3May 2021 Article No.: 37epp 1–3https://doi.org/10.1145/3474087Online:01 September 2021Publication History 0citation38DownloadsMetricsTotal Citations0Total Downloads38Last 12 Months38Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3198113428",
    "type": "article"
  },
  {
    "title": "An Investigational Approach for Vowels of the Salar Language Based on a Database of Speech Acoustic Parameters",
    "doi": "https://doi.org/10.1145/3459927",
    "publication_date": "2021-09-24",
    "publication_year": 2021,
    "authors": "Jun Ma; Hongzhi Yu; Yan Xu; Kaiying Deng",
    "corresponding_authors": "",
    "abstract": "According to relevant specifications, this article divides, marks, and extracts the acquired speech signals of the Salar language, and establishes the speech acoustic parameter database of the Salar language. Then, the vowels of the Salar language are analyzed and studied by using the parameter database. The vowel bitmap (average value at the beginning of words), the vowel bitmap (average value at the abdomen of words), the vowel bitmap (average value at the ending of words), and the vowel bitmap (average value) are obtained. Through the vowel bitmaps, we can observe the vowel in different positions of the word, the overall appearance of an obtuse triangle. The high vowel [i], [o], and low vowel [a] occupy three vertices, respectively. Among the three lines, [i] to [o] are the longest, [i] to [a] are the second longest, and [a] to [o] are the shortest. The lines between [a] to [o] and [a] and [i] are asymmetric. Combining with the vowel bitmap, the vowels were discretized, and the second formant (F2) frequency parameter was used as the coordinate of the X axis, and the first formant (F1) frequency was used as the coordinate of the Y axis to draw the region where the vowel was located, and then the vowel pattern was formed. These studies provide basic data and parameters for the future development of modern phonetics such as the database of Sarah language speech, speech recognition, and speech synthesis. It also provides the basic parameters of speech acoustics for the rare minority acoustic research work of the national language project.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3204419850",
    "type": "article"
  },
  {
    "title": "Confidence Indexing of Automated Detected Synsets: A Case Study on Contemporary Turkish Dictionary",
    "doi": "https://doi.org/10.1145/3469724",
    "publication_date": "2021-11-01",
    "publication_year": 2021,
    "authors": "Erhan Turan; Umut Orhan",
    "corresponding_authors": "",
    "abstract": "In this study, a novel confidence indexing algorithm is proposed to minimize human labor in controlling the reliability of automatically extracted synsets from a non-machine-readable monolingual dictionary. Contemporary Turkish Dictionary of Turkish Language Association is used as the monolingual dictionary data. First, the synonym relations are extracted by traditional text processing methods from dictionary definitions and a graph is prepared in Lemma-Sense network architecture. After each synonym relation is labeled by a proper confidence index, synonym pairs with desired confidence indexes are analyzed to detect synsets with a spanning tree-based method. This approach can label synsets with one of three cumulative confidence levels (CL-1, CL-2, and CL-3). According to the confidence levels, synsets are compared with KeNet which is the only open access Turkish Wordnet. Consequently, while most matches with the synsets of KeNet is determined in CL-1 and CL-2 confidence levels, the synsets determined at CL-3 level reveal errors in the dictionary definitions. This novel approach does not find only the reliability of automatically detected synsets, but it can also point out errors of detected synsets from the dictionary.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3209951149",
    "type": "article"
  },
  {
    "title": "A Statistical Language Model for Pre-Trained Sequence Labeling: A Case Study on Vietnamese",
    "doi": "https://doi.org/10.1145/3483524",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Xianwen Liao; Yongzhong Huang; Peng Yang; Lei Chen",
    "corresponding_authors": "",
    "abstract": "By defining the computable word segmentation unit and studying its probability characteristics, we establish an unsupervised statistical language model (SLM) for a new pre-trained sequence labeling framework in this article. The proposed SLM is an optimization model, and its objective is to maximize the total binding force of all candidate word segmentation units in sentences under the condition of no annotated datasets and vocabularies. To solve SLM, we design a recursive divide-and-conquer dynamic programming algorithm. By integrating SLM with the popular sequence labeling models, Vietnamese word segmentation, part-of-speech tagging and named entity recognition experiments are performed. The experimental results show that our SLM can effectively promote the performance of sequence labeling tasks. Just using less than 10% of training data and without using a dictionary, the performance of our sequence labeling framework is better than the state-of-the-art Vietnamese word segmentation toolkit VnCoreNLP on the cross-dataset test. SLM has no hyper-parameter to be tuned, and it is completely unsupervised and applicable to any other analytic language. Thus, it has good domain adaptability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4200144452",
    "type": "article"
  },
  {
    "title": "Handwritten Annotation Spotting in Printed Documents Using Top-Down Visual Saliency Models",
    "doi": "https://doi.org/10.1145/3485468",
    "publication_date": "2021-12-13",
    "publication_year": 2021,
    "authors": "Shilpa Pandey; Gaurav Harit",
    "corresponding_authors": "",
    "abstract": "In this article, we address the problem of localizing text and symbolic annotations on the scanned image of a printed document. Previous approaches have considered the task of annotation extraction as binary classification into printed and handwritten text. In this work, we further subcategorize the annotations as underlines, encirclements, inline text, and marginal text. We have collected a new dataset of 300 documents constituting all classes of annotations marked around or in-between printed text. Using the dataset as a benchmark, we report the results of two saliency formulations—CRF Saliency and Discriminant Saliency, for predicting salient patches, which can correspond to different types of annotations. We also compare our work with recent semantic segmentation techniques using deep models. Our analysis shows that Discriminant Saliency can be considered as the preferred approach for fast localization of patches containing different types of annotations. The saliency models were learned on a small dataset, but still, give comparable performance to the deep networks for pixel-level semantic segmentation. We show that saliency-based methods give better outcomes with limited annotated data compared to more sophisticated segmentation techniques that require a large training set to learn the model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4200293464",
    "type": "article"
  }
]