[
  {
    "title": "The entity-relationship model—toward a unified view of data",
    "doi": "https://doi.org/10.1145/320434.320440",
    "publication_date": "1976-03-01",
    "publication_year": 1976,
    "authors": "Peter Pin-Shan Chen",
    "corresponding_authors": "Peter Pin-Shan Chen",
    "abstract": "A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented.",
    "cited_by_count": 4540,
    "openalex_id": "https://openalex.org/W2945526390",
    "type": "article"
  },
  {
    "title": "DBSCAN Revisited, Revisited",
    "doi": "https://doi.org/10.1145/3068335",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Erich Schubert; Jörg Sander; Martin Ester; Hans Peter Kriegel; Xiaowei Xu",
    "corresponding_authors": "",
    "abstract": "At SIGMOD 2015, an article was presented with the title “DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation” that won the conference’s best paper award. In this technical correspondence, we want to point out some inaccuracies in the way DBSCAN was represented, and why the criticism should have been directed at the assumption about the performance of spatial index structures such as R-trees and not at an algorithm that can use such indexes. We will also discuss the relationship of DBSCAN performance and the indexability of the dataset, and discuss some heuristics for choosing appropriate DBSCAN parameters. Some indicators of bad parameters will be proposed to help guide future users of this algorithm in choosing parameters such as to obtain both meaningful results and good performance. In new experiments, we show that the new SIGMOD 2015 methods do not appear to offer practical benefits if the DBSCAN parameters are well chosen and thus they are primarily of theoretical interest. In conclusion, the original DBSCAN algorithm with effective indexes and reasonably chosen parameter values performs competitively compared to the method proposed by Gan and Tao.",
    "cited_by_count": 2100,
    "openalex_id": "https://openalex.org/W2740924709",
    "type": "article"
  },
  {
    "title": "TinyDB: an acquisitional query processing system for sensor networks",
    "doi": "https://doi.org/10.1145/1061318.1061322",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Samuel Madden; Michael J. Franklin; Joseph M. Hellerstein; Wei Hong",
    "corresponding_authors": "",
    "abstract": "We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired ( sampled ) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.",
    "cited_by_count": 1942,
    "openalex_id": "https://openalex.org/W2154721480",
    "type": "article"
  },
  {
    "title": "Extending the database relational model to capture more meaning",
    "doi": "https://doi.org/10.1145/320107.320109",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "E. F. Codd",
    "corresponding_authors": "E. F. Codd",
    "abstract": "During the last three or four years several investigators have been exploring “semantic models” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.",
    "cited_by_count": 1573,
    "openalex_id": "https://openalex.org/W2122789628",
    "type": "article"
  },
  {
    "title": "On optimistic methods for concurrency control",
    "doi": "https://doi.org/10.1145/319566.319567",
    "publication_date": "1981-06-01",
    "publication_year": 1981,
    "authors": "H. T. Kung; John T. Robinson",
    "corresponding_authors": "",
    "abstract": "Most current approaches to concurrency control in database systems rely on locking of data objects as a control mechanism. In this paper, two families of nonlocking concurrency controls are presented. The methods used are “optimistic” in the sense that they rely mainly on transaction backup as a control mechanism, “hoping” that conflicts between transactions will not occur. Applications for which these methods should be more efficient than locking are discussed.",
    "cited_by_count": 1438,
    "openalex_id": "https://openalex.org/W2133386065",
    "type": "article"
  },
  {
    "title": "Database abstractions",
    "doi": "https://doi.org/10.1145/320544.320546",
    "publication_date": "1977-06-01",
    "publication_year": 1977,
    "authors": "John Miles Smith; Diane C. P. Smith",
    "corresponding_authors": "",
    "abstract": "Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given.",
    "cited_by_count": 1412,
    "openalex_id": "https://openalex.org/W2039601877",
    "type": "article"
  },
  {
    "title": "The Grid File",
    "doi": "https://doi.org/10.1145/348.318586",
    "publication_date": "1984-03-23",
    "publication_year": 1984,
    "authors": "J. Nievergelt; Hans Hinterberger; Kenneth C. Sevcik",
    "corresponding_authors": "",
    "abstract": "Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of file structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory , which are the keys to a dynamic file structure called the grid file . This file system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper bound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures.",
    "cited_by_count": 1160,
    "openalex_id": "https://openalex.org/W2149173084",
    "type": "article"
  },
  {
    "title": "Semantics and complexity of SPARQL",
    "doi": "https://doi.org/10.1145/1567274.1567278",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Jorge Eduardo Pérez Pérez; Marcelo Arenas; Claudio Gutiérrez",
    "corresponding_authors": "",
    "abstract": "SPARQL is the standard language for querying RDF data. In this article, we address systematically the formal study of the database aspects of SPARQL, concentrating in its graph pattern matching facility. We provide a compositional semantics for the core part of SPARQL, and study the complexity of the evaluation of several fragments of the language. Among other complexity results, we show that the evaluation of general SPARQL patterns is PSPACE-complete. We identify a large class of SPARQL patterns, defined by imposing a simple and natural syntactic restriction, where the query evaluation problem can be solved more efficiently. This restriction gives rise to the class of well-designed patterns. We show that the evaluation problem is coNP-complete for well-designed patterns. Moreover, we provide several rewriting rules for well-designed patterns whose application may have a considerable impact in the cost of evaluating SPARQL queries.",
    "cited_by_count": 1112,
    "openalex_id": "https://openalex.org/W2131785201",
    "type": "article"
  },
  {
    "title": "System R",
    "doi": "https://doi.org/10.1145/320455.320457",
    "publication_date": "1976-06-01",
    "publication_year": 1976,
    "authors": "M. M. Astrahan; Michael W. Blasgen; Donald D. Chamberlin; Kapali P. Eswaran; Jim Gray; Patricia P. Griffiths; William F. King; Raymond A. Lorie; Paul McJones; J. W. Mehl; G. R. Putzolu; Irving L. Traiger; Bradford W. Wade; Victor Watson",
    "corresponding_authors": "",
    "abstract": "System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment. This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.",
    "cited_by_count": 1061,
    "openalex_id": "https://openalex.org/W2116436709",
    "type": "article"
  },
  {
    "title": "ARIES",
    "doi": "https://doi.org/10.1145/128765.128770",
    "publication_date": "1992-03-01",
    "publication_year": 1992,
    "authors": "C. Mohan; Don Haderle; Bruce G. Lindsay; Hamid Pirahesh; Peter Schwarz",
    "corresponding_authors": "",
    "abstract": "DB2 TM , IMS, and Tandem TM systems. ARIES is applicable not only to database management systems but also to persistent object-oriented languages, recoverable file systems and transaction-based operating systems. ARIES has been implemented, to varying degrees, in IBM's OS/2 TM Extended Edition Database Manager, DB2, Workstation Data Save Facility/VM, Starburst and QuickSilver, and in the University of Wisconsin's EXODUS and Gamma database machine.",
    "cited_by_count": 1039,
    "openalex_id": "https://openalex.org/W2104954161",
    "type": "article"
  },
  {
    "title": "A Majority consensus approach to concurrency control for multiple copy databases",
    "doi": "https://doi.org/10.1145/320071.320076",
    "publication_date": "1979-06-01",
    "publication_year": 1979,
    "authors": "Robert Thomas",
    "corresponding_authors": "Robert Thomas",
    "abstract": "A “majority consensus” algorithm which represents a new solution to the update synchronization problem for multiple copy databases is presented. The algorithm embodies distributed control and can function effectively in the presence of communication and database site outages. The correctness of the algorithm is demonstrated and the cost of using it is analyzed. Several examples that illustrate aspects of the algorithm operation are included in the Appendix.",
    "cited_by_count": 1011,
    "openalex_id": "https://openalex.org/W2110765100",
    "type": "article"
  },
  {
    "title": "The functional data model and the data languages DAPLEX",
    "doi": "https://doi.org/10.1145/319540.319561",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "David W. Shipman",
    "corresponding_authors": "David W. Shipman",
    "abstract": "DAPLEX is a database language which incorporates: This paper presents and motivates the DAPLEX language and the underlying data model on which it is based.",
    "cited_by_count": 929,
    "openalex_id": "https://openalex.org/W2147869465",
    "type": "article"
  },
  {
    "title": "Distance browsing in spatial databases",
    "doi": "https://doi.org/10.1145/320248.320255",
    "publication_date": "1999-06-01",
    "publication_year": 1999,
    "authors": "Gı́sli R. Hjaltason; Hanan Samet",
    "corresponding_authors": "",
    "abstract": "We compare two different techniques for browsing through a collection of spatial objects stored in an R-tree spatial data structure on the basis of their distances from an arbitrary spatial query object. The conventional approach is one that makes use of a k -nearest neighbor algorithm where k is known prior to the invocation of the algorithm. Thus if m &lt; k neighbors are needed, the k -nearest neighbor algorithm has to be reinvoked for m neighbors, thereby possibly performing some redundant computations. The second approach is incremental in the sense that having obtained the k nearest neighbors, the k + 1 st neighbor can be obtained without having to calculate the k + 1 nearest neighbors from scratch. The incremental approach is useful when processing complex queries where one of the conditions involves spatial proximity (e.g., the nearest city to Chicago with population greater than a million), in which case a query engine can make use of a pipelined strategy. We present a general incremental nearest neighbor algorithm that is applicable to a large class of hierarchical spatial data structures. This algorithm is adapted to the R-tree and its performance is compared to an existing k -nearest neighbor algorithm for R-trees [Rousseopoulos et al. 1995]. Experiments show that the incremental nearest neighbor algorithm significantly outperforms the k -nearest neighbor algorithm for distance browsing queries in a spatial database that uses the R-tree as a spatial index. Moreover, the incremental nearest neighbor algorithm usually outperforms the k -nearest neighber algorithm when applied to the k -nearest neighbor problem for the R-tree, although the improvement is not nearly as large as for distance browsing queries. In fact, we prove informally that at any step in its execution the incremental nearest neighbor algorithm is optimal with respect to the spatial data structure that is employed. Furthermore, based on some simplifying assumptions, we prove that in two dimensions the number of distance computations and leaf nodes accesses made by the algorithm for finding k neighbors is O ( k + k ).",
    "cited_by_count": 904,
    "openalex_id": "https://openalex.org/W1990111898",
    "type": "article"
  },
  {
    "title": "The design and implementation of INGRES",
    "doi": "https://doi.org/10.1145/320473.320476",
    "publication_date": "1976-09-01",
    "publication_year": 1976,
    "authors": "Michael Stonebraker; Gerald Held; Eugene Wong; Peter Kreps",
    "corresponding_authors": "",
    "abstract": "The currently operational (March 1976) version of the INGRES database management system is described. This multiuser system gives a relational view of data, supports two high level nonprocedural data sublanguages, and runs as a collection of user processes on top of the UNIX operating system for Digital Equipment Corporation PDP 11/40, 11/45, and 11/70 computers. Emphasis is on the design decisions and tradeoffs related to (1) structuring the system into processes, (2) embedding one command language in a general purpose programming language, (3) the algorithms implemented to process interactions, (4) the access methods implemented, (5) the concurrency and recovery control currently provided, and (6) the data structures used for system catalogs and the role of the database administrator. Also discussed are (1) support for integrity constraints (which is only partly operational), (2) the not yet supported features concerning views and protection, and (3) future plans concerning the system.",
    "cited_by_count": 900,
    "openalex_id": "https://openalex.org/W2133384222",
    "type": "article"
  },
  {
    "title": "Progressive skyline computation in database systems",
    "doi": "https://doi.org/10.1145/1061318.1061320",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Dimitris Papadias; Yufei Tao; Greg Fu; Bernhard Seeger",
    "corresponding_authors": "",
    "abstract": "The skyline of a d -dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing.",
    "cited_by_count": 895,
    "openalex_id": "https://openalex.org/W2116396741",
    "type": "article"
  },
  {
    "title": "Multiple-query optimization",
    "doi": "https://doi.org/10.1145/42201.42203",
    "publication_date": "1988-03-01",
    "publication_year": 1988,
    "authors": "Timos Sellis",
    "corresponding_authors": "Timos Sellis",
    "abstract": "Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.",
    "cited_by_count": 815,
    "openalex_id": "https://openalex.org/W2058978608",
    "type": "article"
  },
  {
    "title": "A foundation for representing and querying moving objects",
    "doi": "https://doi.org/10.1145/352958.352963",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "Ralf Hartmut Güting; Michael H. Böhlen; Martin Erwig; Christian S. Jensen; Nikos A. Lorentzos; Markus Schneider; Michalis Vazirgiannis",
    "corresponding_authors": "",
    "abstract": "Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects . Two fundamental abstractions are moving point and moving region , describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a “moving real” to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension.",
    "cited_by_count": 766,
    "openalex_id": "https://openalex.org/W2032521898",
    "type": "article"
  },
  {
    "title": "Database description with SDM",
    "doi": "https://doi.org/10.1145/319587.319588",
    "publication_date": "1981-09-01",
    "publication_year": 1981,
    "authors": "Michael Hammer; Dennis Mc Leod",
    "corresponding_authors": "",
    "abstract": "SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it. SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system.",
    "cited_by_count": 765,
    "openalex_id": "https://openalex.org/W2130474477",
    "type": "article"
  },
  {
    "title": "Multivalued dependencies and a new normal form for relational databases",
    "doi": "https://doi.org/10.1145/320557.320571",
    "publication_date": "1977-09-01",
    "publication_year": 1977,
    "authors": "Ronald Fagin",
    "corresponding_authors": "Ronald Fagin",
    "abstract": "A new type of dependency, which includes the well-known functional dependencies as a special case, is defined for relational databases. By using this concept, a new (“fourth”) normal form for relation schemata is defined. This fourth normal form is strictly stronger than Codd's “improved third normal form” (or “Boyce-Codd normal form”). It is shown that every relation schema can be decomposed into a family of relation schemata in fourth normal form without loss of information (that is, the original relation can be obtained from the new relations by taking joins).",
    "cited_by_count": 712,
    "openalex_id": "https://openalex.org/W2110810394",
    "type": "article"
  },
  {
    "title": "Extendible hashing—a fast access method for dynamic files",
    "doi": "https://doi.org/10.1145/320083.320092",
    "publication_date": "1979-09-01",
    "publication_year": 1979,
    "authors": "Ronald Fagin; Jürg Nievergelt; Nicholas Pippenger; H. Raymond Strong",
    "corresponding_authors": "",
    "abstract": "Extendible hashing is a new access technique, in which the user is guaranteed no more than two page faults to locate the data associated with a given unique identifier, or key. Unlike conventional hashing, extendible hashing has a dynamic structure that grows and shrinks gracefully as the database grows and shrinks. This approach simultaneously solves the problem of making hash tables that are extendible and of making radix search trees that are balanced. We study, by analysis and simulation, the performance of extendible hashing. The results indicate that extendible hashing provides an attractive alternative to other access methods, such as balanced trees.",
    "cited_by_count": 662,
    "openalex_id": "https://openalex.org/W2087966340",
    "type": "article"
  },
  {
    "title": "Flexible support for multiple access control policies",
    "doi": "https://doi.org/10.1145/383891.383894",
    "publication_date": "2001-06-01",
    "publication_year": 2001,
    "authors": "Sushil Jajodia; Pierangela Samarati; Maria Luisa Sapino; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "Although several access control policies can be devised for controlling access to information, all existing authorization models, and the corresponding enforcement mechanisms, are based on a specific policy (usually the closed policy). As a consequence, although different policy choices are possible in theory, in practice only a specific policy can actually be applied within a given system. In this paper, we present a unified framework that can enforce multiple access control policies within a single system. The framework is based on a language through which users can specify security policies to be enforced on specific accesses. The language allows the specification of both positive and negative authorizations and incorporates notions of authorization derivation, conflict resolution, and decision strategies. Different strategies may be applied to different users, groups, objects, or roles, based on the needs of the security policy. The overall result is a flexible and powerful, yet simple, framework that can easily capture many of the traditional access control policies as well as protection requirements that exist in real-world applications, but are seldom supported by existing systems. The major advantage of our approach is that it can be used to specify different access control policies that can all coexist in the same system and be enforced by the same security server.",
    "cited_by_count": 657,
    "openalex_id": "https://openalex.org/W2012419258",
    "type": "article"
  },
  {
    "title": "The temporal query language TQuel",
    "doi": "https://doi.org/10.1145/22952.22956",
    "publication_date": "1987-06-01",
    "publication_year": 1987,
    "authors": "Richard T. Snodgrass",
    "corresponding_authors": "Richard T. Snodgrass",
    "abstract": "Recently, attention has been focused on temporal databases , representing an enterprise over time. We have developed a new language, Tquel , to query a temporal database. TQuel was designed to be a minimal extension, both syntactically and semantically, of Quel, the query language in the Ingres relational database management system. This paper discusses the language informally, then provides a tuple relational calculus semantics for the TQuel statements that differ from their Quel counterparts, including the modification statements. The three additional temporal constructs defined in Tquel are shown to be direct semantic analogues of Quel's where clause and target list. We also discuss reducibility of the semantics to Quel's semantics when applied to a static database. TQuel is compared with ten other query languages supporting time.",
    "cited_by_count": 621,
    "openalex_id": "https://openalex.org/W2067481201",
    "type": "article"
  },
  {
    "title": "Update semantics of relational views",
    "doi": "https://doi.org/10.1145/319628.319634",
    "publication_date": "1981-12-01",
    "publication_year": 1981,
    "authors": "François Bancilhon; Nicolas Spyratos",
    "corresponding_authors": "",
    "abstract": "A database view is a portion of the data structured in a way suitable to a specific application. Updates on views must be translated into updates on the underlying database. This paper studies the translation process in the relational model. The procedure is as follows: first, a “complete” set of updates is defined such that together with every update the set contains a “return” update, that is, one that brings the view back to the original state; given two updates in the set, their composition is also in the set. To translate a complete set, we define a mapping called a “translator,” that associates with each view update a unique database update called a “translation.” The constraint on a translation is to take the database to a state mapping onto the updated view. The constraint on the translator is to be a morphism. We propose a method for defining translators. Together with the user-defined view, we define a “complementary” view such that the database could be computed from the view and its complement. We show that a view can have many different complements and that the choice of a complement determines an update policy. Thus, we fix a view complement and we define the translation of a given view update in such a way that the complement remains invariant (“translation under constant complement”). The main result of the paper states that, given a complete set U of view updates, U has a translator if and only if U is translatable under constant complement.",
    "cited_by_count": 598,
    "openalex_id": "https://openalex.org/W2113582770",
    "type": "article"
  },
  {
    "title": "iDistance",
    "doi": "https://doi.org/10.1145/1071610.1071612",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "H. V. Jagadish; Beng Chin Ooi; Kian‐Lee Tan; Yu Cui; Rui Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, we present an efficient B + -tree based indexing method, called iDistance, for K-nearest neighbor (KNN) search in a high-dimensional metric space. iDistance partitions the data based on a space- or data-partitioning strategy, and selects a reference point for each partition. The data points in each partition are transformed into a single dimensional value based on their similarity with respect to the reference point. This allows the points to be indexed using a B + -tree structure and KNN search to be performed using one-dimensional range search. The choice of partition and reference points adapts the index structure to the data distribution.We conducted extensive experiments to evaluate the iDistance technique, and report results demonstrating its effectiveness. We also present a cost model for iDistance KNN search, which can be exploited in query optimization.",
    "cited_by_count": 594,
    "openalex_id": "https://openalex.org/W1994655805",
    "type": "article"
  },
  {
    "title": "Testing implications of data dependencies",
    "doi": "https://doi.org/10.1145/320107.320115",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "David Maier; Alberto O. Mendelzon; Yehoshua Sagiv",
    "corresponding_authors": "",
    "abstract": "Presented is a computation method—the chase —for testing implication of data dependencies by a set of data dependencies. The chase operates on tableaux similar to those of Aho, Sagiv, and Ullman. The chase includes previous tableau computation methods as special cases. By interpreting tableaux alternately as mappings or as templates for relations, it is possible to test implication of join dependencies (including multivalued dependencies) and functional dependencies by a set of dependencies.",
    "cited_by_count": 591,
    "openalex_id": "https://openalex.org/W1985581502",
    "type": "article"
  },
  {
    "title": "Scheduling real-time transactions",
    "doi": "https://doi.org/10.1145/132271.132276",
    "publication_date": "1992-09-01",
    "publication_year": 1992,
    "authors": "Robert Abbott; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Scheduling real-time transactions: a performance evaluation Authors: Robert K. Abbott Digital Equipment Corp., Littleton, MA Digital Equipment Corp., Littleton, MAView Profile , Hector Garcia-Molina Stanford Univ., Stanford, CA Stanford Univ., Stanford, CAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 17Issue 3Sept. 1992 pp 513–560https://doi.org/10.1145/132271.132276Published:01 September 1992Publication History 420citation2,804DownloadsMetricsTotal Citations420Total Downloads2,804Last 12 Months154Last 6 weeks31 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 577,
    "openalex_id": "https://openalex.org/W2023671193",
    "type": "article"
  },
  {
    "title": "What's hot and what's not: tracking most frequent items dynamically",
    "doi": "https://doi.org/10.1145/1061318.1061325",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Graham Cormode; S. Muthukrishnan",
    "corresponding_authors": "",
    "abstract": "Most database management systems maintain statistics on the underlying relation. One of the important statistics is that of the “hot items” in the relation: those that appear many times (most frequently, or more than some threshold). For example, end-biased histograms keep the hot items as part of the histogram and are used in selectivity estimation. Hot items are used as simple outliers in data mining, and in anomaly detection in many applications.We present new methods for dynamically determining the hot items at any time in a relation which is undergoing deletion operations as well as inserts. Our methods maintain small space data structures that monitor the transactions on the relation, and, when required, quickly output all hot items without rescanning the relation in the database. With user-specified probability, all hot items are correctly reported. Our methods rely on ideas from “group testing.” They are simple to implement, and have provable quality, space, and time guarantees. Previously known algorithms for this problem that make similar quality and performance guarantees cannot handle deletions, and those that handle deletions cannot make similar guarantees without rescanning the database. Our experiments with real and synthetic data show that our algorithms are accurate in dynamically tracking the hot items independent of the rate of insertions and deletions.",
    "cited_by_count": 571,
    "openalex_id": "https://openalex.org/W2167973519",
    "type": "article"
  },
  {
    "title": "Developing a natural language interface to complex data",
    "doi": "https://doi.org/10.1145/320251.320253",
    "publication_date": "1978-06-01",
    "publication_year": 1978,
    "authors": "Gary G. Hendrix; Earl D. Sacerdoti; Daniel Sagalowicz; Jonathan Slocum",
    "corresponding_authors": "",
    "abstract": "Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed.",
    "cited_by_count": 539,
    "openalex_id": "https://openalex.org/W2043794661",
    "type": "article"
  },
  {
    "title": "Efficient locking for concurrent operations on B-trees",
    "doi": "https://doi.org/10.1145/319628.319663",
    "publication_date": "1981-12-01",
    "publication_year": 1981,
    "authors": "Philip L. Lehman; S. Bing Yao",
    "corresponding_authors": "",
    "abstract": "The B-tree and its variants have been found to be highly useful (both theoretically and in practice) for storing large amounts of information, especially on secondary storage devices. We examine the problem of overcoming the inherent difficulty of concurrent operations on such structures, using a practical storage model. A single additional “link” pointer in each node allows a process to easily recover from tree modifications performed by other concurrent processes. Our solution compares favorably with earlier solutions in that the locking scheme is simpler (no read-locks are used) and only a (small) constant number of nodes are locked by any update process at any given time. An informal correctness proof for our system is given.",
    "cited_by_count": 533,
    "openalex_id": "https://openalex.org/W2108179552",
    "type": "article"
  },
  {
    "title": "ACM transactions on database systems",
    "doi": "https://doi.org/10.1145/320434.320436",
    "publication_date": "1976-03-01",
    "publication_year": 1976,
    "authors": "David K. Hsiao",
    "corresponding_authors": "David K. Hsiao",
    "abstract": "Record-keeping and decision-making in industry and government are increasingly based on data stored in computer processable databases. Thus the need for improved computer technology for building, managing, and using these databases is clearly evident. This need is particularly acute in a complex society where the interrelationships among various aspects of the society must be identified and represented. The data which must be used to represent these relationships are growing more complex in nature and becoming greater in size. Furthermore, the increasing on-line use of computer systems and the proliferation and mass introduction of multilevel secondary storage suggests that future computer systems will be primarily oriented toward database management. The large size of future on-line databases will require the computer system to manage local as well as physical resources. The management of logical resources is concerned with the organization, access, update, storage, and sharing of the data and programs in the database. In addition, the sharing of data means that the database system must be capable of providing privacy protection and of controlling access to the users' data. The term data is interpreted broadly to include textual, numeric, and signal data as well as data found in structured records. The aim of ACM Transactions on Database Systems (TODS) is to serve as a focal point for an integrated dissemination of database research and development on storage and processor hardware, system software, applications, information science, information analysis, and file management. These areas are particularly relevant to the following ACM Special Interest Groups: Business Data Processing (SIGBDP), Information Retrieval (SIGIR), and Management of Data (SIGMOD). TODS will also embrace parts of the Management/Database Systems and the Information Retrieval and Language Processing sections of Communications of the ACM . High quality papers on all aspects of computer database systems will be published in TODS. The scope of TODS emphasizes data structures; storage organization; data collection and dissemination; search and retrieval strategies; update strategies; access control techniques; data integrity; security and protection; design and implementation of database software; database related languages including data description languages, query languages, and procedural and nonprocedural data manipulation languages; language processing; analysis and classification of data; database utilities; data translation techniques; distributed database problems and techniques; database recovery and restart; database restructuring; adaptive data structures; concurrent access techniques; database computer hardware architecture; performance and evaluation; intelligent front ends; and related subjects such as privacy and economic issues.",
    "cited_by_count": 522,
    "openalex_id": "https://openalex.org/W2018978908",
    "type": "article"
  },
  {
    "title": "On semantic issues connected with incomplete information databases",
    "doi": "https://doi.org/10.1145/320083.320088",
    "publication_date": "1979-09-01",
    "publication_year": 1979,
    "authors": "Witold Lipski",
    "corresponding_authors": "Witold Lipski",
    "abstract": "Various approaches to interpreting queries in a database with incomplete information are discussed. A simple model of a database is described, based on attributes which can take values in specified attribute domains. Information incompleteness means that instead of having a single value of an attribute, we have a subset of the attribute domain, which represents our knowledge that the actual value, though unknown, is one of the values in this subset. This extends the idea of Codd's null value, corresponding to the case when this subset is the whole attribute domain. A simple query language to communicate with such a system is described and its various semantics are precisely defined. We emphasize the distinction between two different interpretations of the query language—the external one, which refers the queries directly to the real world modeled in an incomplete way by the system, and the internal one, under which the queries refer to the system's information about this world, rather than to the world itself. Both external and internal interpretations are provided with the corresponding sets of axioms which serve as a basis for equivalent transformations of queries. The technique of equivalent transformations of queries is then extensively exploited for evaluating the interpretation of (i.e. the response to) a query.",
    "cited_by_count": 517,
    "openalex_id": "https://openalex.org/W2003017562",
    "type": "article"
  },
  {
    "title": "Join indices",
    "doi": "https://doi.org/10.1145/22952.22955",
    "publication_date": "1987-06-01",
    "publication_year": 1987,
    "authors": "Patrick Valduriez",
    "corresponding_authors": "Patrick Valduriez",
    "abstract": "In new application areas of relational database systems, such as artificial intelligence, the join operator is used more extensively than in conventional applications. In this paper, we propose a simple data structure, called a join index, for improving the performance of joins in the context of complex queries. For most of the joins, updates to join indices incur very little overhead. Some properties of a join index are (i) its efficient use of memory and adaptiveness to parallel execution, (ii) its compatibility with other operations (including select and union), (iii) its support for abstract data type join predicates, (iv) its support for multirelation clustering, and (v) its use in representing directed graphs and in evaluating recursive queries. Finally, the analysis of the join algorithm using join indices shows its excellent performance.",
    "cited_by_count": 512,
    "openalex_id": "https://openalex.org/W2295596515",
    "type": "article"
  },
  {
    "title": "A simple algorithm for finding frequent elements in streams and bags",
    "doi": "https://doi.org/10.1145/762471.762473",
    "publication_date": "2003-03-01",
    "publication_year": 2003,
    "authors": "Richard M. Karp; Scott Shenker; Christos H. Papadimitriou",
    "corresponding_authors": "",
    "abstract": "We present a simple, exact algorithm for identifying in a multiset the items with frequency more than a threshold θ. The algorithm requires two passes, linear time, and space 1/θ. The first pass is an on-line algorithm, generalizing a well-known algorithm for finding a majority element, for identifying a set of at most 1/θ items that includes, possibly among others, all items with frequency greater than θ.",
    "cited_by_count": 509,
    "openalex_id": "https://openalex.org/W2113139394",
    "type": "article"
  },
  {
    "title": "Disjunctive datalog",
    "doi": "https://doi.org/10.1145/261124.261126",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "Thomas Eiter; Georg Gottlob; Heikki Mannila",
    "corresponding_authors": "",
    "abstract": "We consider disjunctive Datalog, a powerful database query language based on disjunctive logic programming. Briefly, disjunctive Datalog is a variant of Datalog where disjunctions may appear in the rule heads; advanced versions also allow for negation in the bodies which can be handled according to a semantics for negation in disjunctive logic programming. In particular, we investigate three different semantics for disjunctive Datalog: the minimal model semantics the perfect model semantics, and the stable model semantics. For each of these semantics, the expressive power and complexity are studied. We show that the possibility variants of these semantics express the same set of queries. In fact, they precisely capture the complexity class Σ P 2 . Thus, unless the Polynomial Hierarchy collapses, disjunctive Datalog is more expressive that normal logic programming with negation. These results are not only of theoretical interest; we demonstrate that problems relevant in practice such as computing the optimal tour value in the Traveling Salesman Problem and eigenvector computations can be handled in disjunctive Datalog, but not Datalog with negation (unless the Polynomial Hierarchy collapses). In addition, we study modularity properties of disjunctive Datalog and investigate syntactic restrictions of the formalisms.",
    "cited_by_count": 500,
    "openalex_id": "https://openalex.org/W2295882966",
    "type": "article"
  },
  {
    "title": "Query processing in a system for distributed databases (SDD-1)",
    "doi": "https://doi.org/10.1145/319628.319650",
    "publication_date": "1981-12-01",
    "publication_year": 1981,
    "authors": "Philip A. Bernstein; Nathan Goodman; Eugene Wong; Christopher L. Reeve; James B. Rothnie",
    "corresponding_authors": "",
    "abstract": "This paper describes the techniques used to optimize relational queries in the SDD-1 distributed database system. Queries are submitted to SDD-1 in a high-level procedural language called Datalanguage. Optimization begins by translating each Datalanguage query into a relational calculus form called an envelope , which is essentially an aggregate-free QUEL query. This paper is primarily concerned with the optimization of envelopes. Envelopes are processed in two phases. The first phase executes relational operations at various sites of the distributed database in order to delimit a subset of the database that contains all data relevant to the envelope. This subset is called a reduction of the database. The second phase transmits the reduction to one designated site, and the query is executed locally at that site. The critical optimization problem is to perform the reduction phase efficiently. Success depends on designing a good repertoire of operators to use during this phase, and an effective algorithm for deciding which of these operators to use in processing a given envelope against a given database. The principal reduction operator that we employ is called a semijoin . In this paper we define the semijoin operator, explain why semijoin is an effective reduction operator, and present an algorithm that constructs a cost-effective program of semijoins, given an envelope and a database.",
    "cited_by_count": 491,
    "openalex_id": "https://openalex.org/W2023195086",
    "type": "article"
  },
  {
    "title": "Multiversion concurrency control—theory and algorithms",
    "doi": "https://doi.org/10.1145/319996.319998",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "Philip A. Bernstein; Nathan Goodman",
    "corresponding_authors": "",
    "abstract": "Concurrency control is the activity of synchronizing operations issued by concurrently executing programs on a shared database. The goal is to produce an execution that has the same effect as a serial (noninterleaved) one. In a multiversion database system, each write on a data item produces a new copy (or version ) of that data item. This paper presents a theory for analyzing the correctness of concurrency control algorithms for multiversion database systems. We use the theory to analyze some new algorithms and some previously published ones.",
    "cited_by_count": 490,
    "openalex_id": "https://openalex.org/W2435606122",
    "type": "article"
  },
  {
    "title": "A language facility for designing database-intensive applications",
    "doi": "https://doi.org/10.1145/320141.320150",
    "publication_date": "1980-06-01",
    "publication_year": 1980,
    "authors": "John Mylopoulos; Philip A. Bernstein; Harry K. T. Wong",
    "corresponding_authors": "",
    "abstract": "TAXIS, a language for the design of interactive information systems (e.g., credit card verification, student-course registration, and airline reservations) is described. TAXIS offers (relational) database management facilities, a means of specifying semantic integrity constraints, and an exception-handling mechanism, integrated into a single language through the concepts of class, property , and the IS-A (generalization) relationship . A description of the main constructs of TAXIS is included and their usefulness illustrated with examples.",
    "cited_by_count": 487,
    "openalex_id": "https://openalex.org/W2027750453",
    "type": "article"
  },
  {
    "title": "Using semantic knowledge for transaction processing in a distributed database",
    "doi": "https://doi.org/10.1145/319983.319985",
    "publication_date": "1983-06-01",
    "publication_year": 1983,
    "authors": "Héctor García-Molina",
    "corresponding_authors": "Héctor García-Molina",
    "abstract": "This paper investigates how the semantic knowledge of an application can be used in a distributed database to process transactions efficiently and to avoid some of the delays associated with failures. The main idea is to allow nonserializable schedules which preserve consistency and which are acceptable to the system users. To produce such schedules, the transaction processing mechanism receives semantic information from the users in the form of transaction semantic types, a division of transactions into steps, compatibility sets, and countersteps. Using these notions, we propose a mechanism which allows users to exploit their semantic knowledge in an organized fashion. The strengths and weaknesses of this approach are discussed.",
    "cited_by_count": 484,
    "openalex_id": "https://openalex.org/W2030689196",
    "type": "article"
  },
  {
    "title": "A linear-time probabilistic counting algorithm for database applications",
    "doi": "https://doi.org/10.1145/78922.78925",
    "publication_date": "1990-06-01",
    "publication_year": 1990,
    "authors": "Kyu-Young Whang; Brad T. Vander-Zanden; Howard M. Taylor",
    "corresponding_authors": "",
    "abstract": "We present a probabilistic algorithm for counting the number of unique values in the presence of duplicates. This algorithm has O ( q ) time complexity, where q is the number of values including duplicates, and produces an estimation with an arbitrary accuracy prespecified by the user using only a small amount of space. Traditionally, accurate counts of unique values were obtained by sorting, which has O ( q log q ) time complexity. Our technique, called linear counting , is based on hashing. We present a comprehensive theoretical and experimental analysis of linear counting. The analysis reveals an interesting result: A load factor (number of unique values/hash table size) much larger than 1.0 (e.g., 12) can be used for accurate estimation (e.g., 1% of error). We present this technique with two important applications to database problems: namely, (1) obtaining the column cardinality (the number of unique values in a column of a relation) and (2) obtaining the join selectivity (the number of unique values in the join column resulting from an unconditional join divided by the number of unique join column values in the relation to he joined). These two parameters are important statistics that are used in relational query optimization and physical database design.",
    "cited_by_count": 472,
    "openalex_id": "https://openalex.org/W2008365755",
    "type": "article"
  },
  {
    "title": "IFO: a formal semantic database model",
    "doi": "https://doi.org/10.1145/32204.32205",
    "publication_date": "1987-11-01",
    "publication_year": 1987,
    "authors": "Serge Abiteboul; Richard Hull",
    "corresponding_authors": "",
    "abstract": "A new, formally defined database model is introduced that combines fundamental principles of “semantic” database modeling in a coherent fashion. Using a graph-based formalism, the IFO model provides mechanisms for representing structured objects, and functional and ISA relationships between them. A number of fundamental results concerning semantic data modeling are obtained in the context of the IFO model. Notably, the types of object structure that can arise as a result of multiple uses of ISA relationships and object construction are described. Also, a natural, formal definition of update propagation is given, and it is shown that (under certain conditions) a correct update always exists.",
    "cited_by_count": 467,
    "openalex_id": "https://openalex.org/W1968945891",
    "type": "article"
  },
  {
    "title": "Preference formulas in relational queries",
    "doi": "https://doi.org/10.1145/958942.958946",
    "publication_date": "2003-12-01",
    "publication_year": 2003,
    "authors": "Jan Chomicki",
    "corresponding_authors": "Jan Chomicki",
    "abstract": "The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making.We propose here a simple, logical framework for formulating preferences as preference formulas . The framework does not impose any restrictions on the preference relations, and allows arbitrary operation and predicate signatures in preference formulas. It also makes the composition of preference relations straightforward. We propose a simple, natural embedding of preference formulas into relational algebra (and SQL) through a single winnow operator parameterized by a preference formula. The embedding makes possible the formulation of complex preference queries, for example, involving aggregation, by piggybacking on existing SQL constructs. It also leads in a natural way to the definition of further, preference-related concepts like ranking. Finally, we present general algebraic laws governing the winnow operator and its interactions with other relational algebra operators. The preconditions on the applicability of the laws are captured by logical formulas. The laws provide a formal foundation for the algebraic optimization of preference queries. We demonstrate the usefulness of our approach through numerous examples.",
    "cited_by_count": 466,
    "openalex_id": "https://openalex.org/W2002699418",
    "type": "article"
  },
  {
    "title": "Computational problems related to the design of normal form relational schemas",
    "doi": "https://doi.org/10.1145/320064.320066",
    "publication_date": "1979-03-01",
    "publication_year": 1979,
    "authors": "Catriel Beeri; Philip A. Bernstein",
    "corresponding_authors": "",
    "abstract": "Problems related to functional dependencies and the algorithmic design of relational schemas are examined. Specifically, the following results are presented: (1) a tree model of derivations of functional dependencies from other functional dependencies; (2) a linear-time algorithm to test if a functional dependency is in the closure of a set of functional dependencies; (3) a quadratic-time implementation of Bernstein's third normal form schema synthesis algorithm. Furthermore, it is shown that most interesting algorithmic questions about Boyce-Codd normal form and keys are NP -complete and are therefore probably not amenable to fast algorithmic solutions.",
    "cited_by_count": 466,
    "openalex_id": "https://openalex.org/W2135105491",
    "type": "article"
  },
  {
    "title": "Tracing the lineage of view data in a warehousing environment",
    "doi": "https://doi.org/10.1145/357775.357777",
    "publication_date": "2000-06-01",
    "publication_year": 2000,
    "authors": "Yingwei Cui; Jennifer Widom; Janet L. Wiener",
    "corresponding_authors": "",
    "abstract": "We consider the view data lineage problem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formally define the lineage problem, develop lineage tracing algorithms for relational views with aggregation, and propose mechanisms for performing consistent lineage tracing in a multisource data warehousing environment. Our result can form the basis of a tool that allows analysts to browse warehouse data, select view tuples of interest, and then “drill-through” to examine the exact source tuples that produced the view tuples of interest.",
    "cited_by_count": 455,
    "openalex_id": "https://openalex.org/W1983178058",
    "type": "article"
  },
  {
    "title": "Synthesizing third normal form relations from functional dependencies",
    "doi": "https://doi.org/10.1145/320493.320489",
    "publication_date": "1976-12-01",
    "publication_year": 1976,
    "authors": "Philip A. Bernstein",
    "corresponding_authors": "Philip A. Bernstein",
    "abstract": "It has been proposed that the description of a relational database can be formulated as a set of functional relationships among database attributes. These functional relationships can then be used to synthesize algorithmically a relational scheme. It is the purpose of this paper to present an effective procedure for performing such a synthesis. The schema that results from this procedure is proved to be in Codd's third normal form and to contain the fewest possible number of relations. Problems with earlier attempts to construct such a procedure are also discussed.",
    "cited_by_count": 455,
    "openalex_id": "https://openalex.org/W2023663289",
    "type": "article"
  },
  {
    "title": "Conditional functional dependencies for capturing data inconsistencies",
    "doi": "https://doi.org/10.1145/1366102.1366103",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Wenfei Fan; Floris Geerts; Xibei Jia; Anastasios Kementsietsidis",
    "corresponding_authors": "",
    "abstract": "We propose a class of integrity constraints for relational databases, referred to as conditional functional dependencies (CFDs), and study their applications in data cleaning. In contrast to traditional functional dependencies (FDs) that were developed mainly for schema design, CFDs aim at capturing the consistency of data by enforcing bindings of semantically related values. For static analysis of CFDs we investigate the consistency problem , which is to determine whether or not there exists a nonempty database satisfying a given set of CFDs, and the implication problem , which is to decide whether or not a set of CFDs entails another CFD. We show that while any set of transitional FDs is trivially consistent, the consistency problem is NP-complete for CFDs, but it is in PTIME when either the database schema is predefined or no attributes involved in the CFDs have a finite domain. For the implication analysis of CFDs, we provide an inference system analogous to Armstrong's axioms for FDs, and show that the implication problem is coNP-complete for CFDs in contrast to the linear-time complexity for their traditional counterpart. We also present an algorithm for computing a minimal cover of a set of CFDs. Since CFDs allow data bindings, in some cases CFDs may be physically large, complicating the detection of constraint violations. We develop techniques for detecting CFD violations in SQL as well as novel techniques for checking multiple constraints by a single query. We also provide incremental methods for checking CFDs in response to changes to the database. We experimentally verify the effectiveness of our CFD-based methods for inconsistency detection. This work not only yields a constraint theory for CFDs but is also a step toward a practical constraint-based method for improving data quality.",
    "cited_by_count": 451,
    "openalex_id": "https://openalex.org/W2171332293",
    "type": "article"
  },
  {
    "title": "Index-driven similarity search in metric spaces (Survey Article)",
    "doi": "https://doi.org/10.1145/958942.958948",
    "publication_date": "2003-12-01",
    "publication_year": 2003,
    "authors": "Gı́sli R. Hjaltason; Hanan Samet",
    "corresponding_authors": "",
    "abstract": "Similarity search is a very important operation in multimedia databases and other database applications involving complex objects, and involves finding objects in a data set S similar to a query object q , based on some similarity measure. In this article, we focus on methods for similarity search that make the general assumption that similarity is represented with a distance metric d . Existing methods for handling similarity search in this setting typically fall into one of two classes. The first directly indexes the objects based on distances (distance-based indexing), while the second is based on mapping to a vector space (mapping-based approach). The main part of this article is dedicated to a survey of distance-based indexing methods, but we also briefly outline how search occurs in mapping-based methods. We also present a general framework for performing search based on distances, and present algorithms for common types of queries that operate on an arbitrary \"search hierarchy.\" These algorithms can be applied on each of the methods presented, provided a suitable search hierarchy is defined.",
    "cited_by_count": 441,
    "openalex_id": "https://openalex.org/W2065528935",
    "type": "article"
  },
  {
    "title": "The theory of joins in relational databases",
    "doi": "https://doi.org/10.1145/320083.320091",
    "publication_date": "1979-09-01",
    "publication_year": 1979,
    "authors": "A. V. Aho; Catriel Beeri; Jeffrey D. Ullman",
    "corresponding_authors": "",
    "abstract": "Answering queries in a relational database often requires that the natural join of two or more relations be computed. However, the result of a join may not be what one expects. In this paper we give efficient algorithms to determine whether the join of several relations has the intuitively expected value (is lossless ) and to determine whether a set of relations has a subset with a lossy join. These algorithms assume that all data dependencies are functional. We then discuss the extension of our techniques to the case where data dependencies are multivalued.",
    "cited_by_count": 427,
    "openalex_id": "https://openalex.org/W2032386524",
    "type": "article"
  },
  {
    "title": "Fuzzy functional dependencies and lossless join decomposition of fuzzy relational database systems",
    "doi": "https://doi.org/10.1145/42338.42344",
    "publication_date": "1988-06-01",
    "publication_year": 1988,
    "authors": "K. V. S. V. N. Raju; A.K. Majumdar",
    "corresponding_authors": "",
    "abstract": "This paper deals with the application of fuzzy logic in a relational database environment with the objective of capturing more meaning of the data. It is shown that with suitable interpretations for the fuzzy membership functions, a fuzzy relational data model can be used to represent ambiguities in data values as well as impreciseness in the association among them. Relational operators for fuzzy relations have been studied, and applicability of fuzzy logic in capturing integrity constraints has been investigated. By introducing a fuzzy resemblance measure EQUAL for comparing domain values, the definition of classical functional dependency has been generalized to fuzzy functional dependency (ffd). The implication problem of ffds has been examined and a set of sound and complete inference axioms has been proposed. Next, the problem of lossless join decomposition of fuzzy relations for a given set of fuzzy functional dependencies is investigated. It is proved that with a suitable restriction on EQUAL, the design theory of a classical relational database with functional dependencies can be extended to fuzzy relations satisfying fuzzy functional dependencies.",
    "cited_by_count": 421,
    "openalex_id": "https://openalex.org/W2017978889",
    "type": "article"
  },
  {
    "title": "Vertical partitioning algorithms for database design",
    "doi": "https://doi.org/10.1145/1994.2209",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "Shamkant B. Navathe; Stefano Ceri; Gio Wiederhold; Jinglie Dou",
    "corresponding_authors": "",
    "abstract": "This paper addresses the vertical partitioning of a set of logical records or a relation into fragments. The rationale behind vertical partitioning is to produce fragments, groups of attribute columns, that “closely match” the requirements of transactions. Vertical partitioning is applied in three contexts: a database stored on devices of a single type, a database stored in different memory levels, and a distributed database. In a two-level memory hierarchy, most transactions should be processed using the fragments in primary memory. In distributed databases, fragment allocation should maximize the amount of local transaction processing. Fragments may be nonoverlapping or overlapping. A two-phase approach for the determination of fragments is proposed; in the first phase, the design is driven by empirical objective functions which do not require specific cost information. The second phase performs cost optimization by incorporating the knowledge of a specific application environment. The algorithms presented in this paper have been implemented, and examples of their actual use are shown.",
    "cited_by_count": 421,
    "openalex_id": "https://openalex.org/W2039795745",
    "type": "article"
  },
  {
    "title": "An ontological analysis of the relationship construct in conceptual modeling",
    "doi": "https://doi.org/10.1145/331983.331989",
    "publication_date": "1999-12-01",
    "publication_year": 1999,
    "authors": "Yair Wand; Veda C. Storey; Ron Weber",
    "corresponding_authors": "",
    "abstract": "Conceptual models or semantic data models were developed to capture the meaning of an application domain as perceived by someone. Moreover, concepts employed in semantic data models have recently been adopted in object-oriented approaches to systems analysis and design. To employ conceptual modeling constructs effectively, their meanings have to be defined rigorously. Often, however, rigorous definitions of these constructs are missing. This situation occurs especially in the case of the relationship construct. Empirical evidence shows that use of relationships is often problematical as a way of communicating the meaning of an application domain. For example, users of conceptual modeling methodologies are frequently confused about whether to show an association between things via a relationship, an entity, or an attribute. Because conceptual models are intended to capture knowledge about a real-world domain, we take the view that the meaning of modeling constructs should be sought in models of reality. Accordingly, we use ontology, which is the branch of philosophy dealing with models of reality, to analyze the meaning of common conceptual modeling constructs. Our analysis provides a precise definition of several conceptual modeling constructs. Based on our analysis, we derive rules for the use of relationships in entity-relationship conceptual modeling. Moreover, we show how the rules resolve ambiguities that exist in current practice and how they can enrich the capacity of an entity-relationship conceptual model to capture knowledge about an application domain.",
    "cited_by_count": 419,
    "openalex_id": "https://openalex.org/W2093823648",
    "type": "article"
  },
  {
    "title": "An authorization mechanism for a relational database system",
    "doi": "https://doi.org/10.1145/320473.320482",
    "publication_date": "1976-09-01",
    "publication_year": 1976,
    "authors": "Patricia P. Griffiths; Bradford W. Wade",
    "corresponding_authors": "",
    "abstract": "A multiuser database system must selectively permit users to share data, while retaining the ability to restrict data access. There must be a mechanism to provide protection and security, permitting information to be accessed only by properly authorized users. Further, when tables or restricted views of tables are created and destroyed dynamically, the granting, authentication, and revocation of authorization to use them must also be dynamic. Each of these issues and their solutions in the context of the relational database management system System R are discussed. When a database user creates a table, he is fully and solely authorized to perform upon it actions such as read, insert, update, and delete. He may explicitly grant to any other user any or all of his privileges on the table. In addition he may specify that that user is authorized to further grant these privileges to still other users. The result is a directed graph of granted privileges originating from the table creator. At some later time a user A may revoke some or all of the privileges which he previously granted to another user B. This action usually revokes the entire subgraph of the grants originating from A's grant to B. It may be, however, that B will still possess the revoked privileges by means of a grant from another user C, and therefore some or all of B's grants should not be revoked. This problem is discussed in detail, and an algorithm for detecting exactly which of B's grants should be revoked is presented.",
    "cited_by_count": 402,
    "openalex_id": "https://openalex.org/W2101465039",
    "type": "article"
  },
  {
    "title": "On optimizing an SQL-like nested query",
    "doi": "https://doi.org/10.1145/319732.319745",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Won Bae Kim",
    "corresponding_authors": "Won Bae Kim",
    "abstract": "SQL is a high-level nonprocedural data language which has received wide recognition in relational databases. One of the most interesting features of SQL is the nesting of query blocks to an arbitrary depth. An SQL-like query nested to an arbitrary depth is shown to be composed of five basic types of nesting. Four of them have not been well understood and more work needs to be done to improve their execution efficiency. Algorithms are developed that transform queries involving these basic types of nesting into semantically equivalent queries that are amenable to efficient processing by existing query-processing subsystems. These algorithms are then combined into a coherent strategy for processing a general nested query of arbitrary complexity.",
    "cited_by_count": 393,
    "openalex_id": "https://openalex.org/W2008116177",
    "type": "article"
  },
  {
    "title": "Decomposition—a strategy for query processing",
    "doi": "https://doi.org/10.1145/320473.320479",
    "publication_date": "1976-09-01",
    "publication_year": 1976,
    "authors": "Eugene Wong; Karel Youssefi",
    "corresponding_authors": "",
    "abstract": "Strategy for processing multivariable queries in the database management system INGRES is considered. The general procedure is to decompose the query into a sequence of one-variable queries by alternating between (a) reduction: breaking off components of the query which are joined to it by a single variable, and (b) tuple substitution: substituting for one of the variables a tuple at a time. Algorithms for reduction and for choosing the variable to be substituted are given. In most cases the latter decision depends on estimation of costs; heuristic procedures for making such estimates are outlined.",
    "cited_by_count": 392,
    "openalex_id": "https://openalex.org/W2169012378",
    "type": "article"
  },
  {
    "title": "Locally adaptive dimensionality reduction for indexing large time series databases",
    "doi": "https://doi.org/10.1145/568518.568520",
    "publication_date": "2002-06-01",
    "publication_year": 2002,
    "authors": "Kaushik Chakrabarti; Eamonn Keogh; Sharad Mehrotra; Michael J. Pazzani",
    "corresponding_authors": "",
    "abstract": "Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this article, we introduce a new dimensionality reduction technique, which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower-bounding, but very tight, Euclidean distance approximation, and show how they can support fast exact searching and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.",
    "cited_by_count": 390,
    "openalex_id": "https://openalex.org/W2110704543",
    "type": "article"
  },
  {
    "title": "System level concurrency control for distributed database systems",
    "doi": "https://doi.org/10.1145/320251.320260",
    "publication_date": "1978-06-01",
    "publication_year": 1978,
    "authors": "Daniel J. Rosenkrantz; Richard E. Stearns; Philip Lewis",
    "corresponding_authors": "",
    "abstract": "A distributed database system is one in which the database is spread among several sites and application programs “move” from site to site to access and update the data they need. The concurrency control is that portion of the system that responds to the read and write requests of the application programs. Its job is to maintain the global consistency of the distributed database while ensuring that the termination of the application programs is not prevented by phenomena such as deadlock. We assume each individual site has its own local concurrency control which responds to requests at that site and can only communicate with concurrency controls at other sites when an application program moves from site to site, terminates, or aborts. This paper presents designs for several distributed concurrency controls and demonstrates that they work correctly. It also investigates some of the implications of global consistency of a distributed database and discusses phenomena that can prevent termination of application programs.",
    "cited_by_count": 387,
    "openalex_id": "https://openalex.org/W2009998184",
    "type": "article"
  },
  {
    "title": "Implementation of logical query languages for databases",
    "doi": "https://doi.org/10.1145/3979.3980",
    "publication_date": "1985-09-01",
    "publication_year": 1985,
    "authors": "Jeffrey D. Ullman",
    "corresponding_authors": "Jeffrey D. Ullman",
    "abstract": "We examine methods of implementing queries about relational databases in the case where these queries are expressed in first-order logic as a collection of Horn clauses. Because queries may be defined recursively, straightforward methods of query evaluation do not always work, and a variety of strategies have been proposed to handle subsets of recursive queries. We express such query evaluation techniques as “capture rules” on a graph representing clauses and predicates. One essential property of capture rules is that they can be applied independently, thus providing a clean interface for query-evaluation systems that use several different strategies in different situations. Another is that there be an efficient test for the applicability of a given rule. We define basic capture rules corresponding to application of operators from relational algebra, a top-down capture rule corresponding to “backward chaining,” that is, repeated resolution of goals, a bottom-up rule, corresponding to “forward chaining,” where we attempt to deduce all true facts in a given class, and a “sideways” rule that allows us to pass results from one goal to another.",
    "cited_by_count": 387,
    "openalex_id": "https://openalex.org/W2027276583",
    "type": "article"
  },
  {
    "title": "Concurrency control performance modeling: alternatives and implications",
    "doi": "https://doi.org/10.1145/32204.32220",
    "publication_date": "1987-11-01",
    "publication_year": 1987,
    "authors": "Rakesh Agrawal; Michael J. Carey; Miron Livny",
    "corresponding_authors": "",
    "abstract": "A number of recent studies have examined the performance of concurrency control algorithms for database management systems. The results reported to date, rather than being definitive, have tended to be contradictory. In this paper, rather than presenting “yet another algorithm performance study,” we critically investigate the assumptions made in the models used in past studies and their implications. We employ a fairly complete model of a database environment for studying the relative performance of three different approaches to the concurrency control problem under a variety of modeling assumptions. The three approaches studied represent different extremes in how transaction conflicts are dealt with, and the assumptions addressed pertain to the nature of the database system's resources, how transaction restarts are modeled, and the amount of information available to the concurrency control algorithm about transactions' reference strings. We show that differences in the underlying assumptions explain the seemingly contradictory performance results. We also address the question of how realistic the various assumptions are for actual database systems.",
    "cited_by_count": 384,
    "openalex_id": "https://openalex.org/W2102333161",
    "type": "article"
  },
  {
    "title": "Data exchange: getting to the core",
    "doi": "https://doi.org/10.1145/1061318.1061323",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Ronald Fagin; Phokion G. Kolaitis; Lucian Popa",
    "corresponding_authors": "",
    "abstract": "Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. Given a source instance, there may be many solutions to the data exchange problem, that is, many target instances that satisfy the constraints of the data exchange problem. In an earlier article, we identified a special class of solutions that we call universal . A universal solution has homomorphisms into every possible solution, and hence is a “most general possible” solution. Nonetheless, given a source instance, there may be many universal solutions. This naturally raises the question of whether there is a “best” universal solution, and hence a best solution for data exchange. We answer this question by considering the well-known notion of the core of a structure, a notion that was first studied in graph theory, and has also played a role in conjunctive-query processing. The core of a structure is the smallest substructure that is also a homomorphic image of the structure. All universal solutions have the same core (up to isomorphism); we show that this core is also a universal solution, and hence the smallest universal solution. The uniqueness of the core of a universal solution together with its minimality make the core an ideal solution for data exchange. We investigate the computational complexity of producing the core. Well-known results by Chandra and Merlin imply that, unless P = NP, there is no polynomial-time algorithm that, given a structure as input, returns the core of that structure as output. In contrast, in the context of data exchange, we identify natural and fairly broad conditions under which there are polynomial-time algorithms for computing the core of a universal solution. We also analyze the computational complexity of the following decision problem that underlies the computation of cores: given two graphs G and H , is H the core of G ? Earlier results imply that this problem is both NP-hard and coNP-hard. Here, we pinpoint its exact complexity by establishing that it is a DP-complete problem. Finally, we show that the core is the best among all universal solutions for answering existential queries, and we propose an alternative semantics for answering queries in data exchange settings.",
    "cited_by_count": 384,
    "openalex_id": "https://openalex.org/W2159686758",
    "type": "article"
  },
  {
    "title": "On the correct translation of update operations on relational views",
    "doi": "https://doi.org/10.1145/319732.319740",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Umeshwar Dayal; Philip A. Bernstein",
    "corresponding_authors": "",
    "abstract": "Most relational database systems provide a facility for supporting user views. Permitting this level of abstraction has the danger, however, that update requests issued by a user within the context of his view may not translate correctly into equivalent updates on the underlying database. The purpose of this paper is to formalize the notion of update translation and derive conditions under which translation procedures will produce correct translations of view updates.",
    "cited_by_count": 371,
    "openalex_id": "https://openalex.org/W2040293060",
    "type": "article"
  },
  {
    "title": "GALILEO: a strongly-typed, interactive conceptual language",
    "doi": "https://doi.org/10.1145/3857.3859",
    "publication_date": "1985-06-01",
    "publication_year": 1985,
    "authors": "Antonio Albano; Luca Cardelli; Renzo Orsini",
    "corresponding_authors": "",
    "abstract": "Galileo, a programming language for database applications, is presented. Galileo is a strongly-typed, interactive programming language designed specifically to support semantic data model features (classification, aggregation, and specialization), as well as the abstraction mechanisms of modern programming languages (types, abstract types, and modularization). The main contributions of Galileo are (a) a flexible type system to model database structure and semantic integrity constraints; (b) the inclusion of type hierarchies to support the specialization abstraction mechanisms of semantic data models; (c) a modularization mechanism to structure data and operations into interrelated units (d) the integration of abstraction mechanisms into an expression-based language that allows interactive use of the database without resorting to a new stand-alone query language. Galileo will be used in the immediate future as a tool for database design and, in the long term, as a high-level interface for DBMSs.",
    "cited_by_count": 370,
    "openalex_id": "https://openalex.org/W2089042876",
    "type": "article"
  },
  {
    "title": "Transaction management in the R* distributed database management system",
    "doi": "https://doi.org/10.1145/7239.7266",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "C. Mohan; Bruce G. Lindsay; Ron Obermarck",
    "corresponding_authors": "",
    "abstract": "This paper deals with the transaction management aspects of the R* distributed database system. It concentrates primarily on the description of the R * commit protocols, Presumed Abort (PA) and Presumed Commit (PC). PA and PC are extensions of the well-known, two-phase (2P) commit protocol. PA is optimized for read-only transactions and a class of multisite update transactions, and PC is optimized for other classes of multisite update transactions. The optimizations result in reduced intersite message traffic and log writes, and, consequently, a better response time. The paper also discusses R * 's approach toward distributed deadlock detection and resolution.",
    "cited_by_count": 365,
    "openalex_id": "https://openalex.org/W2169379483",
    "type": "article"
  },
  {
    "title": "Logic-based approach to semantic query optimization",
    "doi": "https://doi.org/10.1145/78922.78924",
    "publication_date": "1990-06-01",
    "publication_year": 1990,
    "authors": "Upen S. Chakravarthy; John Grant; Jack Minker",
    "corresponding_authors": "",
    "abstract": "The purpose of semantic query optimization is to use semantic knowledge (e.g., integrity constraints) for transforming a query into a form that may be answered more efficiently than the original version. In several previous papers we described and proved the correctness of a method for semantic query optimization in deductive databases couched in first-order logic. This paper consolidates the major results of these papers emphasizing the techniques and their applicability for optimizing relational queries. Additionally, we show how this method subsumes and generalizes earlier work on semantic query optimization. We also indicate how semantic query optimization techniques can be extended to databases that support recursion and integrity constraints that contain disjunction, negation, and recursion.",
    "cited_by_count": 350,
    "openalex_id": "https://openalex.org/W2006519067",
    "type": "article"
  },
  {
    "title": "Some high level language constructs for data of type relation",
    "doi": "https://doi.org/10.1145/320557.320568",
    "publication_date": "1977-09-01",
    "publication_year": 1977,
    "authors": "Joachim W. Schmidt",
    "corresponding_authors": "Joachim W. Schmidt",
    "abstract": "For the extension of high level languages by data types of mode relation, three language constructs are proposed and discussed: a repetition statement controlled by relations, predicates as a generalization of Boolean expressions, and a constructor for relations using predicates. The language constructs are developed step by step starting with a set of elementary operations on relations. They are designed to fit into PASCAL without introducing too many additional concepts.",
    "cited_by_count": 346,
    "openalex_id": "https://openalex.org/W2110338707",
    "type": "article"
  },
  {
    "title": "Join processing in database systems with large main memories",
    "doi": "https://doi.org/10.1145/6314.6315",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Leonard Shapiro",
    "corresponding_authors": "Leonard Shapiro",
    "abstract": "We study algorithms for computing the equijoin of two relations in a system with a standard architecture hut with large amounts of main memory. Our algorithms are especially efficient when the main memory available is a significant fraction of the size of one of the relations to he joined; but they can be applied whenever there is memory equal to approximately the square root of the size of one relation. We present a new algorithm which is a hybrid of two hash-based algorithms and which dominates the other algorithms we present, including sort-merge. Even in a virtual memory environment, the hybrid algorithm dominates all the others we study. Finally, we describe how three popular tools to increase the efficiency of joins, namely filters, Babb arrays, and semijoins, can he grafted onto any of our algorithms.",
    "cited_by_count": 338,
    "openalex_id": "https://openalex.org/W2169486917",
    "type": "article"
  },
  {
    "title": "Principles of database buffer management",
    "doi": "https://doi.org/10.1145/1994.2022",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "Wolfgang Effelsberg; Theo Haerder",
    "corresponding_authors": "",
    "abstract": "This paper discusses the implementation of a database buffer manager as a component of a DBMS. The interface between calling components of higher system layers and the buffer manager is described; the principal differences between virtual memory paging and database buffer management are outlined; the notion of referencing versus addressing of database pages is introduced; and the concept of fixing pages in the buffer to prevent uncontrolled replacement is explained. Three basic tasks have to be performed by the buffer manager: buffer search, allocation of frames to concurrent transactions, and page replacement. For each of these tasks, implementation alternatives are discussed and illustrated by examples from a performance evaluation project of a CODASYL DBMS.",
    "cited_by_count": 337,
    "openalex_id": "https://openalex.org/W2163772659",
    "type": "article"
  },
  {
    "title": "Consensus on transaction commit",
    "doi": "https://doi.org/10.1145/1132863.1132867",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Jim Gray; Leslie Lamport",
    "corresponding_authors": "",
    "abstract": "The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. The Paxos Commit algorithm runs a Paxos consensus algorithm on the commit/abort decision of each participant to obtain a transaction commit protocol that uses 2 F + 1 coordinators and makes progress if at least F + 1 of them are working properly. Paxos Commit has the same stable-storage write delay, and can be implemented to have the same message delay in the fault-free case as Two-Phase Commit, but it uses more messages. The classic Two-Phase Commit algorithm is obtained as the special F = 0 case of the Paxos Commit algorithm.",
    "cited_by_count": 335,
    "openalex_id": "https://openalex.org/W2114407468",
    "type": "article"
  },
  {
    "title": "Optimizing bitmap indices with efficient compression",
    "doi": "https://doi.org/10.1145/1132863.1132864",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Kesheng Wu; Ekow Otoo; Arie Shoshani",
    "corresponding_authors": "",
    "abstract": "Bitmap indices are efficient for answering queries on low-cardinality attributes. In this article, we present a new compression scheme called Word-Aligned Hybrid (WAH) code that makes compressed bitmap indices efficient even for high-cardinality attributes. We further prove that the new compressed bitmap index, like the best variants of the B-tree index, is optimal for one-dimensional range queries. More specifically, the time required to answer a one-dimensional range query is a linear function of the number of hits. This strongly supports the well-known observation that compressed bitmap indices are efficient for multidimensional range queries because results of one-dimensional range queries computed with bitmap indices can be easily combined to answer multidimensional range queries. Our timing measurements on range queries not only confirm the linear relationship between the query response time and the number of hits, but also demonstrate that WAH compressed indices answer queries faster than the commonly used indices including projection indices, B-tree indices, and other compressed bitmap indices.",
    "cited_by_count": 328,
    "openalex_id": "https://openalex.org/W2003421875",
    "type": "article"
  },
  {
    "title": "An algorithm for concurrency control and recovery in replicated distributed databases",
    "doi": "https://doi.org/10.1145/1994.2207",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "Philip A. Bernstein; Nathan Goodman",
    "corresponding_authors": "",
    "abstract": "In a one-copy distributed database, each data item is stored at exactly one site. In a replicated database, some data items may be stored at multiple sites. The main motivation is improved reliability: by storing important data at multiple sites, the DBS can operate even though some sites have failed. This paper describes an algorithm for handling replicated data, which allows users to operate on data so long as one copy is “available.” A copy is “available” when (i) its site is up, and (ii) the copy is not out-of-date because of an earlier crash. The algorithm handles clean, detectable site failures, but not Byzantine failures or network partitions.",
    "cited_by_count": 320,
    "openalex_id": "https://openalex.org/W2033223918",
    "type": "article"
  },
  {
    "title": "Differential files",
    "doi": "https://doi.org/10.1145/320473.320484",
    "publication_date": "1976-09-01",
    "publication_year": 1976,
    "authors": "Dennis G. Severance; Guy M. Lohman",
    "corresponding_authors": "",
    "abstract": "The representation of a collection of data in terms of its differences from some preestablished point of reference is a basic storage compaction technique which finds wide applicability. This paper describes a differential database representation which is shown to be an efficient method for storing large and volatile databases. The technique confines database modifications to a relatively small area of physical storage and as a result offers two significant operational advantages. First, because the “reference point” for the database is inherently static, it can be simply and efficiently stored. Second, since all modifications to the database are physically localized, the process of backup and the process of recovery are relatively fast and inexpensive.",
    "cited_by_count": 309,
    "openalex_id": "https://openalex.org/W2159886933",
    "type": "article"
  },
  {
    "title": "A simplied universal relation assumption and its properties",
    "doi": "https://doi.org/10.1145/319732.319735",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Ronald Fagin; Alberto O. Mendelzon; Jeffrey D. Ullman",
    "corresponding_authors": "",
    "abstract": "One problem concerning the universal relation assumption is the inability of known methods to obtain a database scheme design in the general case, where the real-world constraints are given by a set of dependencies that includes embedded multivalued dependencies. We propose a simpler method of describing the real world, where constraints are given by functional dependencies and a single join dependency. The relationship between this method of defining the real world and the classical methods is exposed. We characterize in terms of hypergraphs those multivalued dependencies that are the consequence of a given join dependency. Also characterized in terms of hypergraphs are those join dependencies that are equivalent to a set of multivalued dependencies.",
    "cited_by_count": 308,
    "openalex_id": "https://openalex.org/W2039610696",
    "type": "article"
  },
  {
    "title": "Efficient similarity joins for near-duplicate detection",
    "doi": "https://doi.org/10.1145/2000824.2000825",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Chuan Xiao; Wei Wang; Xuemin Lin; Jeffrey Xu Yu; Guoren Wang",
    "corresponding_authors": "",
    "abstract": "With the increasing amount of data and the need to integrate data from multiple data sources, one of the challenging issues is to identify near-duplicate records efficiently. In this article, we focus on efficient algorithms to find a pair of records such that their similarities are no less than a given threshold. Several existing algorithms rely on the prefix filtering principle to avoid computing similarity values for all possible pairs of records. We propose new filtering techniques by exploiting the token ordering information; they are integrated into the existing methods and drastically reduce the candidate sizes and hence improve the efficiency. We have also studied the implementation of our proposed algorithm in stand-alone and RDBMS-based settings. Experimental results show our proposed algorithms can outperform previous algorithms on several real datasets.",
    "cited_by_count": 305,
    "openalex_id": "https://openalex.org/W2065259291",
    "type": "article"
  },
  {
    "title": "PrivBayes",
    "doi": "https://doi.org/10.1145/3134428",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Jun Zhang; Graham Cormode; Cecilia M. Procopiuc; Divesh Srivastava; Xiaokui Xiao",
    "corresponding_authors": "",
    "abstract": "Privacy-preserving data publishing is an important problem that has been the focus of extensive study. The state-of-the-art solution for this problem is differential privacy, which offers a strong degree of privacy protection without making restrictive assumptions about the adversary. Existing techniques using differential privacy, however, cannot effectively handle the publication of high-dimensional data. In particular, when the input dataset contains a large number of attributes, existing methods require injecting a prohibitive amount of noise compared to the signal in the data, which renders the published data next to useless. To address the deficiency of the existing methods, this paper presents P riv B ayes , a differentially private method for releasing high-dimensional data. Given a dataset D , P riv B ayes first constructs a Bayesian network N , which (i) provides a succinct model of the correlations among the attributes in D and (ii) allows us to approximate the distribution of data in D using a set P of low-dimensional marginals of D . After that, P riv B ayes injects noise into each marginal in P to ensure differential privacy and then uses the noisy marginals and the Bayesian network to construct an approximation of the data distribution in D . Finally, P riv B ayes samples tuples from the approximate distribution to construct a synthetic dataset, and then releases the synthetic data. Intuitively, P riv B ayes circumvents the curse of dimensionality, as it injects noise into the low-dimensional marginals in P instead of the high-dimensional dataset D . Private construction of Bayesian networks turns out to be significantly challenging, and we introduce a novel approach that uses a surrogate function for mutual information to build the model more accurately. We experimentally evaluate P riv B ayes on real data and demonstrate that it significantly outperforms existing solutions in terms of accuracy.",
    "cited_by_count": 302,
    "openalex_id": "https://openalex.org/W4234726042",
    "type": "article"
  },
  {
    "title": "Prefix <i>B</i> -trees",
    "doi": "https://doi.org/10.1145/320521.320530",
    "publication_date": "1977-03-01",
    "publication_year": 1977,
    "authors": "Rudolf Bayer; Karl Unterauer",
    "corresponding_authors": "",
    "abstract": "Two modifications of B -trees are described, simple prefix B -trees and prefix B -trees. Both store only parts of keys, namely prefixes, in the index part of a B * -tree. In simple prefix B -trees those prefixes are selected carefully to minimize their length. In prefix B -trees the prefixes need not be fully stored, but are reconstructed as the tree is searched. Prefix B -trees are designed to combine some of the advantages of B -trees, digital search trees, and key compression techniques while reducing the processing overhead of compression techniques.",
    "cited_by_count": 280,
    "openalex_id": "https://openalex.org/W2008627910",
    "type": "article"
  },
  {
    "title": "Relational query coprocessing on graphics processors",
    "doi": "https://doi.org/10.1145/1620585.1620588",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Bingsheng He; Mian Lu; Ke Yang; Rui Fang; Naga K. Govindaraju; Qiong Luo; Pedro V. Sander",
    "corresponding_authors": "",
    "abstract": "Graphics processors (GPUs) have recently emerged as powerful coprocessors for general purpose computation. Compared with commodity CPUs, GPUs have an order of magnitude higher computation power as well as memory bandwidth. Moreover, new-generation GPUs allow writes to random memory locations, provide efficient interprocessor communication through on-chip local memory, and support a general purpose parallel programming model. Nevertheless, many of the GPU features are specialized for graphics processing, including the massively multithreaded architecture, the Single-Instruction-Multiple-Data processing style, and the execution model of a single application at a time. Additionally, GPUs rely on a bus of limited bandwidth to transfer data to and from the CPU, do not allow dynamic memory allocation from GPU kernels, and have little hardware support for write conflicts. Therefore, a careful design and implementation is required to utilize the GPU for coprocessing database queries. In this article, we present our design, implementation, and evaluation of an in-memory relational query coprocessing system, GDB, on the GPU. Taking advantage of the GPU hardware features, we design a set of highly optimized data-parallel primitives such as split and sort, and use these primitives to implement common relational query processing algorithms. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. Furthermore, we propose coprocessing techniques that take into account both the computation resources and the GPU-CPU data transfer cost so that each operator in a query can utilize suitable processors—the CPU, the GPU, or both—for an optimized overall performance. We have evaluated our GDB system on a machine with an Intel quad-core CPU and an NVIDIA GeForce 8800 GTX GPU. Our workloads include microbenchmark queries on memory-resident data as well as TPC-H queries that involve complex data types and multiple query operators on data sets larger than the GPU memory. Our results show that our GPU-based algorithms are 2--27x faster than their optimized CPU-based counterparts on in-memory data. Moreover, the performance of our coprocessing scheme is similar to, or better than, both the GPU-only and the CPU-only schemes.",
    "cited_by_count": 279,
    "openalex_id": "https://openalex.org/W2068418796",
    "type": "article"
  },
  {
    "title": "Pufferfish",
    "doi": "https://doi.org/10.1145/2514689",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Daniel Kifer; Ashwin Machanavajjhala",
    "corresponding_authors": "",
    "abstract": "In this article, we introduce a new and general privacy framework called Pufferfish. The Pufferfish framework can be used to create new privacy definitions that are customized to the needs of a given application. The goal of Pufferfish is to allow experts in an application domain, who frequently do not have expertise in privacy, to develop rigorous privacy definitions for their data sharing needs. In addition to this, the Pufferfish framework can also be used to study existing privacy definitions. We illustrate the benefits with several applications of this privacy framework: we use it to analyze differential privacy and formalize a connection to attackers who believe that the data records are independent; we use it to create a privacy definition called hedging privacy, which can be used to rule out attackers whose prior beliefs are inconsistent with the data; we use the framework to define and study the notion of composition in a broader context than before; we show how to apply the framework to protect unbounded continuous attributes and aggregate information; and we show how to use the framework to rigorously account for prior data releases.",
    "cited_by_count": 241,
    "openalex_id": "https://openalex.org/W2033092546",
    "type": "article"
  },
  {
    "title": "Fast and Accurate Time-Series Clustering",
    "doi": "https://doi.org/10.1145/3044711",
    "publication_date": "2017-06-01",
    "publication_year": 2017,
    "authors": "John Paparrizos; Luis Gravano",
    "corresponding_authors": "",
    "abstract": "The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but also because it is often a preprocessing step or subroutine for other techniques. In this article, we present k -Shape and k -MultiShapes ( k -MS), two novel algorithms for time-series clustering. k -Shape and k -MS rely on a scalable iterative refinement procedure. As their distance measure, k -Shape and k -MS use shape-based distance (SBD), a normalized version of the cross-correlation measure, to consider the shapes of time series while comparing them. Based on the properties of SBD, we develop two new methods, namely ShapeExtraction (SE) and MultiShapesExtraction (MSE), to compute cluster centroids that are used in every iteration to update the assignment of time series to clusters. k -Shape relies on SE to compute a single centroid per cluster based on all time series in each cluster. In contrast, k -MS relies on MSE to compute multiple centroids per cluster to account for the proximity and spatial distribution of time series in each cluster. To demonstrate the robustness of SBD, k -Shape, and k -MS, we perform an extensive experimental evaluation on 85 datasets against state-of-the-art distance measures and clustering methods for time series using rigorous statistical analysis. SBD, our efficient and parameter-free distance measure, achieves similar accuracy to Dynamic Time Warping (DTW), a highly accurate but computationally expensive distance measure that requires parameter tuning. For clustering, we compare k -Shape and k -MS against scalable and non-scalable partitional, hierarchical, spectral, density-based, and shapelet-based methods, with combinations of the most competitive distance measures. k -Shape outperforms all scalable methods in terms of accuracy. Furthermore, k -Shape also outperforms all non-scalable approaches, with one exception, namely k -medoids with DTW, which achieves similar accuracy. However, unlike k -Shape, this approach requires tuning of its distance measure and is significantly slower than k -Shape. k -MS performs similarly to k -Shape in comparison to rival methods, but k -MS is significantly more accurate than k -Shape. Beyond clustering, we demonstrate the effectiveness of k -Shape to reduce the search space of one-nearest-neighbor classifiers for time series. Overall, SBD, k -Shape, and k -MS emerge as domain-independent, highly accurate, and efficient methods for time-series comparison and clustering with broad applications.",
    "cited_by_count": 188,
    "openalex_id": "https://openalex.org/W2622816133",
    "type": "article"
  },
  {
    "title": "An adaptive data replication algorithm",
    "doi": "https://doi.org/10.1145/249978.249982",
    "publication_date": "1997-06-01",
    "publication_year": 1997,
    "authors": "Ouri Wolfson; Sushil Jajodia; Yixiu Huang",
    "corresponding_authors": "",
    "abstract": "This article addresses the performance of distributed database systems. Specifically, we present an algorithm for dynamic replication of an object in distributed systems. The algorithm is adaptive in the sence that it changes the replication scheme of the object i.e., the set of processors at which the object inreplicated) as changes occur in the read-write patern of the object (i.e., the number of reads and writes issued by each processor). The algorithm continuously moves the replication scheme towards an optimal one. We show that the algorithm can be combined with the concurrency control and recovery mechanisms of ta distributed database management system. The performance of the algorithm is analyzed theoretically and experimentally. On the way we provide a lower bound on the performance of any dynamic replication algorith.",
    "cited_by_count": 372,
    "openalex_id": "https://openalex.org/W2107483781",
    "type": "article"
  },
  {
    "title": "Path sharing and predicate evaluation for high-performance XML filtering",
    "doi": "https://doi.org/10.1145/958942.958947",
    "publication_date": "2003-12-01",
    "publication_year": 2003,
    "authors": "Yanlei Diao; Mehmet Altınel; Michael J. Franklin; Hao Zhang; Peter M. Fischer",
    "corresponding_authors": "",
    "abstract": "XML filtering systems aim to provide fast, on-the-fly matching of XML-encoded data to large numbers of query specifications containing constraints on both structure and content. It is now well accepted that approaches using event-based parsing and Finite State Machines (FSMs) can provide the basis for highly scalable structure-oriented XML filtering systems. The XFilter system [Altinel and Franklin 2000] was the first published FSM-based XML filtering approach. XFilter used a separate FSM per path query and a novel indexing mechanism to allow all of the FSMs to be executed simultaneously during the processing of a document. Building on the insights of the XFilter work, we describe a new method, called \"YFilter\" that combines all of the path queries into a single Nondeterministic Finite Automaton (NFA). YFilter exploits commonality among queries by merging common prefixes of the query paths such that they are processed at most once. The resulting shared processing provides tremendous improvements in structure matching performance but complicates the handling of value-based predicates.In this article, we first describe the XFilter and YFilter approaches and present results of a detailed performance comparison of structure matching for these algorithms as well as a hybrid approach. The results show that the path sharing employed by YFilter can provide order-of-magnitude performance benefits. We then propose two alternative techniques for extending YFilter's shared structure matching with support for value-based predicates, and compare the performance of these two techniques. The results of this latter study demonstrate some key differences between shared XML filtering and traditional database query processing. Finally, we describe how the YFilter approach is extended to handle more complicated queries containing nested path expressions.",
    "cited_by_count": 366,
    "openalex_id": "https://openalex.org/W2135611729",
    "type": "article"
  },
  {
    "title": "ProbView",
    "doi": "https://doi.org/10.1145/261124.261131",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "Laks V. S. Lakshmanan; Nicola Leone; Robert Ross; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "Probability theory is mathematically the best understood paradigm for modeling and manipulating uncertain information. Probabilities of complex events can be computed from those of basic events on which they depend, using any of a number of strategies. Which strategy is appropriate depends very much on the known interdependencies among the events involved. Previous work on probabilistic databases has assumed a fixed and restrictive combination strategy (e.g., assuming all events are pairwise independent). In this article, we characterize, using postulates, whole classes of strategies for conjunction, disjunction, and negation, meaningful from the viewpoint of probability theory. (1) We propose a probabilistic relational data model and a generic probabilistic relational algebra that neatly captures various strategies satisfying the postulates, within a single unified framework. (2) We show that as long as the chosen strategies can be computed in polynomial time, queries in the positive fragment of the probabilistic relational algebra have essentially the same data complexity as classical relational algebra. (3) We establish various containments and equivalences between algebraic expressions, similar in spirit to those in classical algebra. (4) We develop algorithms for maintaining materialized probabilistic views. (5) Based on these ideas, we have developed a prototype probabilistic database system called ProbView on top of Dbase V.0. We validate our complexity results with experiments and show that rewriting certain types of queries to other equivalent forms often yields substantial savings.",
    "cited_by_count": 352,
    "openalex_id": "https://openalex.org/W2024400846",
    "type": "article"
  },
  {
    "title": "Inverted files versus signature files for text indexing",
    "doi": "https://doi.org/10.1145/296854.277632",
    "publication_date": "1998-12-01",
    "publication_year": 1998,
    "authors": "Justin Zobel; Alistair Moffat; Kotagiri Ramamohanarao",
    "corresponding_authors": "",
    "abstract": "Two well-known indexing methods are inverted files and signature files. We have undertaken a detailed comparison of these two approaches in the context of text indexing, paying particular attention to query evaluation speed and space requirements. We have examined their relative performance using both experimentation and a refined approach to modeling of signature files, and demonstrate that inverted files are distinctly superior to signature files. Not only can inverted files be used to evaluate typical queries in less time than can signature files, but inverted files require less space and provide greater functionality. Our results also show that a synthetic text database can provide a realistic indication of the behavior of an actual text database. The tools used to generate the synthetic database have been made publicly available",
    "cited_by_count": 342,
    "openalex_id": "https://openalex.org/W2163652601",
    "type": "article"
  },
  {
    "title": "Evaluating top- <i>k</i> queries over web-accessible databases",
    "doi": "https://doi.org/10.1145/1005566.1005569",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Amélie Marian; Nicolas Bruno; Luis Gravano",
    "corresponding_authors": "",
    "abstract": "A query to a web search engine usually consists of a list of keywords, to which the search engine responds with the best or \"top\" k pages for the query. This top- k query model is prevalent over multimedia collections in general, but also over plain relational data for certain applications. For example, consider a relation with information on available restaurants, including their location, price range for one diner, and overall food rating. A user who queries such a relation might simply specify the user's location and target price range, and expect in return the best 10 restaurants in terms of some combination of proximity to the user, closeness of match to the target price range, and overall food rating. Processing top- k queries efficiently is challenging for a number of reasons. One critical such reason is that, in many web applications, the relation attributes might not be available other than through external web-accessible form interfaces, which we will have to query repeatedly for a potentially large set of candidate objects. In this article, we study how to process top- k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present a sequential algorithm for processing such queries, but observe that any sequential top- k query processing strategy is bound to require unnecessarily long query processing times, since web accesses exhibit high and variable latency. Fortunately, web sources can be probed in parallel, and each source can typically process concurrent requests, although sources may impose some restrictions on the type and number of probes that they are willing to accept. We adapt our sequential query processing technique and introduce an efficient algorithm that maximizes source-access parallelism to minimize query response time, while satisfying source-access constraints. We evaluate our techniques experimentally using both synthetic and real web-accessible data and show that parallel algorithms can be significantly more efficient than their sequential counterparts.",
    "cited_by_count": 339,
    "openalex_id": "https://openalex.org/W2099797738",
    "type": "article"
  },
  {
    "title": "Using semantic values to facilitate interoperability among heterogeneous information systems",
    "doi": "https://doi.org/10.1145/176567.176570",
    "publication_date": "1994-06-01",
    "publication_year": 1994,
    "authors": "Edward Sciore; Michael Siegel; Arnon Rosenthal",
    "corresponding_authors": "",
    "abstract": "Large organizations need to exchange information among many separately developed systems. In order for this exchange to be useful, the individual systems must agree on the meaning of their exchanged data. That is, the organization must ensure semantic interoperability . This paper provides a theory of semantic values as a unit of exchange that facilitates semantic interoperability betweeen heterogeneous information systems. We show how semantic values can either be stored explicitly or be defined by environments . A system architecture is presented that allows autonomous components to share semantic values. The key component in this architecture is called the context mediator , whose job is to identify and construct the semantic values being sent, to determine when the exchange is meaningful, and to convert the semantic values to the form required by the receiver. Our theory is then applied to the relational model. We provide an interpretation of standard SQL queries in which context conversions and manipulations are transparent to the user. We also introduce an extension of SQL, called Context-SQL (C-SQL), in which the context of a semantic value can be explicitly accessed and updated. Finally, we describe the implementation of a prototype context mediator for a relational C-SQL system.",
    "cited_by_count": 336,
    "openalex_id": "https://openalex.org/W2005379079",
    "type": "article"
  },
  {
    "title": "Data caching issues in an information retrieval system",
    "doi": "https://doi.org/10.1145/88636.87848",
    "publication_date": "1990-09-01",
    "publication_year": 1990,
    "authors": "Rafael Alonso; Daniel Barbará; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "Currently, a variety of information retrieval systems are available to potential users.… While in many cases these systems are accessed from personal computers, typically no advantage is taken of the computing resources of those machines (such as local processing and storage). In this paper we explore the possibility of using the user's local storage capabilities to cache data at the user's site. This would improve the response time of user queries albeit at the cost of incurring the overhead required in maintaining multiple copies. In order to reduce this overhead it may be appropriate to allow copies to diverge in a controlled fashion.… Thus, we introduce the notion of quasi-copies , which embodies the ideas sketched above. We also define the types of deviations that seem useful, and discuss the available implementation strategies.—From the Authors' Abstract",
    "cited_by_count": 331,
    "openalex_id": "https://openalex.org/W2126356880",
    "type": "article"
  },
  {
    "title": "A model of authorization for next-generation database systems",
    "doi": "https://doi.org/10.1145/103140.103144",
    "publication_date": "1991-03-01",
    "publication_year": 1991,
    "authors": "Fausto Rabitti; Elisa Bertino; Won Bae Kim; Darrell Woelk",
    "corresponding_authors": "",
    "abstract": "The conventional models of authorization have been designed for database systems supporting the hierarchical, network, and relational models of data. However, these models are not adequate for next-generation database systems that support richer data models that include object-oriented concepts and semantic data modeling concepts. Rabitti, Woelk, and Kim [14] presented a preliminary model of authorization for use as the basis of an authorization mechanism in such database systems. In this paper we present a fuller model of authorization that fills a few major gaps that the conventional models of authorization cannot fill for next-generation database systems. We also further formalize the notion of implicit authorization and refine the application of the notion of implicit authorization to object-oriented and semantic modeling concepts. We also describe a user interface for using the model of authorization and consider key issues in implementing the authorization model.",
    "cited_by_count": 328,
    "openalex_id": "https://openalex.org/W2073300799",
    "type": "article"
  },
  {
    "title": "A homogeneous relational model and query languages for temporal databases",
    "doi": "https://doi.org/10.1145/49346.50065",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Shashi K. Gadia",
    "corresponding_authors": "Shashi K. Gadia",
    "abstract": "In a temporal database, time values are associated with data item to indicate their periods of validity. We propose a model for temporal databases within the framework of the classical database theory. Our model is realized as a temporal parameterization of static relations. We do not impose any restrictions upon the schemes of temporal relations. The classical concepts of normal forms and dependencies are easily extended to our model, allowing a suitable design for a database scheme. We present a relational algebra and a tuple calculus for our model and prove their equivalence. Our data model is homogeneous in the sense that the periods of validity of all the attributes in a given tuple of a temporal relation are identical. We discuss how to relax the homogeneity requirement to extend the application domain of our approach.",
    "cited_by_count": 319,
    "openalex_id": "https://openalex.org/W2083125010",
    "type": "article"
  },
  {
    "title": "Making snapshot isolation serializable",
    "doi": "https://doi.org/10.1145/1071610.1071615",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Alan Fekete; Dimitrios Liarokapis; Elizabeth O’Neil; Patrick O’Neil; Dennis Shasha",
    "corresponding_authors": "",
    "abstract": "Snapshot Isolation (SI) is a multiversion concurrency control algorithm, first described in Berenson et al. [1995]. SI is attractive because it provides an isolation level that avoids many of the common concurrency anomalies, and has been implemented by Oracle and Microsoft SQL Server (with certain minor variations). SI does not guarantee serializability in all cases, but the TPC-C benchmark application [TPC-C], for example, executes under SI without serialization anomalies. All major database system products are delivered with default nonserializable isolation levels, often ones that encounter serialization anomalies more commonly than SI, and we suspect that numerous isolation errors occur each day at many large sites because of this, leading to corrupt data sometimes noted in data warehouse applications. The classical justification for lower isolation levels is that applications can be run under such levels to improve efficiency when they can be shown not to result in serious errors, but little or no guidance has been offered to application programmers and DBAs by vendors as to how to avoid such errors. This article develops a theory that characterizes when nonserializable executions of applications can occur under SI. Near the end of the article, we apply this theory to demonstrate that the TPC-C benchmark application has no serialization anomalies under SI, and then discuss how this demonstration can be generalized to other applications. We also present a discussion on how to modify the program logic of applications that are nonserializable under SI so that serializability will be guaranteed.",
    "cited_by_count": 318,
    "openalex_id": "https://openalex.org/W1996952974",
    "type": "article"
  },
  {
    "title": "Managing uncertainty in moving objects databases",
    "doi": "https://doi.org/10.1145/1016028.1016030",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Goce Trajcevski; Ouri Wolfson; Klaus Hinrichs; Sam Chamberlain",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of managing Moving Objects Databases (MODs) which capture the inherent imprecision of the information about the moving object's location at a given time. We deal systematically with the issues of constructing and representing the trajectories of moving objects and querying the MOD. We propose to model an uncertain trajectory as a three-dimensional (3D) cylindrical body and we introduce a set of novel but natural spatio-temporal operators which capture the uncertainty and are used to express spatio-temporal range queries. We devise and analyze algorithms for processing the operators and demonstrate that the model incorporates the uncertainty in a manner which enables efficient querying, thus striking a balance between the modeling power and computational efficiency. We address some implementation aspects which we experienced in our DOMINO project, as a part of which the operators that we introduce have been implemented. We also report on some experimental observations of a practical relevance.",
    "cited_by_count": 316,
    "openalex_id": "https://openalex.org/W2003571118",
    "type": "article"
  },
  {
    "title": "<i>GlOSS</i>",
    "doi": "https://doi.org/10.1145/320248.320252",
    "publication_date": "1999-06-01",
    "publication_year": 1999,
    "authors": "Luis Gravano; Héctor García-Molina; Anthony Tomasic",
    "corresponding_authors": "",
    "abstract": "The dramatic growth of the Internet has created a new problem for users: location of the relevant sources of documents. This article presents a framework for (and experimentally analyzes a solution to) this problem, which we call the text-source discovery problem . Our approach consists of two phases. First, each text source exports its contents to a centralized service. Second, users present queries to the service, which returns an ordered list of promising text sources. This article describes GlOSS , Glossary of Servers Server, with two versions: bGlOSS , which provides a Boolean query retrieval model, and vGlOSS , which provides a vector-space retrieval model. We also present hGlOSS , which provides a decentralized version of the system. We extensively describe the methodology for measuring the retrieval effectiveness of these systems and provide experimental evidence, based on actual data, that all three systems are highly effective in determining promising text sources for a given query.",
    "cited_by_count": 310,
    "openalex_id": "https://openalex.org/W2016892599",
    "type": "article"
  },
  {
    "title": "On the foundations of the universal relation model",
    "doi": "https://doi.org/10.1145/329.318580",
    "publication_date": "1984-06-03",
    "publication_year": 1984,
    "authors": "David Maier; Jeffrey D. Ullman; Moshe Y. Vardi",
    "corresponding_authors": "",
    "abstract": "The universal relation model aims at achieving complete access-path independence in relational databases by relieving the user of the need for logical navigation among relations. We clarify the assumptions underlying it and explore the approaches suggested for implementing it. The essential idea of the universal relation model is that access paths are embedded in attribute names. Thus attribute names must play unique “roles.” Furthermore, it assumes that for every set of attributes there is a basic relationship that the user has in mind. The user's queries refer to these basic relationships rather than to the underlying database. Two fundamentally different approaches to the universal relation model have been taken. According to the first approach, the user's view of the database is a universal relation or many universal relations, about which the user poses queries. The second approach sees the model as having query-processing capabilities that relieve the user of the need to specify the logical access path. Thus, while the first approach gives a denotational semantics to query answering, the second approach gives it an operational semantics. We investigate the relationship between these two approaches.",
    "cited_by_count": 294,
    "openalex_id": "https://openalex.org/W1983428002",
    "type": "article"
  },
  {
    "title": "Extended algebra and calculus for nested relational databases",
    "doi": "https://doi.org/10.1145/49346.49347",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Mark A. Roth; Herry F. Korth; Abraham Silberschatz",
    "corresponding_authors": "",
    "abstract": "Relaxing the assumption that relations are always in First-Normal-Form (1NF) necessitates a reexamination of the fundamentals of relational database theory. In this paper we take a first step towards unifying the various theories of ¬1NF databases. We start by determining an appropriate model to couch our formalisms in. We then define an extended relational calculus as the theoretical basis for our ¬1NF database query language. We define a minimal extended relational algebra and prove its equivalence to the ¬1NF relational calculus. We define a class of ¬1NF relations with certain “good” properties and extend our algebra operators to work within this domain. We prove certain desirable equivalences that hold only if we restrict our language to this domain.",
    "cited_by_count": 288,
    "openalex_id": "https://openalex.org/W1969375478",
    "type": "article"
  },
  {
    "title": "A normal form for XML documents",
    "doi": "https://doi.org/10.1145/974750.974757",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Marcelo Arenas; Leonid Libkin",
    "corresponding_authors": "",
    "abstract": "This article takes a first step towards the design and normalization theory for XML documents. We show that, like relational databases, XML documents may contain redundant information, and may be prone to update anomalies. Furthermore, such problems are caused by certain functional dependencies among paths in the document. Our goal is to find a way of converting an arbitrary DTD into a well-designed one, that avoids these problems. We first introduce the concept of a functional dependency for XML, and define its semantics via a relational representation of XML. We then define an XML normal form, XNF, that avoids update anomalies and redundancies. We study its properties, and show that XNF generalizes BCNF; we also discuss the relationship between XNF and normal forms for nested relations. Finally, we present a lossless algorithm for converting any DTD into one in XNF.",
    "cited_by_count": 287,
    "openalex_id": "https://openalex.org/W1983304353",
    "type": "article"
  },
  {
    "title": "Indexing large metric spaces for similarity search queries",
    "doi": "https://doi.org/10.1145/328939.328959",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Tolga Bozkaya; Meral Özsoyoğlu",
    "corresponding_authors": "",
    "abstract": "One of the common queries in many database applications is finding approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance-based index structures are proposed for applications where the distance computations between objects of the data domain are expensive (such as high-dimensional data) and the distance function is metric. In this paper we consider using distance-based index structures for similarity queries on large metric spaces. We elaborate on the approach that uses reference points (vantage points) to partition the data space into spherical shell-like regions in a hierarchical manner. We introduce the multivantage point tree structure (mvp-tree) that uses more than one vantage point to partiton the space into spherical cuts at each level. In answering similarity-based queries, the mvp-tree also utilizes the precomputed (at construction time) distances between the data points and the vantage points. We summarize the experiments comparing mvp-trees to vp-trees that have a similar partitioning strategy, but use only one vantage point at each level and do not make use of the precomputed distances. Empirical studies show that the mvp-tree outperforms the vp-tree by 20% to 80% for varying query ranges and different distance distributions. Next, we generalize the idea of using multiple vantage points and discuss the results of experiments we have made to see how varying the number of vantage points in a node affects affects performance and how much is gained in performance by making use of precomputed distances. The results show that, after all, it may be best to use a large number of vantage points in an internal node in order to end up with a single directory node and keep as many of the precomputed distances as possible to provide more efficient filtering during search operations. Finally, we provide some experimental results that compare mvp-trees with M-trees, which is a dynamic distance-based index structure for metric domains.",
    "cited_by_count": 278,
    "openalex_id": "https://openalex.org/W2061959470",
    "type": "article"
  },
  {
    "title": "Aggregate nearest neighbor queries in spatial databases",
    "doi": "https://doi.org/10.1145/1071610.1071616",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Dimitris Papadias; Yufei Tao; Kyriakos Mouratidis; Chun Kit Hui",
    "corresponding_authors": "",
    "abstract": "Given two spatial datasets P (e.g., facilities) and Q (queries), an aggregate nearest neighbor (ANN) query retrieves the point(s) of P with the smallest aggregate distance(s) to points in Q . Assuming, for example, n users at locations q 1 ,… q n , an ANN query outputs the facility p ∈ P that minimizes the sum of distances | pq i | for 1 ≤ i ≤ n that the users have to travel in order to meet there. Similarly, another ANN query may report the point p ∈ P that minimizes the maximum distance that any user has to travel, or the minimum distance from some user to his/her closest facility. If Q fits in memory and P is indexed by an R-tree, we develop algorithms for aggregate nearest neighbors that capture several versions of the problem, including weighted queries and incremental reporting of results. Then, we analyze their performance and propose cost models for query optimization. Finally, we extend our techniques for disk-resident queries and approximate ANN retrieval. The efficiency of the algorithms and the accuracy of the cost models are evaluated through extensive experiments with real and synthetic datasets.",
    "cited_by_count": 278,
    "openalex_id": "https://openalex.org/W2136046112",
    "type": "article"
  },
  {
    "title": "The hB-tree: a multiattribute indexing method with good guaranteed performance",
    "doi": "https://doi.org/10.1145/99935.99949",
    "publication_date": "1990-12-01",
    "publication_year": 1990,
    "authors": "David Lomet; Betty Salzberg",
    "corresponding_authors": "",
    "abstract": "A new multiattribute index structure called the hB-tree is introduced. It is derived from the K-D-B-tree of Robinson [15] but has additional desirable properties. The hB-tree internode search and growth processes are precisely analogous to the corresponding processes in B-trees [1]. The intranode processes are unique. A k-d tree is used as the structure within nodes for very efficient searching. Node splitting requires that this k-d tree be split. This produces nodes which no longer represent brick-like regions in k-space, but that can be characterized as holey bricks, bricks in which subregions have been extracted. We present results that guarantee hB-tree users decent storage utilization, reasonable size index terms, and good search and insert performance. These results guarantee that the hB-tree copes well with arbitrary distributions of keys.",
    "cited_by_count": 277,
    "openalex_id": "https://openalex.org/W2108908993",
    "type": "article"
  },
  {
    "title": "Modeling concepts for VLSI CAD objects",
    "doi": "https://doi.org/10.1145/3979.4018",
    "publication_date": "1985-09-01",
    "publication_year": 1985,
    "authors": "Don Batory; Won Bae Kim",
    "corresponding_authors": "",
    "abstract": "VLSI CAD applications deal with design objects that have an interface description and an implementation description. Versions of design objects have a common interface but differ in their implementations. A molecular object is a modeling construct which enables a database entity to be represented by two sets of heterogeneous records, one set describes the object's interface and the other describes its implementation. Thus a reasonable starting point for modeling design objects is to begin with the concept of molecular objects. In this paper, we identify modeling concepts that are fundamental to capturing the semantics of VLSI CAD design objects and versions in terms of molecular objects. A provisional set of user operations on design objects, consistent with these modeling concepts, is also defined. The modeling framework that we present has been found useful for investigating physical storage techniques and change notification problems in version control.",
    "cited_by_count": 264,
    "openalex_id": "https://openalex.org/W2023062025",
    "type": "article"
  },
  {
    "title": "Effective page refresh policies for Web crawlers",
    "doi": "https://doi.org/10.1145/958942.958945",
    "publication_date": "2003-12-01",
    "publication_year": 2003,
    "authors": "Junghoo Cho; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "In this article, we study how we can maintain local copies of remote data sources \"fresh,\" when the source data is updated autonomously and independently. In particular, we study the problem of Web crawlers that maintain local copies of remote Web pages for Web search engines. In this context, remote data sources (Websites) do not notify the copies (Web crawlers) of new changes, so we need to periodically poll the sources to maintain the copies up-to-date. Since polling the sources takes significant time and resources, it is very difficult to keep the copies completely up-to-date.This article proposes various refresh policies and studies their effectiveness. We first formalize the notion of \"freshness\" of copied data by defining two freshness metrics, and we propose a Poisson process as the change model of data sources. Based on this framework, we examine the effectiveness of the proposed refresh policies analytically and experimentally. We show that a Poisson process is a good model to describe the changes of Web pages and we also show that our proposed refresh policies improve the \"freshness\" of data very significantly. In certain cases, we got orders of magnitude improvement from existing policies.",
    "cited_by_count": 263,
    "openalex_id": "https://openalex.org/W2038378248",
    "type": "article"
  },
  {
    "title": "Physical integrity in a large segmented database",
    "doi": "https://doi.org/10.1145/320521.320540",
    "publication_date": "1977-03-01",
    "publication_year": 1977,
    "authors": "Raymond A. Lorie",
    "corresponding_authors": "Raymond A. Lorie",
    "abstract": "A database system can generally be divided into three major components. One component supports the logical database as seen by the user. Another component maps the information into physical records. The third component, called the storage component, is responsible for mapping these records onto auxiliary storage (generally disks) and controlling their transfer to and from main storage. This paper is primarily concerned with the implementation of a storage component. It considers a simple and classical interface to the storage component: Seen at this level the database is a collection of segments. Each segment is a linear address space. A recovery scheme is first proposed for system failure (hardware or software error which causes the contents of main storage to be lost). It is based on maintaining a dual mapping between pages and their location on disk. One mapping represents the current state of a segment being modified; the other represents a previous backup state. At any time the backup state can be replaced by the current state without any data merging. Procedures for segment modification, save, and restore are analyzed. Another section proposes a facility for protection against damage to the auxiliary storage itself. It is shown how such protection can be obtained by copying on a tape (checkpoint) only those pages that have been modified since the last checkpoint.",
    "cited_by_count": 262,
    "openalex_id": "https://openalex.org/W2031032746",
    "type": "article"
  },
  {
    "title": "Data allocation in distributed database systems",
    "doi": "https://doi.org/10.1145/44498.45063",
    "publication_date": "1988-09-01",
    "publication_year": 1988,
    "authors": "Peter M. G. Apers",
    "corresponding_authors": "Peter M. G. Apers",
    "abstract": "The problem of allocating the data of a database to the sites of a communication network is investigated. This problem deviates from the well-known file allocation problem in several aspects. First, the objects to be allocated are not known a priori; second, these objects are accessed by schedules that contain transmissions between objects to produce the result. A model that makes it possible to compare the cost of allocations is presented; the cost can be computed for different cost functions and for processing schedules produced by arbitrary query processing algorithms. For minimizing the total transmission cost, a method is proposed to determine the fragments to be allocated from the relations in the conceptual schema and the queries and updates executed by the users. For the same cost function, the complexity of the data allocation problem is investigated. Methods for obtaining optimal and heuristic solutions under various ways of computing the cost of an allocation are presented and compared. Two different approaches to the allocation management problem are presented and their merits are discussed.",
    "cited_by_count": 262,
    "openalex_id": "https://openalex.org/W2093583339",
    "type": "article"
  },
  {
    "title": "Implementing a relational database by means of specialzed hardware",
    "doi": "https://doi.org/10.1145/320064.320065",
    "publication_date": "1979-03-01",
    "publication_year": 1979,
    "authors": "E. Babb",
    "corresponding_authors": "E. Babb",
    "abstract": "New hardware is described which allows the rapid execution of queries demanding the joining of physically stored relations. The main feature of the hardware is a special store which can rapidly remember or recall data. This data might be pointers from one file to another, in which case the memory helps with queries on joins of files. Alternatively, the memory can help remove redundant data during projection, giving a considerable speed advantage over conventional hardware.",
    "cited_by_count": 258,
    "openalex_id": "https://openalex.org/W1964111545",
    "type": "article"
  },
  {
    "title": "Introduction to a system for distributed databases (SDD-1)",
    "doi": "https://doi.org/10.1145/320128.320129",
    "publication_date": "1980-03-01",
    "publication_year": 1980,
    "authors": "James B. Rothnie; P. A. Bernstein; Stephen Fox; Nathan Goodman; M. M. Hammer; Terry A. Landers; Christopher L. Reeve; David W. Shipman; Eugene Wong",
    "corresponding_authors": "",
    "abstract": "The declining cost of computer hardware and the increasing data processing needs of geographically dispersed organizations have led to substantial interest in distributed data management. SDD-1 is a distributed database management system currently being developed by Computer Corporation of America. Users interact with SDD-1 precisely as if it were a nondistributed database system because SDD-1 handles all issues arising from the distribution of data. These issues include distributed concurrency control, distributed query processing, resiliency to component failure, and distributed directory management. This paper presents an overview of the SDD-1 design and its solutions to the above problems. This paper is the first of a series of companion papers on SDD-1 (Bernstein and Shipman [2], Bernstein et al. [4], and Hammer and Shipman [14]).",
    "cited_by_count": 251,
    "openalex_id": "https://openalex.org/W2094711531",
    "type": "article"
  },
  {
    "title": "Updating derived relations: detecting irrelevant and autonomously computable updates",
    "doi": "https://doi.org/10.1145/68012.68015",
    "publication_date": "1989-09-01",
    "publication_year": 1989,
    "authors": "José A. Blakeley; Neil Coburn; Per-Åke Larson",
    "corresponding_authors": "",
    "abstract": "Consider a database containing not only base relations but also stored derived relations (also called materialized or concrete views). When a base relation is updated, it may also be necessary to update some of the derived relations. This paper gives sufficient and necessary conditions for detecting when an update of a base relation cannot affect a derived relation (an irrelevant update), and for detecting when a derived relation can be correctly updated using no data other than the derived relation itself and the given update operation (an autonomously computable update). The class of derived relations considered is restricted to those defined by PSJ -expressions, that is, any relational algebra expressions constructed from an arbitrary number of project, select and join operations (but containing no self-joins). The class of update operations consists of insertions, deletions, and modifications, where the set of tuples to be deleted or modified is specified by a selection condition on attributes of the relation being updated.",
    "cited_by_count": 250,
    "openalex_id": "https://openalex.org/W1968210840",
    "type": "article"
  },
  {
    "title": "Join and Semijoin Algorithms for a Multiprocessor Database Machine",
    "doi": "https://doi.org/10.1145/348.318590",
    "publication_date": "1984-03-23",
    "publication_year": 1984,
    "authors": "Patrick Valduriez; Georges Gardarin",
    "corresponding_authors": "",
    "abstract": "This paper presents and analyzes algorithms for computing joins and semijoins of relations in a multiprocessor database machine. First, a model of the multiprocessor architecture is described, incorporating parameters defining I/O, CPU, and message transmission times that permit calculation of the execution times of these algorithms. Then, three join algorithms are presented and compared. It is shown that, for a given configuration, each algorithm has an application domain defined by the characteristics of the operand and result relations. Since a semijoin operator is useful for decreasing I/O and transmission times in a multiprocessor system, we present and compare two equi-semijoin algorithms and one non-equi-semijoin algorithm. The execution times of these algorithms are generally linearly proportional to the size of the operand and result relations, and inversely proportional to the number of processors. We then compare a method which consists of joining two relations to a method whereby one joins their semijoins. Finally, it is shown that the latter method, using semijoins, is generally better. The various algorithms presented are implemented in the SABRE database system; an evaluation model selects the best algorithm for performing a join according to the results presented here. A first version of the SABRE system is currently operational at INRIA.",
    "cited_by_count": 248,
    "openalex_id": "https://openalex.org/W1970196737",
    "type": "article"
  },
  {
    "title": "Implications of certain assumptions in database performance evauation",
    "doi": "https://doi.org/10.1145/329.318578",
    "publication_date": "1984-06-03",
    "publication_year": 1984,
    "authors": "Stavros Christodoulakis",
    "corresponding_authors": "Stavros Christodoulakis",
    "abstract": "The assumptions of uniformity and independence of attribute values in a file, uniformity of queries, constant number of records per block, and random placement of qualifying records among the blocks of a file are frequently used in database performance evaluation studies. In this paper we show that these assumptions often result in predicting only an upper bound of the expected system cost. We then discuss the implications of nonrandom placement, nonuniformity, and dependencies of attribute values on database design and database performance evaluation.",
    "cited_by_count": 248,
    "openalex_id": "https://openalex.org/W1977570286",
    "type": "article"
  },
  {
    "title": "Spatial join techniques",
    "doi": "https://doi.org/10.1145/1206049.1206056",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Edwin Jacox; Hanan Samet",
    "corresponding_authors": "",
    "abstract": "A variety of techniques for performing a spatial join are reviewed. Instead of just summarizing the literature and presenting each technique in its entirety, distinct components of the different techniques are described and each is decomposed into an overall framework for performing a spatial join. A typical spatial join technique consists of the following components: partitioning the data, performing internal-memory spatial joins on subsets of the data, and checking if the full polygons intersect. Each technique is decomposed into these components and each component addressed in a separate section so as to compare and contrast similar aspects of each technique. The goal of this survey is to describe the algorithms within each component in detail, comparing and contrasting competing methods, thereby enabling further analysis and experimentation with each component and allowing the best algorithms for a particular situation to be built piecemeal, or, even better, enabling an optimizer to choose which algorithms to use.",
    "cited_by_count": 248,
    "openalex_id": "https://openalex.org/W1993997820",
    "type": "article"
  },
  {
    "title": "An access control model supporting periodicity constraints and temporal reasoning",
    "doi": "https://doi.org/10.1145/293910.293151",
    "publication_date": "1998-09-01",
    "publication_year": 1998,
    "authors": "Elisa Bertino; Cláudio Bettini; Elena Ferrari; Pierangela Samarati",
    "corresponding_authors": "",
    "abstract": "Access control models, such as the ones supported by commercial DBMSs, are not yet able to fully meet many application needs. An important requirement derives from the temporal dimension that permissions have in many real-world situations. Permissions are often limited in time or may hold only for specific periods of time. In this article, we present an access control model in which periodic temporal intervals are associated with authorizations. An authorization is automatically granted in the specified intervals and revoked when such intervals expire. Deductive temporal rules with periodicity and order constraints are provided to derive new authorizations based on the presence or absence of other authorizations in specific periods of time. We provide a solution to the problem of ensuring the uniqueness of the global set of valid authorizations derivable at each instant, and we propose an algorithm to compute this set. Moreover, we address issues related to the efficiency of access control by adopting a materialization approach. The resulting model provides a high degree of flexibility and supports the specification of several protection requirements that cannot be expressed in traditional access control models.",
    "cited_by_count": 248,
    "openalex_id": "https://openalex.org/W2063997960",
    "type": "article"
  },
  {
    "title": "Top- <i>k</i> selection queries over relational databases",
    "doi": "https://doi.org/10.1145/568518.568519",
    "publication_date": "2002-06-01",
    "publication_year": 2002,
    "authors": "Nicolas Bruno; Surajit Chaudhuri; Luis Gravano",
    "corresponding_authors": "",
    "abstract": "In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the \"top k \" tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top- k query by translating it into a single range query that a traditional relational database management system (RDBMS) can process efficiently. In particular, we study how to determine a range query to evaluate a top- k query by exploiting the statistics available to an RDBMS, and the impact of the quality of these statistics on the retrieval efficiency of the resulting scheme. We also report the first experimental evaluation of the mapping strategies over a real RDBMS, namely over Microsoft's SQL Server 7.0. The experiments show that our new techniques are robust and significantly more efficient than previously known strategies requiring at least one sequential scan of the data sets.",
    "cited_by_count": 248,
    "openalex_id": "https://openalex.org/W2134195632",
    "type": "article"
  },
  {
    "title": "A new approach to developing and implementing eager database replication protocols",
    "doi": "https://doi.org/10.1145/363951.363955",
    "publication_date": "2000-09-01",
    "publication_year": 2000,
    "authors": "Bettina Kemme; Gustavo Alonso",
    "corresponding_authors": "",
    "abstract": "Database replication is traditionally seen as a way to increase the availability and performance of distributed databases. Although a large number of protocols providing data consistency and fault-tolerance have been proposed, few of these ideas have ever been used in commercial products due to their complexity and performance implications. Instead, current products allow inconsistencies and often resort to centralized approaches which eliminates some of the advantages of replication. As an alternative, we propose a suite of replication protocols that addresses the main problems related to database replication. On the one hand, our protocols maintain data consistency and the same transactional semantics found in centralized systems. On the other hand, they provide flexibility and reasonable performance. To do so, our protocols take advantage of the rich semantics of group communication primitives and the relaxed isolation guarantees provided by most databases. This allows us to eliminate the possibility of deadlocks, reduce the message overhead and increase performance. A detailed simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented.",
    "cited_by_count": 246,
    "openalex_id": "https://openalex.org/W1988311692",
    "type": "article"
  },
  {
    "title": "The Escrow transactional method",
    "doi": "https://doi.org/10.1145/7239.7265",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "Patrick O’Neil",
    "corresponding_authors": "Patrick O’Neil",
    "abstract": "A method is presented for permitting record updates by long-lived transactions without forbidding simultaneous access by other users to records modified. Earlier methods presented separately by Gawlick and Reuter are comparable but concentrate on “hot-spot” situations, where even short transactions cannot lock frequently accessed fields without causing bottlenecks. The Escrow Method offered here is designed to support nonblocking record updates by transactions that are “long lived” and thus require long periods to complete. Recoverability of intermediate results prior to commit thus becomes a design goal, so that updates as of a given time can be guaranteed against memory or media failure while still retaining the prerogative to abort. This guarantee basically completes phase one of a two-phase commit, and several advantages result: (1) As with Gawlick's and Reuter's methods, high-concurrency items in the database will not act as a bottleneck; (2) transaction commit of different updates can be performed asynchronously, allowing natural distributed transactions; indeed, distributed transactions in the presence of delayed messages or occasional line disconnection become feasible in a way that we argue will tie up minimal resources for the purpose intended; and (3) it becomes natural to allow for human interaction in the middle of a transaction without loss of concurrent access or any special difficulty for the application programmer. The Escrow Method, like Gawlick's Fast Path and Reuter's Method, requires the database system to be an “expert” about the type of transactional updates performed, most commonly updates involving incremental changes to aggregate quantities. However, the Escrow Method is extendable to other types of updates.",
    "cited_by_count": 244,
    "openalex_id": "https://openalex.org/W1975637639",
    "type": "article"
  },
  {
    "title": "LH*—a scalable, distributed data structure",
    "doi": "https://doi.org/10.1145/236711.236713",
    "publication_date": "1996-12-01",
    "publication_year": 1996,
    "authors": "Witold Litwin; M.-A. Neimat; Donovan A. Schneider",
    "corresponding_authors": "",
    "abstract": "We present a scalable distributed data structure called LH*. LH* generalizes Linear Hashing (LH) to distributed RAM and disk files. An LH* file can be created from records with primary keys, or objects with OIDs, provided by any number of distributed and autonomous clients. It does not require a central directory, and grows gracefully, through splits of one bucket at a time, to virtually any number of servers. The number of messages per random insertion is one in general, and three in the worst case, regardless of the file size. The number of messages per key search is two in general, and four in the worst case. The file supports parallel operations, e.g., hash joins and scans. Performing a parallel operation on a file of M buckets costs at most 2 M + 1 messages, and between 1 and O (log 2 M rounds of messages. We first describle the basic LH* scheme where a coordinator site manages abucket splits, and splits a bucket every time a collision occurs. We show that the average load factor of an LH* file is 65%–70% regardless of file size, and bucket capacity. We then enhance the scheme with load control, performed at no additional message cost. The average load factor then increases to 80–95%. These values are about that of LH, but the load factor for LH* varies more. We nest define LH* schemes without a coordinator. We show that insert and search costs are the same as for the basic scheme. The splitting cost decreases on the average, but becomes more variable, as cascading splits are needed to prevent file overload. Next, we briefly describe two variants of splitting policy, using parallel splits and presplitting that should enhance performance for high-performance applications. All together, we show that LH* files can efficiently scale to files that are orders of magnitude larger in size than single-site files. LH* files that reside in main memory may also be much faster than single-site disk files. Finally, LH* files can be more efficient than any distributed file with a centralized directory, or a static parallel or distributed hash file.",
    "cited_by_count": 242,
    "openalex_id": "https://openalex.org/W2028928922",
    "type": "article"
  },
  {
    "title": "Principles and realization strategies of multilevel transaction management",
    "doi": "https://doi.org/10.1145/103140.103145",
    "publication_date": "1991-03-01",
    "publication_year": 1991,
    "authors": "Gerhard Weikum",
    "corresponding_authors": "Gerhard Weikum",
    "abstract": "One of the demands of database system transaction management is to achieve a high degree of concurrency by taking into consideration the semantics of high-level operations. On the other hand, the implementation of such operations must pay attention to conflicts on the storage representation levels below. To meet these requirements in a layered architecture, we propose a multilevel transaction management utilizing layer-specific semantics. Based on the theoretical notion of multilevel serializability, a family of concurrency control strategies is developed. Suitable recovery protocols are investigated for aborting single transactions and for restarting the system after a crash. The choice of levels involved in a multilevel transaction strategy reveals an inherent trade-off between increased concurrency and growing recovery costs. A series of measurements has been performed in order to compare several strategies. Preliminary results indicate considerable performance gains of the multilevel transaction approach.",
    "cited_by_count": 241,
    "openalex_id": "https://openalex.org/W2044555065",
    "type": "article"
  },
  {
    "title": "The SIFT information dissemination system",
    "doi": "https://doi.org/10.1145/331983.331992",
    "publication_date": "1999-12-01",
    "publication_year": 1999,
    "authors": "Tak W. Yan; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "Information dissemination is a powerful mechanism for finding information in wide-area environments. An information dissemination server accepts long-term user queries, collects new documents from information sources, matches the documents against the queries, and continuously updates the users with relevant information. This paper is a retrospective of the Stanford Information Filtering Service (SIFT), a system that as of April 1996 was processing over 40,000 worldwide subscriptions and over 80,000 daily documents. The paper describes some of the indexing mechanisms that were developed for SIFT, as well as the evaluations that were conducted to select a scheme to implement. It also describes the implementation of SIFT, and experimental results for the actual system. Finally, it also discusses and experimentally evaluates techniques for distributing a service such as SIFT for added performance and availability.",
    "cited_by_count": 239,
    "openalex_id": "https://openalex.org/W1971652947",
    "type": "article"
  },
  {
    "title": "Specification and implementation of exceptions in workflow management systems",
    "doi": "https://doi.org/10.1145/328939.328996",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Fabio Casati; Stefano Ceri; Stefano Paraboschi; Guiseppe Pozzi",
    "corresponding_authors": "",
    "abstract": "Although workflow management systems are most applicable when an organization follows standard business processes and routines, any of these processes faces the need for handling exceptions, i.e., asynchronous and anomalous situations that fall outside the normal control flow. In this paper we concentrate upon anomalous situtations that, although unusual, are part of the semantics of workflow applications, and should be specified and monitored coherently; in most real-life applications, such exceptions affect a significant fraction of workflow cases. However, very few workflow management systems are integrated with a highly expressive language for specifying this kind of exception and with a system component capable of handling it. We present Chimera-Exc, a language for the specification of exceptions for workflows based on detached active rules, and then describe the architecture of a system, called FAR, that implements Chimera-Exc and integrates it with a commercial workflow management system and database server. We discuss the main issues that were solved by our implementation, and report on the performance of FAR. We also discuss design criteria for exceptions in light of the formal properties of their execution. Finally, we focus on the portability of FAR on its unbundling to a generic architecture with detached active rules.",
    "cited_by_count": 239,
    "openalex_id": "https://openalex.org/W1984192117",
    "type": "article"
  },
  {
    "title": "Concurrency control in a system for distributed databases (SDD-1)",
    "doi": "https://doi.org/10.1145/320128.320131",
    "publication_date": "1980-03-01",
    "publication_year": 1980,
    "authors": "Philip A. Bernstein; David W. Shipman; James B. Rothnie",
    "corresponding_authors": "",
    "abstract": "This paper presents the concurrency control strategy of SDD-1. SDD-1, a System for Distributed Databases, is a prototype distributed database system being developed by Computer Corporation of America. In SDD-1, portions of data distributed throughout a network may be replicated at multiple sites. The SDD-1 concurrency control guarantees database consistency in the face of such distribution and replication. This paper is one of a series of companion papers on SDD-1 [4, 10, 12, 21].",
    "cited_by_count": 235,
    "openalex_id": "https://openalex.org/W1963979953",
    "type": "article"
  },
  {
    "title": "Physical database design for relational databases",
    "doi": "https://doi.org/10.1145/42201.42205",
    "publication_date": "1988-03-01",
    "publication_year": 1988,
    "authors": "Sheldon J. Finkelstein; Mario Schkolnick; Paolo Tiberio",
    "corresponding_authors": "",
    "abstract": "This paper describes the concepts used in the implementation of DBDSGN, an experimental physical design tool for relational databases developed at the IBM San Jose Research Laboratory. Given a workload for System R (consisting of a set of SQL statements and their execution frequencies), DBDSGN suggests physical configurations for efficient performance. Each configuration consists of a set of indices and an ordering for each table. Workload statements are evaluated only for atomic configurations of indices, which have only one index per table. Costs for any configuration can be obtained from those of the atomic configurations. DBDSGN uses information supplied by the System R optimizer both to determine which columns might be worth indexing and to obtain estimates of the cost of executing statements in different configurations. The tool finds efficient solutions to the index-selection problem; if we assume the cost estimates supplied by the optimizer are the actual execution costs, it finds the optimal solution. Optionally, heuristics can be used to reduce execution time. The approach taken by DBDSGN in solving the index-selection problem for multiple-table statements significantly reduces the complexity of the problem. DBDSGN's principles were used in the Relational Design Tool (RDT), an IBM product based on DBDSGN, which performs design for SQL/DS, a relational system based on System R. System R actually uses DBDSGN's suggested solutions as the tool expects because cost estimates and other necessary information can be obtained from System R using a new SQL statement, the EXPLAIN statement. This illustrates how a system can export a model of its internal assumptions and behavior so that other systems (such as tools) can share this model.",
    "cited_by_count": 232,
    "openalex_id": "https://openalex.org/W2047085757",
    "type": "article"
  },
  {
    "title": "Efficient algorithms for processing XPath queries",
    "doi": "https://doi.org/10.1145/1071610.1071614",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Georg Gottlob; Christoph Koch; Reinhard Pichler",
    "corresponding_authors": "",
    "abstract": "Our experimental analysis of several popular XPath processors reveals a striking fact: Query evaluation in each of the systems requires time exponential in the size of queries in the worst case. We show that XPath can be processed much more efficiently, and propose main-memory algorithms for this problem with polynomial-time combined query evaluation complexity. Moreover, we show how the main ideas of our algorithm can be profitably integrated into existing XPath processors. Finally, we present two fragments of XPath for which linear-time query processing algorithms exist and another fragment with linear-space/quadratic-time query processing.",
    "cited_by_count": 231,
    "openalex_id": "https://openalex.org/W2134826526",
    "type": "article"
  },
  {
    "title": "Formal semantics for time in databases",
    "doi": "https://doi.org/10.1145/319983.319986",
    "publication_date": "1983-06-01",
    "publication_year": 1983,
    "authors": "James Clifford; David S. Warren",
    "corresponding_authors": "",
    "abstract": "The concept of a historical database is introduced as a tool for modeling the dynamic nature of some part of the real world. Just as first-order logic has been shown to be a useful formalism for expressing and understanding the underlying semantics of the relational database model, intensional logic is presented as an analogous formalism for expressing and understanding the temporal semantics involved in a historical database. The various components of the relational model, as extended to include historical relations, are discussed in terms of the model theory for the logic IL s , a variation of the logic IL formulated by Richard Montague. The modal concepts of intensional and extensional data constraints and queries are introduced and contrasted. Finally, the potential application of these ideas to the problem of natural language database querying is discussed.",
    "cited_by_count": 230,
    "openalex_id": "https://openalex.org/W3123141741",
    "type": "article"
  },
  {
    "title": "Language features for flexible handling of exceptions in information systems",
    "doi": "https://doi.org/10.1145/4879.4995",
    "publication_date": "1985-12-01",
    "publication_year": 1985,
    "authors": "Alexander Borgida",
    "corresponding_authors": "Alexander Borgida",
    "abstract": "An exception-handling facility suitable for languages used to implement database-intensive information systems is presented. Such a mechanism facilitates the development and maintenance of more flexible software systems by supporting the abstraction of details concerning special or abnormal occurrences. The type constraints imposed by the schema as well as various semantic integrity assertions are considered to be normalcy conditions, and the key contribution of this work is to allow exceptions to these constraints to persist. To achieve this, solutions are proposed to a range of problems, including sharing and computing with exceptional information, exception handling by users, the logic of constraints with exceptions, and implementation issues. The use of exception handling in dealing with null values, estimates, and measurement is also illustrated.",
    "cited_by_count": 228,
    "openalex_id": "https://openalex.org/W2035382654",
    "type": "article"
  },
  {
    "title": "Extending relational algebra and relational calculus with set-valued attributes and aggregate functions",
    "doi": "https://doi.org/10.1145/32204.32219",
    "publication_date": "1987-11-01",
    "publication_year": 1987,
    "authors": "Gültekin Özsoyoğlu; Z. Meral Özsoyoğlu; Victor Matos",
    "corresponding_authors": "",
    "abstract": "In commercial network database management systems, set-valued fields and aggregate functions are commonly supported. However, the relational database model, as defined by Codd, does not include set-valued attributes or aggregate functions. Recently, Klug extended the relational model by incorporating aggregate functions and by defining relational algebra and calculus languages. In this paper, relational algebra and relational calculus database query languages (as defined by Klug) are extended to manipulate set-valued attributes and to utilize aggregate functions. The expressive power of the extended languages is shown to be equivalent. We extend the relational algebra with three new operators, namely, pack, unpack, and aggregation-by-template. The extended languages form a theoretical framework for statistical database query languages.",
    "cited_by_count": 228,
    "openalex_id": "https://openalex.org/W2095263945",
    "type": "article"
  },
  {
    "title": "A normal form for relational databases that is based on domains and keys",
    "doi": "https://doi.org/10.1145/319587.319592",
    "publication_date": "1981-09-01",
    "publication_year": 1981,
    "authors": "Ronald Fagin",
    "corresponding_authors": "Ronald Fagin",
    "abstract": "A new normal form for relational databases, called domain-key normal form (DK/NF), is defined. Also, formal definitions of insertion anomaly and deletion anomaly are presented. It is shown that a schema is in DK/NF if and only if it has no insertion or deletion anomalies. Unlike previously defined normal forms, DK/NF is not defined in terms of traditional dependencies (functional, multivalued, or join). Instead, it is defined in terms of the more primitive concepts of domain and key, along with the general concept of a “constraint.” We also consider how the definitions of traditional normal forms might be modified by taking into consideration, for the first time, the combinatorial consequences of bounded domain sizes. It is shown that after this modification, these traditional normal forms are all implied by DK/NF. In particular, if all domains are infinite, then these traditional normal forms are all implied by DK/NF.",
    "cited_by_count": 227,
    "openalex_id": "https://openalex.org/W2000773957",
    "type": "article"
  },
  {
    "title": "Synthesis of extended transaction models using ACTA",
    "doi": "https://doi.org/10.1145/185827.185843",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Panos K. Chrysanthis; Krithi Ramamritham",
    "corresponding_authors": "",
    "abstract": "ACTA is a comprehensive transaction framework that facilitates the formal description of properties of extended transaction models. Specifically, using ACTA, one can specify and reason about (1) the effects of transactions on objects and (2) the interactions between transactions. This article presents ACTA as a tool for the synthesis of extended transaction models , one which supports the development and analysis of new extended transaction models in a systematic manner. Here, this is demonstrated by deriving new transaction definitions (1) by modifying the specifications of existing transaction models, (2) by combining the specifications of existing models, and (3) by starting from first principles. To exemplify the first, new models are synthesized from atomic transactions and join transactions . To illustrate the second, we synthesize a model that combines aspect of the nested - and split-transaction models. We demonstrate the latter by deriving the specification of an open-nested-transaction model from high-level requirements.",
    "cited_by_count": 224,
    "openalex_id": "https://openalex.org/W2043318789",
    "type": "article"
  },
  {
    "title": "On the optimal nesting order for computing <i>N</i> -relational joins",
    "doi": "https://doi.org/10.1145/1270.1498",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Toshihide Ibaraki; Tiko Kameda",
    "corresponding_authors": "",
    "abstract": "Using the nested loops method, this paper addresses the problem of minimizing the number of page fetches necessary to evaluate a given query to a relational database. We first propose a data structure whereby the number of page fetches required for query evaluation is substantially reduced and then derive a formula for the expected number of page fetches. An optimal solution to our problem is the nesting order of relations in the evaluation program, which minimizes the number of page fetches. Since the minimization of the formula is NP-hard, as shown in the Appendix, we propose a heuristic algorithm which produces a good suboptimal solution in polynomial time. For the special case where the input query is a “tree query,” we present an efficient algorithm for finding an optimal nesting order.",
    "cited_by_count": 224,
    "openalex_id": "https://openalex.org/W2066623874",
    "type": "article"
  },
  {
    "title": "Dynamic voting algorithms for maintaining the consistency of a replicated database",
    "doi": "https://doi.org/10.1145/78922.78926",
    "publication_date": "1990-06-01",
    "publication_year": 1990,
    "authors": "Sushil Jajodia; David Mutchler",
    "corresponding_authors": "",
    "abstract": "There are several replica control algorithms for managing replicated files in the face of network partitioning due to site or communication link failures. Pessimistic algorithms ensure consistency at the price of reduced availability; they permit at most one (distinguished) partition to process updates at any given time. The best known pessimistic algorithm, voting , is a “static” algorithm, meaning that all potential distinguished partitions can be listed in advance. We present a dynamic extension of voting called dynamic voting . This algorithm permits updates in a partition provided it contains more than half of the up-to-date copies of the replicated file. We also present an extension of dynamic voting called dynamic voting with linearly ordered copies (abbreviated as dynamic-linear ). These algorithms are dynamic because the order in which past distinguished partitions were created plays a role in the selection of the next distinguished partition. Our algorithms have all the virtues of ordinary voting, including its simplicity, and provide improved availability as well. We provide two stochastic models to support the latter claim. In the first (site) model, sites may fail but communication links are infallible; in the second (link) model the reverse is true. We prove that under the site model, dynamic-linear has greater availability than any static algorithm, including weighted voting, if there are four or more sites in the network. In the link model, we consider all biconnected five-site networks and a wide variety of failure and repair rates. In all cases considered, dynamic-linear had greater availability than any static algorithm.",
    "cited_by_count": 223,
    "openalex_id": "https://openalex.org/W2056646076",
    "type": "article"
  },
  {
    "title": "Automated resolution of semantic heterogeneity in multidatabases",
    "doi": "https://doi.org/10.1145/176567.176569",
    "publication_date": "1994-06-01",
    "publication_year": 1994,
    "authors": "M. W. Bright; Ali R. Hurson; S. H. Pakzad",
    "corresponding_authors": "",
    "abstract": "A multidatabase system provides integrated access to heterogeneous, autonomous local databases in a distributed system. An important problem in current multidatabase systems is identification of semantically similar data in different local databases. The Summary Schemas Model (SSM) is proposed as an extension to multidatabase systems to aid in semantic identification. The SSM uses a global data structure to abstract the information available in a multidatabase system. This abstracted form allows users to use their own terms (imprecise queries) when accessing data rather than being forced to use system-specified terms. The system uses the global data structure to match the user's terms to the semantically closest available system terms. A simulation of the SSM is presented to compare imprecise-query processing with corresponding query-processing costs in a standard multidatabase system. The costs and benefits of the SSM are discussed, and future research directions are presented.",
    "cited_by_count": 223,
    "openalex_id": "https://openalex.org/W2075043418",
    "type": "article"
  },
  {
    "title": "A probabilistic relational model and algebra",
    "doi": "https://doi.org/10.1145/232753.232796",
    "publication_date": "1996-09-01",
    "publication_year": 1996,
    "authors": "Debabrata Dey; Sumit Sarkar",
    "corresponding_authors": "",
    "abstract": "Although the relational model for databases provides a great range of advantages over other data models, it lacks a comprehensive way to handle incomplete and uncertain data. Uncertainty in data values, however, is pervasive in all real-world environments and has received much attention in the literature. Several methods have been proposed for incorporating uncertain data into relational databases. However, the current approaches have many shortcomings and have not established an acceptable extension of the relational model. In this paper, we propose a consistent extension of the relational model. We present a revised relational structure and extend the relational algebra. The extended algebra is shown to be closed, a consistent extension of the conventional relational algebra, and reducible to the latter.",
    "cited_by_count": 221,
    "openalex_id": "https://openalex.org/W2085988990",
    "type": "article"
  },
  {
    "title": "Efficient optimization of a class of relational expressions",
    "doi": "https://doi.org/10.1145/320107.320112",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "A. V. Aho; Yehoshua Sagiv; J. D. Ullman",
    "corresponding_authors": "",
    "abstract": "The design of several database query languages has been influenced by Codd's relational algebra. This paper discusses the difficulty of optimizing queries based on the relational algebra operations select, project, and join. A matrix, called a tableau, is proposed as a useful device for representing the value of a query, and optimization of queries is couched in terms of finding a minimal tableau equivalent to a given one. Functional dependencies can be used to imply additional equivalences among tableaux. Although the optimization problem is NP-complete, a polynomial time algorithm exists to optimize tableaux that correspond to an important subclass of queries.",
    "cited_by_count": 218,
    "openalex_id": "https://openalex.org/W1967992696",
    "type": "article"
  },
  {
    "title": "Iterative dynamic programming",
    "doi": "https://doi.org/10.1145/352958.352982",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "Donald Kossmann; Konrad Stocker",
    "corresponding_authors": "",
    "abstract": "The query optimizer is one of the most important components of a database system. Most commercial query optimizers today are based on a dynamic-programming algorithm, as proposed in Selinger et al. [1979]. While this algorithm produces good optimization results (i.e, good plans), its high complexity can be prohibitive if complex queries need to be processed, new query execution techniques need to be integrated, or in certain programming environments (e.g., distributed database systems). In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming , or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable and as good-as possible plans if dynamic programming turns out to be not viable. Three, all IDP-algorithms can very easily be integrated into an existing optimizer which is based on dynamic programming.",
    "cited_by_count": 218,
    "openalex_id": "https://openalex.org/W2154888102",
    "type": "article"
  },
  {
    "title": "Discovering all most specific sentences",
    "doi": "https://doi.org/10.1145/777943.777945",
    "publication_date": "2003-06-01",
    "publication_year": 2003,
    "authors": "Dimitrios Gunopulos; Roni Khardon; Heikki Mannila; Sanjeev Saluja; Hannu Toivonen; Ram Sewak Sharma",
    "corresponding_authors": "",
    "abstract": "Data mining can be viewed, in many instances, as the task of computing a representation of a theory of a model or a database, in particular by finding a set of maximally specific sentences satisfying some property. We prove some hardness results that rule out simple approaches to solving the problem.The a priori algorithm is an algorithm that has been successfully applied to many instances of the problem. We analyze this algorithm, and prove that is optimal when the maximally specific sentences are \"small\". We also point out its limitations.We then present a new algorithm, the Dualize and Advance algorithm, and prove worst-case complexity bounds that are favorable in the general case. Our results use the concept of hypergraph transversals. Our analysis shows that the a priori algorithm can solve the problem of enumerating the transversals of a hypergraph, improving on previously known results in a special case. On the other hand, using results for the general case of the hypergraph transversal enumeration problem, we can show that the Dualize and Advance algorithm has worst-case running time that is sub-exponential to the output size (i.e., the number of maximally specific sentences).We further show that the problem of finding maximally specific sentences is closely related to the problem of exact learning with membership queries studied in computational learning theory.",
    "cited_by_count": 217,
    "openalex_id": "https://openalex.org/W2143182259",
    "type": "article"
  },
  {
    "title": "Optimized stratified sampling for approximate query processing",
    "doi": "https://doi.org/10.1145/1242524.1242526",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Surajit Chaudhuri; Gautam Das; Vivek Narasayya",
    "corresponding_authors": "",
    "abstract": "The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem where, given a workload of queries, we select a stratified random sample of the original data such that the error in answering the workload queries using the sample is minimized. A key novelty of our approach is that we can tailor the choice of samples to be robust, even for workloads that are “similar” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server that demonstrate the superior quality of our method compared to previous work.",
    "cited_by_count": 215,
    "openalex_id": "https://openalex.org/W2152029707",
    "type": "article"
  },
  {
    "title": "Database repairing using updates",
    "doi": "https://doi.org/10.1145/1093382.1093385",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Jef Wijsen",
    "corresponding_authors": "Jef Wijsen",
    "abstract": "Repairing a database means bringing the database in accordance with a given set of integrity constraints by applying some minimal change. If a database can be repaired in more than one way, then the consistent answer to a query is defined as the intersection of the query answers on all repaired versions of the database.Earlier approaches have confined the repair work to deletions and insertions of entire tuples. We propose a theoretical framework that also covers updates as a repair primitive. Update-based repairing is interesting in that it allows rectifying an error within a tuple without deleting the tuple, thereby preserving consistent values in the tuple. Another novel idea is the construct of nucleus: a single database that yields consistent answers to a class of queries, without the need for query rewriting. We show the construction of nuclei for full dependencies and conjunctive queries. Consistent query answering and constructing nuclei is generally intractable under update-based repairing. Nevertheless, we also show some tractable cases of practical interest.",
    "cited_by_count": 214,
    "openalex_id": "https://openalex.org/W2045054164",
    "type": "article"
  },
  {
    "title": "Secure statistical databases with random sample queries",
    "doi": "https://doi.org/10.1145/320613.320616",
    "publication_date": "1980-09-01",
    "publication_year": 1980,
    "authors": "Dorothy E. Denning",
    "corresponding_authors": "Dorothy E. Denning",
    "abstract": "A new inference control, called random sample queries, is proposed for safeguarding confidential data in on-line statistical databases. The random sample queries control deals directly with the basic principle of compromise by making it impossible for a questioner to control precisely the formation of query sets. Queries for relative frequencies and averages are computed using random samples drawn from the query sets. The sampling strategy permits the release of accurate and timely statistics and can be implemented at very low cost. Analysis shows the relative error in the statistics decreases as the query set size increases; in contrast, the effort required to compromise increases with the query set size due to large absolute errors. Experiments performed on a simulated database support the analysis.",
    "cited_by_count": 214,
    "openalex_id": "https://openalex.org/W2132591512",
    "type": "article"
  },
  {
    "title": "Limitations of record-based information models",
    "doi": "https://doi.org/10.1145/320064.320070",
    "publication_date": "1979-03-01",
    "publication_year": 1979,
    "authors": "William Kent",
    "corresponding_authors": "William Kent",
    "abstract": "Record structures are generally efficient, familiar, and easy to use for most current data processing applications. But they are not complete in their ability to represent information, nor are they fully self-describing.",
    "cited_by_count": 212,
    "openalex_id": "https://openalex.org/W2028052727",
    "type": "article"
  },
  {
    "title": "The tracker",
    "doi": "https://doi.org/10.1145/320064.320069",
    "publication_date": "1979-03-01",
    "publication_year": 1979,
    "authors": "Dorothy E. Denning; Peter J. Denning; Mayer D. Schwartz",
    "corresponding_authors": "",
    "abstract": "The query programs of certain databases report raw statistics for query sets, which are groups of records specified implicitly by a characteristic formula. The raw statistics include query set size and sums of powers of values in the query set. Many users and designers believe that the individual records will remain confidential as long as query programs refuse to report the statistics of query sets which are too small. It is shown that the compromise of small query sets can in fact almost always be accomplished with the help of characteristic formulas called trackers. Schlörer's individual tracker is reviewed; it is derived from known characteristics of a given individual and permits deducing additional characteristics he may have. The general tracker is introduced: It permits calculating statistics for arbitrary query sets, without requiring preknowledge of anything in the database. General trackers always exist if there are enough distinguishable classes of individuals in the database, in which case the trackers have a simple form. Almost all databases have a general tracker, and general trackers are almost always easy to find. Security is not guaranteed by the lack of a general tracker.",
    "cited_by_count": 212,
    "openalex_id": "https://openalex.org/W2138520657",
    "type": "article"
  },
  {
    "title": "Topological relationships between complex spatial objects",
    "doi": "https://doi.org/10.1145/1132863.1132865",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Markus Schneider; Thomas Behr",
    "corresponding_authors": "",
    "abstract": "For a long time topological relationships between spatial objects have been a focus of research in a number of disciplines like artificial intelligence, cognitive science, linguistics, robotics, and spatial reasoning. Especially as predicates they support the design of suitable query languages for spatial data retrieval and analysis in spatial databases and geographical information systems (GIS). Unfortunately, they have so far only been defined for and applicable to simplified abstractions of spatial objects like single points, continuous lines, and simple regions. With the introduction of complex spatial data types an issue arises regarding the design, definition, and number of topological relationships operating on these complex types. This article closes this gap and first introduces definitions of general and versatile spatial data types for complex points , complex lines , and complex regions . Based on the well known 9-intersection model, it then determines the complete sets of mutually exclusive topological relationships for all type combinations. Completeness and mutual exclusion are shown by a proof technique called proof-by-constraint-and-drawing . Due to the resulting large numbers of predicates and the difficulty of handling them, the user is provided with the concepts of topological cluster predicates and topological predicate groups , which permit one to reduce the number of predicates to be dealt with in a user-defined and/or application-specific manner.",
    "cited_by_count": 210,
    "openalex_id": "https://openalex.org/W1997862802",
    "type": "article"
  },
  {
    "title": "Secure databases",
    "doi": "https://doi.org/10.1145/320064.320068",
    "publication_date": "1979-03-01",
    "publication_year": 1979,
    "authors": "David Dobkin; Anita K. Jones; Richard J. Lipton",
    "corresponding_authors": "",
    "abstract": "Users may be able to compromise databases by asking a series of questions and then inferring new information from the answers. The complexity of protecting a database against this technique is discussed here.",
    "cited_by_count": 209,
    "openalex_id": "https://openalex.org/W2067858436",
    "type": "article"
  },
  {
    "title": "A data distortion by probability distribution",
    "doi": "https://doi.org/10.1145/3979.4017",
    "publication_date": "1985-09-01",
    "publication_year": 1985,
    "authors": "Chong K. Liew; Uinam J. Choi; Chung J. Liew",
    "corresponding_authors": "",
    "abstract": "This paper introduces data distortion by probability distribution, a probability distortion that involves three steps. The first step is to identify the underlying density function of the original series and to estimate the parameters of this density function. The second step is to generate a series of data from the estimated density function. And the final step is to map and replace the generated series for the original one. Because it is replaced by the distorted data set, probability distortion guards the privacy of an individual belonging to the original data set. At the same time, the probability distorted series provides asymptotically the same statistical properties as those of the original series, since both are under the same distribution. Unlike conventional point distortion, probability distortion is difficult to compromise by repeated queries, and provides a maximum exposure for statistical analysis.",
    "cited_by_count": 206,
    "openalex_id": "https://openalex.org/W1981961329",
    "type": "article"
  },
  {
    "title": "Parallel algorithms for the execution of relational database operations",
    "doi": "https://doi.org/10.1145/319989.319991",
    "publication_date": "1983-09-01",
    "publication_year": 1983,
    "authors": "Dina Bitton; Haran Boral; David J. DeWitt; W. Kevin Wilkinson",
    "corresponding_authors": "",
    "abstract": "This paper presents and analyzes algorithms for parallel processing of relational database operations in a general multiprocessor framework. To analyze alternative algorithms, we introduce an analysis methodology which incorporates I/O, CPU, and message costs and which can be adjusted to fit different multiprocessor architectures. Algorithms are presented and analyzed for sorting, projection, and join operations. While some of these algorithms have been presented and analyzed previously, we have generalized each in order to handle the case where the number of pages is significantly larger than the number of processors. In addition, we present and analyze algorithms for the parallel execution of update and aggregate operations.",
    "cited_by_count": 206,
    "openalex_id": "https://openalex.org/W2124170481",
    "type": "article"
  },
  {
    "title": "Spatial management of data",
    "doi": "https://doi.org/10.1145/320610.320648",
    "publication_date": "1980-12-01",
    "publication_year": 1980,
    "authors": "Christopher F. Herot",
    "corresponding_authors": "Christopher F. Herot",
    "abstract": "Spatial data management is a technique for organizing and retrieving information by positioning it in a graphical data space (GDS). This graphical data space is viewed through a color raster-scan display which enables users to traverse the GDS surface or zoom into the image to obtain greater detail. In contrast to conventional database management systems, in which users access data by asking questions in a formal query language, a spatial data management system (SDMS) presents the information graphically in a form that seems to encourage browsing and to require less prior knowledge of the contents and organization of the database. This paper presents an overview of the SDMS concept and describes its implementation in a prototype system for retrieving information from both a symbolic database management system and an optical videodisk.",
    "cited_by_count": 205,
    "openalex_id": "https://openalex.org/W2062168151",
    "type": "article"
  },
  {
    "title": "Concurrent manipulation of binary search trees",
    "doi": "https://doi.org/10.1145/320613.320619",
    "publication_date": "1980-09-01",
    "publication_year": 1980,
    "authors": "H. T. Kung; Philip L. Lehman",
    "corresponding_authors": "",
    "abstract": "The concurrent manipulation of a binary search tree is considered in this paper. The systems presented can support any number of concurrent processes which perform searching, insertion, deletion, and rotation (reorganization) on the tree, but allow any process to lock only a constant number of nodes at any time. Also, in the systems, searches are essentially never blocked. The concurrency control techniques introduced in the paper include the use of special nodes and pointers to redirect searches, and the use of copies of sections of the tree to introduce many changes simultaneously and therefore avoid unpredictable interleaving. Methods developed in this paper may provide new insights into other problems in the area of concurrent database manipulation.",
    "cited_by_count": 202,
    "openalex_id": "https://openalex.org/W2431458507",
    "type": "article"
  },
  {
    "title": "Casper*",
    "doi": "https://doi.org/10.1145/1620585.1620591",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Chi-Yin Chow; Mohamed F. Mokbel; Walid G. Aref",
    "corresponding_authors": "",
    "abstract": "In this article, we present a new privacy-aware query processing framework, Capser *, in which mobile and stationary users can obtain snapshot and/or continuous location-based services without revealing their private location information. In particular, we propose a privacy-aware query processor embedded inside a location-based database server to deal with snapshot and continuous queries based on the knowledge of the user's cloaked location rather than the exact location. Our proposed privacy-aware query processor is completely independent of how we compute the user's cloaked location. In other words, any existing location anonymization algorithms that blur the user's private location into cloaked rectilinear areas can be employed to protect the user's location privacy. We first propose a privacy-aware query processor that not only supports three new privacy-aware query types, but also achieves a trade-off between query processing cost and answer optimality. Then, to improve system scalability of processing continuous privacy-aware queries, we propose a shared execution paradigm that shares query processing among a large number of continuous queries. The proposed scalable paradigm can be tuned through two parameters to trade off between system scalability and answer optimality. Experimental results show that our query processor achieves high quality snapshot and continuous location-based services while supporting queries and/or data with cloaked locations.",
    "cited_by_count": 200,
    "openalex_id": "https://openalex.org/W2622005089",
    "type": "article"
  },
  {
    "title": "Efficient sort-based skyline evaluation",
    "doi": "https://doi.org/10.1145/1412331.1412343",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Ilaria Bartolini; Paolo Ciaccia; Marco Patella",
    "corresponding_authors": "",
    "abstract": "Skyline queries compute the set of Pareto-optimal tuples in a relation, that is, those tuples that are not dominated by any other tuple in the same relation. Although several algorithms have been proposed for efficiently evaluating skyline queries, they either necessitate the relation to have been indexed or have to perform the dominance tests on all the tuples in order to determine the result. In this article we introduce salsa, a novel skyline algorithm that exploits the idea of presorting the input data so as to effectively limit the number of tuples to be read and compared. This makes salsa also attractive when skyline queries are executed on top of systems that do not understand skyline semantics, or when the skyline logic runs on clients with limited power and/or bandwidth. We prove that, if one considers symmetric sorting functions, the number of tuples to be read is minimized by sorting data according to a “minimum coordinate,” minC, criterion, and that performance can be further improved if data distribution is known and an asymmetric sorting function is used. Experimental results obtained on synthetic and real datasets show that salsa consistently outperforms state-of-the-art sequential skyline algorithms and that its performance can be accurately predicted.",
    "cited_by_count": 199,
    "openalex_id": "https://openalex.org/W1972541019",
    "type": "article"
  },
  {
    "title": "Sequentiality and prefetching in database systems",
    "doi": "https://doi.org/10.1145/320263.320276",
    "publication_date": "1978-09-01",
    "publication_year": 1978,
    "authors": "Alan Jay Smith",
    "corresponding_authors": "Alan Jay Smith",
    "abstract": "Sequentiality of access is an inherent characteristic of many database systems. We use this observation to develop an algorithm which selectively prefetches data blocks ahead of the point of reference. The number of blocks prefetched is chosen by using the empirical run length distribution and conditioning on the observed number of sequential block references immediately preceding reference to the current block. The optimal number of blocks to prefetch is estimated as a function of a number of “costs,” including the cost of accessing a block not resident in the buffer (a miss), the cost of fetching additional data blocks at fault times, and the cost of fetching blocks that are never referenced. We estimate this latter cost, described as memory pollution, in two ways. We consider the treatment (in the replacement algorithm) of prefetched blocks, whether they are treated as referenced or not, and find that it makes very little difference. Trace data taken from an operational IMS database system is analyzed and the results are presented. We show how to determine optimal block sizes. We find that anticipatory fetching of data can lead to significant improvements in system operation.",
    "cited_by_count": 192,
    "openalex_id": "https://openalex.org/W2043213682",
    "type": "article"
  },
  {
    "title": "Distributed deadlock detection algorithm",
    "doi": "https://doi.org/10.1145/319702.319717",
    "publication_date": "1982-06-01",
    "publication_year": 1982,
    "authors": "Ron Obermarck",
    "corresponding_authors": "Ron Obermarck",
    "abstract": "We propose an algorithm for detecting deadlocks among transactions running concurrently in a distributed processing network (i.e., a distributed database system). The proposed algorithm is a distributed deadlock detection algorithm. A proof of the correctness of the distributed portion of the algorithm is given, followed by an example of the algorithm in operation. The performance characteristics of the algorithm are also presented.",
    "cited_by_count": 192,
    "openalex_id": "https://openalex.org/W2102685956",
    "type": "article"
  },
  {
    "title": "Independent components of relations",
    "doi": "https://doi.org/10.1145/320576.320580",
    "publication_date": "1977-12-01",
    "publication_year": 1977,
    "authors": "J. Rissanen",
    "corresponding_authors": "J. Rissanen",
    "abstract": "In a multiattribute relation or, equivalently, a multicolumn table a certain collection of the projections can be shown to be independent in much the same way as the factors in a Cartesian product or orthogonal components of a vector. A precise notion of independence for relations is defined and studied. The main result states that the operator which reconstructs the original relation from its independent components is the natural join, and that independent components split the full family of functional dependencies into corresponding component families. These give an easy-to-check criterion for independence.",
    "cited_by_count": 191,
    "openalex_id": "https://openalex.org/W1998512182",
    "type": "article"
  },
  {
    "title": "Parallelism and recovery in database systems",
    "doi": "https://doi.org/10.1145/320141.320146",
    "publication_date": "1980-06-01",
    "publication_year": 1980,
    "authors": "Raymond Bayer; Hans Heller; Angelika Reiser",
    "corresponding_authors": "",
    "abstract": "In this paper a new method to increase parallelism in database systems is described. Use is made of the fact that for recovery reasons, we often have two values for one object in the database—the new one and the old one. Introduced and discussed in detail is a certain scheme by which readers and writers may work simultaneously on the same object. It is proved that transactions executed according to this scheme have the correct effect; i.e., consistency is preserved. Several variations of the basic scheme which are suitable depending on the degree of parallelism required, are described.",
    "cited_by_count": 191,
    "openalex_id": "https://openalex.org/W2011839903",
    "type": "article"
  },
  {
    "title": "Efficient checking of temporal integrity constraints using bounded history encoding",
    "doi": "https://doi.org/10.1145/210197.210200",
    "publication_date": "1995-06-01",
    "publication_year": 1995,
    "authors": "Jan Chomicki",
    "corresponding_authors": "Jan Chomicki",
    "abstract": "We present an efficient implementation method for temporal integrity constraints formulated in Past Temporal Logic. Although the constraints can refer to past states of the database, their checking does not require that the entire database history be stored. Instead, every database state is extended with auxiliary relations that contain the historical information necessary for checking constraints. Auxiliary relations can be implemented as materialized relational views.",
    "cited_by_count": 189,
    "openalex_id": "https://openalex.org/W2025315843",
    "type": "article"
  },
  {
    "title": "Composing schema mappings",
    "doi": "https://doi.org/10.1145/1114244.1114249",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Ronald Fagin; Phokion G. Kolaitis; Lucian Popa; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). A fundamental problem is composing schema mappings: given two successive schema mappings, derive a schema mapping between the source schema of the first and the target schema of the second that has the same effect as applying successively the two schema mappings.In this article, we give a rigorous semantics to the composition of schema mappings and investigate the definability and computational complexity of the composition of two schema mappings. We first study the important case of schema mappings in which the specification is given by a finite set of source-to-target tuple-generating dependencies (source-to-target tgds). We show that the composition of a finite set of full source-to-target tgds with a finite set of tgds is always definable by a finite set of source-to-target tgds, but the composition of a finite set of source-to-target tgds with a finite set of full source-to-target tgds may not be definable by any set (finite or infinite) of source-to-target tgds; furthermore, it may not be definable by any formula of least fixed-point logic, and the associated composition query may be NP-complete. After this, we introduce a class of existential second-order formulas with function symbols and equalities, which we call second-order tgds , and make a case that they are the “right” language for composing schema mappings. Specifically, we show that second-order tgds form the smallest class (up to logical equivalence) that contains every source-to-target tgd and is closed under conjunction and composition. Allowing equalities in second-order tgds turns out to be of the essence, even though the “obvious” way to define second-order tgds does not require equalities. We show that second-order tgds without equalities are not sufficiently expressive to define the composition of finite sets of source-to-target tgds. Finally, we show that second-order tgds possess good properties for data exchange and query answering: the chase procedure can be extended to second-order tgds so that it produces polynomial-time computable universal solutions in data exchange settings specified by second-order tgds.",
    "cited_by_count": 189,
    "openalex_id": "https://openalex.org/W2997050146",
    "type": "article"
  },
  {
    "title": "Amalgamating knowledge bases",
    "doi": "https://doi.org/10.1145/176567.176571",
    "publication_date": "1994-06-01",
    "publication_year": 1994,
    "authors": "V. S. Subrahmanian",
    "corresponding_authors": "V. S. Subrahmanian",
    "abstract": "The integration of knowledge for multiple sources is an important aspect of automated reasoning systems. When different knowledge bases are used to store knowledge provided by multiple sources, we are faced with the problem of integrating multiple knowledge bases: Under these circumstances, we are also confronted with the prospect of inconsistency. In this paper we present a uniform theoretical framework, based on annotated logics , for amalgamating multiple knowledge bases when these knowledge bases (possibly) contain inconsistencies, uncertainties, and nonmonotonic modes of negation. We show that annotated logics may be used, with some modifications, to mediate between different knowledge bases. The multiple knowledge bases are amalgamated by a transformation of the individual knowledge bases into new annotated logic programs, together with the addition of a new axiom scheme. We characterize the declarative semantics of such amalgamated knowledge bases and study how the semantics of the amalgam is related to the semantics of the individual knowledge bases being combined. —Author's Abstract",
    "cited_by_count": 187,
    "openalex_id": "https://openalex.org/W1984704149",
    "type": "article"
  },
  {
    "title": "Multilevel atomicity—a new correctness criterion for database concurrency control",
    "doi": "https://doi.org/10.1145/319996.319999",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "Nancy Lynch",
    "corresponding_authors": "Nancy Lynch",
    "abstract": "Multilevel atomicity , a new correctness criteria for database concurrency control, is defined. It weakens the usual notion of serializability by permitting controlled interleaving among transactions. It appears to be especially suitable for applications in which the set of transactions has a natural hierarchical structure based on the hierarchical structure of an organization. A characterization for multilevel atomicity, in terms of the absence of cycles in a dependency relation among transaction steps, is given. Some remarks are made concerning implementation.",
    "cited_by_count": 186,
    "openalex_id": "https://openalex.org/W2060098605",
    "type": "article"
  },
  {
    "title": "Strong functional dependencies and their application to normal forms in XML",
    "doi": "https://doi.org/10.1145/1016028.1016029",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Millist W. Vincent; Jixue Liu; Chengfei Liu",
    "corresponding_authors": "",
    "abstract": "In this article, we address the problem of how to extend the definition of functional dependencies (FDs) in incomplete relations to XML documents (called XFDs) using the well-known strong satisfaction approach.We propose a syntactic definition of strong XFD satisfaction in an XML document and then justify it by showing that, similar to the case in relational databases, for the case of simple paths, keys in XML are a special case of XFDs. We also propose a normal form for XML documents based on our definition of XFDs and provide a formal justification for it by proving that it is a necessary and sufficient condition for the elimination of redundancy in an XML document.",
    "cited_by_count": 186,
    "openalex_id": "https://openalex.org/W2142421097",
    "type": "article"
  },
  {
    "title": "Efficiently monitoring relational databases",
    "doi": "https://doi.org/10.1145/320083.320099",
    "publication_date": "1979-09-01",
    "publication_year": 1979,
    "authors": "O. Buneman; ﻿Eric K. Clemons",
    "corresponding_authors": "",
    "abstract": "An alerter is a program which monitors a database and reports to some user or program when a specified condition occurs. It may be that the condition is a complicated expression involving several entities in the database; in this case the evaluation of the expression may be computationally expensive. A scheme is presented in which alerters may be placed on a complex query involving a relational database, and a method is demonstrated for reducing the amount of computation involved in checking whether an alerter should be triggered.",
    "cited_by_count": 186,
    "openalex_id": "https://openalex.org/W2150282015",
    "type": "article"
  },
  {
    "title": "Processing XML streams with deterministic automata and stream indexes",
    "doi": "https://doi.org/10.1145/1042046.1042051",
    "publication_date": "2004-12-12",
    "publication_year": 2004,
    "authors": "Todd J. Green; Ashish Gupta; Gerome Miklau; Makoto Onizuka; Dan Suciu",
    "corresponding_authors": "",
    "abstract": "We consider the problem of evaluating a large number of XPath expressions on a stream of XML packets. We contribute two novel techniques. The first is to use a single Deterministic Finite Automaton (DFA). The contribution here is to show that the DFA can be used effectively for this problem: in our experiments we achieve a constant throughput, independently of the number of XPath expressions. The major issue is the size of the DFA, which, in theory, can be exponential in the number of XPath expressions. We provide a series of theoretical results and experimental evaluations that show that the lazy DFA has a small number of states, for all practical purposes. These results are of general interest in XPath processing, beyond stream processing. The second technique is the Streaming IndeX (SIX), which consists of adding a small amount of binary data to each XML packet that allows the query processor to achieve significant speedups. As an application of these techniques we describe the XML Toolkit (XMLTK), a collection of command-line tools providing highly scalable XML data processing.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W1969783316",
    "type": "article"
  },
  {
    "title": "Serializable isolation for snapshot databases",
    "doi": "https://doi.org/10.1145/1620585.1620587",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Michael J. Cahill; Uwe Röhm; Alan Fekete",
    "corresponding_authors": "",
    "abstract": "Many popular database management systems implement a multiversion concurrency control algorithm called snapshot isolation rather than providing full serializability based on locking. There are well-known anomalies permitted by snapshot isolation that can lead to violations of data consistency by interleaving transactions that would maintain consistency if run serially. Until now, the only way to prevent these anomalies was to modify the applications by introducing explicit locking or artificial update conflicts, following careful analysis of conflicts between all pairs of transactions. This article describes a modification to the concurrency control algorithm of a database management system that automatically detects and prevents snapshot isolation anomalies at runtime for arbitrary applications, thus providing serializable isolation. The new algorithm preserves the properties that make snapshot isolation attractive, including that readers do not block writers and vice versa. An implementation of the algorithm in a relational DBMS is described, along with a benchmark and performance study, showing that the throughput approaches that of snapshot isolation in most cases.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W2725226870",
    "type": "article"
  },
  {
    "title": "An integrated efficient solution for computing frequent and top- <i>k</i> elements in data streams",
    "doi": "https://doi.org/10.1145/1166074.1166084",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Ahmed Metwally; Divyakant Agrawal; Amr El Abbadi",
    "corresponding_authors": "",
    "abstract": "We propose an approximate integrated approach for solving both problems of finding the most popular k elements, and finding frequent elements in a data stream coming from a large domain. Our solution is space efficient and reports both frequent and top- k elements with tight guarantees on errors. For general data distributions, our top- k algorithm returns k elements that have roughly the highest frequencies; and it uses limited space for calculating frequent elements. For realistic Zipfian data, the space requirement of the proposed algorithm for solving the exact frequent elements problem decreases dramatically with the parameter of the distribution; and for top- k queries, the analysis ensures that only the top- k elements, in the correct order, are reported. The experiments, using real and synthetic data sets, show space reductions with hardly any loss in accuracy. Having proved the effectiveness of the proposed approach through both analysis and experiments, we extend it to be able to answer continuous queries about frequent and top- k elements. Although the problems of incremental reporting of frequent and top- k elements are useful in many applications, to the best of our knowledge, no solution has been proposed.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W1970779762",
    "type": "article"
  },
  {
    "title": "Read-only transactions in a distributed database",
    "doi": "https://doi.org/10.1145/319702.319704",
    "publication_date": "1982-06-01",
    "publication_year": 1982,
    "authors": "Héctor García-Molina; Gio Wiederhold",
    "corresponding_authors": "",
    "abstract": "A read-only transaction or query is a transaction which does not modify any data. Read-only transactions could be processed with general transaction processing algorithms, but in many cases it is more efficient to process read-only transactions with special algorithms which take advantage of the knowledge that the transaction only reads. This paper defines the various consistency and currency requirements that read-only transactions may have. The processing of the different classes of read-only transactions in a distributed database is discussed. The concept of R insularity is introduced to characterize both the read-only and update algorithms. Several simple update and read-only transaction processing algorithms are presented to illustrate how the query requirements and the update algorithms affect the read-only transaction processing algorithms.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W2005099092",
    "type": "article"
  },
  {
    "title": "A temporally oriented data model",
    "doi": "https://doi.org/10.1145/7239.7350",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "Gad Ariav",
    "corresponding_authors": "Gad Ariav",
    "abstract": "The research into time and data models has so far focused on the identification of extensions to the classical relational model that would provide it with “adequate” semantic capacity to deal with time. The temporally oriented data model (TODM) presented in this paper is a result of a different approach, namely, it directly operationalizes the pervasive three-dimensional metaphor for time. One of the main results is thus the development of the notion of the data cube: a three-dimensional and inherently temporal data construct where time, objects, and attributes are the primary dimensions of stored data. TODM's cube adds historical depth to the tabular notions of data and provides a framework for storing and retrieving data within their temporal context. The basic operations in the model allow the formation of new cubic views from existing ones, or viewing data as one moves up and down in time within cubes. This paper introduces TODM, a consistent set of temporally oriented data constructs, operations, and constraints, and then presents TOSQL, a corresponding end-user's SQL-like query syntax. The model is a restricted but consistent superset of the relational model, and the query syntax incorporates temporal notions in a manner that likewise avoids penalizing users who are interested solely in the current view of data (rather than in a temporal perspective). The naturalness of the spatial reference to time and the added semantic capacity of TODM come with a price—the definitions of the cubic constructs and basic operations are relatively cumbersome. As rudimentary as it is, TODM nonetheless provides a comprehensive basis for formulating an external data model for a temporally oriented database.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W2013277984",
    "type": "article"
  },
  {
    "title": "Integrity = validity + completeness",
    "doi": "https://doi.org/10.1145/76902.76904",
    "publication_date": "1989-12-01",
    "publication_year": 1989,
    "authors": "Amihai Motro",
    "corresponding_authors": "Amihai Motro",
    "abstract": "Database integrity has two complementary components: validity , which guarantees that all false information is excluded from the database, and completeness , which guarantees that all true information is included in the database. This article describes a uniform model of integrity for relational databases, that considers both validity and completeness. To a large degree, this model subsumes the prevailing model of integrity (i.e., integrity constraints). One of the features of the new model is the determination of the integrity of answers issued by the database system in response to user queries. To users, answers that are accompanied with such detailed certifications of their integrity are more meaningful. First, the model is defined and discussed. Then, a specific mechanism is described that implements this model. With this mechanism, the determination of the integrity of an answer is a process analogous to the determination of the answer itself.",
    "cited_by_count": 182,
    "openalex_id": "https://openalex.org/W2009285751",
    "type": "article"
  },
  {
    "title": "Maintaining availability in partitioned replicated databases",
    "doi": "https://doi.org/10.1145/63500.63501",
    "publication_date": "1989-06-01",
    "publication_year": 1989,
    "authors": "Amr El Abbadi; Sam Toueg",
    "corresponding_authors": "",
    "abstract": "In a replicated database, a data item may have copies residing on several sites. A replica control protocol is necessary to ensure that data items with several copies behave as if they consist of a single copy, as far as users can tell. We describe a new replica control protocol that allows the accessing of data in spite of site failures and network partitioning. This protocol provides the database designer with a large degree of flexibility in deciding the degree of data availability, as well as the cost of accessing data.",
    "cited_by_count": 180,
    "openalex_id": "https://openalex.org/W1985188419",
    "type": "article"
  },
  {
    "title": "Semantics-based concurrency control",
    "doi": "https://doi.org/10.1145/128765.128771",
    "publication_date": "1992-03-01",
    "publication_year": 1992,
    "authors": "B. R. Badrinath; Krithi Ramamritham",
    "corresponding_authors": "",
    "abstract": "The concurrency of transactions executing on atomic data types can be enhanced through the use of semantic information about operations defined on these types. Hitherto, commutativity of operations has been exploited to provide enchanced concurrency while avoiding cascading aborts. We have identified a property known as recoverability which can be used to decrease the delay involved in processing noncommuting operations while still avoiding cascading aborts. When an invoked operation is recoverable with respect to an uncommitted operation, the invoked operation can be executed by forcing a commit dependency between the invoked operation and the uncommitted operation; the transaction invoking the operation will not have to wait for the uncommitted operation to abort or commit. Further, this commit dependency only affects the order in which the operations should commit, if both commit; if either operation aborts, the other can still commit thus avoiding cascading aborts. To ensure the serializability of transactions, we force the recoverability relationship between transactions to be acyclic. Simulation studies, based on the model presented by Agrawal et al. [1], indicate that using recoverability, the turnaround time of transactions can be reduced. Further, our studies show enchancement in concurrency even when resource constraints are taken into consideration. The magnitude of enchancement is dependent on the resource contention; the lower the resource contention, the higher the improvement.",
    "cited_by_count": 180,
    "openalex_id": "https://openalex.org/W2140876280",
    "type": "article"
  },
  {
    "title": "A compression technique to materialize transitive closure",
    "doi": "https://doi.org/10.1145/99935.99944",
    "publication_date": "1990-12-01",
    "publication_year": 1990,
    "authors": "H. V. Jagadish",
    "corresponding_authors": "H. V. Jagadish",
    "abstract": "An important feature of database support for expert systems is the ability of the database to answer queries regarding the existence of a path from one node to another in the directed graph underlying some database relation. Given just the database relation, answering such a query is time-consuming, but given the transitive closure of the database relation a table look-up suffices. We present an indexing scheme that permits the storage of the pre-computed transitive closure of a database relation in a compressed form. The existence of a specified tuple in the closure can be determined from this compressed store by a single look-up followed by an index comparision. We show how to add nodes and arcs to the compressed closure incrementally. We also suggest how this compression technique can be used to reduce the effort required to compute the transitive closure.",
    "cited_by_count": 179,
    "openalex_id": "https://openalex.org/W1965109038",
    "type": "article"
  },
  {
    "title": "A database encryption system with subkeys",
    "doi": "https://doi.org/10.1145/319566.319580",
    "publication_date": "1981-06-01",
    "publication_year": 1981,
    "authors": "George I. Davida; David L. Wells; John B. Kam",
    "corresponding_authors": "",
    "abstract": "A new cryptosystem that is suitable for database encryption is presented. The system has the important property of having subkeys that allow the encryption and decryption of fields within a record. The system is based on the Chinese Remainder Theorem.",
    "cited_by_count": 179,
    "openalex_id": "https://openalex.org/W2005571213",
    "type": "article"
  },
  {
    "title": "On the menbership problem for functional and multivalued dependencies in relational databases",
    "doi": "https://doi.org/10.1145/320613.320614",
    "publication_date": "1980-09-01",
    "publication_year": 1980,
    "authors": "Catriel Beeri",
    "corresponding_authors": "Catriel Beeri",
    "abstract": "The problem of whether a given dependency in a database relation can be derived from a given set of dependencies is investigated. We show that the problem can be decided in polynomial time when the given set consists of either multivalued dependencies only or of both functional and multivalued dependencies and the given dependency is also either a functional or a multivalued dependency. These results hold when the derivations are restricted not to use the complementation rule.",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W1968137129",
    "type": "article"
  },
  {
    "title": "Object operations benchmark",
    "doi": "https://doi.org/10.1145/128765.128766",
    "publication_date": "1992-03-01",
    "publication_year": 1992,
    "authors": "R. G. G. Cattell; Jim Skeen",
    "corresponding_authors": "",
    "abstract": "Performance is a major issue in the acceptance of object-oriented and relational database systems aimed at engineering applications such as computer-aided software engineering (CASE) and computer-aided design (CAD). Because traditional database systems benchmarks are inapproriate to measure performance for operations on engineering objects, we designed a new benchmark Object Operations version 1 (OO1) to focus on important characteristics of these applications. OO1 is descended from an earlier benchmark for simple database operations and is based on several years experience with that benchmark. In this paper we describe the OO1 benchmark and results we obtained running it on a variety of database systems. We provide a careful specification of the benchmark, show how it can be implemented on database systems, and present evidence that more than an order of magnitude difference in performance can result from a DBMS implementation quite different from current products; minimizing overhead per database call, offloading database server functionality to workstations, taking advantage of large main memories, and using link-based methods.",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W2025194432",
    "type": "article"
  },
  {
    "title": "Index structures for selective dissemination of information under the Boolean model",
    "doi": "https://doi.org/10.1145/176567.176573",
    "publication_date": "1994-06-01",
    "publication_year": 1994,
    "authors": "Tak W. Yan; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "The number, size, and user population of bibliographic and full-text document databases are rapidly growing. With a high document arrival rate, it becomes essential for users of such databases to have access to the very latest documents; yet the high document arrival rate also makes it difficult for users to keep themselves updated. It is desirable to allow users to submit profiles, i.e., queries that are constantly evaluated, so that they will be automatically informed of new additions that may be of interest. Such service is traditionally called Selective Dissemination of Information (SDI). The high document arrival rate, the huge number of users, and the timeliness requirement of the service pose a challenge in achieving efficient SDL. In this article, we propose several index structures for indexing profiles and algorithms that efficiently match documents against large number of profiles. We also present analysis and simulation results to compare their performance under different scenarios.",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W2054311069",
    "type": "article"
  },
  {
    "title": "Optimal allocation of resources in distributed information networks",
    "doi": "https://doi.org/10.1145/320434.320449",
    "publication_date": "1976-03-01",
    "publication_year": 1976,
    "authors": "Safwat A. Mahmoud; J. S. Riordon",
    "corresponding_authors": "",
    "abstract": "The problems of file allocation and capacity assignment in a fixed topology distributed computer network are examined. These two aspects of the design are tightly coupled by means of an average message delay constraint. The objective is to allocate copies of information files to network nodes and capacities to network links so that a minimum cost is achieved subject to network delay and file availability constraints. A model for solving the problem is formulated and the resulting optimization problem is shown to fall into a class of nonlinear integer programming problems. Deterministic techniques for solving this class of problems are computationally cumbersome, even for small size problems. A new heuristic algorithm is developed, which is based on a decomposition technique that greatly reduces the computational complexity of the problem. Numerical results for a variety of network configurations indicate that the heuristic algorithm, while not theoretically convergent, yields practicable low cost solutions with substantial savings in computer processing time and storage requirements. Moreover, it is shown that this algorithm is capable of solving realistic network problems whose solutions using deterministic techniques are computationally intractable.",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W2140234885",
    "type": "article"
  },
  {
    "title": "The design of a rotating associative memory for relational database applications",
    "doi": "https://doi.org/10.1145/320434.320447",
    "publication_date": "1976-03-01",
    "publication_year": 1976,
    "authors": "C. S. Lin; Diane C. P. Smith; John Miles Smith",
    "corresponding_authors": "",
    "abstract": "The design and motivation for a rotating associative relational store (RARES) is described. RARES is designed to enhance the performance of an optimizing relational query interface by supporting important high level optimization techniques. In particular, it can perform tuple selection operations at the storage device and also can provide a mechanism for efficient sorting. Like other designs for rotating associative stores, RARES contains search logic which is attached to the heads of a rotating head-per-track storage device. RARES is distinct from other designs in that it utilizes a novel “orthogonal” storage layout. This layout allows a high output rate of selected tuples even when a sort order in the stored relation must be preserved. As in certain other designs, RARES can usually output a tuple as soon as it is found to satisfy the selection criteria. However, relative to these designs, the orthogonal layout allows an order of magnitude reduction in the capacity of storage local to the search logic.",
    "cited_by_count": 176,
    "openalex_id": "https://openalex.org/W2026731931",
    "type": "article"
  },
  {
    "title": "Duplicate record elimination in large data files",
    "doi": "https://doi.org/10.1145/319983.319987",
    "publication_date": "1983-06-01",
    "publication_year": 1983,
    "authors": "Dina Bitton; David J. DeWitt",
    "corresponding_authors": "",
    "abstract": "The issue of duplicate elimination for large data files in which many occurrences of the same record may appear is addressed. A comprehensive cost analysis of the duplicate elimination operation is presented. This analysis is based on a combinatorial model developed for estimating the size of intermediate runs produced by a modified merge-sort procedure. The performance of this modified merge-sort procedure is demonstrated to be significantly superior to the standard duplicate elimination technique of sorting followed by a sequential pass to locate duplicate records. The results can also be used to provide critical input to a query optimizer in a relational database system.",
    "cited_by_count": 175,
    "openalex_id": "https://openalex.org/W2020191321",
    "type": "article"
  },
  {
    "title": "View indexing in relational databases",
    "doi": "https://doi.org/10.1145/319702.319729",
    "publication_date": "1982-06-01",
    "publication_year": 1982,
    "authors": "N. Roussopoulos",
    "corresponding_authors": "N. Roussopoulos",
    "abstract": "The design and maintenance of a useful database system require efficient optimization of the logical access paths which demonstrate repetitive usage patterns. Views (classes of queries given by a query model) are an appropriate intermediate logical representation for database. Frequently accessed views of databases need to be supported by indexing to enhance retrieval. This paper investigates the problem of selecting an optimal index set of views and describes an efficient algorithm for this selection.",
    "cited_by_count": 174,
    "openalex_id": "https://openalex.org/W2047809518",
    "type": "article"
  },
  {
    "title": "The design of a query monitoring system",
    "doi": "https://doi.org/10.1145/1508857.1508858",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Chaitanya Mishra; Nick Koudas",
    "corresponding_authors": "",
    "abstract": "Query monitoring refers to the problem of observing and predicting various parameters related to the execution of a query in a database system. In addition to being a useful tool for database users and administrators, it can also serve as an information collection service for resource allocation and adaptive query processing techniques. In this article, we present a query monitoring system from the ground up, describing various new techniques for query monitoring, their implementation inside a real database system, and a novel interface that presents the observed and predicted information in an accessible manner. To enable this system, we introduce several lightweight online techniques for progressively estimating and refining the cardinality of different relational operators using information collected at query execution time. These include binary and multiway joins as well as typical grouping operations and combinations thereof. We describe the various algorithms used to efficiently implement estimators and present the results of an evaluation of a prototype implementation of our framework in an open-source data management system. Our results demonstrate the feasibility and practical utility of the approach presented herein.",
    "cited_by_count": 170,
    "openalex_id": "https://openalex.org/W2057545667",
    "type": "article"
  },
  {
    "title": "Expressiveness and complexity of XML Schema",
    "doi": "https://doi.org/10.1145/1166074.1166076",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Wim Martens; Frank Neven; Thomas Schwentick; Geert Jan Bex",
    "corresponding_authors": "",
    "abstract": "The common abstraction of XML Schema by unranked regular tree languages is not entirely accurate. To shed some light on the actual expressive power of XML Schema, intuitive semantical characterizations of the Element Declarations Consistent (EDC) rule are provided. In particular, it is obtained that schemas satisfying EDC can only reason about regular properties of ancestors of nodes. Hence, with respect to expressive power, XML Schema is closer to DTDs than to tree automata. These theoretical results are complemented with an investigation of the XML Schema Definitions (XSDs) occurring in practice, revealing that the extra expressiveness of XSDs over DTDs is only used to a very limited extent. As this might be due to the complexity of the XML Schema specification and the difficulty of understanding the effect of constraints on typing and validation of schemas, a simpler formalism equivalent to XSDs is proposed. It is based on contextual patterns rather than on recursive types and it might serve as a light-weight front end for XML Schema. Next, the effect of EDC on the way XML documents can be typed is discussed. It is argued that a cleaner, more robust, larger but equally feasible class is obtained by replacing EDC with the notion of 1-pass preorder typing (1PPT): schemas that allow one to determine the type of an element of a streaming document when its opening tag is met. This notion can be defined in terms of grammars with restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns. Finally, algorithms for recognition, simplification, and inclusion of schemas for the various classes are given.",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W2109486375",
    "type": "article"
  },
  {
    "title": "Optimization of query evaluation algorithms",
    "doi": "https://doi.org/10.1145/320071.320072",
    "publication_date": "1979-06-01",
    "publication_year": 1979,
    "authors": "S. Bing Yao",
    "corresponding_authors": "S. Bing Yao",
    "abstract": "A model of database storage and access is presented. The model represents many evaluation algorithms as special cases, and helps to break a complex algorithm into simple access operations. Generalized access cost equations associated with the model are developed and analyzed. Optimization of these cost equations yields an optimal access algorithm which can be synthesized by a query subsystem whose design is based on the modular access operations.",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W2111607006",
    "type": "article"
  },
  {
    "title": "Domain-independent data cleaning via analysis of entity-relationship graph",
    "doi": "https://doi.org/10.1145/1138394.1138401",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Dmitri V. Kalashnikov; Sharad Mehrotra",
    "corresponding_authors": "",
    "abstract": "In this article, we address the problem of reference disambiguation . Specifically, we consider a situation where entities in the database are referred to using descriptions (e.g., a set of instantiated attributes). The objective of reference disambiguation is to identify the unique entity to which each description corresponds. The key difference between the approach we propose (called RelDC) and the traditional techniques is that RelDC analyzes not only object features but also inter-object relationships to improve the disambiguation quality. Our extensive experiments over two real data sets and over synthetic datasets show that analysis of relationships significantly improves quality of the result.",
    "cited_by_count": 168,
    "openalex_id": "https://openalex.org/W2087183379",
    "type": "article"
  },
  {
    "title": "Disk allocation for Cartesian product files on multiple-disk systems",
    "doi": "https://doi.org/10.1145/319682.319698",
    "publication_date": "1982-03-01",
    "publication_year": 1982,
    "authors": "Hang Du; Jessica Sobolewski",
    "corresponding_authors": "",
    "abstract": "Cartesian product files have recently been shown to exhibit attractive properties for partial match queries. This paper considers the file allocation problem for Cartesian product files, which can be stated as follows: Given a k -attribute Cartesian product file and an m -disk system, allocate buckets among the m disks in such a way that, for all possible partial match queries, the concurrency of disk accesses is maximized. The Disk Modulo (DM) allocation method is described first, and it is shown to be strict optimal under many conditions commonly occurring in practice, including all possible partial match queries when the number of disks is 2 or 3. It is also shown that although it has good performance, the DM allocation method is not strict optimal for all possible partial match queries when the number of disks is greater than 3. The General Disk Modulo (GDM) allocation method is then described, and a sufficient but not necessary condition for strict optimality of the GDM method for all partial match queries and any number of disks is then derived. Simulation studies comparing the DM and random allocation methods in terms of the average number of disk accesses, in response to various classes of partial match queries, show the former to be significantly more effective even when the number of disks is greater than 3, that is, even in cases where the DM method is not strict optimal. The results that have been derived formally and shown by simulation can be used for more effective design of optimal file systems for partial match queries. When considering multiple-disk systems with independent access paths, it is important to ensure that similar records are clustered into the same or similar buckets, while similar buckets should be dispersed uniformly among the disks.",
    "cited_by_count": 167,
    "openalex_id": "https://openalex.org/W2041989747",
    "type": "article"
  },
  {
    "title": "Human factors comparison of a procedural and a nonprocedural query language",
    "doi": "https://doi.org/10.1145/319628.319656",
    "publication_date": "1981-12-01",
    "publication_year": 1981,
    "authors": "Charles Welty; David Stemple",
    "corresponding_authors": "",
    "abstract": "Two experiments testing the ability of subjects to write queries in two different query languages were run. The two languages, SQL and TABLET, differ primarily in their procedurality; both languages use the relational data model, and their Halstead levels are similar. Constructs in the languages which do not affect their procedurality are identical. The two languages were learned by the experimental subjects almost exclusively from manuals presenting the same examples and problems ordered identically for both languages. The results of the experiments show that subjects using the more procedural language wrote difficult queries better than subjects using the less procedural language. The results of the experiments are also used to compare corresponding constructs in the two languages and to recommend improvements for these constructs.",
    "cited_by_count": 162,
    "openalex_id": "https://openalex.org/W1964523235",
    "type": "article"
  },
  {
    "title": "Normalization and hierarchical dependencies in the relational data model",
    "doi": "https://doi.org/10.1145/320263.320271",
    "publication_date": "1978-09-01",
    "publication_year": 1978,
    "authors": "Claude Delobel",
    "corresponding_authors": "Claude Delobel",
    "abstract": "The purpose of this paper is to present a new approach to the conceptual design of logical schemata for relational databases. One-to-one, one-to-many, and many-to-many relationships between the attributes of database relations are modeled by means of functional dependencies and multivalued dependencies. A new type of dependency is introduced: first-order hierarchical decomposition. The properties of this new type of dependency are studied and related to the normalization process of relations. The relationship between the concept of first-order hierarchical decomposition and the notion of hierarchical organization of data is discussed through the normalization process.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W1971475250",
    "type": "article"
  },
  {
    "title": "Fault-tolerance in the borealis distributed stream processing system",
    "doi": "https://doi.org/10.1145/1331904.1331907",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Magdalena Balazinska; Hari Balakrishnan; Samuel Madden; Michael Stonebraker",
    "corresponding_authors": "",
    "abstract": "Over the past few years, Stream Processing Engines (SPEs) have emerged as a new class of software systems, enabling low latency processing of streams of data arriving at high rates. As SPEs mature and get used in monitoring applications that must continuously run (e.g., in network security monitoring), a significant challenge arises: SPEs must be able to handle various software and hardware faults that occur, masking them to provide high availability (HA). In this article, we develop, implement, and evaluate DPC (Delay, Process, and Correct), a protocol to handle crash failures of processing nodes and network failures in a distributed SPE. Like previous approaches to HA, DPC uses replication and masks many types of node and network failures. In the presence of network partitions, the designer of any replication system faces a choice between providing availability or data consistency across the replicas. In DPC, this choice is made explicit: the user specifies an availability bound (no result should be delayed by more than a specified delay threshold even under failure if the corresponding input is available), and DPC attempts to minimize the resulting inconsistency between replicas (not all of which might have seen the input data) while meeting the given delay threshold. Although conceptually simple, the DPC protocol tolerates the occurrence of multiple simultaneous failures as well as any further failures that occur during recovery. This article describes DPC and its implementation in the Borealis SPE. We show that DPC enables a distributed SPE to maintain low-latency processing at all times, while also achieving eventual consistency, where applications eventually receive the complete and correct output streams. Furthermore, we show that, independent of system size and failure location, it is possible to handle failures almost up-to the user-specified bound in a manner that meets the required availability without introducing any inconsistency.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W2008333370",
    "type": "article"
  },
  {
    "title": "Range search on multidimensional uncertain data",
    "doi": "https://doi.org/10.1145/1272743.1272745",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Yufei Tao; Xiaokui Xiao; Reynold Cheng",
    "corresponding_authors": "",
    "abstract": "In an uncertain database, every object o is associated with a probability density function, which describes the likelihood that o appears at each position in a multidimensional workspace. This article studies two types of range retrieval fundamental to many analytical tasks. Specifically, a nonfuzzy query returns all the objects that appear in a search region r q with at least a certain probability t q . On the other hand, given an uncertain object q , fuzzy search retrieves the set of objects that are within distance ε q from q with no less than probability t q . The core of our methodology is a novel concept of “probabilistically constrained rectangle”, which permits effective pruning/validation of nonqualifying/qualifying data. We develop a new index structure called the U-tree for minimizing the query overhead. Our algorithmic findings are accompanied with a thorough theoretical analysis, which reveals valuable insight into the problem characteristics, and mathematically confirms the efficiency of our solutions. We verify the effectiveness of the proposed techniques with extensive experiments.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W2042502381",
    "type": "article"
  },
  {
    "title": "The statistical security of a statistical database",
    "doi": "https://doi.org/10.1145/1994.383392",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "J. F. Traub; Y. Yemini; H. Woźniakowski",
    "corresponding_authors": "",
    "abstract": "This note proposes a statistical perturbation scheme to protect a statistical database against compromise. The proposed scheme can handle the security of numerical as well as nonnumerical sensitive fields. Furthermore, knowledge of some records in a database does not help to compromise unknown records. We use Chebyshev's inequality to analyze the trade-offs among the magnitude of the perturbations, the error incurred by statistical queries, and the size of the query set to which they apply. We show that if the statistician is given absolute error guarantees, then a compromise is possible, but the cost is made exponential in the size of the database.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W2044307594",
    "type": "article"
  },
  {
    "title": "A Theory of Pricing Private Data",
    "doi": "https://doi.org/10.1145/2691190.2691191",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Chao Li; Daniel Yang Li; Gerome Miklau; Dan Suciu",
    "corresponding_authors": "",
    "abstract": "Personal data has value to both its owner and to institutions who would like to analyze it. Privacy mechanisms protect the owner's data while releasing to analysts noisy versions of aggregate query results. But such strict protections of the individual's data have not yet found wide use in practice. Instead, Internet companies, for example, commonly provide free services in return for valuable sensitive information from users, which they exploit and sometimes sell to third parties. As awareness of the value of personal data increases, so has the drive to compensate the end-user for her private information. The idea of monetizing private data can improve over the narrower view of hiding private data, since it empowers individuals to control their data through financial means. In this article we propose a theoretical framework for assigning prices to noisy query answers as a function of their accuracy, and for dividing the price amongst data owners who deserve compensation for their loss of privacy. Our framework adopts and extends key principles from both differential privacy and query pricing in data markets. We identify essential properties of the pricing function and micropayments, and characterize valid solutions.",
    "cited_by_count": 158,
    "openalex_id": "https://openalex.org/W1989291641",
    "type": "article"
  },
  {
    "title": "Optimism and consistency in partitioned distributed database systems",
    "doi": "https://doi.org/10.1145/1270.1499",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Susan B. Davidson",
    "corresponding_authors": "Susan B. Davidson",
    "abstract": "A protocol for transaction processing during partition failures is presented which guarantees mutual consistency between copies of data-items after repair is completed. The protocol is “optimistic” in that transactions are processed without restrictions during failure; conflicts are then detected at repair time using a precedence graph , and are resolved by backing out transactions according to some backout strategy . The resulting database state then corresponds to a serial execution of some subset of transactions run during the failure. Results from simulation and probabilistic modeling show that the optimistic protocol is a reasonable alternative in many cases. Conditions under which the protocol performs well are noted, and suggestions are made as to how performance can be improved. In particular, a backout strategy is presented which takes into account individual transaction costs and attempts to minimize total backout cost. Although the problem of choosing transactions to minimize total backout cost is, in general, NP-complete, the backout strategy is efficient and produces very good results.",
    "cited_by_count": 158,
    "openalex_id": "https://openalex.org/W2030195504",
    "type": "article"
  },
  {
    "title": "A new normal form for nested relations",
    "doi": "https://doi.org/10.1145/12047.13676",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "Z. Meral Özsoyoğlu; Li-Yan Yuan",
    "corresponding_authors": "",
    "abstract": "We consider nested relations whose schemes are structured as trees, called scheme trees, and introduce a normal form for such relations, called the nested normal form. Given a set of attributes U , and a set of multivalued dependencies (MVDs) M over these attributes, we present an algorithm to obtain a nested normal form decomposition of U with respect to M . Such a decomposition has several desirable properties, such as explicitly representing a set of full and embedded MVDs implied by M , and being a faithful and nonredundant representation of U . Moreover, if the given set of MVDs is conflict-free, then the nested normal form decomposition is also dependency-preserving. Finally, we show that if M is conflict-free, then the set of root-to-leaf paths of scheme trees in nested normal form decomposition is precisely the unique 4NF decomposition [9, 16] of U with respect to M .",
    "cited_by_count": 157,
    "openalex_id": "https://openalex.org/W2057852703",
    "type": "article"
  },
  {
    "title": "EXPRESS",
    "doi": "https://doi.org/10.1145/320544.320549",
    "publication_date": "1977-06-01",
    "publication_year": 1977,
    "authors": "Nan Shu; Barron C. Housel; Robert W. Taylor; Soumyadip Ghosh; Vincent Y. Lum",
    "corresponding_authors": "",
    "abstract": "EXPRESS is an experimental prototype data translation system which can access a wide variety of data and restructure it for new uses. The system is driven by two very high level nonprocedural languages: DEFINE for data description and CONVERT for data restructuring. Program generation and cooperating process techniques are used to achieve efficient operation. This paper describes the design and implementation of EXPRESS. DEFINE and CONVERT are summarized and the implementation architecture presented. The DEFINE description is compiled into a customized PL/1 program for accessing source data. The restructuring specified in CONVERT is compiled into a set of customized PL/1 procedures to derive multiple target files from multiple input files. Job steps and job control statements are generated automatically. During execution, the generated procedures run under control of a process supervisor, which coordinates buffer management and handles file allocation, deallocation, and all input/output requests. The architecture of EXPRESS allows efficiency in execution by avoiding unnecessary secondary storage references while at the same time allowing the individual procedures to be independent of each other. Its modular structure permits the system to be extended or transferred to another environment easily.",
    "cited_by_count": 156,
    "openalex_id": "https://openalex.org/W1966294080",
    "type": "article"
  },
  {
    "title": "ElasTraS",
    "doi": "https://doi.org/10.1145/2445583.2445588",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Sudipto Das; Divyakant Agrawal; Amr El Abbadi",
    "corresponding_authors": "",
    "abstract": "A database management system (DBMS) serving a cloud platform must handle large numbers of application databases (or tenants ) that are characterized by diverse schemas, varying footprints, and unpredictable load patterns. Scaling out using clusters of commodity servers and sharing resources among tenants (i.e., multitenancy ) are important features of such systems. Moreover, when deployed on a pay-per-use infrastructure, minimizing the system's operating cost while ensuring good performance is also an important goal. Traditional DBMSs were not designed for such scenarios and hence do not possess the mentioned features critical for DBMSs in the cloud. We present ElasTraS, which combines three design principles to build an elastically-scalable multitenant DBMS for transaction processing workloads. These design principles are gleaned from a careful analysis of the years of research in building scalable key-value stores and decades of research in high performance transaction processing systems. ElasTraS scales to thousands of tenants, effectively consolidates tenants with small footprints while scaling-out large tenants across multiple servers in a cluster. ElasTraS also supports low-latency multistep ACID transactions , is fault-tolerant, self-managing, and highly available to support mission critical applications. ElasTraS leverages Albatross, a low overhead on-demand live database migration technique, for elastic load balancing by adding more servers during high load and consolidating to fewer servers during usage troughs. This elastic scaling minimizes the operating cost and ensures good performance even in the presence of unpredictable changes to the workload. We elucidate the design principles, explain the architecture, describe a prototype implementation, present the detailed design and implementation of Albatross, and experimentally evaluate the implementation using a variety of transaction processing workloads. On a cluster of 20 commodity servers, our prototype serves thousands of tenants and serves more than 1 billion transactions per day while migrating tenant databases with minimal overhead to allow lightweight elastic scaling. Using a cluster of 30 commodity servers, ElasTraS can scale-out a terabyte TPC-C database serving an aggregate throughput of approximately one quarter of a million TPC-C transactions per minute.",
    "cited_by_count": 156,
    "openalex_id": "https://openalex.org/W2150503355",
    "type": "article"
  },
  {
    "title": "Encryption policies for regulating access to outsourced data",
    "doi": "https://doi.org/10.1145/1735886.1735891",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Sabrina De Capitani di Vimercati; Sara Foresti; Sushil Jajodia; Stefano Paraboschi; Pierangela Samarati",
    "corresponding_authors": "",
    "abstract": "Current access control models typically assume that resources are under the strict custody of a trusted party which monitors each access request to verify if it is compliant with the specified access control policy. There are many scenarios where this approach is becoming no longer adequate. Many clear trends in Web technology are creating a need for owners of sensitive information to manage access to it by legitimate users using the services of honest but curious third parties, that is, parties trusted with providing the required service but not authorized to read the actual data content. In this scenario, the data owner encrypts the data before outsourcing and stores them at the server. Only the data owner and users with knowledge of the key will be able to decrypt the data. Possible access authorizations are to be enforced by the owner. In this article, we address the problem of enforcing selective access on outsourced data without need of involving the owner in the access control process. The solution puts forward a novel approach that combines cryptography with authorizations, thus enforcing access control via selective encryption . The article presents a formal model for access control management and illustrates how an authorization policy can be translated into an equivalent encryption policy while minimizing the amount of keys and cryptographic tokens to be managed. The article also introduces a two-layer encryption approach that allows the data owner to outsource, besides the data, the complete management of the authorization policy itself, thus providing efficiency and scalability in dealing with policy updates. We also discuss experimental results showing that our approach is able to efficiently manage complex scenarios.",
    "cited_by_count": 154,
    "openalex_id": "https://openalex.org/W2071940270",
    "type": "article"
  },
  {
    "title": "Transactions and consistency in distributed database systems",
    "doi": "https://doi.org/10.1145/319732.319734",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Irving L. Traiger; Jim Gray; Cesare A. Galtieri; Bruce G. Lindsay",
    "corresponding_authors": "",
    "abstract": "The concepts of transaction and of data consistency are defined for a distributed system. The cases of partitioned data, where fragments of a file are stored at multiple nodes, and replicated data, where a file is replicated at several nodes, are discussed. It is argued that the distribution and replication of data should be transparent to the programs which use the data. That is, the programming interface should provide location transparency, replica transparency, concurrency transparency, and failure transparency. Techniques for providing such transparencies are abstracted and discussed. By extending the notions of system schedule and system clock to handle multiple nodes, it is shown that a distributed system can be modeled as a single sequential execution sequence. This model is then used to discuss simple techniques for implementing the various forms of transparency.",
    "cited_by_count": 154,
    "openalex_id": "https://openalex.org/W2083875802",
    "type": "article"
  },
  {
    "title": "Finding maximal cliques in massive networks",
    "doi": "https://doi.org/10.1145/2043652.2043654",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "James Cheng; Yiping Ke; Ada Wai-Chee Fu; Jeffrey Xu Yu; Linhong Zhu",
    "corresponding_authors": "",
    "abstract": "Maximal clique enumeration is a fundamental problem in graph theory and has important applications in many areas such as social network analysis and bioinformatics. The problem is extensively studied; however, the best existing algorithms require memory space linear in the size of the input graph. This has become a serious concern in view of the massive volume of today's fast-growing networks. We propose a general framework for designing external-memory algorithms for maximal clique enumeration in large graphs. The general framework enables maximal clique enumeration to be processed recursively in small subgraphs of the input graph, thus allowing in-memory computation of maximal cliques without the costly random disk access. We prove that the set of cliques obtained by the recursive local computation is both correct (i.e., globally maximal) and complete. The subgraph to be processed each time is defined based on a set of base vertices that can be flexibly chosen to achieve different purposes. We discuss the selection of the base vertices to fully utilize the available memory in order to minimize I/O cost in static graphs, and for update maintenance in dynamic graphs. We also apply our framework to design an external-memory algorithm for maximum clique computation in a large graph.",
    "cited_by_count": 153,
    "openalex_id": "https://openalex.org/W1998341696",
    "type": "article"
  },
  {
    "title": "Maximal objects and the semantics of universal relation databases",
    "doi": "https://doi.org/10.1145/319830.319831",
    "publication_date": "1983-03-01",
    "publication_year": 1983,
    "authors": "David Maier; Jeffrey D. Ullman",
    "corresponding_authors": "",
    "abstract": "The universal relation concept is intended to provide the database user with a simplified model in which he can compose queries without regard to the underlying structure of the relations in the database. Frequently, the lossless join criterion provides the query interpreter with the clue needed to interpret the query as the user intended. However, some examples exist where interpretation by the lossless-join rule runs contrary to our intuition. To handle some of these cases, we propose a concept called maximal objects , which modifies the universal relation concept in exactly those situations where it appears to go awry—when the underlying relational structure has “cycles.” We offer examples of how the maximal object concept provides intuitively correct interpretations. We also consider how one might construct maximal objects mechanically from purely syntactic structural information—the relation schemes and functional dependencies—about the database.",
    "cited_by_count": 153,
    "openalex_id": "https://openalex.org/W2066954445",
    "type": "article"
  },
  {
    "title": "Expressive Languages for Path Queries over Graph-Structured Data",
    "doi": "https://doi.org/10.1145/2389241.2389250",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Pablo Barceló; Leonid Libkin; Anthony W. Lin; Peter T. Wood",
    "corresponding_authors": "",
    "abstract": "For many problems arising in the setting of graph querying (such as finding semantic associations in RDF graphs, exact and approximate pattern matching, sequence alignment, etc.), the power of standard languages such as the widely studied conjunctive regular path queries (CRPQs) is insufficient in at least two ways. First, they cannot output paths and second, more crucially, they cannot express relationships among paths. We thus propose a class of extended CRPQs, called ECRPQs, which add regular relations on tuples of paths, and allow path variables in the heads of queries. We provide several examples of their usefulness in querying graph structured data, and study their properties. We analyze query evaluation and representation of tuples of paths in the output by means of automata. We present a detailed analysis of data and combined complexity of queries, and consider restrictions that lower the complexity of ECRPQs to that of relational conjunctive queries. We study the containment problem, and look at further extensions with first-order features, and with nonregular relations that add arithmetic constraints on the lengths of paths and numbers of occurrences of labels.",
    "cited_by_count": 150,
    "openalex_id": "https://openalex.org/W2117657788",
    "type": "article"
  },
  {
    "title": "Supporting views in data stream management systems",
    "doi": "https://doi.org/10.1145/1670243.1670244",
    "publication_date": "2008-02-15",
    "publication_year": 2008,
    "authors": "Thanaa M. Ghanem; Ahmed K. Elmagarmid; Per-Åke Larson; Walid G. Aref",
    "corresponding_authors": "",
    "abstract": "In relational database management systems, views supplement basic query constructs to cope with the demand for “higher-level” views of data. Moreover, in traditional query optimization, answering a query using a set of existing materialized views can yield a more efficient query execution plan. Due to their effectiveness, views are attractive to data stream management systems. In order to support views over streams, a data stream management system should employ a closed (or composable) continuous query language. A closed query language is a language in which query inputs and outputs are interpreted in the same way, hence allowing query composition. This article introduces the Synchronized SQL (or SyncSQL) query language that defines a data stream as a sequence of modify operations against a relation. SyncSQL enables query composition through the unified interpretation of query inputs and outputs. An important issue in continuous queries over data streams is the frequency by which the answer gets refreshed and the conditions that trigger the refresh. Coarser periodic refresh requirements are typically expressed as sliding windows. In this article, the sliding window approach is generalized by introducing the synchronization principle that empowers SyncSQL with a formal mechanism to express queries with arbitrary refresh conditions. After introducing the semantics and syntax, we lay the algebraic foundation for SyncSQL and propose a query-matching algorithm for deciding containment of SyncSQL expressions. Then, the article introduces the Nile-SyncSQL prototype to support SyncSQL queries. Nile-SyncSQL employs a pipelined incremental evaluation paradigm in which the query pipeline consists of a set of differential operators. A cost model is developed to estimate the cost of SyncSQL query execution pipelines and to choose the best execution plan from a set of different plans for the same query. An experimental study is conducted to evaluate the performance of Nile-SyncSQL. The experimental results illustrate the effectiveness of Nile-SyncSQL and the significant performance gains when views are enabled in data stream management systems.",
    "cited_by_count": 148,
    "openalex_id": "https://openalex.org/W2034964304",
    "type": "article"
  },
  {
    "title": "Ontology-Based Data Access",
    "doi": "https://doi.org/10.1145/2661643",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Meghyn Bienvenu; Balder ten Cate; Carsten Lutz; Frank Wolter",
    "corresponding_authors": "",
    "abstract": "Ontology-based data access is concerned with querying incomplete data sources in the presence of domain-specific knowledge provided by an ontology. A central notion in this setting is that of an ontology-mediated query , which is a database query coupled with an ontology. In this article, we study several classes of ontology-mediated queries, where the database queries are given as some form of conjunctive query and the ontologies are formulated in description logics or other relevant fragments of first-order logic, such as the guarded fragment and the unary negation fragment. The contributions of the article are threefold. First, we show that popular ontology-mediated query languages have the same expressive power as natural fragments of disjunctive datalog, and we study the relative succinctness of ontology-mediated queries and disjunctive datalog queries. Second, we establish intimate connections between ontology-mediated queries and constraint satisfaction problems (CSPs) and their logical generalization, MMSNP formulas. Third, we exploit these connections to obtain new results regarding: (i) first-order rewritability and datalog rewritability of ontology-mediated queries; (ii) P/NP dichotomies for ontology-mediated queries; and (iii) the query containment problem for ontology-mediated queries.",
    "cited_by_count": 147,
    "openalex_id": "https://openalex.org/W2763521382",
    "type": "article"
  },
  {
    "title": "Authority-based keyword search in databases",
    "doi": "https://doi.org/10.1145/1331904.1331905",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Vagelis Hristidis; Heasoo Hwang; Yannis Papakonstantinou",
    "corresponding_authors": "",
    "abstract": "Our system applies authority-based ranking to keyword search in databases modeled as labeled graphs. Three ranking factors are used: the relevance to the query, the specificity and the importance of the result. All factors are handled using authority-flow techniques that exploit the link-structure of the data graph, in contrast to traditional Information Retrieval. We address the performance challenges in computing the authority flows in databases by using precomputation and exploiting the database schema if present. We conducted user surveys and performance experiments on multiple real and synthetic datasets, to assess the semantic meaningfulness and performance of our system.",
    "cited_by_count": 142,
    "openalex_id": "https://openalex.org/W2070032453",
    "type": "article"
  },
  {
    "title": "A survey on representation, composition and application of preferences in database systems",
    "doi": "https://doi.org/10.1145/2000824.2000829",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Kostas Stefanidis; Georgia Koutrika; Evaggelia Pitoura",
    "corresponding_authors": "",
    "abstract": "Preferences have been traditionally studied in philosophy, psychology, and economics and applied to decision making problems. Recently, they have attracted the attention of researchers in other fields, such as databases where they capture soft criteria for queries. Databases bring a whole fresh perspective to the study of preferences, both computational and representational. From a representational perspective, the central question is how we can effectively represent preferences and incorporate them in database querying. From a computational perspective, we can look at how we can efficiently process preferences in the context of database queries. Several approaches have been proposed but a systematic study of these works is missing. The purpose of this survey is to provide a framework for placing existing works in perspective and highlight critical open challenges to serve as a springboard for researchers in database systems. We organize our study around three axes: preference representation, preference composition, and preference query processing.",
    "cited_by_count": 138,
    "openalex_id": "https://openalex.org/W2168976073",
    "type": "article"
  },
  {
    "title": "Reliability mechanisms for SDD-1",
    "doi": "https://doi.org/10.1145/320610.320621",
    "publication_date": "1980-12-01",
    "publication_year": 1980,
    "authors": "M. M. Hammer; David W. Shipman",
    "corresponding_authors": "",
    "abstract": "This paper presents the reliability mechanisms of SDD-1, a prototype distributed database system being developed by the Computer Corporation of America. Reliability algorithms in SDD-1 center around the concept of the Reliable Network (RelNet). The RelNet is a communications medium incorporating facilities for site status monitoring, event timestamping, multiply buffered message delivery, and the atomic control of distributed transactions. This paper is one of a series of companion papers on SDD-1 [3, 4, 6, 13].",
    "cited_by_count": 136,
    "openalex_id": "https://openalex.org/W1968333714",
    "type": "article"
  },
  {
    "title": "Mergeable summaries",
    "doi": "https://doi.org/10.1145/2500128",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Pankaj K. Agarwal; Graham Cormode; Zengfeng Huang; Jeff M. Phillips; Zhewei Wei; Ke Yi",
    "corresponding_authors": "",
    "abstract": "We study the mergeability of data summaries. Informally speaking, mergeability requires that, given two summaries on two datasets, there is a way to merge the two summaries into a single summary on the two datasets combined together, while preserving the error and size guarantees. This property means that the summaries can be merged in a way akin to other algebraic operators such as sum and max, which is especially useful for computing summaries on massive distributed data. Several data summaries are trivially mergeable by construction, most notably all the sketches that are linear functions of the datasets. But some other fundamental ones, like those for heavy hitters and quantiles, are not (known to be) mergeable. In this article, we demonstrate that these summaries are indeed mergeable or can be made mergeable after appropriate modifications. Specifically, we show that for ϵ-approximate heavy hitters, there is a deterministic mergeable summary of size O(1/ϵ); for ϵ-approximate quantiles, there is a deterministic summary of size O((1/ϵ) log(ϵ n)) that has a restricted form of mergeability, and a randomized one of size O((1/ϵ) log3/2(1/ϵ)) with full mergeability. We also extend our results to geometric summaries such as ϵ-approximations which permit approximate multidimensional range counting queries. While most of the results in this article are theoretical in nature, some of the algorithms are actually very simple and even perform better than the previously best known algorithms, which we demonstrate through experiments in a simulated sensor network. We also achieve two results of independent interest: (1) we provide the best known randomized streaming bound for ϵ-approximate quantiles that depends only on ϵ, of size O((1/ϵ) log3/2(1/ϵ)), and (2) we demonstrate that the MG and the SpaceSaving summaries for heavy hitters are isomorphic",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W2294895103",
    "type": "article"
  },
  {
    "title": "Incremental graph pattern matching",
    "doi": "https://doi.org/10.1145/2489791",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Wenfei Fan; Xin Wang; Yinghui Wu",
    "corresponding_authors": "",
    "abstract": "Graph pattern matching is commonly used in a variety of emerging applications such as social network analysis. These applications highlight the need for studying the following two issues. First, graph pattern matching is traditionally defined in terms of subgraph isomorphism or graph simulation. These notions, however, often impose too strong a topological constraint on graphs to identify meaningful matches. Second, in practice a graph is typically large, and is frequently updated with small changes. It is often prohibitively expensive to recompute matches starting from scratch via batch algorithms when the graph is updated. This article studies these two issues. (1) We propose to define graph pattern matching based on a notion of bounded simulation , which extends graph simulation by specifying the connectivity of nodes in a graph within a predefined number of hops. We show that bounded simulation is able to find sensible matches that the traditional matching notions fail to catch. We also show that matching via bounded simulation is in cubic time, by giving such an algorithm. (2) We provide an account of results on incremental graph pattern matching, for matching defined with graph simulation, bounded simulation, and subgraph isomorphism. We show that the incremental matching problem is unbounded , that is, its cost is not determined alone by the size of the changes in the input and output, for all these matching notions. Nonetheless, when matching is defined in terms of simulation or bounded simulation, incremental matching is semibounded , that is, its worst-time complexity is bounded by a polynomial in the size of the changes in the input, output, and auxiliary information that is necessarily maintained to reuse previous computation, and the size of graph patterns. We also develop incremental matching algorithms for graph simulation and bounded simulation, by minimizing unnecessary recomputation. In contrast, matching based on subgraph isomorphism is neither bounded nor semibounded. (3) We experimentally verify the effectiveness and efficiency of these algorithms, and show that: (a) the revised notion of graph pattern matching allows us to identify communities commonly found in real-life networks, and (b) the incremental algorithms substantially outperform their batch counterparts in response to small changes. These suggest a promising framework for real-life graph pattern matching.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W2062705952",
    "type": "article"
  },
  {
    "title": "Size Bounds for Factorised Representations of Query Results",
    "doi": "https://doi.org/10.1145/2656335",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Dan Olteanu; Jakub Závodný",
    "corresponding_authors": "",
    "abstract": "We study two succinct representation systems for relational data based on relational algebra expressions with unions, Cartesian products, and singleton relations: f-representations, which employ algebraic factorisation using distributivity of product over union, and d-representations, which are f-representations where further succinctness is brought by explicit sharing of repeated subexpressions. In particular we study such representations for results of conjunctive queries. We derive tight asymptotic bounds for representation sizes and present algorithms to compute representations within these bounds. We compare the succinctness of f-representations and d-representations for results of equi-join queries, and relate them to fractional edge covers and fractional hypertree decompositions of the query hypergraph. Recent work showed that f-representations can significantly boost the performance of query evaluation in centralised and distributed settings and of machine learning tasks.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2050100848",
    "type": "article"
  },
  {
    "title": "Strong simulation",
    "doi": "https://doi.org/10.1145/2528937",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Shuai Ma; Yang Cao; Wenfei Fan; Jinpeng Huai; Tianyu Wo",
    "corresponding_authors": "",
    "abstract": "Graph pattern matching is finding all matches in a data graph for a given pattern graph and is often defined in terms of subgraph isomorphism, an NP -complete problem. To lower its complexity, various extensions of graph simulation have been considered instead. These extensions allow graph pattern matching to be conducted in cubic time. However, they fall short of capturing the topology of data graphs, that is, graphs may have a structure drastically different from pattern graphs they match, and the matches found are often too large to understand and analyze. To rectify these problems, this article proposes a notion of strong simulation , a revision of graph simulation for graph pattern matching. (1) We identify a set of criteria for preserving the topology of graphs matched. We show that strong simulation preserves the topology of data graphs and finds a bounded number of matches. (2) We show that strong simulation retains the same complexity as earlier extensions of graph simulation by providing a cubic-time algorithm for computing strong simulation. (3) We present the locality property of strong simulation which allows us to develop an effective distributed algorithm to conduct graph pattern matching on distributed graphs. (4) We experimentally verify the effectiveness and efficiency of these algorithms using both real-life and synthetic data.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W2055898276",
    "type": "article"
  },
  {
    "title": "Differential dependencies",
    "doi": "https://doi.org/10.1145/2000824.2000826",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Shaoxu Song; Lei Chen",
    "corresponding_authors": "",
    "abstract": "The importance of difference semantics (e.g., “similar” or “dissimilar”) has been recently recognized for declaring dependencies among various types of data, such as numerical values or text values. We propose a novel form of Differential Dependencies (dds), which specifies constraints on difference, called differential functions , instead of identification functions in traditional dependency notations like functional dependencies. Informally, a differential dependency states that if two tuples have distances on attributes X agreeing with a certain differential function, then their distances on attributes Y should also agree with the corresponding differential function on Y . For example, [date(≤ 7)]→[price(&lt; 100)] states that the price difference of any two days within a week length should be no greater than 100 dollars. Such differential dependencies are useful in various applications, for example, violation detection, data partition, query optimization, record linkage, etc. In this article, we first address several theoretical issues of differential dependencies, including formal definitions of dds and differential keys, subsumption order relation of differential functions, implication of dds, closure of a differential function, a sound and complete inference system, and minimal cover for dds. Then, we investigate a practical problem, that is, how to discover dds and differential keys from a given dataset. Due to the intrinsic hardness, we develop several pruning methods to improve the discovery efficiency in practice. Finally, through an extensive experimental evaluation on real datasets, we demonstrate the discovery performance and the effectiveness of dds in several real applications.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W2108132403",
    "type": "article"
  },
  {
    "title": "Efficient Computation of the Tree Edit Distance",
    "doi": "https://doi.org/10.1145/2699485",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Mateusz Pawlik; Nikolaus Augsten",
    "corresponding_authors": "",
    "abstract": "We consider the classical tree edit distance between ordered labelled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worst-case complexity but the worst case happens frequently, or they are very efficient for some tree shapes but degenerate for others. This leads to unpredictable and often infeasible runtimes. There is no obvious way to choose between the algorithms. In this article we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of our algorithm is smaller than or equal to the complexity of the best competitors for any input instance, that is, our algorithm is both efficient and worst-case optimal. This is achieved by computing a dynamic decomposition strategy that depends on the input trees. RTED is shown optimal among all algorithms that use LRH ( left-right-heavy ) strategies, which include RTED and the fastest tree edit distance algorithms presented in literature. In our experiments on synthetic and real-world data we empirically evaluate our solution and compare it to the state-of-the-art.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W2028509346",
    "type": "article"
  },
  {
    "title": "EmptyHeaded",
    "doi": "https://doi.org/10.1145/3129246",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Christopher R. Aberger; Andrew Lamb; Susan Tu; Andres Nötzli; Kunle Olukotun; Christopher Ré",
    "corresponding_authors": "",
    "abstract": "There are two types of high-performance graph processing engines: low- and high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide optimized data structures and computation models but require users to write low-level imperative code, hence ensuring that efficiency is the burden of the user. In high-level engines, users write in query languages like datalog (SociaLite) or SQL (Grail). High-level engines are easier to use but are orders of magnitude slower than the low-level graph engines. We present EmptyHeaded, a high-level engine that supports a rich datalog-like query language and achieves performance comparable to that of low-level engines. At the core of EmptyHeaded’s design is a new class of join algorithms that satisfy strong theoretical guarantees, but have thus far not achieved performance comparable to that of specialized graph processing engines. To achieve high performance, EmptyHeaded introduces a new join engine architecture, including a novel query optimizer and execution engine that leverage single-instruction multiple data (SIMD) parallelism. With this architecture, EmptyHeaded outperforms high-level approaches by up to three orders of magnitude on graph pattern queries, PageRank, and Single-Source Shortest Paths (SSSP) and is an order of magnitude faster than many low-level baselines. We validate that EmptyHeaded competes with the best-of-breed low-level engine (Galois), achieving comparable performance on PageRank and at most 3× worse performance on SSSP. Finally, we show that the EmptyHeaded design can easily be extended to accommodate a standard resource description framework (RDF) workload, the LUBM benchmark. On the LUBM benchmark, we show that EmptyHeaded can compete with and sometimes outperform two high-level, but specialized RDF baselines (TripleBit and RDF-3X), while outperforming MonetDB by up to three orders of magnitude and LogicBlox by up to two orders of magnitude.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W4244579861",
    "type": "article"
  },
  {
    "title": "Exact Model Counting of Query Expressions",
    "doi": "https://doi.org/10.1145/2984632",
    "publication_date": "2017-02-03",
    "publication_year": 2017,
    "authors": "Paul Beame; Jerry Li; Sudeepa Roy; Dan Suciu",
    "corresponding_authors": "",
    "abstract": "We prove exponential lower bounds on the running time of the state-of-the-art exact model counting algorithms—algorithms for exactly computing the number of satisfying assignments, or the satisfying probability, of Boolean formulas. These algorithms can be seen, either directly or indirectly, as building Decision-Decomposable Negation Normal Form (decision-DNNF) representations of the input Boolean formulas. Decision-DNNFs are a special case of d -DNNFs where d stands for deterministic . We show that any knowledge compilation representations from a class (called DLDDs in this article) that contain decision-DNNFs can be converted into equivalent Free Binary Decision Diagrams (FBDDs) , also known as Read-Once Branching Programs , with only a quasi-polynomial increase in representation size. Leveraging known exponential lower bounds for FBDDs, we then obtain similar exponential lower bounds for decision-DNNFs, which imply exponential lower bounds for model-counting algorithms. We also separate the power of decision-DNNFs from d -DNNFs and a generalization of decision-DNNFs known as AND-FBDDs. We then prove new lower bounds for FBDDs that yield exponential lower bounds on the running time of these exact model counters when applied to the problem of query evaluation in tuple-independent probabilistic databases—computing the probability of an answer to a query given independent probabilities of the individual tuples in a database instance. This approach to the query evaluation problem, in which one first obtains the lineage for the query and database instance as a Boolean formula and then performs weighted model counting on the lineage, is known as grounded inference . A second approach, known as lifted inference or extensional query evaluation , exploits the high-level structure of the query as a first-order formula. Although it has been widely believed that lifted inference is strictly more powerful than grounded inference on the lineage alone, no formal separation has previously been shown for query evaluation. In this article, we show such a formal separation for the first time. In particular, we exhibit a family of database queries for which polynomial-time extensional query evaluation techniques were previously known but for which query evaluation via grounded inference using the state-of-the-art exact model counters requires exponential time.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2586666171",
    "type": "article"
  },
  {
    "title": "A Survey of Spatial Crowdsourcing",
    "doi": "https://doi.org/10.1145/3291933",
    "publication_date": "2019-03-15",
    "publication_year": 2019,
    "authors": "Srinivasa Raghavendra Bhuvan Gummidi; Xike Xie; Torben Bach Pedersen",
    "corresponding_authors": "",
    "abstract": "Widespread use of advanced mobile devices has led to the emergence of a new class of crowdsourcing called spatial crowdsourcing. Spatial crowdsourcing advances the potential of a crowd to perform tasks related to real-world scenarios involving physical locations, which were not feasible with conventional crowdsourcing methods. The main feature of spatial crowdsourcing is the presence of spatial tasks that require workers to be physically present at a particular location for task fulfillment. Research related to this new paradigm has gained momentum in recent years, necessitating a comprehensive survey to offer a bird's-eye view of the current state of spatial crowdsourcing literature. In this article, we discuss the spatial crowdsourcing infrastructure and identify the fundamental differences between spatial and conventional crowdsourcing. Furthermore, we provide a comprehensive view of the existing literature by introducing a taxonomy, elucidate the issues/challenges faced by different components of spatial crowdsourcing, and suggest potential research directions for the future.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2922336629",
    "type": "article"
  },
  {
    "title": "Transactional client-server cache consistency",
    "doi": "https://doi.org/10.1145/261124.261125",
    "publication_date": "1997-09-01",
    "publication_year": 1997,
    "authors": "Michael J. Franklin; Michael J. Carey; Miron Livny",
    "corresponding_authors": "",
    "abstract": "Client-server database systems based on a data shipping model can exploit client memory resources by caching copies of data items across transaction boundaries. Caching reduces the need to obtain data from servers or other sites on the network. In order to ensure that such caching does not result in the violation of transaction semantics, a transactional cache consistency maintenance algorithm is required. Many such algorithms have been proposed in the literature and, as all provide the same functionality, performance is a primary concern in choosing among them. In this article we present a taxonomy that describes the design space for transactional cache consistency maintenance algorithms and show how proposed algorithms relate to one another. We then investigate the performance of six of these algorithms, and use these results to examine the tradeoffs inherent in the design choices identified in the taxonomy. The results show that the interactions among dimensions of the design space impact performance in many ways, and that classifications of algorithms as simply “pessimistic” or “optimistic” do not accurately characterize the similarities and differences among the many possible cache consistency algorithms.",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W1971279826",
    "type": "article"
  },
  {
    "title": "SilkRoute",
    "doi": "https://doi.org/10.1145/582410.582413",
    "publication_date": "2002-12-01",
    "publication_year": 2002,
    "authors": "Mary Fernández; Yana Kadiyska; Dan Suciu; Atsuyuki Morishima; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "XML is the \"lingua franca\" for data exchange between interenterprise applications. In this work, we describe SilkRoute, a framework for publishing relational data in XML. In SilkRoute, relational data is published in three steps: the relational tables are presented to the database administrator in a canonical XML view; the database administrator defines in the XQuery query language a public, virtual XML view over the canonical XML view; and an application formulates an XQuery query over the public view. SilkRoute composes the application query with the public-view query, translates the result into SQL, executes this on the relational engine, and assembles the resulting tuple streams into an XML document. This work makes some key contributions to XML query processing. First, it describes an algorithm that translates an XQuery expression into SQL. The translation depends on a query representation that separates the structure of the output XML document from the computation that produces the document's content. The second contribution addresses the optimization problem of how to decompose an XML view over a relational database into an optimal set of SQL queries. We define formally the optimization problem, describe the search space, and propose a greedy, cost-based optimization algorithm, which obtains its cost estimates from the relational engine. Experiments confirm that the algorithm produces queries that are nearly optimal.",
    "cited_by_count": 168,
    "openalex_id": "https://openalex.org/W2075552472",
    "type": "article"
  },
  {
    "title": "Emancipating instances from the tyranny of classes in information modeling",
    "doi": "https://doi.org/10.1145/357775.357778",
    "publication_date": "2000-06-01",
    "publication_year": 2000,
    "authors": "Jeffrey Parsons; Yair Wand",
    "corresponding_authors": "",
    "abstract": "Database design commonly assumes, explicitly or implicitly, that instances must belong to classes. This can be termed the assumption of inherent classification . We argue that the extent and complexity of problems in schema integration, schema evolution, and interoperability are, to a large degree, consequences of inherent classification. Furthermore, we make the case that the assumption of inherent classification violates philosophical and cognitive guidelines on classification and is, therefore, inappropriate in view of the role of data modeling in representing knowledge about application domains. As an alternative, we propose a layered approach to modeling in which information about instances is separated from any particular classification. Two data modeling layers are proposed: (1) an instance model consisting of an instance base (i.e., information about instances and properties) and operations to populate, use, and maintain it; and (2) a class model consisting of a class base (i.e., information about classes defined in terms of properties) and operations to populate, use, and maintain it. The two-layered model provides class independence . This is analogous to the arguments of data independence offered by the relational model in comparison to hierarchical and network models. We show that a two-layered approach yields several advantages. In particular, schema integration is shown to be partially an artifact of inherent classification that can be greatly simplified in designing a database based on a layered model; schema evolution is supported without the complexity of operations currently required by class-based models; and the difficulties associated with interoperability among heterogeneous databases are reduced because there is no need to agree on the semantics of classes among independent databases. We conclude by considering the adequacy of a two-layered approach, outlining possible implementation strategies, and drawing attention to some practical considerations.",
    "cited_by_count": 165,
    "openalex_id": "https://openalex.org/W2090154982",
    "type": "article"
  },
  {
    "title": "On the semantics of “now” in databases",
    "doi": "https://doi.org/10.1145/249978.249980",
    "publication_date": "1997-06-01",
    "publication_year": 1997,
    "authors": "James Clifford; Curtis Dyreson; Tomás Isakowitz; Christian S. Jensen; Richard T. Snodgrass",
    "corresponding_authors": "",
    "abstract": "Although “ now ” is expressed in SQL and CURRENT_TIMESTAMP within queries, this value cannot be stored in the database. How ever, this notion of an ever-increasing current-time value has been reflected in some temporal data models by inclusion of database-resident variables, such as “ now ” “ until-changed, ” “**,” “@,” and “-”. Time variables are very desirable, but their used also leads to a new type of database, consisting of tuples with variables, termed a variable database.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W2004851292",
    "type": "article"
  },
  {
    "title": "Atomicity and isolation for transactional processes",
    "doi": "https://doi.org/10.1145/507234.507236",
    "publication_date": "2002-03-01",
    "publication_year": 2002,
    "authors": "Heiko Schuldt; Gustavo Alonso; Catriel Beeri; Hans‐Jörg Schek",
    "corresponding_authors": "",
    "abstract": "Processes are increasingly being used to make complex application logic explicit. Programming using processes has significant advantages but it poses a difficult problem from the system point of view in that the interactions between processes cannot be controlled using conventional techniques. In terms of recovery, the steps of a process are different from operations within a transaction. Each one has its own termination semantics and there are dependencies among the different steps. Regarding concurrency control, the flow of control of a process is more complex than in a flat transaction. A process may, for example, partially roll back its execution or may follow one of several alternatives. In this article, we deal with the problem of atomicity and isolation in the context of processes. We propose a unified model for concurrency control and recovery for processes and show how this model can be implemented in practice, thereby providing a complete framework for developing middleware applications using processes.",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W2002313146",
    "type": "article"
  },
  {
    "title": "The multicast policy and its relationship to replicated data placement",
    "doi": "https://doi.org/10.1145/103140.103146",
    "publication_date": "1991-03-01",
    "publication_year": 1991,
    "authors": "Ouri Wolfson; Amir Milo",
    "corresponding_authors": "",
    "abstract": "In this paper we consider the communication complexity of maintaining the replicas of a logical data-item, in a database distributed over a computer network. We propose a new method, called the minimum spanning tree write, by which a processor in the network should multicast a write of a logical data-item, to all the processors that store replicas of the items. Then we show that the minimum spanning tree write is optimal from the communication cost point of view. We also demonstate that the method by which a write is multicast to all the replicas of a data-item affects the optimal replication scheme of the item, i.e., at which processors in the network the replicas should be located. Therefore, next we consider the problem of determining an optimal replicaiton scheme for a data item, assuming that each processor employs the minimum spanning tree write at run-time. The problem for general networks is shown NP-Complete, but we provide efficient algorithms to obtain an optimal allocation scheme for three common types of network topologies. They are completely-connected, tree, and ring networks. For these topologies, efficient algorithms are also provided for the case in which reliability considerations dictate a minimum number of replicas.",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W2048666037",
    "type": "article"
  },
  {
    "title": "Supporting valid-time indeterminacy",
    "doi": "https://doi.org/10.1145/288086.288087",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Curtis Dyreson; Richard T. Snodgrass",
    "corresponding_authors": "",
    "abstract": "In valid-time indeterminacy it is known that an event stored in a database did in fact occur, but it is not known exactly when . In this paper we extend the SQL data model and query language to support valid-time indeterminacy. We represent the occurrence time of an event with a set of possible instants, delimiting when the event might have occurred, and a probability distribution over that set. We also describe query language constructs to retrieve information in the presence of indeterminacy. These constructs enable users to specify their credibility in the underlying data and their plausibility in the relationships among that data. A denotational semantics for SQL's select statement with optional credibility and plausibility constructs is given. We show that this semantics is reliable, in that it never produces incorrect information, is maximal, in that if it were extended to be more informative, the results may not be reliable, and reduces to the previous semantics when there is no indeterminacy. Although the extended data model and query language provide needed modeling capabilities, these extensions appear initially to carry a significant execution cost. A contribution of this paper is to demonstrate that our approach is useful and practical. An efficient representation of valid-time indeterminacy and efficient query processing algorithms are provided. The cost of support for indeterminacy is empirically measured, and is shown to be modest. Finally, we show that the approach is general, by applying it to the temporal query language constructs being proposed for SQL3.",
    "cited_by_count": 156,
    "openalex_id": "https://openalex.org/W2000290232",
    "type": "article"
  },
  {
    "title": "Conflict detection tradeoffs for replicated data",
    "doi": "https://doi.org/10.1145/115302.115289",
    "publication_date": "1991-12-01",
    "publication_year": 1991,
    "authors": "Michael J. Carey; Miron Livny",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Conflict detection tradeoffs for replicated data Authors: Michael J. Carey Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile , Miron Livny Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 16Issue 4Dec. 1991 pp 703–746https://doi.org/10.1145/115302.115289Published:01 December 1991Publication History 113citation909DownloadsMetricsTotal Citations113Total Downloads909Last 12 Months27Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 153,
    "openalex_id": "https://openalex.org/W2108350817",
    "type": "article"
  },
  {
    "title": "Static analysis techniques for predicting the behavior of active database rules",
    "doi": "https://doi.org/10.1145/202106.202107",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Alex Aiken; Joseph M. Hellerstein; Jennifer Widom",
    "corresponding_authors": "",
    "abstract": "This article gives methods for statically analyzing sets of active database rules to determine if the rules are (1) guaranteed to terminate, (2) guaranteed to produce a unique final database state, and (3) guaranteed to produce a unique stream of observable actions. If the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System .",
    "cited_by_count": 152,
    "openalex_id": "https://openalex.org/W1966276082",
    "type": "article"
  },
  {
    "title": "On modeling of information retrieval concepts in vector spaces",
    "doi": "https://doi.org/10.1145/22952.22957",
    "publication_date": "1987-06-01",
    "publication_year": 1987,
    "authors": "S. K. M. Wong; Wojciech Ziarko; Vijay V. Raghavan; P. C. N. Wong",
    "corresponding_authors": "",
    "abstract": "The Vector Space Model (VSM) has been adopted in information retrieval as a means of coping with inexact representation of documents and queries, and the resulting difficulties in determining the relevance of a document relative to a given query. The major problem in employing this approach is that the explicit representation of term vectors is not known a priori. Consequently, earlier researchers made the assumption that the vectors corresponding to terms are pairwise orthogonal. Such an assumption is clearly unrealistic. Although attempts have been made to compensate for this assumption by some separate, corrective steps, such methods are ad hoc and, in most cases, formally inconsistent. In this paper, a generalization of the VSM, called the GVSM, is advanced. The developments provide a solution not only for the computation of a measure of similarity (correlation) between terms, but also for the incorporation of these similarities into the retrieval process. The major strength of the GVSM derives from the fact that it is theoretically sound and elegant. Furthermore, experimental evaluation of the model on several test collections indicates that the performance is better than that of the VSM. Experiments have been performed on some variations of the GVSM, and all these results have also been compared to those of the VSM, based on inverse document frequency weighting. These results and some ideas for the efficient implementation of the GVSM are discussed.",
    "cited_by_count": 151,
    "openalex_id": "https://openalex.org/W1987936485",
    "type": "article"
  },
  {
    "title": "Automatic generation of production rules for integrity maintenance",
    "doi": "https://doi.org/10.1145/185827.185828",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Stefano Ceri; Piero Fraternali; Stefano Paraboschi; Letizia Tanca",
    "corresponding_authors": "",
    "abstract": "In this article we present an approach to integrity maintenance, consisting of automatically generating production rules for integrity enforcement. Constraints are expressed as particular formulas of Domain Relational Calculus; they are automatically translated into a set of repair actions, encoded as production rules of an active database system. Production rules may be redundant (they enforce the same constraint in different ways) and conflicting (because repairing one constraint may cause the violation of another constraint). Thus, it is necessary to develop techniques for analyzing the properties of the set of active rules and for ensuring that any computation of production rules after any incorrect transaction terminates and produces a consistent database state. Along these guidelines, we describe a specific architecture for constraint definition and enforcement. The components of the architecture include a Rule Generator , for producing all possible repair actions, and a Rule Analyzer and Selector , for producing a collection of production rules such that their execution after an incorrect transaction always terminates in a consistent state (possibly by rolling back the transaction); moreover, the needs of applications are modeled, so that integrity-enforcing rules reach the final state that better represents the original intentions of the transaction's supplier. Specific input from the designer can also drive the process and integrate or modify the rules generated automatically by the method. Experimental results of a prototype implementation of the proposed architecture are also described.",
    "cited_by_count": 150,
    "openalex_id": "https://openalex.org/W2026024665",
    "type": "article"
  },
  {
    "title": "Extending a database system with procedures",
    "doi": "https://doi.org/10.1145/27629.27631",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Michael Stonebraker; Jeff Anton; Eric N. Hanson",
    "corresponding_authors": "",
    "abstract": "This paper suggests that more powerful database systems (DBMS) can be built by supporting database procedures as full-fledged database objects. In particular, allowing fields of a database to be a collection of queries in the query language of the system is shown to allow the natural expression of complex data relationships. Moreover, many of the features present in object-oriented systems and semantic data models can be supported by this facility. In order to implement this construct, extensions to a typical relational query language must be made, and considerable work on the execution engine of the underlying DBMS must be accomplished. This paper reports on the extensions for one particular query language and data manager and then gives performance figures for a prototype implementation. Even though the performance of the prototype is competitive with that of a conventional system, suggestions for improvement are presented.",
    "cited_by_count": 150,
    "openalex_id": "https://openalex.org/W2065419751",
    "type": "article"
  },
  {
    "title": "Are quorums an alternative for data replication?",
    "doi": "https://doi.org/10.1145/937598.937601",
    "publication_date": "2003-09-01",
    "publication_year": 2003,
    "authors": "Ricardo Jiménez; Marta Patiño-Martı́nez; Gustavo Alonso; Bettina Kemme",
    "corresponding_authors": "",
    "abstract": "Data replication is playing an increasingly important role in the design of parallel information systems. In particular, the widespread use of cluster architectures often requires to replicate data for performance and availability reasons. However, maintaining the consistency of the different replicas is known to cause severe scalability problems. To address this limitation, quorums are often suggested as a way to reduce the overall overhead of replication. In this article, we analyze several quorum types in order to better understand their behavior in practice. The results obtained challenge many of the assumptions behind quorum based replication. Our evaluation indicates that the conventional read-one/write-all-available approach is the best choice for a large range of applications requiring data replication. We believe this is an important result for anybody developing code for computing clusters as the read-one/write-all-available strategy is much simpler to implement and more flexible than quorum-based approaches. In this article, we show that, in addition, it is also the best choice using a number of other selection criteria.",
    "cited_by_count": 149,
    "openalex_id": "https://openalex.org/W1973115038",
    "type": "article"
  },
  {
    "title": "Properties and update semantics of consistent views",
    "doi": "https://doi.org/10.1145/49346.50068",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Georg Gottlob; Paolo Paolini; Roberto V. Zicari",
    "corresponding_authors": "",
    "abstract": "The problem of translating view updates to database updates is considered. Both databases and views are modeled as data abstractions. A data abstraction consists of a set of states and of a set of primitive update operators representing state transition functions. It is shown how complex update programs can be built from primitive update operators and how view update programs are translated into database update programs. Special attention is paid to a class of views that we call “consistent.” Loosely speaking, a consistent view is a view with the following property: If the effect of a view update program on a view state is determined, then the effect of the corresponding database update is unambiguously determined. Thus, in order to know how to translate a given view update into a database update, it is sufficient to be aware of a functional specification of such a program. We show that consistent views have a number of interesting properties with respect to the concurrency of (high-level) update transactions. Moreover we show that the class of consistent views includes as a subset the class of views that translate updates under maintenance of a constant complement. However, we show that there exist consistent views that do not translate under constant complement. The results of Bancilhon and Spyratos [6] are generalized in order to capture the update semantics of the entire class of consistent views. In particular we show that the class of consistent views is obtained if we relax the requirement of a constant complement by allowing the complement to decrease according to a suitable partial order.",
    "cited_by_count": 147,
    "openalex_id": "https://openalex.org/W1985223393",
    "type": "article"
  },
  {
    "title": "Transaction chopping",
    "doi": "https://doi.org/10.1145/211414.211427",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Dennis Shasha; François Llirbat; Eric Simon; Patrick Valduriez",
    "corresponding_authors": "",
    "abstract": "Chopping transactions into pieces is good for performance but may lead to nonserializable executions. Many researchers have reacted to this fact by either inventing new concurrency-control mechanisms, weakening serializability, or both. We adopt a different approach. We assume a user who —has access only to user-level tools such as (1) choosing isolation degrees 1ndash;4, (2) the ability to execute a portion of a transaction using multiversion read consistency, and (3) the ability to reorder the instructions in transaction programs; and —knows the set of transactions that may run during a certain interval (users are likely to have such knowledge for on-line or real-time transactional applications). Given this information, our algorithm finds the finest chopping of a set of transactions TranSet with the following property: If the pieces of the chopping execute serializably, then TranSet executes serializably . This permits users to obtain more concurrency while preserving correctness. Besides obtaining more intertransaction concurrency, chopping transactions in this way can enhance intratransaction parallelism. The algorithm is inexpensive, running in O(n×(e+m)) time, once conflicts are identified, using a naive implementation, where n is the number of concurrent transactions in the interval, e is the number of edges in the conflict graph among the transactions, and m is the maximum number of accesses of any transaction. This makes it feasible to add as a tuning knob to real systems.",
    "cited_by_count": 147,
    "openalex_id": "https://openalex.org/W2133752438",
    "type": "article"
  },
  {
    "title": "Buffer management in relational database systems",
    "doi": "https://doi.org/10.1145/7239.7336",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "Giovanni Maria Sacco; Mario Schkolnick",
    "corresponding_authors": "",
    "abstract": "The hot-set model, characterizing the buffer requirements of relational queries, is presented. This model allows the system to determine the optimal buffer space to be allocated to a query; it can also be used by the query optimizer to derive efficient execution plans accounting for the available buffer space, and by a query scheduler to prevent thrashing. The hot-set model is compared with the working-set model. A simulation study is presented.",
    "cited_by_count": 146,
    "openalex_id": "https://openalex.org/W2159209233",
    "type": "article"
  },
  {
    "title": "Apologizing versus asking permission: optimistic concurrency control for abstract data types",
    "doi": "https://doi.org/10.1145/77643.77647",
    "publication_date": "1990-03-01",
    "publication_year": 1990,
    "authors": "Maurice Herlihy",
    "corresponding_authors": "Maurice Herlihy",
    "abstract": "An optimistic concurrency control technique is one that allows transactions to execute without synchronization, relying on commit-time validation to ensure serializability. Several new optimistic concurrency control techniques for objects in decentralized distributed systems are described here, their correctness and optimality properties are proved, and the circumstances under which each is likely to be useful are characterized. Unlike many methods that classify operations only as Reads or Writes, these techniques systematically exploit type-specific properties of objects to validate more interleavings. Necessary and sufficient validation conditions can be derived directly from an object's data type specification. These techniques are also modular: they can be applied selectively on a per-object (or even per-operation) basis in conjunction with standard pessimistic techniques such as two-phase locking, permitting optimistic methods to be introduced exactly where they will be most effective. These techniques can be used to reduce the algorithmic complexity of achieving high levels of concurrency, since certain scheduling decisions that are NP-complete for pessimistic schedulers can be validated after the fact in time, independent of the level of concurrency. These techniques can also enhance the availability of replicated data, circumventing certain tradeoffs between concurrency and availability imposed by comparable pessimistic techniques.",
    "cited_by_count": 143,
    "openalex_id": "https://openalex.org/W2066219930",
    "type": "article"
  },
  {
    "title": "Optimization of queries with user-defined predicates",
    "doi": "https://doi.org/10.1145/320248.320249",
    "publication_date": "1999-06-01",
    "publication_year": 1999,
    "authors": "Surajit Chaudhuri; Kyuseok Shim",
    "corresponding_authors": "",
    "abstract": "Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations.) We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis and experimental comparison of the algorithms.",
    "cited_by_count": 143,
    "openalex_id": "https://openalex.org/W2079806729",
    "type": "article"
  },
  {
    "title": "Fast incremental maintenance of approximate histograms",
    "doi": "https://doi.org/10.1145/581751.581753",
    "publication_date": "2002-09-01",
    "publication_year": 2002,
    "authors": "Phillip B. Gibbons; Yossi Matias; Viswanath Poosala",
    "corresponding_authors": "",
    "abstract": "Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for incremental maintenance of approximate histograms. By scheduling updates to the histogram based on the updates to the database, our techniques are the first to maintain histograms effectively up to date at all times and avoid computing overheads when unnecessary. Our techniques provide highly accurate approximate histograms belonging to the equidepth and Compressed classes. Experimental results show that our new approaches provide orders of magnitude more accurate estimation than previous approaches.An important aspect employed by these new approaches is a backing sample , an up-to-date random sample of the tuples currently in a relation. We provide efficient solutions for maintaining a uniformly random sample of a relation in the presence of updates to the relation. The backing sample techniques can be used for any other application that relies on random samples of data.",
    "cited_by_count": 142,
    "openalex_id": "https://openalex.org/W1992023276",
    "type": "article"
  },
  {
    "title": "Locking performance in centralized databases",
    "doi": "https://doi.org/10.1145/4879.4880",
    "publication_date": "1985-12-01",
    "publication_year": 1985,
    "authors": "Y. C. Tay; Nathan Goodman; R. Suri",
    "corresponding_authors": "",
    "abstract": "An analytic model is used to study the performance of dynamic locking. The analysis uses only the steady-state average values of the variables. The solution to the model is given by a cubic, which has exactly one valid root for the range of parametric values that is of interest. The model's predictions agree well with simulation results for transactions that require up to twenty locks. The model separates data contention from resource contention, thus facilitating an analysis of their separate effects and their interaction. It shows that systems with a particular form of nonuniform access, or with shared locks, are equivalent to systems with uniform access and only exclusive locks. Blocking due to conflicts is found to impose an upper bound on transaction throughput; this fact leads to a rule of thumb on how much data contention should be permitted in a system. Throughput can exceed this bound if a transaction is restarted whenever it encounters a conflict, provided restart costs and resource contention are low. It can also be exceeded by making transactions predeclare their locks. Raising the multiprogramming level to increase throughput also raises the number of restarts per completion. Transactions should minimize their lock requests, because data contention is proportional to the square of the number of requests. The choice of how much data to lock at a time depends on which part of a general granularity curve the system sees.",
    "cited_by_count": 142,
    "openalex_id": "https://openalex.org/W4229791920",
    "type": "article"
  },
  {
    "title": "Accelerating XPath evaluation in any RDBMS",
    "doi": "https://doi.org/10.1145/974750.974754",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Torsten Grust; Maurice van Keulen; Jens Teubner",
    "corresponding_authors": "",
    "abstract": "This article is a proposal for a database index structure, the XPath accelerator , that has been specifically designed to support the evaluation of XPath path expressions. As such, the index is capable to support all XPath axes (including ancestor, following, preceding-sibling, descendant-or-self, etc.). This feature lets the index stand out among related work on XML indexing structures which had a focus on the child and descendant axes only. The index has been designed with a close eye on the XPath semantics as well as the desire to engineer its internals so that it can be supported well by existing relational database query processing technology: the index (a) permits set-oriented (or, rather, sequence-oriented) path evaluation, and (b) can be implemented and queried using well-established relational index structures, notably B-trees and R-trees.We discuss the implementation of the XPath accelerator on top of different database backends and show that the index performs well on all levels of the memory hierarchy, including disk-based and main-memory based database systems.",
    "cited_by_count": 141,
    "openalex_id": "https://openalex.org/W2137924956",
    "type": "article"
  },
  {
    "title": "Exploiting <i>k</i> -constraints to reduce memory overhead in continuous queries over data streams",
    "doi": "https://doi.org/10.1145/1016028.1016032",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Shivnath Babu; Utkarsh Srivastava; Jennifer Widom",
    "corresponding_authors": "",
    "abstract": "Continuous queries often require significant run-time state over arbitrary data streams. However, streams may exhibit certain data or arrival patterns, or constraints , that can be detected and exploited to reduce state considerably without compromising correctness. Rather than requiring constraints to be satisfied precisely, which can be unrealistic in a data streams environment, we introduce k-constraints , where k is an adherence parameter specifying how closely a stream adheres to the constraint. (Smaller k 's are closer to strict adherence and offer better memory reduction.) We present a query processing architecture, called k-Mon , that detects useful k -constraints automatically and exploits the constraints to reduce run-time state for a wide range of continuous queries. Experimental results showed dramatic state reduction, while only modest computational overhead was incurred for our constraint monitoring and query execution algorithms.",
    "cited_by_count": 138,
    "openalex_id": "https://openalex.org/W2023977759",
    "type": "article"
  },
  {
    "title": "Optimizing object queries using an effective calculus",
    "doi": "https://doi.org/10.1145/377674.377676",
    "publication_date": "2000-12-01",
    "publication_year": 2000,
    "authors": "Leonidas Fegaras; David Maier",
    "corresponding_authors": "",
    "abstract": "Object-oriented databases (OODBs) provide powerful data abstractions and modeling facilities, but they generally lack a suitable framework for query processing and optimization. The development of an effective query optimizer is one of the key factors for OODB systems to successfully compete with relational systems, as well as to meet the performance requirements of many nontraditional applications. We propose an effective framework with a solid theoretical basis for optimizing OODB query languages. Our calculus, called the monoid comprehension calculus, captures most features of ODMG OQL, and is a good basis for expressing various optimization algorithms concisely. This article concentrates on query unnesting (also known as query decorrelation), an optimization that, even though it improves performance considerably, is not treated properly (if at all) by most OODB systems. Our framework generalizes many unnesting techniques proposed recently in the literature, and is capable of removing any form of query nesting using a very simple and efficient algorithm. The simplicity of our method is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comprehension calculus treats operations over multiple collection types, aggregates, and quantifiers in a similar way, resulting in a uniform method of unnesting queries, regardless of their type of nesting.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2059267851",
    "type": "article"
  },
  {
    "title": "Limitations of concurrency in transaction processing",
    "doi": "https://doi.org/10.1145/3148.3160",
    "publication_date": "1985-03-01",
    "publication_year": 1985,
    "authors": "P. A. Franaszek; John T. Robinson",
    "corresponding_authors": "",
    "abstract": "Given the pairwise probability of conflict p among transactions in a transaction processing system, together with the total number of concurrent transactions n, the effective level of concurrency E(n,p) is defined as the expected number of the n transactions that can run concurrently and actually do useful work. Using a random graph model of concurrency, we show for three general classes of concurrency control methods, examples of which are (1) standard locking, (2) strict priority scheduling, and (3) optimistic methods, that (1) E(n, p) ⩽ n(1 - p/2) n-1 , (2) E(n, p) ⩽ (1 - (1 - p) n )/p, and (3) 1 + ((1 - p)/p)ln(p(n - 1) + 1) ⩽ E(n, p) ⩽ 1 + (1/p)ln(p(n - 1) + 1). Thus, for fixed p, as n ↣ ∞), (1) E ↣ 0 for standard locking methods, (2) E ⩽ 1/p for strict priority scheduling methods, and (3) E ↣ ∞ for optimistic methods. Also found are bounds on E in the case where conflicts are analyzed so as to maximize E. The predictions of the random graph model are confirmed by simulations of an abstract transaction processing system. In practice, though, there is a price to pay for the increased effective level of concurrency of methods (2) and (3): using these methods there is more wasted work (i.e., more steps executed by transactions that are later aborted). In response to this problem, three new concurrency control methods suggested by the random graph model analysis are developed. Two of these, called (a) running priority and (b) older or running priority, are shown by the simulation results to perform better than the previously known methods (l)-(3) for relatively large n or large p, in terms of achieving a high effective level of concurrency at a comparatively small cost in wasted work.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2168262674",
    "type": "article"
  },
  {
    "title": "Archiving scientific data",
    "doi": "https://doi.org/10.1145/974750.974752",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Peter Buneman; Sanjeev Khanna; Keishi Tajima; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "Archiving is important for scientific data, where it is necessary to record all past versions of a database in order to verify findings based upon a specific version. Much scientific data is held in a hierachical format and has a key structure that provides a canonical identification for each element of the hierarchy. In this article, we exploit these properties to develop an archiving technique that is both efficient in its use of space and preserves the continuity of elements through versions of the database, something that is not provided by traditional minimum-edit-distance diff approaches. The approach also uses timestamps. All versions of the data are merged into one hierarchy where an element appearing in multiple versions is stored only once along with a timestamp. By identifying the semantic continuity of elements and merging them into one data structure, our technique is capable of providing meaningful change descriptions, the archive allows us to easily answer certain temporal queries such as retrieval of any specific version from the archive and finding the history of an element. This is in contrast with approaches that store a sequence of deltas where such operations may require undoing a large number of changes or significant reasoning with the deltas. A suite of experiments also demonstrates that our archive does not incur any significant space overhead when contrasted with diff approaches. Another useful property of our approach is that we use XML format to represent hierarchical data and the resulting archive is also in XML. Hence, XML tools can be directly applied on our archive. In particular, we apply an XML compressor on our archive, and our experiments show that our compressed archive outperforms compressed diff-based repositories in space efficiency. We also show how we can extend our archiving tool to an external memory archiver for higher scalability and describe various index structures that can further improve the efficiency of some temporal queries on our archive.",
    "cited_by_count": 136,
    "openalex_id": "https://openalex.org/W2137133993",
    "type": "article"
  },
  {
    "title": "Containment of conjunctive queries",
    "doi": "https://doi.org/10.1145/211414.211419",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Yannis Ioannidis; Raghu Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Containment of conjunctive queries: beyond relations as sets Authors: Yannis E. Ioannidis Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile , Raghu Ramakrishnan View Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 20Issue 3Sept. 1995 pp 288–324https://doi.org/10.1145/211414.211419Published:01 September 1995Publication History 89citation759DownloadsMetricsTotal Citations89Total Downloads759Last 12 Months36Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 136,
    "openalex_id": "https://openalex.org/W2163491591",
    "type": "article"
  },
  {
    "title": "Cactis: a self-adaptive, concurrent implementation of an object-oriented database management system",
    "doi": "https://doi.org/10.1145/68012.68013",
    "publication_date": "1989-09-01",
    "publication_year": 1989,
    "authors": "Scott E. Hudson; Roger King",
    "corresponding_authors": "",
    "abstract": "Cactis is an object-oriented, multiuser DBMS developed at the University of Colorado. The system supports functionally-defined data and uses techniques based on attributed graphs to optimize the maintenance of functionally-defined data. The implementation is self-adaptive in that the physical organization and the update algorithms dynamically change in order to reduce disk access. The system is also concurrent. At any given time there are some number of computations that must be performed to bring the database up to date; these computations are scheduled independently and are performed when the expected cost to do so is minimal. The DBMS runs in the Unix/C Sun workstation environment. Cactis is designed to support applications that require rich data modeling capabilities and the ability to specify functionally-defined data, but that also demand good performance. Specifically, Cactis is intended for use in the support of such applications as VLSI and PCB design, and software environments.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W1967307074",
    "type": "article"
  },
  {
    "title": "Dynamic quorum adjustment for partitioned data",
    "doi": "https://doi.org/10.1145/22952.22953",
    "publication_date": "1987-06-01",
    "publication_year": 1987,
    "authors": "Maurice Herlihy",
    "corresponding_authors": "Maurice Herlihy",
    "abstract": "A partition occurs when functioning sites in a distributed system are unable to communicate. This paper introduces a new method for managing replicated data objects in the presence of partitions. Each operation provided by a replicated object has a set. of quorums, which are sets of sites whose cooperation suffices to execute the operation. The method permits an object's quorums to be adjusted dynamically in response to failures and recoveries. A transaction that is unable to progress using one set of quorums may switch to another, more favorable set, and transactions in different. Partitions may progress using different sets. This method has three novel aspects: (1) it supports a wider range of quorums than earlier proposals, (2) it, scales up effectively to large systems because quorum adjustments do not require global reconfiguration, and (3) it, systematically exploits the semantics of typed objects to support more flexible quorum adjustment.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W1978193280",
    "type": "article"
  },
  {
    "title": "A security machanism for statistical database",
    "doi": "https://doi.org/10.1145/320613.320617",
    "publication_date": "1980-09-01",
    "publication_year": 1980,
    "authors": "Leland L. Beck",
    "corresponding_authors": "Leland L. Beck",
    "abstract": "The problem of user inference in statistical databases is discussed and illustrated with several examples. It is assumed that the database allows “total,” “average,” “count,” and “percentile” queries; a query may refer to any arbitrary subset of the database. Methods for protecting the security of such a database are considered; it is shown that any scheme which gives “statistically correct” answers is vulnerable to penetration. A precise definition of compromisability (in a statistical sense) is given. A general model of user inference is proposed; two special cases of this model appear to contain all previously published strategies for compromising a statistical database. A method for protecting the security of such a statistical database against these types of user inference is presented and discussed. It is shown that the number of queries required to compromise the database can be made arbitrarily large by accepting moderate increases in the variance of responses to queries. A numerical example is presented to illustrate the application of the techniques discussed.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W1989799419",
    "type": "article"
  },
  {
    "title": "Using semantic knowledge of transactions to increase concurrency",
    "doi": "https://doi.org/10.1145/76902.76905",
    "publication_date": "1989-12-01",
    "publication_year": 1989,
    "authors": "Abdel Aziz Farrag; M. TAMER ÖZSU",
    "corresponding_authors": "",
    "abstract": "When the only information available about transactions is syntactic information, serializability is the main correctness criterion for concurrency control. Serializability requires that the execution of each transaction must appear to every other transaction as a single atomic step (i.e., the execution of the transaction cannot be interrupted by other transactions). Many researchers, however, have realized that this requirement is unnecessarily strong for many applications and can significantly increase transaction response time. To overcome this problem, a new approach for controlling concurrency that exploits the semantic information available about transactions to allow controlled nonserializable interleavings has recently been proposed. This approach is useful when the cost of producing only serializable interleavings is unacceptably high. The main drawback of the approach is the extra overhead incurred by utilizing the semantic information. We examine this new approach in this paper and discuss its strengths and weaknesses. We introduce a new formalization for the concurrency control problem when semantic information is available about the transactions. This semantic information takes the form of transaction types, transaction steps, and transaction break-points. We define a new class of “safe” schedules called relatively consistent (RC) schedules. This class contains serializable as well as nonserializable schedules. We prove that the execution of an RC schedule cannot violate consistency and propose a new concurrency control mechanism that produces only RC schedules. Our mechanism assumes fewer restrictions on the interleavings among transactions than previously introduced semantic-based mechanisms.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W1990973243",
    "type": "article"
  },
  {
    "title": "Improving hash join performance through prefetching",
    "doi": "https://doi.org/10.1145/1272743.1272747",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Shimin Chen; Anastassia Ailamaki; Phillip B. Gibbons; Todd C. Mowry",
    "corresponding_authors": "",
    "abstract": "Hash join algorithms suffer from extensive CPU cache stalls. This article shows that the standard hash join algorithm for disk-oriented databases (i.e. GRACE) spends over 80% of its user time stalled on CPU cache misses, and explores the use of CPU cache prefetching to improve its cache performance. Applying prefetching to hash joins is complicated by the data dependencies, multiple code paths, and inherent randomness of hashing. We present two techniques, group prefetching and software-pipelined prefetching , that overcome these complications. These schemes achieve 1.29--4.04X speedups for the join phase and 1.37--3.49X speedups for the partition phase over GRACE and simple prefetching approaches. Moreover, compared with previous cache-aware approaches (i.e. cache partitioning), the schemes are at least 36% faster on large relations and do not require exclusive use of the CPU cache to be effective. Finally, comparing the elapsed real times when disk I/Os are in the picture, our cache prefetching schemes achieve 1.12--1.84X speedups for the join phase and 1.06--1.60X speedups for the partition phase over the GRACE hash join algorithm.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W2099580433",
    "type": "article"
  },
  {
    "title": "Concepts and effectiveness of the cover-coefficient-based clustering methodology for text databases",
    "doi": "https://doi.org/10.1145/99935.99938",
    "publication_date": "1990-12-01",
    "publication_year": 1990,
    "authors": "Fazlı Can; Esen A. Ozkarahan",
    "corresponding_authors": "",
    "abstract": "A new algorithm for document clustering is introduced. The base concept of the algorithm, the cover coefficient (CC) concept, provides a means of estimating the number of clusters within a document database and related indexing and clustering analytically. The CC concept is used also to identify the cluster seeds and to form clusters with these seeds. It is shown that the complexity of the clustering process is very low. The retrieval experiments show that the information-retrieval effectiveness of the algorithm is compatible with a very demanding complete linkage clustering method that is known to have good retrieval performance. The experiments also show that the algorithm is 15.1 to 63.5 (with an average of 47.5) percent better than four other clustering algorithms in cluster-based information retrieval. The experiments have validated the indexing-clustering relationships and the complexity of the algorithm and have shown improvements in retrieval effectiveness. In the experiments two document databases are used: TODS214 and INSPEC. The latter is a common database with 12,684 documents.",
    "cited_by_count": 133,
    "openalex_id": "https://openalex.org/W2040810008",
    "type": "article"
  },
  {
    "title": "Optimization techniques for queries with expensive methods",
    "doi": "https://doi.org/10.1145/292481.277627",
    "publication_date": "1998-06-01",
    "publication_year": 1998,
    "authors": "Joseph M. Hellerstein",
    "corresponding_authors": "Joseph M. Hellerstein",
    "abstract": "Object-relational database management systems allow knowledgeable users to define new data types as well as new methods (operators) for the types. This flexibility produces an attendant complexity, which must be handled in new ways for an object-relational database management system to be efficient. In this article we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by “pushdown” rules. These rules apply selections in an arbitrary order before as many joins as possible, using th e assumption that selection takes no time. However, users of object-relational systems can embed complex methods in selections. Thus selections may take significant amounts of time, and the query optimization model must be enhanced. In this article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial object-relational database management system Illustra, and discuss practical issues that affect our earlier assumptions. We compare Predicate Migration to a variety of simplier optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we present may be useful for constrained workloads.",
    "cited_by_count": 133,
    "openalex_id": "https://openalex.org/W2060883486",
    "type": "article"
  },
  {
    "title": "Metric space similarity joins",
    "doi": "https://doi.org/10.1145/1366102.1366104",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Edwin Jacox; Hanan Samet",
    "corresponding_authors": "",
    "abstract": "Similarity join algorithms find pairs of objects that lie within a certain distance ϵ of each other. Algorithms that are adapted from spatial join techniques are designed primarily for data in a vector space and often employ some form of a multidimensional index. For these algorithms, when the data lies in a metric space, the usual solution is to embed the data in vector space and then make use of a multidimensional index. Such an approach has a number of drawbacks when the data is high dimensional as we must eventually find the most discriminating dimensions, which is not trivial. In addition, although the maximum distance between objects increases with dimension, the ability to discriminate between objects in each dimension does not. These drawbacks are overcome via the introduction of a new method called Quickjoin that does not require a multidimensional index and instead adapts techniques used in distance-based indexing for use in a method that is conceptually similar to the Quicksort algorithm. A formal analysis is provided of the Quickjoin method. Experiments show that the Quickjoin method significantly outperforms two existing techniques.",
    "cited_by_count": 132,
    "openalex_id": "https://openalex.org/W2115500858",
    "type": "article"
  },
  {
    "title": "On an authorization mechanism",
    "doi": "https://doi.org/10.1145/320263.320288",
    "publication_date": "1978-09-01",
    "publication_year": 1978,
    "authors": "Ronald Fagin",
    "corresponding_authors": "Ronald Fagin",
    "abstract": "Griffiths and Wade ( ACM Trans. Database Syst. 1,3, (Sept. 1976), 242-255) have defined a dynamic authorization mechanism that goes beyond the traditional password approach. A database user can grant or revoke privileges (such as to read, insert, or delete) on a file that he has created. Furthermore, he can authorize others to grant these same privileges. The database management system keeps track of a directed graph, emanating from the creator, of granted privileges. The nodes of the graph correspond to users, and the edges (each of which is labeled with a timestamp) correspond to grants. The edges are of two types, corresponding to whether or not the recipient of the grant has been given the option to make further grants of this privilege. Furthermore, for each pair A, B of nodes, there can be no more than one edge of each type from A to B . We modify this approach by allowing graphs in which there can be multiple edges of each type from one node to another. We prove correctness (in a certain strong sense) for our modified authorization mechanism. Further, we show by example that under the original mechanism, the system might forbid some user from exercising or granting a privilege that he “should” be allowed to exercise or grant.",
    "cited_by_count": 132,
    "openalex_id": "https://openalex.org/W2135510799",
    "type": "article"
  },
  {
    "title": "Semantics and implementation of continuous sliding window queries over data streams",
    "doi": "https://doi.org/10.1145/1508857.1508861",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Jürgen Krämer; Bernhard Seeger",
    "corresponding_authors": "",
    "abstract": "In recent years the processing of continuous queries over potentially infinite data streams has attracted a lot of research attention. We observed that the majority of work addresses individual stream operations and system-related issues rather than the development of a general-purpose basis for stream processing systems. Furthermore, example continuous queries are often formulated in some declarative query language without specifying the underlying semantics precisely enough. To overcome these deficiencies, this article presents a consistent and powerful operator algebra for data streams which ensures that continuous queries have well-defined, deterministic results. In analogy to traditional database systems, we distinguish between a logical and a physical operator algebra. While the logical algebra specifies the semantics of the individual operators in a descriptive but concrete way over temporal multisets, the physical algebra provides efficient implementations in the form of stream-to-stream operators. By adapting and enhancing research from temporal databases to meet the challenging requirements in streaming applications, we are able to carry over the conventional transformation rules from relational databases to stream processing. For this reason, our approach not only makes it possible to express continuous queries with a sound semantics, but also provides a solid foundation for query optimization, one of the major research topics in the stream community. Since this article seamlessly explains the steps from query formulation to query execution, it outlines the innovative features and operational functionality implemented in our state-of-the-art stream processing infrastructure.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2049410874",
    "type": "article"
  },
  {
    "title": "Approximation and streaming algorithms for histogram construction problems",
    "doi": "https://doi.org/10.1145/1132863.1132873",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Sudipto Guha; Nick Koudas; Kyuseok Shim",
    "corresponding_authors": "",
    "abstract": "Histograms and related synopsis structures are popular techniques for approximating data distributions. These have been successful in query optimization and a variety of applications, including approximate querying, similarity searching, and data mining, to name a few. Histograms were a few of the earliest synopsis structures proposed and continue to be used widely. The histogram construction problem is to construct the best histogram restricted to a space bound that reflects the data distribution most accurately under a given error measure.The histograms are used as quick and easy estimates. Thus, a slight loss of accuracy, compared to the optimal histogram under the given error measure, can be offset by fast histogram construction algorithms. A natural question arises in this context: Can we find a fast near optimal approximation algorithm for the histogram construction problem? In this article, we give the first linear time (1+ϵ)-factor approximation algorithms (for any ϵ &gt; 0) for a large number of histogram construction problems including the use of piecewise small degree polynomials to approximate data, workloads, etc. Several of our algorithms extend to data streams.Using synthetic and real-life data sets, we demonstrate that in many scenarios the approximate histograms are almost identical to optimal histograms in quality and are significantly faster to construct.",
    "cited_by_count": 129,
    "openalex_id": "https://openalex.org/W2057058417",
    "type": "article"
  },
  {
    "title": "An incremental access method for ViewCache",
    "doi": "https://doi.org/10.1145/111197.111215",
    "publication_date": "1991-09-01",
    "publication_year": 1991,
    "authors": "N. Roussopoulos",
    "corresponding_authors": "N. Roussopoulos",
    "abstract": "A ViewCache is a stored collection of pointers pointing to records of underlying relations needed to materialize a view. This paper presents an Incremental Access Method (IAM) that amortizes the maintenance cost of ViewCaches over a long time period or indefinitely. Amortization is based on deferred and other update propagation strategies. A deferred update strategy allows a ViewCache to remain outdated until a query needs to selectively or exhaustively materialize the view. At that point, an incremental update of the ViewCache is performed. This paper defines a set of conditions under which incremental access to the ViewCache is cost effective. The decision criteria are based on some dynamically maintained cost parameters, which provide accurate information but require inexpensive bookkeeping. The IAM capitalizes on the ViewCache storage organization for performing the update and the materialization of the ViewCaches in an interleaved mode using one-pass algorithms. Compared to the standard technique for supporting views that requires reexecution of the definition of the view, the IAM offers significant performance advantages. We will show that under favorable conditions, most of which depend on the size of the incremental update logs between consecutive accesses of the views, the incremental access method outperforms query modification. Performance gains are higher for multilevel ViewCaches because all the I/O and CPU for handling intermediate results are avoided.",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W1998147193",
    "type": "article"
  },
  {
    "title": "Optimal histograms for limiting worst-case error propagation in the size of join results",
    "doi": "https://doi.org/10.1145/169725.169708",
    "publication_date": "1993-12-01",
    "publication_year": 1993,
    "authors": "Yannis Ioannidis; Stavros Christodoulakis",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Optimal histograms for limiting worst-case error propagation in the size of join results Authors: Yannis E. Ioannidis University of Wisconsin University of WisconsinView Profile , Stavros Christodoulakis Technical University of Crete Technical University of CreteView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 18Issue 4pp 709–748https://doi.org/10.1145/169725.169708Published:01 December 1993Publication History 107citation864DownloadsMetricsTotal Citations107Total Downloads864Last 12 Months73Last 6 weeks21 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W2010890196",
    "type": "article"
  },
  {
    "title": "Improving the human factors aspect of database interactions",
    "doi": "https://doi.org/10.1145/320289.320295",
    "publication_date": "1978-12-01",
    "publication_year": 1978,
    "authors": "Ben Shneiderman",
    "corresponding_authors": "Ben Shneiderman",
    "abstract": "The widespread dissemination of computer and information systems to nontechnically trained individuals requires a new approach to the design and development of database interfaces. This paper provides the motivational background for controlled psychological experimentation in exploring the person/machine interface. Frameworks for the reductionist approach are given, research methods discussed, research issues presented, and a small experiment is offered as an example of what can be accomplished. This experiment is a comparison of natural and artificial language query facilities. Although subjects posed approximately equal numbers of valid queries with either facility, natural language users made significantly more invalid queries which could not be answered from the database that was described.",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W2067978534",
    "type": "article"
  },
  {
    "title": "Database partitioning in a cluster of processors",
    "doi": "https://doi.org/10.1145/3148.3161",
    "publication_date": "1985-03-01",
    "publication_year": 1985,
    "authors": "Domenico Saccà; Gio Wiederhold",
    "corresponding_authors": "",
    "abstract": "In a distributed database system the partitioning and allocation of the database over the processor nodes of the network can be a critical aspect of the database design effort. In this paper we develop and evaluate algorithms that perform this task in a computationally feasible manner. The network we consider is characterized by a relatively high communication bandwidth, considering the processing and input output capacities in its processors. Such a balance is typical if the processors are connected via busses or local networks. The common constraint that transactions have a specific root node no longer exists, so that there are more distribution choices. However, a poor distribution leads to less efficient computation, higher costs, and higher loads in the nodes or in the communication network so that the system may not be able to handle the required set of transactions. Our approach is to first split the database into fragments which constitute appropriate units for allocation. The fragments to be allocated are selected based on maximal benefit criteria using a greedy heuristic. The assignment to processor nodes uses a first-fit algorithm. The complete algorithm, called GFF, is stated in a procedural form. The complexity of the problem and of its candidate solutions are analyzed and several interesting relationships are proven. Alternate benefit metrics are considered, since the execution cost of the allocation procedure varies by orders of magnitude with the alternatives of benefit evaluation. A mixed benefit evaluation strategy is eventually proposed. A model for evaluation is presented. Two of the strategies are experimentally evaluated, and the reported results support the discussion. The approach should be suitable for other cases where resources have to be allocated subject to resource constraints.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W1987995589",
    "type": "article"
  },
  {
    "title": "Automatic verification of database transaction safety",
    "doi": "https://doi.org/10.1145/68012.68014",
    "publication_date": "1989-09-01",
    "publication_year": 1989,
    "authors": "Tim Sheard; David Stemple",
    "corresponding_authors": "",
    "abstract": "Maintaining the integrity of databases is one of the promises of database management systems. This includes assuring that integrity constraints are invariants of database transactions. This is very difficult to accomplish efficiently in the presence of complex constraints and large amounts of data. One way to minimize the amount of processing required to maintain database integrity over transaction processing is to prove at compile-time that transactions cannot, if run atomically, disobey integrity constraints. We report on a system that performs such verification for a robust set of constraint and transaction classes. The system accepts database schemas written in a more or less traditional style and accepts programs in a high-level programming language. Automatic verification fast enough to be effective on current workstation hardware is performed.",
    "cited_by_count": 125,
    "openalex_id": "https://openalex.org/W1984958950",
    "type": "article"
  },
  {
    "title": "Practical data-swapping: the first steps",
    "doi": "https://doi.org/10.1145/348.349",
    "publication_date": "1984-03-23",
    "publication_year": 1984,
    "authors": "Steven P. Reiss",
    "corresponding_authors": "Steven P. Reiss",
    "abstract": "The problem of statistical database confidentiality in releasing microdata is addressed through the use of approximate data-swapping. Here, a portion of the microdata is replaced with a database that has been selected with approximately the same statistics. The result guarantees the confidentiality of the original data, while providing microdata with accurate statistics. Methods for achieving such transformations are considered and analyzed through simulation.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2041025307",
    "type": "article"
  },
  {
    "title": "Workload-aware anonymization techniques for large-scale datasets",
    "doi": "https://doi.org/10.1145/1386118.1386123",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Kristen LeFevre; David J. DeWitt; Raghu Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Protecting individual privacy is an important problem in microdata distribution and publishing. Anonymization algorithms typically aim to satisfy certain privacy definitions with minimal impact on the quality of the resulting data. While much of the previous literature has measured quality through simple one-size-fits-all measures, we argue that quality is best judged with respect to the workload for which the data will ultimately be used. This article provides a suite of anonymization algorithms that incorporate a target class of workloads, consisting of one or more data mining tasks as well as selection predicates. An extensive empirical evaluation indicates that this approach is often more effective than previous techniques. In addition, we consider the problem of scalability. The article describes two extensions that allow us to scale the anonymization algorithms to datasets much larger than main memory. The first extension is based on ideas from scalable decision trees, and the second is based on sampling. A thorough performance evaluation indicates that these techniques are viable in practice.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2164812149",
    "type": "article"
  },
  {
    "title": "Conditional XPath",
    "doi": "https://doi.org/10.1145/1114244.1114247",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "M. Marx",
    "corresponding_authors": "M. Marx",
    "abstract": "XPath 1.0 is a variable free language designed to specify paths between nodes in XML documents. Such paths can alternatively be specified in first-order logic. The logical abstraction of XPath 1.0, usually called Navigational or Core XPath, is not powerful enough to express every first-order definable path. In this article, we show that there exists a natural expansion of Core XPath in which every first-order definable path in XML document trees is expressible. This expansion is called Conditional XPath. It contains additional axis relations of the form (child::n[F])+, denoting the transitive closure of the path expressed by child::n[F]. The difference with XPath's descendant::n[F] is that the path (child::n[F])+ is conditional on the fact that all nodes in between the start and end node of the path should also be labeled by n and should make the predicate F true. This result can be viewed as the XPath analogue of the expressive completeness of the relational algebra with respect to first-order logic.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2294034341",
    "type": "article"
  },
  {
    "title": "Achieving robustness in distributed database systems",
    "doi": "https://doi.org/10.1145/319989.319992",
    "publication_date": "1983-09-01",
    "publication_year": 1983,
    "authors": "Derek L. Eager; Kenneth C. Sevcik",
    "corresponding_authors": "",
    "abstract": "The problem of concurrency control in distributed database systems in which site and communication link failures may occur is considered. The possible range of failures is not restricted; in particular, failures may induce an arbitrary network partitioning. It is desirable to attain a high “level of robustness” in such a system; that is, these failures should have only a small impact on system operation. A level of robustness termed maximal partial operability is identified. Under our models of concurrency control and robustness, this robustness level is the highest level attainable without significantly degrading performance. A basis for the implementation of maximal partial operability is presented. To illustrate its use, it is applied to a distributed locking concurrency control method and to a method that utilizes timestamps. When no failures are present, the robustness modifications for these methods induce no significant additional overhead.",
    "cited_by_count": 123,
    "openalex_id": "https://openalex.org/W2014104232",
    "type": "article"
  },
  {
    "title": "On Concurrency Control by Multiple Versions",
    "doi": "https://doi.org/10.1145/348.318588",
    "publication_date": "1984-03-23",
    "publication_year": 1984,
    "authors": "Christos H. Papadimitriou; Paris C. Kanellakis",
    "corresponding_authors": "",
    "abstract": "We examine the problem of concurrency control when the database management system supports multiple versions of the data. We characterize the limit of the parallelism achievable by the multiversion approach and demonstrate the resulting space-parallelism trade-off.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W1992044086",
    "type": "article"
  },
  {
    "title": "A divide-and-merge methodology for clustering",
    "doi": "https://doi.org/10.1145/1189769.1189779",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Chi Cheng; Ravi Kannan; Santosh Vempala; Grant Wang",
    "corresponding_authors": "",
    "abstract": "We present a divide-and-merge methodology for clustering a set of objects that combines a top-down “divide” phase with a bottom-up “merge” phase. In contrast, previous algorithms use either top-down or bottom-up methods to construct a hierarchical clustering or produce a flat clustering using local search (e.g., k -means). For the divide phase, which produces a tree whose leaves are the elements of the set, we suggest an efficient spectral algorithm. When the data is in the form of a sparse document-term matrix, we show how to modify the algorithm so that it maintains sparsity and runs in linear space. The merge phase quickly finds the optimal partition that respects the tree for many natural objective functions, for example, k -means, min-diameter, min-sum, correlation clustering, etc. We present a thorough experimental evaluation of the methodology. We describe the implementation of a meta-search engine that uses this methodology to cluster results from web searches. We also give comparative empirical results on several real datasets.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W2057320839",
    "type": "article"
  },
  {
    "title": "A database cache for high performance and fast restart in database systems",
    "doi": "https://doi.org/10.1145/1994.1995",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "Klaus Elhardt; Rudolf Bayer",
    "corresponding_authors": "",
    "abstract": "Performance in database systems is strongly influenced by buffer management and transaction recovery methods. This paper presents the principles of the database cache, which replaces the traditional buffer. In comparison to buffer management, cache management is more carefully coordinated with transaction management, and integrates transaction recovery. High throughput of small- and medium-sized transactions is achieved by fast commit processing and low database traffic. Very fast handling of transaction failures and short restart time after system failure are guaranteed in such an environment. Very long retrieval and update transactions are also supported.",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W1985467476",
    "type": "article"
  },
  {
    "title": "Indexing the past, present, and anticipated future positions of moving objects",
    "doi": "https://doi.org/10.1145/1132863.1132870",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Mindaugas Pelanis; Simonas Šaltenis; Christian S. Jensen",
    "corresponding_authors": "",
    "abstract": "With the proliferation of wireless communications and geo-positioning, e-services are envisioned that exploit the positions of a set of continuously moving users to provide context-aware functionality to each individual user. Because advances in disk capacities continue to outperform Moore's Law, it becomes increasingly feasible to store online all the position information obtained from the moving e-service users. With the much slower advances in I/O speeds and many concurrent users, indexing techniques are of the essence in this scenario.Existing indexing techniques come in two forms. Some techniques capture the position of an object up until the time of the most recent position sample, while other techniques represent an object's position as a constant or linear function of time and capture the position from the current time and into the (near) future. This article offers an indexing technique capable of capturing the positions of moving objects at all points in time. The index substantially modifies partial persistence techniques, which support transaction time, to support valid time for monitoring applications. The performance of a timeslice query is independent of the number of past position samples stored for an object. No existing indices exist with these characteristics.",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W2027450468",
    "type": "article"
  },
  {
    "title": "Concurrent search structure algorithms",
    "doi": "https://doi.org/10.1145/42201.42204",
    "publication_date": "1988-03-01",
    "publication_year": 1988,
    "authors": "Dennis Shasha; Nathan Goodman",
    "corresponding_authors": "",
    "abstract": "A dictionary is an abstract data type supporting the actions member, insert, and delete. A search structure is a data structure used to implement a dictionary. Examples include B trees, hash structures, and unordered lists. Concurrent algorithms on search structures can achieve more parallelism than standard concurrency control methods would suggest, by exploiting the fact that many different search structure states represent one dictionary state. We present a framework for verifying such algorithms and for inventing new ones. We give several examples, one of which exploits the structure of Banyan family interconnection networks. We also discuss the interaction between concurrency control and recovery as applied to search structures.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W2036122593",
    "type": "article"
  },
  {
    "title": "Optimal partial-match retrieval when fields are independently specified",
    "doi": "https://doi.org/10.1145/320071.320074",
    "publication_date": "1979-06-01",
    "publication_year": 1979,
    "authors": "Alfred V. Aho; Jeffrey D. Ullman",
    "corresponding_authors": "",
    "abstract": "This paper considers the design of a system to answer partial-match queries from a file containing a collection of records, each record consisting of a sequence of fields. A partial-match query is a specification of values for zero or more fields of a record, and the answer to a query is a listing of all records in the file whose fields match the specified values. A design is considered in which the file is stored in a set of bins. A formula is derived for the optimal number of bits in a bin address to assign to each field, assuming the probability that a given field is specified in a query is independent of what other fields are specified. Implications of the optimality criterion on the size of bins are also discussed.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2091467328",
    "type": "article"
  },
  {
    "title": "An implementation technique for database query languages",
    "doi": "https://doi.org/10.1145/319702.319711",
    "publication_date": "1982-06-01",
    "publication_year": 1982,
    "authors": "Peter Buneman; Robert E. Frankel; Rishiyur S. Nikhil",
    "corresponding_authors": "",
    "abstract": "Structured query languages, such as those available for relational databases, are becoming increasingly desirable for all database management systems. Such languages are applicative: there is no need for an assignment or update statement. A new technique is described that allows for the implementation of applicative query languages against most commonly used database systems. The technique involves “lazy” evaluation and has a number of advantages over existing methods: it allows queries and functions of arbitrary complexity to be constructed; it reduces the use of secondary storage; it provides a simple control structure through which interfaces to other programs may be constructed; and the implementation, including the database interface, is quite compact. Although the technique is presented for a specific functional programming system and for a CODASYL DBMS, it is general and may be used for other query languages and database systems.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W1987682002",
    "type": "article"
  },
  {
    "title": "Efficient and accurate nearest neighbor and closest pair search in high-dimensional space",
    "doi": "https://doi.org/10.1145/1806907.1806912",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Yufei Tao; Ke Yi; Cheng Sheng; Panos Kalnis",
    "corresponding_authors": "",
    "abstract": "Nearest Neighbor (NN) search in high-dimensional space is an important problem in many applications. From the database perspective, a good solution needs to have two properties: (i) it can be easily incorporated in a relational database, and (ii) its query cost should increase sublinearly with the dataset size, regardless of the data and query distributions. Locality-Sensitive Hashing (LSH) is a well-known methodology fulfilling both requirements, but its current implementations either incur expensive space and query cost, or abandon its theoretical guarantee on the quality of query results. Motivated by this, we improve LSH by proposing an access method called the Locality-Sensitive B-tree (LSB-tree) to enable fast, accurate, high-dimensional NN search in relational databases. The combination of several LSB-trees forms a LSB-forest that has strong quality guarantees, but improves dramatically the efficiency of the previous LSH implementation having the same guarantees. In practice, the LSB-tree itself is also an effective index which consumes linear space, supports efficient updates, and provides accurate query results. In our experiments, the LSB-tree was faster than: (i) iDistance (a famous technique for exact NN search) by two orders of magnitude, and (ii) MedRank (a recent approximate method with nontrivial quality guarantees) by one order of magnitude, and meanwhile returned much better results. As a second step, we extend our LSB technique to solve another classic problem, called Closest Pair (CP) search, in high-dimensional space. The long-term challenge for this problem has been to achieve subquadratic running time at very high dimensionalities, which fails most of the existing solutions. We show that, using a LSB-forest, CP search can be accomplished in (worst-case) time significantly lower than the quadratic complexity, yet still ensuring very good quality. In practice, accurate answers can be found using just two LSB-trees, thus giving a substantial reduction in the space and running time. In our experiments, our technique was faster: (i) than distance browsing (a well-known method for solving the problem exactly) by several orders of magnitude, and (ii) than D-shift (an approximate approach with theoretical guarantees in low-dimensional space) by one order of magnitude, and at the same time, outputs better results.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W2107427524",
    "type": "article"
  },
  {
    "title": "Towards multidimensional subspace skyline analysis",
    "doi": "https://doi.org/10.1145/1189769.1189774",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Jian Pei; Yidong Yuan; Xuemin Lin; Wen Jin; Martin Ester; Qing Liu; Wei Wang; Yufei Tao; Jeffrey Xu Yu; Qing Zhang",
    "corresponding_authors": "",
    "abstract": "The skyline operator is important for multicriteria decision-making applications. Although many recent studies developed efficient methods to compute skyline objects in a given space, none of them considers skylines in multiple subspaces simultaneously. More importantly, the fundamental problem on the semantics of skylines remains open: Why and in which subspaces is (or is not) an object in the skyline? Practically, users may also be interested in the skylines in any subspaces. Then, what is the relationship between the skylines in the subspaces and those in the super-spaces? How can we effectively analyze the subspace skylines? Can we efficiently compute skylines in various subspaces and answer various analytical queries?In this article, we tackle the problem of multidimensional subspace skyline computation and analysis. We explore skylines in subspaces. First, we propose the concept of Skycube, which consists of skylines of all possible nonempty subspaces of a given full space. Once a Skycube is materialized, any subspace skyline queries can be answered online. However, Skycube cannot fully address the semantic concerns and may contain redundant information. To tackle the problem, we introduce a novel notion of skyline group which essentially is a group of objects that coincide in the skylines of some subspaces. We identify the decisive subspaces that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and drill-down analysis is introduced. We also develop efficient algorithms to compute Skycube, skyline groups and their decisive subspaces. A systematic performance study using both real data sets and synthetic data sets is reported to evaluate our approach.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W2130696419",
    "type": "article"
  },
  {
    "title": "Probabilistic information retrieval approach for ranking of database query results",
    "doi": "https://doi.org/10.1145/1166074.1166085",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Surajit Chaudhuri; Gautam Das; Vagelis Hristidis; Gerhard Weikum",
    "corresponding_authors": "",
    "abstract": "We investigate the problem of ranking the answers to a database query when many tuples are returned. In particular, we present methodologies to tackle the problem for conjunctive and range queries, by adapting and applying principles of probabilistic models from information retrieval for structured data. Our solution is domain independent and leverages data and workload statistics and correlations. We evaluate the quality of our approach with a user survey on a real database. Furthermore, we present and experimentally evaluate algorithms to efficiently retrieve the top ranked results, which demonstrate the feasibility of our ranking system.",
    "cited_by_count": 115,
    "openalex_id": "https://openalex.org/W2136121557",
    "type": "article"
  },
  {
    "title": "A characterization of globally consistent databases and their correct access paths",
    "doi": "https://doi.org/10.1145/319983.319988",
    "publication_date": "1983-06-01",
    "publication_year": 1983,
    "authors": "Yehoshua Sagiv",
    "corresponding_authors": "Yehoshua Sagiv",
    "abstract": "The representative instance is proposed as a representation of the data stored in a database whose relations are not the projections of a universal instance. Database schemes are characterized for which local consistency implies global consistency. (Local consistency means that each relation satisfies its own functional dependencies; global consistency means that the representative instance satisfies all the functional dependencies.) A method of efficiently computing projections of the representative instance is given, provided that local consistency implies global consistency. Throughout, it is assumed that a cover of the functional dependencies is embodied in the database scheme in the form of keys.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2146899290",
    "type": "article"
  },
  {
    "title": "On the expressiveness of implicit provenance in query and update languages",
    "doi": "https://doi.org/10.1145/1412331.1412340",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Peter Buneman; James Cheney; Stijn Vansummeren",
    "corresponding_authors": "",
    "abstract": "Information describing the origin of data, generally referred to as provenance , is important in scientific and curated databases where it is the basis for the trust one puts in their contents. Since such databases are constructed using operations of both query and update languages, it is of paramount importance to describe the effect of these languages on provenance. In this article we study provenance for query and update languages that are closely related to SQL, and compare two ways in which they can manipulate provenance so that elements of the input are rearranged to elements of the output: implicit provenance , where a query or update only provides the rearranged output, and provenance is provided implicitly by a default provenance semantics; and explicit provenance , where a query or update provides both the output and the description of the provenance of each component of the output. Although explicit provenance is in general more expressive, we show that the classes of implicit provenance operations expressible by query and update languages correspond to natural semantic subclasses of the explicit provenance queries. One of the consequences of this study is that provenance separates the expressive power of query and update languages. The model is also relevant to annotation propagation schemes in which annotations on the input to a query or update have to be transferred to the output or vice versa.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2156399060",
    "type": "article"
  },
  {
    "title": "Effects of locking granularity in a database management system",
    "doi": "https://doi.org/10.1145/320557.320566",
    "publication_date": "1977-09-01",
    "publication_year": 1977,
    "authors": "Daniel R. Ries; Michael Stonebraker",
    "corresponding_authors": "",
    "abstract": "Many database systems guarantee some form of integrity control upon multiple concurrent updates by some form of locking. Some “granule” of the database is chosen as the unit which is individually locked, and a lock management algorithm is used to ensure integrity. Using a simulation model, this paper explores the desired size of a granule. Under a wide variety of seemingly realistic conditions, surprisingly coarse granularity is called for. The paper concludes with some implications of these results concerning the viability of so-called “predicate locking”.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W2028511104",
    "type": "article"
  },
  {
    "title": "Security of statistical databases",
    "doi": "https://doi.org/10.1145/319540.319555",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "Jan Schlörer",
    "corresponding_authors": "Jan Schlörer",
    "abstract": "The concept of multidimensional transformation of statistical databases is described. A given set of statistical output may be compatible with more than one statistical database. A transformed database D ' is a database which (1) differs from the original database D in its record content, but (2) produces, within certain limits, the same statistical output as the original database. For a transformable database D there are two options: One may physically transform D into a suitable database D ', or one may release only that output which will not permit the users to decide whether it comes from D or D '. The second way is, of course, the easier one. Basic structural requirements for transformable statistical databases are investigated. The closing section discusses advantages, drawbacks, and open questions.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W2125099941",
    "type": "article"
  },
  {
    "title": "Generation and search of clustered files",
    "doi": "https://doi.org/10.1145/320289.320291",
    "publication_date": "1978-12-01",
    "publication_year": 1978,
    "authors": "Gerard Salton; Anita M.-Y. Wong",
    "corresponding_authors": "",
    "abstract": "A classified, or clustered file is one where related, or similar records are grouped into classes, or clusters of items in such a way that all items within a cluster are jointly retrievable. Clustered files are easily adapted to broad and narrow search strategies, and simple file updating methods are available. An inexpensive file clustering method applicable to large files is given together with appropriate file search methods. An abstract model is then introduced to predict the retrieval effectiveness of various search methods in a clustered file environment. Experimental evidence is included to test the versatility of the model and to demonstrate the role of various parameters in the cluster search process.",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W2054409659",
    "type": "article"
  },
  {
    "title": "Views and queries",
    "doi": "https://doi.org/10.1145/1806907.1806913",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Alan Nash; Luc Segoufin; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "We investigate the question of whether a query Q can be answered using a set V of views. We first define the problem in information-theoretic terms: we say that V determines Q if V provides enough information to uniquely determine the answer to Q . Next, we look at the problem of rewriting Q in terms of V using a specific language. Given a view language V and query language Q , we say that a rewriting language R is complete for V -to- Q rewritings if every Q ∈ Q can be rewritten in terms of V ∈ V using a query in R , whenever V determines Q . While query rewriting using views has been extensively investigated for some specific languages, the connection to the information-theoretic notion of determinacy, and the question of completeness of a rewriting language have received little attention. In this article we investigate systematically the notion of determinacy and its connection to rewriting. The results concern decidability of determinacy for various view and query languages, as well as the power required of complete rewriting languages. We consider languages ranging from first-order to conjunctive queries.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2066684585",
    "type": "article"
  },
  {
    "title": "On searching transposed files",
    "doi": "https://doi.org/10.1145/320107.320125",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "Don Batory",
    "corresponding_authors": "Don Batory",
    "abstract": "A transposed file is a collection of nonsequential files called subfiles. Each subfile contains selected attribute data for all records. It is shown that transposed file performance can be enhanced by using a proper strategy to process queries. Analytic cost expressions for processing conjunctive, disjunctive, and batched queries are developed and an effective heuristic for minimizing query processing costs is presented. Formulations of the problem of optimally processing queries for a particular family or transposed files are shown to be NP-complete. Query processing performance comparisons of multilist, inverted, and nonsequential files with transposed files are also considered.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W2021284998",
    "type": "article"
  },
  {
    "title": "SYSTEM/U: a database system based on the universal relation assumption",
    "doi": "https://doi.org/10.1145/1270.1209",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Henry F. Korth; Gabriel M. Kuper; Joan Feigenbaum; Allen Van Gelder; Jeffrey D. Ullman",
    "corresponding_authors": "",
    "abstract": "System/U is a universal relation database system under development at Standford University which uses the language C on UNIX. The system is intended to test the use of the universal view, in which the entire database is seen as one relation. This paper describes the theory behind System/U, in particular the theory of maximal objects and the connection between a set of attributes. We also describe the implementation of the DDL (Data Description Language) and the DML (Data Manipulation Language), and discuss in detail how the DDL finds maximal objects and how the DML determines the connection between the attributes that appear in a query.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2140136514",
    "type": "article"
  },
  {
    "title": "Efficient top- <i>k</i> aggregation of ranked inputs",
    "doi": "https://doi.org/10.1145/1272743.1272749",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Nikos Mamoulis; Man Lung Yiu; Kit Hung Cheng; David W. Cheung",
    "corresponding_authors": "",
    "abstract": "A top- k query combines different rankings of the same set of objects and returns the k objects with the highest combined score according to an aggregate function. We bring to light some key observations, which impose two phases that any top- k algorithm, based on sorted accesses, should go through. Based on them, we propose a new algorithm, which is designed to minimize the number of object accesses, the computational cost, and the memory requirements of top- k search with monotone aggregate functions. We provide an analysis for its cost and show that it is always no worse than the baseline “no random accesses” algorithm in terms of computations, accesses, and memory required. As a side contribution, we perform a space analysis, which indicates the memory requirements of top- k algorithms that only perform sorted accesses. For the case, where the required space exceeds the available memory, we propose disk-based variants of our algorithm. We propose and optimize a multiway top- k join operator, with certain advantages over evaluation trees of binary top- k join operators. Finally, we define and study the computation of top- k cubes and the implementation of roll-up and drill-down operations in such cubes. Extensive experiments with synthetic and real data show that, compared to previous techniques, our method accesses fewer objects, while being orders of magnitude faster.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W1989800045",
    "type": "article"
  },
  {
    "title": "Inverting schema mappings",
    "doi": "https://doi.org/10.1145/1292609.1292615",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Ronald Fagin",
    "corresponding_authors": "Ronald Fagin",
    "abstract": "A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). Although the notion of an inverse of a schema mapping is important, the exact definition of an inverse mapping is somewhat elusive. This is because a schema mapping may associate many target instances with each source instance, and many source instances with each target instance. Based on the notion that the composition of a mapping and its inverse is the identity, we give a formal definition for what it means for a schema mapping M′ to be an inverse of a schema mapping M for a class S of source instances. We call such an inverse an S- inverse . A particular case of interest arises when S is the class of all source instances, in which case an S-inverse is a global inverse. We focus on the important and practical case of schema mappings specified by source-to-target tuple-generating dependencies, and uncover a rich theory. When S is specified by a set of dependencies with a finite chase, we show how to construct an S-inverse when one exists. In particular, we show how to construct a global inverse when one exists. Given M and M′, we show how to define the largest class S such that M′ is an S-inverse of M.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W1993268751",
    "type": "article"
  },
  {
    "title": "An attribute based model for database access cost analysis",
    "doi": "https://doi.org/10.1145/320521.320535",
    "publication_date": "1977-03-01",
    "publication_year": 1977,
    "authors": "S. Bing Yao",
    "corresponding_authors": "S. Bing Yao",
    "abstract": "A generalized model for physical database organizations is presented. Existing database organizations are shown to fit easily into the model as special cases. Generalized access algorithms and cost equations associated with the model are developed and analyzed. The model provides a general design framework in which the distinguishing properties of database organizations are made explicit and their performances can be compared.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2162362492",
    "type": "article"
  },
  {
    "title": "Answering queries without revealing secrets",
    "doi": "https://doi.org/10.1145/319830.319833",
    "publication_date": "1983-03-01",
    "publication_year": 1983,
    "authors": "George L. Sicherman; Wiebren de Jonge; Reind P. van de Riet",
    "corresponding_authors": "",
    "abstract": "Question-answering systems must often keep certain information secret. This can be accomplished, for example, by sometimes refusing to answer a query. Here the danger of revealing a secret by refusing to answer a query is investigated. First several criteria that can be used to decide whether or not to answer a query are developed. Then it is shown which of these criteria are safe if the questioner knows nothing at all about what is kept secret. Furthermore, it is proved that one of these criteria is safe even if the user of the system knows which information is to be kept secret.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2045709548",
    "type": "article"
  },
  {
    "title": "A survey of B-tree locking techniques",
    "doi": "https://doi.org/10.1145/1806907.1806908",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Goetz Graefe",
    "corresponding_authors": "Goetz Graefe",
    "abstract": "B-trees have been ubiquitous in database management systems for several decades, and they are used in other storage systems as well. Their basic structure and basic operations are well and widely understood including search, insertion, and deletion. Concurrency control of operations in B-trees, however, is perceived as a difficult subject with many subtleties and special cases. The purpose of this survey is to clarify, simplify, and structure the topic of concurrency control in B-trees by dividing it into two subtopics and exploring each of them in depth.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2075372521",
    "type": "article"
  },
  {
    "title": "A foundation of CODD's relational maybe-operations",
    "doi": "https://doi.org/10.1145/319996.320014",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "Joachim Biskup",
    "corresponding_authors": "Joachim Biskup",
    "abstract": "Database relations which possibly contain maybe-tuples and null values of type “value at present unknown” are studied. Maybe-tuples and null values are formally interpreted by our notion of representation , which uses classical notions of predicate logic, elaborates Codd's proposal of maybe-tuples, and adopts Reiter's concept of a closed world. Precise notions of information content and redundancy , associated with our notion of representation, are investigated. Extensions of the relational algebra to relations with maybe-tuples and null values are proposed. Our extensions are essentially Codd's, with some modifications. It is proved that these extensions have natural properties which are formally stated as being adequate and restricted . By the treatment of difference and division, our formal framework can be used even for operations that require “negative information.” Finally, extensions of update operations are discussed.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2167272239",
    "type": "article"
  },
  {
    "title": "Inference of concise regular expressions and DTDs",
    "doi": "https://doi.org/10.1145/1735886.1735890",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Geert Jan Bex; Frank Neven; Thomas Schwentick; Stijn Vansummeren",
    "corresponding_authors": "",
    "abstract": "We consider the problem of inferring a concise Document Type Definition (DTD) for a given set of XML-documents, a problem that basically reduces to learning concise regular expressions from positive examples strings. We identify two classes of concise regular expressions—the single occurrence regular expressions (SOREs) and the chain regular expressions (CHAREs)—that capture the far majority of expressions used in practical DTDs. For the inference of SOREs we present several algorithms that first infer an automaton for a given set of example strings and then translate that automaton to a corresponding SORE, possibly repairing the automaton when no equivalent SORE can be found. In the process, we introduce a novel automaton to regular expression rewrite technique which is of independent interest. When only a very small amount of XML data is available, however (for instance when the data is generated by Web service requests or by answers to queries), these algorithms produce regular expressions that are too specific. Therefore, we introduce a novel learning algorithm crx that directly infers CHAREs (which form a subclass of SOREs) without going through an automaton representation. We show that crx performs very well within its target class on very small datasets.",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W2063985934",
    "type": "article"
  },
  {
    "title": "Locking granularity revisited",
    "doi": "https://doi.org/10.1145/320071.320078",
    "publication_date": "1979-06-01",
    "publication_year": 1979,
    "authors": "Daniel R. Ries; Michael Stonebraker",
    "corresponding_authors": "",
    "abstract": "Locking granularity refers to the size and hence the number of locks used to ensure the consistency of a database during multiple concurrent updates. In an earlier simulation study we concluded that coarse granularity, such as area or file locking, is to be preferred to fine granularity such as individual page or record locking. However, alternate assumptions than those used in the original paper can change that conclusion. First, we modified the assumptions concerning the placement of the locks on the database with respect to the accessing transactions. In the original model the locks were assumed to be well placed. Under worse case and random placement assumptions when only very small transactions access the database, fine granularity is preferable. Second, we extended the simulation to model a lock hierarchy where large transactions use large locks and small transactions use small locks. In this scenario, again under the random and worse case lock placement assumptions, fine granularity is preferable if all transactions accessing more than 1 percent of the database use large locks. Finally, the simulation was extended to model a “claim as needed” locking strategy together with the resultant possibility of deadlock. In the original study all locks were claimed in one atomic operation at the beginning of a transaction. The claim as needed strategy does not change the conclusions concerning the desired granularity.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2159927064",
    "type": "article"
  },
  {
    "title": "Query Rewriting and Optimization for Ontological Databases",
    "doi": "https://doi.org/10.1145/2638546",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Georg Gottlob; Giorgio Orsi; Andréas Pieris",
    "corresponding_authors": "",
    "abstract": "Ontological queries are evaluated against a knowledge base consisting of an extensional database and an ontology (i.e., a set of logical assertions and constraints that derive new intensional knowledge from the extensional database), rather than directly on the extensional database. The evaluation and optimization of such queries is an intriguing new problem for database research. In this article, we discuss two important aspects of this problem: query rewriting and query optimization. Query rewriting consists of the compilation of an ontological query into an equivalent first-order query against the underlying extensional database. We present a novel query rewriting algorithm for rather general types of ontological constraints that is well suited for practical implementations. In particular, we show how a conjunctive query against a knowledge base, expressed using linear and sticky existential rules, that is, members of the recently introduced Datalog± family of ontology languages, can be compiled into a union of conjunctive queries (UCQ) against the underlying database. Ontological query optimization, in this context, attempts to improve this rewriting process soas to produce possibly small and cost-effective UCQ rewritings for an input query.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W1970956440",
    "type": "article"
  },
  {
    "title": "A clustering algorithm for hierarchical structures",
    "doi": "https://doi.org/10.1145/320521.320531",
    "publication_date": "1977-03-01",
    "publication_year": 1977,
    "authors": "Mario Schkolnick",
    "corresponding_authors": "Mario Schkolnick",
    "abstract": "article Free Access Share on A clustering algorithm for hierarchical structuresACM Transactions on Database SystemsVolume 2Issue 1March 1977 pp 27–44https://doi.org/10.1145/320521.320531Published:01 March 1977Publication History 61citation787DownloadsMetricsTotal Citations61Total Downloads787Last 12 Months21Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W2144420390",
    "type": "article"
  },
  {
    "title": "Performance evaluation of a relational associative processor",
    "doi": "https://doi.org/10.1145/320544.320553",
    "publication_date": "1977-06-01",
    "publication_year": 1977,
    "authors": "Esen A. Ozkarahan; S. A. Schuster; K. C. Sevcik",
    "corresponding_authors": "",
    "abstract": "An associative processor called RAP has been designed to provide hardware support for the use and manipulation of databases. RAP is particularly suited for supporting relational databases. In this paper, the relational operations provided by the RAP hardware are described, and a representative approach to providing the same relational operations with conventional software and hardware is devised. Analytic models are constructed for RAP and the conventional system. The execution times of several of the operations are shown to be vastly improved with RAP for large relations.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W2159111714",
    "type": "article"
  },
  {
    "title": "Private Analysis of Graph Structure",
    "doi": "https://doi.org/10.1145/2611523",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Vishesh Karwa; Sofya Raskhodnikova; Adam Smith; Grigory Yaroslavtsev",
    "corresponding_authors": "",
    "abstract": "We present efficient algorithms for releasing useful statistics about graph data while providing rigorous privacy guarantees. Our algorithms work on datasets that consist of relationships between individuals, such as social ties or email communication. The algorithms satisfy edge differential privacy , which essentially requires that the presence or absence of any particular relationship be hidden. Our algorithms output approximate answers to subgraph counting queries . Given a query graph H , for example, a triangle, k -star, or k -triangle, the goal is to return the number of edge-induced isomorphic copies of H in the input graph. The special case of triangles was considered by Nissim et al. [2007] and a more general investigation of arbitrary query graphs was initiated by Rastogi et al. [2009]. We extend the approach of Nissim et al. to a new class of statistics, namely k -star queries. We also give algorithms for k -triangle queries using a different approach based on the higher-order local sensitivity. For the specific graph statistics we consider (i.e., k -stars and k -triangles), we significantly improve on the work of Rastogi et al.: our algorithms satisfy a stronger notion of privacy that does not rely on the adversary having a particular prior distribution on the data, and add less noise to the answers before releasing them. We evaluate the accuracy of our algorithms both theoretically and empirically, using a variety of real and synthetic datasets. We give explicit, simple conditions under which these algorithms add a small amount of noise. We also provide the average-case analysis in the Erdős-Rényi-Gilbert G ( n , p ) random graph model. Finally, we give hardness results indicating that the approach Nissim et al. used for triangles cannot easily be extended to k -triangles (hence justifying our development of a new algorithmic approach).",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W1992461601",
    "type": "article"
  },
  {
    "title": "Static analysis and optimization of semantic web queries",
    "doi": "https://doi.org/10.1145/2500130",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Andrés Letelier; Jorge Eduardo Pérez Pérez; Reinhard Pichler; Sebastian Skritek",
    "corresponding_authors": "",
    "abstract": "Static analysis is a fundamental task in query optimization. In this article we study static analysis and optimization techniques for SPARQL, which is the standard language for querying Semantic Web data. Of particular interest for us is the optionality feature in SPARQL. It is crucial in Semantic Web data management, where data sources are inherently incomplete and the user is usually interested in partial answers to queries. This feature is one of the most complicated constructors in SPARQL and also the one that makes this language depart from classical query languages such as relational conjunctive queries. We focus on the class of well-designed SPARQL queries, which has been proposed in the literature as a fragment of the language with good properties regarding query evaluation. We first propose a tree representation for SPARQL queries, called pattern trees, which captures the class of well-designed SPARQL graph patterns. Among other results, we propose several rules that can be used to transform pattern trees into a simple normal form, and study equivalence and containment. We also study the evaluation and enumeration problems for this class of queries.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2033267049",
    "type": "article"
  },
  {
    "title": "On the Hardness and Approximation of Euclidean DBSCAN",
    "doi": "https://doi.org/10.1145/3083897",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Junhao Gan; Yufei Tao",
    "corresponding_authors": "",
    "abstract": "DBSCAN is a method proposed in 1996 for clustering multi-dimensional points, and has received extensive applications. Its computational hardness is still unsolved to this date. The original KDD‚96 paper claimed an algorithm of O ( n log n ) ”average runtime complexity„ (where n is the number of data points) without a rigorous proof. In 2013, a genuine O ( n log n )-time algorithm was found in 2D space under Euclidean distance. The hardness of dimensionality d ≥3 has remained open ever since. This article considers the problem of computing DBSCAN clusters from scratch (assuming no existing indexes) under Euclidean distance. We prove that, for d ≥3, the problem requires ω( n 4/3 ) time to solve, unless very significant breakthroughs—ones widely believed to be impossible—could be made in theoretical computer science. Motivated by this, we propose a relaxed version of the problem called ρ- approximate DBSCAN , which returns the same clusters as DBSCAN, unless the clusters are ”unstable„ (i.e., they change once the input parameters are slightly perturbed). The ρ-approximate problem can be settled in O ( n ) expected time regardless of the constant dimensionality d . The article also enhances the previous result on the exact DBSCAN problem in 2D space. We show that, if the n data points have been pre-sorted on each dimension (i.e., one sorted list per dimension), the problem can be settled in O ( n ) worst-case time. As a corollary, when all the coordinates are integers, the 2D DBSCAN problem can be solved in O ( n log log n ) time deterministically, improving the existing O ( n log n ) bound.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2740025111",
    "type": "article"
  },
  {
    "title": "Optimal Bloom Filters and Adaptive Merging for LSM-Trees",
    "doi": "https://doi.org/10.1145/3276980",
    "publication_date": "2018-12-08",
    "publication_year": 2018,
    "authors": "Niv Dayan; Manos Athanassoulis; Stratos Idreos",
    "corresponding_authors": "",
    "abstract": "In this article, we show that key-value stores backed by a log-structured merge-tree (LSM-tree) exhibit an intrinsic tradeoff between lookup cost, update cost, and main memory footprint, yet all existing designs expose a suboptimal and difficult to tune tradeoff among these metrics. We pinpoint the problem to the fact that modern key-value stores suboptimally co-tune the merge policy, the buffer size, and the Bloom filters’ false-positive rates across the LSM-tree’s different levels. We present Monkey, an LSM-tree based key-value store that strikes the optimal balance between the costs of updates and lookups with any given main memory budget. The core insight is that worst-case lookup cost is proportional to the sum of the false-positive rates of the Bloom filters across all levels of the LSM-tree. Contrary to state-of-the-art key-value stores that assign a fixed number of bits-per-element to all Bloom filters, Monkey allocates memory to filters across different levels so as to minimize the sum of their false-positive rates. We show analytically that Monkey reduces the asymptotic complexity of the worst-case lookup I/O cost, and we verify empirically using an implementation on top of RocksDB that Monkey reduces lookup latency by an increasing margin as the data volume grows (50--80% for the data sizes we experimented with). Furthermore, we map the design space onto a closed-form model that enables adapting the merging frequency and memory allocation to strike the best tradeoff among lookup cost, update cost and main memory, depending on the workload (proportion of lookups and updates), the dataset (number and size of entries), and the underlying hardware (main memory available, disk vs. flash). We show how to use this model to answer what-if design questions about how changes in environmental parameters impact performance and how to adapt the design of the key-value store for optimal performance.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2903162282",
    "type": "article"
  },
  {
    "title": "Smart Meter Data Analytics",
    "doi": "https://doi.org/10.1145/3004295",
    "publication_date": "2016-11-21",
    "publication_year": 2016,
    "authors": "Xiufeng Liu; Lukasz Golab; Wojciech Golab; Ihab F. Ilyas; Shichao Jin",
    "corresponding_authors": "",
    "abstract": "Smart electricity meters have been replacing conventional meters worldwide, enabling automated collection of fine-grained (e.g., every 15 minutes or hourly) consumption data. A variety of smart meter analytics algorithms and applications have been proposed, mainly in the smart grid literature. However, the focus has been on what can be done with the data rather than how to do it efficiently. In this paper, we examine smart meter analytics from a software performance perspective. First, we design a performance benchmark that includes common smart meter analytics tasks. These include off-line feature extraction and model building as well a framework for on-line anomaly detection that we propose. Second, since obtaining real smart meter data is difficult due to privacy issues, we present an algorithm for generating large realistic data sets from a small seed of real data. Third, we implement the proposed benchmark using five representative platforms: a traditional numeric computing platform (Matlab), a relational DBMS with a built-in machine learning toolkit (PostgreSQL/MADlib), a main-memory column store (“System C”), and two distributed data processing platforms (Hive and Spark/Spark Streaming). We compare the five platforms in terms of application development effort and performance on a multicore machine as well as a cluster of 16 commodity servers.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2553567329",
    "type": "article"
  },
  {
    "title": "SQL’s Three-Valued Logic and Certain Answers",
    "doi": "https://doi.org/10.1145/2877206",
    "publication_date": "2016-03-18",
    "publication_year": 2016,
    "authors": "Leonid Libkin",
    "corresponding_authors": "Leonid Libkin",
    "abstract": "The goal of the article is to bridge the difference between theoretical and practical approaches to answering queries over databases with nulls. Theoretical research has long ago identified the notion of correctness of query answering over incomplete data: one needs to find certain answers, which are true regardless of how incomplete information is interpreted. This serves as the notion of correctness of query answering, but carries a huge complexity tag. In practice, on the other hand, query answering must be very efficient, and to achieve this, SQL uses three-valued logic for evaluating queries on databases with nulls. Due to the complexity mismatch, the two approaches cannot coincide, but perhaps they are related in some way. For instance, does SQL always produce answers we can be certain about? This is not so: SQL’s and certain answers semantics could be totally unrelated. We show, however, that a slight modification of the three-valued semantics for relational calculus queries can provide the required certainty guarantees. The key point of the new scheme is to fully utilize the three-valued semantics, and classify answers not into certain or noncertain, as was done before, but rather into certainly true, certainly false, or unknown. This yields relatively small changes to the evaluation procedure, which we consider at the level of both declarative (relational calculus) and procedural (relational algebra) queries. These new evaluation procedures give us certainty guarantees even for queries returning tuples with null values.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2297697128",
    "type": "article"
  },
  {
    "title": "MobilityDB",
    "doi": "https://doi.org/10.1145/3406534",
    "publication_date": "2020-12-06",
    "publication_year": 2020,
    "authors": "Esteban Zimányi; Mahmoud Sakr; Arthur Lesuisse",
    "corresponding_authors": "",
    "abstract": "Despite two decades of research in moving object databases and a few research prototypes that have been proposed, there is not yet a mainstream system targeted for industrial use. In this article, we present MobilityDB, a moving object database that extends the type system of PostgreSQL and PostGIS with abstract data types for representing moving object data. The types are fully integrated into the platform to reuse its powerful data management features. Furthermore, MobilityDB builds on existing operations, indexing, aggregation, and optimization framework. This is all made accessible via the SQL query interface.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W3112327940",
    "type": "article"
  },
  {
    "title": "A model for compound type changes encountered in schema evolution",
    "doi": "https://doi.org/10.1145/352958.352983",
    "publication_date": "2000-03-01",
    "publication_year": 2000,
    "authors": "Barbara Lerner",
    "corresponding_authors": "Barbara Lerner",
    "abstract": "Schema evolution is a problem that is faced by long-lived data. When a schema changes, existing persistent data can become inaccessible unless the database system provides mechanisms to access data created with previous versions of the schema. Most existing systems that support schema evolution focus on changes local to individual types within the schema, thereby limiting the changes that the database maintainer can perform. We have developed a model of type changes involving multiple types. The model describes both type changes and their impact on data by defining derivation rules to initialize new data based on the existing data. The derivation rules can describe local and nonlocal changes to types to capture the intent of a large class of type change operations. We have built a system called Tess (Type Evolution Software System) that uses this model to recognize type changes by comparing schemas and then produces a transformer that can update data in a database to correspond to a newer version of the schema.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W1978350877",
    "type": "article"
  },
  {
    "title": "Conceptual schema analysis",
    "doi": "https://doi.org/10.1145/293910.293150",
    "publication_date": "1998-09-01",
    "publication_year": 1998,
    "authors": "Silvana Castano; V. De Antonellis; Mariagrazia Fugini; Barbara Pernici",
    "corresponding_authors": "",
    "abstract": "The problem of analyzing and classifying conceptual schemas is becomig increasingly important due to the availability of a large number of schemas related to existing applications. The purposes of schema analysis and classification activities can be different: to extract information on intensional properties of legacy systems in order to restructure or migrate to new architectures; to build libraries of reference conceptual components to be used in building new applications in a given domain; and to identify information flows and possible replication of data in an organization. This article proposes a set of techniques for schema analysis and classification to be used separately or in combination. The techniques allow the analyst to derive significant properties from schemas, with human intervention limited as far as possible. In particular, techniques for associating descriptors with schemas, for abstracting reference conceptual schemas based on schema clustering, and for determining schema similarity are presented. A methodology for systematic schema analysis is illustrated, with the purpose of identifying and abstracting into reference components the similar and potentially reusable parts of a set of schemas. Experiences deriving from the application of the proposed techniques and methodology on a large set of Entity-Relationship conceptual schemas of information systems in the Italian Public Administration domain are described",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2086980839",
    "type": "article"
  },
  {
    "title": "Broadcast protocols to support efficient retrieval from databases by mobile users",
    "doi": "https://doi.org/10.1145/310701.310710",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "Anindya Datta; Debra VanderMeer; Aslihan Celik; Vijay Kumar",
    "corresponding_authors": "",
    "abstract": "Mobile computing has the potential for managing information globally. Data management issues in mobile computing have received some attention in recent times, and the design of adaptive braodcast protocols has been posed as an important probllem. Such protocols are employed by database servers to decide on the content of bbroadcasts dynamically, in response to client mobility and demand patterns. In this paper we design such protocols and also propose efficient retrieval strategies that may be employed by clients to download information from broadcasts. The goal is to design cooperative strategies between server and client to provide access to information in such a way as to minimize energy expenditure by clients. We evaluate the performance of our protocols both analytically and through simulation.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W2131256609",
    "type": "article"
  },
  {
    "title": "A cost model for query processing in high dimensional data spaces",
    "doi": "https://doi.org/10.1145/357775.357776",
    "publication_date": "2000-06-01",
    "publication_year": 2000,
    "authors": "Christian Böhm",
    "corresponding_authors": "Christian Böhm",
    "abstract": "During the last decade, multimedia databases have become increasingly important in many application areas such as medicine, CAD, geography, and molecular biology. An important research topic in multimedia databases is similarity search in large data sets. Most current approaches that address similarity search use the feature approach, which transforms important properties of the stored objects into points of a high-dimensional space (feature vectors). Thus, similarity search is transformed into a neighborhood search in feature space. Multidimensional index structures are usually applied when managing feature vectors. Query processing can be improved substantially with optimization techniques such as blocksize optimization, data space quantization, and dimension reduction. To determine optimal parameters, an accurate estimate of index-based query processing performance is crucial. In this paper we develop a cost model for index structures for point databases such as the R*-tree and the X-tree. It provides accurate estimates of the number of data page accesses for range queries and nearest-neighbor queries under a Euclidean metric and a maximum metric and a maximum metric. The problems specific to high-dimensional data spaces, called boundary effects, are considered. The concept of the fractal dimension is used to take the effects of correlated data into account.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W2137565219",
    "type": "article"
  },
  {
    "title": "Information gathering in the World-Wide Web",
    "doi": "https://doi.org/10.1145/296854.277639",
    "publication_date": "1998-12-01",
    "publication_year": 1998,
    "authors": "David Konopnicki; Oded Shmueli",
    "corresponding_authors": "",
    "abstract": "The World Wide Web (WWW) is a fast growing global information resource. It contains an enormous amount of information and provides access to a variety of services. Since there is no central control and very few standards of information organization or service offering, searching for information and services is a widely recognized problem. To some degree this problem is solved by “search services,” also known as “indexers,” such as Lycos, AltaVista, Yahoo, and others. These sites employ search engines known as “robots” or “knowbots” that scan the network periodically and form text-based indices. These services are limited in certain important aspects. First, the structural information, namely, the organization of the document into parts pointing to each other, is usually lost. Second, one is limited by the kind of textual analysis provided by the “search service.” Third, search services are incapable of navigating “through” forms. Finally, one cannot prescribe a complex database-like search. We view the WWW as a huge database. We have designed a high-level SQL-like language called W3QL to support effective and flexible query processing, which addresses the structure and content of WWW nodes and their varied sorts of data. We have implemented a system called W3QS to execute W3QL queries. In W3QS, query results are declaratively specified and continuously maintained as views when desired. The current architecture of W3QS provides a server that enables users to pose queries as well as integrate their own data analysis tools. The system and its query language set a framework for the development of database-like tools over the WWW. A significant contribution of this article is in formalizing the WWW and query processing over it.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W1969750686",
    "type": "article"
  },
  {
    "title": "<i>XQBE</i> ( <i>XQ</i> uery <i>B</i> y <i>E</i> xample)",
    "doi": "https://doi.org/10.1145/1071610.1071613",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Daniele Braga; Alessandro Campi; Stefano Ceri",
    "corresponding_authors": "",
    "abstract": "The spreading of XML data in many contexts of modern computing infrastructures and systems causes a pressing need for adequate XML querying capabilities; to address this need, the W3C is proposing XQuery as the standard query language for XML, with a language paradigm and a syntactic flavor comparable to the SQL relational language. XQuery is designed for meeting the requirements of skilled database programmers; its inherent complexity makes the new language unsuited to unskilled users.In this article we present XQBE (XQuery By Example), a visual query language for expressing a large subset of XQuery in a visual form. In designing XQBE, we targeted both unskilled users and expert users wishing to speed up the construction of their queries; we have been inspired by QBE, a relational language initially proposed as an alternative to SQL, which is supported by Microsoft Access. QBE is extremely successful among users who are not computer professionals and do not understand the subtleties of query languages, as well as among professionals who can draft their queries very quickly.According to the hierarchical nature of XML, XQBE's main graphical elements are trees. One or more trees denote the documents assumed as query input, and one tree denotes the document produced by the query. Similar to QBE, trees are annotated so as to express selection predicates, joins, and the passing of information from the input trees to the output tree.This article formally defines the syntax and semantics of XQBE, provides a large set of examples, and presents a prototype implementation.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W2045359371",
    "type": "article"
  },
  {
    "title": "Outerjoin simplification and reordering for query optimization",
    "doi": "https://doi.org/10.1145/244810.244812",
    "publication_date": "1997-03-01",
    "publication_year": 1997,
    "authors": "César Galindo-Legaria; Arnon Rosenthal",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Outerjoin simplification and reordering for query optimization Authors: César Galindo-Legaria Harvard Univ., Cambridge, MA Harvard Univ., Cambridge, MAView Profile , Arnon Rosenthal MITRE Corporation, Bedford, MA MITRE Corporation, Bedford, MAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 22Issue 1March 1997 pp 43–74https://doi.org/10.1145/244810.244812Online:01 March 1997Publication History 77citation1,291DownloadsMetricsTotal Citations77Total Downloads1,291Last 12 Months72Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W2049578111",
    "type": "article"
  },
  {
    "title": "Towards a semantic view of an extended entity-relationship model",
    "doi": "https://doi.org/10.1145/111197.111200",
    "publication_date": "1991-09-01",
    "publication_year": 1991,
    "authors": "Martin Gogolla; Uwe Hohenstein",
    "corresponding_authors": "",
    "abstract": "Nearly all query languages discussed recently for the Entity-Relationship (ER) model do not possess a formal semantics. Languages are often defined by means of examples only. The reason for this phenomenon is the essential gap between features of query languages and theoretical foundations like algebras and calculi. Known languages offer arithmetic capabilities and allow for aggregates, but algebras and calculi defined for ER models do not. This paper introduces an extended ER model concentrating nearly all concepts of known so-called semantic data models in a few syntactical constructs. Moreover, we provide our extended ER model with a formal mathematical semantics. On this basis a well-founded calculus is developed taking into account data operations on arbitrary user-defined data types and aggregate functions. We pay special attention to arithmetic operations, as well as multivalued terms allowing nested queries, in a uniform and consistent manner. We prove our calculus only allows the formulation of safe terms and queries yielding a finite result, and to be (at least) as expressive as the relational calculi.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2070270825",
    "type": "article"
  },
  {
    "title": "Distributed query evaluation on semistructured data",
    "doi": "https://doi.org/10.1145/507234.507235",
    "publication_date": "2002-03-01",
    "publication_year": 2002,
    "authors": "Dan Suciu",
    "corresponding_authors": "Dan Suciu",
    "abstract": "Semistructured data is modeled as a rooted, labeled graph. The simplest kinds of queries on such data are those which traverse paths described by regular path expressions. More complex queries combine several regular path expressions, with complex data restructuring, and with sub-queries. This article addresses the problem of efficient query evaluation on distributed, semistructured databases. In our setting, the nodes of the database are distributed over a fixed number of sites, and the edges are classified into local (with both ends in the same site) and cross edges (with ends in two distinct sites). Efficient evaluation in this context means that the number of communication steps is fixed (independent on the data or the query), and that the total amount of data sent depends only on the number of cross links and of the size of the query's result. We give such algorithms in three different settings. First, for the simple case of queries consisting of a single regular expression; second, for all queries in a calculus for graphs based on structural recursion which in addition to regular path expressions can perform nontrivial restructuring of the graph; and third, for a class of queries we call select-where queries that combine pattern matching and regular path expressions with data restructuring and subqueries. This article also includes a discussion on how these methods can be used to derive efficient view maintenance algorithms.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2001655988",
    "type": "article"
  },
  {
    "title": "Data Mining with optimized two-dimensional association rules",
    "doi": "https://doi.org/10.1145/383891.383893",
    "publication_date": "2001-06-01",
    "publication_year": 2001,
    "authors": "Takeshi Fukuda; Yasuhiko Morimoto; Shinichi Morishita; Takeshi Tokuyama",
    "corresponding_authors": "",
    "abstract": "We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, Age and Balance are two numeric attributes, and CardLoan is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form ((Age,Balance) ∈P)⇒(CardLoan = Yes), which implies that bank customers whose ages and balances fall within a planar region P tend to take out credit card loans with a high probability.We consider two classes of regions, rectangles and admissible (i.e., connected and x-monotone) regions. For each class, we propose efficient algorithms for computing the regions that give optimal association rules for gain, support, and confidence, respectively. We have implemented the algorithms for admissible regions as well as several advanced functions based on them in our data mining system named SONAR (System for Optimized Numeric Association Rules), where the rules are visualized by using a graphic user interface to make it easy for users to gain an intuitive understanding of rules.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2009419163",
    "type": "article"
  },
  {
    "title": "Incremental validation of XML documents",
    "doi": "https://doi.org/10.1145/1042046.1042050",
    "publication_date": "2004-12-12",
    "publication_year": 2004,
    "authors": "Andrey Balmin; Yannis Papakonstantinou; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "We investigate the incremental validation of XML documents with respect to DTDs, specialized DTDs, and XML Schemas, under updates consisting of element tag renamings, insertions, and deletions. DTDs are modeled as extended context-free grammars. \"Specialized DTDs\" allow the decoupling of element types from element tags. XML Schemas are abstracted as specialized DTDs with limitations on the type assignment. For DTDs and XML Schemas, we exhibit an O ( m log n ) incremental validation algorithm using an auxiliary structure of size O ( n ), where n is the size of the document and m the number of updates. The algorithm does not handle the incremental validation of XML Schema wrt renaming of internal nodes, which is handled by the specialized DTDs incremental validation algorithm. For specialized DTDs, we provide an O ( m log 2 n ) incremental algorithm, again using an auxiliary structure of size O ( n ). This is a significant improvement over brute-force re-validation from scratch.We exhibit a restricted class of DTDs called local that arise commonly in practice and for which incremental validation can be done in practically constant time by maintaining only a list of counters. We present implementations of both general incremental validation and local validation on an XML database built on top of a relational database.Our experimentation includes a study of the applicability of local validation in practice, results on the calibration of parameters of the auxiliary data structure, and results on the performance comparison between the general incremental validation technique, the local validation technique, and brute-force validation from scratch.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2014729492",
    "type": "article"
  },
  {
    "title": "Logical design for temporal databases with multiple granularities",
    "doi": "https://doi.org/10.1145/249978.249979",
    "publication_date": "1997-06-01",
    "publication_year": 1997,
    "authors": "X. Sean Wang; Cláudio Bettini; Alexander Brodsky; Sushil Jajodia",
    "corresponding_authors": "",
    "abstract": "The purpose of good database logical design is to eliminate data redundancy and isertion and deletion anomalies. In order to achieve this objective for temporal databases, the notions of temporal types , which formalize time granularities, and temporal functional dependencies (TFDs) are intrduced. A temporal type is a monotonic mapping from ticks of time (represented by positive integers) to time sets (represented by subsets of reals) and is used to capture various standard and user-defined calendars. A TFD is a proper extension of the traditional functional dependency and takes the form X → μ Y, meaning that there is a unique value for Y during one tick of the temporal type μ for one particular X value. An axiomatization for TFDs is given. Because a finite set TFDs usually implies an infinite number of TFDs, we introduce the notion of and give an axiomatization for a finite closure to effectively capture a finite set of implied TFDs that are essential of the logical design. Temporal normalization procedures with respect to TFDs are given. Specifically, temporal Boyce-Codd normal form (TBCNF) that avoids all data redundancies due to TFDs, and temporal third normal form (T3NF) that allows dependency preservation, are defined. Both normal forms are proper extensions of their traditional counterparts, BCNF and 3NF. Decompositition algorithms are presented that give lossless TBCNF decompositions and lossless, dependency-preserving, T3NF decompositions.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2029435358",
    "type": "article"
  },
  {
    "title": "Indefinite and maybe information in relational databases",
    "doi": "https://doi.org/10.1145/77643.77644",
    "publication_date": "1990-03-01",
    "publication_year": 1990,
    "authors": "Ken‐Chih Liu; Rajshekhar Sunderraman",
    "corresponding_authors": "",
    "abstract": "This paper extends the relational model to represent indefinite and maybe kinds of incomplete information. A data structure, called an I-table, which is capable of representing indefinite and maybe facts, is introduced. The information content of I-tables is precisely defined, and an operator to remove redundant facts is presented. The relational algebra is then extended in a semantically correct way to operate on I-tables. Queries are posed in the same way as in conventional relational algebra; however, the user may now expect indefinite as well as maybe answers.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2056147406",
    "type": "article"
  },
  {
    "title": "Searching in metric spaces with user-defined and approximate distances",
    "doi": "https://doi.org/10.1145/582410.582412",
    "publication_date": "2002-12-01",
    "publication_year": 2002,
    "authors": "Paolo Ciaccia; Marco Patella",
    "corresponding_authors": "",
    "abstract": "Novel database applications, such as multimedia, data mining, e-commerce, and many others, make intensive use of similarity queries in order to retrieve the objects that better fit a user request. Since the effectiveness of such queries improves when the user is allowed to personalize the similarity criterion according to which database objects are evaluated and ranked, the development of access methods able to efficiently support user-defined similarity queries becomes a basic requirement. In this article we introduce the first index structure, called the QIC-M-tree, that can process user-defined queries in generic metric spaces, that is, where the only information about indexed objects is their relative distances. The QIC-M-tree is a metric access method that can deal with several distinct distances at a time: (1) a query (user-defined) distance , (2) an index distance (used to build the tree), and (3) a comparison (approximate) distance (used to quickly discard from the search uninteresting parts of the tree). We develop an analytical cost model that accurately characterizes the performance of the QIC-M-tree and validate such model through extensive experimentation on real metric data sets. In particular, our analysis is able to predict the best evaluation strategy (i.e., which distances to use) under a variety of configurations, by properly taking into account relevant factors such as the distribution of distances, the cost of computing distances, and the actual index structure. We also prove that the overall saving in CPU search costs when using an approximate distance can be estimated by using information on the data set only (thus such measure is independent of the underlying access method) and show that performance results are closely related to a novel \"indexing\" error measure.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2110254452",
    "type": "article"
  },
  {
    "title": "The role of domain ontologies in database design",
    "doi": "https://doi.org/10.1145/1166074.1166083",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Vijayan Sugumaran; Veda C. Storey",
    "corresponding_authors": "",
    "abstract": "Database design is difficult because it involves a database designer understanding an application and translating the design requirements into a conceptual model. However, the designer may have little or no knowledge about the application or task for which the database is being designed. This research presents a methodology for supporting database design creation and evaluation that makes use of domain-specific knowledge about an application stored in the form of domain ontologies. The methodology is implemented in a prototype system, the Ontology Management and Database Design Environment. Initial testing of the prototype illustrates that the incorporation and use of ontologies is effective in creating entity-relationship models.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W1993539737",
    "type": "article"
  },
  {
    "title": "The generalized tree quorum protocol",
    "doi": "https://doi.org/10.1145/146931.146935",
    "publication_date": "1992-12-01",
    "publication_year": 1992,
    "authors": "Divyakant Agrawal; Amr El Abbadi",
    "corresponding_authors": "",
    "abstract": "In this paper, we present a low-cost fault-tolerant protocol for managing replicated data. We impose a logical tree structure on the set of copies of an object and develop a protocol that uses the information available in the logical structure to reduce the communication requirements for read and write operations. The tree quorum protocol is a generalization of the static voting protocol with two degrees of freedom for choosing quorums. In general, this results in significantly lower communication costs for comparable data availability. The protocol exhibits the property of graceful degradation, i.e., communication costs for executing operations are minimal in a failure-free environment but may increase as failures occur. This approach in designing distributed systems is desirable since it provides fault-tolerance without imposing unnecessary costs on the failure-free mode of operations.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2006101861",
    "type": "article"
  },
  {
    "title": "Automatic complex schema matching across Web query interfaces",
    "doi": "https://doi.org/10.1145/1132863.1132872",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Bin He; Kevin Chen–Chuan Chang",
    "corresponding_authors": "",
    "abstract": "To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources. While complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this article takes a conceptually novel approach by viewing schema matching as correlation mining , for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this “deep Web ” query interfaces generally form complex matchings between attribute groups (e.g., {author} corresponds to {first name, last name} in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., {first name, last name}) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach. In particular, we develop the DCM framework, which consists of data preprocessing , dual mining of positive and negative correlations, and finally matching construction . We evaluate the DCM framework on manually extracted interfaces and the results show good accuracy for discovering complex matchings. Further, to automate the entire matching process, we incorporate automatic techniques for interface extraction. Executing the DCM framework on automatically extracted interfaces, we find that the inevitable errors in automatic interface extraction may significantly affect the matching result. To make the DCM framework robust against such “noisy” schemas, we integrate it with a novel “ensemble” approach, which creates an ensemble of DCM matchers, by randomizing the schema data into many trials and aggregating their ranked results by taking majority voting. As a principled basis, we provide analytic justification of the robustness of the ensemble approach. Empirically, our experiments show that the “ensemblization” indeed significantly boosts the matching accuracy, over automatically extracted and thus noisy schema data. By employing the DCM framework with the ensemble approach, we thus complete an automatic process of matchings Web query interfaces.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2118966476",
    "type": "article"
  },
  {
    "title": "Multikey access methods based on superimposed coding techniques",
    "doi": "https://doi.org/10.1145/32204.32222",
    "publication_date": "1987-11-01",
    "publication_year": 1987,
    "authors": "Ron Sacks‐Davis; Alan J. Kent; Kotagiri Ramamohanarao",
    "corresponding_authors": "",
    "abstract": "Both single-level and two-level indexed descriptor schemes for multikey retrieval are presented and compared. The descriptors are formed using superimposed coding techniques and stored using a bit-inversion technique. A fast-batch insertion algorithm for which the cost of forming the bit-inverted file is less than one disk access per record is presented. For large data files, it is shown that the two-level implementation is generally more efficient for queries with a small number of matching records. For queries that specify two or more values, there is a potential problem with the two-level implementation in that costs may accrue when blocks of records match the query but individual records within these blocks do not. One approach to overcoming this problem is to set bits in the descriptors based on pairs of indexed terms. This approach is presented and analyzed.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2004620064",
    "type": "article"
  },
  {
    "title": "On taxonomic reasoning in conceptual design",
    "doi": "https://doi.org/10.1145/132271.132272",
    "publication_date": "1992-09-01",
    "publication_year": 1992,
    "authors": "Sonia Bergamaschi; Claudio Sartori",
    "corresponding_authors": "",
    "abstract": "Taxonomic reasoning is a typical task performed by many AI knowledge representation systems. In this paper, the effectiveness of taxonomic reasoning techniques as an active support to knowledge acquisition and conceptual schema design is shown. The idea developed is that by extending conceptual models with defined concepts and giving them rigorous logic semantics, it is possible to infer isa relationships between concepts on the basis of their descriptions. From a theoretical point of view, this approach makes it possible to give a formal definition for consistency and minimality of a conceptual schema. From a pragmatic point of view it is possible to develop an active environment that allows automatic classification of a new concept in the right position of a given taxonomy, ensuring the consistency and minimality of a conceptual schema. A formalism that includes the data semantics of models giving prominence to type constructors (E/R, TAXIS, GALILEO) and algorithms for taxonomic inferences are presented: their soundness, completeness, and tractability properties are proved. Finally, an extended formalism and taxonomic inference algorithms for models giving prominence to attributes (FDM, IFO) are given.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2136027622",
    "type": "article"
  },
  {
    "title": "Representing and querying XML with incomplete information",
    "doi": "https://doi.org/10.1145/1132863.1132869",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Serge Abiteboul; Luc Segoufin; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "We study the representation and querying of XML with incomplete information. We consider a simple model for XML data and their DTDs, a very simple query language, and a representation system for incomplete information in the spirit of the representations systems developed by Imielinski and Lipski [1984] for relational databases. In the scenario we consider, the incomplete information about an XML document is continuously enriched by successive queries to the document. We show that our representation system can represent partial information about the source document acquired by successive queries, and that it can be used to intelligently answer new queries. We also consider the impact on complexity of enriching our representation system or query language with additional features. The results suggest that our approach achieves a practically appealing balance between expressiveness and tractability.",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W1969340050",
    "type": "article"
  },
  {
    "title": "Representing extended entity-relationship structures in relational databases",
    "doi": "https://doi.org/10.1145/132271.132273",
    "publication_date": "1992-09-01",
    "publication_year": 1992,
    "authors": "Victor Markowitz; Arie Shoshani",
    "corresponding_authors": "",
    "abstract": "A common approach to database design is to describe the structures and constraints of the database application in terms of a semantic data model, and then represent the resulting schema using the data model of a commercial database management system. Often, in practice, Extended Entity-Relationship (EER) schemas are translated into equivalent relational schemas. This translation involves different aspects: representing the EER schema using relational constructs, assigning names to relational attributes, normalization, and merging relations. Considering these aspects together, as is usually done in the design methodologies proposed in the literature, is confusing and leads to inaccurate results. We propose to treat separately these aspects and split the translation into four stages (modules) corresponding to the four aspects mentioned above. We define criteria for both evaluating the correctness of and characterizing the relationship between alternative relational representations of EER schemas.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W1967291753",
    "type": "article"
  },
  {
    "title": "Graph indexing based on discriminative frequent structure analysis",
    "doi": "https://doi.org/10.1145/1114244.1114248",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Xifeng Yan; Philip S. Yu; Jiawei Han",
    "corresponding_authors": "",
    "abstract": "Graphs have become increasingly important in modelling complicated structures and schemaless data such as chemical compounds, proteins, and XML documents. Given a graph query , it is desirable to retrieve graphs quickly from a large database via indices. In this article, we investigate the issues of indexing graphs and propose a novel indexing model based on discriminative frequent structures that are identified through a graph mining process. We show that the compact index built under this model can achieve better performance in processing graph queries. Since discriminative frequent structures capture the intrinsic characteristics of the data, they are relatively stable to database updates, thus facilitating sampling-based feature extraction and incremental index maintenance. Our approach not only provides an elegant solution to the graph indexing problem, but also demonstrates how database indexing and query processing can benefit from data mining, especially frequent pattern mining. Furthermore, the concepts developed here can be generalized and applied to indexing sequences, trees, and other complicated structures as well.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2100080493",
    "type": "article"
  },
  {
    "title": "Direct transitive closure algorithms: design and performance evaluation",
    "doi": "https://doi.org/10.1145/88636.88888",
    "publication_date": "1990-09-01",
    "publication_year": 1990,
    "authors": "Rakesh Agrawal; Shaul Dar; H. V. Jagadish",
    "corresponding_authors": "",
    "abstract": "We present new algorithms for computing transitive closure of large database relations. Unlike iterative algorithms, such as the seminaive and logarithmic algorithms, the termination of our algorithms does not depend on the length of paths in the underlying graph (hence the name direct algorithms). Besides reachability computations, the proposed algorithms can also be used for solving path problems. We discuss issues related to the efficient implementation of these algorithms, and present experimental results that show the direct algorithms perform uniformly better than the iterative algorithms. A side benefit of this work is that we have proposed a new methodology for evaluating the performance of recursive queries.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W1993504147",
    "type": "article"
  },
  {
    "title": "A geometric approach to monitoring threshold functions over distributed data streams",
    "doi": "https://doi.org/10.1145/1292609.1292613",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Izchak Sharfman; Assaf Schuster; Daniel Keren",
    "corresponding_authors": "",
    "abstract": "Monitoring data streams in a distributed system is the focus of much research in recent years. Most of the proposed schemes, however, deal with monitoring simple aggregated values, such as the frequency of appearance of items in the streams. More involved challenges, such as the important task of feature selection (e.g., by monitoring the information gain of various features), still require very high communication overhead using naive, centralized algorithms. We present a novel geometric approach which reduces monitoring the value of a function (vis-à-vis a threshold) to a set of constraints applied locally on each of the streams. The constraints are used to locally filter out data increments that do not affect the monitoring outcome, thus avoiding unnecessary communication. As a result, our approach enables monitoring of arbitrary threshold functions over distributed data streams in an efficient manner. We present experimental results on real-world data which demonstrate that our algorithms are highly scalable, and considerably reduce communication load in comparison to centralized algorithms.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2046089875",
    "type": "article"
  },
  {
    "title": "Out-of-core coherent closed quasi-clique mining from large dense graph databases",
    "doi": "https://doi.org/10.1145/1242524.1242530",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Zhiping Zeng; Jianyong Wang; Lizhu Zhou; George Karypis",
    "corresponding_authors": "",
    "abstract": "Due to the ability of graphs to represent more generic and more complicated relationships among different objects, graph mining has played a significant role in data mining, attracting increasing attention in the data mining community. In addition, frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database, and mining frequently occurring coherent subgraphs from large dense graph databases has witnessed several applications and received considerable attention in the graph mining community recently. In this article, we study how to efficiently mine the complete set of coherent closed quasi-cliques from large dense graph databases, which is an especially challenging task due to the fact that the downward-closure property no longer holds. By fully exploring some properties of quasi-cliques, we propose several novel optimization techniques which can prune the unpromising and redundant subsearch spaces effectively. Meanwhile, we devise an efficient closure checking scheme to facilitate the discovery of closed quasi-cliques only. Since large databases cannot be held in main memory, we also design an out-of-core solution with efficient index structures for mining coherent closed quasi-cliques from large dense graph databases. We call this Cocain*. Thorough performance study shows that Cocain* is very efficient and scalable for large dense graph databases.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W2119995149",
    "type": "article"
  },
  {
    "title": "Consequences of assuming a universal relation",
    "doi": "https://doi.org/10.1145/319628.319630",
    "publication_date": "1981-12-01",
    "publication_year": 1981,
    "authors": "William Kent",
    "corresponding_authors": "William Kent",
    "abstract": "Although central to the current direction of dependency theory, the assumption of a universal relation is incompatible with some aspects of relational database theory and practice. Furthermore, the universal relation is itself ill defined in some important ways. And, under the universal relation assumption, the decomposition approach to database design becomes virtually indistinguishable from the synthetic approach.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W1984149034",
    "type": "article"
  },
  {
    "title": "Capturing summarizability with integrity constraints in OLAP",
    "doi": "https://doi.org/10.1145/1093382.1093388",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Carlos Hurtado; Claudio Gutiérrez; Alberto O. Mendelzon",
    "corresponding_authors": "",
    "abstract": "In multidimensional data models intended for online analytic processing (OLAP), data are viewed as points in a multidimensional space. Each dimension has structure, described by a directed graph of categories, a set of members for each category, and a child/parent relation between members. An important application of this structure is to use it to infer summarizability, that is, whether an aggregate view defined for some category can be correctly derived from a set of precomputed views defined for other categories. A dimension is called structurally heterogeneous if two members in a given category are allowed to have ancestors in different categories. In this article, we propose a class of integrity constraints, dimension constraints , that allow us to reason about summarizability in heterogeneous dimensions. We introduce the notion of frozen dimensions which are minimal homogeneous dimension instances representing the different structures that are implicitly combined in a heterogeneous dimension. Frozen dimensions provide the basis for efficiently testing the implication of dimension constraints and are a useful aid to understanding heterogeneous dimensions. We give a sound and complete algorithm for solving the implication of dimension constraints that uses heuristics based on the structure of the dimension and the constraints to speed up its execution. We study the intrinsic complexity of the implication problem and the running time of our algorithm.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2006315879",
    "type": "article"
  },
  {
    "title": "Support for repetitive transactions and ad hoc queries in System R",
    "doi": "https://doi.org/10.1145/319540.319550",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "Donald D. Chamberlin; M. M. Astrahan; William F. King; Raymond A. Lorie; J. W. Mehl; Tom Price; Mario Schkolnick; Patricia G. Selinger; Donald R. Slutz; Bradford W. Wade; Robert A. Yost",
    "corresponding_authors": "",
    "abstract": "System R supports a high-level relational user language called SQL which may be used by ad hoc users at terminals or as an embedded data sublanguage in PL/I or COBOL. Host-language programs with embedded SQL statements are processed by the System R precompiler which replaces the SQL statements by calls to a machine-language access module. The precompilation approach removes much of the work of parsing, name binding, and access path selection from the path of a running program, enabling highly efficient support for repetitive transactions. Ad hoc queries are processed by a similar approach of name binding and access path selection which takes place on-line when the query is specified. By providing a flexible spectrum of binding times, System R permits transaction-oriented programs and ad hoc query users to share a database without loss of efficiency. System R is an experimental database management system designed and built by members of the IBM San Jose Research Laboratory as part of a research program on the relational model of data. This paper describes the architecture of System R, and gives some preliminary measurements of system performance in both the ad hoc query and the “canned program” environments.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2025952212",
    "type": "article"
  },
  {
    "title": "A statistical approach to incomplete information in database systems",
    "doi": "https://doi.org/10.1145/319732.319747",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Eugene Wong",
    "corresponding_authors": "Eugene Wong",
    "abstract": "There are numerous situations in which a database cannot provide a precise answer to some of the questions that are posed. Sources of imprecision vary and include examples such as recording errors, incompatible scaling, and obsolete data. In many such situations, considerable prior information concerning the imprecision exists and can be exploited to provide valuable information for queries to which no exact answer can be given. The objective of this paper is to provide a framework for doing so.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2103343882",
    "type": "article"
  },
  {
    "title": "A rule-based language with functions and sets",
    "doi": "https://doi.org/10.1145/103140.103141",
    "publication_date": "1991-03-01",
    "publication_year": 1991,
    "authors": "Serge Abiteboul; Stéphane Grumbach",
    "corresponding_authors": "",
    "abstract": "A logic based language for manipulating complex objects constructed using set and tuple constructors is introduced. A key feature of the COL language is the use of base and derived data functions. Under some stratification restrictions, the semantics of programs is given by a minimal and justified model that can be computed using a finite sequence of fixpoints. The language is extended using external functions and predicates. An implementation of COL in a functional language is briefly discussed.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2025594092",
    "type": "article"
  },
  {
    "title": "Integrated concurrency control and recovery mechanisms: design and performance evaluation",
    "doi": "https://doi.org/10.1145/4879.4958",
    "publication_date": "1985-12-01",
    "publication_year": 1985,
    "authors": "Rakesh Agrawal; David J. DeWitt",
    "corresponding_authors": "",
    "abstract": "In spite of the wide variety of concurrency control and recovery mechanisms proposed during the past decade, the behavior and the performance of various concurrency control and recovery mechanisms remain largely not well understood. In addition, although concurrency control and recovery mechanisms are intimately related, the interaction between them has not been adequately explored. In this paper, we take a unified view of the problems associated with concurrency control and recovery for transaction-oriented multiuser centralized database management systems, and we present several integrated mechanisms. We then develop analytical models to study the behavior and compare the performance of these integrated mechanisms, and we present the results of our performance evaluation.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2093508352",
    "type": "article"
  },
  {
    "title": "The difficulty of optimum index selection",
    "doi": "https://doi.org/10.1145/320289.320296",
    "publication_date": "1978-12-01",
    "publication_year": 1978,
    "authors": "Douglas E. Comer",
    "corresponding_authors": "Douglas E. Comer",
    "abstract": "Given a file on a secondary store in which each record has several attributes, it is usually advantageous to build an index mechanism to decrease the cost of conducting transactions to the file. The problem of selecting attributes over which to index has been studied in the context of various storage structures and access assumptions. One algorithm to make an optimum index selection requires 2 k steps in the worst case, where k is the number of attributes in the file. We examine the question of whether a more efficient algorithm might exist and show that even under a simple cost criterion the problem is computationally difficult in a precise sense. Our results extend directly to other related problems where the cost of the index depends on fixed values which are assigned to each attribute. Some practical implications are discussed.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2058765797",
    "type": "article"
  },
  {
    "title": "The design of a relational database system with abstract data types for domains",
    "doi": "https://doi.org/10.1145/6314.6461",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Sylvia L. Osborn; T. E. Heaven",
    "corresponding_authors": "",
    "abstract": "An extension to the relational model is described in which domains can he arbitrarily defined as abstract data types. Operations on these data types include primitive operations, aggregates, and transformations. It is shown that these operations make the query language complete in the sense of Chandra and Harel. The system has been designed in such a way that new data types and their operations can be defined with a minimal amount of interaction with the database management system.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2012534139",
    "type": "article"
  },
  {
    "title": "Detecting outlying properties of exceptional objects",
    "doi": "https://doi.org/10.1145/1508857.1508864",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Fabrizio Angiulli; Fabio Fassetti; Luigi Palopoli",
    "corresponding_authors": "",
    "abstract": "Assume you are given a data population characterized by a certain number of attributes. Assume, moreover, you are provided with the information that one of the individuals in this data population is abnormal, but no reason whatsoever is given to you as to why this particular individual is to be considered abnormal. In several cases, you will be indeed interested in discovering such reasons. This article is precisely concerned with this problem of discovering sets of attributes that account for the (a priori stated) abnormality of an individual within a given dataset. A criterion is presented to measure the abnormality of combinations of attribute values featured by the given abnormal individual with respect to the reference population. In this respect, each subset of attributes is intended to somehow represent a “property” of individuals. We distinguish between global and local properties. Global properties are subsets of attributes explaining the given abnormality with respect to the entire data population. With local ones, instead, two subsets of attributes are singled out, where the former one justifies the abnormality within the data subpopulation selected using the values taken by the exceptional individual on those attributes included in the latter one. The problem of individuating abnormal properties with associated explanations is formally stated and analyzed. Such a formal characterization is then exploited in order to devise efficient algorithms for detecting both global and local forms of most abnormal properties. The experimental evidence, which is accounted for in the article, shows that the algorithms are both able to mine meaningful information and to accomplish the computational task by examining a negligible fraction of the search space.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W1991758224",
    "type": "article"
  },
  {
    "title": "Probabilistic top- <i>k</i> and ranking-aggregate queries",
    "doi": "https://doi.org/10.1145/1386118.1386119",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Mohamed A. Soliman; Ihab F. Ilyas; Kevin Chen–Chuan Chang",
    "corresponding_authors": "",
    "abstract": "Ranking and aggregation queries are widely used in data exploration, data analysis, and decision-making scenarios. While most of the currently proposed ranking and aggregation techniques focus on deterministic data, several emerging applications involve data that is unclean or uncertain. Ranking and aggregating uncertain (probabilistic) data raises new challenges in query semantics and processing, making conventional methods inapplicable. Furthermore, uncertainty imposes probability as a new ranking dimension that does not exist in the traditional settings. In this article we introduce new probabilistic formulations for top- k and ranking-aggregate queries in probabilistic databases. Our formulations are based on marriage of traditional top- k semantics with possible worlds semantics. In the light of these formulations, we construct a generic processing framework supporting both query types, and leveraging existing query processing and indexing capabilities in current RDBMSs. The framework encapsulates a state space model and efficient search algorithms to compute query answers. Our proposed techniques minimize the number of accessed tuples and the size of materialized search space to compute query answers. Our experimental study shows the efficiency of our techniques under different data distributions with orders of magnitude improvement over naïve methods.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2121433213",
    "type": "article"
  },
  {
    "title": "ODE",
    "doi": "https://doi.org/10.1145/1538909.1538914",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Weifeng Su; Jiying Wang; Frederick H. Lochovsky",
    "corresponding_authors": "",
    "abstract": "Online databases respond to a user query with result records encoded in HTML files. Data extraction, which is important for many applications, extracts the records from the HTML files automatically. We present a novel data extraction method, ODE (Ontology-assisted Data Extraction), which automatically extracts the query result records from the HTML pages. ODE first constructs an ontology for a domain according to information matching between the query interfaces and query result pages from different Web sites within the same domain. Then, the constructed domain ontology is used during data extraction to identify the query result section in a query result page and to align and label the data values in the extracted records. The ontology-assisted data extraction method is fully automatic and overcomes many of the deficiencies of current automatic data extraction methods. Experimental results show that ODE is extremely accurate for identifying the query result section in an HTML page, segmenting the query result section into query result records, and aligning and labeling the data values in the query result records.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2035302703",
    "type": "article"
  },
  {
    "title": "Multi-resolution bitmap indexes for scientific data",
    "doi": "https://doi.org/10.1145/1272743.1272746",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Rishi Rakesh Sinha; Marianne Winslett",
    "corresponding_authors": "",
    "abstract": "The unique characteristics of scientific data and queries cause traditional indexing techniques to perform poorly on scientific workloads, occupy excessive space, or both. Refinements of bitmap indexes have been proposed previously as a solution to this problem. In this article, we describe the difficulties we encountered in deploying bitmap indexes with scientific data and queries from two real-world domains. In particular, previously proposed methods of binning, encoding, and compressing bitmap vectors either were quite slow for processing the large-range query conditions our scientists used, or required excessive storage space. Nor could the indexes easily be built or used on parallel platforms. In this article, we show how to solve these problems through the use of multi-resolution, parallelizable bitmap indexes, which support a fine-grained trade-off between storage requirements and query performance. Our experiments with large data sets from two scientific domains show that multi-resolution, parallelizable bitmap indexes occupy an acceptable amount of storage while improving range query performance by roughly a factor of 10, compared to a single-resolution bitmap index of reasonable size.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W1971065220",
    "type": "article"
  },
  {
    "title": "Statistical database design",
    "doi": "https://doi.org/10.1145/319540.319558",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "Francis Y. L. Chin; Gültekin Özsoyoğlu",
    "corresponding_authors": "",
    "abstract": "The security problem of a statistical database is to limit the use of the database so that no sequence of statistical queries is sufficient to deduce confidential or private information. In this paper it is suggested that the problem be investigated at the conceptual data model level. The design of a statistical database should utilize a statistical security management facility to enforce the security constraints at the conceptual model level. Information revealed to users is well defined in the sense that it can at most be reduced to nondecomposable information involving a group of individuals. In addition, the design also takes into consideration means of storing the query information for auditing purposes, changes in the database, users' knowledge, and some security measures.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2055725320",
    "type": "article"
  },
  {
    "title": "Scalable approximate query processing with the DBO engine",
    "doi": "https://doi.org/10.1145/1412331.1412335",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Chris Jermaine; Subramanian Arumugam; Abhijit Pol; Alin Dobra",
    "corresponding_authors": "",
    "abstract": "This article describes query processing in the DBO database system. Like other database systems designed for ad hoc analytic processing, DBO is able to compute the exact answers to queries over a large relational database in a scalable fashion. Unlike any other system designed for analytic processing, DBO can constantly maintain a guess as to the final answer to an aggregate query throughout execution, along with statistically meaningful bounds for the guess's accuracy. As DBO gathers more and more information, the guess gets more and more accurate, until it is 100% accurate as the query is completed. This allows users to stop the execution as soon as they are happy with the query accuracy, and thus encourages exploratory data analysis.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2063546264",
    "type": "article"
  },
  {
    "title": "A framework for efficient data anonymization under privacy and accuracy constraints",
    "doi": "https://doi.org/10.1145/1538909.1538911",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Gabriel Ghinita; Panagiotis Karras; Panos Kalnis; Nikos Mamoulis",
    "corresponding_authors": "",
    "abstract": "Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy-preserving paradigms of k -anonymity and l -diversity. k -anonymity protects against the identification of an individual's record. l -diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) l -diversification is solved by techniques developed for the simpler k -anonymization problem, causing unnecessary information loss. (ii) The anonymization process is inefficient in terms of computational and I/O cost. (iii) Previous research focused exclusively on the privacy-constrained problem and ignored the equally important accuracy-constrained (or dual) anonymization problem. In this article, we propose a framework for efficient anonymization of microdata that addresses these deficiencies. First, we focus on one-dimensional (i.e., single-attribute) quasi-identifiers, and study the properties of optimal solutions under the k -anonymity and l -diversity models for the privacy-constrained (i.e., direct) and the accuracy-constrained (i.e., dual) anonymization problems. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multidimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the existing approaches in terms of execution time and information loss.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2073216087",
    "type": "article"
  },
  {
    "title": "Unified framework for fast exact and approximate search in dissimilarity spaces",
    "doi": "https://doi.org/10.1145/1292609.1292619",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Tomáš Skopal",
    "corresponding_authors": "Tomáš Skopal",
    "abstract": "In multimedia systems we usually need to retrieve database (DB) objects based on their similarity to a query object, while the similarity assessment is provided by a measure which defines a (dis)similarity score for every pair of DB objects. In most existing applications, the similarity measure is required to be a metric, where the triangle inequality is utilized to speed up the search for relevant objects by use of metric access methods (MAMs), for example, the M-tree. A recent research has shown, however, that nonmetric measures are more appropriate for similarity modeling due to their robustness and ease to model a made-to-measure similarity. Unfortunately, due to the lack of triangle inequality, the nonmetric measures cannot be directly utilized by MAMs. From another point of view, some sophisticated similarity measures could be available in a black-box nonanalytic form (e.g., as an algorithm or even a hardware device), where no information about their topological properties is provided, so we have to consider them as nonmetric measures as well. From yet another point of view, the concept of similarity measuring itself is inherently imprecise and we often prefer fast but approximate retrieval over an exact but slower one. To date, the mentioned aspects of similarity retrieval have been solved separately, that is, exact versus approximate search or metric versus nonmetric search. In this article we introduce a similarity retrieval framework which incorporates both of the aspects into a single unified model. Based on the framework, we show that for any dissimilarity measure (either a metric or nonmetric) we are able to change the “amount” of triangle inequality, and so obtain an approximate or full metric which can be used for MAM-based retrieval. Due to the varying “amount” of triangle inequality, the measure is modified in a way suitable for either an exact but slower or an approximate but faster retrieval. Additionally, we introduce the TriGen algorithm aimed at constructing the desired modification of any black-box distance automatically, using just a small fraction of the database.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2108053094",
    "type": "article"
  },
  {
    "title": "A fast procedure for finding a tracker in a statistical database",
    "doi": "https://doi.org/10.1145/320128.320138",
    "publication_date": "1980-03-01",
    "publication_year": 1980,
    "authors": "Dorothy E. Denning; Jan Schlörer",
    "corresponding_authors": "",
    "abstract": "To avoid trivial compromises, most on-line statistical databases refuse to answer queries for statistics about small subgroups. Previous research discovered a powerful snooping tool, the tracker, with which the answers to these unanswerable queries are easily calculated. However, the extent of this threat was not clear, for no one had shown that finding a tracker is guaranteed to be easy. This paper gives a simple algorithm for finding a tracker when the maximum number of identical records is not too large. The number of queries required to find a tracker is at most Ο (log 2 S ) queries, where S is the number of distinct records possible. Experimental results show that the procedure often finds a tracker with just a few queries. The threat posed by trackers is therefore considerable.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2042908342",
    "type": "article"
  },
  {
    "title": "Quintary trees",
    "doi": "https://doi.org/10.1145/320613.320618",
    "publication_date": "1980-09-01",
    "publication_year": 1980,
    "authors": "D. T. Lee; C. K. Wong",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Quintary trees: a file structure for multidimensional datbase sytems Authors: D. T. Lee Northwestern Univ., Evanston, IL Northwestern Univ., Evanston, ILView Profile , C. K. Wong IBM Thomas J. Watson Research Center, Yorktown Heights, NY IBM Thomas J. Watson Research Center, Yorktown Heights, NYView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 5Issue 3Sept. 1980 pp 339–353https://doi.org/10.1145/320613.320618Published:01 September 1980Publication History 64citation559DownloadsMetricsTotal Citations64Total Downloads559Last 12 Months29Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2085603093",
    "type": "article"
  },
  {
    "title": "A model of statistical database their security",
    "doi": "https://doi.org/10.1145/320521.320525",
    "publication_date": "1977-03-01",
    "publication_year": 1977,
    "authors": "John B. Kam; Jeffrey D. Ullman",
    "corresponding_authors": "",
    "abstract": "Considered here, for a particular model of databases in which only information about relatively large sets of records can be obtained, is the question of whether one can from statistical information obtain information about individuals. Under the assumption that the data in the database is taken from arbitrary integers, it is shown that essentially nothing can be inferred. It is also shown that when the values are known to be imprecise in some fixed range, one can often deduce the values of individual records.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2002653658",
    "type": "article"
  },
  {
    "title": "The recovery of a schema mapping",
    "doi": "https://doi.org/10.1145/1620585.1620589",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Marcelo Arenas; Jorge Pérez; Cristian Riveros",
    "corresponding_authors": "",
    "abstract": "A schema mapping is a specification that describes how data from a source schema is to be mapped to a target schema. Once the data has been transferred from the source to the target, a natural question is whether one can undo the process and recover the initial data, or at least part of it. In fact, it would be desirable to find a reverse schema mapping from target to source that specifies how to bring the exchanged data back. In this article, we introduce the notion of a recovery of a schema mapping: it is a reverse mapping, M′ for a mapping M, that recovers sound data with respect to M. We further introduce an order relation on recoveries. This allows us to choose mappings that recover the maximum amount of sound information. We call such mappings maximum recoveries. We study maximum recoveries in detail, providing a necessary and sufficient condition for their existence. In particular, we prove that maximum recoveries exist for the class of mappings specified by FO-to-CQ source-to-target dependencies. This class subsumes the class of source-to-target tuple-generating dependencies used in previous work on data exchange. For the class of mappings specified by FO-to-CQ dependencies, we provide an exponential-time algorithm for computing maximum recoveries, and a simplified version for full dependencies that works in quadratic time. We also characterize the language needed to express maximum recoveries, and we include a detailed comparison with the notion of inverse (and quasi-inverse) mapping previously proposed in the data exchange literature. In particular, we show that maximum recoveries strictly generalize inverses. We finally study the complexity of some decision problems related to the notions of recovery and maximum recovery.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W1965776893",
    "type": "article"
  },
  {
    "title": "Calculating constraints on relational expression",
    "doi": "https://doi.org/10.1145/320613.320615",
    "publication_date": "1980-09-01",
    "publication_year": 1980,
    "authors": "Anthony C. Klug",
    "corresponding_authors": "Anthony C. Klug",
    "abstract": "This paper deals with the problem of determining which of a certain class of constraints hold on a given relational algebra expression where the base relations come from a given schema. The class of constraints includes functional dependencies, equality of domains, and constancy of domains. The relational algebra consists of projection, selection, restriction, cross product, union, and difference. The problem as given is undecidable, but if set difference is removed from the algebra, there is a solution. Operators specifying a closure function (similar to functional dependency closure on one relation) are defined; these will generate exactly the set of constraints valid on the given relational algebra expression. We prove that the operators are sound and complete.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2007964756",
    "type": "article"
  },
  {
    "title": "Artifact systems with data dependencies and arithmetic",
    "doi": "https://doi.org/10.1145/2338626.2338628",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Elio Damaggio; Alin Deutsch; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "We study the static verification problem for data-centric business processes, specified in a variant of IBM's “business artifact” model. Artifacts are records of variables that correspond to business-relevant objects and are updated by a set of services equipped with pre- and postconditions, that implement business process tasks. The verification problem consists in statically checking whether all runs of an artifact system satisfy desirable properties expressed in a first-order extension of linear-time temporal logic. Previous work identified the class of guarded artifact systems and properties, for which verification is decidable. However, the results suffer an important limitation: they fail in the presence of even very simple data dependencies or arithmetic, both crucial to real-life business processes. In this article, we extend the artifact model and verification results to alleviate this limitation. We identify a practically significant class of business artifacts with data dependencies and arithmetic, for which verification is decidable. The technical machinery needed to establish the results is fundamentally different from previous work. While the worst-case complexity of verification is nonelementary, we identify various realistic restrictions yielding more palatable upper bounds.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2075536383",
    "type": "article"
  },
  {
    "title": "Concepts and capabilities of a database computer\\",
    "doi": "https://doi.org/10.1145/320289.320292",
    "publication_date": "1978-12-01",
    "publication_year": 1978,
    "authors": "Jayanta Banerjee; David K. Hsiao; Richard I. Baum",
    "corresponding_authors": "",
    "abstract": "The concepts and capabilities of a database computer (DBC) are given in this paper. The proposed design overcomes many of the traditional problems of database system software and is one of the first to describe a complete data-secure computer capable of handling large databases. This paper begins by characterizing the major problems facing today's database system designers. These problems are intrinsically related to the nature of conventional hardware and can only be solved by introducing new architectural concepts. Several such concepts are brought to bear in the later sections of this paper. These architectural principles have a major impact upon the design of the system and so they are discussed in some detail. A key aspect of these principles is that they can be implemented with near-term technology. The rest of the paper is devoted to the functional characteristics and the theory of operation of the DBC. The theory of operation is based on a series of abstract models of the components and data structures employed by the DBC. These models are used to illustrate how the DBC performs access operations, manages data structures and security specifications, and enforces security requirements. Short Algol-like algorithms are used to show how these operations are carried out. This part of the paper concludes with a high-level description of the DBC organization. The actual details of the DBC hardware are quite involved and so their presentation is not the subject of this paper. A sample database is included in the Appendix to illustrate the working of the security and clustering mechanisms of the DBC.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W1983959468",
    "type": "article"
  },
  {
    "title": "Embedding-based subsequence matching in time-series databases",
    "doi": "https://doi.org/10.1145/2000824.2000827",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Panagiotis Papapetrou; Vassilis Athitsos; Michalis Potamias; George Kollios; Dimitrios Gunopulos",
    "corresponding_authors": "",
    "abstract": "We propose an embedding-based framework for subsequence matching in time-series databases that improves the efficiency of processing subsequence matching queries under the Dynamic Time Warping (DTW) distance measure. This framework partially reduces subsequence matching to vector matching, using an embedding that maps each query sequence to a vector and each database time series into a sequence of vectors. The database embedding is computed offline, as a preprocessing step. At runtime, given a query object, an embedding of that object is computed online. Relatively few areas of interest are efficiently identified in the database sequences by comparing the embedding of the query with the database vectors. Those areas of interest are then fully explored using the exact DTW-based subsequence matching algorithm. We apply the proposed framework to define two specific methods. The first method focuses on time-series subsequence matching under unconstrained Dynamic Time Warping. The second method targets subsequence matching under constrained Dynamic Time Warping (cDTW), where warping paths are not allowed to stray too much off the diagonal. In our experiments, good trade-offs between retrieval accuracy and retrieval efficiency are obtained for both methods, and the results are competitive with respect to current state-of-the-art methods.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2049120089",
    "type": "article"
  },
  {
    "title": "Path-tree",
    "doi": "https://doi.org/10.1145/1929934.1929941",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "Ruoming Jin; Ning Ruan; Yang Xiang; Haixun Wang",
    "corresponding_authors": "",
    "abstract": "Reachability query is one of the fundamental queries in graph database. The main idea behind answering reachability queries is to assign vertices with certain labels such that the reachability between any two vertices can be determined by the labeling information. Though several approaches have been proposed for building these reachability labels, it remains open issues on how to handle increasingly large number of vertices in real-world graphs, and how to find the best tradeoff among the labeling size, the query answering time, and the construction time. In this article, we introduce a novel graph structure, referred to as path-tree , to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We show path-tree can be generalized to chain-tree which theoretically can has smaller labeling cost. On top of path-tree and chain-tree index, we also introduce a new compression scheme which groups vertices with similar labels together to further reduce the labeling size. In addition, we also propose an efficient incremental update algorithm for dynamic index maintenance. Finally, we demonstrate both analytically and empirically the effectiveness and efficiency of our new approaches.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W1968401677",
    "type": "article"
  },
  {
    "title": "Efficient reasoning about a robust XML key fragment",
    "doi": "https://doi.org/10.1145/1538909.1538912",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Sven Hartmann; Sebastian Link",
    "corresponding_authors": "",
    "abstract": "We review key constraints in the context of XML as introduced by Buneman et al. We demonstrate that: (1) one of the proposed inference rules is not sound in general, and (2) the inference rules are incomplete for XML key implication, even for nonempty sets of simple key paths. This shows, in contrast to earlier statements, that the axiomatizability of XML keys is still open, and efficient algorithms for deciding their implication still need to be developed. Solutions to these problems have a wide range of applications including consistency validation, XML schema design, data exchange and integration, consistent query answering, XML query optimization and rewriting, and indexing. In this article, we investigate the axiomatizability and implication problem for XML keys with nonempty sets of simple key paths. In particular, we propose a set of inference rules that is indeed sound and complete for the implication of such XML keys. We demonstrate that this fragment is robust by showing the duality of XML key implication to the reachability problem of fixed nodes in a suitable digraph. This enables us to develop a quadratic-time algorithm for deciding implication, and shows that reasoning about this XML key fragment is practically efficient. Therefore, XML applications can be unlocked effectively since they benefit not only from those XML keys specified explicitly by the data designer but also from those that are specified implicitly.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2043117781",
    "type": "article"
  },
  {
    "title": "A survey of B-tree logging and recovery techniques",
    "doi": "https://doi.org/10.1145/2109196.2109197",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Goetz Graefe",
    "corresponding_authors": "Goetz Graefe",
    "abstract": "B-trees have been ubiquitous in database management systems for several decades, and they serve in many other storage systems as well. Their basic structure and their basic operations are well understood including search, insertion, and deletion. However, implementation of transactional guarantees such as all-or-nothing failure atomicity and durability in spite of media and system failures seems to be difficult. High-performance techniques such as pseudo-deleted records, allocation-only logging, and transaction processing during crash recovery are widely used in commercial B-tree implementations but not widely understood. This survey collects many of these techniques as a reference for students, researchers, system architects, and software developers. Central in this discussion are physical data independence, separation of logical database contents and physical representation, and the concepts of user transactions and system transactions. Many of the techniques discussed are applicable beyond B-trees.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1994073219",
    "type": "article"
  },
  {
    "title": "Determining the Currency of Data",
    "doi": "https://doi.org/10.1145/2389241.2389244",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Wenfei Fan; Floris Geerts; Jef Wijsen",
    "corresponding_authors": "",
    "abstract": "Data in real-life databases become obsolete rapidly. One often finds that multiple values of the same entity reside in a database. While all of these values were once correct, most of them may have become stale and inaccurate. Worse still, the values often do not carry reliable timestamps. With this comes the need for studying data currency, to identify the current value of an entity in a database and to answer queries with the current values, in the absence of reliable timestamps. This article investigates the currency of data. (1) We propose a model that specifies partial currency orders in terms of simple constraints. The model also allows us to express what values are copied from other data sources, bearing currency orders in those sources, in terms of copy functions defined on correlated attributes. (2) We study fundamental problems for data currency, to determine whether a specification is consistent, whether a value is more current than another, and whether a query answer is certain no matter how partial currency orders are completed. (3) Moreover, we identify several problems associated with copy functions, to decide whether a copy function imports sufficient current data to answer a query, whether a copy function can be extended to import necessary current data for a query while respecting the constraints, and whether it suffices to copy data of a bounded size. (4) We establish upper and lower bounds of these problems, all matching, for combined complexity and data complexity, and for a variety of query languages. We also identify special cases that warrant lower complexity.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2092770599",
    "type": "article"
  },
  {
    "title": "Foster b-trees",
    "doi": "https://doi.org/10.1145/2338626.2338630",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Goetz Graefe; Hideaki Kimura; Harumi Kuno",
    "corresponding_authors": "",
    "abstract": "Foster B-trees are a new variant of B-trees that combines advantages of prior B-tree variants optimized for many-core processors and modern memory hierarchies with flash storage and nonvolatile memory. Specific goals include: (i) minimal concurrency control requirements for the data structure, (ii) efficient migration of nodes to new storage locations, and (iii) support for continuous and comprehensive self-testing. Like B link -trees, Foster B-trees optimize latching without imposing restrictions or specific designs on transactional locking, for example, key range locking. Like write-optimized B-trees, and unlike B link -trees, Foster B-trees enable large writes on RAID and flash devices as well as wear leveling and efficient defragmentation. Finally, they support continuous and inexpensive yet comprehensive verification of all invariants, including all cross-node invariants of the B-tree structure. An implementation and a performance evaluation show that the Foster B-tree supports high concurrency and high update rates without compromising consistency, correctness, or read performance.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2091076568",
    "type": "article"
  },
  {
    "title": "Efficient Processing of Spatial Group Keyword Queries",
    "doi": "https://doi.org/10.1145/2772600",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Xin Cao; Gao Cong; Tao Guo; Christian S. Jensen; Beng Chin Ooi",
    "corresponding_authors": "",
    "abstract": "With the proliferation of geo-positioning and geo-tagging techniques, spatio-textual objects that possess both a geographical location and a textual description are gaining in prevalence, and spatial keyword queries that exploit both location and textual description are gaining in prominence. However, the queries studied so far generally focus on finding individual objects that each satisfy a query rather than finding groups of objects where the objects in a group together satisfy a query. We define the problem of retrieving a group of spatio-textual objects such that the group's keywords cover the query's keywords and such that the objects are nearest to the query location and have the smallest inter-object distances. Specifically, we study three instantiations of this problem, all of which are NP-hard. We devise exact solutions as well as approximate solutions with provable approximation bounds to the problems. In addition, we solve the problems of retrieving top- k groups of three instantiations, and study a weighted version of the problem that incorporates object weights. We present empirical studies that offer insight into the efficiency of the solutions, as well as the accuracy of the approximate solutions.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2101903378",
    "type": "article"
  },
  {
    "title": "High-level change detection in RDF(S) KBs",
    "doi": "https://doi.org/10.1145/2445583.2445584",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Vicky Papavasileiou; Giorgos Flouris; Irini Fundulaki; Dimitris Kotzinos; Vassilis Christophides",
    "corresponding_authors": "",
    "abstract": "With the increasing use of Web 2.0 to create, disseminate, and consume large volumes of data, more and more information is published and becomes available for potential data consumers, that is, applications/services, individual users and communities, outside their production site. The most representative example of this trend is Linked Open Data (LOD), a set of interlinked data and knowledge bases. The main challenge in this context is data governance within loosely coordinated organizations that are publishing added-value interlinked data on the Web, bringing together issues related to data management and data quality, in order to support the full lifecycle of data production, consumption, and management. In this article, we are interested in curation issues for RDF(S) data, which is the default data model for LOD. In particular, we are addressing change management for RDF(S) data maintained by large communities (scientists, librarians, etc.) which act as curators to ensure high quality of data. Such curated Knowledge Bases (KBs) are constantly evolving for various reasons, such as the inclusion of new experimental evidence or observations, or the correction of erroneous conceptualizations. Managing such changes poses several research problems, including the problem of detecting the changes (delta) between versions of the same KB developed and maintained by different groups of curators, a crucial task for assisting them in understanding the involved changes. This becomes all the more important as curated KBs are interconnected (through copying or referencing) and thus changes need to be propagated from one KB to another either within or across communities. This article addresses this problem by proposing a change language which allows the formulation of concise and intuitive deltas. The language is expressive enough to describe unambiguously any possible change encountered in curated KBs expressed in RDF(S), and can be efficiently and deterministically detected in an automated way. Moreover, we devise a change detection algorithm which is sound and complete with respect to the aforementioned language, and study appropriate semantics for executing the deltas expressed in our language in order to move backwards and forwards in a multiversion repository, using only the corresponding deltas. Finally, we evaluate through experiments the effectiveness and efficiency of our algorithms using real ontologies from the cultural, bioinformatics, and entertainment domains.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W1995950372",
    "type": "article"
  },
  {
    "title": "Materialization Optimizations for Feature Selection Workloads",
    "doi": "https://doi.org/10.1145/2877204",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "Ce Zhang; Arun Kumar; Christopher Ré",
    "corresponding_authors": "",
    "abstract": "There is an arms race in the data management industry to support statistical analytics. Feature selection, the process of selecting a feature set that will be used to build a statistical model, is widely regarded as the most critical step of statistical analytics. Thus, we argue that managing the feature selection process is a pressing data management challenge. We study this challenge by describing a feature selection language and a supporting prototype system that builds on top of current industrial R-integration layers. From our interactions with analysts, we learned that feature selection is an interactive human-in-the-loop process, which means that feature selection workloads are rife with reuse opportunities. Thus, we study how to materialize portions of this computation using not only classical database materialization optimizations but also methods that have not previously been used in database optimization, including structural decomposition methods (like QR factorization) and warmstart. These new methods have no analogue in traditional SQL systems, but they may be interesting for array and scientific database applications. On a diverse set of datasets and programs, we find that traditional database-style approaches that ignore these new opportunities are more than two orders of magnitude slower than an optimal plan in this new trade-off space across multiple R backends. Furthermore, we show that it is possible to build a simple cost-based optimizer to automatically select a near-optimal execution plan for feature selection.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2998715488",
    "type": "article"
  },
  {
    "title": "Learning Join Queries from User Examples",
    "doi": "https://doi.org/10.1145/2818637",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Angela Bonifati; Radu Ciucanu; Sławek Staworko",
    "corresponding_authors": "",
    "abstract": "We investigate the problem of learning join queries from user examples. The user is presented with a set of candidate tuples and is asked to label them as positive or negative examples, depending on whether or not she would like the tuples as part of the join result. The goal is to quickly infer an arbitrary n -ary join predicate across an arbitrary number m of relations while keeping the number of user interactions as minimal as possible. We assume no prior knowledge of the integrity constraints across the involved relations. Inferring the join predicate across multiple relations when the referential constraints are unknown may occur in several applications, such as data integration, reverse engineering of database queries, and schema inference. In such scenarios, the number of tuples involved in the join is typically large. We introduce a set of strategies that let us inspect the search space and aggressively prune what we call uninformative tuples, and we directly present to the user the informative ones—that is, those that allow the user to quickly find the goal query she has in mind. In this article, we focus on the inference of joins with equality predicates and also allow disjunctive join predicates and projection in the queries. We precisely characterize the frontier between tractability and intractability for the following problems of interest in these settings: consistency checking, learnability, and deciding the informativeness of a tuple. Next, we propose several strategies for presenting tuples to the user in a given order that allows minimization of the number of interactions. We show the efficiency of our approach through an experimental study on both benchmark and synthetic datasets.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W1911540707",
    "type": "article"
  },
  {
    "title": "COMPRESS",
    "doi": "https://doi.org/10.1145/3015457",
    "publication_date": "2017-05-10",
    "publication_year": 2017,
    "authors": "Yunheng Han; Weiwei Sun; Baihua Zheng",
    "corresponding_authors": "",
    "abstract": "More and more advanced technologies have become available to collect and integrate an unprecedented amount of data from multiple sources, including GPS trajectories about the traces of moving objects. Given the fact that GPS trajectories are vast in size while the information carried by the trajectories could be redundant, we focus on trajectory compression in this article. As a systematic solution, we propose a comprehensive framework, namely, COMPRESS ( &lt;underline&gt;Com&lt;/underline&gt;prehensive &lt;underline&gt;P&lt;/underline&gt;aralleled &lt;underline&gt;R&lt;/underline&gt;oad-Network-Based Trajectory Compr&lt;underline&gt;ess&lt;/underline&gt;ion ), to compress GPS trajectory data in an urban road network. In the preprocessing step, COMPRESS decomposes trajectories into spatial paths and temporal sequences, with a thorough justification for trajectory decomposition. In the compression step, COMPRESS performs spatial compression on spatial paths, and temporal compression on temporal sequences in parallel. It introduces two alternative algorithms with different strengths for lossless spatial compression and designs lossy but error-bounded algorithms for temporal compression. It also presents query processing algorithms to support error-bounded location-based queries on compressed trajectories without full decompression. All algorithms under COMPRESS are efficient and have the time complexity of O (| T |), where | T | is the size of the input trajectory T . We have also conducted a comprehensive experimental study to demonstrate the effectiveness of COMPRESS, whose compression ratio is significantly better than related approaches.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2612068780",
    "type": "article"
  },
  {
    "title": "Dependencies for Graphs",
    "doi": "https://doi.org/10.1145/3287285",
    "publication_date": "2019-02-13",
    "publication_year": 2019,
    "authors": "Wenfei Fan; Ping Lü",
    "corresponding_authors": "",
    "abstract": "This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2914111272",
    "type": "article"
  },
  {
    "title": "Efficient Algorithms for Approximate Single-Source Personalized PageRank Queries",
    "doi": "https://doi.org/10.1145/3360902",
    "publication_date": "2019-10-23",
    "publication_year": 2019,
    "authors": "Sibo Wang; Renchi Yang; Runhui Wang; Xiaokui Xiao; Zhewei Wei; Wenqing Lin; Yin Yang; Nan Tang",
    "corresponding_authors": "",
    "abstract": "Given a graph G , a source node s, and a target node t , the personalized PageRank ( PPR ) of t with respect to s is the probability that a random walk starting from s terminates at t . An important variant of the PPR query is single-source PPR ( SSPPR ), which enumerates all nodes in G and returns the top- k nodes with the highest PPR values with respect to a given source s . PPR in general and SSPPR in particular have important applications in web search and social networks, e.g., in Twitter’s Who-To-Follow recommendation service. However, PPR computation is known to be expensive on large graphs and resistant to indexing. Consequently, previous solutions either use heuristics, which do not guarantee result quality, or rely on the strong computing power of modern data centers, which is costly. Motivated by this, we propose effective index-free and index-based algorithms for approximate PPR processing, with rigorous guarantees on result quality. We first present FORA, an approximate SSPPR solution that combines two existing methods—Forward Push (which is fast but does not guarantee quality) and Monte Carlo Random Walk (accurate but slow)—in a simple and yet non-trivial way, leading to both high accuracy and efficiency. Further, FORA includes a simple and effective indexing scheme, as well as a module for top- k selection with high pruning power. Extensive experiments demonstrate that the proposed solutions are orders of magnitude more efficient than their respective competitors. Notably, on a billion-edge Twitter dataset, FORA answers a top-500 approximate SSPPR query within 1s, using a single commodity server.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2982171596",
    "type": "article"
  },
  {
    "title": "SkinnerDB: Regret-bounded Query Evaluation via Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3464389",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Immanuel Trummer; Junxiong Wang; Ziyun Wei; Deepak Maram; Samuel H. Moseley; Saehan Jo; Joseph Antonakakis; Ankush Rayabhari",
    "corresponding_authors": "",
    "abstract": "SkinnerDB uses reinforcement learning for reliable join ordering, exploiting an adaptive processing engine with specialized join algorithms and data structures. It maintains no data statistics and uses no cost or cardinality models. Also, it uses no training workloads nor does it try to link the current query to seemingly similar queries in the past. Instead, it uses reinforcement learning to learn optimal join orders from scratch during the execution of the current query. To that purpose, it divides the execution of a query into many small time slices. Different join orders are tried in different time slices. SkinnerDB merges result tuples generated according to different join orders until a complete query result is obtained. By measuring execution progress per time slice, it identifies promising join orders as execution proceeds. Along with SkinnerDB, we introduce a new quality criterion for query execution strategies. We upper-bound expected execution cost regret, i.e., the expected amount of execution cost wasted due to sub-optimal join order choices. SkinnerDB features multiple execution strategies that are optimized for that criterion. Some of them can be executed on top of existing database systems. For maximal performance, we introduce a customized execution engine, facilitating fast join order switching via specialized multi-way join algorithms and tuple representations. We experimentally compare SkinnerDB’s performance against various baselines, including MonetDB, Postgres, and adaptive processing methods. We consider various benchmarks, including the join order benchmark, TPC-H, and JCC-H, as well as benchmark variants with user-defined functions. Overall, the overheads of reliable join ordering are negligible compared to the performance impact of the occasional, catastrophic join order choice.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3203329898",
    "type": "article"
  },
  {
    "title": "Unified Route Planning for Shared Mobility: An Insertion-based Framework",
    "doi": "https://doi.org/10.1145/3488723",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Yongxin Tong; Yuxiang Zeng; Zimu Zhou; Lei Chen; Ke Xu",
    "corresponding_authors": "",
    "abstract": "There has been a dramatic growth of shared mobility applications such as ride-sharing, food delivery, and crowdsourced parcel delivery. Shared mobility refers to transportation services that are shared among users, where a central issue is route planning . Given a set of workers and requests, route planning finds for each worker a route, i.e., a sequence of locations to pick up and drop off passengers/parcels that arrive from time to time, with different optimization objectives. Previous studies lack practicability due to their conflicted objectives and inefficiency in inserting a new request into a route, a basic operation called insertion . In addition, previous route planning solutions fail to exploit the appearance patterns of future requests hidden in historical data for optimization. In this paper, we present a unified formulation of route planning called URPSM. It has a well-defined parameterized objective function which eliminates the contradicted objectives in previous studies and enables flexible multi-objective route planning for shared mobility. We propose two insertion-based frameworks to solve the URPSM problem. The first is built upon the plain-insertion widely used in prior studies, which processes online requests only, whereas the second relies on a new insertion operator called prophet-insertion that handles both online and predicted requests. Novel dynamic programming algorithms are designed to accelerate both insertions to only linear time. Theoretical analysis shows that no online algorithm can have a constant competitive ratio for the URPSM problem under the competitive analysis model, yet our prophet-insertion-based framework can achieve a constant optimality ratio under the instance-optimality model. Extensive experimental results on real datasets show that our insertion-based solutions outperform the state-of-the-art algorithms in both effectiveness and efficiency by a large margin (e.g., up to 30 \\( \\times \\) more effective in the objective and up to 20 \\( \\times \\) faster).",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W4225271226",
    "type": "article"
  },
  {
    "title": "SchemaSQL",
    "doi": "https://doi.org/10.1145/503099.503102",
    "publication_date": "2001-12-01",
    "publication_year": 2001,
    "authors": "Laks V. S. Lakshmanan; Fereidoon Sadri; Subbu N. Subramanian",
    "corresponding_authors": "",
    "abstract": "We provide a principled extension of SQL, called SchemaSQL , that offers the capability of uniform manipulation of data and schema in relational multidatabase systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavor of SQL while supporting querying of both data and schema. (2) It can be used to transform data in a database in a structure substantially different from original database, in which data and schema may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits \"horizontal\" aggregation and even aggregation over more general \"blocks\" of information. (5) SchemaSQL provides a useful facility for interoperability and data/schema manipulation in relational multidatabase systems. We provide many examples to illustrate our claims. We clearly spell out the formal semantics of SchemaSQL that accounts for all these features. We describe an architecture for the implementation of SchemaSQL and develop implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. We also discuss the applicability of SchemaSQL for handling semantic heterogeneity arising in a multidatabase system.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W1997533292",
    "type": "article"
  },
  {
    "title": "Multiway spatial joins",
    "doi": "https://doi.org/10.1145/503099.503101",
    "publication_date": "2001-12-01",
    "publication_year": 2001,
    "authors": "Nikos Mamoulis; Dimitris Papadias",
    "corresponding_authors": "",
    "abstract": "Due to the evolution of Geographical Information Systems, large collections of spatial data having various thematic contents are currently available. As a result, the interest of users is not limited to simple spatial selections and joins, but complex query types that implicate numerous spatial inputs become more common. Although several algorithms have been proposed for computing the result of pairwise spatial joins, limited work exists on processing and optimization of multiway spatial joins . In this article, we review pairwise spatial join algorithms and show how they can be combined for multiple inputs. In addition, we explore the application of synchronous traversal (ST), a methodology that processes synchronously all inputs without producing intermediate results. Then, we integrate the two approaches in an engine that includes ST and pairwise algorithms, using dynamic programming to determine the optimal execution plan. The results show that, in most cases, multiway spatial joins are best processed by combining ST with pairwise methods. Finally, we study the optimization of very large queries by employing randomized search algorithms.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2005083765",
    "type": "article"
  },
  {
    "title": "Spatial queries in dynamic environments",
    "doi": "https://doi.org/10.1145/777943.777944",
    "publication_date": "2003-06-01",
    "publication_year": 2003,
    "authors": "Yufei Tao; Dimitris Papadias",
    "corresponding_authors": "",
    "abstract": "Conventional spatial queries are usually meaningless in dynamic environments since their results may be invalidated as soon as the query or data objects move. In this paper we formulate two novel query types, time parameterized and continuous queries , applicable in such environments. A time-parameterized query retrieves the actual result at the time when the query is issued, the expiry time of the result given the current motion of the query and database objects, and the change that causes the expiration. A continuous query retrieves tuples of the form &lt; result , interval &gt;, where each result is accompanied by a future interval , during which it is valid. We study time-parameterized and continuous versions of the most common spatial queries (i.e., window queries, nearest neighbors, spatial joins), proposing efficient processing algorithms and accurate cost models.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2142908528",
    "type": "article"
  },
  {
    "title": "Temporal statement modifiers",
    "doi": "https://doi.org/10.1145/377674.377665",
    "publication_date": "2000-12-01",
    "publication_year": 2000,
    "authors": "Michael H. Böhlen; Christian S. Jensen; Richard T. Snodgrass",
    "corresponding_authors": "",
    "abstract": "A wide range of database applications manage time-varying data. Many temporal query languages have been proposed, each one the result of many carefully made yet subtly interacting design decisions. In this article we advocate a different approach to articulating a set of requirements, or desiderata, that directly imply the syntactic structure and core semantics of a temporal extension of an (arbitrary) nontemporal query language. These desiderata facilitate transitioning applications from a nontemporal query language and data model, which has received only scant attention thus far. The paper then introduces the notion of statement modifiers that provide a means of systematically adding temporal support to an existing query language. Statement modifiers apply to all query language statements, for example, queries, cursor definitions, integrity constraints, assertions, views, and data manipulation statements. We also provide a way to systematically add temporal support to an existing implementation. The result is a temporal query language syntax, semantics, and implementation that derives from first principles. We exemplify this approach by extending SQL-92 with statement modifiers. This extended language, termed ATSQL, is formally defined via a denotational-semantics-style mapping of temporal statements to expressions using a combination of temporal and conventional relational algebraic operators.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W1986739594",
    "type": "article"
  },
  {
    "title": "Probabilistic temporal databases, I",
    "doi": "https://doi.org/10.1145/383734.383736",
    "publication_date": "2001-03-01",
    "publication_year": 2001,
    "authors": "Alex Dekhtyar; Robert Ross; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "Dyreson and Snodgrass have drawn attention to the fact that, in many temporal database applications, there is often uncertainty about the start time of events, the end time of events, and the duration of events. When the granularity of time is small (e.g., milliseconds), a statement such as “Packet p was shipped sometime during the first 5 days of January, 1998” leads to a massive amount of uncertainty (5×24×60×60×1000) possibilities. As noted in Zaniolo et al. [1997], past attempts to deal with uncertainty in databases have been restricted to relatively small amounts of uncertainty in attributes. Dyreson and Snodgrass have taken an important first step towards solving this problem. In this article, we first introduce the syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations. We then present a theoretical annotated temporal algebra (TATA). Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large. Next, we present a temporal probabilistic algebra (TPA). We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on top of ODBC.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2118048287",
    "type": "article"
  },
  {
    "title": "Expressing and optimizing sequence queries in database systems",
    "doi": "https://doi.org/10.1145/1005566.1005568",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Reza Sadri; Carlo Zaniolo; Amir M. Zarkesh; Jafar Adibi",
    "corresponding_authors": "",
    "abstract": "The need to search for complex and recurring patterns in database sequences is shared by many applications. In this paper, we investigate the design and optimization of a query language capable of expressing and supporting efficiently the search for complex sequential patterns in database systems. Thus, we first introduce SQL-TS, an extension of SQL to express these patterns, and then we study how to optimize the queries for this language. We take the optimal text search algorithm of Knuth, Morris and Pratt, and generalize it to handle complex queries on sequences. Our algorithm exploits the interdependencies between the elements of a pattern to minimize repeated passes over the same data. Experimental results on typical sequence queries, such as double bottom queries, confirm that substantial speedups are achieved by our new optimization techniques.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2149233085",
    "type": "article"
  },
  {
    "title": "Reasoning about functional dependencies generalized for semantic data models",
    "doi": "https://doi.org/10.1145/128765.128767",
    "publication_date": "1992-03-01",
    "publication_year": 1992,
    "authors": "Grant Weddell",
    "corresponding_authors": "Grant Weddell",
    "abstract": "We propose a more general form of functional dependency for semantic data models that derives from their common feature in which the separate notions of domain and relation in the relational model are combined into a single notion of class . This usually results in a richer terminological component for their query languages, whereby terms may navigate through any number of properties, including none. We prove the richer expressiveness of this more general functional dependency, and exhibit a sound and complete set of inference axioms. Although the general problem of decidability of their logical implication remains open at this time, we present decision procedures for cases in which the dependencies included in a schema correspond to keys, or in which the schema itself is acyclic. The theory is then extended to include a form of conjunctive query. Of particular significance is that the query becomes an additional source of functional dependency. Finally, we outline several applications of the theory to various problems in physical design and in query optimization. The applications derive from an ability to predict when a query can have at most one solution.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2090785509",
    "type": "article"
  },
  {
    "title": "Formal query languages for secure relational databases",
    "doi": "https://doi.org/10.1145/195664.195675",
    "publication_date": "1994-12-01",
    "publication_year": 1994,
    "authors": "Marianne Winslett; Kenny Smith; Xiaolei Qian",
    "corresponding_authors": "",
    "abstract": "The addition of stringent security specifications to the list of requirements for an application poses many new problems in DBMS design and implementation, as well as database design, use, and maintenance. Tight security requirements, such as those that result in silent masking of witholding of true information from a user or the introduction of false information into query answers, also raise fundamental questions about the meaning of the database and the semantics of accompanying query languages. In this paper, we propose a belief-based semantics for secure databases, which provides a semantics for databases that can “lie” about the state of the world, or about their knowledge about the state of the world, in order to preserve security. This kind of semantics can be used as a helpful retrofit for the proposals for a “multilevel secure” database model (a particularly stringent form of security), and may be useful for less restrictive security policies as well. We also propose a family of query languages for multilevel secure relational database applications, and base the semantics of those languages on our semantics for secure databases. Our query languages are free of the semantic problems associated with use of ordinary SQL in a multilevel secure context, and should be easy for users to understand and employ.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W1969843881",
    "type": "article"
  },
  {
    "title": "Concurrency control for high contention environments",
    "doi": "https://doi.org/10.1145/128903.128906",
    "publication_date": "1992-06-01",
    "publication_year": 1992,
    "authors": "P. A. Franaszek; John T. Robinson; Alexander Thomasian",
    "corresponding_authors": "",
    "abstract": "Future transaction processing systems may have substantially higher levels of concurrency due to reasons which include: (1) increasing disparity between processor speeds and data access latencies, (2) large numbers of processors, and (3) distributed databases. Another influence is the trend towards longer or more complex transactions. A possible consequence is substantially more data contention, which could limit total achievable throughput. In particular, it is known that the usual locking method of concurrency control is not well suited to environments where data contention is a significant factor. Here we consider a number of concurrency control concepts and transaction scheduling techniques that are applicable to high contention environments, and that do not rely on database semantics to reduce contention. These include access invariance and its application to prefetching of data, approximations to essential blocking such as wait depth limited scheduling, and phase dependent control. The performance of various concurrency control methods based on these concepts are studied using detailed simulation models. The results indicate that the new techniques can offer substantial benefits for systems with high levels of data contention.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2056082601",
    "type": "article"
  },
  {
    "title": "On completeness of historical relational query languages",
    "doi": "https://doi.org/10.1145/174638.174642",
    "publication_date": "1994-03-01",
    "publication_year": 1994,
    "authors": "James Clifford; Albert Croker; Alexander Tuzhilin",
    "corresponding_authors": "",
    "abstract": "Numerous proposals for extending the relational data model to incorporate the temporal dimension of data have appeared in the past several years. These proposals have differed considerably in the way that the temporal dimension has been incorporated both into the structure of the extended relations of these temporal models and into the extended relational algebra or calculus that they define. Because of these differences, it has been difficult to compare the proposed models and to make judgments as to which of them might in some sense be equivalent or even better . In this paper we define temporally grouped and temporally ungrouped historical data models and propose two notions of historical relational completeness , analogous to Codd's notion of relational completeness, one for each type of model. We show that the temporally ungrouped models are less expressive than the grouped models, but demonstrate a technique for extending the ungrouped models with a grouping mechanism to capture the additional semantic power of temporal grouping. For the ungrouped models, we define three different languages, a logic with explicit reference to time, a temporal logic, and a temporal algebra, and motivate our choice for the first of these as the basis for completeness for these models. For the grouped models, we define a many-sorted logic with variables over ordinary values, historical values, and times. Finally, we demonstrate the equivalence of this grouped calculus and the ungrouped calculus extended with a grouping mechanism. We believe the classification of historical data models into grouped and ungrouped models provides a useful framework for the comparison of models in the literature, and furthermore, the exposition of equivalent languages for each type provides reasonable standards for common, and minimal, notions of historical relational completeness.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W3124520529",
    "type": "article"
  },
  {
    "title": "An axiomatic model of dynamic schema evolution in objectbase systems",
    "doi": "https://doi.org/10.1145/244810.244813",
    "publication_date": "1997-03-01",
    "publication_year": 1997,
    "authors": "Randel J. Peters; M. TAMER ÖZSU",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on An axiomatic model of dynamic schema evolution in objectbase systems Authors: Randel J. Peters Univ. of Manitoba, Manitoba, Canada Univ. of Manitoba, Manitoba, CanadaView Profile , M. Tamer Özsu Univ. of Alberta, Alberta, Canada Univ. of Alberta, Alberta, CanadaView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 22Issue 1March 1997 pp 75–114https://doi.org/10.1145/244810.244813Published:01 March 1997Publication History 59citation658DownloadsMetricsTotal Citations59Total Downloads658Last 12 Months11Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2081725016",
    "type": "article"
  },
  {
    "title": "Efficient dynamic mining of constrained frequent sets",
    "doi": "https://doi.org/10.1145/958942.958944",
    "publication_date": "2003-12-01",
    "publication_year": 2003,
    "authors": "Laks V. S. Lakshmanan; Carson K. Leung; Raymond T. Ng",
    "corresponding_authors": "",
    "abstract": "Data mining is supposed to be an iterative and exploratory process. In this context, we are working on a project with the overall objective of developing a practical computing environment for the human-centered exploratory mining of frequent sets. One critical component of such an environment is the support for the dynamic mining of constrained frequent sets of items. Constraints enable users to impose a certain focus on the mining process; dynamic means that, in the middle of the computation, users are able to (i) change (such as tighten or relax) the constraints and/or (ii) change the minimum support threshold, thus having a decisive influence on subsequent computations. In a real-life situation, the available buffer space may be limited, thus adding another complication to the problem.In this article, we develop an algorithm, called DCF, for Dynamic Constrained Frequent-set computation . This algorithm is enhanced with a few optimizations, exploiting a lightweight structure called a segment support map . It enables DCF to (i) obtain sharper bounds on the support of sets of items, and to (ii) better exploit properties of constraints. Furthermore, when handling dynamic changes to constraints, DCF relies on the concept of a delta member generating function , which generates precisely the sets of items that satisfy the new but not the old constraints. Our experimental results show the effectiveness of these enhancements.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2150433972",
    "type": "article"
  },
  {
    "title": "Object normal forms and dependency constraints for object-oriented schemata",
    "doi": "https://doi.org/10.1145/278245.278247",
    "publication_date": "1997-12-01",
    "publication_year": 1997,
    "authors": "Zahir Tari; John Stokes; Stefano Spaccapietra",
    "corresponding_authors": "",
    "abstract": "We address the development of a normalization theory for object-oriented data models that have common features to support objects. We first provide an extension of functional dependencies to cope with the richer semantics of relationships between objects, called path dependency, local dependency, and global dependency constraints. Using these dependency constraints, we provide normal forms for object-oriented data models based on the notions of user interpretation (user-specified dependency constraints) and object model . In constrast to conventional data models in which a normalized object has a unique interpretation, in object-oriented data models, an object may have many multiple interpretations that form the model for that object. An object will then be in a normal form if and only if the user's interpretation is derivable from the model of the object. Our normalization process is by nature iiterative, in which objects are restructured until their models reflect the user's interpretation.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W1988902465",
    "type": "article"
  },
  {
    "title": "Converting nested algebra expressions into flat algebra expressions",
    "doi": "https://doi.org/10.1145/128765.128768",
    "publication_date": "1992-03-01",
    "publication_year": 1992,
    "authors": "Jan Paredaens; Dirk Van Gucht",
    "corresponding_authors": "",
    "abstract": "Nested relations generalize ordinary flat relations by allowing tuple values to be either atomic or set valued. The nested algebra is a generalization of the flat relational algebra to manipulate nested relations. In this paper we study the expressive power of the nested algebra relative to its operation on flat relational databases. We show that the flat relational algebra is rich enough to extract the same “flat information” from a flat database as the nested algebra does. Theoretically, this result implies that recursive queries such as the transitive closure of a binary relation cannot be expressed in the nested algebra. Practically, this result is relevant to (flat) relational query optimization.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2100220081",
    "type": "article"
  },
  {
    "title": "Solving satisfiability and implication problems in database systems",
    "doi": "https://doi.org/10.1145/232616.232692",
    "publication_date": "1996-06-01",
    "publication_year": 1996,
    "authors": "Sha Guo; Wei Sun; Mark Allen Weiss",
    "corresponding_authors": "",
    "abstract": "Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, ( X op Y ) and ( X op C ), where X and Y are attributes, C is a constant of the domain or X , and op ∈{&lt;, ≤, =, ≠, &gt;, ≥). These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a 0-join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({&lt;, ≤, =, ≥, &gt;} and {&lt;, ≤, =, ≠, ≥, &gt;}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input. The C++ code can be obtained by an anonymous ftp from &lt;archive.fiu.edu&gt;.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2071875892",
    "type": "article"
  },
  {
    "title": "Transitive closure algorithms based on graph traversal",
    "doi": "https://doi.org/10.1145/155271.155273",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Yannis Ioannidis; Raghu Ramakrishnan; Linda Winger",
    "corresponding_authors": "",
    "abstract": "Several graph-based algorithms have been proposed in the literature to compute the transitive closure of a directed graph. We develop two new algorithms (Basic_TC and Gobal_DFTC) and compare the performance of their implementations in a disk-based environment with a well-known graph-based algorithm proposed by Schmitz. Our algorithms use depth-first search to traverse a graph and a technique called marking to avoid processing some of the arcs in the graph. They compute the closure by processing nodes in reverse topological order, building descendent sets by adding the descendent sets of children. While the details of these algorithms differ considerably, one important difference among them is the time at which descendent set additions are performed. Basic_TC, results in superior performance. The first reason is that early additions result in larger descendent set sizes on the average over the duration of the execution, thereby causing more I/O; very often this turns out to more than offset the gains of not having to fetch certain sets again to add them. The second reason is that information collected in the first pass can be used to apply several optimizations in the second pass. To the extent possible, we also adapt these algorithms to perform path computations. Again, our performance comparison confirms the trends seen in reachability queries. Taken in conjunction with another performance study our results indicate that all graph-based algorithms significantly outperform other types of algorithms such as Seminaive and Warren.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2099624270",
    "type": "article"
  },
  {
    "title": "Wavelet synopses for general error metrics",
    "doi": "https://doi.org/10.1145/1114244.1114246",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Minos Garofalakis; Amit Kumar",
    "corresponding_authors": "",
    "abstract": "Several studies have demonstrated the effectiveness of the wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate query answers. Conventional wavelet synopses that greedily minimize the overall root-mean-squared (i.e., L 2 -norm) error in the data approximation can suffer from important problems, including severe bias and wide variance in the quality of the data reconstruction, and lack of nontrivial guarantees for individual approximate answers. Thus, probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control maximum approximation-error metrics (e.g., maximum relative error).A key open problem is whether it is possible to design efficient deterministic wavelet-thresholding algorithms for minimizing general, non-L 2 error metrics that are relevant to approximate query processing systems, such as maximum relative or maximum absolute error. Obviously, such algorithms can guarantee better maximum-error wavelet synopses and avoid the pitfalls of probabilistic techniques (e.g., “bad” coin-flip sequences) leading to poor solutions; in addition, they can be used to directly optimize the synopsis construction process for other useful error metrics, such as the mean relative error in data-value reconstruction. In this article, we propose novel, computationally efficient schemes for deterministic wavelet thresholding with the objective of optimizing general approximation-error metrics . We first consider the problem of constructing wavelet synopses optimized for maximum error , and introduce an optimal low polynomial-time algorithm for one-dimensional wavelet thresholding---our algorithm is based on a new Dynamic-Programming (DP) formulation, and can be employed to minimize the maximum relative or absolute error in the data reconstruction. Unfortunately, directly extending our one-dimensional DP algorithm to multidimensional wavelets results in a super-exponential increase in time complexity with the data dimensionality. Thus, we also introduce novel, polynomial-time approximation schemes (with tunable approximation guarantees) for deterministic wavelet thresholding in multiple dimensions. We then demonstrate how our optimal and approximate thresholding algorithms for maximum error can be extended to handle a broad, natural class of distributive error metrics , which includes several important error measures, such as mean weighted relative error and weighted L p -norm error. Experimental results on real-world and synthetic data sets evaluate our novel optimization algorithms, and demonstrate their effectiveness against earlier wavelet-thresholding schemes.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2064174652",
    "type": "article"
  },
  {
    "title": "Safety and translation of relational calculus",
    "doi": "https://doi.org/10.1145/114325.103712",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Allen Van Gelder; Rodney Topor",
    "corresponding_authors": "",
    "abstract": "Not all queries in relational calculus can be answered sensibly when disjunction, negation, and universal quantification are allowed. The class of relation calculus queries or formulas that have sensible answers is called the domain independent class which is known to be undecidable. Subsequent research has focused on identifying large decidable subclasses of domain independent formulas. In this paper we investigate the properties of two such classes: the evaluable formulas and the allowed formulas. Although both classes have been defined before, we give simplified definitions, present short proofs of their main properties, and describe a method to incorporate equality. Although evaluable queries have sensible answers, it is not straightforward to compute them efficiently or correctly. We introduce relational algebra normal form for formulas from which form the correct translation into relational algebra is trivial. We give algorithms to transform an evaluable formula into an equivalent allowed formula and from there into relational algebra normal form. Our algorithms avoid use of the so-called Dom relation, consisting of all constants appearing in the database or the query. Finally, we describe a restriction under which every domain independent formula is evaluable and argue that the class of evaluable formulas is the largest decidable subclass of the domain independent formulas that can be efficiently recognized.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2065603413",
    "type": "article"
  },
  {
    "title": "A universal-scheme approach to statistical databases containing homogeneous summary tables",
    "doi": "https://doi.org/10.1145/169725.169712",
    "publication_date": "1993-12-01",
    "publication_year": 1993,
    "authors": "Francesco M. Malvestuto",
    "corresponding_authors": "Francesco M. Malvestuto",
    "abstract": "article Free Access Share on A universal-scheme approach to statistical databases containing homogeneous summary tables Author: Francesco M. Malvestuto L'Aquila Univ., L'Aquila, Italy L'Aquila Univ., L'Aquila, ItalyView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 18Issue 4pp 678–708https://doi.org/10.1145/169725.169712Published:01 December 1993Publication History 62citation529DownloadsMetricsTotal Citations62Total Downloads529Last 12 Months16Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2005992727",
    "type": "article"
  },
  {
    "title": "Applications of Byzantine agreement in database systems",
    "doi": "https://doi.org/10.1145/5236.5243",
    "publication_date": "1986-03-01",
    "publication_year": 1986,
    "authors": "Hector Garcia Molina; Frank M. Pittelli; Susan B. Davidson",
    "corresponding_authors": "",
    "abstract": "In this paper we study when and how B Byzantine agreement protocol can he used in general-purpose database management systems. We present an overview of the failure model used for Byzantine agreement, and of the protocol itself. We then present correctness criteria for database processing in this failure environment and discuss strategies for satisfying them. In doing this, we present new failure models for input/output nodes and study ways to distribute input transactions to processing nodes under these models. Finally, we investigate applications of Byzantine agreement protocols in the more common failure environment where processors are assumed to halt after a failure.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2079436279",
    "type": "article"
  },
  {
    "title": "New methods and fast algorithms for database normalization",
    "doi": "https://doi.org/10.1145/44498.44499",
    "publication_date": "1988-09-01",
    "publication_year": 1988,
    "authors": "Jim Diederich; Jack Milton",
    "corresponding_authors": "",
    "abstract": "A new method for computing minimal covers is presented using a new type of closure that allows significant reductions in the number of closures computed for normalizing relations. Benchmarks are reported comparing the new and the standard techniques.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2140100614",
    "type": "article"
  },
  {
    "title": "Performance analysis of recovery techniques",
    "doi": "https://doi.org/10.1145/1994.1996",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "Andreas Reuter",
    "corresponding_authors": "Andreas Reuter",
    "abstract": "Various logging and recovery techniques for centralized transaction-oriented database systems under performance aspects are described and discussed. The classification of functional principles that has been developed in a companion paper is used as a terminological basis. In the main sections, a set of analytic models is introduced and evaluated in order to compare the performance characteristics of nine different recovery techniques with respect to four key parameters and a set of other parameters with less influence. Finally, the results of model evaluation as well as the limitations of the models themselves are discussed.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W1967626497",
    "type": "article"
  },
  {
    "title": "Reactive consistency control in deductive databases",
    "doi": "https://doi.org/10.1145/115302.115298",
    "publication_date": "1991-12-01",
    "publication_year": 1991,
    "authors": "Guido Moerkotte; Peter C. Lockemann",
    "corresponding_authors": "",
    "abstract": "article Free AccessReactive consistency control in deductive databases Authors: Guido Moerkotte Univ. Karlsruhe, Karlsruhe, Germany Univ. Karlsruhe, Karlsruhe, GermanyView Profile , Peter C. Lockemann Univ. Karlsruhe, Karlsruhe, Germany Univ. Karlsruhe, Karlsruhe, GermanyView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 16Issue 4pp 670–702https://doi.org/10.1145/115302.115298Published:01 December 1991Publication History 55citation380DownloadsMetricsTotal Citations55Total Downloads380Last 12 Months13Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W1981183777",
    "type": "article"
  },
  {
    "title": "Implementation concepts for an extensible data model and data language",
    "doi": "https://doi.org/10.1145/44498.45062",
    "publication_date": "1988-09-01",
    "publication_year": 1988,
    "authors": "Don Batory; T. Y. Cliff Leung; T. E. Wise",
    "corresponding_authors": "",
    "abstract": "Future database systems must feature extensible data models and data languages in order to accommodate the novel data types and special-purpose operations that are required by nontraditional database applications. In this paper, we outline a functional data model and data language that are targeted for the semantic interface of GENESIS, an extensible DBMS. The model and language are generalizations of FQL [11] and DAPLEX [40], and have an implementation that fits ideally with the modularity required by extensible database technologies. We explore different implementations of functional operators and present experimental evidence that they have efficient implementations. We also explain the advantages of a functional front-end to ¬1NF databases, and show how our language and implementation are being used to process queries on both 1NF and ¬1NF relations.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2082375048",
    "type": "article"
  },
  {
    "title": "Tree queries",
    "doi": "https://doi.org/10.1145/319758.319775",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "Nathan Goodman; Oded Shmueli",
    "corresponding_authors": "",
    "abstract": "One can partition the class of relational database schemas into tree schemas and cyclic schemas. (These are called acyclic hypergraphs and cyclic hypergraphs elsewhere in the literature.) This partition has interesting implications in query processing, dependency theory, and graph theory. The tree/cyclic partitioning of database schemas originated with a similar partition of equijoin queries. Given an arbitrary equijoin query one can obtain an equivalent query that calculates the natural join of all relations in (an efficiently) derived database; such a query is called a natural join (NJ) query. If the derived database is a tree schema the original query is said to be a tree query, and otherwise a cyclic query. In this paper we analyze query processing consequences of the tree/cyclic partitioning. We are able to argue, qualitatively, that queries which imply a tree schema are easier to process than those implying a cyclic schema. Our results also extend the study of the semijoin operator.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W1977735896",
    "type": "article"
  },
  {
    "title": "Formal semantics of SQL queries",
    "doi": "https://doi.org/10.1145/111197.111212",
    "publication_date": "1991-09-01",
    "publication_year": 1991,
    "authors": "Mauro Negri; Giuseppe Pelagatti; Licia Sbattella",
    "corresponding_authors": "",
    "abstract": "The semantics of SQL queries is formally defined by stating a set of rules that determine a syntax-driven translation of an SQL query to a formal model. The target model, called Extended Three Valued Predicate Calculus (E3VPC), is largely based on a set of well-known mathematical concepts. The rules which allow the transformation of a general E3VPC expression to a Canonical Form, which can be manipulated using traditional, two-valued predicate calculus are also given; in this way, problems like equivalence analysis of SQL queries are completely solved. Finally, the fact that reasoning about the equivalence of SQL queries using two-valued predicate calculus, without taking care of the real SQL semantics can lead to errors is shown, and the reasons for this are analyzed.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2020949660",
    "type": "article"
  },
  {
    "title": "The <i>pq</i> -gram distance between ordered labeled trees",
    "doi": "https://doi.org/10.1145/1670243.1670247",
    "publication_date": "2008-02-15",
    "publication_year": 2008,
    "authors": "Nikolaus Augsten; Michael H. Böhlen; Johann Gamper",
    "corresponding_authors": "",
    "abstract": "When integrating data from autonomous sources, exact matches of data items that represent the same real-world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. Typically the matching must be approximate since the representations in the sources differ. We propose pq -grams to approximately match hierarchical data from autonomous sources and define the pq -gram distance between ordered labeled trees as an effective and efficient approximation of the fanout weighted tree edit distance. We prove that the pq -gram distance is a lower bound of the fanout weighted tree edit distance and give a normalization of the pq -gram distance for which the triangle inequality holds. Experiments on synthetic and real-world data (residential addresses and XML) confirm the scalability of our approach and show the effectiveness of pq -grams.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2033629528",
    "type": "article"
  },
  {
    "title": "Forensic analysis of database tampering",
    "doi": "https://doi.org/10.1145/1412331.1412342",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Kyriacos E. Pavlou; Richard T. Snodgrass",
    "corresponding_authors": "",
    "abstract": "Regulations and societal expectations have recently expressed the need to mediate access to valuable databases, even by insiders. One approach is tamper detection via cryptographic hashing. This article shows how to determine when the tampering occurred, what data was tampered with, and perhaps, ultimately, who did the tampering, via forensic analysis. We present four successively more sophisticated forensic analysis algorithms: the Monochromatic, RGBY, Tiled Bitmap, and a3D algorithms, and characterize their “forensic cost” under worst-case, best-case, and average-case assumptions on the distribution of corruption sites. A lower bound on forensic cost is derived, with RGBY and a3D being shown optimal for a large number of corruptions. We also provide validated cost formulæ for these algorithms and recommendations for the circumstances in which each algorithm is indicated.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2048573274",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1206049.1206050",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Richard T. Snodgrass",
    "corresponding_authors": "Richard T. Snodgrass",
    "abstract": "This editorial analyzes from a variety of perspectives the controversial issue of single-blind versus double-blind reviewing. In single-blind reviewing, the reviewer is unknown to the author, but the identity of the author is known to the reviewer. Double-blind reviewing is more symmetric: The identity of the author and the reviewer are not revealed to each other. We first examine the significant scholarly literature regarding blind reviewing. We then list six benefits claimed for double-blind reviewing and 21 possible costs. To compare these benefits and costs, we propose a double-blind policy for TODS that attempts to minimize the costs while retaining the core benefit of fairness that double-blind reviewing provides, and evaluate that policy against each of the listed benefits and costs. Following that is a general discussion considering several questions: What does this have to do with TODS , does bias exist in computer science, and what is the appropriate decision procedure? We explore the “knobs” a policy design can manipulate to fine-tune a double-blind review policy. This editorial ends with a specific decision.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2037305461",
    "type": "editorial"
  },
  {
    "title": "On database systems development through logic",
    "doi": "https://doi.org/10.1145/319682.319700",
    "publication_date": "1982-03-01",
    "publication_year": 1982,
    "authors": "Verónica Dahl",
    "corresponding_authors": "Verónica Dahl",
    "abstract": "The use of logic as a single tool for formalizing and implementing different aspects of database systems in a uniform manner is discussed. The discussion focuses on relational databases with deductive capabilities and very high-level querying and defining features. The computational interpretation of logic is briefly reviewed, and then several pros and cons concerning the description of data, programs, queries, and language parser in terms of logic programs are examined. The inadequacies are discussed, and it is shown that they can be overcome by the introduction of convenient extensions into logic programming. Finally, an experimental database query system with a natural language front end, implemented in PROLOG, is presented as an illustration of these concepts. A description of the latter from the user's point of view and a sample consultation session in Spanish are included.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2056911890",
    "type": "article"
  },
  {
    "title": "A unifying model of physical databases",
    "doi": "https://doi.org/10.1145/319758.319760",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "Don Batory; C. C. Gotlieb",
    "corresponding_authors": "",
    "abstract": "A unifying model for the study of database performance is proposed. Applications of the model are shown to relate and extend important work concerning batched searching, transposed files, index selection, dynamic hash-based files, generalized access path structures, differential files, network databases, and multifile query processing.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1994179273",
    "type": "article"
  },
  {
    "title": "Analyses of multi-level and multi-component compressed bitmap indexes",
    "doi": "https://doi.org/10.1145/1670243.1670245",
    "publication_date": "2008-02-15",
    "publication_year": 2008,
    "authors": "Kesheng Wu; Arie Shoshani; Kurt Stockinger",
    "corresponding_authors": "",
    "abstract": "Bitmap indexes are known as the most effective indexing methods for range queries on append-only data, and many different bitmap indexes have been proposed in the research literature. However, only two of the simplest ones are used in commercial products. To better understand the benefits offered by the more sophisticated variations, we conduct an analytical comparison of well-known bitmap indexes, most of which are in the class of multi-component bitmap indexes. Our analysis is the first to fully incorporate the effects of compression on their performance. We produce closed-form formulas for both the index sizes and the query processing costs for the worst cases. One surprising finding is that the two simple indexes are in fact the best among multi-component indexes. Additionally, we investigate a number of novel variations in a class of multi-level indexes, and find that they answer queries faster than the best of multi-component indexes. More specifically, some two-level indexes are predicted by analyses and verified with experiments to be 5 to 10 times faster than well-known indexes. Furthermore, these two-level indexes have the optimal computational complexity for answering queries.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2071774356",
    "type": "article"
  },
  {
    "title": "On the design of relational database schemata",
    "doi": "https://doi.org/10.1145/319540.319542",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "Carlo Zaniolo; Miachel A. Meklanoff",
    "corresponding_authors": "",
    "abstract": "The purpose of this paper is to present a new approach to the conceptual design of relational databases based on the complete relatability conditions (CRCs). It is shown that current database design methodology based upon the elimination of anomalies is not adequate. In contradistinction, the CRCs are shown to provide a powerful criticism for decomposition. A decomposition algorithm is presented which (1) permits decomposition of complex relations into simple, well-defined primitives, (2) preserves all the original information, and (3) minimizes redundancy. The paper gives a complete derivation of the CRCs, beginning with a unified treatment of functional and multivalued dependencies, and introduces the concept of elementary functional dependencies and multiple elementary multivalued dependencies. Admissibility of covers and validation of results are also discussed, and it is shown how these concepts may be used to improve the design of 3NF schemata. Finally, a convenient graphical representation is proposed, and several examples are described in detail to illustrate the method.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2036758891",
    "type": "article"
  },
  {
    "title": "Determining View dependencies using tableaux",
    "doi": "https://doi.org/10.1145/319732.319738",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Anthony C. Klug; Rod Price",
    "corresponding_authors": "",
    "abstract": "A relational database models some part of the real world by a set of relations and a set of constraints. The constraints model properties of the stored information and must be maintained true at all times. For views defined over physically stored (base) relations, this is done by determining whether the view constraints are logical consequences of base relation constraints. A technique for determining such valid view constraints is presented in this paper. A generalization of the tableau chase is used. The idea of the method is to generate a tableau for the expression whose summary violates the test constraints in a “canonical” way. The chase then tries to remove this violation. It is also shown how this method has applications to schema design. Relations not in normal form or having other deficiencies can be replaced by normal form projections without losing the ability to represent all constraint information.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2047929536",
    "type": "article"
  },
  {
    "title": "Retrospection on a database system",
    "doi": "https://doi.org/10.1145/320141.320158",
    "publication_date": "1980-06-01",
    "publication_year": 1980,
    "authors": "Michael Stonebraker",
    "corresponding_authors": "Michael Stonebraker",
    "abstract": "This paper describes the implementation history of the INGRES database system. It focuses on mistakes that were made in progress rather than on eventual corrections. Some attention is also given to the role of structured design in a database system implementation and to the problem of supporting nontrivial users. Lastly, miscellaneous impressions of UNIX, the PDP-11, and data models are given.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2107388383",
    "type": "article"
  },
  {
    "title": "Performance analysis of linear hashing with partial expansions",
    "doi": "https://doi.org/10.1145/319758.319763",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "Per-Åke Larson",
    "corresponding_authors": "Per-Åke Larson",
    "abstract": "Linear hashing with partial expansions is a new file organization primarily intended for files which grow and shrink dynamically. This paper presents a mathematical analysis of the expected performance of the new scheme. The following measures are considered: length of successful and unsuccessful searches, accesses required to insert or delete a record, and the size of the overflow area. The performance is cyclical. For all performance measures, the necessary formulas are derived for computing the expected performance at any point of a cycle and the average over a cycle. Furthermore, the expected worst case in connection with searching is analyzed. The overall performance depends on several file parameters. The numerical results show that for many realistic parameter combinations the performance is expected to be extremely good. Even the longest search is expected to be of quite reasonable length.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W1984887437",
    "type": "article"
  },
  {
    "title": "Algorithms and metrics for processing multiple heterogeneous continuous queries",
    "doi": "https://doi.org/10.1145/1331904.1331909",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Mohamed A. Sharaf; Panos K. Chrysanthis; Alexandros Labrinidis; Kirk Pruhs",
    "corresponding_authors": "",
    "abstract": "The emergence of monitoring applications has precipitated the need for Data Stream Management Systems (DSMSs), which constantly monitor incoming data feeds (through registered continuous queries), in order to detect events of interest. In this article, we examine the problem of how to schedule multiple Continuous Queries (CQs) in a DSMS to optimize different Quality of Service (QoS) metrics. We show that, unlike traditional online systems, scheduling policies in DSMSs that optimize for average response time will be different from policies that optimize for average slowdown, which is a more appropriate metric to use in the presence of a heterogeneous workload. Towards this, we propose policies to optimize for the average-case performance for both metrics. Additionally, we propose a hybrid scheduling policy that strikes a fine balance between performance and fairness, by looking at both the average- and worst-case performance, for both metrics. We also show how our policies can be adaptive enough to handle the inherent dynamic nature of monitoring applications. Furthermore, we discuss how our policies can be efficiently implemented and extended to exploit sharing in optimized multi-query plans and multi-stream CQs. Finally, we experimentally show using real data that our policies consistently outperform currently used ones.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2023749213",
    "type": "article"
  },
  {
    "title": "A methodology for creating user views in database design",
    "doi": "https://doi.org/10.1145/44498.45064",
    "publication_date": "1988-09-01",
    "publication_year": 1988,
    "authors": "Veda C. Storey; Robert C. Goldstein",
    "corresponding_authors": "",
    "abstract": "The View Creation System (VCS) is an expert system that engages a user in a dialogue about the information requirements for some application, develops an Entity-Relationship model for the user's database view, and then converts the E-R model to a set of Fourth Normal Form relations. This paper describes the knowledge base of VCS. That is, it presents a formal methodology, capable of mechanization as a computer program, for accepting requirements from a user, identifying and resolving inconsistencies, redundancies, and ambiguities, and ultimately producing a normalized relational representation. Key aspects of the methodology are illustrated by applying VCS's knowledge base to an actual database design task.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2126591553",
    "type": "article"
  },
  {
    "title": "Concurrency in linear hashing",
    "doi": "https://doi.org/10.1145/22952.22954",
    "publication_date": "1987-06-01",
    "publication_year": 1987,
    "authors": "Carla Schlatter Ellis",
    "corresponding_authors": "Carla Schlatter Ellis",
    "abstract": "Concurrent access to complex shared data structures, particularly structures useful as database indices, has long been of interest in the database community. In dynamic databases, tree structures such as B-trees have been used as indices because of their ability to handle growth; whereas hashing has been used for fast access in relatively static databases. Recently, a number of techniques for dynamic hashing have appeared. They address the major deficiency of traditional hashing when applied to databases that experience significant change in the amount of data being stored. This paper presents a solution that allows concurrency in one of these dynamic hashing data structures, namely linear hash files. The solution is based on locking protocols and minor modifications in the data structures.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W1993101773",
    "type": "article"
  },
  {
    "title": "Database states and their tableaux",
    "doi": "https://doi.org/10.1145/329.318579",
    "publication_date": "1984-06-03",
    "publication_year": 1984,
    "authors": "Alberto O. Mendelzon",
    "corresponding_authors": "Alberto O. Mendelzon",
    "abstract": "Recent work considers a database state to satisfy a set of dependencies if there exists a satisfying universal relation whose projections contain each of the relations in the state. Such relations are called weak instances for the state. We propose the set of all weak instances for a state as an embodiment of the information represented by the state. We characterize states that have the same set of weak instances by the equivalence of their associated tableaux. We apply this notion to the comparison of database schemes and characterize all pairs of schemes such that for every legal state of one of them there exists an equivalent legal state of the other one. We use this approach to provide a new characterization of Boyce-Codd Normal Form relation schemes.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2113633436",
    "type": "article"
  },
  {
    "title": "Adaptive record clustering",
    "doi": "https://doi.org/10.1145/3857.3861",
    "publication_date": "1985-06-01",
    "publication_year": 1985,
    "authors": "C. Yu; Cheing-mei Suen; K.C. Lam; Man‐Keung Siu",
    "corresponding_authors": "",
    "abstract": "An algorithm for record clustering is presented. It is capable of detecting sudden changes in users' access patterns and then suggesting an appropriate assignment of records to blocks. It is conceptually simple, highly intuitive, does not need to classify queries into types, and avoids collecting individual query statistics. Experimental results indicate that it converges rapidly; its performance is about 50 percent better than that of the total sort method, and about 100 percent better than that of randomly assigning records to blocks.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W1988475829",
    "type": "article"
  },
  {
    "title": "Sketches for size of join estimation",
    "doi": "https://doi.org/10.1145/1386118.1386121",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Florin Rusu; Alin Dobra",
    "corresponding_authors": "",
    "abstract": "Sketching techniques provide approximate answers to aggregate queries both for data-streaming and distributed computation. Small space summaries that have linearity properties are required for both types of applications. The prevalent method for analyzing sketches uses moment analysis and distribution-independent bounds based on moments. This method produces clean, easy to interpret, theoretical bounds that are especially useful for deriving asymptotic results. However, the theoretical bounds obscure fine details of the behavior of various sketches and they are mostly not indicative of which type of sketches should be used in practice. Moreover, no significant empirical comparison between various sketching techniques has been published, which makes the choice even harder. In this article we take a close look at the sketching techniques proposed in the literature from a statistical point of view with the goal of determining properties that indicate the actual behavior and producing tighter confidence bounds. Interestingly, the statistical analysis reveals that two of the techniques, Fast-AGMS and Count-Min, provide results that are in some cases orders of magnitude better than the corresponding theoretical predictions. We conduct an extensive empirical study that compares the different sketching techniques in order to corroborate the statistical analysis with the conclusions we draw from it. The study indicates the expected performance of various sketches, which is crucial if the techniques are to be used by practitioners. The overall conclusion of the study is that Fast-AGMS sketches are, for the full spectrum of problems, either the best, or close to the best, sketching technique. We apply the insights obtained from the statistical study and the experimental results to design effective algorithms for sketching interval data. We show how the two basic methods for sketching interval data, DMAP and fast range-summation, can be improved significantly with respect to the update time without a significant loss in accuracy. The gain in update time can be as large as two orders of magnitude, thus making the improved methods practical. The empirical study suggests that DMAP is preferable when update time is the critical requirement and fast range-summation is desirable for better accuracy.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2066588467",
    "type": "article"
  },
  {
    "title": "The determination of efficient record segmentations and blocking factors for shared data files",
    "doi": "https://doi.org/10.1145/320557.320574",
    "publication_date": "1977-09-01",
    "publication_year": 1977,
    "authors": "Salvatore T. March; Dennis G. Serverance",
    "corresponding_authors": "",
    "abstract": "It is generally believed that 80 percent of all retrieval from a commercial database is directed at only 20 percent of the stored data items. By partitioning data items into primary and secondary record segments, storing them in physically separate files, and judiciously allocating available buffer space to the two files, it is possible to significantly reduce the average cost of information retrieval from a shared database. An analytic model, based upon knowledge of data item lengths, data access costs, and user retrieval patterns, is developed to assist an analyst with this assignment problem. A computationally tractable design algorithm is presented and results of its application are described.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2016768048",
    "type": "article"
  },
  {
    "title": "Efficient query processing on graph databases",
    "doi": "https://doi.org/10.1145/1508857.1508859",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "James Cheng; Yiping Ke; Wilfred Ng",
    "corresponding_authors": "",
    "abstract": "We study the problem of processing subgraph queries on a database that consists of a set of graphs. The answer to a subgraph query is the set of graphs in the database that are supergraphs of the query. In this article, we propose an efficient index, FG*-index , to solve this problem. The cost of processing a subgraph query using most existing indexes mainly consists of two parts: the index probing cost and the candidate verification cost. Index probing is to find the query in the index, or to find the graphs from which we can generate a candidate answer set for the query. Candidate verification is to test whether each graph in the candidate set is indeed a supergraph of the query. We design FG*-index to minimize these two costs as follows. FG*-index consists of three components: the FG-index , the feature-index , and the FAQ-index . First, the FG-index employs the concept of Frequent subGraph ( FG ) to allow the set of queries that are FGs to be answered without candidate verification. We call this set of queries FG-queries . We can enlarge the set of FG-queries so that more queries can be answered without candidate verification; however, a larger set of FG-queries implies a larger FG-index and hence the index probing cost also increases. We propose the feature-index to reduce the index probing cost. The feature-index uses features to filter false results that are matched in the FG-index, so that we can quickly find the truly matching graphs for a query. For processing non-FG-queries, we propose the FAQ-index, which is dynamically constructed from the set of Frequently Asked non-FG-Queries ( FAQs ). Using the FAQ-index, verification is not required for processing FAQs and only a small number of candidates need to be verified for processing non-FG-queries that are not frequently asked . Finally, a comprehensive set of experiments verifies that query processing using FG*-index is up to orders of magnitude more efficient than state-of-the-art indexes and it is also more scalable.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2079668713",
    "type": "article"
  },
  {
    "title": "Snapshot isolation and integrity constraints in replicated databases",
    "doi": "https://doi.org/10.1145/1538909.1538913",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Yi Lin; Bettina Kemme; Ricardo Jiménez; Marta Patiño-Martı́nez; José Enrique Armendáriz-Íñigo",
    "corresponding_authors": "",
    "abstract": "Database replication is widely used for fault tolerance and performance. However, it requires replica control to keep data copies consistent despite updates. The traditional correctness criterion for the concurrent execution of transactions in a replicated database is 1-copy-serializability. It is based on serializability, the strongest isolation level in a nonreplicated system. In recent years, however, Snapshot Isolation (SI), a slightly weaker isolation level, has become popular in commercial database systems. There exist already several replica control protocols that provide SI in a replicated system. However, most of the correctness reasoning for these protocols has been rather informal. Additionally, most of the work so far ignores the issue of integrity constraints. In this article, we provide a formal definition of 1-copy-SI using and extending a well-established definition of SI in a nonreplicated system. Our definition considers integrity constraints in a way that conforms to the way integrity constraints are handled in commercial systems. We discuss a set of necessary and sufficient conditions for a replicated history to be producible under 1-copy-SI. This makes our formalism a convenient tool to prove the correctness of replica control algorithms.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2112527958",
    "type": "article"
  },
  {
    "title": "Approximate continuous querying over distributed streams",
    "doi": "https://doi.org/10.1145/1366102.1366106",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Graham Cormode; Minos Garofalakis",
    "corresponding_authors": "",
    "abstract": "While traditional database systems optimize for performance on one-shot query processing, emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically distributed streams. Thus, effective solutions have to be simultaneously space/time efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. In a nutshell, our algorithms rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication- and space/time-efficient solutions. The end result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model. Experiments with real data validate our approach, revealing significant savings over naive solutions as well as our analytical worst-case guarantees.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W1995328140",
    "type": "article"
  },
  {
    "title": "A locking protocol for resource coordination in distributed databases",
    "doi": "https://doi.org/10.1145/320141.320143",
    "publication_date": "1980-06-01",
    "publication_year": 1980,
    "authors": "Daniel A. Menascé; Gerald J. Popek; Richard R. Muntz",
    "corresponding_authors": "",
    "abstract": "A locking protocol to coordinate access to a distributed database and to maintain system consistency throughout normal and abnormal conditions is presented. The proposed protocol is robust in the face of crashes of any participating site, as well as communication failures. Recovery from any number of failures during normal operation or any of the recovery stages is supported. Recovery is done in such a way that maximum forward progress is achieved by the recovery procedures. Integration of virtually any locking discipline including predicate lock methods is permitted by this protocol. The locking algorithm operates, and operates correctly, when the network is partitioned, either intentionally or by failure of communication lines. Each partition is able to continue with work local to it, and operation merges gracefully when the partitions are reconnected. A subroutine of the protocol, that assures reliable communication among sites, is shown to have better performance than two-phase commit methods. For many topologies of interest, the delay introduced by the overall protocol is not a direct function of the size of the network. The communications cost is shown to grow in a relatively slow, linear fashion with the number of sites participating in the transaction. An informal proof of the correctness of the algorithm is also presented in this paper. The algorithm has as its core a centralized locking protocol with distributed recovery procedures. A centralized controller with local appendages at each site coordinates all resource control, with requests initiated by application programs at any site. However, no site experiences undue load. Recovery is broken down into three disjoint mechanisms: for single node recovery, merge of partitions, and reconstruction of the centralized controller and tables. The disjointness of the mechanisms contributes to comprehensibility and ease of proof. The paper concludes with a proposal for an extension aimed at optimizing operation of the algorithm to adapt to highly skewed distributions of activity. The extension applies nicely to interconnected computer networks.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2025687007",
    "type": "article"
  },
  {
    "title": "Implementing a generalized access path structure for a relational database system",
    "doi": "https://doi.org/10.1145/320263.320284",
    "publication_date": "1978-09-01",
    "publication_year": 1978,
    "authors": "Theo Haerder",
    "corresponding_authors": "Theo Haerder",
    "abstract": "A new kind of implementation technique for access paths connecting sets of tuples qualified by attribute values is described. It combines the advantages of pointer chain and multilevel index implementation techniques. Compared to these structures the generalized access path structure is at least competitive in performing retrieval and update operations, while a considerable storage space saving is gained. Some additional features of this structure support m -way joins and the evaluation of multirelation queries, and allow efficient checks of integrity assertions and simple reorganization schemes.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2099993385",
    "type": "article"
  },
  {
    "title": "Space-optimal heavy hitters with strong error bounds",
    "doi": "https://doi.org/10.1145/1862919.1862923",
    "publication_date": "2010-10-12",
    "publication_year": 2010,
    "authors": "Radu Berinde; Piotr Indyk; Graham Cormode; Martin J. Strauss",
    "corresponding_authors": "",
    "abstract": "The problem of finding heavy hitters and approximating the frequencies of items is at the heart of many problems in data stream analysis. It has been observed that several proposed solutions to this problem can outperform their worst-case guarantees on real data. This leads to the question of whether some stronger bounds can be guaranteed. We answer this in the positive by showing that a class of counter-based algorithms (including the popular and very space-efficient Frequent and SpacesSaving algorithms) provides much stronger approximation guarantees than previously known. Specifically, we show that errors in the approximation of individual elements do not depend on the frequencies of the most frequent elements, but only on the frequency of the remaining tail. This shows that counter-based methods are the most space-efficient (in fact, space-optimal) algorithms having this strong error bound. This tail guarantee allows these algorithms to solve the sparse recovery problem. Here, the goal is to recover a faithful representation of the vector of frequencies, f . We prove that using space O ( k ), the algorithms construct an approximation f * to the frequency vector f so that the L 1 error ∥∥ f −∥ f *∥ 1 is close to the best possible error min f ′ ∥ f ′ − f ∥ 1 , where f′ ranges over all vectors with at most k non-zero entries. This improves the previously best known space bound of about O ( k log n ) for streams without element deletions (where n is the size of the domain from which stream elements are drawn). Other consequences of the tail guarantees are results for skewed (Zipfian) data, and guarantees for accuracy of merging multiple summarized streams.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2056012370",
    "type": "article"
  },
  {
    "title": "Minimum cost selection of secondary indexes for formatted files",
    "doi": "https://doi.org/10.1145/320521.320537",
    "publication_date": "1977-03-01",
    "publication_year": 1977,
    "authors": "Henry Dwight Anderson; P. Bruce Berra",
    "corresponding_authors": "",
    "abstract": "Secondary indexes are often used in database management systems for secondary key retrieval. Although their use can improve retrieval time significantly, the cost of index maintenance and storage increases the overhead of the file processing application. The optimal set of indexed secondary keys for a particular application depends on a number of application dependent factors. In this paper a cost function is developed for the evaluation of candidate indexing choices and applied to the optimization of index selection. Factors accounted for include file size, the relative rates of retrieval and maintenance and the distribution of retrieval and maintenance over the candidate keys, index structure, and system charging rates. Among the results demonstrated are the increased effectiveness of secondary indexes for large files, the effect of the relative rates of retrieval and maintenance, the greater cost of allowing for arbitrarily formulated queries, and the impact on cost of the use of different index structures.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2061200062",
    "type": "article"
  },
  {
    "title": "Security in statistical databases for queries with small counts",
    "doi": "https://doi.org/10.1145/320241.320250",
    "publication_date": "1978-03-01",
    "publication_year": 1978,
    "authors": "Francis Y. L. Chin",
    "corresponding_authors": "Francis Y. L. Chin",
    "abstract": "The security problem of statistical databases containing anonymous but individual records which may be evaluated by queries about sums and averages is considered. A model, more realistic than the previous ones, is proposed, in which nonexisting records for some keys can be allowed. Under the assumption that the system protects the individual's information by the well-known technique which avoids publishing summaries with small counts, several properties about the system and a necessary and sufficient condition for compromising the database have been derived. The minimum number of queries needed to compromise the database is also discussed.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2062882367",
    "type": "article"
  },
  {
    "title": "Information preserving XML schema embedding",
    "doi": "https://doi.org/10.1145/1331904.1331908",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Wenfei Fan; Philip Bohannon",
    "corresponding_authors": "",
    "abstract": "A fundamental concern of data integration in an XML context is the ability to embed one or more source documents in a target document so that (a) the target document conforms to a target schema and (b) the information in the source documents is preserved . In this paper, information preservation for XML is formally studied, and the results of this study guide the definition of a novel notion of schema embedding between two XML DTD schemas represented as graphs. Schema embedding generalizes the conventional notion of graph similarity by allowing an edge in a source DTD schema to be mapped to a path in the target DTD. Instance-level embeddings can be derived from the schema embedding in a straightforward manner, such that conformance to a target schema and information preservation are guaranteed. We show that it is NP-complete to find an embedding between two DTD schemas. We also outline efficient heuristic algorithms to find candidate embeddings, which have proved effective by our experimental study. These yield the first systematic and effective approach to finding information preserving XML mappings.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2019540568",
    "type": "article"
  },
  {
    "title": "Decompositions and functional dependencies in relations",
    "doi": "https://doi.org/10.1145/320610.320620",
    "publication_date": "1980-12-01",
    "publication_year": 1980,
    "authors": "William W. Armstrong; C. Deobel",
    "corresponding_authors": "",
    "abstract": "A general study is made of two basic integrity constraints on relations: functional and multivalued dependencies. The latter are studied via an equivalent concept: decompositions. A model is constructed for any possible combination of functional dependencies and decompositions. The model embodies some decompositions as unions of relations having different schemata of functional dependencies. This suggests a new, stronger integrity constraint, the degenerate decomposition. More generally, the theory demonstrates the importance of using the union operation in database design and of allowing different schemata on the operands of a union. Techniques based on the union lead to a method for solving the problem of membership of a decomposition in the closure of a given set of functional dependencies and decompositions. The concept of antiroot is introduced as a tool for describing families of decompositions, and its fundamental importance for database design is indicated.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2142968084",
    "type": "article"
  },
  {
    "title": "Batched searching of sequential and tree structured files",
    "doi": "https://doi.org/10.1145/320473.320487",
    "publication_date": "1976-09-01",
    "publication_year": 1976,
    "authors": "Ben Shneiderman",
    "corresponding_authors": "Ben Shneiderman",
    "abstract": "The technique of batching searches has been ignored in the context of disk based online data retrieval systems. This paper suggests that batching be reconsidered for such systems since the potential reduction in processor demand may actually reduce response time. An analysis with sample numerical results and algorithms is presented.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2031226847",
    "type": "article"
  },
  {
    "title": "Restructuring for large databases",
    "doi": "https://doi.org/10.1145/320455.320461",
    "publication_date": "1976-06-01",
    "publication_year": 1976,
    "authors": "Shamkant B. Navathe; James P. Fry",
    "corresponding_authors": "",
    "abstract": "The development of a powerful restructuring function involves two important components—the unambiguous specification of the restructuring operations and the realization of these operations in a software system. This paper is directed to the first component in the belief that a precise specification will provide a firm foundation for the development of restructuring algorithms and, subsequently, their implementation. The paper completely defines the semantics of the restructuring of tree structured databases. The delineation of the restructuring function is accomplished by formulating three different levels of abstraction, with each level of abstraction representing successively more detailed semantics of the function. At the first level of abstraction, the schema modification, three types are identified—naming, combining, and relating; these three types are further divided into eight schema operations. The second level of abstraction, the instance operations, constitutes the transformations on the data instances; they are divided into group operations such as replication, factoring, union, etc., and group relation operations such as collapsing, refinement, fusion, etc. The final level, the item value operations, includes the actual item operations, such as copy value, delete value, or create a null value.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2012032026",
    "type": "article"
  },
  {
    "title": "Hashing and trie algorithms for partial match retrieval",
    "doi": "https://doi.org/10.1145/320455.320469",
    "publication_date": "1976-06-01",
    "publication_year": 1976,
    "authors": "Walter A. Burkhard",
    "corresponding_authors": "Walter A. Burkhard",
    "abstract": "File designs suitable for retrieval from a file of k -letter words when queries may be only partially specified are examined. A new class of partial match file designs (called PMF designs) based upon hash coding and trie search algorithms which provide good worst-case performance is introduced. Upper bounds on the worst-case performance of these designs are given along with examples of files achieving the bound. Other instances of PMF designs are known to have better worst-case performances. The implementation of the file designs with associated retrieval algorithms is considered. The amount of storage required is essentially that required of the records themselves.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W1999501660",
    "type": "article"
  },
  {
    "title": "The implication problem of data dependencies over SQL table definitions",
    "doi": "https://doi.org/10.1145/2188349.2188355",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Sven Hartmann; Sebastian Link",
    "corresponding_authors": "",
    "abstract": "We investigate the implication problem for classes of data dependencies over SQL table definitions. Under Zaniolo's “no information” interpretation of null markers we establish an axiomatization and algorithms to decide the implication problem for the combined class of functional and multivalued dependencies in the presence of NOT NULL constraints. The resulting theory subsumes three previously orthogonal frameworks. We further show that the implication problem of this class is equivalent to that in a propositional fragment of Schaerf and Cadoli's [1995] family of para-consistent S-3 logics. In particular, S is the set of variables that correspond to attributes declared NOT NULL. We also show how our equivalences for multivalued dependencies can be extended to Delobel's class of full first-order hierarchical decompositions, and the equivalences for functional dependencies can be extended to arbitrary Boolean dependencies. These dualities allow us to transfer several findings from the propositional fragments to the corresponding classes of data dependencies, and vice versa. We show that our results also apply to Codd's null interpretation “value unknown at present”, but not to Imielinski's [1989] or-relations utilizing Levene and Loizou's weak possible world semantics [Levene and Loizou 1998]. Our findings establish NOT NULL constraints as an effective mechanism to balance not only the certainty in database relations but also the expressiveness with the efficiency of entailment relations. They also control the degree by which the implication of data dependencies over total relations is soundly approximated in SQL table definitions.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W1975125888",
    "type": "article"
  },
  {
    "title": "Output privacy in data mining",
    "doi": "https://doi.org/10.1145/1929934.1929935",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "Ting Wang; Ling Liu",
    "corresponding_authors": "",
    "abstract": "Privacy has been identified as a vital requirement in designing and implementing data mining systems. In general, privacy preservation demands protecting both input and output privacy: the former refers to sanitizing the raw data itself before performing mining; while the latter refers to preventing the mining output (models or patterns) from malicious inference attacks. This article presents a systematic study on the problem of protecting output privacy in data mining, and particularly, stream mining: (i) we highlight the importance of this problem by showing that even sufficient protection of input privacy does not guarantee that of output privacy; (ii) we present a general inferencing and disclosure model that exploits the intrawindow and interwindow privacy breaches in stream mining output; (iii) we propose a light-weighted countermeasure that effectively eliminates these breaches without explicitly detecting them, while minimizing the loss of output accuracy; (iv) we further optimize the basic scheme by taking account of two types of semantic constraints, aiming at maximally preserving utility-related semantics while maintaining hard privacy guarantee; (v) finally, we conduct extensive experimental evaluation over both synthetic and real data to validate the efficacy of our approach.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2007933321",
    "type": "article"
  },
  {
    "title": "Privacy-aware location data publishing",
    "doi": "https://doi.org/10.1145/1806907.1806910",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Haibo Hu; Jianliang Xu; Sai Tung On; Jing Du; Joseph Kee‐Yin Ng",
    "corresponding_authors": "",
    "abstract": "This article examines a new problem of k -anonymity with respect to a reference dataset in privacy-aware location data publishing: given a user dataset and a sensitive event dataset, we want to generalize the user dataset such that by joining it with the event dataset through location, each event is covered by at least k users. Existing k -anonymity algorithms generalize every k user locations to the same vague value, regardless of the events. Therefore, they tend to overprotect against the privacy compromise and make the published data less useful. In this article, we propose a new generalization paradigm called local enlargement , as opposed to conventional hierarchy- or partition-based generalization. Local enlargement guarantees that user locations are enlarged just enough to cover all events k times, and thus maximize the usefulness of the published data. We develop an O ( H n )-approximate algorithm under the local enlargement paradigm, where n is the maximum number of events a user could possibly cover and H n is the Harmonic number of n . With strong pruning techniques and mathematical analysis, we show that it runs efficiently and that the generalized user locations are up to several orders of magnitude smaller than those by the existing algorithms. In addition, it is robust enough to protect against various privacy attacks.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2025483242",
    "type": "article"
  },
  {
    "title": "The complexity of regular expressions and property paths in SPARQL",
    "doi": "https://doi.org/10.1145/2494529",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Katja Losemann; Wim Martens",
    "corresponding_authors": "",
    "abstract": "The World Wide Web Consortium (W3C) recently introduced property paths in SPARQL 1.1, a query language for RDF data. Property paths allow SPARQL queries to evaluate regular expressions over graph-structured data. However, they differ from standard regular expressions in several notable aspects. For example, they have a limited form of negation, they have numerical occurrence indicators as syntactic sugar, and their semantics on graphs is defined in a nonstandard manner. We formalize the W3C semantics of property paths and investigate various query evaluation problems on graphs. More specifically, let x and y be two nodes in an edge-labeled graph and r be an expression. We study the complexities of: (1) deciding whether there exists a path from x to y that matches r and (2) counting how many paths from x to y match r . Our main results show that, compared to an alternative semantics of regular expressions on graphs, the complexity of (1) and (2) under W3C semantics is significantly higher. Whereas the alternative semantics remains in polynomial time for large fragments of expressions, the W3C semantics makes problems (1) and (2) intractable almost immediately. As a side-result, we prove that the membership problem for regular expressions with numerical occurrence indicators and negation is in polynomial time.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2045978585",
    "type": "article"
  },
  {
    "title": "Characterizing schema mappings via data examples",
    "doi": "https://doi.org/10.1145/2043652.2043656",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Bogdan Alexe; Balder ten Cate; Phokion G. Kolaitis; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "Schema mappings are high-level specifications that describe the relationship between two database schemas; they are considered to be the essential building blocks in data exchange and data integration, and have been the object of extensive research investigations. Since in real-life applications schema mappings can be quite complex, it is important to develop methods and tools for understanding, explaining, and refining schema mappings. A promising approach to this effect is to use “good” data examples that illustrate the schema mapping at hand. We develop a foundation for the systematic investigation of data examples and obtain a number of results on both the capabilities and the limitations of data examples in explaining and understanding schema mappings. We focus on schema mappings specified by source-to-target tuple generating dependencies (s-t tgds) and investigate the following problem: which classes of s-t tgds can be “uniquely characterized” by a finite set of data examples? Our investigation begins by considering finite sets of positive and negative examples, which are arguably the most natural choice of data examples. However, we show that they are not powerful enough to yield interesting unique characterizations. We then consider finite sets of universal examples, where a universal example is a pair consisting of a source instance and a universal solution for that source instance. We first show that unique characterizations via universal examples is, in a precise sense, equivalent to the existence of Armstrong bases (a relaxation of the classical notion of Armstrong databases). After this, we show that every schema mapping specified by LAV s-t tgds is uniquely characterized by a finite set of universal examples with respect to the class of LAV s-t tgds. Moreover, this positive result extends to the much broader classes of n -modular schema mappings, n a positive integer. Finally, we study the unique characterizability of GAV schema mappings. It turns out that some GAV schema mappings are uniquely characterizable by a finite set of universal examples with respect to the class of GAV s-t tgds, while others are not. By unveiling a tight connection with homomorphism dualities, we establish an effective, sound, and complete criterion for determining whether or not a GAV schema mapping is uniquely characterizable by a finite set of universal examples with respect to the class of GAV s-t tgds.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2077397644",
    "type": "article"
  },
  {
    "title": "Extending string similarity join to tolerant fuzzy token matching",
    "doi": "https://doi.org/10.1145/2535628",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Jiannan Wang; Guoliang Li; Jianhua Feng",
    "corresponding_authors": "",
    "abstract": "String similarity join that finds similar string pairs between two string sets is an essential operation in many applications and has attracted significant attention recently in the database community. A significant challenge in similarity join is to implement an effective fuzzy match operation to find all similar string pairs which may not match exactly. In this article, we propose a new similarity function, called fuzzy-token-matching-based similarity which extends token-based similarity functions (e.g., jaccard similarity and cosine similarity) by allowing fuzzy match between two tokens. We study the problem of similarity join using this new similarity function and present a signature-based method to address this problem. We propose new signature schemes and develop effective pruning techniques to improve the performance. We also extend our techniques to support weighted tokens. Experimental results show that our method achieves high efficiency and result quality and significantly outperforms state-of-the-art approaches.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2170478076",
    "type": "article"
  },
  {
    "title": "Using structural information in XML keyword search effectively",
    "doi": "https://doi.org/10.1145/1929934.1929938",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "Arash Termehchy; Marianne Winslett",
    "corresponding_authors": "",
    "abstract": "The popularity of XML has exacerbated the need for an easy-to-use, high precision query interface for XML data. When traditional document-oriented keyword search techniques do not suffice, natural language interfaces and keyword search techniques that take advantage of XML structure make it very easy for ordinary users to query XML databases. Unfortunately, current approaches to processing these queries rely heavily on heuristics that are intuitively appealing but ultimately ad hoc. These approaches often retrieve false positive answers, overlook correct answers, and cannot rank answers appropriately. To address these problems for data-centric XML, we propose coherency ranking (CR), a domain- and database design-independent ranking method for XML keyword queries that is based on an extension of the concepts of data dependencies and mutual information. With coherency ranking, the results of a keyword query are invariant under a class of equivalency-preserving schema reorganizations. We analyze the way in which previous approaches to XML keyword search approximate coherency ranking, and present efficient algorithms to process queries and rank their answers using coherency ranking. Our empirical evaluation with two real-world XML data sets shows that coherency ranking has better precision and recall and provides better ranking than all previous approaches.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2073537868",
    "type": "article"
  },
  {
    "title": "Design and analysis of a ranking approach to private location-based services",
    "doi": "https://doi.org/10.1145/1966385.1966388",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Man Lung Yiu; Christian S. Jensen; Jesper Møller; Hua Lu",
    "corresponding_authors": "",
    "abstract": "Users of mobile services wish to retrieve nearby points of interest without disclosing their locations to the services. This article addresses the challenge of optimizing the query performance while satisfying given location privacy and query accuracy requirements. The article's proposal, SpaceTwist, aims to offer location privacy for k nearest neighbor (kNN) queries at low communication cost without requiring a trusted anonymizer. The solution can be used with a conventional DBMS as well as with a server optimized for location-based services. In particular, we believe that this is the first solution that expresses the server-side functionality in a single SQL statement. In its basic form, SpaceTwist utilizes well-known incremental NN query processing on the server. When augmented with a server-side granular search technique, SpaceTwist is capable of exploiting relaxed query accuracy guarantees for obtaining better performance. We extend SpaceTwist with so-called ring ranking, which improves the communication cost, delayed termination, which improves the privacy afforded the user, and the ability to function in spatial networks in addition to Euclidean space. We report on analytical and empirical studies that offer insight into the properties of SpaceTwist and suggest that our proposal is indeed capable of offering privacy with very good performance in realistic settings.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2166636216",
    "type": "article"
  },
  {
    "title": "I/O-Efficient Algorithms on Triangle Listing and Counting",
    "doi": "https://doi.org/10.1145/2691190.2691193",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Xiaocheng Hu; Yufei Tao; Chin‐Wan Chung",
    "corresponding_authors": "",
    "abstract": "This article studies I/O-efficient algorithms for the triangle listing problem and the triangle counting problem , whose solutions are basic operators in dealing with many other graph problems. In the former problem, given an undirected graph G , the objective is to find all the cliques involving 3 vertices in G . In the latter problem, the objective is to report just the number of such cliques without having to enumerate them. Both problems have been well studied in internal memory, but still remain as difficult challenges when G does not fit in memory, thus making it crucial to minimize the number of disk I/Os performed. Although previous research has attempted to tackle these challenges, the state-of-the-art solutions rely on a set of crippling assumptions to guarantee good performance. Motivated by this, we develop a new algorithm that is provably I/O and CPU efficient at the same time, without making any assumption on the input G at all. The algorithm uses ideas drastically different from all the previous approaches, and outperforms the existing competitors by a factor of over an order of magnitude in our extensive experimentation.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2080312846",
    "type": "article"
  },
  {
    "title": "Computational Fact Checking through Query Perturbations",
    "doi": "https://doi.org/10.1145/2996453",
    "publication_date": "2017-01-09",
    "publication_year": 2017,
    "authors": "You Wu; Pankaj K. Agarwal; Chengkai Li; Jun Yang; Cong Yu",
    "corresponding_authors": "",
    "abstract": "Our media is saturated with claims of “facts” made from data. Database research has in the past focused on how to answer queries, but has not devoted much attention to discerning more subtle qualities of the resulting claims, for example, is a claim “cherry-picking”? This article proposes a framework that models claims based on structured data as parameterized queries. Intuitively, with its choice of the parameter setting, a claim presents a particular (and potentially biased) view of the underlying data. A key insight is that we can learn a lot about a claim by “perturbing” its parameters and seeing how its conclusion changes. For example, a claim is not robust if small perturbations to its parameters can change its conclusions significantly. This framework allows us to formulate practical fact-checking tasks—reverse-engineering vague claims, and countering questionable claims—as computational problems. Along with the modeling framework, we develop an algorithmic framework that enables efficient instantiations of “meta” algorithms by supplying appropriate algorithmic building blocks. We present real-world examples and experiments that demonstrate the power of our model, efficiency of our algorithms, and usefulness of their results.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2567809919",
    "type": "article"
  },
  {
    "title": "Random Walk with Restart on Large Graphs Using Block Elimination",
    "doi": "https://doi.org/10.1145/2901736",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Jinhong Jung; Kijung Shin; Lee Sael; U Kang",
    "corresponding_authors": "",
    "abstract": "Given a large graph, how can we calculate the relevance between nodes fast and accurately? Random walk with restart (RWR) provides a good measure for this purpose and has been applied to diverse data mining applications including ranking, community detection, link prediction, and anomaly detection. Since calculating RWR from scratch takes a long time, various preprocessing methods, most of which are related to inverting adjacency matrices, have been proposed to speed up the calculation. However, these methods do not scale to large graphs because they usually produce large dense matrices that do not fit into memory. In addition, the existing methods are inappropriate when graphs dynamically change because the expensive preprocessing task needs to be computed repeatedly. In this article, we propose B ear , a fast, scalable, and accurate method for computing RWR on large graphs. B ear has two versions: a preprocessing method B ear S for static graphs and an incremental update method B ear D for dynamic graphs. B ear S consists of the preprocessing step and the query step. In the preprocessing step, B ear S reorders the adjacency matrix of a given graph so that it contains a large and easy-to-invert submatrix, and precomputes several matrices including the Schur complement of the submatrix. In the query step, B ear S quickly computes the RWR scores for a given query node using a block elimination approach with the matrices computed in the preprocessing step. For dynamic graphs, B ear D efficiently updates the changed parts in the preprocessed matrices of B ear S based on the observation that only small parts of the preprocessed matrices change when few edges are inserted or deleted. Through extensive experiments, we show that B ear S significantly outperforms other state-of-the-art methods in terms of preprocessing and query speed, space efficiency, and accuracy. We also show that B ear D quickly updates the preprocessed matrices and immediately computes queries when the graph changes.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2378467122",
    "type": "article"
  },
  {
    "title": "Dichotomies for Queries with Negation in Probabilistic Databases",
    "doi": "https://doi.org/10.1145/2877203",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "Robert Fink; Dan Olteanu",
    "corresponding_authors": "",
    "abstract": "This article charts the tractability frontier of two classes of relational algebra queries in tuple-independent probabilistic databases. The first class consists of queries with join, projection, selection, and negation but without repeating relation symbols and union. The second class consists of quantified queries that express the following binary relationships among sets of entities: set division, set inclusion, set equivalence, and set incomparability. Quantified queries are expressible in relational algebra using join, projection, nested negation, and repeating relation symbols. Each query in the two classes has either polynomial-time or #P-hard data complexity and the tractable queries can be recognised efficiently. Our result for the first query class extends a known dichotomy for conjunctive queries without self-joins to such queries with negation. For quantified queries, their tractability is sensitive to their outermost projection operator: They are tractable if no attribute representing set identifiers is projected away and #P-hard otherwise.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2284810639",
    "type": "article"
  },
  {
    "title": "From a Comprehensive Experimental Survey to a Cost-based Selection Strategy for Lightweight Integer Compression Algorithms",
    "doi": "https://doi.org/10.1145/3323991",
    "publication_date": "2019-06-17",
    "publication_year": 2019,
    "authors": "Patrick Damme; Annett Ungethüm; Juliana Hildebrandt; Dirk Habich; Wolfgang Lehner",
    "corresponding_authors": "",
    "abstract": "Lightweight integer compression algorithms are frequently applied in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the vectorization of basic techniques such as delta coding and null suppression has considerably enlarged the corpus of available algorithms. As a result, today there is a large number of algorithms to choose from, while different algorithms are tailored to different data characteristics. However, a comparative evaluation of these algorithms with different data and hardware characteristics has never been sufficiently conducted in the literature. To close this gap, we conducted an exhaustive experimental survey by evaluating several state-of-the-art lightweight integer compression algorithms as well as cascades of basic techniques. We systematically investigated the influence of data as well as hardware properties on the performance and the compression rates. The evaluated algorithms are based on publicly available implementations as well as our own vectorized reimplementations. We summarize our experimental findings leading to several new insights and to the conclusion that there is no single-best algorithm. Moreover, in this article, we also introduce and evaluate a novel cost model for the selection of a suitable lightweight integer compression algorithm for a given dataset.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2950461515",
    "type": "article"
  },
  {
    "title": "Optimizing One-time and Continuous Subgraph Queries using Worst-case Optimal Joins",
    "doi": "https://doi.org/10.1145/3446980",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Amine Mhedhbi; Chathura Kankanamge; Semih Salihoğlu",
    "corresponding_authors": "",
    "abstract": "We study the problem of optimizing one-time and continuous subgraph queries using the new worst-case optimal join plans. Worst-case optimal plans evaluate queries by matching one query vertex at a time using multiway intersections. The core problem in optimizing worst-case optimal plans is to pick an ordering of the query vertices to match. We make two main contributions: 1. A cost-based dynamic programming optimizer for one-time queries that (i) picks efficient query vertex orderings for worst-case optimal plans and (ii) generates hybrid plans that mix traditional binary joins with worst-case optimal style multiway intersections. In addition to our optimizer, we describe an adaptive technique that changes the query vertex orderings of the worst-case optimal subplans during query execution for more efficient query evaluation. The plan space of our one-time optimizer contains plans that are not in the plan spaces based on tree decompositions from prior work. 2. A cost-based greedy optimizer for continuous queries that builds on the delta subgraph query framework. Given a set of continuous queries, our optimizer decomposes these queries into multiple delta subgraph queries, picks a plan for each delta query, and generates a single combined plan that evaluates all of the queries. Our combined plans share computations across operators of the plans for the delta queries if the operators perform the same intersections. To increase the amount of computation shared, we describe an additional optimization that shares partial intersections across operators. Our optimizers use a new cost metric for worst-case optimal plans called intersection-cost . When generating hybrid plans, our dynamic programming optimizer for one-time queries combines intersection-cost with the cost of binary joins. We demonstrate the effectiveness of our plans, adaptive technique, and partial intersection sharing optimization through extensive experiments. Our optimizers are integrated into GraphflowDB.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3166524313",
    "type": "article"
  },
  {
    "title": "Influence Maximization Revisited: Efficient Sampling with Bound Tightened",
    "doi": "https://doi.org/10.1145/3533817",
    "publication_date": "2022-05-19",
    "publication_year": 2022,
    "authors": "Qintian Guo; Sibo Wang; Zhewei Wei; Wenqing Lin; Jing Tang",
    "corresponding_authors": "",
    "abstract": "Given a social network G with n nodes and m edges, a positive integer k , and a cascade model C , the influence maximization (IM) problem asks for k nodes in G such that the expected number of nodes influenced by the k nodes under cascade model C is maximized. The state-of-the-art approximate solutions run in O(k(n+m) log n/ ε 2 ) expected time while returning a (1 - 1/ e - ε) approximate solution with at least 1 - 1/ n probability. A key phase of these IM algorithms is the random reverse reachable (RR) set generation, and this phase significantly affects the efficiency and scalability of the state-of-the-art IM algorithms. In this article, we present a study on this key phase and propose an efficient random RR set generation algorithm under IC model. With the new algorithm, we show that the expected running time of existing IM algorithms under IC model can be improved to O(k ċ n log n ċ 2 ), when for any node v , the total weight of its incoming edges is no larger than a constant. For the general IC model where the weights are skewed, we present a sampling algorithm SKIP. To the best of our knowledge, it is the first index-free algorithm that achieves the optimal time complexity of the sorted subset sampling problem. Moreover, existing approximate IM algorithms suffer from scalability issues in high influence networks where the size of random RR sets is usually quite large. We tackle this challenging issue by reducing the average size of random RR sets without sacrificing the approximation guarantee. The proposed solution is orders of magnitude faster than states of the art as shown in our experiment. Besides, we investigate the issues of forward propagation and derive its time complexity with our proposed subset sampling techniques. We also present a heuristic condition to indicate when the forward propagation approach should be utilized to estimate the expected influence of a given seed set.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4280600316",
    "type": "article"
  },
  {
    "title": "An algebraic approach to static analysis of active database rules",
    "doi": "https://doi.org/10.1145/363951.363954",
    "publication_date": "2000-09-01",
    "publication_year": 2000,
    "authors": "Elena Baralis; Jennifer Widom",
    "corresponding_authors": "",
    "abstract": "Rules in active database systems can be very difficult to program due to the unstructured and unpredictable nature of rule processing. We provide static analysis techniques for predicting whether a given rule set is guaranteed to terminate and whether rule execution is confluent (guaranteed to have a unique final state). Our methods are based on previous techniques for analyzing rules in active database systems. We improve considerably on the previous techniques by providing analysis criteria that are much less conservative: our methods often determine that a rule set will terminate or is confluent when previous methods could not make this determination. Our improved analysis is based on a “propagation” algorithm, which uses an extended relational algebra to accurately determine when the action of one rule can affect the condition of another, and determine when rule actions commute. We consider both conditon-action rules and event-condition-action-rules, making our approach widely applicable to relational active database rule languages and to the trigger language in the SQL:1999 standard.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2068535784",
    "type": "article"
  },
  {
    "title": "A case for dynamic view management",
    "doi": "https://doi.org/10.1145/503099.503100",
    "publication_date": "2001-12-01",
    "publication_year": 2001,
    "authors": "Yannis Kotidis; Nick Roussopoulos",
    "corresponding_authors": "",
    "abstract": "Materialized aggregate views represent a set of redundant entities in a data warehouse that are frequently used to accelerate On-Line Analytical Processing (OLAP). Due to the complex structure of the data warehouse and the different profiles of the users who submit queries, there is need for tools that will automate and ease the view selection and management processes. In this article we present DynaMat, a system that manages dynamic collections of materialized aggregate views in a data warehouse. At query time, DynaMat utilizes a dedicated disk space for storing computed aggregates that are further engaged for answering new queries. Queries are executed independently or can be bundled within a multiquery expression. In the latter case, we present an execution mechanism that exploits dependencies among the queries and the materialized set to further optimize their execution. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We show how to derive an efficient update plan with respect to the available maintenance window, the different update policies for the views and the dependencies that exist among them.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2061751973",
    "type": "article"
  },
  {
    "title": "Security of random data perturbation methods",
    "doi": "https://doi.org/10.1145/331983.331986",
    "publication_date": "1999-12-01",
    "publication_year": 1999,
    "authors": "Krishnamurty Muralidhar; Rathindra Sarathy",
    "corresponding_authors": "",
    "abstract": "Statistical databases often use random data perturbation (RDP) methods to protect against disclosure of confidential numerical attributes. One of the key requirements of RDP methods is that they provide the appropriate level of security against snoopers who attempt to obtain information on confidential attributes through statistical inference. In this study, we evaluate the security provided by three methods of perturbation. The results of this study allow the database administrator to select the most effective RDP method that assures adequate protection against disclosure of confidential information.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2128082768",
    "type": "article"
  },
  {
    "title": "A normal form for precisely characterizing redundancy in nested relations",
    "doi": "https://doi.org/10.1145/227604.227612",
    "publication_date": "1996-03-01",
    "publication_year": 1996,
    "authors": "Wai Yin Mok; Yiu‐Kai Ng; David W. Embley",
    "corresponding_authors": "",
    "abstract": "We give a straightforward definition for redundancy in individual nested relations and define a new normal form that precisely characterizes redundancy for nested relations. We base our definition of redundancy on an arbitrary set of functional and multivalued dependencies, and show that our definition of nested normal form generalizes standard relational normalization theory. In addition, we give a condition that can prevent an unwanted structural anomaly in nested relations, namely, embedded nested relations with at most one tuple. Like other normal forms, our nested normal form can serve as a guide for database design.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2041601261",
    "type": "article"
  },
  {
    "title": "I/O reference behavior of production database workloads and the TPC benchmarks—an analysis at the logical level",
    "doi": "https://doi.org/10.1145/383734.383737",
    "publication_date": "2001-03-01",
    "publication_year": 2001,
    "authors": "Windsor W. Hsu; Alan Jay Smith; Honesty C. Young",
    "corresponding_authors": "",
    "abstract": "As improvements in processor performance continue to far outpace improvements in storage performance, I/O is increasingly the bottleneck in computer systems, especially in large database systems that manage huge amoungs of data. The key to achieving good I/O performance is to thoroughly understand its characteristics. In this article we present a comprehensive analysis of the logical I/O reference behavior of the peak productiondatabase workloads from ten of the world's largest corporations. In particular, we focus on how these workloads respond to different techniques for caching, prefetching, and write buffering. Our findings include several broadly applicable rules of thumb that describe how effective the various I/O optimization techniques are for the production workloads. For instance, our results indicate that the buffer pool miss ratio tends to be related to the ratio of buffer pool size to data size by an inverse square root rule. A similar fourth root rule relates the write miss ratio and the ration of buffer pool size to data size. In addition, we characterize the reference characteristics of workloads similar to the Transaction Processing Performance Council (TPC) benchmarks C (TPC-C) and D(TPC-D), which are de facto standard performance measures for online transaction processing (OLTP) systems and decision support systems (DSS), respectively. Since benchmarks such as TPC-C and TPC-D can only be used effectively if their strengths and limitations are understood, a major focus of our analysis is to identify aspects of the benchmarks that stress the system differently than the production workloads. We discover that for the most part, the reference behavior of TPC-C and TPC-D fall within the range of behavior exhibited by the production workloads. However, there are some noteworthy exceptions that affect well-known I/O optimization techniques such as caching (LRU is further from the optimal for TPC-C, while there is little sharing of pages between transactions for TPC-D), prefetching (TPC-C exhibits no significant sequentiality), and write buffering (write buffering is lees effective for the TPC benchmarks). While the two TPC benchmarks generally complement one another in reflecting the characteristics of the production workloads, there remain aspects of the real workloads that are not represented by either of the benchmarks.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W1965893058",
    "type": "article"
  },
  {
    "title": "Probabilistic wavelet synopses",
    "doi": "https://doi.org/10.1145/974750.974753",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Minos Garofalakis; Phillip B. Gibbons",
    "corresponding_authors": "",
    "abstract": "Recent work has demonstrated the effectiveness of the wavelet decomposition in reducing large amounts of data to compact sets of wavelet coefficients (termed \"wavelet synopses\") that can be used to provide fast and reasonably accurate approximate query answers. A major shortcoming of these existing wavelet techniques is that the quality of the approximate answers they provide varies widely, even for identical queries on nearly identical values in distinct parts of the data. As a result, users have no way of knowing whether a particular approximate answer is highly-accurate or off by many orders of magnitude. In this article, we introduce Probabilistic Wavelet Synopses , the first wavelet-based data reduction technique optimized for guaranteed accuracy of individual approximate answers. Whereas previous approaches rely on deterministic thresholding for selecting the wavelet coefficients to include in the synopsis, our technique is based on a novel, probabilistic thresholding scheme that assigns each coefficient a probability of being included based on its importance to the reconstruction of individual data values, and then flips coins to select the synopsis. We show how our scheme avoids the above pitfalls of deterministic thresholding, providing unbiased , highly accurate answers for individual data values in a data vector. We propose several novel optimization algorithms for tuning our probabilistic thresholding scheme to minimize desired error metrics. Experimental results on real-world and synthetic data sets evaluate these algorithms, and demonstrate the effectiveness of our probabilistic wavelet synopses in providing fast, highly accurate answers with improved quality guarantees.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2164363676",
    "type": "article"
  },
  {
    "title": "A method for automatic rule derivation to support semantic query optimization",
    "doi": "https://doi.org/10.1145/146931.146932",
    "publication_date": "1992-12-01",
    "publication_year": 1992,
    "authors": "Michael Siegel; Edward Sciore; Sharon C. Salveter",
    "corresponding_authors": "",
    "abstract": "The use of inference rules to support intelligent data processing is an increasingly important tool in many areas of computer science. In database systems, rules are used in semantic query optimization as a method for reducing query processing costs. The savings is dependent on the ability of experts to supply a set of useful rules and the ability of the optimizer to quickly find the appropriate transformations generated by these rules. Unfortunately, the most useful rules are not always those that would or could be specified by an expert. This paper describes the architecture of a system having two interrelated components: a combined conventional/semantic query optimizer, and an automatic rule deriver. Our automatic rule derivation method uses intermediate results from the optimization process to direct the search for learning new rules. Unlike a system employing only user-specified rules, a system with an automatic capability can derive rules that may be true only in the current state of the database and can modify the rule set to reflect changes in the database and its usage pattern. This system has been implemented as an extension of the EXODUS conventional query optimizer generator. We describe the implementation, and show how semantic query optimization is an extension of conventional optimization in this context.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W1972665620",
    "type": "article"
  },
  {
    "title": "Altruistic locking",
    "doi": "https://doi.org/10.1145/174638.174639",
    "publication_date": "1994-03-01",
    "publication_year": 1994,
    "authors": "Kenneth Salem; Héctor García-Molina; Jeannie Shands",
    "corresponding_authors": "",
    "abstract": "Long-lived transactions (LLTs) hold on to database resources for relatively long periods of time, significantly delaying the completion of shorter and more common transactions. To alleviate this problem we propose an extension to two-phase locking, called altruistic locking, whereby LLTs can release their locks early. Transactions that access this released data are said to run in the wake of the LLT and must follow special locking rules. Like two-phase locking, altruistic locking is easy to implement and guarantees serializability.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2295683682",
    "type": "article"
  },
  {
    "title": "Temporal FDs on complex objects",
    "doi": "https://doi.org/10.1145/310701.310715",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "Jef Wijsen",
    "corresponding_authors": "Jef Wijsen",
    "abstract": "Temporal functional dependencies (TFD) are defined for temporal databases that include object identity. It is argued that object identity can overcome certain semantic diffuculties with existing temporal relational data models. Practical applications of TFDs in object bases are discussed. Reasoning about TFDs is at the center of this paper. It turns out that the distinction between acyclic and cyclic schemas is significant. For acyclic schemas, a complete axiomatization for finite implication is given and an algorithm for deciding finite implication provided. The same axiomatization is proven complete for unrestricted implication in unrestricted schemas, which can be cyclic. An interesting result is that there are cyclic schemas for which unrestricted and finite implication do not coincide. TFDs relate and extend some earlier work on dependency theory in temporal databases. Throughout this paper, the construct of TFD is compared with the notion of temporal FD introduced by Wang et al. (1997). A comparison with other related work is provided at the end of the article.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W1988249571",
    "type": "article"
  },
  {
    "title": "A modified random perturbation method for database security",
    "doi": "https://doi.org/10.1145/174638.174641",
    "publication_date": "1994-03-01",
    "publication_year": 1994,
    "authors": "Patrick Tendick; Norman Matloff",
    "corresponding_authors": "",
    "abstract": "The random data perturbation (RDP) method of preserving the privacy of individual records in a statistical database is discussed. In particular, it is shown that if confidential attributes are allowed as query-defining variables, severe biases may result in responses to queries. It is also shown that even if query definition through confidential variables is not allowed, biases can still occur in responses to queries such as those involving proportions or counts. In either case, serious distortions may occur in user statistical analyses. A modified version of RDP is presented, in the form of a query adjustment procedure and specialized perturbation structure which will produce unbiased results.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2010182665",
    "type": "article"
  },
  {
    "title": "Two-phase locking performance and its thrashing behavior",
    "doi": "https://doi.org/10.1145/169725.169720",
    "publication_date": "1993-12-01",
    "publication_year": 1993,
    "authors": "Alexander Thomasian",
    "corresponding_authors": "Alexander Thomasian",
    "abstract": "article Free Access Share on Two-phase locking performance and its thrashing behavior Author: Alexander Thomasian IBM T. J. Watson Research Center IBM T. J. Watson Research CenterView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 18Issue 4Dec. 1993 pp 579–625https://doi.org/10.1145/169725.169720Published:01 December 1993Publication History 58citation1,233DownloadsMetricsTotal Citations58Total Downloads1,233Last 12 Months123Last 6 weeks62 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2088261070",
    "type": "article"
  },
  {
    "title": "The performance of current B-tree algorithms",
    "doi": "https://doi.org/10.1145/151284.151286",
    "publication_date": "1993-03-01",
    "publication_year": 1993,
    "authors": "Theodore Johnson; Dennis Sasha",
    "corresponding_authors": "",
    "abstract": "article Free AccessThe performance of current B-tree algorithms Authors: Theodore Johnson Univ. of Florida, Gainesville Univ. of Florida, GainesvilleView Profile , Dennis Sasha New York Univ., New York, NY New York Univ., New York, NYView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 18Issue 1pp 51–101https://doi.org/10.1145/151284.151286Published:01 March 1993Publication History 57citation1,840DownloadsMetricsTotal Citations57Total Downloads1,840Last 12 Months77Last 6 weeks15 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1971289999",
    "type": "article"
  },
  {
    "title": "Analysis of predictive spatio-temporal queries",
    "doi": "https://doi.org/10.1145/958942.958943",
    "publication_date": "2003-12-01",
    "publication_year": 2003,
    "authors": "Yufei Tao; Jimeng Sun; Dimitris Papadias",
    "corresponding_authors": "",
    "abstract": "Given a set of objects S , a spatio-temporal window query q retrieves the objects of S that will intersect the window during the (future) interval q T . A nearest neighbor query q retrieves the objects of S closest to q during q T . Given a threshold d , a spatio-temporal join retrieves the pairs of objects from two datasets that will come within distance d from each other during q T . In this article, we present probabilistic cost models that estimate the selectivity of spatio-temporal window queries and joins, and the expected distance between a query and its nearest neighbor(s). Our models capture any query/object mobility combination (moving queries, moving objects or both) and any data type (points and rectangles) in arbitrary dimensionality. In addition, we develop specialized spatio-temporal histograms, which take into account both location and velocity information, and can be incrementally maintained. Extensive performance evaluation verifies that the proposed techniques produce highly accurate estimation on both uniform and non-uniform data.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W1971119807",
    "type": "article"
  },
  {
    "title": "Cache investment",
    "doi": "https://doi.org/10.1145/377674.377677",
    "publication_date": "2000-12-01",
    "publication_year": 2000,
    "authors": "Donald Kossmann; Michael J. Franklin; Gerhard Drasch; Wig Ag",
    "corresponding_authors": "",
    "abstract": "Emerging distributed query-processing systems support flexible execution strategies in which each query can be run using a combination of data shipping and query shipping. As in any distributed environment, these systems can obtain tremendous performance and availability benefits by employing dynamic data caching. When flexible execution and dynamic caching are combined, however, a circular dependency arises: Caching occurs as a by-product of query operator placement, but query operator placement decisions are based on (cached) data location. The practical impact of this dependency is that query optimization decisions that appear valid on a per-query basis can actually cause suboptimal performance for all queries in the long run. To address this problem, we developed Cache Investment - a novel approach for integrating query optimization and data placement that looks beyond the performance of a single query. Cache Investment sometimes intentionally generates a “suboptimal” plan for a particular query in the interest of effecting a better data placement for subsequent queries. Cache Investment can be integrated into a distributed database system without changing the internals of the query optimizer. In this paper, we propose Cache Investment mechanisms and policies and analyze their performance. The analysis uses results from both an implementation on the SHORE storage manager and a detailed simulation model. Our results show that Cache Investment can significantly improve the overall performance of a system and demonstrate the trade-offs among various alternative policies.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W1989893188",
    "type": "article"
  },
  {
    "title": "Rule-based optimization and query processing in an extensible geometric database system",
    "doi": "https://doi.org/10.1145/128903.128905",
    "publication_date": "1992-06-01",
    "publication_year": 1992,
    "authors": "Ludger Becker; Ralf Hartmut Güting",
    "corresponding_authors": "",
    "abstract": "Gral is an extensible database system, based on the formal concept of a many-sorted relational algebra. Many-sorted algebra is used to define any application's query language, its query execution language, and its optimiztion rules. In this paper we describe Gral's optimization component. It provides (1) a sophisticated rule language—rules are transformations of abstract algebra expressions, (2) a general optimization framework under which more specific optimization algorithms can be implemented, and (3) several control mechanisms for the application of rules. An optimization algorithm can be specified as a series of steps. Each step is defined by its own collection of rules together with a selected control strategy. The general facilities are illustrated by the complete design of an example optimizer—in the form of a rule file—for a small nonstandard query language and an associated execution language. The query language includes selection, join, ordering, embedding derived values, aggregate functions, and several geometric operations. The example shows in particular how the special processing techniques of a geometric database systems, such as spatial join methods and geometric index structures, can be integrated into query processing and optimization of a relational database system. A similar, though larger, optimizer is fully functional within the geometric database system implemented as a Gral prototype.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2146168643",
    "type": "article"
  },
  {
    "title": "Characterizing memory requirements for queries over continuous data streams",
    "doi": "https://doi.org/10.1145/974750.974756",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Arvind Arasu; Brian Babcock; Shivnath Babu; Jon McAlister; Jennifer Widom",
    "corresponding_authors": "",
    "abstract": "This article deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2015180052",
    "type": "article"
  },
  {
    "title": "Bounded ignorance",
    "doi": "https://doi.org/10.1145/195664.195670",
    "publication_date": "1994-12-01",
    "publication_year": 1994,
    "authors": "Narayanan Krishnakumar; Arthur Bernstein",
    "corresponding_authors": "",
    "abstract": "Databases are replicated to improve performance and availability. The notion of correctness that has commonly been adopted for concurrent access by transactions to shared, possibly replicated, data is serializability. However, serializability may be impractical in high-performance applications since it imposes too stringent a restriction on concurrency. When serializability is relaxed, the integrity constraints describing the data may be violated. By allowing bounded violations of the integrity constraints, however, we are able to increase the concurrency of transactions that execute in a replicated environment. In this article, we introduce the notion of an N-ignorant transaction, which is a transaction that may be ignorant of the results of at most N prior transactions, which is a transaction that may be ignorant of the results of at most N prior transactions. A system in which all transactions are N-ignorant can have an N + 1-fold increase in concurrency over serializable systems, at the expense of bounded violations of its integrity constraints. We present algorithms for implementing replicated databases in N-ignorant systems. We then provide constructive methods for calculating the reachable states in such systems, given the value of N , so that one may assess the maximum liability that is incurred in allowing constraint violation. Finally, we generalize the notion of N-ignorance to a matrix of ignorance for the purpose of higher concurrency.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2071596433",
    "type": "article"
  },
  {
    "title": "Peer data exchange",
    "doi": "https://doi.org/10.1145/1189769.1189778",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Ariel Fuxman; Phokion G. Kolaitis; Renée J. Miller; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "In this article, we introduce and study a framework, called peer data exchange , for sharing and exchanging data between peers. This framework is a special case of a full-fledged peer data management system and a generalization of data exchange between a source schema and a target schema. The motivation behind peer data exchange is to model authority relationships between peers, where a source peer may contribute data to a target peer, specified using source-to-target constraints, and a target peer may use target-to-source constraints to restrict the data it is willing to receive, but cannot modify the data of the source peer.A fundamental algorithmic problem in this framework is that of deciding the existence of a solution: given a source instance and a target instance for a fixed peer data exchange setting, can the target instance be augmented in such a way that the source instance and the augmented target instance satisfy all constraints of the setting? We investigate the computational complexity of the problem for peer data exchange settings in which the constraints are given by tuple generating dependencies. We show that this problem is always in NP, and that it can be NP-complete even for “acyclic” peer data exchange settings. We also show that the data complexity of the certain answers of target conjunctive queries is in coNP, and that it can be coNP-complete even for “acyclic” peer data exchange settings.After this, we explore the boundary between tractability and intractability for deciding the existence of a solution and for computing the certain answers of target conjunctive queries. To this effect, we identify broad syntactic conditions on the constraints between the peers under which the existence-of-solutions problem is solvable in polynomial time. We also identify syntactic conditions between peer data exchange settings and target conjunctive queries that yield polynomial-time algorithms for computing the certain answers. For both problems, these syntactic conditions turn out to be tight, in the sense that minimal relaxations of them lead to intractability. Finally, we introduce the concept of a universal basis of solutions in peer data exchange and explore its properties.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W4256119157",
    "type": "article"
  },
  {
    "title": "Management of a remote backup copy for disaster recovery",
    "doi": "https://doi.org/10.1145/114325.103715",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Richard P. King; Nagui Halim; Héctor García-Molina; Christos A. Polyzois",
    "corresponding_authors": "",
    "abstract": "A remote backup database system tracks the state of a primary system, taking over transaction processing when disaster hits the primary site. The primary and backup sites are physically isolated so that failures at one site are unlikely to propogate to the other. For correctness, the execution schedule at the backup must be equivalent to that at the primary. When the primary and backup sites contain a single processor, it is easy to achieve this property. However, this is harder to do when each site contains multiple processors and sites are connected via multiple communication lines. We present an efficient transaction processing mechanism for multiprocessor systems that guarantees this and other important properties. We also present a database initialization algorithm that copies the database to a backup site while transactions are being processed.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2126113943",
    "type": "article"
  },
  {
    "title": "Finite representation of infinite query answers",
    "doi": "https://doi.org/10.1145/151634.151635",
    "publication_date": "1993-06-01",
    "publication_year": 1993,
    "authors": "Jan Chomicki; Tomasz Imieliński",
    "corresponding_authors": "",
    "abstract": "We define here a formal notion of finite representation of infinite query answers in logic programs. We apply this notion to Datalog nS programs may be infinite and consequently queries may have infinite answers. We present a method to finitely represent infinite least Herbrand models of Datalog nS program (and its underlying computational engine) can be forgotten. Given a query to be evaluated, it is easy to obtain from the relational specification finitely many answer substitutions that represent infinitely many answer substitutions to the query. The method involved is a combination of a simple, unificationless, computational mechanism (graph traversal, congruence closure, or term rewriting) and standard relational query evaluation methods. Second, a relational specification is effectively computable and its computation is no harder, in the sense of the complexity class, than answering yes-no queries. Our method is applicable to every range-restricted Datalog nS program. We also show that for some very simple non-Datalog nS logic programs, finite representations of query answers do not exist.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W1971297231",
    "type": "article"
  },
  {
    "title": "A cost-benefit decision model: analysis, comparison amd selection of data management",
    "doi": "https://doi.org/10.1145/27629.33403",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Stanley Y. W. Su; Jozo Dujmović; Don Batory; Sham Navathe; Richard A. Elnicki",
    "corresponding_authors": "",
    "abstract": "This paper describes a general cost-benefit decision model that is applicable to the evaluation, comparison, and selection of alternative products with a multiplicity of features, such as complex computer systems. The application of this model is explained and illustrated using the selection of data management systems as an example. The model has the following features: (1) it is mathematically based on an extended continuous logic and a theory of complex criteria; (2) the decision-making procedure is very general yet systematic, well-structured, and quantitative; (3) the technique is based on a comprehensive cost analysis and an elaborate analysis of benefits expressed in terms of the decision maker's preferences. The decision methodology, when applied to the problem of selecting a data management system, takes into consideration the life cycle of a DMS and the objectives and goals for the new systems under evaluation. It allows the cost and preference analyses to be carried out separately using two different models. The model for preference analysis makes use of comprehensive performance (or preference) parameters and allows what we call a “logic scoring of preferences” using continuous values between zero and one, to express the degree with which candidate systems satisfy stated requirements. It aggregates preference parameters based on their relative weights and logical relationships to compute a global performance (preference) score for each system. The cost model incorporates an aggregation of costs which may be estimated over different time horizons and discounted at appropriate discount rates. A procedure to establish an overall ranking of alternative systems based on their global preference scores and global costs is also discussed.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W1992051230",
    "type": "article"
  },
  {
    "title": "Query optimization in a memory-resident domain relational calculus database system",
    "doi": "https://doi.org/10.1145/77643.77646",
    "publication_date": "1990-03-01",
    "publication_year": 1990,
    "authors": "Kyu-Young Whang; Ravi Krishnamurthy",
    "corresponding_authors": "",
    "abstract": "We present techniques for optimizing queries in memory-resident database systems. Optimization techniques in memory-resident database systems differ significantly from those in conventional disk-resident database systems. In this paper we address the following aspects of query optimization in such systems and present specific solutions for them: (1) a new approach to developing a CPU-intensive cost model; (2) new optimization strategies for main-memory query processing; (3) new insight into join algorithms and access structures that take advantage of memory residency of data; and (4) the effect of the operating system's scheduling algorithm on the memory-residency assumption. We present an interesting result that a major cost of processing queries in memory-resident database systems is incurred by evaluation of predicates. We discuss optimization techniques using the Office-by-Example (OBE) that has been under development at IBM Research. We also present the results of performance measurements, which prove to be excellent in the current state of the art. Despite recent work on memory-resident database systems, query optimization aspects in these systems have not been well studied. We believe this paper opens the issues of query optimization in memory-resident database systems and presents practical solutions to them.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W1979648938",
    "type": "article"
  },
  {
    "title": "Designing a Portable Natural Language Database Query System",
    "doi": "https://doi.org/10.1145/348.318584",
    "publication_date": "1984-03-23",
    "publication_year": 1984,
    "authors": "S. Jerrold Kaplan",
    "corresponding_authors": "S. Jerrold Kaplan",
    "abstract": "One barrier to the acceptance of natural language database query systems is the substantial installation effort required for each new database. Much of this effort involves the encoding of semantic knowledge for the domain of discourse, necessary to correctly interpret and respond to natural language questions. For such systems to be practical, techniques must be developed to increase their portability to new domains. This paper discusses several issues involving the portability of natural language interfaces to database systems, and presents the approach taken in CO-OP — a natural language database query system that provides cooperative responses to English questions and operates with a typical CODA-SYL database system. CO-OP derives its domain-specific knowledge from a lexicon (the list of words known to the system) and the information already present in the structure and content of the underlying database. Experience with the implementation suggests that strategies that are not directly derivative of cognitive or linguistic models may nonetheless play an important role in the development of practical natural language systems.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2086962308",
    "type": "article"
  },
  {
    "title": "Feature-based similarity search in graph structures",
    "doi": "https://doi.org/10.1145/1189769.1189777",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Xifeng Yan; Feida Zhu; Philip S. Yu; Jiawei Han",
    "corresponding_authors": "",
    "abstract": "Similarity search of complex structures is an important operation in graph-related applications since exact matching is often too restrictive. In this article, we investigate the issues of substructure similarity search using indexed features in graph databases. By transforming the edge relaxation ratio of a query graph into the maximum allowed feature misses, our structural filtering algorithm can filter graphs without performing pairwise similarity computation. It is further shown that using either too few or too many features can result in poor filtering performance. Thus the challenge is to design an effective feature set selection strategy that could maximize the filtering capability. We prove that the complexity of optimal feature set selection is Ω(2 m ) in the worst case, where m is the number of features for selection. In practice, we identify several criteria to build effective feature sets for filtering, and demonstrate that combining features with similar size and selectivity can improve the filtering and search performance significantly within a multifilter composition framework. The proposed feature-based filtering concept can be generalized and applied to searching approximate nonconsecutive sequences, trees, and other structured data as well.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2167101903",
    "type": "article"
  },
  {
    "title": "An improved third normal form for relational databases",
    "doi": "https://doi.org/10.1145/319566.319583",
    "publication_date": "1981-06-01",
    "publication_year": 1981,
    "authors": "Tok-Wang Ling; Frank Wm. Tompa; Tiko Kameda",
    "corresponding_authors": "",
    "abstract": "In this paper, we show that some Codd third normal form relations may contain “superfluous” attributes because the definitions of transitive dependency and prime attribute are inadequate when applied to sets of relations. To correct this, an improved third normal form is defined and an algorithm is given to construct a set of relations from a given set of functional dependencies in such a way that the superfluous attributes are guaranteed to be removed. This new normal form is compared with other existing definitions of third normal form, and the deletion normalization method proposed is shown to subsume the decomposition method of normalization.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W1994410342",
    "type": "article"
  },
  {
    "title": "Automatic virtual machine configuration for database workloads",
    "doi": "https://doi.org/10.1145/1670243.1670250",
    "publication_date": "2008-02-15",
    "publication_year": 2008,
    "authors": "Ahmed A. Soror; Umar Farooq Minhas; Ashraf Aboulnaga; Kenneth Salem; Peter Kokosielis; Sunil Kamath",
    "corresponding_authors": "",
    "abstract": "Virtual machine monitors are becoming popular tools for the deployment of database management systems and other enterprise software. In this article, we consider a common resource consolidation scenario in which several database management system instances, each running in a separate virtual machine, are sharing a common pool of physical computing resources. We address the problem of optimizing the performance of these database management systems by controlling the configurations of the virtual machines in which they run. These virtual machine configurations determine how the shared physical resources will be allocated to the different database system instances. We introduce a virtualization design advisor that uses information about the anticipated workloads of each of the database systems to recommend workload-specific configurations offline. Furthermore, runtime information collected after the deployment of the recommended configurations can be used to refine the recommendation and to handle changes in the workload. To estimate the effect of a particular resource allocation on workload performance, we use the query optimizer in a new what-if mode. We have implemented our approach using both PostgreSQL and DB2, and we have experimentally evaluated its effectiveness using DSS and OLTP workloads.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W1967155922",
    "type": "article"
  },
  {
    "title": "Time- and space-optimality in B-trees",
    "doi": "https://doi.org/10.1145/319540.319565",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "Arnold L. Rosenberg; Lawrence Snyder",
    "corresponding_authors": "",
    "abstract": "A B-tree is compact if it is minimal in number of nodes, hence has optimal space utilization, among equally capacious B-trees of the same order. The space utilization of compact B-trees is analyzed and compared with that of noncompact B-trees and with (node)-visit-optimal B-trees, which minimize the expected number of nodes visited per key access. Compact B-trees can be as much as a factor of 2.5 more space efficient than visit-optimal B-trees; and the node-visit cost of a compact tree is never more than 1 + the node-visit cost of an optimal tree. The utility of initializing a B-tree to be compact (which initialization can be done in time linear in the number of keys if the keys are presorted) is demonstrated by comparing the space utilization of a compact tree that has been augmented by random insertions with that of a tree that has been grown entirely by random insertions. Even after increasing the number of keys by a modest amount, the effects of compact initialization are still felt. Once the tree has grown so large that these effects are no longer discernible, the tree can be expeditiously compacted in place using an algorithm presented here; and the benefits of compactness resume.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W1980034063",
    "type": "article"
  },
  {
    "title": "Quasi-inverses of schema mappings",
    "doi": "https://doi.org/10.1145/1366102.1366108",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Ronald Fagin; Phokion G. Kolaitis; Lucian Popa; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "Schema mappings are high-level specifications that describe the relationship between two database schemas. Two operators on schema mappings, namely the composition operator and the inverse operator, are regarded as especially important. Progress on the study of the inverse operator was not made until very recently, as even finding the exact semantics of this operator turned out to be a fairly delicate task. Furthermore, this notion is rather restrictive, since it is rare that a schema mapping possesses an inverse. In this article, we introduce and study the notion of a quasi-inverse of a schema mapping. This notion is a principled relaxation of the notion of an inverse of a schema mapping; intuitively, it is obtained from the notion of an inverse by not differentiating between instances that are equivalent for data-exchange purposes. For schema mappings specified by source-to-target tuple-generating dependencies (s-t tgds), we give a necessary and sufficient combinatorial condition for the existence of a quasi-inverse, and then use this condition to obtain both positive and negative results about the existence of quasi-inverses. In particular, we show that every LAV (local-as-view) schema mapping has a quasi-inverse, but that there are schema mappings specified by full s-t tgds that have no quasi-inverse. After this, we study the language needed to express quasi-inverses of schema mappings specified by s-t tgds, and we obtain a complete characterization. We also characterize the language needed to express inverses of schema mappings, and thereby solve a problem left open in the earlier study of the inverse operator. Finally, we show that quasi-inverses can be used in many cases to recover the data that was exported by the original schema mapping when performing data exchange.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2042287357",
    "type": "article"
  },
  {
    "title": "A new normal form for the design of relational database schemata",
    "doi": "https://doi.org/10.1145/319732.319749",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Carlo Zaniolo",
    "corresponding_authors": "Carlo Zaniolo",
    "abstract": "This paper addresses the problem of database schema design in the framework of the relational data model and functional dependencies. It suggests that both Third Normal Form (3NF) and Boyce-Codd Normal Form (BCNF) supply an inadequate basis for relational schema design. The main problem with 3NF is that it is too forgiving and does not enforce the separation principle as strictly as it should. On the other hand, BCNF is incompatible with the principle of representation and prone to computational complexity. Thus a new normal form, which lies between these two and captures the salient qualities of both is proposed. The new normal form is stricter than 3NF, but it is still compatible with the representation principle. First a simpler definition of 3NF is derived, and the analogy of this new definition to the definition of BCNF is noted. This analogy is used to derive the new normal form. Finally, it is proved that Bernstein's algorithm for schema design synthesizes schemata that are already in the new normal form.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2117671567",
    "type": "article"
  },
  {
    "title": "Query optimization in star computer networks",
    "doi": "https://doi.org/10.1145/319758.319778",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "Larry Kerschberg; Peter D. Ting; S. Bing Yao",
    "corresponding_authors": "",
    "abstract": "Query processing is investigated for relational databases distributed over several computers organized in a star network. Minimal response-time processing strategies are presented for queries involving the select, project, and join commands. These strategies depend on system parameters such as communication costs and different machine processing speeds; database parameters such as relation cardinality and file size; and query parameters such as estimates of the size and number of tuples in the result relation. The optimal strategies specify relation preparation processes, the shipping strategy, serial or parallel processing, and, where applicable, the site of join filtering and merging. Strategies for optimizing select and join queries have been implemented and tested.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2054338562",
    "type": "article"
  },
  {
    "title": "An adaptive packed-memory array",
    "doi": "https://doi.org/10.1145/1292609.1292616",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Michael A. Bender; Haodong Hu",
    "corresponding_authors": "",
    "abstract": "The packed-memory array ( PMA ) is a data structure that maintains a dynamic set of N elements in sorted order in a Θ( N )-sized array. The idea is to intersperse Θ( N ) empty spaces or gaps among the elements so that only a small number of elements need to be shifted around on an insert or delete. Because the elements are stored physically in sorted order in memory or on disk, the PMA can be used to support extremely efficient range queries. Specifically, the cost to scan L consecutive elements is O (1 + L / B ) memory transfers. This article gives the first adaptive packed-memory array ( APMA ), which automatically adjusts to the input pattern. Like the traditional PMA, any pattern of updates costs only O (log 2 N ) amortized element moves and O (1 + (log 2 N )/ B ) amortized memory transfers per update. However, the APMA performs even better on many common input distributions achieving only O (log N ) amortized element moves and O (1+ (log N )/ B ) amortized memory transfers. The article analyzes sequential inserts, where the insertions are to the front of the APMA, hammer inserts, where the insertions “hammer” on one part of the APMA, random inserts, where the insertions are after random elements in the APMA, and bulk inserts, where for constant α ϵ [0, 1], N α elements are inserted after random elements in the APMA. The article then gives simulation results that are consistent with the asymptotic bounds. For sequential insertions of roughly 1.4 million elements, the APMA has four times fewer element moves per insertion than the traditional PMA and running times that are more than seven times faster.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2069574596",
    "type": "article"
  },
  {
    "title": "An architecture for automatic relational database sytem conversion",
    "doi": "https://doi.org/10.1145/319702.319724",
    "publication_date": "1982-06-01",
    "publication_year": 1982,
    "authors": "Ben Shneiderman; Glenn Thomas",
    "corresponding_authors": "",
    "abstract": "Changes in requirements for database systems necessitate schema restructuring, database translation, and application or query program conversion. An alternative to the lengthy manual revision process is proposed by offering a set of 15 transformations keyed to the relational model of data and the relational algebra. Motivations, examples, and detailed descriptions are provided.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2079588361",
    "type": "article"
  },
  {
    "title": "Composition of mappings given by embedded dependencies",
    "doi": "https://doi.org/10.1145/1206049.1206053",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Alan Nash; Philip A. Bernstein; Sergey Melnik",
    "corresponding_authors": "",
    "abstract": "Composition of mappings between schemas is essential to support schema evolution, data exchange, data integration, and other data management tasks. In many applications, mappings are given by embedded dependencies. In this article, we study the issues involved in composing such mappings. Our algorithms and results extend those of Fagin et al. [2004], who studied the composition of mappings given by several kinds of constraints. In particular, they proved that full source-to-target tuple-generating dependencies (tgds) are closed under composition, but embedded source-to-target tgds are not. They introduced a class of second-order constraints, SO tgds , that is closed under composition and has desirable properties for data exchange. We study constraints that need not be source-to-target and we concentrate on obtaining (first-order) embedded dependencies. As part of this study, we also consider full dependencies and second-order constraints that arise from Skolemizing embedded dependencies. For each of the three classes of mappings that we study, we provide: (a) an algorithm that attempts to compute the composition; and (b) sufficient conditions on the input mappings which guarantee that the algorithm will succeed. In addition, we give several negative results. In particular, we show that full and second-order dependencies that are not limited to be source-to-target are not closed under composition (for the latter, under the additional restriction that no new function symbols are introduced). Furthermore, we show that determining whether the composition can be given by these kinds of dependencies is undecidable.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2090958287",
    "type": "article"
  },
  {
    "title": "On the complexity of join dependencies",
    "doi": "https://doi.org/10.1145/5236.5237",
    "publication_date": "1986-03-01",
    "publication_year": 1986,
    "authors": "Marc Gyssens",
    "corresponding_authors": "Marc Gyssens",
    "abstract": "In [10] a method is proposed for decomposing join dependencies (jds) in a relational database using the notion of a hinge. This method was subsequently studied in [11] and [12]. We show how the technique of decomposition can be used to make integrity checking more efficient. It turns out that it is important to find a decomposition that minimizes the number of edges of its largest element. We show that the decompositions obtained with the method described in [10] are optimal in this respect. This minimality criterion leads to the definition of the degree of cyclicity , which allows us to classify jds and leads to the notion of n-cyclicity , of which acyclicity is a special case for n = 2. We then show that, for a fixed value of n (which may be greater than 2). integrity checking can be performed in polynomial time provided we restrict ourselves to n-cyclic jds. Finally, we generalize a well-known characterization for acyclic jds by proving that n-cyclicity is equivalent to “n-wise consistency implies global consistency.” As a consequence, consistency checking can be performed in polynomial time if we restrict ourselves to n-cyclic jds, for a tired value of n, not necessarily equal to 2.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2000391493",
    "type": "article"
  },
  {
    "title": "Reverse skyline search in uncertain databases",
    "doi": "https://doi.org/10.1145/1670243.1670246",
    "publication_date": "2008-02-15",
    "publication_year": 2008,
    "authors": "Xiang Lian; Lei Chen",
    "corresponding_authors": "",
    "abstract": "Reverse skyline queries over uncertain databases have many important applications such as sensor data monitoring and business planning. Due to the wide existence of uncertainty in many real-world data, answering reverse skyline queries accurately and efficiently over uncertain data has become increasingly important. In this article, we formalize the probabilistic reverse skyline query over uncertain data, in both monochromatic and bichromatic cases, and propose effective pruning methods, namely spatial pruning and probabilistic pruning , to reduce the search space of the reverse skyline query processing. Moreover, efficient query procedures have been presented seamlessly integrating the proposed pruning methods. Furthermore, a novel query type, namely Probabilistic Reverse Furthest Skyline (PRFS) query, is proposed and tackled under “the larger, the better” dominance semantics of skyline. Variants of probabilistic reverse skyline have been proposed and tackled, including those that return objects with top- k highest probabilities and that retrieve top- k reverse skylines. Extensive experiments demonstrated the efficiency and effectiveness of our approaches with various experimental settings.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2041984268",
    "type": "article"
  },
  {
    "title": "Processing spatial skyline queries in both vector spaces and spatial network databases",
    "doi": "https://doi.org/10.1145/1567274.1567276",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Mehdi Sharifzadeh; Cyrus Shahabi; Leyla Kazemi",
    "corresponding_authors": "",
    "abstract": "In this article, we first introduce the concept of Spatial Skyline Queries (SSQ). Given a set of data points P and a set of query points Q , each data point has a number of derived spatial attributes each of which is the point's distance to a query point. An SSQ retrieves those points of P which are not dominated by any other point in P considering their derived spatial attributes. The main difference with the regular skyline query is that this spatial domination depends on the location of the query points Q . SSQ has application in several domains such as emergency response and online maps. The main intuition and novelty behind our approaches is that we exploit the geometric properties of the SSQ problem space to avoid the exhaustive examination of all the point pairs in P and Q . Consequently, we reduce the complexity of SSQ search from O (| P | 2 | Q |) to O (| S | 2 | C | + √| P |), where | S | and | C | are the solution size and the number of vertices of the convex hull of Q , respectively. Considering Euclidean distance, we propose two algorithms, B 2 S 2 and VS 2 , for static query points and one algorithm, VCS 2 , for streaming Q whose points change location over time (e.g., are mobile). VCS 2 exploits the pattern of change in Q to avoid unnecessary recomputation of the skyline and hence efficiently perform updates. We also propose two algorithms, SNS 2 and VSNS 2 , that compute the spatial skyline with respect to the network distance in a spatial network database. Our extensive experiments using real-world datasets verify that both R-tree-based B 2 S 2 and Voronoi-based VS 2 outperform the best competitor approach in terms of both processing time and I/O cost. Furthermore, their output computed based on Euclidean distance is a good approximation of the spatial skyline in network space. For accurate computation of spatial skylines in network space, our experiments showed the superiority of VSNS 2 over SNS 2 .",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2005695184",
    "type": "article"
  },
  {
    "title": "Querying and repairing inconsistent numerical databases",
    "doi": "https://doi.org/10.1145/1735886.1735893",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Sergio Flesca; Filippo Furfaro; Francesco Parisi",
    "corresponding_authors": "",
    "abstract": "The problem of extracting consistent information from relational databases violating integrity constraints on numerical data is addressed. In particular, aggregate constraints defined as linear inequalities on aggregate-sum queries on input data are considered. The notion of repair as consistent set of updates at attribute-value level is exploited, and the characterization of several data-complexity issues related to repairing data and computing consistent query answers is provided. Moreover, a method for computing “reasonable” repairs of inconsistent numerical databases is provided, for a restricted but expressive class of aggregate constraints. Several experiments are presented which assess the effectiveness of the proposed approach in real-life application scenarios.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W1977653641",
    "type": "article"
  },
  {
    "title": "NaLIX",
    "doi": "https://doi.org/10.1145/1292609.1292620",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Yunyao Li; Huahai Yang; H. V. Jagadish",
    "corresponding_authors": "",
    "abstract": "We describe the construction of a generic natural language query interface to an XML database. Our interface can accept a large class of English sentences as a query, which can be quite complex and include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression. The translation is based on mapping grammatical proximity of natural language parsed tokens in the parse tree of the query sentence to proximity of corresponding elements in the XML data to be retrieved. Iterative search in the form of followup queries is also supported. Our experimental assessment, through a user study, demonstrates that this type of natural language interface is good enough to be usable now, with no restrictions on the application domain.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W1997640482",
    "type": "article"
  },
  {
    "title": "A dynamic database reorganization algorithm",
    "doi": "https://doi.org/10.1145/320455.320467",
    "publication_date": "1976-06-01",
    "publication_year": 1976,
    "authors": "S. Bing Yao; Kapotaksha Das; Toby J. Teorey",
    "corresponding_authors": "",
    "abstract": "Reorganization is necessary in some databases for overcoming the performance deterioration caused by updates. The paper presents a dynamic reorganization algorithm which makes the reorganization decision by measuring the database search costs. Previously, the reorganization intervals could only be determined for linear deterioration and known database lifetime. It is shown that the dynamic reorganization algorithm is near optimum for constant reorganization cost and is superior for increasing reorganization cost. In addition, it can be applied to cases of unknown database lifetime and nonlinear performance deterioration. The simplicity, generality, and efficiency appear to make this good heuristic for database reorganization.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2169861247",
    "type": "article"
  },
  {
    "title": "Compiling mappings to bridge applications and databases",
    "doi": "https://doi.org/10.1145/1412331.1412334",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Sergey Melnik; Atul Adya; Philip A. Bernstein",
    "corresponding_authors": "",
    "abstract": "Translating data and data access operations between applications and databases is a longstanding data management problem. We present a novel approach to this problem, in which the relationship between the application data and the persistent storage is specified using a declarative mapping, which is compiled into bidirectional views that drive the data transformation engine. Expressing the application model as a view on the database is used to answer queries, while expressing the database schema as a view on the application model allows us to leverage view maintenance algorithms for update translation. This approach has been implemented in a commercial product. It enables developers to interact with a relational database via a conceptual schema and an object-oriented programming surface. We outline the implemented system and focus on the challenges of mapping compilation, which include rewriting queries under constraints and supporting nonrelational constructs.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2002711156",
    "type": "article"
  },
  {
    "title": "Optimal policy for batch operations",
    "doi": "https://doi.org/10.1145/320557.320558",
    "publication_date": "1977-09-01",
    "publication_year": 1977,
    "authors": "Guy M. Lohman; John A. Muckstadt",
    "corresponding_authors": "",
    "abstract": "Many database maintenance operations are performed periodically in batches, even in realtime systems. The purpose of this paper is to present a general model for determining the optimal frequency of these batch operations. Specifically, optimal backup, checkpointing, batch updating, and reorganization policies are derived. The approach used exploits inventory parallels by seeking the optimal number of items—rather than a time interval—to trigger a batch. The Renewal Reward Theorem is used to find the average long run costs for backup, recovery, and item storage, per unit time, which is then minimized to find the optimal backup policy. This approach permits far less restrictive assumptions about the update arrival process than did previous models, as well as inclusion of storage costs for the updates. The optimal checkpointing, batch updating, and reorganization policies are shown to be special cases of this optimal backup policy. The derivation of previous results as special cases of this model, and an example, demonstrate the generality of the methodology developed.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2068522942",
    "type": "article"
  },
  {
    "title": "Data abstractions for database systems",
    "doi": "https://doi.org/10.1145/320064.320067",
    "publication_date": "1979-03-01",
    "publication_year": 1979,
    "authors": "Peter C. Lockemann; Heinrich C. Mayr; Wolfgang H. Weil; Wolfgang H. Wohlleber",
    "corresponding_authors": "",
    "abstract": "Data abstractions were originally conceived as a specification tool in programming. They also appear to be useful for exploring and explaining the capabilities and shortcomings of the data definition and manipulation facilities of present-day database systems. Moreover they may lead to new approaches to the design of these facilities. In the first section the paper introduces an axiomatic method for specifying data abstractions and, on that basis, gives precise meaning to familiar notions such as data model, data type, and database schema. In a second step the various possibilities for specifying data types within a given data model are examined and illustrated. It is shown that data types prescribe the individual operations that are allowed within a database. Finally, some additions to the method are discussed which permit the formulation of interrelationships between arbitrary operations.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2031993032",
    "type": "article"
  },
  {
    "title": "The monte carlo database system",
    "doi": "https://doi.org/10.1145/2000824.2000828",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Ravi Jampani; Fei Xu; Mingxi Wu; Luis L. Perez; Chris Jermaine; Peter J. Haas",
    "corresponding_authors": "",
    "abstract": "The application of stochastic models and analysis techniques to large datasets is now commonplace. Unfortunately, in practice this usually means extracting data from a database system into an external tool (such as SAS, R, Arena, or Matlab), and then running the analysis there. This extract-and-model paradigm is typically error-prone, slow, does not support fine-grained modeling, and discourages what-if and sensitivity analyses. In this article we describe MCDB, a database system that permits a wide spectrum of stochastic models to be used in conjunction with the data stored in a large database, without ever extracting the data. MCDB facilitates in-database execution of tasks such as risk assessment, prediction, and imputation of missing data, as well as management of errors due to data integration, information extraction, and privacy-preserving data anonymization. MCDB allows a user to define “random” relations whose contents are determined by stochastic models. The models can then be queried using standard SQL. Monte Carlo techniques are used to analyze the probability distribution of the result of an SQL query over random relations. Novel “tuple-bundle” processing techniques can effectively control the Monte Carlo overhead, as shown in our experiments.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2086128349",
    "type": "article"
  },
  {
    "title": "Search strategy and selection function for an inferential relational system",
    "doi": "https://doi.org/10.1145/320241.320242",
    "publication_date": "1978-03-01",
    "publication_year": 1978,
    "authors": "Jack Minker",
    "corresponding_authors": "Jack Minker",
    "abstract": "An inferential relational system is one in which data in the system consists of both explicit facts and general axioms (or “views”). The general axioms are used together with the explicit facts to derive the facts that are implicit (virtual relations) within the system. A top-down algorithm, as used in artificial intelligence work, is described to develop inferences within the system. The top-down approach starts with the query, a conjunction of relations, to be answered. Either a relational fact solves a given relation in a conjunct, or the relation is replaced by a conjunct of relations which must be solved to solve the given relation. The approach requires that one and only one relation in a conjunction be replaced (or expanded) by the given facts and general axioms. The decision to expand only a single relation is termed a selection function. It is shown for relational systems that such a restriction still guarantees that a solution to the problem will be found if one exists. The algorithm provides for heuristic direction in the search process. Experimental results are presented which illustrate the techniques. A bookkeeping mechanism is described which permits one to know when subproblems are solved. It further facilitates the outputting of reasons for the deductively found answer in a coherent fashion.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2013084276",
    "type": "article"
  },
  {
    "title": "Collaborative data sharing via update exchange and provenance",
    "doi": "https://doi.org/10.1145/2500127",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Grigoris Karvounarakis; Todd J. Green; Zachary G. Ives; Val Tannen",
    "corresponding_authors": "",
    "abstract": "Recent work [Ives et al. 2005] proposed a new class of systems for supporting data sharing among scientific and other collaborations: this new collaborative data sharing system connects heterogeneous logical peers using a network of schema mappings. Each peer has a locally controlled and edited database instance, but wants to incorporate related data from other peers as well. To achieve this, every peer's data and updates propagate along the mappings to the other peers. However, this operation, termed update exchange , is filtered by trust conditions —expressing what data and sources a peer judges to be authoritative—which may cause a peer to reject another's updates. In order to support such filtering, updates carry provenance information. This article develops methods for realizing such systems: we build upon techniques from data integration, data exchange, incremental view maintenance, and view update to propagate updates along mappings, both to derived and optionally to source instances. We incorporate a novel model for tracking data provenance, such that curators may filter updates based on trust conditions over this provenance. We implement our techniques in a layer above an off-the-shelf RDBMS, and we experimentally demonstrate the viability of these techniques in the Orchestra prototype system.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2019390807",
    "type": "article"
  },
  {
    "title": "Efficient Algorithms and Cost Models for Reverse Spatial-Keyword <i>k</i> -Nearest Neighbor Search",
    "doi": "https://doi.org/10.1145/2576232",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Ying Lü; Jiaheng Lu; Gao Cong; Wei Wu; Cyrus Shahabi",
    "corresponding_authors": "",
    "abstract": "Geographic objects associated with descriptive texts are becoming prevalent, justifying the need for spatial-keyword queries that consider both locations and textual descriptions of the objects. Specifically, the relevance of an object to a query is measured by spatial-textual similarity that is based on both spatial proximity and textual similarity. In this article, we introduce the Reverse Spatial-Keyword k -Nearest Neighbor (RSK k NN) query, which finds those objects that have the query as one of their k -nearest spatial-textual objects. The RSK k NN queries have numerous applications in online maps and GIS decision support systems. To answer RSK k NN queries efficiently, we propose a hybrid index tree, called IUR-tree (Intersection-Union R-tree) that effectively combines location proximity with textual similarity. Subsequently, we design a branch-and-bound search algorithm based on the IUR-tree. To accelerate the query processing, we improve IUR-tree by leveraging the distribution of textual description, leading to some variants of the IUR-tree called Clustered IUR-tree (CIUR-tree) and combined clustered IUR-tree (C 2 IUR-tree), for each of which we develop optimized algorithms. We also provide a theoretical cost model to analyze the efficiency of our algorithms. Our empirical studies show that the proposed algorithms are efficient and scalable.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W1971005684",
    "type": "article"
  },
  {
    "title": "Building Efficient Query Engines in a High-Level Language",
    "doi": "https://doi.org/10.1145/3183653",
    "publication_date": "2018-03-31",
    "publication_year": 2018,
    "authors": "Amir Shaikhha; Yannis Klonatos; Christoph Koch",
    "corresponding_authors": "",
    "abstract": "Abstraction without regret refers to the vision of using high-level programming languages for systems development without experiencing a negative impact on performance. A database system designed according to this vision offers both increased productivity and high performance instead of sacrificing the former for the latter as is the case with existing, monolithic implementations that are hard to maintain and extend. In this article, we realize this vision in the domain of analytical query processing. We present LegoBase, a query engine written in the high-level programming language Scala. The key technique to regain efficiency is to apply generative programming: LegoBase performs source-to-source compilation and optimizes database systems code by converting the high-level Scala code to specialized, low-level C code. We show how generative programming allows to easily implement a wide spectrum of optimizations, such as introducing data partitioning or switching from a row to a column data layout, which are difficult to achieve with existing low-level query compilers that handle only queries. We demonstrate that sufficiently powerful abstractions are essential for dealing with the complexity of the optimization effort, shielding developers from compiler internals and decoupling individual optimizations from each other. We evaluate our approach with the TPC-H benchmark and show that (a) with all optimizations enabled, our architecture significantly outperforms a commercial in-memory database as well as an existing query compiler. (b) Programmers need to provide just a few hundred lines of high-level code for implementing the optimizations, instead of complicated low-level code that is required by existing query compilation approaches. (c) These optimizations may potentially come at the cost of using more system memory for improved performance. (d) The compilation overhead is low compared to the overall execution time, thus making our approach usable in practice for compiling query engines.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2797202077",
    "type": "article"
  },
  {
    "title": "Parallelizing Sequential Graph Computations",
    "doi": "https://doi.org/10.1145/3282488",
    "publication_date": "2018-12-16",
    "publication_year": 2018,
    "authors": "Wenfei Fan; Wenyuan Yu; Jingbo Xu; Jingren Zhou; Xiaojian Luo; Qiang Yin; Ping Lü; Yang Cao; Ruiqi Xu",
    "corresponding_authors": "",
    "abstract": "This article presents GRAPE, a parallel &lt;underline&gt;GRAP&lt;/underline&gt;h &lt;underline&gt;E&lt;/underline&gt;ngine for graph computations. GRAPE differs from prior systems in its ability to parallelize existing sequential graph algorithms as a whole, without the need for recasting the entire algorithm into a new model. Underlying GRAPE are a simple programming model and a principled approach based on fixpoint computation that starts with partial evaluation and uses an incremental function as the intermediate consequence operator. We show that users can devise existing sequential graph algorithms with minor additions, and GRAPE parallelizes the computation. Under a monotonic condition, the GRAPE parallelization guarantees to converge at correct answers as long as the sequential algorithms are correct. Moreover, we show that algorithms in MapReduce, BSP, and PRAM can be optimally simulated on GRAPE. In addition to the ease of programming, we experimentally verify that GRAPE achieves comparable performance to the state-of-the-art graph systems using real-life and synthetic graphs.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3001851942",
    "type": "article"
  },
  {
    "title": "Incremental graph pattern matching",
    "doi": "https://doi.org/10.1145/2508020.2489791",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Wenfei Fan; Xin Wang; Yinghui Wu",
    "corresponding_authors": "",
    "abstract": "Graph pattern matching is commonly used in a variety of emerging applications such as social network analysis. These applications highlight the need for studying the following two issues. First, graph pattern matching is traditionally defined in terms of subgraph isomorphism or graph simulation. These notions, however, often impose too strong a topological constraint on graphs to identify meaningful matches. Second, in practice a graph is typically large, and is frequently updated with small changes. It is often prohibitively expensive to recompute matches starting from scratch via batch algorithms when the graph is updated.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W4244071313",
    "type": "article"
  },
  {
    "title": "Scalable Atomic Visibility with RAMP Transactions",
    "doi": "https://doi.org/10.1145/2909870",
    "publication_date": "2016-07-18",
    "publication_year": 2016,
    "authors": "Peter Bailis; Alan Fekete; Ali Ghodsi; Joseph M. Hellerstein; Ion Stoica",
    "corresponding_authors": "",
    "abstract": "Databases can provide scalability by partitioning data across several servers. However, multipartition, multioperation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, many real-world systems avoid mechanisms that provide useful semantics for multipartition operations. This leads to incorrect behavior for a large class of applications including secondary indexing, foreign key enforcement, and materialized view maintenance. In this work, we identify a new isolation model—Read Atomic (RA) isolation—that matches the requirements of these use cases by ensuring atomic visibility : either all or none of each transaction’s updates are observed by other transactions. We present algorithms for Read Atomic Multipartition (RAMP) transactions that enforce atomic visibility while offering excellent scalability, guaranteed commit despite partial failures (via coordination-free execution ), and minimized communication between servers (via partition independence ). These RAMP transactions correctly mediate atomic visibility of updates and provide readers with snapshot access to database state by using limited multiversioning and by allowing clients to independently resolve nonatomic reads. We demonstrate that, in contrast with existing algorithms, RAMP transactions incur limited overhead—even under high contention—and scale linearly to 100 servers.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2497521232",
    "type": "article"
  },
  {
    "title": "Classification of annotation semirings over containment of conjunctive queries",
    "doi": "https://doi.org/10.1145/2556524",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Egor V. Kostylev; Juan L. Reutter; András Z. Salamon",
    "corresponding_authors": "",
    "abstract": "We study the problem of query containment of conjunctive queries over annotated databases. Annotations are typically attached to tuples and represent metadata, such as probability, multiplicity, comments, or provenance. It is usually assumed that annotations are drawn from a commutative semiring. Such databases pose new challenges in query optimization, since many related fundamental tasks, such as query containment, have to be reconsidered in the presence of propagation of annotations. We axiomatize several classes of semirings for each of which containment of conjunctive queries is equivalent to existence of a particular type of homomorphism. For each of these types, we also specify all semirings for which existence of a corresponding homomorphism is a sufficient (or necessary) condition for the containment. We develop new decision procedures for containment for some semirings which are not in any of these classes. This generalizes and systematizes previous approaches.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1967893237",
    "type": "article"
  },
  {
    "title": "Uncertain Graph Processing through Representative Instances",
    "doi": "https://doi.org/10.1145/2818182",
    "publication_date": "2015-10-23",
    "publication_year": 2015,
    "authors": "Panos Parchas; Francesco Gullo; Dimitris Papadias; Francesco Bonchi",
    "corresponding_authors": "",
    "abstract": "Data in several applications can be represented as an uncertain graph whose edges are labeled with a probability of existence. Exact query processing on uncertain graphs is prohibitive for most applications, as it involves evaluation over an exponential number of instantiations. Thus, typical approaches employ Monte-Carlo sampling, which (i) draws a number of possible graphs (samples), (ii) evaluates the query on each of them, and (iii) aggregates the individual answers to generate the final result. However, this approach can also be extremely time consuming for large uncertain graphs commonly found in practice. To facilitate efficiency, we study the problem of extracting a single representative instance from an uncertain graph. Conventional processing techniques can then be applied on this representative to closely approximate the result on the original graph. In order to maintain data utility, the representative instance should preserve structural characteristics of the uncertain graph. We start with representatives that capture the expected vertex degrees, as this is a fundamental property of the graph topology. We then generalize the notion of vertex degree to the concept of n -clique cardinality, that is, the number of cliques of size n that contain a vertex. For the first problem, we propose two methods: Average Degree Rewiring (ADR), which is based on random edge rewiring, and Approximate B-Matching (ABM), which applies graph matching techniques. For the second problem, we develop a greedy approach and a game-theoretic framework. We experimentally demonstrate, with real uncertain graphs, that indeed the representative instances can be used to answer, efficiently and accurately, queries based on several metrics such as shortest path distance, clustering coefficient, and betweenness centrality.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1998369386",
    "type": "article"
  },
  {
    "title": "Exact and Approximate Maximum Inner Product Search with LEMP",
    "doi": "https://doi.org/10.1145/2996452",
    "publication_date": "2016-12-03",
    "publication_year": 2016,
    "authors": "Christina Teflioudi; Rainer Gemulla",
    "corresponding_authors": "",
    "abstract": "We study exact and approximate methods for maximum inner product search, a fundamental problem in a number of data mining and information retrieval tasks. We propose the LEMP framework, which supports both exact and approximate search with quality guarantees. At its heart, LEMP transforms a maximum inner product search problem over a large database of vectors into a number of smaller cosine similarity search problems. This transformation allows LEMP to prune large parts of the search space immediately and to select suitable search algorithms for each of the remaining problems individually. LEMP is able to leverage existing methods for cosine similarity search, but we also provide a number of novel search algorithms tailored to our setting. We conducted an extensive experimental study that provides insight into the performance of many state-of-the-art techniques—including LEMP—on multiple real-world datasets. We found that LEMP often was significantly faster or more accurate than alternative methods.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2560304626",
    "type": "article"
  },
  {
    "title": "Guarded-Based Disjunctive Tuple-Generating Dependencies",
    "doi": "https://doi.org/10.1145/2976736",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Pierre Bourhis; Marco Manna; Michael Morak; Andréas Pieris",
    "corresponding_authors": "",
    "abstract": "We perform an in-depth complexity analysis of query answering under guarded-based classes of disjunctive tuple-generating dependencies (DTGDs), focusing on (unions of) conjunctive queries ((U)CQs). We show that the problem under investigation is very hard, namely 2E xp T ime -complete, even for fixed sets of dependencies of a very restricted form. This is a surprising lower bound that demonstrates the enormous impact of disjunction on query answering under guarded-based tuple-generating dependencies, and also reveals the source of complexity for expressive logics such as the guarded fragment of first-order logic. We then proceed to investigate whether prominent subclasses of (U)CQs (i.e., queries of bounded treewidth and hypertree-width, and acyclic queries) have a positive impact on the complexity of the problem under consideration. We show that queries of bounded treewidth and bounded hypertree-width do not reduce the complexity of our problem, even if we focus on predicates of bounded arity or on fixed sets of DTGDs. Regarding acyclic queries, although the problem remains 2E xp T ime -complete in general, in some relevant settings the complexity reduces to E xp T ime -complete. Finally, with the aim of identifying tractable cases, we focus our attention on atomic queries. We show that atomic queries do not make the query answering problem easier under classes of guarded-based DTGDs that allow more than one atom to occur in the body of the dependencies. However, the complexity significantly decreases in the case of dependencies that can have only one atom in the body. In particular, we obtain a P time -completeness if we focus on predicates of bounded arity, and AC 0 -membership when the set of dependencies and the query are fixed. Interestingly, our results can be used as a generic tool for establishing complexity results for query answering under various description logics.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2533772561",
    "type": "article"
  },
  {
    "title": "Outlier Detection over Massive-Scale Trajectory Streams",
    "doi": "https://doi.org/10.1145/3013527",
    "publication_date": "2017-04-28",
    "publication_year": 2017,
    "authors": "Yanwei Yu; Lei Cao; Elke A. Rundensteiner; Qin Wang",
    "corresponding_authors": "",
    "abstract": "The detection of abnormal moving objects over high-volume trajectory streams is critical for real-time applications ranging from military surveillance to transportation management. Yet this outlier detection problem, especially along both the spatial and temporal dimensions, remains largely unexplored. In this work, we propose a rich taxonomy of novel classes of neighbor-based trajectory outlier definitions that model the anomalous behavior of moving objects for a large range of real-time applications. Our theoretical analysis and empirical study on two real-world datasets—the Beijing Taxi trajectory data and the Ground Moving Target Indicator data stream—and one generated Moving Objects dataset demonstrate the effectiveness of our taxonomy in effectively capturing different types of abnormal moving objects. Furthermore, we propose a general strategy for efficiently detecting these new outlier classes called the &lt;underline&gt;m&lt;/underline&gt;inimal &lt;underline&gt;ex&lt;/underline&gt;amination (MEX) framework. The MEX framework features three core optimization principles, which leverage spatiotemporal as well as the predictability properties of the neighbor evidence to minimize the detection costs. Based on this foundation, we design algorithms that detect the outliers based on these classes of new outlier semantics that successfully leverage our optimization principles. Our comprehensive experimental study demonstrates that our proposed MEX strategy drives the detection costs 100-fold down into the practical realm for applications that analyze high-volume trajectory streams in near real time.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2607545413",
    "type": "article"
  },
  {
    "title": "Extending the Kernel of a Relational DBMS with Comprehensive Support for Sequenced Temporal Queries",
    "doi": "https://doi.org/10.1145/2967608",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Anton Dignös; Michael H. Böhlen; Johann Gamper; Christian S. Jensen",
    "corresponding_authors": "",
    "abstract": "Many databases contain temporal, or time-referenced, data and use intervals to capture the temporal aspect. While SQL-based database management systems (DBMSs) are capable of supporting the management of interval data, the support they offer can be improved considerably. A range of proposed temporal data models and query languages offer ample evidence to this effect. Natural queries that are very difficult to formulate in SQL are easy to formulate in these temporal query languages. The increased focus on analytics over historical data where queries are generally more complex exacerbates the difficulties and thus the potential benefits of a temporal query language. Commercial DBMSs have recently started to offer limited temporal functionality in a step-by-step manner, focusing on the representation of intervals and neglecting the implementation of the query evaluation engine. This article demonstrates how it is possible to extend the relational database engine to achieve a full-fledged, industrial-strength implementation of sequenced temporal queries, which intuitively are queries that are evaluated at each time point. Our approach reduces temporal queries to nontemporal queries over data with adjusted intervals, and it leaves the processing of nontemporal queries unaffected. Specifically, the approach hinges on three concepts: interval adjustment , timestamp propagation , and attribute scaling . Interval adjustment is enabled by introducing two new relational operators, a temporal normalizer and a temporal aligner, and the latter two concepts are enabled by the replication of timestamp attributes and the use of so-called scaling functions. By providing a set of reduction rules, we can transform any temporal query, expressed in terms of temporal relational operators, to a query expressed in terms of relational operators and the two new operators. We prove that the size of a transformed query is linear in the number of temporal operators in the original query. An integration of the new operators and the transformation rules, along with query optimization rules, into the kernel of PostgreSQL is reported. Empirical studies with the resulting temporal DBMS are covered that offer insights into pertinent design properties of the article's proposal. The new system is available as open-source software.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2546460896",
    "type": "article"
  },
  {
    "title": "Learning Models over Relational Data Using Sparse Tensors and Functional Dependencies",
    "doi": "https://doi.org/10.1145/3375661",
    "publication_date": "2020-06-27",
    "publication_year": 2020,
    "authors": "Mahmoud Abo Khamis; Hung Q. Ngo; XuanLong Nguyen; Dan Olteanu; Maximilian Schleich",
    "corresponding_authors": "",
    "abstract": "Integrated solutions for analytics over relational databases are of great practical importance as they avoid the costly repeated loop data scientists have to deal with on a daily basis: select features from data residing in relational databases using feature extraction queries involving joins, projections, and aggregations; export the training dataset defined by such queries; convert this dataset into the format of an external learning tool; and train the desired model using this tool. These integrated solutions are also a fertile ground of theoretically fundamental and challenging problems at the intersection of relational and statistical data models. This article introduces a unified framework for training and evaluating a class of statistical learning models over relational databases. This class includes ridge linear regression, polynomial regression, factorization machines, and principal component analysis. We show that, by synergizing key tools from database theory such as schema information, query structure, functional dependencies, recent advances in query evaluation algorithms, and from linear algebra such as tensor and matrix operations, one can formulate relational analytics problems and design efficient (query and data) structure-aware algorithms to solve them. This theoretical development informed the design and implementation of the AC/DC system for structure-aware learning. We benchmark the performance of AC/DC against R, MADlib, libFM, and TensorFlow. For typical retail forecasting and advertisement planning applications, AC/DC can learn polynomial regression models and factorization machines with at least the same accuracy as its competitors and up to three orders of magnitude faster than its competitors whenever they do not run out of memory, exceed 24-hour timeout, or encounter internal design limitations.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3038576231",
    "type": "article"
  },
  {
    "title": "A Declarative Framework for Linking Entities",
    "doi": "https://doi.org/10.1145/2894748",
    "publication_date": "2016-07-18",
    "publication_year": 2016,
    "authors": "Douglas Burdick; Ronald Fagin; Phokion G. Kolaitis; Lucian Popa; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "We introduce and develop a declarative framework for entity linking and, in particular, for entity resolution. As in some earlier approaches, our framework is based on a systematic use of constraints. However, the constraints we adopt are link-to-source constraints, unlike in earlier approaches where source-to-link constraints were used to dictate how to generate links. Our approach makes it possible to focus entirely on the intended properties of the outcome of entity linking, thus separating the constraints from any procedure of how to achieve that outcome. The core language consists of link-to-source constraints that specify the desired properties of a link relation in terms of source relations and built-in predicates such as similarity measures. A key feature of the link-to-source constraints is that they employ disjunction, which enables the declarative listing of all the reasons two entities should be linked. We also consider extensions of the core language that capture collective entity resolution by allowing interdependencies among the link relations. We identify a class of “good” solutions for entity-linking specifications, which we call maximum-value solutions and which capture the strength of a link by counting the reasons that justify it. We study natural algorithmic problems associated with these solutions, including the problem of enumerating the “good” solutions and the problem of finding the certain links, which are the links that appear in every “good” solution. We show that these problems are tractable for the core language but may become intractable once we allow interdependencies among the link relations. We also make some surprising connections between our declarative framework, which is deterministic, and probabilistic approaches such as ones based on Markov Logic Networks.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2479722630",
    "type": "article"
  },
  {
    "title": "Joins via Geometric Resolutions",
    "doi": "https://doi.org/10.1145/2967101",
    "publication_date": "2016-11-08",
    "publication_year": 2016,
    "authors": "Mahmoud Abo Khamis; Hung Q. Ngo; Christopher Ré; Atri Rudra",
    "corresponding_authors": "",
    "abstract": "We present a simple geometric framework for the relational join. Using this framework, we design an algorithm that achieves the fractional hypertree-width bound, which generalizes classical and recent worst-case algorithmic results on computing joins. In addition, we use our framework and the same algorithm to show a series of what are colloquially known as beyond worst-case results. The framework allows us to prove results for data stored in BTrees, multidimensional data structures, and even multiple indices per table. A key idea in our framework is formalizing the inference one does with an index as a type of geometric resolution, transforming the algorithmic problem of computing joins to a geometric problem. Our notion of geometric resolution can be viewed as a geometric analog of logical resolution. In addition to the geometry and logic connections, our algorithm can also be thought of as backtracking search with memoization.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2552956925",
    "type": "article"
  },
  {
    "title": "Computing Optimal Repairs for Functional Dependencies",
    "doi": "https://doi.org/10.1145/3360904",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Ester Livshits; Benny Kimelfeld; Sudeepa Roy",
    "corresponding_authors": "",
    "abstract": "We investigate the complexity of computing an optimal repair of an inconsistent database, in the case where integrity constraints are Functional Dependencies (FDs). We focus on two types of repairs: an optimal subset repair (optimal S-repair), which is obtained by a minimum number of tuple deletions, and an optimal update repair (optimal U-repair), which is obtained by a minimum number of value (cell) updates. For computing an optimal S-repair, we present a polynomial-time algorithm that succeeds on certain sets of FDs and fails on others. We prove the following about the algorithm. When it succeeds, it can also incorporate weighted tuples and duplicate tuples. When it fails, the problem is NP-hard and, in fact, APX-complete (hence, cannot be approximated better than some constant). Thus, we establish a dichotomy in the complexity of computing an optimal S-repair. We present general analysis techniques for the complexity of computing an optimal U-repair, some based on the dichotomy for S-repairs. We also draw a connection to a past dichotomy in the complexity of finding a “most probable database” that satisfies a set of FDs with a single attribute on the left-hand side; the case of general FDs was left open, and we show how our dichotomy provides the missing generalization and thereby settles the open problem.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3008589870",
    "type": "article"
  },
  {
    "title": "Discovering Graph Functional Dependencies",
    "doi": "https://doi.org/10.1145/3397198",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Wenfei Fan; Chunming Hu; Xueli Liu; Ping Lü",
    "corresponding_authors": "",
    "abstract": "This article studies discovery of Graph Functional Dependencies (GFDs), a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard in general. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3041128325",
    "type": "article"
  },
  {
    "title": "Supporting Better Insights of Data Science Pipelines with Fine-grained Provenance",
    "doi": "https://doi.org/10.1145/3644385",
    "publication_date": "2024-02-09",
    "publication_year": 2024,
    "authors": "Adriane Chapman; Luca Lauro; Paolo Missier; Riccardo Torlone",
    "corresponding_authors": "",
    "abstract": "Successful data-driven science requires complex data engineering pipelines to clean, transform, and alter data in preparation for machine learning, and robust results can only be achieved when each step in the pipeline can be justified, and its effect on the data explained. In this framework, we aim at providing data scientists with facilities to gain an in-depth understanding of how each step in the pipeline affects the data, from the raw input to training sets ready to be used for learning. Starting from an extensible set of data preparation operators commonly used within a data science setting, in this work we present a provenance management infrastructure for generating, storing, and querying very granular accounts of data transformations, at the level of individual elements within datasets whenever possible. Then, from the formal definition of a core set of data science preprocessing operators, we derive a provenance semantics embodied by a collection of templates expressed in PROV, a standard model for data provenance. Using those templates as a reference, our provenance generation algorithm generalises to any operator with observable input/output pairs. We provide a prototype implementation of an application-level provenance capture library to produce, in a semi-automatic way, complete provenance documents that account for the entire pipeline. We report on the ability of that reference implementation to capture provenance in real ML benchmark pipelines and over TCP-DI synthetic data. We finally show how the collected provenance can be used to answer a suite of provenance benchmark queries that underpin some common pipeline inspection questions, as expressed on the Data Science Stack Exchange.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4391682392",
    "type": "article"
  },
  {
    "title": "Apache IoTDB: A Time Series Database for Large Scale IoT Applications",
    "doi": "https://doi.org/10.1145/3726523",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Chen Wang; Jialin Qiao; Xiangdong Huang; Shaoxu Song; Haonan Hou; Tian Jiang; Lei Rui; Jianmin Wang; Jiaguang Sun",
    "corresponding_authors": "",
    "abstract": "A typical industrial scenario encounters thousands of devices with millions of sensors, consistently generating billions of data points. It poses new requirements of time series data management, not well addressed in existing solutions, including (1) device-defined ever-evolving schema, (2) mostly periodical data collection, (3) strongly correlated series, (4) variously delayed data arrival, and (5) highly concurrent data ingestion. In this paper, we present a time series database management system, Apache IoTDB. It consists of (i) a time series native file format, TsFile, with specially designed data encoding, and (ii) an IoTDB engine for efficiently handling delayed data arrivals and processing queries. We introduce a native distributed solution with distributed queries optimized by parallel operators. We also explore efficient TsFile synchronization mechanisms, ensuring seamless data integration without the need for ETL processes. The system achieves a throughput of 10 million inserted values per second. Queries such as 1-day data selection of 0.1 million points and 3-year data aggregation over 10 million points can be processed in 100 ms. Comparisons with InfluxDB, TimescaleDB, KairosDB, Parquet and ORC over real world data loads demonstrate the superiority of IoTDB and TsFile.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408943130",
    "type": "article"
  },
  {
    "title": "Probabilistic object bases",
    "doi": "https://doi.org/10.1145/502030.502031",
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Thomas Eiter; James Lu; Thomas Lukasiewicz; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "Although there are many applications where an object-oriented data model is a good way of representing and querying data, current object database systems are unable to handle objects whose attributes are uncertain. In this article, we extend previous work by Kornatzky and Shimony to develop an algebra to handle object bases with uncertainty. We propose concepts of consistency for such object bases, together with an NP-completeness result, and classes of probabilistic object bases for which consistency is polynomially checkable. In addition, as certain operations involve conjunctions and disjunctions of events, and as the probability of conjunctive and disjunctive events depends both on the probabilities of the primitive events involved as well as on what is known (if anything) about the relationship between the events, we show how all our algebraic operations may be performed under arbitrary probabilistic conjunction and disjunction strategies. We also develop a host of equivalence results in our algebra, which may be used as rewrite rules for query optimization. Last but not least, we have developed a prototype probabilistic object base server on top of ObjectStore. We describe experiments to assess the efficiency of different possible rewrite rules.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2003203425",
    "type": "article"
  },
  {
    "title": "A structured approach for the definition of the semantics of active databases",
    "doi": "https://doi.org/10.1145/219035.219042",
    "publication_date": "1995-12-01",
    "publication_year": 1995,
    "authors": "Piero Fraternali; Letizia Tanca",
    "corresponding_authors": "",
    "abstract": "Active DBMSs couple database technology with rule-based programming to achieve the capability of reaction to database (and possibly external) stimuli, called events . The reactive capabilities of active databases are useful for a wide spectrum of applications, including security, view materialization, integrity checking and enforcement, or heterogeneous database integration, which makes this technology very promising for the near future. An active database system consists of a (passive) database and a set of active rules ; the most popular form of active rule is the so-called event-condition-action (ECA) rule, which specifies an action to be executed upon the occurrence of one or more events, provided that a condition holds. Several active database systems and prototypes have been designed and partially or completely implemented. Unfortunately, they have been designed in a totally independent way, without the support of a common theory dictating the semantics of ECA rules, and thus often show different behaviors for rules with a similar form. In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA . A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2038726607",
    "type": "article"
  },
  {
    "title": "Querying ATSQL databases with temporal logic",
    "doi": "https://doi.org/10.1145/383891.383892",
    "publication_date": "2001-06-01",
    "publication_year": 2001,
    "authors": "Jan Chomicki; David Toman; Michael H. Böhlen",
    "corresponding_authors": "",
    "abstract": "We establish a correspondence between temporal logic and a subset of ATSQL, a temporal extension of SQL-92. In addition, we provide an effective translation from temporal logic to ATSQL that enables a user to write high-level queries which are then evaluated against a space-efficient representation of the database. A reverse translation, also provided in this paper, characterizes the expressive power of a syntactically defined subset of ATSQL queries.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2054655669",
    "type": "article"
  },
  {
    "title": "Improving database design through the analysis of relationships",
    "doi": "https://doi.org/10.1145/331983.331984",
    "publication_date": "1999-12-01",
    "publication_year": 1999,
    "authors": "Debabrata Dey; Veda C. Storey; Terence M. Barron",
    "corresponding_authors": "",
    "abstract": "Much of the work on conceptual modeling involves the use of an entity-relationship model in which binary relationships appear as associations between two entities. Relationships involving more than two entities are considered rare and, therefore, have not received adequate attention. This research provides a general framework for the analysis of relationships in which binary relationships simply become a special case. The framework helps a designer to identify ternary and other higher-degree relationships that are commonly represented, often inappropriately, as either entities or binary relationships. Generalized rules are also provided for representing higher-degree relationships in the relational model. This uniform treatment of relationships should significantly ease the burden on a designer by enabling him or her to extract more information from a real-world situation and represent it properly in a conceptual design.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W1986899967",
    "type": "article"
  },
  {
    "title": "The logical data model",
    "doi": "https://doi.org/10.1145/155271.155274",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Gabriel M. Kuper; Moshe Y. Vardi",
    "corresponding_authors": "",
    "abstract": "We propose an object-oriented data model that generalizes the relational, hierarchical, and network models. A database scheme in this model is a directed graph, whose leaves represent data and whose internal nodes represent connections among the data. Instances are constructed from objects, which have separate names and values. We define a logic for the model, and describe a nonprocedural query language that is based on the logic. We also describe an algebraic query language and show that it is equivalent to the logical language.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2036461602",
    "type": "article"
  },
  {
    "title": "Polymorphism and type inference in database programming",
    "doi": "https://doi.org/10.1145/227604.227609",
    "publication_date": "1996-03-01",
    "publication_year": 1996,
    "authors": "Peter Buneman; Atsushi Ohori",
    "corresponding_authors": "",
    "abstract": "In order to find a static type system that adequately supports database languages, we need to express the most general type of a program that involves database operations. This can be achieved through an extension to the type system of ML that captures the polymorphic nation of field selection, together with a techniques that generalizes relational operators to arbitrary data structures. The combination provides a statically typed language in which generalized relational databases may be cleanly represented as typed structures. As in ML types are inferred, which relieves the programmer of making the type assertions that may be required in a complex database environment. These extensions may also be used to provide static polymorphic typechecking in object-oriented languages and databases. A problem that arises with object-oriented databases is the apparent need for dynamic typechecking when dealing queries on heterogeneous collections of objects. An extension of the type system needed for generalized relational operations can also be used for manipulating collections of dynamically typed values in a statically typed language. A prototype language based on these ideas has been implemented. While it lacks a proper treatment of persistent data, it demonstrates that a wide variety of database structures can be cleanly represented in a polymorphic programming language.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2151856838",
    "type": "article"
  },
  {
    "title": "Adaptive algorithms for set containment joins",
    "doi": "https://doi.org/10.1145/762471.762474",
    "publication_date": "2003-03-01",
    "publication_year": 2003,
    "authors": "Sergey Melnik; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "A set containment join is a join between set-valued attributes of two relations, whose join condition is specified using the subset (⊆) operator. Set containment joins are deployed in many database applications, even those that do not support set-valued attributes. In this article, we propose two novel partitioning algorithms, called the Adaptive Pick-and-Sweep Join (APSJ) and the Adaptive Divide-and-Conquer Join (ADCJ), which allow computing set containment joins efficiently. We show that APSJ outperforms previously suggested algorithms for many data sets, often by an order of magnitude. We present a detailed analysis of the algorithms and study their performance on real and synthetic data using an implemented testbed.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2010454887",
    "type": "article"
  },
  {
    "title": "Optimizing equijoin queries in distributed databases where relations are hash partitioned",
    "doi": "https://doi.org/10.1145/114325.103713",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Dennis Shasha; Tsong-Li Wang",
    "corresponding_authors": "",
    "abstract": "Consider the class of distributed database systems consisting of a set of nodes connected by a high bandwidth network. Each node consists of a processor, a random access memory, and a slower but much larger memory such as a disk. There is no shared memory among the nodes. The data are horizontally partitioned often using a hash function. Such a description characterizes many parallel or distributed database systems that have recently been proposed, both commercial and academic. We study the optimization problem that arises when the query processor must repartition the relations and intermediate results participating in a multijoin query. Using estimates of the sizes of intermediate relations, we show (1) optimum solutions for closed chain queries; (2) the NP-completeness of the optimization problem for star, tree, and general graph queries; and (3) effective heuristics for these hard cases. Our general approach and many of our results extend to other attribute partitioning schemes, for example, sort-partitioning on attributes, and to partitioned object databases.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2139018928",
    "type": "article"
  },
  {
    "title": "Empirical performance evaluation of concurrency and coherency control protocols for database sharing systems",
    "doi": "https://doi.org/10.1145/151634.151639",
    "publication_date": "1993-06-01",
    "publication_year": 1993,
    "authors": "Erhard Rahm",
    "corresponding_authors": "Erhard Rahm",
    "abstract": "Database Sharing (DB-sharing) refers to a general approach for building a distributed high performance transaction system. The nodes of a DB-sharing system are locally coupled via a high-speed interconnect and share a common database at the disk level. This is also known as a “shared disk” approach. We compare database sharing with the database partitioning (shared nothing) approach and discuss the functional DBMS components that require new and coordinated solutions for DB-sharing. The performance of DB-sharing systems critically depends on the protocols used for concurrency and coherency control. The frequency of communication required for these functions has to be kept as low as possible in order to achieve high transation rates and short response times. A trace-driven simulation system for DB-sharing complexes has been developed that allows a realistic performance comparison of four different concurrency and coherency control protocols. We consider two locking and two optimistic schemes which operate either under central or distributed control. For coherency control, we investigate so-called on-request and broadcast invalidation schemes, and employ buffer-to-buffer communication to exchange modified pages directly between different nodes. The performance impact of random routing versus affinity-based load distribution and different communication costs is also examined. In addition, we analyze potential performance bottlenecks created by hot spot pages.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2098836692",
    "type": "article"
  },
  {
    "title": "View updates in relational databases with an independent scheme",
    "doi": "https://doi.org/10.1145/77643.77645",
    "publication_date": "1990-03-01",
    "publication_year": 1990,
    "authors": "Rom Langerak",
    "corresponding_authors": "Rom Langerak",
    "abstract": "A view on a database is a mapping that provides a user or application with a suitable way of looking at the data. Updates specified on a view have to be translated into updates on the underlying database. We study the view update translation problem for a relational data model in which the base relations may contain (indexed) nulls. The representative instance is considered to be the correct representation of all data in the database; the class of views that is studied consists of total projections of the representative instance. Only independent database schemes are considered, that is, schemes for which global consistency is implied by local consistency. A view update can be an insertion, a deletion, or a modification of a single view tuple. It is proven that the constant complement method of Bancilhon and Spyratos is too restrictive to be useful in this context. Structural properties of extension joins are derived that are important for understanding views. On the basis of these properties, minimal algorithms for translating a single view-tuple update are given.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2055099648",
    "type": "article"
  },
  {
    "title": "Consistency and orderability",
    "doi": "https://doi.org/10.1145/155271.155276",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Divyakant Agrawal; Amr El Abbadi; Ambuj K. Singh",
    "corresponding_authors": "",
    "abstract": "The semantics of objects and transactions in database systems are investigated. User-defined predicates called consistency assertions are used to specify user programs. Three new correctness criteria are proposed. The first correctness criterion consistency is based solely on the users' specifications and admit nonserializable executions that are acceptable to the users. Integrity constraints of the database are maintained through consistency assertions. The second correctness criterion orderability is a generalization of view serializability and represents a weak notion of equivalence to a serial schedule. Finally, the third correctness criterion strong order-ability is introduced as a generalization of conflict serializability. Unlike consistency, the notions of orderability allow users to operate an isolation as maintenance of the integrity constrainst now becomes the responsibility of the database system.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W1997182736",
    "type": "article"
  },
  {
    "title": "Statistical estimators for aggregate relational algebra queries",
    "doi": "https://doi.org/10.1145/115302.115300",
    "publication_date": "1991-12-01",
    "publication_year": 1991,
    "authors": "Wen‐Chi Hou; Gültekin Özsoyoğlu",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Statistical estimators for aggregate relational algebra queries Authors: Wen-Chi Hou Case Western Reserve Univ., Cleveland, OH Case Western Reserve Univ., Cleveland, OHView Profile , Gultekin Ozsoyoglu Case Western Reserve Univ., Cleveland, OH Case Western Reserve Univ., Cleveland, OHView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 16Issue 4pp 600–654https://doi.org/10.1145/115302.115300Published:01 December 1991Publication History 44citation559DownloadsMetricsTotal Citations44Total Downloads559Last 12 Months24Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2108717033",
    "type": "article"
  },
  {
    "title": "Relational languages for metadata integration",
    "doi": "https://doi.org/10.1145/1071610.1071618",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Catharine M. Wyss; Edward L. Robertson",
    "corresponding_authors": "",
    "abstract": "In this article, we develop a relational algebra for metadata integration, Federated Interoperable Relational Algebra (FIRA). FIRA has many desirable properties such as compositionality, closure, a deterministic semantics, a modest complexity, support for nested queries, a subalgebra equivalent to canonical Relational Algebra (RA), and robustness under certain classes of schema evolution. Beyond this, FIRA queries are capable of producing fully dynamic output schemas, where the number of relations and/or the number of columns in relations of the output varies dynamically with the input instance. Among existing query languages for relational metadata integration, only FIRA provides generalized dynamic output schemas, where the values in any (fixed) number of input columns can determine output schemas.Further contributions of this article include development of an extended relational model for metadata integration, the Federated Relational Data Model , which is strictly downward compatible with the relational model. Additionally, we define the notion of Transformational Completeness for relational query languages and postulate FIRA as a canonical transformationally complete language. We also give a declarative, SQL-like query language that is equivalent to FIRA, called Federated Interoperable Structured Query Language (FISQL).While our main contributions are conceptual, the federated model, FISQL/FIRA, and the notion of transformational completeness nevertheless have important applications to data integration and OLAP. In addition to summarizing these applications, we illustrate the use of FIRA to optimize FISQL queries using rule-based transformations that directly parallel their canonical relational counterparts. We conclude the article with an extended discussion of related work as well as an indication of current and future work on FISQL/FIRA.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2114766826",
    "type": "article"
  },
  {
    "title": "Adaptive rank-aware query optimization in relational databases",
    "doi": "https://doi.org/10.1145/1189769.1189772",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Ihab F. Ilyas; Walid G. Aref; Ahmed K. Elmagarmid; Hicham G. Elmongui; Rahul Shah; Jeffrey Scott Vitter",
    "corresponding_authors": "",
    "abstract": "Rank-aware query processing has emerged as a key requirement in modern applications. In these applications, efficient and adaptive evaluation of top- k queries is an integral part of the application semantics. In this article, we introduce a rank-aware query optimization framework that fully integrates rank-join operators into relational query engines. The framework is based on extending the System R dynamic programming algorithm in both enumeration and pruning. We define ranking as an interesting physical property that triggers the generation of rank-aware query plans. Unlike traditional join operators, optimizing for rank-join operators depends on estimating the input cardinality of these operators. We introduce a probabilistic model for estimating the input cardinality, and hence the cost of a rank-join operator. To our knowledge, this is the first effort in estimating the needed input size for optimal rank aggregation algorithms. Costing ranking plans is key to the full integration of rank-join operators in real-world query processing engines.Since optimal execution strategies picked by static query optimizers lose their optimality due to estimation errors and unexpected changes in the computing environment, we introduce several adaptive execution strategies for top- k queries that respond to these unexpected changes and costing errors. Our reactive reoptimization techniques change the execution plan at runtime to significantly enhance the performance of running queries. Since top- k query plans are usually pipelined and maintain a complex ranking state, altering the execution strategy of a running ranking query is an important and challenging task.We conduct an extensive experimental study to evaluate the performance of the proposed framework. The experimental results are twofold: (1) we show the effectiveness of our cost-based approach of integrating ranking plans in dynamic programming cost-based optimizers; and (2) we show a significant speedup (up to 300%) when using our adaptive execution of ranking plans over the state-of-the-art mid-query reoptimization strategies.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2118265701",
    "type": "article"
  },
  {
    "title": "Index scans using a finite LRU buffer: a validated I/O model",
    "doi": "https://doi.org/10.1145/68012.68016",
    "publication_date": "1989-09-01",
    "publication_year": 1989,
    "authors": "Lothar F. Mackert; Guy M. Lohman",
    "corresponding_authors": "",
    "abstract": "Indexes are commonly employed to retrieve a portion of a file or to retrieve its records in a particular order. An accurate performance model of indexes is essential to the design, analysis, and tuning of file management and database systems, and particularly to database query optimization. Many previous studies have addressed the problem of estimating the number of disk page fetches when randomly accessing k records out of N given records stored on T disk pages. This paper generalizes these results, relaxing two assumptions that usually do not hold in practice: unlimited buffer and unique records for each key value. Experiments show that the performance of an index scan is very sensitive to buffer size limitations and multiple records per key value. A model for these more practical situations is presented and a formula derived for estimating the performance of an index scan. We also give a closed-form approximation that is easy to compute. The theoretical results are validated using the R * distributed relational database system. Although we use database terminology throughout the paper, the model is more generally applicable whenever random accesses are made using keys.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2040892657",
    "type": "article"
  },
  {
    "title": "XSQ",
    "doi": "https://doi.org/10.1145/1071610.1071617",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Feng Peng; Sudarshan S. Chawathe",
    "corresponding_authors": "",
    "abstract": "We have implemented and released the XSQ system for evaluating XPath queries on streaming XML data. XSQ supports XPath features such as multiple predicates, closures, and aggregation, which pose interesting challenges for streaming evaluation. Our implementation is based on using a hierarchical arrangement of augmented finite state automata. A design goal of XSQ is buffering data for the least amount of time possible. We present a detailed experimental study that characterizes the performance of XSQ and related systems, and that illustrates the performance implications of XPath features such as closures.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2172247142",
    "type": "article"
  },
  {
    "title": "Automatic deduction of temporal information",
    "doi": "https://doi.org/10.1145/146931.146934",
    "publication_date": "1992-12-01",
    "publication_year": 1992,
    "authors": "Roberto Maiocchi; Barbara Pernici; Federico Barbic",
    "corresponding_authors": "",
    "abstract": "In many computer-based applications, temporal information has to be stored, retrieved, and related to other temporal information. Several time models have been proposed to manage temporal knowledge in the fields of conceptual modeling, database systems, and artificial intelligence. In this paper we present TSOS, a system for reasoning about time that can be integrated as a time expert in environments designed for broader problem-solving domains. The main intended goal of TSOS is to allow a user to infer further information on the temporal data stored in the database through a set of deduction rules handling various aspects of time. For this purpose, TSOS provides the capability of answering queries about the temporal specifications it has in its temporal database. Distinctive time-modeling features of TSOS are the introduction of temporal modalitites , i.e., the possibility of specifying if a piece of information is always true within a time interval, or if it is only sometimes true, and the capability of answering about the possibility and the necessity of the validity of some information at a given time, the association of temporal knowledge both to instances of data and to types of data , and the development of a time calculus for reasoning on temporal data. Another relevant feature of TSOS is the capability to reason about temporal data specified at different time granularities.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2169216195",
    "type": "article"
  },
  {
    "title": "Theory of nearest neighbors indexability",
    "doi": "https://doi.org/10.1145/1166074.1166077",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Uri Shaft; Raghu Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "In this article, we consider whether traditional index structures are effective in processing unstable nearest neighbors workloads. It is known that under broad conditions, nearest neighbors workloads become unstable ---distances between data points become indistinguishable from each other. We complement this earlier result by showing that if the workload for an application is unstable, you are not likely to be able to index it efficiently using (almost all known) multidimensional index structures. For a broad class of data distributions, we prove that these index structures will do no better than a linear scan of the data as dimensionality increases.Our result has implications for how experiments should be designed on index structures such as R-Trees, X-Trees, and SR-Trees: simply put, experiments trying to establish that these index structures scale with dimensionality should be designed to establish crossover points , rather than to show that the methods scale to an arbitrary number of dimensions. In other words, experiments should seek to establish the dimensionality of the dataset at which the proposed index structure deteriorates to linear scan, for each data distribution of interest; that linear scan will eventually dominate is a given.An important problem is to analytically characterize the rate at which index structures degrade with increasing dimensionality, because the dimensionality of a real data set may well be in the range that a particular method can handle. The results in this article can be regarded as a step toward solving this problem. Although we do not characterize the rate at which a structure degrades, our techniques allow us to reason directly about a broad class of index structures rather than the geometry of the nearest neighbors problem, in contrast to earlier work.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2012509146",
    "type": "article"
  },
  {
    "title": "Query processing techniques in the summary-table-by-example database query language",
    "doi": "https://doi.org/10.1145/76902.76906",
    "publication_date": "1989-12-01",
    "publication_year": 1989,
    "authors": "Gültekin Özsoyoğlu; Victor Matos; Meral Özsoyoğlu",
    "corresponding_authors": "",
    "abstract": "Summary-Table-by-Example (STBE) is a graphical language suitable for statistical database applications. STBE queries have a hierarchical subquery structure and manipulate summary tables and relations with set-valued attributes. The hierarchical arrangement of STBE queries naturally implies a tuple-by-tuple subquery evaluation strategy (similar to the nested loops join implementation technique) which may not be the best query processing strategy. In this paper we discuss the query processing techniques used in STBE. We first convert an STBE query into an “extended” relational algebra (ERA) expression. Two transformations are introduced to remove the hierarchical arrangement of subqueries so that query optimization is possible. To solve the “empty partition” problem of aggregate function evaluation, directional join (one-sided outer-join) is utilized. We give the algebraic properties of the ERA operators to obtain an “improved” ERA expression. Finally we briefly discuss the generation of alternative implementations of a given ERA expression. STBE is implemented in a prototype statistical database management system. We discuss the STBE-related features of the implemented system.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2002171029",
    "type": "article"
  },
  {
    "title": "NFQL: the natural forms query language",
    "doi": "https://doi.org/10.1145/63500.64125",
    "publication_date": "1989-06-01",
    "publication_year": 1989,
    "authors": "David W. Embley",
    "corresponding_authors": "David W. Embley",
    "abstract": "A means by which ordinary forms can be exploited to provide a basis for nonprocedural specification of information processing is discussed. The Natural Forms Query Language (NFQL) is defined. In NFQL data retrieval requests and computation specifications are formulated by sketching ordinary forms to show what data are desired and update operations are specified by altering data on filled-in forms. The meaning of a form depends on a store of knowledge that includes extended abstract data types for defining elementary data items, a database scheme defined by an entity-relationship model, and a conceptual model of an ordinary form. Based on this store of knowledge, several issues are addressed and resolved in the context of NFQL. These issues include automatic generation of query expressions from weak specifications, the view update problem, power and completeness, and a heuristic approach to resolving computational relationships. A brief status report of an implementation of NFQL is also given.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2057243780",
    "type": "article"
  },
  {
    "title": "Recursive linear hashing",
    "doi": "https://doi.org/10.1145/1270.1285",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Kotagiri Ramamohanarao; Ron Sacks‐Davis",
    "corresponding_authors": "",
    "abstract": "A modification of linear hashing is proposed for which the conventional use of overflow records is avoided. Furthermore, an implementation of linear hashing is presented for which the amount of physical storage claimed is only fractionally more than the minimum required. This implementation uses a fixed amount of in-core space. Simulation results are given which indicate that even for storage utilizations approaching 95 percent, the average successful search cost for this method is close to one disk access.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W1969586533",
    "type": "article"
  },
  {
    "title": "The partition model: a deductive database model",
    "doi": "https://doi.org/10.1145/12047.22718",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "Nicolas Spyratos",
    "corresponding_authors": "Nicolas Spyratos",
    "abstract": "We present a new database model in which each attribute is modeled by a family of disjoint subsets of an underlying population of objects. Such a family is called a partitioning, and the set of all partitionings is turned into a lattice by appropriately defining product and sum. A database is seen as a function from a sublattice into the lattice of partitionings. The model combines the following features: (1) syntactic simplicity (essentially that of the relational model), (2) powerful means for the specification of semantic information (in the form of lattice equations), and (3) deductive capability (essentially that of set theory). The relational model of data and the basic constructs of semantic modeling can be embedded into our model in a simple and straightforward manner.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W1991408691",
    "type": "article"
  },
  {
    "title": "Linear hashing with separators—a dynamic hashing scheme achieving one-access",
    "doi": "https://doi.org/10.1145/44498.44500",
    "publication_date": "1988-09-01",
    "publication_year": 1988,
    "authors": "Per-Åke Larson",
    "corresponding_authors": "Per-Åke Larson",
    "abstract": "A new dynamic hashing scheme is presented. Its most outstanding feature is that any record can be retrieved in exactly one disk access. This is achieved by using a small amount of supplemental internal storage that stores enough information to uniquely determine the current location of any record. The amount of internal storage required is small: typically one byte for each page of the file. The necessary address computation, insertion, and expansion algorithms are presented and the performance is studied by means of simulation. The new method is the first practical method offering one-access retrieval for large dynamic files.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2040062099",
    "type": "article"
  },
  {
    "title": "Decompiling CODASYL DML into retional queries",
    "doi": "https://doi.org/10.1145/319682.319688",
    "publication_date": "1982-03-01",
    "publication_year": 1982,
    "authors": "Randy H. Katz; Eugene Wong",
    "corresponding_authors": "",
    "abstract": "A “decompilation” algorithm is developed to transform a program written with the procedural operations of CODASYL DML into one which interacts with a relational system via a nonprocedural query specification. An Access Path Model is introduced to interpret the semantic accesses performed by the program. Data flow analysis is used to determine how FIND operations implement semantic accesses. A sequence of these is mapped into a relational query and embedded into the original program. The class of programs for which the algorithm succeeds is characterized.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2113192909",
    "type": "article"
  },
  {
    "title": "Concurrency control in a dynamic search structure",
    "doi": "https://doi.org/10.1145/1270.318576",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Udi Manbar; Richard E. Ladner",
    "corresponding_authors": "",
    "abstract": "A design of a data structure and efficient algorithms for concurrent manipulations of a dynamic search structure by independent user processes is presented in this paper. The algorithms include updating data, inserting new elements, and deleting elements. The algorithms support a high level of concurrency. Each of the operations listed above requires only constant amount of locking. In order to make the system even more efficient for the user processes, maintenance processes are introduced. The maintenance processes operate independently in the background to reorganize the data structure and “clean up” after the (more urgent) user processes. A proof of correctness of the algorithms is given and some experimental results and extensions are examined.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2007049062",
    "type": "article"
  },
  {
    "title": "XSKETCH synopses for XML data graphs",
    "doi": "https://doi.org/10.1145/1166074.1166082",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Neoklis Polyzotis; Minos Garofalakis",
    "corresponding_authors": "",
    "abstract": "Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows (1) path navigation and branching through the label structure of the XML data graph, and (2) predicates on the values of specific path/branch nodes, in order to reach the desired data elements. Clearly, optimizing such queries requires approximating the result cardinality of the referenced paths and hence hinges on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over the base XML data. In this article, we introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSketch) exploits localized graph stability and value-distribution summaries (e.g., histograms) to accurately approximate (in limited space) the path and branching distribution, as well as the complex correlation patterns that can exist between and across path structure and element values in the data graph. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured XML data with values, and complex (branching) path expressions.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2172007538",
    "type": "article"
  },
  {
    "title": "Hierarchical schemata for relational databases",
    "doi": "https://doi.org/10.1145/319540.319546",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "Y. Edmund Lien",
    "corresponding_authors": "Y. Edmund Lien",
    "abstract": "Most database design methods for the relational model produce a flat database, that is, a family of relations with no explicit interrelational connections. The user of a flat database is likely to be unaware of certain interrelational semantics. In contrast, the entity-relationship model provides schema graphs as a description of the database, as well as for navigating the database. Nevertheless, the user of an entity-relationship database may still commit semantic errors, such as performing a lossy join. This paper proposes a nonflat, or hierarchical, view of relational databases. Relations are grouped together to form relation hierarchies in which lossless joins are explicitly shown whereas lossy joins are excluded. Relation hierarchies resemble the schema graphs in the entity-relationship model. An approach to the design of relation hierarchies is outlined in the context of data dependencies and relational decomposition. The approach consists of two steps; each is described as an algorithm. Algorithm DEC decomposes a given universal relation according to a given set of data dependencies and produces a set of nondecomposable relation schemes. This algorithm differs from its predecessors in that it produces no redundant relation schemes. Algorithm RH further structures the relation schemes produced by Algorithm DEC into a hierarchical schema. These algorithms can be useful software tools for database designers.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1969779730",
    "type": "article"
  },
  {
    "title": "A state transition model for distributed query processing",
    "doi": "https://doi.org/10.1145/6314.6460",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Stéphane Lafortune; Eugene Wong",
    "corresponding_authors": "",
    "abstract": "A state transition model for the optimization of query processing in a distributed database system is presented. The problem is parameterized by means of a state describing the amount of processing that has been performed at each site where the database is located. A state transition occurs each time a new join or semijoin is executed. Dynamic programming is used to compute recursively the costs of the states and the globally optimal solution, taking into account communication and local processing costs. The state transition model is general enough to account for the possibility of parallel processing among the various sites, as well as for redundancy in the database. The model also permits significant reductions of the necessary computations by taking advantage of simple additivity and site-uniformity properties of a cost model, and of clever strategies that improve on the basic dynamic programming algorithm.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1993291251",
    "type": "article"
  },
  {
    "title": "Repair localization for query answering from inconsistent databases",
    "doi": "https://doi.org/10.1145/1366102.1366107",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Thomas Eiter; Michael Fink; Gianluigi Greco; Domenico Lembo",
    "corresponding_authors": "",
    "abstract": "Query answering from inconsistent databases amounts to finding “meaningful” answers to queries posed over database instances that do not satisfy integrity constraints specified over their schema. A declarative approach to this problem relies on the notion of repair, that is, a database that satisfies integrity constraints and is obtained from the original inconsistent database by “minimally” adding and/or deleting tuples. Consistent answers to a user query are those answers that are in the evaluation of the query over each repair. Motivated by the fact that computing consistent answers from inconsistent databases is in general intractable, the present paper investigates techniques that allow to localize the difficult part of the computation on a small fragment of the database at hand, called “affected” part. Based on a number of localization results, an approach to query answering from inconsistent data is presented, in which the query is evaluated over each of the repairs of the affected part only, augmented with the part that is not affected. Single query results are then suitably recombined. For some relevant settings, techniques are also discussed to factorize repairs into components that can be processed independently of one another, thereby guaranteeing exponential gain w.r.t. the basic approach, which is not based on localization. The effectiveness of the results is demonstrated for consistent query answering over expressive schemas, based on logic programming specifications as proposed in the literature.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2122997609",
    "type": "article"
  },
  {
    "title": "Estimating the cost of updates in a relational database",
    "doi": "https://doi.org/10.1145/3857.3863",
    "publication_date": "1985-06-01",
    "publication_year": 1985,
    "authors": "Mario Schkolnick; Paolo Tiberio",
    "corresponding_authors": "",
    "abstract": "In this paper, cost formulas are derived for the updates of data and indexes in a relational database. The costs depend on the data scan type and the predicates involved in the update statements. We show that update costs have a considerable influence, both in the context of the physical database design problem and in access path selection in query optimization for relational DBMSs.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W1971828701",
    "type": "article"
  },
  {
    "title": "Schema analysis for database restructuring",
    "doi": "https://doi.org/10.1145/320141.320147",
    "publication_date": "1980-06-01",
    "publication_year": 1980,
    "authors": "Shamkant B. Navathe",
    "corresponding_authors": "Shamkant B. Navathe",
    "abstract": "The problem of generalized restructuring of databases has been addressed with two limitations: first, it is assumed that the restructuring user is able to describe the source and target databases in terms of the implicit data model of a particular methodology; second, the restructuring user is faced with the task of judging the scope and applicability of the defined types of restructuring to his database implementation and then of actually specifying his restructuring needs by translating them into the restructuring operations on a foreign data model. A certain amount of analysis of the logical and physical structure of databases must be performed, and the basic ingredients for such an analysis are developed here. The distinction between hierarchical and nonhierarchical data relationships is discussed, and a classification for database schemata is proposed. Examples are given to illustrate how these schemata arise in the conventional hierarchical and network systems. Application of the schema analysis methodology to restructuring specification is also discussed. An example is presented to illustrate the different implications of restructuring three seemingly identical database structures.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2012336070",
    "type": "article"
  },
  {
    "title": "Processor allocation strategies for multiprocessor database machines",
    "doi": "https://doi.org/10.1145/319566.319570",
    "publication_date": "1981-06-01",
    "publication_year": 1981,
    "authors": "Haran Boral; David J. DeWitt",
    "corresponding_authors": "",
    "abstract": "In this paper four alternative strategies for assigning processors to queries in multiprocessor database machines are described and evaluated. The results demonstrate that SIMD database machines are indeed a poor design when their performance is compared with that of the three MIMD strategies presented. Also introduced is the application of data-flow machine techniques to the processing of relational algebra queries. A strategy that employs data-flow techniques is shown to be superior to the other strategies described by several experiments. Furthermore, if the data-flow query processing strategy is employed, the results indicate that a two-level storage hierarchy (in which relations are paged between a shared data cache and mass storage) does not have a significant impact on performance.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2044521103",
    "type": "article"
  },
  {
    "title": "Efficient online index construction for text databases",
    "doi": "https://doi.org/10.1145/1386118.1386125",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Nicholas Lester; Alistair Moffat; Justin Zobel",
    "corresponding_authors": "",
    "abstract": "Inverted index structures are a core element of current text retrieval systems. They can be constructed quickly using offline approaches, in which one or more passes are made over a static set of input data, and, at the completion of the process, an index is available for querying. However, there are search environments in which even a small delay in timeliness cannot be tolerated, and the index must always be queryable and up to date. Here we describe and analyze a geometric partitioning mechanism for online index construction that provides a range of tradeoffs between costs, and can be adapted to different balances of insertion and querying operations. Detailed experimental results are provided that show the extent of these tradeoffs, and that these new methods can yield substantial savings in online indexing costs.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2157008678",
    "type": "article"
  },
  {
    "title": "New file organization based on dynamic hashing",
    "doi": "https://doi.org/10.1145/319540.319564",
    "publication_date": "1981-03-01",
    "publication_year": 1981,
    "authors": "Micheł Scholl",
    "corresponding_authors": "Micheł Scholl",
    "abstract": "New file organizations based on hashing and suitable for data whose volume may vary rapidly recently appeared in the literature. In the three schemes which have been independently proposed, rehashing is avoided, storage space is dynamically adjusted to the number of records actually stored, and there are no overflow records. Two of these techniques employ an index to the data file. Retrieval is fast and storage utilization is low. In order to increase storage utilization, we introduce two schemes based on a similar idea and analyze the performance of the second scheme. Both techniques use an index of much smaller size. In both schemes, overflow records are accepted. The price which has to be paid for the improvement in storage utilization is a slight access cost degradation.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1963493704",
    "type": "article"
  },
  {
    "title": "Optimal file designs and reorganization points",
    "doi": "https://doi.org/10.1145/319682.319696",
    "publication_date": "1982-03-01",
    "publication_year": 1982,
    "authors": "Don Batory",
    "corresponding_authors": "Don Batory",
    "abstract": "A model for studying the combined problems of file design and file reorganization is presented. New modeling techniques for predicting the performance evolution of files and for finding optimal reorganization points for files are introduced. Applications of the model to hash-based and indexed-sequential files reveal important relationships between initial loading factors and reorganization frequency. A practical file design strategy, based on these relationships, is proposed.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1977597747",
    "type": "article"
  },
  {
    "title": "The correctness of concurrency control mechanisms in a system for distributed databases (SDD-1)",
    "doi": "https://doi.org/10.1145/320128.320133",
    "publication_date": "1980-03-01",
    "publication_year": 1980,
    "authors": "Philip A. Bernstein; David W. Shipman",
    "corresponding_authors": "",
    "abstract": "This paper presents a formal analysis of the concurrency control strategy of SDD-1. SDD-1, a System for Distributed Databases, is a prototype distributed database system being developed by Computer Corporation of America. In SDD-1, portions of data distributed throughout a network may be replicated at multiple sites. The SDD-1 concurrency control guarantees database consistency in the face of such distribution and replication. This paper is one of a series of companion papers on SDD-1 [2, 8].",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2048111017",
    "type": "article"
  },
  {
    "title": "Database buffer paging in virtual storage systems",
    "doi": "https://doi.org/10.1145/320576.320585",
    "publication_date": "1977-12-01",
    "publication_year": 1977,
    "authors": "Tomás Lang; Christopher Wood; Eduardo B. Fernández",
    "corresponding_authors": "",
    "abstract": "Three models, corresponding to different sets of assumptions, are analyzed to study the behavior of a database buffer in a paging environment. The models correspond to practical situations and vary in their search strategies and replacement algorithms. The variation of I/O cost with respect to buffer size is determined for the three models. The analysis is valid for arbitrary database and buffer sizes, and the I/O cost is obtained in terms of the miss ratio, the buffer size, the number of main memory pages available for the buffer, and the relative buffer and database access costs.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2050902871",
    "type": "article"
  },
  {
    "title": "Maximizing Conjunctive Views in Deletion Propagation",
    "doi": "https://doi.org/10.1145/2389241.2389243",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Benny Kimelfeld; Jan Vondrák; Ryan Williams",
    "corresponding_authors": "",
    "abstract": "In deletion propagation, tuples from the database are deleted in order to reflect the deletion of a tuple from the view. Such an operation may result in the (often necessary) deletion of additional tuples from the view, besides the intentionally deleted one. The article studies the complexity of deletion propagation, where the view is defined by a conjunctive query (CQ), and the goal is to maximize the number of tuples that remain in the view. Buneman et al. showed that for some simple CQs, this problem can be solved by a straightforward algorithm, which is called here the unidimensional algorithm. The article identifies additional cases of CQs where the unidimensional algorithm succeeds, and in contrast, shows that for some other CQs the problem is NP-hard to approximate better than some constant ratio. In fact, it is shown here that among the CQs without self joins, the hard CQs are exactly the ones that the unidimensional algorithm fails on. In other words, the following dichotomy result is proved: for every CQ without self joins, deletion propagation is either APX-hard or solvable (in polynomial time) by the unidimensional algorithm. The article then presents approximation algorithms for certain CQs where deletion propagation is APX-hard. Specifically, two constant-ratio (and polynomial-time) approximation algorithms are given for the class of sunflower CQs (i.e., CQs having a sunflower hypergraph) without self joins. The first algorithm, providing the approximation ratio 1 − 1/ e , is obtained by formulating the problem at hand as that of maximizing a monotone submodular function subject to a matroid constraint, and then using a known algorithm for such maximization. The second algorithm gives a smaller approximation ratio, 1/2, yet in polynomial time even under combined complexity. Finally, it is shown that self joins can significantly harden approximation in deletion propagation.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2036118854",
    "type": "article"
  },
  {
    "title": "An architecture for recycling intermediates in a column-store",
    "doi": "https://doi.org/10.1145/1862919.1862921",
    "publication_date": "2010-10-12",
    "publication_year": 2010,
    "authors": "Milena G. Ivanova; Martin Kersten; Niels Nes; Romulo Gonçalves",
    "corresponding_authors": "",
    "abstract": "Automatic recycling of intermediate results to improve both query response time and throughput is a grand challenge for state-of-the-art databases. Tuples are loaded and streamed through a tuple-at-a-time processing pipeline, avoiding materialization of intermediates as much as possible. This limits the opportunities for reuse of overlapping computations to DBA-defined materialized views and function/result cache tuning. In contrast, the operator-at-a-time execution paradigm produces fully materialized results in each step of the query plan. To avoid resource contention, these intermediates are evicted as soon as possible. In this article we study an architecture that harvests the byproducts of the operator-at-a-time paradigm in a column-store system using a lightweight mechanism, the recycler. The key challenge then becomes the selection of the policies to admit intermediates to the resource pool, to determine their retention period, and devise the eviction strategy when facing resource limitations. The proposed recycling architecture has been implemented in an open-source system. An experimental analysis against the TPC-H ad-hoc decision support benchmark and a complex, real-world application (SkyServer) demonstrates its effectiveness in terms of self-organizing behavior and its significant performance gains. The results indicate the potentials of recycling intermediates and charts a route for further development of database kernels.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2081593332",
    "type": "article"
  },
  {
    "title": "Relative information completeness",
    "doi": "https://doi.org/10.1145/1862919.1862924",
    "publication_date": "2010-10-12",
    "publication_year": 2010,
    "authors": "Wenfei Fan; Floris Geerts",
    "corresponding_authors": "",
    "abstract": "This article investigates the question of whether a partially closed database has complete information to answer a query. In practice an enterprise often maintains master data D m , a closed-world database. We say that a database D is partially closed if it satisfies a set V of containment constraints of the form q ( D ) ⊆ p ( D m ), where q is a query in a language L C and p is a projection query. The part of D not constrained by ( D m , V ) is open, from which some tuples may be missing. The database D is said to be complete for a query Q relative to ( D m , V ) if for all partially closed extensions D ' of D , Q ( D ') = Q ( D ), i.e., adding tuples to D either violates some constraints in V or does not change the answer to Q . We first show that the proposed model can also capture the consistency of data, in addition to its relative completeness. Indeed, integrity constraints studied for data consistency can be expressed as containment constraints. We then study two problems. One is to decide, given D m , V , a query Q in a language L Q , and a partially closed database D , whether D is complete for Q relative to ( D m , V ). The other is to determine, given D m , V and Q , whether there exists a partially closed database that is complete for Q relative to ( D m , V ). We establish matching lower and upper bounds on these problems for a variety of languages L Q and L C . We also provide characterizations for a database to be relatively complete, and for a query to allow a relatively complete database, when L Q and L C are conjunctive queries.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2140859480",
    "type": "article"
  },
  {
    "title": "Query-preserving watermarking of relational databases and Xml documents",
    "doi": "https://doi.org/10.1145/1929934.1929937",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "David Gross-Amblard",
    "corresponding_authors": "David Gross-Amblard",
    "abstract": "Watermarking allows robust and unobtrusive insertion of information in a digital document. During the last few years, techniques have been proposed for watermarking relational databases or Xml documents, where information insertion must preserve a specific measure on data (for example the mean and variance of numerical attributes). In this article we investigate the problem of watermarking databases or Xml while preserving a set of parametric queries in a specified language, up to an acceptable distortion. We first show that unrestricted databases can not be watermarked while preserving trivial parametric queries. We then exhibit query languages and classes of structures that allow guaranteed watermarking capacity, namely 1) local query languages on structures with bounded degree Gaifman graph, and 2) monadic second-order queries on trees or treelike structures. We relate these results to an important topic in computational learning theory, the VC-dimension. We finally consider incremental aspects of query-preserving watermarking.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2120201091",
    "type": "article"
  },
  {
    "title": "Secure distributed computation of anonymized views of shared databases",
    "doi": "https://doi.org/10.1145/2188349.2188353",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Tamir Tassa; Ehud Gudes",
    "corresponding_authors": "",
    "abstract": "We consider the problem of computing efficient anonymizations of partitioned databases. Given a database that is partitioned between several sites, either horizontally or vertically, we devise secure distributed algorithms that allow the different sites to obtain a k -anonymized and ℓ-diverse view of the union of their databases, without disclosing sensitive information. Our algorithms are based on the sequential algorithm [Goldberger and Tassa 2010] that offers anonymizations with utility that is significantly better than other anonymization algorithms, and in particular those that were implemented so far in the distributed setting. Our algorithms can apply to different generalization techniques and utility measures and to any number of sites. While previous distributed algorithms depend on costly cryptographic primitives, the cryptographic assumptions of our solution are surprisingly minimal.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1978838204",
    "type": "article"
  },
  {
    "title": "Learning schema mappings",
    "doi": "https://doi.org/10.1145/2539032.2539035",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Balder ten Cate; Víctor Dalmau; Phokion G. Kolaitis",
    "corresponding_authors": "",
    "abstract": "A schema mapping is a high-level specification of the relationship between a source schema and a target schema. Recently, a line of research has emerged that aims at deriving schema mappings automatically or semi-automatically with the help of data examples, that is, pairs consisting of a source instance and a target instance that depict, in some precise sense, the intended behavior of the schema mapping. Several different uses of data examples for deriving, refining, or illustrating a schema mapping have already been proposed and studied. In this article, we use the lens of computational learning theory to systematically investigate the problem of obtaining algorithmically a schema mapping from data examples. Our aim is to leverage the rich body of work on learning theory in order to develop a framework for exploring the power and the limitations of the various algorithmic methods for obtaining schema mappings from data examples. We focus on GAV schema mappings, that is, schema mappings specified by GAV (Global-As-View) constraints. GAV constraints are the most basic and the most widely supported language for specifying schema mappings. We present an efficient algorithm for learning GAV schema mappings using Angluin's model of exact learning with membership and equivalence queries. This is optimal, since we show that neither membership queries nor equivalence queries suffice, unless the source schema consists of unary relations only. We also obtain results concerning the learnability of schema mappings in the context of Valiant's well-known PAC (Probably-Approximately-Correct) learning model, and concerning the learnability of restricted classes of GAV schema mappings. Finally, as a byproduct of our work, we show that there is no efficient algorithm for approximating the shortest GAV schema mapping fitting a given set of examples, unless the source schema consists of unary relations only.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2009182243",
    "type": "article"
  },
  {
    "title": "Relational languages and data models for continuous queries on sequences and data streams",
    "doi": "https://doi.org/10.1145/1966385.1966386",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Yan-Nei Law; Haixun Wang; Carlo Zaniolo",
    "corresponding_authors": "",
    "abstract": "Most data stream management systems are based on extensions of the relational data model and query languages, but rigorous analyses of the problems and limitations of this approach, and how to overcome them, are still wanting. In this article, we elucidate the interaction between stream-oriented extensions of the relational model and continuous query language constructs, and show that the resulting expressive power problems are even more serious for data streams than for databases. In particular, we study the loss of expressive power caused by the loss of blocking query operators, and characterize nonblocking queries as monotonic functions on the database. Thus we introduce the notion of NB -completeness to assure that a query language is as suitable for continuous queries as it is for traditional database queries. We show that neither RA nor SQL are NB -complete on unordered sets of tuples, and the problem is even more serious when the data model is extended to support order—a sine-qua-non in data stream applications. The new limitations of SQL, compounded with well-known problems in applications such as sequence queries and data mining, motivate our proposal of extending the language with user-defined aggregates (UDAs). These can be natively coded in SQL, according to simple syntactic rules that set nonblocking aggregates apart from blocking ones. We first prove that SQL with UDAs is Turing complete. We then prove that SQL with monotonic UDAs and union operators can express all monotonic set functions computable by a Turing machine ( NB -completeness) and finally extend this result to queries on sequences ordered by their timestamps. The proposed approach supports data stream models that are more sophisticated than append-only relations, along with data mining queries, and other complex applications.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2104923072",
    "type": "article"
  },
  {
    "title": "Top-k and Clustering with Noisy Comparisons",
    "doi": "https://doi.org/10.1145/2684066",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Susan B. Davidson; Sanjeev Khanna; Tova Milo; Sudeepa Roy",
    "corresponding_authors": "",
    "abstract": "We study the problems of max/top- k and clustering when the comparison operations may be performed by oracles whose answer may be erroneous. Comparisons may either be of type or of value : given two data elements, the answer to a type comparison is “yes” if the elements have the same type and therefore belong to the same group (cluster); the answer to a value comparison orders the two data elements. We give efficient algorithms that are guaranteed to achieve correct results with high probability, analyze the cost of these algorithms in terms of the total number of comparisons (i.e., using a fixed-cost model), and show that they are essentially the best possible. We also show that fewer comparisons are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases. Finally, we examine another important class of cost functions, concave functions, which balances the number of rounds of interaction with the oracle with the number of questions asked of the oracle. Results of this article form an important first step in providing a formal basis for max/top- k and clustering queries in crowdsourcing applications, that is, when the oracle is implemented using the crowd. We explain what simplifying assumptions are made in the analysis, what results carry to a generalized crowdsourcing setting, and what extensions are required to support a full-fledged model.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1971077223",
    "type": "article"
  },
  {
    "title": "A partition-based method for string similarity joins with edit-distance constraints",
    "doi": "https://doi.org/10.1145/2487259.2487261",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Guoliang Li; Dong Deng; Jianhua Feng",
    "corresponding_authors": "",
    "abstract": "As an essential operation in data cleaning, the similarity join has attracted considerable attention from the database community. In this article, we study string similarity joins with edit-distance constraints, which find similar string pairs from two large sets of strings whose edit distance is within a given threshold. Existing algorithms are efficient either for short strings or for long strings, and there is no algorithm that can efficiently and adaptively support both short strings and long strings. To address this problem, we propose a new filter, called the segment filter . We partition a string into a set of segments and use the segments as a filter to find similar string pairs. We first create inverted indices for the segments. Then for each string, we select some of its substrings, identify the selected substrings from the inverted indices, and take strings on the inverted lists of the found substrings as candidates of this string. Finally, we verify the candidates to generate the final answer. We devise efficient techniques to select substrings and prove that our method can minimize the number of selected substrings. We develop novel pruning techniques to efficiently verify the candidates. We also extend our techniques to support normalized edit distance. Experimental results show that our algorithms are efficient for both short strings and long strings, and outperform state-of-the-art methods on real-world datasets.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2073112656",
    "type": "article"
  },
  {
    "title": "Certain conjunctive query answering in first-order logic",
    "doi": "https://doi.org/10.1145/2188349.2188351",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Jef Wijsen",
    "corresponding_authors": "Jef Wijsen",
    "abstract": "Primary key violations provide a natural means for modeling uncertainty in the relational data model. A repair (or possible world) of a database is then obtained by selecting a maximal number of tuples without ever selecting two distinct tuples that have the same primary key value. For a Boolean query q , the problem CERTAINTY( q ) takes as input a database db and asks whether q evaluates to true on every repair of db . We are interested in determining queries q for which CERTAINTY( q ) is first-order expressible (and hence in the low complexity class AC °). For queries q in the class of conjunctive queries without self-join, we provide a necessary syntactic condition for first-order expressibility of CERTAINTY( q ). For acyclic queries (in the sense of Beeri et al. [1983]), this necessary condition is also a sufficient condition. So we obtain a decision procedure for first-order expressibility of CERTAINTY( q ) when q is acyclic and without self-join. We also show that if CERTAINTY( q ) is first-order expressible, its first-order definition, commonly called certain first-order rewriting, can be constructed in a rather straightforward way.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2098764906",
    "type": "article"
  },
  {
    "title": "Moving spatial keyword queries",
    "doi": "https://doi.org/10.1145/2445583.2445590",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Dingming Wu; Man Lung Yiu; Christian S. Jensen",
    "corresponding_authors": "",
    "abstract": "Web users and content are increasingly being geo-positioned. This development gives prominence to spatial keyword queries, which involve both the locations and textual descriptions of content. We study the efficient processing of continuously moving top-k spatial keyword (MkSK) queries over spatial text data. State-of-the-art solutions for moving queries employ safe zones that guarantee the validity of reported results as long as the user remains within the safe zone associated with a result. However, existing safe-zone methods focus solely on spatial locations and ignore text relevancy. We propose two algorithms for computing safe zones that guarantee correct results at any time and that aim to optimize the server-side computation as well as the communication between the server and the client. We exploit tight and conservative approximations of safe zones and aggressive computational space pruning. We present techniques that aim to compute the next safe zone efficiently, and we present two types of conservative safe zones that aim to reduce the communication cost. Empirical studies with real data suggest that the proposals are efficient. To understand the effectiveness of the proposed safe zones, we study analytically the expected area of a safe zone, which indicates on average for how long a safe zone remains valid, and we study the expected number of influence objects needed to define a safe zone, which gives an estimate of the average communication cost. The analytical modeling is validated through empirical studies.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2122872794",
    "type": "article"
  },
  {
    "title": "Time- and Space-Efficient Sliding Window Top-k Query Processing",
    "doi": "https://doi.org/10.1145/2736701",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Krešimir Pripužić; Ivana Podnar Žarko; Karl Aberer",
    "corresponding_authors": "",
    "abstract": "A sliding window top-k ( top-k/w ) query monitors incoming data stream objects within a sliding window of size w to identify the k highest-ranked objects with respect to a given scoring function over time. Processing of such queries is challenging because, even when an object is not a top-k/w object at the time when it enters the processing system, it might become one in the future. Thus a set of potential top-k/w objects has to be stored in memory while its size should be minimized to efficiently cope with high data streaming rates. Existing approaches typically store top-k/w and candidate sliding window objects in a k-skyband over a two-dimensional score-time space. However, due to continuous changes of the k-skyband, its maintenance is quite costly. Probabilistic k-skyband is a novel data structure storing data stream objects from a sliding window with significant probability to become top-k/w objects in future. Continuous probabilistic k-skyband maintenance offers considerably improved runtime performance compared to k-skyband maintenance, especially for large values of k , at the expense of a small and controllable error rate. We propose two possible probabilistic k-skyband usages: ( i ) When it is used to process all sliding window objects, the resulting top-k/w algorithm is approximate and adequate for processing random-order data streams. ( ii ) When probabilistic k-skyband is used to process only a subset of most recent sliding window objects, it can improve the runtime performance of continuous k-skyband maintenance, resulting in a novel exact top-k/w algorithm. Our experimental evaluation systematically compares different top-k/w processing algorithms and shows that while competing algorithms offer either time efficiency at the expanse of space efficiency or vice-versa, our algorithms based on the probabilistic k-skyband are both time and space efficient.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2074495191",
    "type": "article"
  },
  {
    "title": "Faster Random Walks by Rewiring Online Social Networks On-the-Fly",
    "doi": "https://doi.org/10.1145/2847526",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Zhuojie Zhou; Nan Zhang; Zhiguo Gong; Gautam Das",
    "corresponding_authors": "",
    "abstract": "Many online social networks feature restrictive web interfaces that only allow the query of a user’s local neighborhood. To enable analytics over such an online social network through its web interface, many recent efforts use Markov Chain Monte Carlo (MCMC) methods such as random walks to sample users in the social network and thereby support analytics based on the samples. The problem with such an approach, however, is the large amount of queries often required for a random walk to converge to a desired (stationary) sampling distribution. In this article, we consider a novel problem of enabling a faster random walk over online social networks by “rewiring” the social network on-the-fly. Specifically, we develop a Modified TOpology Sampling (MTO-Sampling) scheme that, by using only information exposed by the restrictive web interface, constructs a “virtual” random-walk-friendly overlay topology of the social network while performing a random walk and ensures that the random walk follows the modified overlay topology rather than the original one. We describe in this article instantiations of MTO-Sampling for various types of random walks, such as Simple Random Walk (MTO-SRW), Metropolis-Hastings Random Walk (MTO-MHRW), and General Random Walk (MTO-GRW). We not only rigidly prove that MTO-Sampling improves the efficiency of sampling, but we also demonstrate the significance of such improvement through experiments on real-world online social networks such as Google Plus, Epinion, Facebook, etc.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2286079480",
    "type": "article"
  },
  {
    "title": "Declarative Probabilistic Programming with Datalog",
    "doi": "https://doi.org/10.1145/3132700",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Vince Bárány; Balder ten Cate; Benny Kimelfeld; Dan Olteanu; Zografoula Vagena",
    "corresponding_authors": "",
    "abstract": "Probabilistic programming languages are used for developing statistical models. They typically consist of two components: a specification of a stochastic process (the prior) and a specification of observations that restrict the probability space to a conditional subspace (the posterior). Use cases of such formalisms include the development of algorithms in machine learning and artificial intelligence. In this article, we establish a probabilistic-programming extension of Datalog that, on the one hand, allows for defining a rich family of statistical models, and on the other hand retains the fundamental properties of declarativity. Our proposed extension provides mechanisms to include common numerical probability functions; in particular, conclusions of rules may contain values drawn from such functions. The semantics of a program is a probability distribution over the possible outcomes of the input database with respect to the program. Observations are naturally incorporated by means of integrity constraints over the extensional and intensional relations. The resulting semantics is robust under different chases and invariant to rewritings that preserve logical equivalence.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2765370125",
    "type": "article"
  },
  {
    "title": "Consistent Query Answering for Self-Join-Free Conjunctive Queries Under Primary Key Constraints",
    "doi": "https://doi.org/10.1145/3068334",
    "publication_date": "2017-06-01",
    "publication_year": 2017,
    "authors": "Paraschos Koutris; Jef Wijsen",
    "corresponding_authors": "",
    "abstract": "A relational database is said to be uncertain if primary key constraints can possibly be violated. A repair (or possible world) of an uncertain database is obtained by selecting a maximal number of tuples without ever selecting two distinct tuples with the same primary key value. For any Boolean query q , CERTAINTY( q ) is the problem that takes an uncertain database db as input and asks whether q is true in every repair of db . The complexity of this problem has been particularly studied for q ranging over the class of self-join-free Boolean conjunctive queries. A research challenge is to determine, given q , whether CERTAINTY( q ) belongs to complexity classes FO , P , or coNP -complete. In this article, we combine existing techniques for studying this complexity classification task. We show that, for any self-join-free Boolean conjunctive query q , it can be decided whether or not CERTAINTY( q ) is in FO . We additionally show how to construct a single SQL query for solving CERTAINTY( q ) if it is in FO . Further, for any self-join-free Boolean conjunctive query q , CERTAINTY( q ) is either in P or coNP -complete and the complexity dichotomy is effective. This settles a research question that has been open for 10 years.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2621561345",
    "type": "article"
  },
  {
    "title": "Synchronization of Queries and Views Upon Schema Evolutions",
    "doi": "https://doi.org/10.1145/2903726",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Loredana Caruccio; Giuseppe Polese; Genoveffa Tortora",
    "corresponding_authors": "",
    "abstract": "One of the problems arising upon the evolution of a database schema is that some queries and views defined on the previous schema version might no longer work properly. Thus, evolving a database schema entails the redefinition of queries and views to adapt them to the new schema. Although this problem has been mainly raised in the context of traditional information systems, solutions to it are also advocated in other database-related areas, such as Data Integration, Web Data Integration, and Data Warehouses. The problem is a critical one, since industrial organizations often need to adapt their databases and data warehouses to frequent changes in the real world. In this article, we provide a survey of existing approaches and tools to the problem of adapting queries and views upon a database schema evolution; we also propose a classification framework to enable a uniform comparison method among many heterogeneous approaches and tools.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2375206141",
    "type": "article"
  },
  {
    "title": "Efficient Discovery of Matching Dependencies",
    "doi": "https://doi.org/10.1145/3392778",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Philipp Schirmer; Thorsten Papenbrock; Ioannis Koumarelas; Felix Naumann",
    "corresponding_authors": "",
    "abstract": "Matching dependencies (MDs) are data profiling results that are often used for data integration, data cleaning, and entity matching. They are a generalization of functional dependencies (FDs) matching similar rather than same elements. As their discovery is very difficult, existing profiling algorithms find either only small subsets of all MDs or their scope is limited to only small datasets. We focus on the efficient discovery of all interesting MDs in real-world datasets. For this purpose, we propose HyMD, a novel MD discovery algorithm that finds all minimal, non-trivial MDs within given similarity boundaries. The algorithm extracts the exact similarity thresholds for the individual MDs from the data instead of using predefined similarity thresholds. For this reason, it is the first approach to solve the MD discovery problem in an exact and truly complete way. If needed, the algorithm can, however, enforce certain properties on the reported MDs, such as disjointness and minimum support, to focus the discovery on such results that are actually required by downstream use cases. HyMD is technically a hybrid approach that combines the two most popular dependency discovery strategies in related work: lattice traversal and inference from record pairs. Despite the additional effort of finding exact similarity thresholds for all MD candidates, the algorithm is still able to efficiently process large datasets, e.g., datasets larger than 3 GB.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3040722616",
    "type": "article"
  },
  {
    "title": "On Directed Densest Subgraph Discovery",
    "doi": "https://doi.org/10.1145/3483940",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Chenhao Ma; Yixiang Fang; Reynold Cheng; Laks V. S. Lakshmanan; Wenjie Zhang; Xuemin Lin",
    "corresponding_authors": "",
    "abstract": "Given a directed graph G , the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G , whose density is the highest among all the subgraphs of G . The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a 3,000-edge graph, it takes three days for one of the best exact algorithms to complete. In this article, we develop an efficient and scalable DDS solution. We introduce the notion of [ x , y ]-core, which is a dense subgraph for G , and show that the densest subgraph can be accurately located through the [ x , y ]-core with theoretical guarantees. Based on the [ x , y ]-core, we develop exact and approximation algorithms. We further study the problems of maintaining the DDS over dynamic directed graphs and finding the weighted DDS on weighted directed graphs, and we develop efficient non-trivial algorithms to solve these two problems by extending our DDS algorithms. We have performed an extensive evaluation of our approaches on 15 real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3212747350",
    "type": "article"
  },
  {
    "title": "Heraclitus",
    "doi": "https://doi.org/10.1145/232753.232801",
    "publication_date": "1996-09-01",
    "publication_year": 1996,
    "authors": "Shahram Ghandeharizadeh; Richard Hull; Dean Jacobs",
    "corresponding_authors": "",
    "abstract": "Traditional database systems provide a user with the ability to query and manipulate one database state, namely the current database state. However, in several emerging applications, the ability to analyze “what-if” scenarios in order to reason about the impact of an update (before committing that update) is of paramount importance. Example applications include hypothetical database access, active database management systems, and version management, to name a few. The central thesis of the Heraclitus paradigm is to provide flexible support for applications such as these by elevating deltas , which represent updates proposed against the current database state, to be first-class citizens. Heraclitus[Alg,C] is a database programming language that extends C to incorporate the relational algebra and deltas. Operators are provided that enable the programmer to explicitly construct, combine, and access deltas. Most interesting is the when operator, that supports hypothetical access to a delta: the expression E when σ yields the value that side effect free expression E would have if the value of delta expression σ were applied to the current database state. This article presents a broad overview of the philosophy underlying the Heraclitus paradigm, and describes the design and prototype implementation of Heraclitus[Alg, C]. A model-independent formalism for the Heraclitus paradigm is also presented. To illustrate the utility of Heraclitus, the article presents an in-depth discussion of how Heraclitus[Alg, C] can be used to specify, and thereby implement, a wide range of execution models for rule application in active databases; this includes both prominent execution models presented in the literature, and more recent “customized” execution models with novel features.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W1968386702",
    "type": "article"
  },
  {
    "title": "Description logics for semantic query optimization in object-oriented database systems",
    "doi": "https://doi.org/10.1145/762471.762472",
    "publication_date": "2003-03-01",
    "publication_year": 2003,
    "authors": "Domenico Beneventano; Sonia Bergamaschi; Claudio Sartori",
    "corresponding_authors": "",
    "abstract": "Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODL RE , providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2034684123",
    "type": "article"
  },
  {
    "title": "Modularization techniques for active rules design",
    "doi": "https://doi.org/10.1145/227604.227605",
    "publication_date": "1996-03-01",
    "publication_year": 1996,
    "authors": "Elena Baralis; Stefano Ceri; Stefano Paraboschi",
    "corresponding_authors": "",
    "abstract": "Active database systems can be used to establish and enforce data management policies. A large amount of the semantics that normally needs to be coded in application programs can be abstracted and assigned to active rules. This trend is sometimes called “knowledge independence” a nice consequence of achieving full knowledge independence is that data management policies can then effectively evolve just by modifying rules instead of application programs. Active rules, however, may be quite complex to understand and manage: rules react to arbitrary event sequences, they trigger each other, and sometimes the outcome of rule processing may depend on the order in which events occur or rules are scheduled. Although reasoning on a large collection of rules is very difficult, the task becomes more manageable when the rules are few. Therefore, we are convinced that modularization, similar to what happens in any software development process, is the key principle for designing active rules; however, this important notion has not been addressed so far. This article introduces a modularization technique for active rules called stratification; it presents a theory of stratification and indicates how stratification can be practically applied. The emphasis of this article is on providing a solution to a very concrete and practical problem; therefore, our approach is illustrated by several examples.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2088552404",
    "type": "article"
  },
  {
    "title": "Database design with common sense business reasoning and learning",
    "doi": "https://doi.org/10.1145/278245.278246",
    "publication_date": "1997-12-01",
    "publication_year": 1997,
    "authors": "Veda C. Storey; Roger H.L. Chiang; Debabrata Dey; Robert C. Goldstein; Shankar Sudaresan",
    "corresponding_authors": "",
    "abstract": "Automated database design systems embody knowledge about the database design process. However, their lack of knowledge about the domains for which databases are being developed significantly limits their usefulness. A methodology for acquiring and using general world knowledge about business for database design has been developed and implemented in a system called the Common Sense Business Reasoner, which acquires facts about application domains and organizes them into a a hierarchical, context-dependent knowledge base. This knowledge is used to make intelligent suggestions to a user about the entities, attributes, and relationships to include in a database design. A distance function approach is employed for integrating specific facts, obtained from individual design sessions, into the knowledge base (learning) and for applying the knowledge to subsequent design problems (reasoning).",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2007524563",
    "type": "article"
  },
  {
    "title": "Tools and transformations—rigorous and otherwise—for practical database design",
    "doi": "https://doi.org/10.1145/176567.176568",
    "publication_date": "1994-06-01",
    "publication_year": 1994,
    "authors": "Arnon Rosenthal; David S. Reiner",
    "corresponding_authors": "",
    "abstract": "We describe the tools and theory of a comprehensive system for database design, and show how they work together to support multiple conceptual and logical design processes. The Database Design and Evaluation Workbench (DDEW) system uses a rigorous, information-content-preserving approach to schema transformation, but combines it with heuristics, guess work, and user interactions. The main contribution lies in illustrating how theory was adapted to a practical system, and how the consistency and power of a design system can be increased by use of theory. First, we explain why a design system needs multiple data models, and how implementation over a unified underlying model reduces redundancy and inconsistency. Second, we present a core set of small but fundamental algorithms that reaarange a schema without changing its information content. From these reusable components, we easily built larger tools and transformations that were still formally justified. Third, we describe heuristic tools that attempt to improve a schema, often by adding missing information. In these tools, unreliable techniques such as normalization and relationship inference are bolstered by system-guided user interactions to remove errors. We present a rigorous criterion for identifying unnecessary relationships, and discuss an interactive view integrator. Last, we examine the relevance of database theory to building these practically motivated tools and contrast the paradigms of system builders with those of theoreticians.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2070291209",
    "type": "article"
  },
  {
    "title": "Selection conditions in main memory",
    "doi": "https://doi.org/10.1145/974750.974755",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Kenneth A. Ross",
    "corresponding_authors": "Kenneth A. Ross",
    "abstract": "We consider the fundamental operation of applying a compound filtering condition to a set of records. With large main memories available cheaply, systems may choose to keep the data entirely in main memory, in order to improve query and/or update performance.The design of a data-intensive algorithm in main memory needs to take into account the architectural characteristics of modern processors, just as a disk-based method needs to consider the physical characteristics of disk devices. An important architectural feature that influences the performance of main memory algorithms is the branch misprediction penalty. We demonstrate that branch misprediction has a substantial impact on the performance of an algorithm for applying selection conditions.We describe a space of \"query plans\" that are logically equivalent, but differ in terms of performance due to variations in their branch prediction behavior. We propose a cost model that takes branch prediction into account, and develop a query optimization algorithm that chooses a plan with optimal estimated cost for conjunctive conditions. We also develop an efficient heuristic optimization algorithm. We also show how records can be ordered to further reduce branch misprediction effects.We provide experimental results for a case study based on an event notification system. Our results show the effectiveness of the proposed optimization techniques. Our results also demonstrate that significant improvements in performance can be obtained by applying a methodology that takes branch misprediction latency into account.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2019186666",
    "type": "article"
  },
  {
    "title": "On the computation of relational view complements",
    "doi": "https://doi.org/10.1145/777943.777946",
    "publication_date": "2003-06-01",
    "publication_year": 2003,
    "authors": "Jens Lechtenbörger; Gottfried Vossen",
    "corresponding_authors": "",
    "abstract": "Views as a means to describe parts of a given data collection play an important role in many database applications. In dynamic environments where data is updated, not only information provided by views, but also information provided by data sources yet missing from views turns out to be relevant: Previously, this missing information has been characterized in terms of view complements ; recently, it has been shown that view complements can be exploited in the context of data warehouses to guarantee desirable warehouse properties such as independence and self-maintainability. As the complete source information is a trivial complement for any view, a natural interest for \"small\" or even \"minimal\" complements arises. However, the computation of minimal complements is still not very well understood. In this article, it is shown how to compute reasonably small (and in special cases even minimal) complements for a large class of relational views.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2039896344",
    "type": "article"
  },
  {
    "title": "Answering queries with useful bindings",
    "doi": "https://doi.org/10.1145/502030.502032",
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Chen Li; Edward Yi Chang",
    "corresponding_authors": "",
    "abstract": "In information-integration systems, sources may have diverse and limited query capabilities. To obtain maximum information from these restrictive sources to answer a query, one can access sources that are not specified in the query (i.e., off-query sources). In this article, we propose a query-planning framework to answer queries in the presence of limited access patterns. In the framework, a query and source descriptions are translated to a recursive datalog program. We then solve optimization problems in this framework, including how to decide whether accessing off-query sources is necessary, how to choose useful sources for a query, and how to test query containment. We develop algorithms to solve these problems, and thus construct an efficient program to answer a query.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2078752758",
    "type": "article"
  },
  {
    "title": "The model, language, and implementation of an object-oriented multimedia knowledge base management system",
    "doi": "https://doi.org/10.1145/151284.151285",
    "publication_date": "1993-03-01",
    "publication_year": 1993,
    "authors": "Hiroshi Ishikawa; Fumio Suzuki; Fumihiko Kozakura; Akifumi Makinouchi; Mika Miyagishima; Y. Izumida; Masaaki Aoshima; Yasuo Yamane",
    "corresponding_authors": "",
    "abstract": "New applications such as CAD, AI, and hypermedia require direct representation and flexible use of complex objects, behavioral knowledge, and multimedia data. To this end, we have devised a knowledge base management system called Jasmine. An object-oriented approach in a programming language also seems promising for use in Jasmine. Jasmine extends the current object-oriented approach and provides the following features. Our object model is based on functional data models and well-established set theory. Attributes or functions composing objects can represent both structural and behavioral knowledge. The object model can represent incomplete and generic knowledge. The model can support the basic storage and operations of multimedia data. The facets of attributes can flexibly represent constraints and triggers. The object manipulation language can support associative access of objects. The structural and behavioral knowledge can be uniformly treated to allow the user to specify complex object operations in a compact manner. The user-defined and system-defined attributes can be uniformly specified to ease user customization of the language. The classes and instances can be uniformly accessed. Incomplete knowledge can be flexibly accessed. The system has a layered architecture. Objects are stored in nested relations provided by extensive DBMS as a sublayer. User query of objects is compiled into relational operations such as select and join, which can be efficiently processed using hashing. The behavioral knowledge is compiled into predicate and manipulation function interfaces that can directly access tuples in a buffer.— Authors' Abstract",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2044005000",
    "type": "article"
  },
  {
    "title": "Formal aspects of concurrency control in long-duration transaction systems using the NT/PV model",
    "doi": "https://doi.org/10.1145/185827.185854",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Henry F. Korth; Greg Speegle",
    "corresponding_authors": "",
    "abstract": "In the typical database system, an execution is correct if it is equivalent to some serial execution. This criterion, called serializability, is unacceptable for new database applications which require long-duration transactions. We present a new transaction model which allows correctness criteria more suitable for these applications. This model combines three enhancements to the standard model: nested transactions, explicit predicates, and multiple versions. These features yield the name of the new model, nested transactions with predicates and versions, or NT/PV. The modular nature of the NT/PV model allows a straightforward representation of simple systems. It also provides a formal framework for describing complex interactions. The most complex interactions the model allows can be captured by a protocol which exploits all of the semantics available to the NT/PV model. An example of these interactions is shown in a CASE application. The example shows how a system based on the NT/PV model is superior to both standard database techniques and unrestricted systems in both correctness and performance.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2047880211",
    "type": "article"
  },
  {
    "title": "LH* <sub>RS</sub> ---a highly-available scalable distributed data structure",
    "doi": "https://doi.org/10.1145/1093382.1093386",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Witold Litwin; Rim Moussa; Thomas Schwarz",
    "corresponding_authors": "",
    "abstract": "LH* RS is a high-availability scalable distributed data structure (SDDS). An LH* RS file is hash partitioned over the distributed RAM of a multicomputer, for example, a network of PCs, and supports the unavailability of any k ≥ 1 of its server nodes. The value of k transparently grows with the file to offset the reliability decline. Only the number of the storage nodes potentially limits the file growth. The high-availability management uses a novel parity calculus that we have developed, based on Reed-Salomon erasure correcting coding. The resulting parity storage overhead is about the lowest possible. The parity encoding and decoding are faster than for any other candidate coding we are aware of. We present our scheme and its performance analysis, including experiments with a prototype implementation on Wintel PCs. The capabilities of LH* RS offer new perspectives to data intensive applications, including the emerging ones of grids and of P2P computing.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1998024670",
    "type": "article"
  },
  {
    "title": "A taxonomy for secure object-oriented databases",
    "doi": "https://doi.org/10.1145/174638.174640",
    "publication_date": "1994-03-01",
    "publication_year": 1994,
    "authors": "Martin S. Olivier; Sebastiaan H. von Solms",
    "corresponding_authors": "",
    "abstract": "This paper proposes a taxonomy for secure object-oriented databases in order to clarify the issues in modeling and implementing such databases. It also indicates some implications of the various choices one may make when designing such a database. Most secure database models have been designed for relational databases. The object-oriented database model is more complex than the relational model. For these reasons, models for secure object-oriented databases are more complex than their relational counterparts. Furthermore, since views of the object-oriented model differ, each security model has to make some assumptions about the object-oriented model used for its particular database. A number of models for secure object-oriented databases have been proposed. These models differ in many respects, because they focus on different aspects of the security problem, or because they make different assumptions about what constitutes a secure database or because they make different assumptions about the object-oriented model. The taxonomy proposed in this paper may be used to compare the various models: Models that focus on specific issues may be positioned in the broader context with the aid of the taxonomy. The taxonomy also identifies the major aspects where security models may differ and indicates some alternatives available to the system designer for each such design choice. We show some implications of using specific alternatives. Since differences between models for secure object-oriented databases are often subtle, a formal notation is necessary for a proper comparison. Such a formal notation also facilitates the formal derivation of restrictions that apply under specific conditions. The formal approach also gives a clear indication about the assumptions made by us—given as axioms—and the consequences of those assumptions (and of design choices made by the model designer)—given as theorems.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2114338328",
    "type": "article"
  },
  {
    "title": "Proxy-based acceleration of dynamically generated content on the world wide web",
    "doi": "https://doi.org/10.1145/1005566.1005571",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Anindya Datta; Kaushik Dutta; Helen Thomas; Debra VanderMeer; Krithi Ramamritham",
    "corresponding_authors": "",
    "abstract": "As Internet traffic continues to grow and websites become increasingly complex, performance and scalability are major issues for websites. Websites are increasingly relying on dynamic content generation applications to provide website visitors with dynamic, interactive, and personalized experiences. However, dynamic content generation comes at a cost---each request requires computation as well as communication across multiple components.To address these issues, various dynamic content caching approaches have been proposed. Proxy-based caching approaches store content at various locations outside the site infrastructure and can improve website performance by reducing content generation delays, firewall processing delays, and bandwidth requirements. However, existing proxy-based caching approaches either (a) cache at the page level, which does not guarantee that correct pages are served and provides very limited reusability, or (b) cache at the fragment level, which is associated with several design-level and runtime scalability issues. To address these issues, several back-end caching approaches have been proposed, including query result caching and fragment level caching. While back-end approaches guarantee the correctness of results and offer the advantages of fine-grained caching, they neither address firewall delays nor reduce bandwidth requirements.In this article, we present an approach and an implementation of a dynamic proxy caching technique which combines the benefits of both proxy-based and back-end caching approaches, yet does not suffer from their above-mentioned limitations. Our dynamic proxy caching technique allows granular, proxy-based caching in highly dynamic scenarios, accessible outside the site infrastructure. We present two possible configurations for our dynamic proxy caching technique: (1) a reverse proxy configuration, and (2) a forward proxy configuration. Analysis of the performance of our approach indicates that it is capable of providing significant reductions in bandwidth. We have deployed our proposed dynamic proxy caching technique at a major financial institution. The results of this implementation indicate that our technique is capable of providing up to 3x reductions in bandwidth and response times in real-world dynamic Web applications when compared to existing caching solutions.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2100824669",
    "type": "article"
  },
  {
    "title": "XML stream processing using tree-edit distance embeddings",
    "doi": "https://doi.org/10.1145/1061318.1061326",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Minos Garofalakis; Amit Kumar",
    "corresponding_authors": "",
    "abstract": "We propose the first known solution to the problem of correlating, in small space, continuous streams of XML data through approximate (structure and content) matching, as defined by a general tree-edit distance metric. The key element of our solution is a novel algorithm for obliviously embedding tree-edit distance metrics into an L 1 vector space while guaranteeing a (worst-case) upper bound of O (log 2 n log* n ) on the distance distortion between any data trees with at most n nodes. We demonstrate how our embedding algorithm can be applied in conjunction with known random sketching techniques to (1) build a compact synopsis of a massive, streaming XML data tree that can be used as a concise surrogate for the full tree in approximate tree-edit distance computations; and (2) approximate the result of tree-edit-distance similarity joins over continuous XML document streams. Experimental results from an empirical study with both synthetic and real-life XML data trees validate our approach, demonstrating that the average-case behavior of our embedding techniques is much better than what would be predicted from our theoretical worst-case distortion bounds. To the best of our knowledge, these are the first algorithmic results on low-distortion embeddings for tree-edit distance metrics, and on correlating (e.g., through similarity joins) XML data in the streaming model.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1996186909",
    "type": "article"
  },
  {
    "title": "Partial expansions for file organizations with an index",
    "doi": "https://doi.org/10.1145/12047.12049",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "David Lomet",
    "corresponding_authors": "David Lomet",
    "abstract": "A new way to increase file space in dynamically growing files is introduced in which substantial improvement in file utilization can be achieved. It makes use of partial expansions in which, instead of doubling the space associated with some part of the file, the space grows at a slower rate. Unlike previous versions of partial expansion in which the number of buckets involved in file growth is increased by less than a factor of two, the new method expands file space by increasing bucket size via “elastic buckets.” This permits partial expansions to be used with a wide range of indexed files, including B-trees. The results of using partial expansions are analyzed, and the analysis confirmed by a simulation study. The analysis and simulation demonstrate that the file utilization gains are substantial and that fears of excessive insertion cost resulting from more frequent file growth are unfounded.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2008721634",
    "type": "article"
  },
  {
    "title": "Rewriting queries with arbitrary aggregation functions using views",
    "doi": "https://doi.org/10.1145/1138394.1138400",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Sara Cohen; Werner Nutt; Yehoshua Sagiv",
    "corresponding_authors": "",
    "abstract": "The problem of rewriting aggregate queries using views is studied for conjunctive queries with arbitrary aggregation functions and built-in predicates. Two types of queries over views are introduced for rewriting aggregate queries: pure candidates and aggregate candidates . Pure candidates can be used to rewrite arbitrary aggregate queries. Aggregate candidates can be used to rewrite queries containing aggregate functions definable in terms of a commutative-semigroup operation. For both types of candidates (as well as for several relaxations of these candidates), the unfolding property holds. This allows characterizations for query equivalence to be used to determine whether a candidate is a rewriting of a query. The complexity of the rewriting-existence problem is also studied and upper and lower complexity bounds are given.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2029038562",
    "type": "article"
  },
  {
    "title": "Modeling the storage architectures of commercial database systems",
    "doi": "https://doi.org/10.1145/4879.5392",
    "publication_date": "1985-12-01",
    "publication_year": 1985,
    "authors": "Don Batory",
    "corresponding_authors": "Don Batory",
    "abstract": "Modeling the storage structures of a DBMS is a prerequisite to understanding and optimizing database performance. Previously, such modeling was very difficult because the fundamental role of conceptual-to-internal mappings in DBMS implementations went unrecognized. In this paper we present a model of physical databases, called the transformation model, that makes conceptual-to-internal mappings explicit. By exposing such mappings, we show that it is possible to model the storage architectures (i.e., the storage structures and mappings) of many commercial DBMSs in a precise, systematic, and comprehendible way. Models of the INQUIRE, ADABAS, and SYSTEM 2000 storage architectures are presented as examples of the model's utility. We believe the transformation model helps bridge the gap between physical database theory and practice. It also reveals the possibility of a technology to automate the development of physical database software.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2082070582",
    "type": "article"
  },
  {
    "title": "Maintenance of <i>K</i> -nn and spatial join queries on continuously moving points",
    "doi": "https://doi.org/10.1145/1138394.1138396",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Glenn S. Iwerks; Hanan Samet; Kenny Smith",
    "corresponding_authors": "",
    "abstract": "Cars, aircraft, mobile cell phones, ships, tanks, and mobile robots all have the common property that they are moving objects. A kinematic representation can be used to describe the location of these objects as a function of time. For example, a moving point can be represented by the function p ( t ) = x → 0 + ( t - t 0 ) v → , where x → 0 is the start location, t 0 is the start time, and v → is its velocity vector. Instead of storing the location of the object at a given time in a database, the coefficients of the function are stored. When an object's behavior changes enough so that the function describing its location is no longer accurate, the function coefficients for the object are updated. Because the location of each object is represented as a function of time, spatial query results can change even when no transactions update the database. We present efficient algorithms to maintain k -nearest neighbor, and spatial join queries in this domain as time advances and updates occur. We assume no previous knowledge of what the updates will be before they occur. We experimentally compare these new algorithms with more straight forward adaptations of previous work to support updates. Experiments are conducted using synthetic uniformly distributed data, and real aircraft flight data. The primary metric of comparison is the number of I/O disk accesses needed to maintain the query results and the supporting data structures.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2110059191",
    "type": "article"
  },
  {
    "title": "Optimization of join operations in horizontally partitioned database systems",
    "doi": "https://doi.org/10.1145/5236.5241",
    "publication_date": "1986-03-01",
    "publication_year": 1986,
    "authors": "Arie Segev",
    "corresponding_authors": "Arie Segev",
    "abstract": "This paper analyzes the problem of joining two horizontally partitioned relations in a distributed database system. Two types of semijoin strategies are introduced, local and remote. Local semijoins are performed at the site of the restricted relation (or fragment), and remote semijoins can be performed at an arbitrary site. A mathematical model of a semijoin strategy for the case of remote semijoins is developed, and lower bounding and heuristic procedures are proposed. The results of computational experiments are reported. The experiments include an analysis of the heuristics' performance relative to the lower bounds, sensitivity analysis, and error analysis. These results reveal a good performance of the heuristic procedures, and demonstrate the benefit of using semijoin operations to reduce the size of fragments prior to their transmission. The algorithms for the case of remote semijoins were found to be superior to the algorithms for the case of local semijoins. In addition, we found that the estimation accuracy of the selectivity factors has a significant effect on the incurred communication cost.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2071697688",
    "type": "article"
  },
  {
    "title": "Performance analysis of several back-end database architectures",
    "doi": "https://doi.org/10.1145/5236.5242",
    "publication_date": "1986-03-01",
    "publication_year": 1986,
    "authors": "Robert B. Hagmann; Domenico Ferrari",
    "corresponding_authors": "",
    "abstract": "The growing acceptance of database systems makes their performance increasingly more important. One way to gain performance is to off-load some of the functions of the database system to aback-end computer. The problem is what functions should be off-loaded to maximize the benefits of distributed processing. Our approach to this problem consisted of constructing several variants of an existing relational database system. INGRES, that partition the database system software into two parts, and assigning these two parts to two computers connected by a local area network. For the purposes of this experiment, six different variants of the database software were constructed to test the sir most interesting functional subdivisions. Each variant was then benchmarked using two different databases and query streams. The communication medium and the communication software were also benchmarked to measure their contribution to the performance of each configuration. Combining the database and network measurement results, various conclusions were reached about the viability of the configurations, the desirable properties of the communications mechanisms to he used, the operating system interface and overhead, and the performance of the database system. The variants to be preferred depend on the hardware technology, operating system features, database system internal structure, and network software overhead.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2086964875",
    "type": "article"
  },
  {
    "title": "A simple bounded disorder file organization with good performance",
    "doi": "https://doi.org/10.1145/49346.50067",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "David Lomet",
    "corresponding_authors": "David Lomet",
    "abstract": "A bounded-disorder (BD) file is one in which data are organized into nodes that are indexed, e.g., by means of a B-tree. The data nodes are multibucket nodes that are accessed by hashing. In this paper we present two important improvements to the BD organization as originally described. First, records in a data node that overflow their designated primary bucket are stored in a single overflow bucket which is itself a bucket of the data node. Second, when file space needs to be increased, partial expansions are used that employ elastic buckets. Analysis and simulation results demonstrate that this variant of the BD organization has utilization, random access performance, and file growth performance that can be competitive with good extendible hashing methods, while supporting high-performance sequential access. The simplicity of the organization results in simple algorithms for realizing the organization.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1973922778",
    "type": "article"
  },
  {
    "title": "Estimating the selectivity of approximate string queries",
    "doi": "https://doi.org/10.1145/1242524.1242529",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Artūras Mažeika; Michael H. Böhlen; Nick Koudas; Divesh Srivastava",
    "corresponding_authors": "",
    "abstract": "Approximate queries on string data are important due to the prevalence of such data in databases and various conventions and errors in string data. We present the VSol estimator, a novel technique for estimating the selectivity of approximate string queries. The VSol estimator is based on inverse strings and makes the performance of the selectivity estimator independent of the number of strings. To get inverse strings we decompose all database strings into overlapping substrings of length q (q-grams) and then associate each q-gram with its inverse string: the IDs of all strings that contain the q-gram. We use signatures to compress inverse strings, and clustering to group similar signatures. We study our technique analytically and experimentally. The space complexity of our estimator only depends on the number of neighborhoods in the database and the desired estimation error. The time to estimate the selectivity is independent of the number of database strings and linear with respect to the length of query string. We give a detailed empirical performance evaluation of our solution for synthetic and real-world datasets. We show that VSol is effective for large skewed databases of short strings.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1993831977",
    "type": "article"
  },
  {
    "title": "Query optimization in distributed networks of autonomous database systems",
    "doi": "https://doi.org/10.1145/1138394.1138397",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Fragkiskos Pentaris; Yannis Ioannidis",
    "corresponding_authors": "",
    "abstract": "Large-scale distributed environments, where each node is completely autonomous and offers services to its peers through external communication, pose significant challenges to query processing and optimization. Autonomy is the main source of the problem, as it results in lack of knowledge about any particular node with respect to the information it can produce and its characteristics, for example, cost of production or quality of produced results. In this article, inspired by e-commerce technology, we recognize queries as commodities and model query optimization as a trading negotiation process. Subquery answers and subquery operator execution jobs are traded between nodes until deals are struck with some nodes for all of them. Such trading may also occur recursively, in the sense that some nodes may play the role of intermediaries between other nodes (subcontracting). We identify the key parameters of the overall framework and suggest several potential alternatives for each one. In comparison to trading negotiations for e-commerce, query optimization faces unique new challenges that stem primarily from the fact that queries have a complex structure and can be broken into smaller parts. We address these challenges through a particular instantiation of our framework focusing primarily on the optimization algorithms run on “buying” and “selling” nodes, the evaluation metrics of the queries, and the negotiation strategy. Finally, we present the results of several experiments that demonstrate the performance characteristics of our approach compared to those of traditional query optimization.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2026935307",
    "type": "article"
  },
  {
    "title": "Resolving the query inference problem using Steiner trees",
    "doi": "https://doi.org/10.1145/1270.1275",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Joseph A. Wald; Paul Sorenson",
    "corresponding_authors": "",
    "abstract": "The query inference problem is to translate a sentence of a query language into an unambiguous representation of a query. A query is represented as an expression over a set of query trees. A metric is introduced for measuring the complexity of a query and also a proposal that a sentence be translated into the least complex query which “satisfies” the sentence. This method of query inference can be used to resolve ambiguous sentences and leads to easier formulation of sentences.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2110954374",
    "type": "article"
  },
  {
    "title": "Bounded index exponential hashing",
    "doi": "https://doi.org/10.1145/319830.319837",
    "publication_date": "1983-03-01",
    "publication_year": 1983,
    "authors": "David Lomet",
    "corresponding_authors": "David Lomet",
    "abstract": "Bounded index exponential hashing, a new form of extendible hashing, is described. It has the important advantages over most of the other extendible hashing variants of both (i) providing random access to any record of a file in close to one disk access and (ii) having performance which does not vary with file size. It is straightforward to implement and demands only a fixed and specifiable amount of main storage to achieve this performance. Its underlying physical disk storage is readily managed and record overflow is handled so as to insure that unsuccessful searches never take more than two accesses. The method's ability to access data in close to a single disk access makes it possible to organize a database, in which files have a primary key and multiple secondary keys, such that the result is a significant performance advantage over existing organizations.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2057757215",
    "type": "article"
  },
  {
    "title": "Correctness of query execution strategies in distributed databases",
    "doi": "https://doi.org/10.1145/319996.320009",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "S. Ceri; Giuseppe Pelagatti",
    "corresponding_authors": "",
    "abstract": "A major requirement of a Distributed DataBase Management System (DDBMS) is to enable users to write queries as though the database were not distributed (distribution transparency). The DDBMS transforms the user's queries into execution strategies, that is, sequences of operations on the various nodes of the network and of transmissions between them. An execution strategy on a distributed database is correct if it returns the same result as if the query were applied to a nondistributed database. This paper analyzes the correctness problem for query execution strategies. A formal model, called Multirelational Algebra, is used as a unifying framework for this purpose. The problem of proving the correctness of execution strategies is reduced to the problem of proving the equivalence of two expressions of Multirelational Algebra. A set of theorems on equivalence is given in order to facilitate this task. The proposed approach can be used also for the generation of correct execution strategies, because it defines the rules which allow the transformation of a correct strategy into an equivalent one. This paper does not deal with the problem of evaluating equivalent strategies, and therefore is not in itself a proposal for a query optimizer for distributed databases. However, it constitutes a theoretical foundation for the design of such optimizers.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2080042338",
    "type": "article"
  },
  {
    "title": "Robust approximate aggregation in sensor data management systems",
    "doi": "https://doi.org/10.1145/1508857.1508863",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Jeffrey Considine; Marios Hadjieleftheriou; Feifei Li; John W. Byers; George Kollios",
    "corresponding_authors": "",
    "abstract": "In the emerging area of sensor-based systems, a significant challenge is to develop scalable, fault-tolerant methods to extract useful information from the data the sensors collect. An approach to this data management problem is the use of sensor database systems, which allow users to perform aggregation queries such as MIN, COUNT, and AVG on the readings of a sensor network. In addition, more advanced queries such as frequency counting and quantile estimation can be supported. Due to energy limitations in sensor-based networks, centralized data collection is generally impractical, so most systems use in-network aggregation to reduce network traffic. However, even these aggregation strategies remain bandwidth-intensive when combined with the fault-tolerant, multipath routing methods often used in these environments. To avoid this expense, we investigate the use of approximate in-network aggregation using small sketches. We present duplicate-insensitive sketching techniques that can be implemented efficiently on small sensor devices with limited hardware support and we analyze both their performance and accuracy. Finally, we present an experimental evaluation that validates the effectiveness of our methods.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1972848834",
    "type": "article"
  },
  {
    "title": "On static and dynamic methods for condensation-based privacy-preserving data mining",
    "doi": "https://doi.org/10.1145/1331904.1331906",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Charų C. Aggarwal; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "In recent years, privacy-preserving data mining has become an important problem because of the large amount of personal data which is tracked by many business applications. In many cases, users are unwilling to provide personal information unless the privacy of sensitive information is guaranteed. In this paper, we propose a new framework for privacy-preserving data mining of multidimensional data. Previous work for privacy-preserving data mining uses a perturbation approach which reconstructs data distributions in order to perform the mining. Such an approach treats each dimension independently and therefore ignores the correlations between the different dimensions. In addition, it requires the development of a new distribution-based algorithm for each data mining problem, since it does not use the multidimensional records, but uses aggregate distributions of the data as input. This leads to a fundamental re-design of data mining algorithms. In this paper, we will develop a new and flexible approach for privacy-preserving data mining that does not require new problem-specific algorithms, since it maps the original data set into a new anonymized data set. These anonymized data closely match the characteristics of the original data including the correlations among the different dimensions. We will show how to extend the method to the case of data streams. We present empirical results illustrating the effectiveness of the method. We also show the efficiency of the method for data streams.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2020422256",
    "type": "article"
  },
  {
    "title": "Linear queries in statistical databases",
    "doi": "https://doi.org/10.1145/320071.320073",
    "publication_date": "1979-06-01",
    "publication_year": 1979,
    "authors": "Mark D. Schwartz; Dorothy E. Denning; Peter J. Denning",
    "corresponding_authors": "",
    "abstract": "A database is compromised if a user can determine the data elements associated with keys which he did not know previously. If it is possible, compromise can be achieved by posing a finite set of queries over sets of data elements and employing initial information to solve the resulting system of equations. Assuming the allowable queries are linear, that is, weighted sums of data elements, we show how compromise can be achieved and we characterize the maximal initial information permitted of a user in a secure system. When compromise is possible, the initial information and the number of queries required to achieve it is surprisingly small.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2143579677",
    "type": "article"
  },
  {
    "title": "Theseus—a programming language for relational databeses",
    "doi": "https://doi.org/10.1145/320107.320121",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "Jonathan E. Shopiro",
    "corresponding_authors": "Jonathan E. Shopiro",
    "abstract": "Theseus, a very high-level programming language extending EUCLID, is described. Data objects in Theseus include relations and a-sets, a generalization of records. The primary design goals of Theseus are to facilitate the writing of well-structured programs for database applications and to serve as a vehicle for research in automatic program optimization.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1979062141",
    "type": "article"
  },
  {
    "title": "Security of statistical databases",
    "doi": "https://doi.org/10.1145/320610.320645",
    "publication_date": "1980-12-01",
    "publication_year": 1980,
    "authors": "Jan Schlöer",
    "corresponding_authors": "Jan Schlöer",
    "abstract": "Statistical evaluation of databases which contain personal records may entail risks for the confidentiality of the individual records. The risk has increased with the availability of flexible interactive evaluation programs which permit the use of trackers, the most dangerous class of snooping tools known. A class of trackers, called union trackers, is described. They permit reconstruction of the entire database without supplementary knowledge and include the general tracker recently described as a special case. For many real statistical databases the overwhelming majority of definable sets of records will form trackers. For such databases a random search for a tracker is likely to succeed rapidly. Individual trackers are redefined and counted and their cardinalities are investigated. If there are n records in the database, then most individual trackers employ innocent cardinalities near n /3, making them difficult to detect. Disclosure with trackers usually requires little effort per retrieved data element.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3142133991",
    "type": "article"
  },
  {
    "title": "Transparent anonymization",
    "doi": "https://doi.org/10.1145/1735886.1735887",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Xiaokui Xiao; Yufei Tao; Nick Koudas",
    "corresponding_authors": "",
    "abstract": "Numerous generalization techniques have been proposed for privacy-preserving data publishing. Most existing techniques, however, implicitly assume that the adversary knows little about the anonymization algorithm adopted by the data publisher. Consequently, they cannot guard against privacy attacks that exploit various characteristics of the anonymization mechanism. This article provides a practical solution tothis problem. First, we propose an analytical model for evaluating disclosure risks, when an adversary knows everything in the anonymization process, except the sensitive values. Based on this model, we develop a privacy principle, transparent l-diversity , which ensures privacy protection against such powerful adversaries. We identify three algorithms that achieve transparent l -diversity, and verify their effectiveness and efficiency through extensive experiments with real data.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1984237017",
    "type": "article"
  },
  {
    "title": "Static analysis of active XML systems",
    "doi": "https://doi.org/10.1145/1620585.1620590",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Serge Abiteboul; Luc Segoufin; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "Active XML is a high-level specification language tailored to data-intensive, distributed, dynamic Web services. Active XML is based on XML documents with embedded function calls. The state of a document evolves depending on the result of internal function calls (local computations) or external ones (interactions with users or other services). Function calls return documents that may be active, and so may activate new subtasks. The focus of this article is on the verification of temporal properties of runs of Active XML systems, specified in a tree-pattern-based temporal logic, Tree-LTL, which allows expressing a rich class of semantic properties of the application. The main results establish the boundary of decidability and the complexity of automatic verification of Tree-LTL properties.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2733686445",
    "type": "article"
  },
  {
    "title": "Partial Evaluation for Distributed XPath Query Processing and Beyond",
    "doi": "https://doi.org/10.1145/2389241.2389251",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Gao Cong; Wenfei Fan; Anastasios Kementsietsidis; Jianzhong Li; Xianmin Liu",
    "corresponding_authors": "",
    "abstract": "This article proposes algorithms for evaluating XPath queries over an XML tree that is partitioned horizontally and vertically, and is distributed across a number of sites. The key idea is based on partial evaluation: it is to send the whole query to each site that partially evaluates the query, in parallel, and sends the results as compact (Boolean) functions to a coordinator that combines these to obtain the result. This approach possesses the following performance guarantees. First, each site is visited at most twice for data-selecting XPath queries, and only once for Boolean XPath queries. Second, the network traffic is determined by the answer to the query, rather than the size of the tree. Third, the total computation is comparable to that of centralized algorithms on the tree stored in a single site, regardless of how the tree is fragmented and distributed. We also present a MapReduce algorithm for evaluating Boolean XPath queries, based on partial evaluation. In addition, we provide algorithms to evaluate XPath queries on very large XML trees, in a centralized setting. We show both analytically and empirically that our techniques are scalable with large trees and complex XPath queries. These results, we believe, illustrate the usefulness and potential of partial evaluation in distributed systems as well as centralized XML stores for evaluating XPath queries and beyond.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1966540900",
    "type": "article"
  },
  {
    "title": "Optimal matching between spatial datasets under capacity constraints",
    "doi": "https://doi.org/10.1145/1735886.1735888",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Leong Hou U; Kyriakos Mouratidis; Man Lung Yiu; Nikos Mamoulis",
    "corresponding_authors": "",
    "abstract": "Consider a set of customers (e.g., WiFi receivers) and a set of service providers (e.g., wireless access points), where each provider has a capacity and the quality of service offered to its customers is anti-proportional to their distance. The Capacity Constrained Assignment (CCA) is a matching between the two sets such that (i) each customer is assigned to at most one provider, (ii) every provider serves no more customers than its capacity, (iii) the maximum possible number of customers are served, and (iv) the sum of Euclidean distances within the assigned provider-customer pairs is minimized. Although max-flow algorithms are applicable to this problem, they require the complete distance-based bipartite graph between the customer and provider sets. For large spatial datasets, this graph is expensive to compute and it may be too large to fit in main memory. Motivated by this fact, we propose efficient algorithms for optimal assignment that employ novel edge-pruning strategies, based on the spatial properties of the problem. Additionally, we develop incremental techniques that maintain an optimal assignment (in the presence of updates) with a processing cost several times lower than CCA recomputation from scratch. Finally, we present approximate (i.e., suboptimal) CCA solutions that provide a tunable trade-off between result accuracy and computation cost, abiding by theoretical quality guarantees. A thorough experimental evaluation demonstrates the efficiency and practicality of the proposed techniques.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2031323533",
    "type": "article"
  },
  {
    "title": "Finding Alternative Shortest Paths in Spatial Networks",
    "doi": "https://doi.org/10.1145/2389241.2389248",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Kexin Xie; Ke Deng; Shuo Shang; Xiaofang Zhou; Kai Zheng",
    "corresponding_authors": "",
    "abstract": "Shortest path query is one of the most fundamental queries in spatial network databases. There exist algorithms that can process shortest path queries in real time. However, many complex applications require more than just the calculation of a single shortest path. For example, one of the common ways to determine the importance (or price) of a vertex or an edge in spatial network is to use Vickrey pricing, which intuitively values the vertex v (or edge e ) based on how much harder for travelling from the sources to the destinations without using v (or e ). In such cases, the alternative shortest paths without using v (or e ) are required. In this article, we propose using a precomputation based approach for both single pair alternative shortest path and all pairs shortest paths processing. To compute the alternative shortest path between a source and a destination efficiently, a naïive way is to precompute and store all alternative shortest paths between every pair of vertices avoiding every possible vertex (or edge), which requires O ( n 4 ) space. Currently, the state of the art approach for reducing the storage cost is to choose a subset of the vertices as center points, and only store the single-source alternative shortest paths from those center points. Such approach has the space complexity of O ( n 2 log n ). We propose a storage scheme termed iSPQF , which utilizes shortest path quadtrees by observing the relationships between each avoiding vertex and its corresponding alternative shortest paths. We have reduced the space complexity from the naïive O ( n 4 ) (or the state of the art O ( n 4 log n )) to O (min( γ, L ) n 1.5 ) with comparable query performance of O ( K ), where K is the number of vertices in the returned paths, L is the diameter of the spatial network, and γ is a value that depends on the structure of the spatial network, which is empirically estimated to be 40 for real road networks. Experiments on real road networks have shown that the space cost of the proposed iSPQF is scalable, and both the algorithms based on iSPQF are efficient.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2051257608",
    "type": "article"
  },
  {
    "title": "Reordering rows for better compression",
    "doi": "https://doi.org/10.1145/2338626.2338633",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Daniel Lemire; Owen Kaser; Eduardo Gutarra",
    "corresponding_authors": "",
    "abstract": "Sorting database tables before compressing them improves the compression rate. Can we do better than the lexicographical order? For minimizing the number of runs in a run-length encoding compression scheme, the best approaches to row-ordering are derived from traveling salesman heuristics, although there is a significant trade-off between running time and compression. A new heuristic, Multiple Lists, which is a variant on Nearest Neighbor that trades off compression for a major running-time speedup, is a good option for very large tables. However, for some compression schemes, it is more important to generate long runs rather than few runs. For this case, another novel heuristic, Vortex, is promising. We find that we can improve run-length encoding up to a factor of 3 whereas we can improve prefix coding by up to 80%: these gains are on top of the gains due to lexicographically sorting the table. We prove that the new row reordering is optimal (within 10%) at minimizing the runs of identical values within columns, in a few cases.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2139000699",
    "type": "article"
  },
  {
    "title": "Asymptotically efficient algorithms for skyline probabilities of uncertain data",
    "doi": "https://doi.org/10.1145/1966385.1966390",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Mikhail J. Atallah; Yinian Qi; Hao Yuan",
    "corresponding_authors": "",
    "abstract": "Skyline computation is widely used in multicriteria decision making. As research in uncertain databases draws increasing attention, skyline queries with uncertain data have also been studied. Some earlier work focused on probabilistic skylines with a given threshold; Atallah and Qi [2009] studied the problem to compute skyline probabilities for all instances of uncertain objects without the use of thresholds, and proposed an algorithm with subquadratic time complexity. In this work, we propose a new algorithm for computing all skyline probabilities that is asymptotically faster: worst-case O(n √ n log n ) time and O(n) space for 2D data; O ( n 2−1/d log d −1 n ) time and O(n log d −2 n ) space for d -dimensional data. Furthermore, we study the online version of the problem: Given any query point p (unknown until the query time), return the probability that no instance in the given data set dominates p . We propose an algorithm for answering such an online query for d -dimensional data in O(n 1−1/ d log d −1 n ) time after preprocessing the data in O(n 2−1/d log d −1 ) time and space.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2048553547",
    "type": "article"
  },
  {
    "title": "Closed world data exchange",
    "doi": "https://doi.org/10.1145/1966385.1966392",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "André Hernich; Leonid Libkin; Nicole Schweikardt",
    "corresponding_authors": "",
    "abstract": "Data exchange deals with translating data structured in some source format into data structured in some target format, given a specification of the relationship between the source and the target and possibly constraints on the target; and answering queries over the target in a way that is semantically consistent with the information in the source. Theoretical foundations of data exchange have been actively explored recently. It was also noticed that the standard semantics for query answering in data exchange may lead to counterintuitive or anomalous answers. In the present article, we explain that this behavior is due to the fact that solutions can contain invented information (information that is not related to the source instance), and that the presence of incomplete information in target instances has been ignored. In particular, proper query evaluation techniques for databases with nulls have not been used, and the distinction between closed and open world semantics has not been made. We present a concept of solutions, called CWA-solutions, that is based on the closed world assumption. For data exchange settings without constraints on the target, the space of CWA-solutions has two extreme points: the canonical universal solution (the maximal CWA-solution) and the core of the universal solutions (the minimal CWA-solution), both of them well studied in data exchange. In the presence of constraints on the target, the core of the universal solutions is still the minimal CWA-solution, but there may be no unique maximal CWA-solution. We show how to define the semantics of query-answering taking into account incomplete information, and show that some of the well-known anomalies go away with the new semantics. The article also contains results on the complexity of query-answering, upper approximations to queries (maybe-answers), and various extensions.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2045316927",
    "type": "article"
  },
  {
    "title": "SCALLA",
    "doi": "https://doi.org/10.1145/2389241.2389246",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Boduo Li; Edward Mazur; Yanlei Diao; Andrew McGregor; Prashant Shenoy",
    "corresponding_authors": "",
    "abstract": "Today’s one-pass analytics applications tend to be data-intensive in nature and require the ability to process high volumes of data efficiently. MapReduce is a popular programming model for processing large datasets using a cluster of machines. However, the traditional MapReduce model is not well-suited for one-pass analytics, since it is geared towards batch processing and requires the dataset to be fully loaded into the cluster before running analytical queries. This article examines, from a systems standpoint, what architectural design changes are necessary to bring the benefits of the MapReduce model to incremental one-pass analytics. Our empirical and theoretical analyses of Hadoop-based MapReduce systems show that the widely used sort-merge implementation for partitioning and parallel processing poses a fundamental barrier to incremental one-pass analytics, despite various optimizations. To address these limitations, we propose a new data analysis platform that employs hash techniques to enable fast in-memory processing, and a new frequent key based technique to extend such processing to workloads that require a large key-state space. Evaluation of our Hadoop-based prototype using real-world workloads shows that our new platform significantly improves the progress of map tasks, allows the reduce progress to keep up with the map progress, with up to 3 orders of magnitude reduction of internal data spills, and enables results to be returned continuously during the job.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2134113644",
    "type": "article"
  },
  {
    "title": "Optimizing Batch Linear Queries under Exact and Approximate Differential Privacy",
    "doi": "https://doi.org/10.1145/2699501",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Ganzhao Yuan; Zhenjie Zhang; Marianne Winslett; Xiaokui Xiao; Yin Yang; Zhifeng Hao",
    "corresponding_authors": "",
    "abstract": "Differential privacy is a promising privacy-preserving paradigm for statistical query processing over sensitive data. It works by injecting random noise into each query result such that it is provably hard for the adversary to infer the presence or absence of any individual record from the published noisy results. The main objective in differentially private query processing is to maximize the accuracy of the query results while satisfying the privacy guarantees. Previous work, notably Li et al. [2010], has suggested that, with an appropriate strategy, processing a batch of correlated queries as a whole achieves considerably higher accuracy than answering them individually. However, to our knowledge there is currently no practical solution to find such a strategy for an arbitrary query batch; existing methods either return strategies of poor quality (often worse than naive methods) or require prohibitively expensive computations for even moderately large domains. Motivated by this, we propose a low-rank mechanism (LRM), the first practical differentially private technique for answering batch linear queries with high accuracy. LRM works for both exact (i.e., ϵ-) and approximate (i.e., (ϵ, δ)-) differential privacy definitions. We derive the utility guarantees of LRM and provide guidance on how to set the privacy parameters, given the user's utility expectation. Extensive experiments using real data demonstrate that our proposed method consistently outperforms state-of-the-art query processing solutions under differential privacy, by large margins.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1516526996",
    "type": "article"
  },
  {
    "title": "Analysis and optimization for boolean expression indexing",
    "doi": "https://doi.org/10.1145/2487259.2487260",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Mohammad Sadoghi; Hans‐Arno Jacobsen",
    "corresponding_authors": "",
    "abstract": "BE-Tree is a novel dynamic data structure designed to efficiently index Boolean expressions over a high-dimensional discrete space. BE Tree-copes with both high-dimensionality and expressiveness of Boolean expressions by introducing an effective two-phase space-cutting technique that specifically utilizes the discrete and finite domain properties of the space. Furthermore, BE-Tree employs self-adjustment policies to dynamically adapt the tree as the workload changes. Moreover, in BE-Tree, we develop two novel cache-conscious predicate evaluation techniques, namely, lazy and bitmap evaluations, that also exploit the underlying discrete and finite space to substantially reduce BE-Tree's matching time by up to 75% BE-Tree is a general index structure for matching Boolean expression which has a wide range of applications including (complex) event processing, publish/subscribe matching, emerging applications in cospaces, profile matching for targeted web advertising, and approximate string matching. Finally, the superiority of BE-Tree is proven through a comprehensive evaluation with state-of-the-art index structures designed for matching Boolean expressions.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2053495332",
    "type": "article"
  },
  {
    "title": "Towards a Painless Index for Spatial Objects",
    "doi": "https://doi.org/10.1145/2629333",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Rui Zhang; Jianzhong Qi; Martin Stradling; Jin Huang",
    "corresponding_authors": "",
    "abstract": "Conventional spatial indexes, represented by the R-tree, employ multidimensional tree structures that are complicated and require enormous efforts to implement in a full-fledged database management system (DBMS). An alternative approach for supporting spatial queries is mapping-based indexing, which maps both data and queries into a one-dimensional space such that data can be indexed and queries can be processed through a one-dimensional indexing structure such as the B + . Mapping-based indexing requires implementing only a few mapping functions, incurring much less effort in implementation compared to conventional spatial index structures. Yet, a major concern about using mapping-based indexes is their lower efficiency than conventional tree structures. In this article, we propose a mapping-based spatial indexing scheme called Size Separation Indexing (SSI). SSI is equipped with a suite of techniques including size separation, data distribution transformation, and more efficient mapping algorithms. These techniques overcome the drawbacks of existing mapping-based indexes and significantly improve the efficiency of query processing. We show through extensive experiments that, for window queries on spatial objects with nonzero extents , SSI has two orders of magnitude better performance than existing mapping-based indexes and competitive performance to the R-tree as a standalone implementation. We have also implemented SSI on top of two off-the-shelf DBMSs, PostgreSQL and a commercial platform, both having R-tree implementation. In this case, SSI is up to two orders of magnitude faster than their provided spatial indexes. Therefore, we achieve a spatial index more efficient than the R-tree in a DBMS implementation that is at the same time easy to implement. This result may upset a common perception that has existed for a long time in this area that the R-tree is the best choice for indexing spatial objects.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2056521113",
    "type": "article"
  },
  {
    "title": "Top-k diversity queries over bounded regions",
    "doi": "https://doi.org/10.1145/2487259.2487262",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Ilio Catallo; Eleonora Ciceri; Piero Fraternali; Davide Martinenghi; Marco Tagliasacchi",
    "corresponding_authors": "",
    "abstract": "Top-k diversity queries over objects embedded in a low-dimensional vector space aim to retrieve the best k objects that are both relevant to given user's criteria and well distributed over a designated region. An interesting case is provided by spatial Web objects, which are produced in great quantity by location-based services that let users attach content to places and are found also in domains like trip planning, news analysis, and real estate. In this article we present a technique for addressing such queries that, unlike existing methods for diversified top- k queries, does not require accessing and scanning all relevant objects in order to find the best k results. Our Space Partitioning and Probing (SPP) algorithm works by progressively exploring the vector space, while keeping track of the already seen objects and of their relevance and position. The goal is to provide a good quality result set in terms of both relevance and diversity. We assess quality by using as a baseline the result set computed by MMR, one of the most popular diversification algorithms, while minimizing the number of accessed objects. In order to do so, SPP exploits score-based and distance-based access methods, which are available, for instance, in most geo-referenced Web data sources. Experiments with both synthetic and real data show that SPP produces results that are relevant and spatially well distributed, while significantly reducing the number of accessed objects and incurring a very low computational overhead.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2113952707",
    "type": "article"
  },
  {
    "title": "Maximizing Range Sum in External Memory",
    "doi": "https://doi.org/10.1145/2629477",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Dong-Wan Choi; Chin‐Wan Chung; Yufei Tao",
    "corresponding_authors": "",
    "abstract": "This article studies the MaxRS problem in spatial databases. Given a set O of weighted points and a rectangle r of a given size, the goal of the MaxRS problem is to find a location of r such that the sum of the weights of all the points covered by r is maximized. This problem is useful in many location-based services such as finding the best place for a new franchise store with a limited delivery range and finding the hotspot with the largest number of nearby attractions for a tourist with a limited reachable range. However, the problem has been studied mainly in the theoretical perspective, particularly in computational geometry. The existing algorithms from the computational geometry community are in-memory algorithms that do not guarantee the scalability. In this article, we propose a scalable external-memory algorithm ( ExactMaxRS ) for the MaxRS problem that is optimal in terms of the I/O complexity. In addition, we propose an approximation algorithm ( ApproxMaxCRS ) for the MaxCRS problem that is a circle version of the MaxRS problem. We prove the correctness and optimality of the ExactMaxRS algorithm along with the approximation bound of the ApproxMaxCRS algorithm. Furthermore, motivated by the fact that all the existing solutions simply assume that there is no tied area for the best location, we extend the MaxRS problem to a more fundamental problem, namely AllMaxRS , so that all the locations with the same best score can be retrieved. We first prove that the AllMaxRS problem cannot be trivially solved by applying the techniques for the MaxRS problem. Then we propose an output-sensitive external-memory algorithm ( TwoPhaseMaxRS ) that gives the exact solution for the AllMaxRS problem through two phases. Also, we prove both the soundness and completeness of the result returned from TwoPhaseMaxRS. From extensive experimental results, we show that ExactMaxRS and ApproxMaxCRS are several orders of magnitude faster than methods adapted from existing algorithms, the approximation bound in practice is much better than the theoretical bound of ApproxMaxCRS, and TwoPhaseMaxRS is not only much faster but also more robust than the straightforward extension of ExactMaxRS.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1991757391",
    "type": "article"
  },
  {
    "title": "Inferring Social Strength from Spatiotemporal Data",
    "doi": "https://doi.org/10.1145/2877200",
    "publication_date": "2016-03-18",
    "publication_year": 2016,
    "authors": "Huy Pham; Cyrus Shahabi; Yan Liu",
    "corresponding_authors": "",
    "abstract": "The advent of geolocation technologies has generated unprecedented rich datasets of people’s location information at a very high fidelity. These location datasets can be used to study human behavior; for example, social studies have shown that people who are seen together frequently at the same place and same time are most probably socially related. In this article, we are interested in inferring these social connections by analyzing people’s location information; this is useful in a variety of application domains, from sales and marketing to intelligence analysis. In particular, we propose an entropy-based model (EBM) that not only infers social connections but also estimates the strength of social connections by analyzing people’s co-occurrences in space and time. We examine two independent methods: diversity and weighted frequency , through which co-occurrences contribute to the strength of a social connection. In addition, we take the characteristics of each location into consideration in order to compensate for cases where only limited location information is available. We also study the role of location semantics in improving our computation of social strength. We develop a parallel implementation of our algorithm using MapReduce to create a scalable and efficient solution for online applications. We conducted extensive sets of experiments with real-world datasets including both people’s location data and their social connections, where we used the latter as the ground truth to verify the results of applying our approach to the former. We show that our approach is valid across different networks and outperforms the competitors.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2298643255",
    "type": "article"
  },
  {
    "title": "An Indexing Framework for Queries on Probabilistic Graphs",
    "doi": "https://doi.org/10.1145/3044713",
    "publication_date": "2017-05-10",
    "publication_year": 2017,
    "authors": "Silviu Maniu; Reynold Cheng; Pierre Senellart",
    "corresponding_authors": "",
    "abstract": "Information in many applications, such as mobile wireless systems, social networks, and road networks, is captured by graphs. In many cases, such information is uncertain. We study the problem of querying a probabilistic graph, in which vertices are connected to each other probabilistically. In particular, we examine “source-to-target” queries (ST-queries), such as computing the shortest path between two vertices. The major difference with the deterministic setting is that query answers are enriched with probabilistic annotations. Evaluating ST-queries over probabilistic graphs is #P-hard, as it requires examining an exponential number of “possible worlds”—database instances generated from the probabilistic graph. Existing solutions to the ST-query problem, which sample possible worlds, have two downsides: (i) a possible world can be very large and (ii) many samples are needed for reasonable accuracy. To tackle these issues, we study the ProbTree , a data structure that stores a succinct, or indexed , version of the possible worlds of the graph. Existing ST-query solutions are executed on top of this structure, with the number of samples and sizes of the possible worlds reduced. We examine lossless and lossy methods for generating the ProbTree, which reflect the tradeoff between the accuracy and efficiency of query evaluation. We analyze the correctness and complexity of these approaches. Our extensive experiments on real datasets show that the ProbTree is fast to generate and small in size. It also enhances the accuracy and efficiency of existing ST-query algorithms significantly.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2613925147",
    "type": "article"
  },
  {
    "title": "Detecting Inclusion Dependencies on Very Many Tables",
    "doi": "https://doi.org/10.1145/3105959",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Fabian Tschirschnitz; Thorsten Papenbrock; Felix Naumann",
    "corresponding_authors": "",
    "abstract": "Detecting inclusion dependencies, the prerequisite of foreign keys, in relational data is a challenging task. Detecting them among the hundreds of thousands or even millions of tables on the web is daunting. Still, such inclusion dependencies can help connect disparate pieces of information on the Web and reveal unknown relationships among tables. With the algorithm M any , we present a novel inclusion dependency detection algorithm, specialized for the very many—but typically small—tables found on the Web. We make use of Bloom filters and indexed bit-vectors to show the feasibility of our approach. Our evaluation on two corpora of Web tables shows a superior runtime over known approaches and its usefulness to reveal hidden structures on the Web.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2741470040",
    "type": "article"
  },
  {
    "title": "Query Nesting, Assignment, and Aggregation in SPARQL 1.1",
    "doi": "https://doi.org/10.1145/3083898",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Mark Kaminski; Egor V. Kostylev; Bernardo Cuenca Grau",
    "corresponding_authors": "",
    "abstract": "Answering aggregate queries is a key requirement of emerging applications of Semantic Technologies, such as data warehousing, business intelligence, and sensor networks. To fulfil the requirements of such applications, the standardization of SPARQL 1.1 led to the introduction of a wide range of constructs that enable value computation, aggregation, and query nesting. In this article, we provide an in-depth formal analysis of the semantics and expressive power of these new constructs as defined in the SPARQL 1.1 specification, and hence lay the necessary foundations for the development of robust, scalable, and extensible query engines supporting complex numerical and analytics tasks.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2745529561",
    "type": "article"
  },
  {
    "title": "The Complexity of Mining Maximal Frequent Subgraphs",
    "doi": "https://doi.org/10.1145/2629550",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Benny Kimelfeld; Phokion G. Kolaitis",
    "corresponding_authors": "",
    "abstract": "A frequent subgraph of a given collection of graphs is a graph that is isomorphic to a subgraph of at least as many graphs in the collection as a given threshold. Frequent subgraphs generalize frequent itemsets and arise in various contexts, from bioinformatics to the Web. Since the space of frequent subgraphs is typically extremely large, research in graph mining has focused on special types of frequent subgraphs that can be orders of magnitude smaller in number, yet encapsulate the space of all frequent subgraphs. Maximal frequent subgraphs (i.e., the ones not properly contained in any frequent subgraph) constitute the most useful such type. In this article, we embark on a comprehensive investigation of the computational complexity of mining maximal frequent subgraphs. Our study is carried out by considering the effect of three different parameters: possible restrictions on the class of graphs; a fixed bound on the threshold; and a fixed bound on the number of desired answers. We focus on specific classes of connected graphs: general graphs, planar graphs, graphs of bounded degree, and graphs of bounded treewidth (trees being a special case). Moreover, each class has two variants: that in which the nodes are unlabeled, and that in which they are uniquely labeled. We delineate the complexity of the enumeration problem for each of these variants by determining when it is solvable in (total or incremental) polynomial time and when it is NP-hard. Specifically, for the labeled classes, we show that bounding the threshold yields tractability but, in most cases, bounding the number of answers does not, unless P=NP; an exception is the case of labeled trees, where bounding either of these two parameters yields tractability. The state of affairs turns out to be quite different for the unlabeled classes. The main (and most challenging to prove) result concerns unlabeled trees: we show NP-hardness, even if the input consists of two trees and both the threshold and the number of desired answers are equal to just two. In other words, we establish that the following problem is NP-complete: given two unlabeled trees, do they have more than one maximal subtree in common?",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2764187464",
    "type": "article"
  },
  {
    "title": "Practical Private Range Search in Depth",
    "doi": "https://doi.org/10.1145/3167971",
    "publication_date": "2018-03-12",
    "publication_year": 2018,
    "authors": "Ioannis Demertzis; Stavros Papadopoulos; Odysseas Papapetrou; Antonios Deligiannakis; Minos Garofalakis; Charalampos Papamanthou",
    "corresponding_authors": "",
    "abstract": "We consider a data owner that outsources its dataset to an untrusted server . The owner wishes to enable the server to answer range queries on a single attribute, without compromising the privacy of the data and the queries. There are several schemes on “practical” private range search (mainly in database venues) that attempt to strike a trade-off between efficiency and security. Nevertheless, these methods either lack provable security guarantees or permit unacceptable privacy leakages. In this article, we take an interdisciplinary approach, which combines the rigor of security formulations and proofs with efficient data management techniques. We construct a wide set of novel schemes with realistic security/performance trade-offs, adopting the notion of Searchable Symmetric Encryption (SSE), primarily proposed for keyword search. We reduce range search to multi-keyword search using range-covering techniques with tree-like indexes, and formalize the problem as Range Searchable Symmetric Encryption (RSSE). We demonstrate that, given any secure SSE scheme, the challenge boils down to (i) formulating leakages that arise from the index structure and (ii) minimizing false positives incurred by some schemes under heavy data skew . We also explain an important concept in the recent SSE bibliography, namely locality , and design generic and specialized ways to attribute locality to our RSSE schemes. Moreover, we are the first to devise secure schemes for answering range aggregate queries, such as range sums and range min/max. We analytically detail the superiority of our proposals over prior work and experimentally confirm their practicality.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2789797077",
    "type": "article"
  },
  {
    "title": "Fast Distributed Transactions and Strongly Consistent Replication for OLTP Database Systems",
    "doi": "https://doi.org/10.1145/2556685",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Alexander Thomson; Thaddeus Diamond; Shu-Chun Weng; Kun Ren; Philip Shao; Daniel J. Abadi",
    "corresponding_authors": "",
    "abstract": "As more data management software is designed for deployment in public and private clouds, or on a cluster of commodity servers, new distributed storage systems increasingly achieve high data access throughput via partitioning and replication. In order to achieve high scalability, however, today's systems generally reduce transactional support, disallowing single transactions from spanning multiple partitions. This article describes Calvin, a practical transaction scheduling and data replication layer that uses a deterministic ordering guarantee to significantly reduce the normally prohibitive contention costs associated with distributed transactions. This allows near-linear scalability on a cluster of commodity machines, without eliminating traditional transactional guarantees, introducing a single point of failure, or requiring application developers to reason about data partitioning. By replicating transaction inputs instead of transactional actions, Calvin is able to support multiple consistency levels—including Paxos-based strong consistency across geographically distant replicas—at no cost to transactional throughput. Furthermore, Calvin introduces a set of tools that will allow application developers to gain the full performance benefit of Calvin's server-side transaction scheduling mechanisms without introducing the additional code complexity and inconvenience normally associated with using DBMS stored procedures in place of ad hoc client-side transactions.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2067055328",
    "type": "article"
  },
  {
    "title": "Declarative Cleaning of Inconsistencies in Information Extraction",
    "doi": "https://doi.org/10.1145/2877202",
    "publication_date": "2016-04-07",
    "publication_year": 2016,
    "authors": "Ronald Fagin; Benny Kimelfeld; Frederick Reiss; Stijn Vansummeren",
    "corresponding_authors": "",
    "abstract": "The population of a predefined relational schema from textual content, commonly known as Information Extraction (IE), is a pervasive task in contemporary computational challenges associated with Big Data. Since the textual content varies widely in nature and structure (from machine logs to informal natural language), it is notoriously difficult to write IE programs that unambiguously extract the sought information. For example, during extraction, an IE program could annotate a substring as both an address and a person name. When this happens, the extracted information is said to be inconsistent , and some way of removing inconsistencies is crucial to compute the final output. Industrial-strength IE systems like GATE and IBM SystemT therefore provide a built-in collection of cleaning operations to remove inconsistencies from extracted relations. These operations, however, are collected in an ad hoc fashion through use cases. Ideally, we would like to allow IE developers to declare their own policies. But existing cleaning operations are defined in an algorithmic way, and hence it is not clear how to extend the built-in operations without requiring low-level coding of internal or external functions. We embark on the establishment of a framework for declarative cleaning of inconsistencies in IE through principles of database theory. Specifically, building upon the formalism of document spanners for IE, we adopt the concept of prioritized repairs , which has been recently proposed as an extension of the traditional database repairs to incorporate priorities among conflicting facts. We show that our framework captures the popular cleaning policies, as well as the POSIX semantics for extraction through regular expressions. We explore the problem of determining whether a cleaning declaration is unambiguous (i.e., always results in a single repair) and whether it increases the expressive power of the extraction language. We give both positive and negative results, some of which are general and some of which apply to policies used in practice.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2319151433",
    "type": "article"
  },
  {
    "title": "Output-Optimal Massively Parallel Algorithms for Similarity Joins",
    "doi": "https://doi.org/10.1145/3311967",
    "publication_date": "2019-04-08",
    "publication_year": 2019,
    "authors": "Xiao Hu; Ke Yi; Yufei Tao",
    "corresponding_authors": "",
    "abstract": "Parallel join algorithms have received much attention in recent years due to the rapid development of massively parallel systems such as MapReduce and Spark. In the database theory community, most efforts have been focused on studying worst-case optimal algorithms. However, the worst-case optimality of these join algorithms relies on the hard instances having very large output sizes. In the case of a two-relation join, the hard instance is just a Cartesian product, with an output size that is quadratic in the input size. In practice, however, the output size is usually much smaller. One recent parallel join algorithm by Beame et al. has achieved output-optimality (i.e., its cost is optimal in terms of both the input size and the output size), but their algorithm only works for a 2-relation equi-join and has some imperfections. In this article, we first improve their algorithm to true optimality. Then we design output-optimal algorithms for a large class of similarity joins. Finally, we present a lower bound, which essentially eliminates the possibility of having output-optimal algorithms for any join on more than two relations.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2938070826",
    "type": "article"
  },
  {
    "title": "Efficient Enumeration Algorithms for Regular Document Spanners",
    "doi": "https://doi.org/10.1145/3351451",
    "publication_date": "2020-02-08",
    "publication_year": 2020,
    "authors": "Fernando Florenzano; Cristian Riveros; Martín Ugarte; Stijn Vansummeren; Domagoj Vrgoč",
    "corresponding_authors": "",
    "abstract": "Regular expressions and automata models with capture variables are core tools in rule-based information extraction. These formalisms, also called regular document spanners , use regular languages to locate the data that a user wants to extract from a text document and then store this data into variables. Since document spanners can easily generate large outputs, it is important to have efficient evaluation algorithms that can generate the extracted data in a quick succession, and with relatively little precomputation time. Toward this goal, we present a practical evaluation algorithm that allows output-linear delay enumeration of a spanner’s result after a precomputation phase that is linear in the document. Although the algorithm assumes that the spanner is specified in a syntactic variant of variable-set automata, we also study how it can be applied when the spanner is specified by general variable-set automata, regex formulas, or spanner algebras. Finally, we study the related problem of counting the number of outputs of a document spanner and provide a fine-grained analysis of the classes of document spanners that support efficient enumeration of their results.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3013397275",
    "type": "article"
  },
  {
    "title": "Maintaining Triangle Queries under Updates",
    "doi": "https://doi.org/10.1145/3396375",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ahmet Kara; Hung Q. Ngo; Miloš Nikolić; Dan Olteanu; Haozhe Zhang",
    "corresponding_authors": "",
    "abstract": "We consider the problem of incrementally maintaining the triangle queries with arbitrary free variables under single-tuple updates to the input relations. We introduce an approach called IVMϵ that exhibits a trade-off between the update time, the space, and the delay for the enumeration of the query result, such that the update time ranges from the square root to linear in the database size while the delay ranges from constant to linear time. IVMϵ achieves Pareto worst-case optimality in the update-delay space conditioned on the Online Matrix-Vector Multiplication conjecture. It is strongly Pareto optimal for the triangle queries with no or three free variables and weakly Pareto optimal for the remaining triangle queries with one or two free variables. IVMϵ recovers prior work such as the suboptimal classical view maintenance approach that uses delta query processing and the worst-case optimal approach that computes all triangles in a static database.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3024315987",
    "type": "article"
  },
  {
    "title": "Incremental Graph Computations: Doable and Undoable",
    "doi": "https://doi.org/10.1145/3500930",
    "publication_date": "2022-03-10",
    "publication_year": 2022,
    "authors": "Wenfei Fan; Chao Tian",
    "corresponding_authors": "",
    "abstract": "The incremental problem for a class \\( {\\mathcal {Q}} \\) of graph queries aims to compute, given a query \\( Q \\in {\\mathcal {Q}} \\) , graph G , answers Q ( G ) to Q in G and updates ΔG to G as input, changes ΔO to output Q ( G ) such that Q ( G ⊕ ΔG ) = Q ( G )⊕ ΔO . It is called bounded if its cost can be expressed as a polynomial function in the sizes of Q , ΔG and ΔO , which reduces the computations on possibly big G to small ΔG and ΔO . No matter how desirable, however, our first results are negative: For common graph queries such as traversal, connectivity, keyword search, pattern matching, and maximum cardinality matching, their incremental problems are unbounded. In light of the negative results, we propose two characterizations for the effectiveness of incremental graph computation: (a) localizable , if its cost is decided by small neighbors of nodes in ΔG instead of the entire G ; and (b) bounded relative to a batch graph algorithm \\( {\\mathcal {T}} \\) , if the cost is determined by the sizes of ΔG and changes to the affected area that is necessarily checked by any algorithms that incrementalize \\( {\\mathcal {T}} \\) . We show that the incremental computations above are either localizable or relatively bounded by providing corresponding incremental algorithms. That is, we can either reduce the incremental computations on big graphs to small data, or incrementalize existing batch graph algorithms by minimizing unnecessary recomputation. Using real-life and synthetic data, we experimentally verify the effectiveness of our incremental algorithms.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4220699347",
    "type": "article"
  },
  {
    "title": "Tractable Orders for Direct Access to Ranked Answers of Conjunctive Queries",
    "doi": "https://doi.org/10.1145/3578517",
    "publication_date": "2023-01-02",
    "publication_year": 2023,
    "authors": "Nofar Carmeli; Nikolaos Tziavelis; Wolfgang Gatterbauer; Benny Kimelfeld; Mirek Riedewald",
    "corresponding_authors": "",
    "abstract": "We study the question of when we can provide direct access to the k-th answer to a Conjunctive Query (CQ) according to a specified order over the answers in time logarithmic in the size of the database, following a preprocessing step that constructs a data structure in time quasilinear in database size. Specifically, we embark on the challenge of identifying the tractable answer orderings , that is, those orders that allow for such complexity guarantees. To better understand the computational challenge at hand, we also investigate the more modest task of providing access to only a single answer (i.e., finding the answer at a given position), a task that we refer to as the selection problem , and ask when it can be performed in quasilinear time. We also explore the question of when selection is indeed easier than ranked direct access. We begin with lexicographic orders . For each of the two problems, we give a decidable characterization (under conventional complexity assumptions) of the class of tractable lexicographic orders for every CQ without self-joins. We then continue to the more general orders by the sum of attribute weights and establish the corresponding decidable characterizations, for each of the two problems, of the tractable CQs without self-joins. Finally, we explore the question of when the satisfaction of Functional Dependencies (FDs) can be utilized for tractability and establish the corresponding generalizations of our characterizations for every set of unary FDs.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4313476522",
    "type": "article"
  },
  {
    "title": "The Ring: Worst-case Optimal Joins in Graph Databases using (Almost) No Extra Space",
    "doi": "https://doi.org/10.1145/3644824",
    "publication_date": "2024-02-08",
    "publication_year": 2024,
    "authors": "Diego Arroyuelo; Adrián Gómez‐Brandón; Aidan Hogan; Gonzalo Navarro; Juan L. Reutter; Javiel Rojas-Ledesma; Adrián Soto",
    "corresponding_authors": "",
    "abstract": "We present an indexing scheme for triple-based graphs that supports join queries in worst-case optimal (wco) time within compact space. This scheme, called a ring , regards each triple as a cyclic string of length 3. Each rotation of the triples is lexicographically sorted and the values of the last attribute are stored as a column, so we obtain the order of the next column by stably re-sorting the triples by its attribute. We show that, by representing the columns with a compact data structure called a wavelet tree, this ordering enables forward and backward navigation between columns without needing pointers. These wavelet trees further support wco join algorithms and cardinality estimations for query planning. While traditional data structures such as B-Trees, tries, and so on, require 6 index orders to support all possible wco joins over triples, we can use one ring to index them all. This ring replaces the graph and uses only sublinear extra space, thus supporting wco joins in almost no space beyond storing the graph itself. Experiments querying a large graph (Wikidata) in memory show that the ring offers nearly the best overall query times while using only a small fraction of the space required by several state-of-the-art approaches. We then turn our attention to some theoretical results for indexing tables of arity d higher than 3 in such a way that supports wco joins. While a single ring of length d no longer suffices to cover all d ! orders, we need much fewer rings to index them all: O (2 d ) rings with a small constant. For example, we need 5 rings instead of 120 orders for d =5. We show that our rings become a particular case of what we dub order graphs , whose nodes are attribute orders and where stably sorting by some attribute leads us from an order to another, thereby inducing an edge labeled by the attribute. The index is then the set of columns associated with the edges, and a set of rings is just one possible graph shape. We show that other shapes, like for example a single ring instead of several ones of length d , can lead us to even smaller indexes, and that other more general shapes are also possible. For example, we handle d =5 attributes within space equivalent to 4 rings.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391653333",
    "type": "article"
  },
  {
    "title": "An extension of the relational data model to incorporate ordered domains",
    "doi": "https://doi.org/10.1145/502030.502033",
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Wilfred Ng",
    "corresponding_authors": "Wilfred Ng",
    "abstract": "We extend the relational data model to incorporate partial orderings into data domains, which we call the ordered relational model. Within the extended model, we define the partially ordered relational algebra (the PORA) by allowing the ordering predicate ⊑ to be used in formulae of the selection operator (σ). The PORA expresses exactly the set of all possible relations that are invariant under order-preserving automorphism of databases. This result characterizes the expressiveness of the PORA and justifies the development of Ordered SQL (OSQL) as a query language for ordered databases. OSQL provides users with the capability of capturing the semantics of ordered data in many advanced applications, such as those having temporal or incomplete information. Ordered functional dependencies (OFDs) on ordered databases are studied, based on two possible extensions of domain orderings: pointwise ordering and lexicographical ordering. We present a sound and complete axiom system for OFDs in the first case and establish a set of sound and complete chase rules for OFDs in the second. Our results suggest that the implication problems for both cases of OFDs are decidable and that the enforcement of OFDs in ordered relations are practically feasible. In a wider perspective, the proposed model explores an important area of object-relational databases, since ordered domains can be viewed as a general kind of data type.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2006637140",
    "type": "article"
  },
  {
    "title": "Optimal disk allocation for partial match queries",
    "doi": "https://doi.org/10.1145/151284.151288",
    "publication_date": "1993-03-01",
    "publication_year": 1993,
    "authors": "Khaled Abdel-Ghaffar; Amr El Abbadi",
    "corresponding_authors": "",
    "abstract": "The problem of disk allocation addresses the issue of how to distribute a file on several disks in order to maximize concurrent disk accesses in response to a partial match query. In this paper a coding-theoretic analysis of this problem is presented, and both necessary and sufficient conditions for the existence of strictly optimal allocation methods are provided. Based on a class of optimal codes, known as maximum distance separable codes, strictly optimal allocation methods are constructed. Using the necessary conditions proved, we argue that the standard definition of strict optimality is too strong and cannot be attained, in general. Hence, we reconsider the definition of optimality. Instead of basing it on an abstract definition that may not be attainable, we propose a new definition based on the best possible allocation method. Using coding theory, allocation methods that are optimal according to our proposed criterion are developed.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1981889860",
    "type": "article"
  },
  {
    "title": "Essential classification rule sets",
    "doi": "https://doi.org/10.1145/1042046.1042048",
    "publication_date": "2004-12-12",
    "publication_year": 2004,
    "authors": "Elena Baralis; Silvia Chiusano",
    "corresponding_authors": "",
    "abstract": "Given a class model built from a dataset including labeled data, classification assigns a new data object to the appropriate class. In associative classification the class model (i.e., the classifier) is a set of association rules. Associative classification is a promising technique for the generation of highly accurate classifiers. In this article, we present a compact form which encodes without information loss the classification knowledge available in a classification rule set. This form includes the rules that are essential for classification purposes, and thus it can replace the complete rule set. The proposed form is particularly effective in dense datasets, where traditional extraction techniques may generate huge rule sets. The reduction in size of the rule set allows decreasing the complexity of both the rule generation step and the rule pruning step. Hence, classification rule extraction can be performed also with low support, in order to extract more, possibly useful, rules.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2068881536",
    "type": "article"
  },
  {
    "title": "Simple conditions for guaranteeing higher normal forms in relational databases",
    "doi": "https://doi.org/10.1145/132271.132274",
    "publication_date": "1992-09-01",
    "publication_year": 1992,
    "authors": "C. J. Date; Ronald Fagin",
    "corresponding_authors": "",
    "abstract": "A key is simple if it consists of a single attribute. It is shown that if a relation schema is in third normal form and every key is simple, then it is in projection-join normal form (sometimes called fifth normal form), the ultimate normal form with respect to projections and joins. Furthermore, it is shown that if a relation schema is in Boyce-Codd normal form and some key is simple, then it is in fourth normal form (but not necessarily projection-join normal form). These results give the database designer simple sufficient conditions, defined in terms of functional dependencies alone, that guarantee that the schema being designed is automatically in higher normal forms.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2172013957",
    "type": "article"
  },
  {
    "title": "A model-based approach to updating databases with incomplete information",
    "doi": "https://doi.org/10.1145/42338.42386",
    "publication_date": "1988-06-01",
    "publication_year": 1988,
    "authors": "Marianne Winslett",
    "corresponding_authors": "Marianne Winslett",
    "abstract": "Suppose one wishes to construct, use, and maintain a database of facts about the real world, even though the state of that world is only partially known. In the artificial intelligence domain, this problem arises when an agent has a base set of beliefs that reflect partial knowledge about the world, and then tries to incorporate new, possibly contradictory knowledge into this set of beliefs. In the database domain, one facet of this situation is the well-known null values problem. We choose to represent such a database as a logical theory, and view the models of the theory as representing possible states of the world that are consistent with all known information. How can new information be incorporated into the database? For example, given the new information that “ b or c is true,” how can one get rid of all outdated information about b and c , add the new information, and yet in the process not disturb any other information in the database? In current-day database management systems, the difficult and tedious burden of determining exactly what to add and remove from the database is placed on the user. The goal of our research was to relieve users of that burden, by equipping the database management system with update algorithms that can automatically determine what to add and remove from the database. Under our approach, new information about the state of the world is input to the database management system as a well-formed formula that the state of the world is now known to satisfy. We have constructed database update algorithms to interpret this update formula and incorporate the new information represented by the formula into the database without further assistance from the user. In this paper we show how to embed the incomplete database and the incoming information in the language of mathematical logic, explain the semantics of our update operators, and discuss the algorithms that implement these operators.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W1976302723",
    "type": "article"
  },
  {
    "title": "Using annotations to support multiple kinds of versioning in an object-oriented database system",
    "doi": "https://doi.org/10.1145/111197.111205",
    "publication_date": "1991-09-01",
    "publication_year": 1991,
    "authors": "Edward Sciore",
    "corresponding_authors": "Edward Sciore",
    "abstract": "The concept of annotation from object-oriented languages is adapted to object-oriented databases. It is shown how annotations can be used to model activities such as constraint checking, default values, and triggers. Annotations also are an appropriate way to model different versioning concepts. This paper discusses three kinds of versioning—histories, revisions, and alternatives—and demonstrates how each one can be modeled effectively using annotations. The use of annotations also allows other kinds of versioning to be defined extensibly, and arbitrary combinations of versions can be handled easily.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2031285359",
    "type": "article"
  },
  {
    "title": "Set query optimization in distributed database systems",
    "doi": "https://doi.org/10.1145/6314.6488",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Bezalel Gavish; Arie Segev",
    "corresponding_authors": "",
    "abstract": "This paper addresses the problem of optimizing queries that involve set operations (set queries) in a distributed relational database system. A particular emphasis is put on the optimization of such queries in horizontally partitioned database systems. A mathematical programming model of the set query problem is developed and its NP-completeness is proved. Solution procedures are proposed and computational results presented. One of the main results of the computational experiments is that, for many queries, the solution procedures are not sensitive to errors in estimating the size of results of set operations.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2053923570",
    "type": "article"
  },
  {
    "title": "An integrated approach to logical design of relational database schemes",
    "doi": "https://doi.org/10.1145/5922.214291",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Catriel Beeri; Michael Kifer",
    "corresponding_authors": "",
    "abstract": "We propose a new approach to the design of relational database schemes. The main features of the approach are the following: A combination of the traditional decomposition and synthesis approaches, thus allowing the use of both functional and multivalued dependencies. Separation of structural dependencies relevant for the design process from integrity constraints, that is, constraints that do not bear any structural information about the data and which should therefore be discarded at the design stage. This separation is supported by a simple syntactic test filtering out nonstructural dependencies. Automatic correction of schemes which lack certain desirable properties.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W1995096548",
    "type": "article"
  },
  {
    "title": "Adaptive pull-based policies for wide area data delivery",
    "doi": "https://doi.org/10.1145/1138394.1138399",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Laura F. Bright; Avigdor Gal; Louiqa Raschid",
    "corresponding_authors": "",
    "abstract": "Wide area data delivery requires timely propagation of up-to-date information to thousands of clients over a wide area network. Applications include web caching, RSS source monitoring, and email access via a mobile network. Data sources vary widely in their update patterns and may experience different update rates at different times or unexpected changes to update patterns. Traditional data delivery solutions are either push-based, which requires servers to push updates to clients, or pull-based, which require clients to check for updates at servers. While push-based solutions ensure timely data delivery, they are not always feasible to implement and may not scale to a large number of clients. In this article, we present adaptive pull-based policies that explicitly aim to reduce the overhead of contacting remote servers, compared to existing pull-based policies, while meeting freshness requirements. We model updates to data sources using update histories, and present two novel history-based policies to estimate when updates occur; they are based on individual history and aggregate history. These policies are presented within an architectural framework that supports their deployment either client-side or server-side. We further develop two adaptive policies to handle objects that initially may have insufficient history or objects that experience changes in update patterns. Extensive experimental evaluation using three data traces from diverse applications shows that history-based policies can reduce contact between clients and servers by up to 60% compared to existing pull-based policies while providing a comparable level of data freshness. Our experiments further demonstrate that our adaptive policies can select the best policy to match the behavior of an object and perform better than any individual policy, thus they dominate standalone policies.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2034778995",
    "type": "article"
  },
  {
    "title": "Order-preserving key transformations",
    "doi": "https://doi.org/10.1145/5922.5923",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Anil Garg; C. C. Gotlieb",
    "corresponding_authors": "",
    "abstract": "File organizations based on conventional hash functions provide faster access to the stored records in comparison with tree-like file structures. Tree structures such as B + -trees and ISAM do provide for sequential processing, but require considerable storage for the indices. When sequential processing is needed a table that performs an order-preserving transformation on keys can be used. H is an order-preserving key transform if H(K 1 ) ⩾ H(K 2 ), for all keys K 1 &gt; K 2 . We present methodologies for constructing such key transforms, and illustrate them for some real-life key sets. Storage requirements for the table needed to carry out the transformation are less than those needed for the indices.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2059944263",
    "type": "article"
  },
  {
    "title": "Analysis of retrieval performance for records and objects using optical disk technology",
    "doi": "https://doi.org/10.1145/22952.23015",
    "publication_date": "1987-06-01",
    "publication_year": 1987,
    "authors": "Stavros Christodoulakis",
    "corresponding_authors": "Stavros Christodoulakis",
    "abstract": "In this paper we examine the problem of object and record retrieval from optical disks. General objects (such as images, documents, etc.) may be long and their length may have high variance. We assume that all the components of an object are stored consecutively in storage to speed-up retrieval performance. We first present an optical disk model and an optimal schedule for retrieval of records and objects which qualify in a single query on a file stored on an optical disk device. We then provide exact and approximate analytic results for evaluating the retrieval performance for objects from an optical disk. The analysis provides some basic analytic tools for studying the performance of various file and database organizations for optical disks. The results involve probability distribution of block accesses, probability distributions of span accesses, and probability distribution of seek times. Record retrieval is an important special case. This analysis differs from similar ones in database environments in the following respects: (1) the large size and large variance of the size of objects; (2) crossing of track boundaries by objects; (3) the capability for span access that optical disks provide (e.g., when the optical assembly is located in a given position, information can be read from a number of consecutive tracks (span) with a small additional cost).",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1972228977",
    "type": "article"
  },
  {
    "title": "ELFS: English language from SQL",
    "doi": "https://doi.org/10.1145/7239.384276",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "Wo-Shun Luk; S. Kloster",
    "corresponding_authors": "",
    "abstract": "In this paper we describe a system which, given a query in SQL-like relational database language, will display its meaning in clear, unambiguous natural language. The syntax-driven translation mechanism is independent of the application domain. It has direct applications in designing computer-based SQL tutorial systems and program debugging systems. The research results obtained in the paper will also be useful in query optimization and design of a more user-friendly language front-end for casual users.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1964555030",
    "type": "article"
  },
  {
    "title": "On the complexity of nonrecursive XQuery and functional query languages on complex values",
    "doi": "https://doi.org/10.1145/1189769.1189771",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Christoph Koch",
    "corresponding_authors": "Christoph Koch",
    "abstract": "This article studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery. We show that monad algebra, with equality restricted to atomic values, is complete for the class TA[2 O ( n ) , O ( n )] of problems solvable in linear exponential time with a linear number of alternations if the query is assumed to be part of the input. The monotone fragment of monad algebra with atomic value equality but without negation is NEXPTIME-complete. For monad algebra with deep value equality, that is, equality of complex values, we establish TA[2 O ( n ) , O ( n )] lower and exponential-space upper bounds. We also study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with “child” as the only axis) is exhibited. The two languages are shown expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra with respect to query and combined complexity. As Core XQuery is NEXPTIME-hard, the best-known techniques for processing such problems require exponential amounts of working memory and doubly exponential time in the worst case. We present a property of queries---the lack of a certain form of composition---that virtually all real-world XQueries have and that allows for query evaluation in PSPACE and thus singly exponential time. Still, we are able to show for an important special case---Core XQuery with equality testing restricted to atomic values---that the composition-free language is just as expressive as the language with composition. Thus, under widely-held complexity-theoretic assumptions, the language with composition is an exponentially more succinct version of the composition-free language.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2000487974",
    "type": "article"
  },
  {
    "title": "Incomplete information costs and database design",
    "doi": "https://doi.org/10.1145/5922.5678",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Haim Mendelson; Aditya Saharia",
    "corresponding_authors": "",
    "abstract": "This paper presents a methodology for trading-off the cost of incomplete information against the data-related costs in the design of database systems. It investigates how the usage patterns of the database, defined by the characteristics of information requests presented to it, affect its conceptual design. The construction of minimum-cost answers to information requests for a variety of query types and cost structures is also studied. The resulting costs of incomplete database information are balanced against the data-related costs in the derivation of the optimal design.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1974069745",
    "type": "article"
  },
  {
    "title": "Operational characteristics of a harware-based pattern matcher",
    "doi": "https://doi.org/10.1145/319830.319832",
    "publication_date": "1983-03-01",
    "publication_year": 1983,
    "authors": "Roger Haskin; Lee A. Hollaar",
    "corresponding_authors": "",
    "abstract": "The design and operation of a new class of hardware-based pattern matchers, such as would be used in a backended database processor in a full-text or other retrieval system, is presented. This recognizer is based on a unique implementation technique for finite state automata consisting of partitioning the state table among a number of simple digital machines. It avoids the problems generally associated with implementing finite state machines, such as large state table memories, complex control mechanisms, and state encodings. Because it consists primarily of memory, with its high regularity and density, needs only limited static interconnections, and operates at a relatively low speed, it can be easily constructed using integrated circuit techniques. After a brief discussion of other pattern-matching hardware, the structure and operation of the partitioned finite state automaton is given, along with a simplified discussion of how the state tables are partitioned. The expected performance of the resulting system and the state table partitioning programs is then discussed.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2027632253",
    "type": "article"
  },
  {
    "title": "Linear hashing with overflow-handling by linear probing",
    "doi": "https://doi.org/10.1145/3148.3324",
    "publication_date": "1985-03-01",
    "publication_year": 1985,
    "authors": "Per-Åke Larson",
    "corresponding_authors": "Per-Åke Larson",
    "abstract": "Linear hashing is a file structure for dynamic files. In this paper, a new, simple method for handling overflow records in connection with linear hashing is proposed. The method is based on linear probing and does not rely on chaining. No dedicated overflow area is required. The expansion sequence of liner hashing is modified to improve the performance, which requires changes in the address computation. A new address computation algorithm and an expansion algorithm are given. The performance of the method is studied by simulation. The algorithms for the basic file operations are very simple, and the overall performance is competitive with that of other variants of linear hashing.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2148227902",
    "type": "article"
  },
  {
    "title": "On the estimation of the number of desired records with respect to a given query",
    "doi": "https://doi.org/10.1145/320241.320245",
    "publication_date": "1978-03-01",
    "publication_year": 1978,
    "authors": "C. Yu; Wo-Shun Luk; Man‐Keung Siu",
    "corresponding_authors": "",
    "abstract": "The importance of the estimation of the number of desired records for a given query is outlined. Two algorithms for the estimation in the “closest neighbors problem” are presented. The numbers of operations of the algorithms are Ο ( ml 2 ) and Ο ( ml ), where m is the number of clusters and l is the “length” of the query.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2001703546",
    "type": "article"
  },
  {
    "title": "Optimum reorganization points for linearly growing files",
    "doi": "https://doi.org/10.1145/320241.320244",
    "publication_date": "1978-03-01",
    "publication_year": 1978,
    "authors": "William G. Tuel",
    "corresponding_authors": "William G. Tuel",
    "abstract": "The problem of finding optimal reorganization intervals for linearly growing files is solved. An approximate reorganization policy, independent of file lifetime, is obtained. Both the optimum and approximate policies are compared to previously published results using a numerical example.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2004698863",
    "type": "article"
  },
  {
    "title": "Performance of a database manager in a virtual memory system",
    "doi": "https://doi.org/10.1145/320493.320494",
    "publication_date": "1976-12-01",
    "publication_year": 1976,
    "authors": "Stephen W. Sherman; Richard Brice",
    "corresponding_authors": "",
    "abstract": "Buffer space is created and managed in database systems in order to reduce accesses to the I/O devices for database information. In systems using virtual memory any increase in the buffer space may be accompanied by an increase in paging. The effects of these factors on system performance are quantified where system performance is a function of page faults and database accesses to I/O devices. This phenomenon is examined through the analysis of empirical data gathered in a multifactor experiment. The factors considered are memory size, size of buffer space, memory replacement algorithm, and buffer management algorithm. The improvement of system performance through an increase in the size of the buffer space is demonstrated. It is also shown that for certain values of the other factors an increase in the size of the buffer space can cause performance to deteriorate.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2005959865",
    "type": "article"
  },
  {
    "title": "Extended wavelets for multiple measures",
    "doi": "https://doi.org/10.1145/1242524.1242527",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Antonios Deligiannakis; Minos Garofalakis; Nick Roussopoulos",
    "corresponding_authors": "",
    "abstract": "Several studies have demonstrated the effectiveness of the Haar wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate answers to user queries. Although originally designed for minimizing the overall mean-squared (i.e., L 2 -norm) error in the data approximation, recently proposed methods also enable the use of Haar wavelets in minimizing other error metrics, such as the relative error in data value reconstruction, which is arguably the most important for approximate query answers. Relatively little attention, however, has been paid to the problem of using wavelet synopses as an approximate query answering tool over complex tabular datasets containing multiple measures , such as those typically found in real-life OLAP applications. Existing decomposition approaches will either operate on each measure individually, or treat all measures as a vector of values and process them simultaneously. As we demonstrate in this article, these existing individual or combined storage approaches for the wavelet coefficients of different measures can easily lead to suboptimal storage utilization, resulting in drastically reduced accuracy for approximate query answers. To address this problem, in this work, we introduce the notion of an extended wavelet coefficient as a flexible, efficient storage method for wavelet coefficients over multimeasure data. We also propose novel algorithms for constructing effective (optimal or near-optimal) extended wavelet-coefficient synopses under a given storage constraint, for both sum-squared error and relative-error norms. Experimental results with both real-life and synthetic datasets validate our approach, demonstrating that our techniques consistently obtain significant gains in approximation accuracy compared to existing solutions.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2064383543",
    "type": "article"
  },
  {
    "title": "On computing temporal aggregates with range predicates",
    "doi": "https://doi.org/10.1145/1366102.1366109",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Donghui Zhang; Alexander Markowetz; Vassilis J. Tsotras; Dimitrios Gunopulos; Bernhard Seeger",
    "corresponding_authors": "",
    "abstract": "Computing temporal aggregates is an important but costly operation for applications that maintain time-evolving data (data warehouses, temporal databases, etc.) Due to the large volume of such data, performance improvements for temporal aggregate queries are critical. Previous approaches have aggregate predicates that involve only the time dimension. In this article we examine techniques to compute temporal aggregates that include key-range predicates as well ( range-temporal aggregates ). In particular we concentrate on the SUM aggregate, while COUNT is a special case. To handle arbitrary key ranges, previous methods would need to keep a separate index for every possible key range. We propose an approach based on a new index structure called the Multiversion SB-Tree , which incorporates features from both the SB-Tree and the Multiversion B+--tree, to handle arbitrary key-range temporal aggregate queries. We analyze the performance of our approach and present experimental results that show its efficiency. Furthermore, we address a novel and practical variation called functional range-temporal aggregates. Here, the value of any record is a function over time. The meaning of aggregates is altered such that the contribution of a record to the aggregate result is proportional to the size of the intersection between the record's time interval and the query time interval. Both analytical and experimental results show the efficiency of our result.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2022734059",
    "type": "article"
  },
  {
    "title": "Process synchronization in database systems",
    "doi": "https://doi.org/10.1145/320263.320279",
    "publication_date": "1978-09-01",
    "publication_year": 1978,
    "authors": "Günter Schlageter",
    "corresponding_authors": "Günter Schlageter",
    "abstract": "The problem of process synchronization in database systems is analyzed in a strictly systematic way, on a rather abstract level; the abstraction is chosen such that the essential characteristics of the problem can be distinctly modeled and investigated. Using a small set of concepts, a consistent description of the whole problem is developed; many widely used, but only vaguely defined, notions are defined exactly within this framework. The abstract treatment of the problem immediately leads to practically useful insights with respect to possible solutions, although implementational aspects are not discussed in detail.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2078299136",
    "type": "article"
  },
  {
    "title": "Towards a query optimizer for text-centric tasks",
    "doi": "https://doi.org/10.1145/1292609.1292611",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Panagiotis G. Ipeirotis; Eugene Agichtein; Pranay Jain; Luis Gravano",
    "corresponding_authors": "",
    "abstract": "Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the Web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or “crawl,” the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output “completeness” (e.g., in terms of recall). Nevertheless, this choice is typically ad hoc and based on heuristics or plain intuition. In this article, we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed, cost-based way. Towards this goal, we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. We adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. Our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. We identify these properties and present efficient techniques for estimating the associated parameters of the cost model. We also present two optimization approaches for text-centric tasks that rely on the cost-model parameters and select efficient execution plans. Overall, our optimization approaches help build efficient execution plans for a task, resulting in significant efficiency and output completeness benefits. We complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2155737120",
    "type": "article"
  },
  {
    "title": "Estimating statistical aggregates on probabilistic data streams",
    "doi": "https://doi.org/10.1145/1412331.1412338",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "T. S. Jayram; Andrew McGregor; S. Muthukrishnan; Erik Vee",
    "corresponding_authors": "",
    "abstract": "The probabilistic stream model was introduced by Jayram et al. [2007]. It is a generalization of the data stream model that is suited to handling probabilistic data, where each item of the stream represents a probability distribution over a set of possible events. Therefore, a probabilistic stream determines a distribution over a potentially exponential number of classical deterministic streams, where each item is deterministically one of the domain values. We present algorithms for computing commonly used aggregates on a probabilistic stream. We present the first one pass streaming algorithms for estimating the expected mean of a probabilistic stream. Next, we consider the problem of estimating frequency moments for probabilistic data. We propose a general approach to obtain unbiased estimators working over probabilistic data by utilizing unbiased estimators designed for standard streams. Applying this approach, we extend a classical data stream algorithm to obtain a one-pass algorithm for estimating F 2 , the second frequency moment. We present the first known streaming algorithms for estimating F 0 , the number of distinct items on probabilistic streams. Our work also gives an efficient one-pass algorithm for estimating the median, and a two-pass algorithm for estimating the range.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2615341736",
    "type": "article"
  },
  {
    "title": "Cache-oblivious databases",
    "doi": "https://doi.org/10.1145/1366102.1366105",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Bingsheng He; Qiong Luo",
    "corresponding_authors": "",
    "abstract": "Cache-oblivious techniques, proposed in the theory community, have optimal asymptotic bounds on the amount of data transferred between any two adjacent levels of an arbitrary memory hierarchy. Moreover, this optimal performance is achieved without any hardware platform specific tuning. These properties are highly attractive to autonomous databases, especially because the hardware architectures are becoming increasingly complex and diverse. In this article, we present our design, implementation, and evaluation of the first cache-oblivious in-memory query processor, EaseDB. Moreover, we discuss the inherent limitations of the cache-oblivious approach as well as the opportunities given by the upcoming hardware architectures. Specifically, a cache-oblivious technique usually requires sophisticated algorithm design to achieve a comparable performance to its cache-conscious counterpart. Nevertheless, this development-time effort is compensated by the automaticity of performance achievement and the reduced ownership cost. Furthermore, this automaticity enables cache-oblivious techniques to outperform their cache-conscious counterparts in multi-threading processors.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1996466054",
    "type": "article"
  },
  {
    "title": "Correlated pattern mining in quantitative databases",
    "doi": "https://doi.org/10.1145/1386118.1386120",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Yiping Ke; James Cheng; Wilfred Ng",
    "corresponding_authors": "",
    "abstract": "We study mining correlations from quantitative databases and show that this is a more effective approach than mining associations to discover useful patterns. We propose the novel notion of quantitative correlated pattern (QCP), which is founded on two formal concepts, mutual information and all-confidence. We first devise a normalization on mutual information and apply it to the problem of QCP mining to capture the dependency between the attributes. We further adopt all-confidence as a quality measure to ensure, at a finer granularity, the dependency between the attributes with specific quantitative intervals. We also propose an effective supervised method that combines the consecutive intervals of the quantitative attributes based on mutual information, such that the interval-combining is guided by the dependency between the attributes. We develop an algorithm, QCoMine , to mine QCPs efficiently by utilizing normalized mutual information and all-confidence to perform bilevel pruning. We also identify the redundancy existing in the set of QCPs and propose effective techniques to eliminate the redundancy. Our extensive experiments on both real and synthetic datasets verify the efficiency of QCoMine and the quality of the QCPs. The experimental results also justify the effectiveness of our proposed techniques for redundancy elimination. To further demonstrate the usefulness and the quality of QCPs, we study an application of QCPs to classification. We demonstrate that the classifier built on the QCPs achieves higher classification accuracy than the state-of-the-art classifiers built on association rules.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2160360175",
    "type": "article"
  },
  {
    "title": "Personalizing queries based on networks of composite preferences",
    "doi": "https://doi.org/10.1145/1735886.1735892",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Georgia Koutrika; Yannis Ioannidis",
    "corresponding_authors": "",
    "abstract": "People's preferences are expressed at varying levels of granularity and detail as a result of partial or imperfect knowledge. One may have some preference for a general class of entities, for example, liking comedies, and another one for a fine-grained, specific class, such as disliking recent thrillers with Al Pacino. In this article, we are interested in capturing such complex, multi-granular preferences for personalizing database queries and in studying their impact on query results. We organize the collection of one's preferences in a preference network (a directed acyclic graph), where each node refers to a subclass of the entities that its parent refers to, and whenever they both apply, more specific preferences override more generic ones. We study query personalization based on networks of preferences and provide efficient algorithms for identifying relevant preferences, modifying queries accordingly, and processing personalized queries. Finally, we present results of both synthetic and real-user experiments, which: (a) demonstrate the efficiency of our algorithms, (b) provide insight as to the appropriateness of the proposed preference model, and (c) show the benefits of query personalization based on composite preferences compared to simpler preference representations.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2070475178",
    "type": "article"
  },
  {
    "title": "A quality-aware optimizer for information extraction",
    "doi": "https://doi.org/10.1145/1508857.1508862",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Alpa Jain; Panagiotis G. Ipeirotis",
    "corresponding_authors": "",
    "abstract": "A large amount of structured information is buried in unstructured text. Information extraction systems can extract structured relations from the documents and enable sophisticated, SQL-like queries over unstructured text. Information extraction systems are not perfect and their output has imperfect precision and recall (i.e., contains spurious tuples and misses good tuples). Typically, an extraction system has a set of parameters that can be used as “knobs” to tune the system to be either precision- or recall-oriented. Furthermore, the choice of documents processed by the extraction system also affects the quality of the extracted relation. So far, estimating the output quality of an information extraction task has been an ad hoc procedure, based mainly on heuristics. In this article, we show how to use Receiver Operating Characteristic (ROC) curves to estimate the extraction quality in a statistically robust way and show how to use ROC analysis to select the extraction parameters in a principled manner. Furthermore, we present analytic models that reveal how different document retrieval strategies affect the quality of the extracted relation. Finally, we present our maximum likelihood approach for estimating, on the fly, the parameters required by our analytic models to predict the runtime and the output quality of each execution plan. Our experimental evaluation demonstrates that our optimization approach predicts accurately the output quality and selects the fastest execution plan that satisfies the output quality restrictions.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2090765848",
    "type": "article"
  },
  {
    "title": "Continuous nearest-neighbor search in the presence of obstacles",
    "doi": "https://doi.org/10.1145/1966385.1966387",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Yunjun Gao; Baihua Zheng; Gang Chen; Chun Chen; Qing Li",
    "corresponding_authors": "",
    "abstract": "Despite the ubiquity of physical obstacles (e.g., buildings, hills, and blindages, etc.) in the real world, most of spatial queries ignore the obstacles. In this article, we study a novel form of continuous nearest-neighbor queries in the presence of obstacles, namely continuous obstructed nearest-neighbor (CONN) search, which considers the impact of obstacles on the distance between objects. Given a data set P , an obstacle set O , and a query line segment q , in a two-dimensional space, a CONN query retrieves the nearest neighbor p ∈ P of each point p′ on q according to the obstructed distance, the shortest path between p and p ′ without crossing any obstacle in O . We formalize CONN search, analyze its unique properties, and develop algorithms for exact CONN query-processing assuming that both P and O are indexed by conventional data-partitioning indices (e.g., R-trees). Our methods tackle CONN retrieval by performing a single query for the entire query line segment, and only process the data points and obstacles relevant to the final query result via a novel concept of control points and an efficient quadratic-based split point computation approach. Then, we extend our techniques to handle variations of CONN queries, including (1) continuous obstructed k nearest neighbor (CO k NN) search which, based on obstructed distances, finds the k (≥ 1) nearest neighbors (NNs) to every point along q ; and (2) trajectory obstructed k nearest-neighbor (TO k NN) search, which, according to obstructed distances, returns the k NNs for each point on an arbitrary trajectory (consisting of several consecutive line segments). Finally, we explore approximate CO k NN (ACO k NN) retrieval. Extensive experiments with both real and synthetic datasets demonstrate the efficiency and effectiveness of our proposed algorithms under various experimental settings.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2113587921",
    "type": "article"
  },
  {
    "title": "Stochastic skylines",
    "doi": "https://doi.org/10.1145/2188349.2188356",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Wenjie Zhang; Xuemin Lin; Ying Zhang; Muhammad Aamir Cheema; Qing Zhang",
    "corresponding_authors": "",
    "abstract": "In many applications involving multiple criteria optimal decision making, users may often want to make a personal trade-off among all optimal solutions for selecting one object that fits best their personal needs. As a key feature, the skyline in a multidimensional space provides the minimum set of candidates for such purposes by removing all points not preferred by any (monotonic) utility/scoring functions; that is, the skyline removes all objects not preferred by any user no matter how their preferences vary. Driven by many recent applications with uncertain data, the probabilistic skyline model is proposed to retrieve uncertain objects based on skyline probabilities. Nevertheless, skyline probabilities cannot capture the preferences of monotonic utility functions. Motivated by this, in this article we propose a novel skyline operator, namely stochastic skylines. In the light of the expected utility principle, stochastic skylines guarantee to provide the minimum set of candidates to optimal solutions over a family of utility functions. We first propose the lskyline operator based on the lower orthant orders . lskyline guarantees to provide the minimum set of candidates to the optimal solutions for the family of monotonic multiplicative utility functions. While lskyline works very effectively for the family of multiplicative functions, it may miss optimal solutions for other utility /scoring functions (e.g., linear functions). To resolve this, we also propose a general stochastic skyline operator, gskyline , based on the usual orders . gskyline provides the minimum candidate set to the optimal solutions for all monotonic functions. For the first time regarding the existing literature, we investigate the complexities of determining a stochastic order between two uncertain objects whose probability distributions are described discretely . We firstly show that determining the lower orthant order is NP-complete with respect to the dimensionality; consequently the problem of computing lskyline is NP-complete. We also show an interesting result as follows. While the usual order involves more complicated geometric forms than the lower orthant order, the usual order may be determined in polynomial time regarding all the inputs, including the dimensionality; this implies that gskyline can be computed in polynomial time. A general framework is developed for efficiently and effectively retrieving lskyline and gskyline from a set of uncertain objects, respectively, together with efficient and effective filtering techniques. Novel and efficient verification algorithms are developed to efficiently compute lskyline over multidimensional uncertain data, which run in polynomial time if the dimensionality is fixed, and to efficiently compute gskyline in polynomial time regarding all inputs. We also show, by theoretical analysis and experiments, that the sizes of lskyline and gskyline are both quite similar to that of conventional skyline over certain data. Comprehensive experiments demonstrate that our techniques are efficient and scalable regarding both CPU and IO costs.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2294226876",
    "type": "article"
  },
  {
    "title": "Cardinal directions between complex regions",
    "doi": "https://doi.org/10.1145/2188349.2188350",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Markus Schneider; Тао Чен; Ganesh Viswanathan; Wenjie Yuan",
    "corresponding_authors": "",
    "abstract": "Besides topological relationships and approximate relationships, cardinal directions like north and southwest have turned out to be an important class of qualitative spatial relationships. They are of interdisciplinary interest in fields like cognitive science, robotics, artificial intelligence, and qualitative spatial reasoning. In spatial databases and Geographic Information Systems (GIS) they are frequently used as join and selection criteria in spatial queries. However, the available computational models of cardinal directions suffer a number of problems like the use of too coarse approximations of the two spatial operand objects in terms of single representative points or minimum bounding rectangles, the lacking property of converseness of the cardinal directions computed, and the limited applicability to simple instead of complex regions only. This article proposes and formally defines a novel two-phase model, called the Objects Interaction Matrix (OIM) model, that solves these problems, and determines cardinal directions for even complex regions. The model consists of a tiling phase and an interpretation phase. In the tiling phase , a tiling strategy first determines the zones belonging to the nine cardinal directions of each individual region object and then intersects them. The result leads to a bounded grid called objects interaction grid . For each grid cell the information about the region objects that intersect it is stored in an objects interaction matrix . In the subsequent interpretation phase , a well-defined interpretation method is applied to such a matrix and determines the cardinal direction. Spatial example queries illustrate our new cardinal direction concept that is embedded in a spatial extension of SQL and provides user-defined cardinal direction predicates.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2088004621",
    "type": "article"
  },
  {
    "title": "Naïve Evaluation of Queries over Incomplete Databases",
    "doi": "https://doi.org/10.1145/2691190.2691194",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Amélie Gheerbrant; Leonid Libkin; Cristina Sirangelo",
    "corresponding_authors": "",
    "abstract": "The term naïve evaluation refers to evaluating queries over incomplete databases as if nulls were usual data values, that is, to using the standard database query evaluation engine. Since the semantics of query answering over incomplete databases is that of certain answers, we would like to know when naïve evaluation computes them, that is, when certain answers can be found without inventing new specialized algorithms. For relational databases it is well known that unions of conjunctive queries possess this desirable property, and results on preservation of formulae under homomorphisms tell us that, within relational calculus, this class cannot be extended under the open-world assumption. Our goal here is twofold. First, we develop a general framework that allows us to determine, for a given semantics of incompleteness, classes of queries for which naïve evaluation computes certain answers. Second, we apply this approach to a variety of semantics, showing that for many classes of queries beyond unions of conjunctive queries, naïve evaluation makes perfect sense under assumptions different from open world. Our key observations are: (1) naïve evaluation is equivalent to monotonicity of queries with respect to a semantics-induced ordering, and (2) for most reasonable semantics of incompleteness, such monotonicity is captured by preservation under various types of homomorphisms. Using these results we find classes of queries for which naïve evaluation works, for example, positive first-order formulae for the closed-world semantics. Even more, we introduce a general relation-based framework for defining semantics of incompleteness, show how it can be used to capture many known semantics and to introduce new ones, and describe classes of first-order queries for which naïve evaluation works under such semantics.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2116631029",
    "type": "article"
  },
  {
    "title": "Observing SQL queries in their natural habitat",
    "doi": "https://doi.org/10.1145/2445583.2445586",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Torsten Grust; Jan Rittinger",
    "corresponding_authors": "",
    "abstract": "We describe H abitat , a declarative observational debugger for SQL. H abitat facilitates true language-level (not: plan-level) debugging of, probably flawed, SQL queries that yield unexpected results. Users mark SQL subexpressions of arbitrary size and then observe whether these evaluate as expected. H abitat understands query nesting and free row variables in correlated subqueries, and generally aims to not constrain users while suspect subexpressions are marked for observation. From the marked SQL text, H abitat 's algebraic compiler derives a new query whose result represents the values of the desired observations. These observations are generated by the target SQL database host itself and are derived from the original data: H abitat does not require prior data extraction or extra debugging middleware. Experiments with TPC-H database instances indicate that observations impose a runtime overhead sufficiently low to allow for interactive debugging sessions.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1964770056",
    "type": "article"
  },
  {
    "title": "Exploiting Web querying for Web people search",
    "doi": "https://doi.org/10.1145/2109196.2109203",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Rabia Nuray-Turan; Dmitri V. Kalashnikov; Sharad Mehrotra",
    "corresponding_authors": "",
    "abstract": "Searching for people on the Web is one of the most common query types submitted to Web search engines today. However, when a person name is queried, the returned Webpages often contain documents related to several distinct namesakes who have the queried name. The task of disambiguating and finding the Webpages related to the specific person of interest is left to the user. Many Web People Search (WePS) approaches have been developed recently that attempt to automate this disambiguation process. Nevertheless, the disambiguation quality of these techniques leaves major room for improvement. In this article, we present a new WePS approach. It is based on issuing additional auxiliary queries to the Web to gain additional knowledge about the Webpages that need to be disambiguated. Thus, the approach uses the Web as an external data source by issuing queries to collect co-occurrence statistics. These statistics are used to assess the overlap of the contextual entities extracted from the Webpages. The article also proposes a methodology to make this Web querying technique efficient. Further, the article proposes an approach that is capable of combining various types of disambiguating information, including other common types of similarities, by applying a correlation clustering approach with after-clustering of singleton clusters. These properties allow the framework to get an advantage in terms of result quality over other state-of-the-art WePS techniques.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1994541786",
    "type": "article"
  },
  {
    "title": "Worst-Case I/O-Efficient Skyline Algorithms",
    "doi": "https://doi.org/10.1145/2389241.2389245",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Cheng Sheng; Yufei Tao",
    "corresponding_authors": "",
    "abstract": "We consider the skyline problem (aka the maxima problem ), which has been extensively studied in the database community. The input is a set P of d -dimensional points. A point dominates another if the coordinate of the former is at most that of the latter on every dimension. The goal is to find the skyline , which is the set of points p ∈ P such that p is not dominated by any other point in P . The main result of this article is that, for any fixed dimensionality d ≥ 3, in external memory the skyline problem can be settled by performing O (( N / B )log M/B d−2 ( N / B )) I/Os in the worst case, where N is the cardinality of P, B the size of a disk block, and M the capacity of main memory. Similar bounds can also be achieved for computing several skyline variants, including the k-dominant skyline, k-skyband , and α-skyline . Furthermore, the performance can be improved if some dimensions of the data space have small domains. When the dimensionality d is not fixed, the challenge is to outperform the naive algorithm that simply checks all pairs of points in P × P . We give an algorithm that terminates in O (( N / B ) log d − 2 N ) I/Os, thus beating the naive solution for any d = O (log N / log log N ).",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2058629542",
    "type": "article"
  },
  {
    "title": "Distributed Geometric Query Monitoring Using Prediction Models",
    "doi": "https://doi.org/10.1145/2602137",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Nikos Giatrakos; Antonios Deligiannakis; Minos Garofalakis; Izchak Sharfman; Assaf Schuster",
    "corresponding_authors": "",
    "abstract": "Many modern streaming applications, such as online analysis of financial, network, sensor, and other forms of data, are inherently distributed in nature. An important query type that is the focal point in such application scenarios regards actuation queries, where proper action is dictated based on a trigger condition placed upon the current value that a monitored function receives. Recent work [Sharfman et al. 2006, 2007b, 2008] studies the problem of (nonlinear) sophisticated function tracking in a distributive manner. The main concept behind the geometric monitoring approach proposed there is for each distributed site to perform the function monitoring over an appropriate subset of the input domain. In the current work, we examine whether the distributed monitoring mechanism can become more efficient, in terms of the number of communicated messages, by extending the geometric monitoring framework to utilize prediction models. We initially describe a number of local estimators (predictors) that are useful for the applications that we consider and which have already been shown particularly useful in past work. We then demonstrate the feasibility of incorporating predictors in the geometric monitoring framework and show that prediction-based geometric monitoring in fact generalizes the original geometric monitoring framework. We propose a large variety of different prediction-based monitoring models for the distributed threshold monitoring of complex functions. Our extensive experimentation with a variety of real datasets, functions, and parameter settings indicates that our approaches can provide significant communication savings ranging between two times and up to three orders of magnitude, compared to the transmission cost of the original monitoring framework.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2061030555",
    "type": "article"
  },
  {
    "title": "High-performance complex event processing over hierarchical data",
    "doi": "https://doi.org/10.1145/2536779",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Barzan Mozafari; Kai Zeng; Loris D’Antoni; Carlo Zaniolo",
    "corresponding_authors": "",
    "abstract": "While Complex Event Processing (CEP) constitutes a considerable portion of the so-called Big Data analytics, current CEP systems can only process data having a simple structure, and are otherwise limited in their ability to efficiently support complex continuous queries on structured or semistructured information. However, XML-like streams represent a very popular form of data exchange, comprising large portions of social network and RSS feeds, financial feeds, configuration files, and similar applications requiring advanced CEP queries. In this article, we present the XSeq language and system that support CEP on XML streams, via an extension of XPath that is both powerful and amenable to an efficient implementation. Specifically, the XSeq language extends XPath with natural operators to express sequential and Kleene-* patterns over XML streams, while remaining highly amenable to efficient execution. In fact, XSeq is designed to take full advantage of the recently proposed Visibly Pushdown Automata (VPA), where higher expressive power can be achieved without compromising the computationally attractive properties of finite state automata. Besides the efficiency and expressivity benefits, the choice of VPA as the underlying model also enables XSeq to go beyond XML streams and be easily applicable to any data with both sequential and hierarchical structures, including JSON messages, RNA sequences, and software traces. Therefore, we illustrate the XSeq's power for CEP applications through examples from different domains and provide formal results on its expressiveness and complexity. Finally, we present several optimization techniques for XSeq queries. Our extensive experiments indicate that XSeq brings outstanding performance to CEP applications: two orders of magnitude improvement is obtained over the same queries executed in general-purpose XML engines.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2081721653",
    "type": "article"
  },
  {
    "title": "Approximation Algorithms for Schema-Mapping Discovery from Data Examples",
    "doi": "https://doi.org/10.1145/3044712",
    "publication_date": "2017-04-28",
    "publication_year": 2017,
    "authors": "Balder ten Cate; Phokion G. Kolaitis; Kun Qian; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "In recent years, data examples have been at the core of several different approaches to schema-mapping design. In particular, Gottlob and Senellart introduced a framework for schema-mapping discovery from a single data example, in which the derivation of a schema mapping is cast as an optimization problem. Our goal is to refine and study this framework in more depth. Among other results, we design a polynomial-time log( n )-approximation algorithm for computing optimal schema mappings from a given set of data examples (where n is the combined size of the given data examples) for a restricted class of schema mappings; moreover, we show that this approximation ratio cannot be improved. In addition to the complexity-theoretic results, we implemented the aforementioned log( n )-approximation algorithm and carried out an experimental evaluation in a real-world mapping scenario.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2609779791",
    "type": "article"
  },
  {
    "title": "Oblivious bounds on the probability of boolean functions",
    "doi": "https://doi.org/10.1145/2532641",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Wolfgang Gatterbauer; Dan Suciu",
    "corresponding_authors": "",
    "abstract": "This article develops upper and lower bounds for the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach dissociation and give an exact characterization of optimal oblivious bounds , that is, when the new probabilities are chosen independently of the probabilities of all other variables. Our motivation comes from the weighted model counting problem (or, equivalently, the problem of computing the probability of a Boolean function), which is #P-hard in general. By performing several dissociations, one can transform a Boolean formula whose probability is difficult to compute into one whose probability is easy to compute, and which is guaranteed to provide an upper or lower bound on the probability of the original formula by choosing appropriate probabilities for the dissociated variables. Our new bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We also show how our theory allows a standard relational database management system (DBMS) to both upper and lower bound hard probabilistic queries in guaranteed polynomial time.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3122109288",
    "type": "article"
  },
  {
    "title": "Multiple Radii DisC Diversity",
    "doi": "https://doi.org/10.1145/2699499",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Marina Drosou; Evaggelia Pitoura",
    "corresponding_authors": "",
    "abstract": "Recently, result diversification has attracted a lot of attention as a means to improve the quality of results retrieved by user queries. In this article, we introduce a novel definition of diversity called DisC diversity. Given a tuning parameter r , which we call radius, we consider two items to be similar if their distance is smaller than or equal to r . A DisC diverse subset of a result contains items such that each item in the result is represented by a similar item in the diverse subset and the items in the diverse subset are dissimilar to each other. We show that locating a minimum DisC diverse subset is an NP-hard problem and provide algorithms for its approximation. We extend our definition to the multiple radii case, where each item is associated with a different radius based on its importance, relevance, or other factors. We also propose adapting DisC diverse subsets to a different degree of diversification by adjusting r , that is, increasing the radius (or zooming-out) and decreasing the radius (or zooming-in). We present efficient implementations of our algorithms based on the M-tree, a spatial index structure, and experimentally evaluate their performance.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2009630144",
    "type": "article"
  },
  {
    "title": "SCANRAW",
    "doi": "https://doi.org/10.1145/2818181",
    "publication_date": "2015-10-23",
    "publication_year": 2015,
    "authors": "Yu Cheng; Florin Rusu",
    "corresponding_authors": "",
    "abstract": "Traditional databases incur a significant data-to-query delay due to the requirement to load data inside the system before querying. Since this is not acceptable in many domains generating massive amounts of raw data (e.g., genomics), databases are entirely discarded. External tables , on the other hand, provide instant SQL querying over raw files. Their performance across a query workload is limited though by the speed of repeated full scans, tokenizing, and parsing of the entire file. In this article, we propose SCANRAW, a novel database meta-operator for in-situ processing over raw files that integrates data loading and external tables seamlessly, while preserving their advantages: optimal performance across a query workload and zero time-to-query. We decompose loading and external table processing into atomic stages in order to identify common functionality. We analyze alternative implementations and discuss possible optimizations for each stage. Our major contribution is a parallel superscalar pipeline implementation that allows SCANRAW to take advantage of the current many- and multicore processors by overlapping the execution of independent stages. Moreover, SCANRAW overlaps query processing with loading by speculatively using the additional I/O bandwidth arising during the conversion process for storing data into the database, such that subsequent queries execute faster. As a result, SCANRAW makes intelligent use of the available system resources—CPU cycles and I/O bandwidth—by switching dynamically between tasks to ensure that optimal performance is achieved. We implement SCANRAW in a state-of-the-art database system and evaluate its performance across a variety of synthetic and real-world datasets. Our results show that SCANRAW with speculative loading achieves the best-possible performance for a query sequence at any point in the processing. Moreover, SCANRAW maximizes resource utilization for the entire workload execution while speculatively loading data and without interfering with normal query processing.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2019386158",
    "type": "article"
  },
  {
    "title": "Generating Plans from Proofs",
    "doi": "https://doi.org/10.1145/2847523",
    "publication_date": "2016-02-03",
    "publication_year": 2016,
    "authors": "Michael Benedikt; Balder ten Cate; Efthymia Tsamoura",
    "corresponding_authors": "",
    "abstract": "We present algorithms for answering queries making use of information about source integrity constraints, access restrictions, and access costs. Our method can exploit the integrity constraints to find plans even when there is no direct access to relations appearing in the query. We look at different kinds of plans, depending on the kind of relational operators that are permitted within their commands. To each type of plan, we associate a semantic property that is necessary for having a plan of that type. The key idea of our method is to move from a search for a plan to a search for a proof of the corresponding semantic property, and then generate a plan from a proof . We provide algorithms for converting proofs to plans and show that they will find a plan of the desired type whenever such a plan exists. We show that while discovery of one proof allows us to find a single plan that answers the query, we can explore alternative proofs to find lower-cost plans.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2254540169",
    "type": "article"
  },
  {
    "title": "Expressive Languages for Querying the Semantic Web",
    "doi": "https://doi.org/10.1145/3238304",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Marcelo Arenas; Georg Gottlob; Andréas Pieris",
    "corresponding_authors": "",
    "abstract": "The problem of querying RDF data is a central issue for the development of the Semantic Web. The query language SPARQL has become the standard language for querying RDF since its W3C standardization in 2008. However, the 2008 version of this language missed some important functionalities: reasoning capabilities to deal with RDFS and OWL vocabularies, navigational capabilities to exploit the graph structure of RDF data, and a general form of recursion much needed to express some natural queries. To overcome these limitations, a new version of SPARQL, called SPARQL 1.1, was released in 2013, which includes entailment regimes for RDFS and OWL vocabularies, and a mechanism to express navigation patterns through regular expressions. Unfortunately, there are a number of useful navigation patterns that cannot be expressed in SPARQL 1.1, and the language lacks a general mechanism to express recursive queries. To the best of our knowledge, no efficient RDF query language that combines the above functionalities is known. It is the aim of this work to fill this gap. To this end, we focus on a core fragment of the OWL 2 QL profile of OWL 2 and show that every SPARQL query enriched with the above features can be naturally translated into a query expressed in a language that is based on an extension of Datalog, which allows for value invention and stratified negation. However, the query evaluation problem for this language is highly intractable, which is not surprising since it is expressive enough to encode some inherently hard queries. We identify a natural fragment of it, and we show it to be tractable and powerful enough to define SPARQL queries enhanced with the desired functionalities.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2900751348",
    "type": "article"
  },
  {
    "title": "Wander Join and XDB",
    "doi": "https://doi.org/10.1145/3284551",
    "publication_date": "2019-01-29",
    "publication_year": 2019,
    "authors": "Feifei Li; Bin Wu; Ke Yi; Zhuoyue Zhao",
    "corresponding_authors": "",
    "abstract": "Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This article proposes a new approach, the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori . Compared with ripple join, wander join is particularly efficient for equality joins involving multiple tables, but also supports θ-joins. Selection predicates and group-by clauses can be handled as well. To demonstrate the usefulness of wander join, we have designed and implemented XDB (approXimate DB) by integrating wander join into various systems including PostgreSQL, Spark, and a stand-alone plug-in version using PL/SQL. The design and implementation of XDB has demonstrated wander join’s practicality in a full-fledged database system. Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2999362268",
    "type": "article"
  },
  {
    "title": "Packing R-trees with Space-filling Curves",
    "doi": "https://doi.org/10.1145/3397506",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Jianzhong Qi; Yufei Tao; Yanchuan Chang; Rui Zhang",
    "corresponding_authors": "",
    "abstract": "The massive amount of data and large variety of data distributions in the big data era call for access methods that are efficient in both query processing and index management, and over both practical and worst-case workloads. To address this need, we revisit two classic multidimensional access methods—the R-tree and the space-filling curve. We propose a novel R-tree packing strategy based on space-filling curves. This strategy produces R-trees with an asymptotically optimal I/O complexity for window queries in the worst case. Experiments show that our R-trees are highly efficient in querying both real and synthetic data of different distributions. The proposed strategy is also simple to parallelize, since it relies only on sorting. We propose a parallel algorithm for R-tree bulk-loading based on the proposed packing strategy and analyze its performance under the massively parallel communication model. To handle dynamic data updates, we further propose index update algorithms that process data insertions and deletions without compromising the optimal query I/O complexity. Experimental results confirm the effectiveness and efficiency of the proposed R-tree bulk-loading and updating algorithms over large data sets.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3041939645",
    "type": "article"
  },
  {
    "title": "Functional Aggregate Queries with Additive Inequalities",
    "doi": "https://doi.org/10.1145/3426865",
    "publication_date": "2020-12-06",
    "publication_year": 2020,
    "authors": "Mahmoud Abo Khamis; Ryan R. Curtin; Benjamin Moseley; Hung Q. Ngo; XuanLong Nguyen; Dan Olteanu; Maximilian Schleich",
    "corresponding_authors": "",
    "abstract": "Motivated by fundamental applications in databases and relational machine learning, we formulate and study the problem of answering functional aggregate queries (FAQ) in which some of the input factors are defined by a collection of additive inequalities between variables. We refer to these queries as FAQ-AI for short. To answer FAQ-AI in the Boolean semiring, we define relaxed tree decompositions and relaxed submodular and fractional hypertree width parameters. We show that an extension of the InsideOut algorithm using Chazelle’s geometric data structure for solving the semigroup range search problem can answer Boolean FAQ-AI in time given by these new width parameters. This new algorithm achieves lower complexity than known solutions for FAQ-AI. It also recovers some known results in database query answering. Our second contribution is a relaxation of the set of polymatroids that gives rise to the counting version of the submodular width, denoted by #subw. This new width is sandwiched between the submodular and the fractional hypertree widths. Any FAQ and FAQ-AI over one semiring can be answered in time proportional to #subw and respectively to the relaxed version of #subw. We present three applications of our FAQ-AI framework to relational machine learning: k -means clustering, training linear support vector machines, and training models using non-polynomial loss. These optimization problems can be solved over a database asymptotically faster than computing the join of the database relations.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3111578933",
    "type": "article"
  },
  {
    "title": "Embedded Functional Dependencies and Data-completeness Tailored Database Design",
    "doi": "https://doi.org/10.1145/3450518",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Ziheng Wei; Sebastian Link",
    "corresponding_authors": "",
    "abstract": "We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3168935107",
    "type": "article"
  },
  {
    "title": "On the Enumeration Complexity of Unions of Conjunctive Queries",
    "doi": "https://doi.org/10.1145/3450263",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Nofar Carmeli; Markus Kröll",
    "corresponding_authors": "",
    "abstract": "We study the enumeration complexity of Unions of Conjunctive Queries (UCQs) . We aim to identify the UCQs that are tractable in the sense that the answer tuples can be enumerated with a linear preprocessing phase and a constant delay between every successive tuples. It has been established that, in the absence of self-joins and under conventional complexity assumptions, the CQs that admit such an evaluation are precisely the free-connex ones. A union of tractable CQs is always tractable. We generalize the notion of free-connexity from CQs to UCQs, thus showing that some unions containing intractable CQs are, in fact, tractable. Interestingly, some unions consisting of only intractable CQs are tractable too. We show how to use the techniques presented in this article also in settings where the database contains cardinality dependencies (including functional dependencies and key constraints) or when the UCQs contain disequalities. The question of finding a full characterization of the tractability of UCQs remains open. Nevertheless, we prove that, for several classes of queries, free-connexity fully captures the tractable UCQs.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3172388843",
    "type": "article"
  },
  {
    "title": "Error Bounded Line Simplification Algorithms for Trajectory Compression: An Experimental Evaluation",
    "doi": "https://doi.org/10.1145/3474373",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Xuelian Lin; Shuai Ma; Jiahao Jiang; Yanchen Hou; Tianyu Wo",
    "corresponding_authors": "",
    "abstract": "Nowadays, various sensors are collecting, storing, and transmitting tremendous trajectory data, and it is well known that the storage, network bandwidth, and computing resources could be heavily wasted if raw trajectory data is directly adopted. Line simplification algorithms are effective approaches to attacking this issue by compressing a trajectory to a set of continuous line segments, and are commonly used in practice. In this article, we first classify the error bounded line simplification algorithms into different categories and review each category of algorithms. We then study the data aging problem of line simplification algorithms and distance metrics from the views of aging friendliness and aging errors. Finally, we present a systematic experimental evaluation of representative error bounded line simplification algorithms, including both compression optimal and sub-optimal methods, in terms of commonly adopted perpendicular Euclidean, synchronous Euclidean, and direction-aware distances. Using real-life trajectory datasets, we systematically evaluate and analyze the performance (compression ratio, average error, running time, aging friendliness, and query friendliness) of error bounded line simplification algorithms with respect to distance metrics, trajectory sizes, and error bounds. Our study provides a full picture of error bounded line simplification algorithms, which leads to guidelines on how to choose appropriate algorithms and distance metrics for practical applications.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3202225718",
    "type": "article"
  },
  {
    "title": "Streaming enumeration on nested documents",
    "doi": "https://doi.org/10.1145/3701557",
    "publication_date": "2024-10-25",
    "publication_year": 2024,
    "authors": "Martín Muñoz; Cristian Riveros",
    "corresponding_authors": "",
    "abstract": "Some of the most relevant document schemas used online, such as XML and JSON, have a nested format. In the last decade, the task of extracting data from nested documents over streams has become especially relevant. We focus on the streaming evaluation of queries with outputs of varied sizes over nested documents. We model queries of this kind as Visibly Pushdown Annotators (VPAnn), a computational model that extends visibly pushdown automata with outputs and has the same expressive power as MSO over nested documents. Since processing a document through a VPAnn can generate a massive number of results, we are interested in reading the input in a streaming fashion and enumerating the outputs one after another as efficiently as possible, namely, with constant delay. This paper presents an algorithm that enumerates these elements with constant delay after processing the document stream in a single pass. Furthermore, we show that this algorithm is worst-case optimal in terms of update-time per symbol and memory usage.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4403771197",
    "type": "article"
  },
  {
    "title": "Fast algorithms for universal quantification in large databases",
    "doi": "https://doi.org/10.1145/210197.210202",
    "publication_date": "1995-06-01",
    "publication_year": 1995,
    "authors": "Goetz Graefe; Richard L. Cole",
    "corresponding_authors": "",
    "abstract": "Universal quantification is not supported directly in most database systems despite the fact that it adds significant power to a system's query processing and inference capabilities, in particular for the analysis of many-to-many relationships and of set-valued attributes. One of the main reasons for this omission has been that universal quantification algorithms and their performance have not been explored for large databases. In this article, we describe and compare three known algorithms and one recently proposed algorithm for relational division, the algebra operator that embodies universal quantification. For each algorithm, we investigate the performance effects of explicit duplicate removal and referential integrity enforcement, variants for inputs larger than memory, and parallel execution strategies. Analytical and experimental performance comparisons illustrate the substantial differences among the algorithms. Moreover, comparisons demonstrate that the recently proposed division algorithm evaluates a universal quantification predicate over two relations as fast as hash (semi-) join evaluates an existential quantification predicate over the same relations. Thus, existential and universal quantification can be supported with equal efficiency by adding the recently proposed algorithm to a query evaluation system. A second result of our study is that universal quantification should be expressed directly in a database query language, because most query optimizers do not recognize the rather indirect formulations available in SQL as relational division and therefore produce very poor evaluation plans for many universal quantification queries.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1996467498",
    "type": "article"
  },
  {
    "title": "Cost and availability tradeoffs in replicated data concurrency control",
    "doi": "https://doi.org/10.1145/151284.151287",
    "publication_date": "1993-03-01",
    "publication_year": 1993,
    "authors": "Akhil Kumar; Arie Segev",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Cost and availability tradeoffs in replicated data concurrency control Authors: Akhil Kumar Cornell Univ., Ithaca, NY Cornell Univ., Ithaca, NYView Profile , Arie Segev Univ. of California, Berkeley and Lawrence Berkeley Lab, Berkeley, CA Univ. of California, Berkeley and Lawrence Berkeley Lab, Berkeley, CAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 18Issue 1pp 102–131https://doi.org/10.1145/151284.151287Published:01 March 1993Publication History 47citation637DownloadsMetricsTotal Citations47Total Downloads637Last 12 Months16Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2045387880",
    "type": "article"
  },
  {
    "title": "Cost models for overlapping and multiversion structures",
    "doi": "https://doi.org/10.1145/581751.581754",
    "publication_date": "2002-09-01",
    "publication_year": 2002,
    "authors": "Yufei Tao; Dimitris Papadias; Jun Zhang",
    "corresponding_authors": "",
    "abstract": "Overlapping and multiversion techniques are two popular frameworks that transform an ephemeral index into a multiple logical-tree structure in order to support versioning databases. Although both frameworks have produced numerous efficient indexing methods, their performance analysis is rather limited; as a result there is no clear understanding about the behavior of the alternative structures and the choice of the best one, given the data and query characteristics. Furthermore, query optimization based on these methods is currently impossible. These are serious problems due to the incorporation of overlapping and multiversion techniques in several traditional (e.g., financial) and emerging (e.g., spatiotemporal) applications. In this article, we reduce performance analysis of overlapping and multiversion structures to that of the corresponding ephemeral structures, thus simplifying the problem significantly. This reduction leads to accurate cost models that predict the sizes of the trees, the node/page accesses, and selectivity of queries. Furthermore, the models offer significant insight into the behavior of the structures and provide guidelines about the selection of the most appropriate method in practice. Extensive experimentation proves that the proposed models yield errors below 5 and 15% for uniform and nonuniform data, respectively.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W1982141094",
    "type": "article"
  },
  {
    "title": "Safe query languages for constraint databases",
    "doi": "https://doi.org/10.1145/288086.288088",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Péter Révész",
    "corresponding_authors": "Péter Révész",
    "abstract": "In the database framework of Kanellakis et al. [1990] it was argued that constraint query languages should take constraint databases as input and give other constraint databases that use the same type of atomic constraints as output. This closed-form requirement has been difficult to realize in constraint query languages that contain the negation symbol. This paper describes a general approach to restricting constraint query languages with negation to safe subsets that contain only programs that are evaluable in closed-form on any valid constraint database input.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2072462004",
    "type": "article"
  },
  {
    "title": "Adaptive, fine-grained sharing in a client-server OODBMS",
    "doi": "https://doi.org/10.1145/278245.278249",
    "publication_date": "1997-12-01",
    "publication_year": 1997,
    "authors": "Markos Zaharioudakis; Michael J. Carey; Michael J. Franklin",
    "corresponding_authors": "",
    "abstract": "For reasons of simplicity and communication efficiency, a number of existing object-oriented database management systems are based on page server architectures; data pages are their minimum unit of transfer and client caching. Despite their efficiency, page servers are often criticized as being too retrictive when it comes to concurrency, as existing systems use pages as the minimum locking unit as well. In this paper we show how to support object-level locking in a page-server context. Several approaches are described, including an adaptive granularity approach that uses page-level locking for most pages but switches to object-level locking when finer-grained sharing is demanded. Each of the approaches is based on extending the idea of callback locking. We study the performance of these approaches, comparing them to both a pure page server and a pure object server. For the range of workload that we have examined, our results indicate that the adaptive page server provides very good performance, usually outperforming the pure page server and the other page-server variants as well. In addition, the adaptive page server is often preferable to the pure object server; our results provides insight into when each approach is likely to perform better.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2078037900",
    "type": "article"
  },
  {
    "title": "Performance evaluation of cautious waiting",
    "doi": "https://doi.org/10.1145/132271.132275",
    "publication_date": "1992-09-01",
    "publication_year": 1992,
    "authors": "Meichun Hsu; Bin Zhang",
    "corresponding_authors": "",
    "abstract": "We study a deadlock-free locking-based concurrency control algorithm, called cautious waiting , which allows for a limited form of waiting. The algorithm is very simple to implement. We present an analytical solution to its performance evaluation based on the mean-value approach proposed by Tay et al. [18]. From the modeling point of view, we are able to do away with a major assumption used in Tay's previous work, and therefore capture more accurately both the restart and the blocking rates in the system. We show that to solve for this model we only need to solve for the root of a polynomial. The analytical tools developed enable us to see that the cautious waiting algorithm manages to achieve a delicate balance between restart and blocking, and therefore is superior (i.e., has higher throughput to both the no-waiting (i.e., immediate restart) and the general waiting algorithms under a wide range of system parameters. The study substantiates the argument that balancing restart and blocking is important in locking systems.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2107599232",
    "type": "article"
  },
  {
    "title": "Searching for dependencies at multiple abstraction levels",
    "doi": "https://doi.org/10.1145/581751.581752",
    "publication_date": "2002-09-01",
    "publication_year": 2002,
    "authors": "Toon Calders; Raymond T. Ng; Jef Wijsen",
    "corresponding_authors": "",
    "abstract": "The notion of roll-up dependency (RUD) extends functional dependencies with generalization hierarchies. RUDs can be applied in OLAP and database design. The problem of discovering RUDs in large databases is at the center of this paper. An algorithm is provided that relies on a number of theoretical results. The algorithm has been implemented; results on two real-life datasets are given. The extension of functional dependency (FD) with roll-ups turns out to capture meaningful rules that are outside the scope of classical FD mining. Performance figures show that RUDs can be discovered in linear time in the number of tuples of the input dataset.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2042315498",
    "type": "article"
  },
  {
    "title": "Theory of dependence values",
    "doi": "https://doi.org/10.1145/363951.363956",
    "publication_date": "2000-09-01",
    "publication_year": 2000,
    "authors": "Rosa Meo",
    "corresponding_authors": "Rosa Meo",
    "abstract": "A new model to evaluate dependencies in data mining problems is presented and discussed. The well-known concept of the association rule is replaced by the new definition of dependence value, which is a single real number uniquely associated with a given itemset. Knowledge of dependence values is sufficient to describe all the dependencies characterizing a given data mining problem. The dependence value of an itemset is the difference between the occurrence probability of the itemset and a corresponding “maximum independence estimate.” This can be determined as a function of joint probabilities of the subsets of the itemset being considered by maximizing a suitable entropy function. So it is possible to separate in an itemset of cardinaltiy k the dependence inherited from its subsets of cardinality (k - 1) and the specific inherent dependence of that itemset. The absolute value of the difference between the probability p( i ) of the event i that indicates the prescence of the itemset {a,b,... } and its maximum independence estimate is constant for any combination of values of 〈a,b,...〉. In addition, the Boolean function specifying the combination of values for which the dependence is positive is a parity function. So the determination of such combinations is immediate. The model appears to be simple and powerful.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2103309275",
    "type": "article"
  },
  {
    "title": "Multiversion-based view maintenance over distributed data sources",
    "doi": "https://doi.org/10.1145/1042046.1042049",
    "publication_date": "2004-12-12",
    "publication_year": 2004,
    "authors": "Songting Chen; Bin Liu; Elke A. Rundensteiner",
    "corresponding_authors": "",
    "abstract": "Materialized views can be maintained by submitting maintenance queries to the data sources. However, the query results may be erroneous due to concurrent source updates. State-of-the-art maintenance strategies typically apply compensations to resolve such conflicts and assume all source schemata remain stable over time. In a loosely coupled dynamic environment, the sources may autonomously change not only their data but also their schema or semantics. Consequently, either the maintenance or the compensation queries may be broken. Unlike compensation-based approaches found in the literature, we instead model the complete materialized view maintenance process as a view maintenance transaction (VM_Transaction). This way, the anomaly problem can be rephrased as the serializability of VM_Transactions. To achieve VM_Transaction serializability, we propose a multiversion concurrency control algorithm, called TxnWrap , which is shown to be the appropriate design for loosely coupled environments with autonomous data sources. TxnWrap is complementary to the maintenance algorithms proposed in the literature, since it removes concurrency issues from consideration allowing the designer to focus on the maintenance logic. We show several optimizations of TxnWrap, in particular, (1) space optimizations on versioned data materialization and (2) parallel maintenance scheduling. With these optimizations, TxnWrap even outperforms state-of-the-art view maintenance solutions in terms of refresh time. Further, several design choices of TxnWrap are studied each having its respective advantages for certain environmental settings. A correctness proof based on transaction theory for TxnWrap is also provided. Last, we have implemented TxnWrap. The experimental results confirm that TxnWrap achieves predictable performance under a varying rate of concurrency.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1994642127",
    "type": "article"
  },
  {
    "title": "Evaluation of remote backup algorithms for transaction-processing systems",
    "doi": "https://doi.org/10.1145/185827.185836",
    "publication_date": "1994-09-01",
    "publication_year": 1994,
    "authors": "Christos A. Polyzois; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "A remote backup is a copy of a primary database maintained at a geographically separate location and is used to increase data availability. Remote backup systems are typically log-based and can be classified into 2-safe and 1-safe, depending on whether transactions commit at both sites simultaneously or first commit at the primary and are later propagated to the backup. We have built an experimental database system on which we evaluated the performance of the epoch and the dependency reconstruction algorithms, two 1-safe algorithms we have developed. We compared the 1-safe with the 2-safe approach under various conditions.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2051699256",
    "type": "article"
  },
  {
    "title": "Iterative spatial join",
    "doi": "https://doi.org/10.1145/937598.937600",
    "publication_date": "2003-09-01",
    "publication_year": 2003,
    "authors": "Edwin Jacox; Hanan Samet",
    "corresponding_authors": "",
    "abstract": "The key issue in performing spatial joins is finding the pairs of intersecting rectangles. For unindexed data sets, this is usually resolved by partitioning the data and then performing a plane sweep on the individual partitions. The resulting join can be viewed as a two-step process where the partition corresponds to a hash-based join while the plane-sweep corresponds to a sort-merge join. In this article, we look at extending the idea of the sort-merge join for one-dimensional data to multiple dimensions and introduce the Iterative Spatial Join . As with the sort-merge join, the Iterative Spatial Join is best suited to cases where the data is already sorted. However, as we show in the experiments, the Iterative Spatial Join performs well when internal memory is limited, compared to the partitioning methods. This suggests that the Iterative Spatial Join would be useful for very large data sets or in situations where internal memory is a shared resource and is therefore limited, such as with today's database engines which share internal memory amongst several queries. Furthermore, the performance of the Iterative Spatial Join is predictable and has no parameters which need to be tuned, unlike other algorithms. The Iterative Spatial Join is based on a plane sweep algorithm, which requires the entire data set to fit in internal memory. When internal memory overflows, the Iterative Spatial Join simply makes additional passes on the data, thereby exhibiting only a gradual performance degradation. To demonstrate the use and efficacy of the Iterative Spatial Join, we first examine and analyze current approaches to performing spatial joins, and then give a detailed analysis of the Iterative Spatial Join as well as present the results of extensive testing of the algorithm, including a comparison with partitioning-based spatial join methods. These tests show that the Iterative Spatial Join overcomes the performance limitations of the other algorithms for data sets of all sizes as well as differing amounts of internal memory.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2008305564",
    "type": "article"
  },
  {
    "title": "On the effect of join operations on relation sizes",
    "doi": "https://doi.org/10.1145/76902.76907",
    "publication_date": "1989-12-01",
    "publication_year": 1989,
    "authors": "Danièle Grady; Claude Puech",
    "corresponding_authors": "",
    "abstract": "We propose a generating function approach to the problem of evaluating the sizes of derived relations in a relational database framework. We present a model of relations and show how to use it to deduce probabilistic estimations of derived relation sizes. These are found to asymptotically follow normal distributions under a variety of assumptions.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1969914452",
    "type": "article"
  },
  {
    "title": "Fragmentation: a technique for efficient query processing",
    "doi": "https://doi.org/10.1145/5922.5638",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Giovanni Maria Sacco",
    "corresponding_authors": "Giovanni Maria Sacco",
    "abstract": "A “divide and conquer” strategy to compute natural joins by sequential scans on unordered relations is described. This strategy is shown to always he better than merging SCBIIS when both relations must he sorted before joining, and generally better in practical cases when only the largest relation mutt be sorted.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1986353419",
    "type": "article"
  },
  {
    "title": "Optimal signature extraction and information loss",
    "doi": "https://doi.org/10.1145/27629.214285",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Christos Faloutsos; Stavros Christodoulakis",
    "corresponding_authors": "",
    "abstract": "Signature files seem to be a promising access method for text and attributes. According to this method, the documents (or records) are stored sequentially in one file (\"text file\"), while abstractions of the documents (\"signatures\") are stored sequentially in another file (\"signature file\"). In order to resolve a query, the signature file is scanned first, and many nonqualifying documents are immediately rejected. We develop a framework that includes primary key hashing, multiattribute hashing, and signature files. Our effort is to find the optimal signature extraction method. The main contribution of this paper is that we present optimal and efficient suboptimal algorithms for assigning words to signatures in several environments. Another contribution is that we use information theory, and study the relationship of the false drop probability F d and the information that is lost during signature extraction. We give tight lower bounds on the achievable F d and show that a simple relationship holds between the two quantities in the case of optimal signature extraction with uniform occurrence and query frequencies. We examine hashing as a method to map words to signatures (instead of the optimal way), and show that the same relationship holds between F d and loss , indicating that an invariant may exist between these two quantities for every signature extraction method.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1996171708",
    "type": "article"
  },
  {
    "title": "Expressive power of an algebra for data mining",
    "doi": "https://doi.org/10.1145/1189769.1189770",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Toon Calders; Laks V. S. Lakshmanan; Raymond T. Ng; Jan Paredaens",
    "corresponding_authors": "",
    "abstract": "The relational data model has simple and clear foundations on which significant theoretical and systems research has flourished. By contrast, most research on data mining has focused on algorithmic issues. A major open question is: what's an appropriate foundation for data mining, which can accommodate disparate mining tasks? We address this problem by presenting a database model and an algebra for data mining. The database model is based on the 3W-model introduced by Johnson et al. [2000]. This model relied on black box mining operators. A main contribution of this article is to open up these black boxes, by using generic operators in a data mining algebra. Two key operators in this algebra are regionize , which creates regions (or models) from data tuples, and a restricted form of looping called mining loop . Then the resulting data mining algebra MA is studied and properties concerning expressive power and complexity are established. We present results in three directions: (1) expressiveness of the mining algebra; (2) relations with alternative frameworks, and (3) interactions between regionize and mining loop.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1971604907",
    "type": "article"
  },
  {
    "title": "On the translation of relational queries into iterative programs",
    "doi": "https://doi.org/10.1145/62032.62033",
    "publication_date": "1989-03-01",
    "publication_year": 1989,
    "authors": "Johann Christoph Freytag; Nathan Goodman",
    "corresponding_authors": "",
    "abstract": "This paper investigates the problem of translating set-oriented query specifications into iterative programs. The translation uses techniques of functional programming and program transformation. We present two algorithms that generate iterative programs from algebra-based query specifications. The first algorithm translates query specifications into recursive programs. Those are simplified by sets of transformation rules before the algorithm generates the final iterative form. The second algorithm uses a two-level translation that generates iterative programs faster than the first algorithm. On the first level a small set of transformation rules performs structural simplification before the functional combination on the second level yields the final iterative form.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2004482302",
    "type": "article"
  },
  {
    "title": "Update and retrieval in a relational database through a universal schema interface",
    "doi": "https://doi.org/10.1145/49346.49884",
    "publication_date": "1988-10-01",
    "publication_year": 1988,
    "authors": "Volkert Brosda; Gottfried Vossen",
    "corresponding_authors": "",
    "abstract": "A database system that is based on the universal relation (UR) model aims at freeing its users from specifying access paths on both the physical and on the logical levels. All information about the logical structure of the database (i.e., its conceptual scheme) is hidden from users; they need only to know the attribute names, which now carry all the semantics of the database. Previous work on UR interfaces has concentrated on the design and implementation of query languages that serve to facilitate retrieval of data from a relational database. On the other hand, updates are always handled as before, which means that users still have to know the logical structure of the database in case they want to insert, delete, or modify tuples. In this paper the concepts underlying a UR interface, which is really “universal,” are presented; it is based on the UR model, and it permits not only queries but also updates: Combinations of attributes that may participate in an update-operation (\"objects\") have to be specified during the design phase of the database, and are then embodied into the database scheme by an extended synthesis algorithm. They form the basis for any insertion or deletion operation. A precise definition of “insertable” tuples, and of the insert- and delete-operation in this new context, is given. It is then shown that these operations modify a database state in such a way that a representative instance always exists. This is accomplished by providing a more detailed version of Sagiv's uniqueness condition and by exploring the structure of nonunique objects. Since the underlying database always has a representative instance, this instance can be used to define the window function for retrieval. It is shown that it is still possible to compute windows by a union of minimal extension joins.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2044827812",
    "type": "article"
  },
  {
    "title": "Generalized multidimensional data mapping and query processing",
    "doi": "https://doi.org/10.1145/1093382.1093383",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Rui Zhang; Panos Kalnis; Beng Chin Ooi; Kian‐Lee Tan",
    "corresponding_authors": "",
    "abstract": "Multidimensional data points can be mapped to one-dimensional space to exploit single dimensional indexing structures such as the B + -tree. In this article we present a Generalized structure for data Mapping and query Processing (GiMP), which supports extensible mapping methods and query processing. GiMP can be easily customized to behave like many competent indexing mechanisms for multi-dimensional indexing, such as the UB-Tree, the Pyramid technique, the iMinMax, and the iDistance. Besides being an extendible indexing structure, GiMP also serves as a framework to study the characteristics of the mapping and hence the efficiency of the indexing scheme. Specifically, we introduce a metric called mapping redundancy to characterize the efficiency of a mapping method in terms of disk page accesses and analyze its behavior for point, range and kNN queries. We also address the fundamental problem of whether an efficient mapping exists and how to define such a mapping for a given data set.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2061567674",
    "type": "article"
  },
  {
    "title": "B-tree concurrency control and recovery in page-server database systems",
    "doi": "https://doi.org/10.1145/1132863.1132866",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Zeinab Ibrahim; Seppo Sippu; Eljas Soisalon-Soininen",
    "corresponding_authors": "",
    "abstract": "We develop new algorithms for the management of transactions in a page-shipping client-server database system in which the physical database is organized as a sparse B-tree index. Our starvation-free fine-grained locking protocol combines adaptive callbacks with key-range locking and guarantees repeatable-read-level isolation (i.e., serializability) for transactions containing any number of record insertions, record deletions, and key-range scans. Partial and total rollbacks of client transactions are performed by the client. Each structure modification such as a page split or merge is defined as an atomic action that affects only two levels of the B-tree and is logged using a single redo-only log record, so that the modification never needs to be undone during transaction rollback or restart recovery. The steal-and-no-force buffering policy is applied by the server when flushing updated pages onto disk and by the clients when shipping updated data pages to the server, while pages involved in a structure modification are forced to the server when the modification is finished. The server performs the restart recovery from client and system failures using an ARIES/CSA-based recovery protocol. Our algorithms avoid accessing stale data but allow a data page to be updated by one client transaction and read by many other client transactions simultaneously, and updates may migrate from a data page to another in structure modifications caused by other transactions while the updating transaction is still active.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1984791273",
    "type": "article"
  },
  {
    "title": "Database performance evaluation in an indexed file environment",
    "doi": "https://doi.org/10.1145/12047.13675",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "Jane Fedorowicz",
    "corresponding_authors": "Jane Fedorowicz",
    "abstract": "The use of database systems for managerial decision making often incorporates information-retrieval capabilities with numeric report generation. Of great concern to the user of such a system is the response time associated with issuing a query to the database. This study presents a procedure for estimating response time for one of the most frequently encountered physical storage mechanisms, the indexed file. The model provides a fairly high degree of accuracy, but is simple enough so that the cost of applying the model is not exorbitant. The model incorporates the knowledge that the distribution of access key occurrences is known to follow Zipf's law. It first estimates the access time required to complete the query, which includes the time needed for all input and output transactions, and CPU time used in performing the search. The effects of multiple users on an individual's response time are then assessed using a simple regression estimation technique. The two-step procedure allows for the separation of access time from multiuser influences.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2002869374",
    "type": "article"
  },
  {
    "title": "The use of regression methodology for the compromise of confidential information in statistical databases",
    "doi": "https://doi.org/10.1145/32204.42174",
    "publication_date": "1987-11-01",
    "publication_year": 1987,
    "authors": "Michael A. Palley; Jeffrey S. Simonoff",
    "corresponding_authors": "",
    "abstract": "A regression methodology based technique can be used to compromise confidentiality in a statistical database. This holds true even when the DBMS prevents application of regression methodology to the database. Existing inference controls, including cell restriction, perturbation, and table restriction approaches, are shown to be generally ineffective against this compromise technique. The effect of incomplete supplemental knowledge on the regression methodology based compromise technique is examined. Finally, some potential complicators of this disclosure scheme are introduced.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2062235982",
    "type": "article"
  },
  {
    "title": "Strategies for query unnesting in XML databases",
    "doi": "https://doi.org/10.1145/1166074.1166081",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Norman May; Sven Helmer; Guido Moerkotte",
    "corresponding_authors": "",
    "abstract": "Queries formulated in a nested way are very common in XQuery. Unfortunately, their evaluation is usually very inefficient when done in a straightforward fashion. We present a framework for handling nested queries that is based on unnesting the queries after having translated them into an algebra. We not only present a collection of algebraic equivalences, but also supply a strategy on how to use them effectively. The full potential of the approach is demonstrated by applying our rewrites to actual queries and showing that performance gains of several orders of magnitude are possible.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2089211036",
    "type": "article"
  },
  {
    "title": "On first-order-logic databases",
    "doi": "https://doi.org/10.1145/27629.27630",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Henryk Rybiński",
    "corresponding_authors": "Henryk Rybiński",
    "abstract": "The use of first-order logic as database logic is shown to be powerful enough for formalizing and implementing not only relational but also hierarchical and network-type databases. It enables one to treat all the types of databases in a uniform manner. This paper focuses on the database language for heterogeneous databases. The language is shown to be general enough to specify constraints for a particular type of database, so that a specification of database type can be “translated” to the specification given in the database language, creating a “logical environment” for different views that can be defined by users. Owing to the fact that any database schema is seen as a first-order theory expressed by a finite set of sentences, the problems concerned with completeness and compactness of the database logic discussed by Jacobs (\"On Database Logic,” J. ACM 29 ,2 (Apr. 1982), 310-332) are avoided.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2008484875",
    "type": "article"
  },
  {
    "title": "Improving instruction cache performance in OLTP",
    "doi": "https://doi.org/10.1145/1166074.1166079",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Stavros Harizopoulos; Anastassia Ailamaki",
    "corresponding_authors": "",
    "abstract": "Instruction-cache misses account for up to 40% of execution time in online transaction processing (OLTP) database workloads. In contrast to data cache misses, instruction misses cannot be overlapped with out-of-order execution. Chip design limitations do not allow increases in the size or associativity of instruction caches that would help reduce misses. On the contrary, the effective instruction cache size is expected to further decrease with the adoption of multicore and multithreading chip designs (multiple on-chip processor cores and multiple simultaneous threads per core). Different concurrent database threads, however, execute similar instruction sequences over their lifetime, too long to be captured and exploited in hardware. The challenge, from a software designer's point of view, is to identify and exploit common code paths across threads executing arbitrary operations, thereby eliminating extraneous instruction misses.In this article, we describe Synchronized Threads through Explicit Processor Scheduling (STEPS), a methodology and tool to increase instruction locality in database servers executing transaction processing workloads. STEPS works at two levels to increase reusability of instructions brought in the cache. At a higher level, synchronization barriers form teams of threads that execute the same system component. Within a team, STEPS schedules special fast context-switches at very fine granularity to reuse sets of instructions across team members. To find points in the code where context-switches should occur, we develop autoSTEPS , a code profiling tool that runs directly on the DBMS binary. STEPS can minimize both capacity and conflict instruction cache misses for arbitrarily long code paths.We demonstrate the effectiveness of our approach on Shore , a research prototype database system shown to be governed by similar bottlenecks as commercial systems. Using microbenchmarks on real and simulated processors, we observe that STEPS eliminates up to 96% of instruction-cache misses for each additional team thread and at the same time eliminates up to 64% of mispredicted branches by providing a repetitive execution pattern to the processor. When performing a full-system evaluation on real hardware using TPC-C, the industry-standard transactional benchmark, STEPS eliminates two-thirds of instruction-cache misses and provides up to 1.4 overall speedup.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2141138026",
    "type": "article"
  },
  {
    "title": "Cautious transaction schedulers with admission control",
    "doi": "https://doi.org/10.1145/3857.3860",
    "publication_date": "1985-06-01",
    "publication_year": 1985,
    "authors": "Naoki Katoh; Toshihide Ibaraki; Tiko Kameda",
    "corresponding_authors": "",
    "abstract": "We propose a new class of schedulers, called cautious schedulers , that grant an input request if it will not necessitate any rollback in the future. In particular, we investigate cautious WRW-schedulers that output schedules in class WRW only. Class WRW consists of all schedules that are serializable, while preserving the write-read and read-write conflict, and is the largest polynomially recognizable subclass of serializable schedules currently known. It is shown, in this paper however, that cautious WRW- scheduling is, in general, NP-complete. Therefore, we introduce a special type ( type 1R ) of transaction, which consists of no more than one read step (an indivisible set of read operations) followed by multiple write steps. It is shown that cautious WRW-scheduling can be performed efficiently if all transactions are of type 1R and if admission control can be exercised. Admission control rejects a transaction unless its first request is immediately grantable.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1977531749",
    "type": "article"
  },
  {
    "title": "Partial-match retrieval using hashing and descriptors",
    "doi": "https://doi.org/10.1145/319996.320006",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "Kotagiri Ramamohanarao; James A. Thom; John W. Lloyd",
    "corresponding_authors": "",
    "abstract": "This paper studies a partial-match retrieval scheme based on hash functions and descriptors. The emphasis is placed on showing how the use of a descriptor file can improve the performance of the scheme. Records in the file are given addresses according to hash functions for each field in the record. Furthermore, each page of the file has associated with it a descriptor, which is a fixed-length bit string, determined by the records actually present in the page. Before a page is accessed to see if it contains records in the answer to a query, the descriptor for the page is checked. This check may show that no relevant records are on the page and, hence, that the page does not have to be accessed. The method is shown to have a very substantial performance advantage over pure hashing schemes, when some fields in the records have large key spaces. A mathematical model of the scheme, plus an algorithm for optimizing performance, is given.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2012286840",
    "type": "article"
  },
  {
    "title": "Indexing and retrieval strategies for natural language fact retrieval",
    "doi": "https://doi.org/10.1145/319989.319995",
    "publication_date": "1983-09-01",
    "publication_year": 1983,
    "authors": "Janet L. Kolodner",
    "corresponding_authors": "Janet L. Kolodner",
    "abstract": "Researchers in artificial intelligence have recently become interested in natural language fact retrieval; currently, their research is at a point where it can begin contributing to the field of Information Retrieval. In this paper, strategies for a natural language fact retrieval system are mapped out, and approaches to many of the organization and retrieval problems are presented. The CYRUS system, which keeps track of important people and is queried in English, is presented and used to illustrate those solutions.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2022683353",
    "type": "article"
  },
  {
    "title": "Optimizing top-k queries for middleware access",
    "doi": "https://doi.org/10.1145/1206049.1206054",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Seung-won Hwang; Kevin Chen–Chuan Chang",
    "corresponding_authors": "",
    "abstract": "This article studies optimizing top- k queries in middlewares. While many assorted algorithms have been proposed, none is generally applicable to a wide range of possible scenarios. Existing algorithms lack both the “generality” to support a wide range of access scenarios and the systematic “adaptivity” to account for runtime specifics. To fulfill this critical lacking, we aim at taking a cost-based optimization approach: By runtime search over a space of algorithms, cost-based optimization is general across a wide range of access scenarios, yet adaptive to the specific access costs at runtime. While such optimization has been taken for granted for relational queries from early on, it has been clearly lacking for ranked queries. In this article, we thus identify and address the barriers of realizing such a unified framework. As the first barrier, we need to define a “comprehensive” space encompassing all possibly optimal algorithms to search over. As the second barrier and a conflicting goal, such a space should also be “focused” enough to enable efficient search. For SQL queries that are explicitly composed of relational operators, such a space, by definition, consists of schedules of relational operators (or “query plans”). In contrast, top- k queries do not have logical tasks , such as relational operators. We thus define the logical tasks of top- k queries as building blocks to identify a comprehensive and focused space for top- k queries. We then develop efficient search schemes over such space for identifying the optimal algorithm. Our study indicates that our framework not only unifies, but also outperforms existing algorithms specifically designed for their scenarios.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2140894345",
    "type": "article"
  },
  {
    "title": "Three principles of representation for semantic networks",
    "doi": "https://doi.org/10.1145/319732.319743",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "Robert L. Griffith",
    "corresponding_authors": "Robert L. Griffith",
    "abstract": "Semantic networks are so intuitive and easy to use that they are often employed without much thought as to the phenomenon of semantic nets themselves. Since they are becoming more and more a tool of artificial intelligence and now database technology, it is appropriate to focus on the principles of semantic nets. Such focus finds a harmonious and consistent base which can increase the semantic quality and usefulness of such nets. Three rules of representation are presented which achieve greater conceptual simplicity for users, simplifications in semantic net implementations and maintenance, and greater consistency across semantic net applications. These rules, applied to elements of the net itself, reveal how fundamental structures should be organized, and show that the common labeled-edge semantic net can be derived from a more primitive structure involving only nodes and membership relationships (and special nodes which represent names). Also, the correlation between binary and n -ary relations is presented.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2032068293",
    "type": "article"
  },
  {
    "title": "Physical design refinement",
    "doi": "https://doi.org/10.1145/1292609.1292618",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Nicolas Bruno; Surajit Chaudhuri",
    "corresponding_authors": "",
    "abstract": "Physical database design tools rely on a DBA-provided workload to pick an “optimal” set of indexes and materialized views. Such tools allow either creating a new such configuration or adding new structures to existing ones. However, these tools do not provide adequate support for the incremental and flexible refinement of existing physical structures. Although such refinements are often very valuable for DBAs, a completely manual approach to refinement can lead to infeasible solutions (e.g., excessive use of space). In this article, we focus on the important problem of physical design refinement and propose a transformational architecture that is based upon two novel primitive operations, called merging and reduction . These operators help refine a configuration, treating indexes and materialized views in a unified way, as well as succinctly explain the refinement process to DBAs.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2119946100",
    "type": "article"
  },
  {
    "title": "A practical guide to the design of differential files for recovery of on-line databases",
    "doi": "https://doi.org/10.1145/319758.319762",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "Houtan Aghili",
    "corresponding_authors": "Houtan Aghili",
    "abstract": "The concept of a differential file has previously been proposed as an efficient means of collecting database updates for on-line systems. This paper studies the problem of database backup and recovery for such systems, and presents an analytic model of their operation. Five key design decisions are identified and an optimization procedure for each is developed. A design algorithm that quickly provides parameters for a near-optimal differential file architecture is provided.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2162523974",
    "type": "article"
  },
  {
    "title": "An object-oriented approach to database system implementation",
    "doi": "https://doi.org/10.1145/319628.319645",
    "publication_date": "1981-12-01",
    "publication_year": 1981,
    "authors": "A. James Baroody; David J. DeWitt",
    "corresponding_authors": "",
    "abstract": "This paper examines object-oriented programming as an implementation technique for database systems. The object-oriented approach encapsulates the representations of database entities and relationships with the procedures that manipulate them. To achieve this, we first define abstractions of the modeling constructs of the data model that describe their common properties and behavior. Then we represent the entity types and relationship types in the conceptual schema and the internal schema by objects that are instances of these abstractions. The generic procedures (data manipulation routines) that comprise the user interface can now be implemented as calls to the procedures associated with these objects. A generic procedure model of database implementation techniques is presented and discussed. Several current database system implementation techniques are illustrated as examples of this model, followed by a critical analysis of our implementation technique based on the use of objects. We demonstrate that the object-oriented approach has advantages of data independence, run-time efficiency due to eliminating access to system descriptors, and support for low-level views.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1967261084",
    "type": "article"
  },
  {
    "title": "Dense multiway trees",
    "doi": "https://doi.org/10.1145/319587.319612",
    "publication_date": "1981-09-01",
    "publication_year": 1981,
    "authors": "Karel Čulík; Th. Ottmann; D. Wood",
    "corresponding_authors": "",
    "abstract": "B-trees of order m are a “balanced” class of m -ary trees, which have applications in the areas of file organization. In fact, they have been the only choice when balanced multiway trees are required. Although they have very simple insertion and deletion algorithms, their storage utilization, that is, the number of keys per page or node, is at worst 50 percent. In the present paper we investigate a new class of balanced m -ary trees, the dense multiway trees, and compare their storage utilization with that of B-trees of order m . Surprisingly, we are able to demonstrate that weakly dense multiway trees have an Ο (log 2 N ) insertion algorithm. We also show that inserting m h - 1 keys in ascending order into an initially empty dense multiway tree yields the complete m -ary tree of height h , and that at intermediate steps in the insertion sequence the intermediate trees can also be considered to be as dense as possible. Furthermore, an analysis of the limiting dynamic behavior of the dense m -ary trees under insertion shows that the average storage utilization tends to 1; that is, the trees become as dense as possible. This motivates the use of the term “dense.”",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1998826086",
    "type": "article"
  },
  {
    "title": "CASDAL",
    "doi": "https://doi.org/10.1145/320241.320246",
    "publication_date": "1978-03-01",
    "publication_year": 1978,
    "authors": "Stanley Y. W. Su; Ahmed Emam",
    "corresponding_authors": "",
    "abstract": "CASDAL is a high level data language designed and implemented for the database machine CASSM. The language is used for the manipulation and maintenance of a database using an unnormalized (hierarchically structured) relational data model. It also has facilities to define, modify, and maintain the data model definition. The uniqueness of CASDAL lies in its power to specify complex operations in terms of several new language constructs and its concepts of tagging or marking tuples and of matching values when walking from relation to relation. The language is a result of a top-down design and development effort for a database machine in which high level language constructs are directly supported by the hardware. This paper (1) gives justifications for the use of an unnormalized relational model on which the language is based, (2) presents the CASDAL language constructs with examples, and (3) describes CASSM's architecture and hardware primitives which match closely with the high level language constructs and facilitate the translation process. This paper also attempts to show how the efficiency of the language and the translation task can be achieved and simplified in a system in which the language is the result of a top-down system design and development.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2021387229",
    "type": "article"
  },
  {
    "title": "Analysis of aarchitectural features for enhancing the performance of a database machine",
    "doi": "https://doi.org/10.1145/320576.320577",
    "publication_date": "1977-12-01",
    "publication_year": 1977,
    "authors": "Esen A. Ozkarahan; K. C. Sevcik",
    "corresponding_authors": "",
    "abstract": "RAP (Relational Associative Processor) is a “back-end” database processor that is intended to take over much of the effort of database management in a computer system. In order to enhance RAP's performance its design includes mechanisms for permitting features analogous to multiprogramming and virtual memory as in general purpose computer systems. It is the purpose of this paper to present the detailed design of these mechanisms, along with some analysis that supports their value. Specifically, (1) the response time provided by RAP under several scheduling disciplines involving priority by class is analyzed, (2) the cost effectiveness of the additional hardware in RAP necessary to support multiprogramming is assessed, and (3) a detailed design of the RAP virtual memory system and its monitor is presented.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2022236446",
    "type": "article"
  },
  {
    "title": "Towards a logical reconstruction of a theory for locally closed databases",
    "doi": "https://doi.org/10.1145/1806907.1806914",
    "publication_date": "2008-07-30",
    "publication_year": 2008,
    "authors": "Marc Denecker; Álvaro Cortés-Calabuig; Maurice Bruynooghes; Ofer Arieli",
    "corresponding_authors": "",
    "abstract": "The Closed World Assumption (CWA) on databases expresses the assumption that an atom not in the database is false. This assumption is applicable only in cases where the database has complete knowledge about the domain of discourse. In this article, we investigate locally closed databases, that is: databases that are sound but partially incomplete about their domain. Such databases consist of a standard database instance, augmented with a collection of Local Closed World Assumptions (LCWAs). A LCWA is a “local” form of the CWA, expressing that a database relation is complete in a certain area, called a window of expertise . In this work, we study locally closed databases both from a knowledge representation and from a computational perspective. At the representation level, the approach taken in this article distinguishes between the data that is conveyed by a database and the metaknowledge about the area in which the data is complete. We study the semantics of the LCWA's and relate it to several knowledge representation formalisms. At the reasoning level, we study the complexity of, and algorithms for two basic reasoning tasks: computing certain and possible answers to queries and determining whether a database has complete knowledge on a query. As the complexity of these tasks is unacceptably high, we develop efficient approximate methods for query answering. We also prove that for useful classes of queries and locally closed databases, these methods are optimal , and thus they solve the original query in a tractable way. As a result, we obtain classes of queries and locally closed databases for which query answering is tractable.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2066832859",
    "type": "article"
  },
  {
    "title": "Hierarchical synopses with optimal error guarantees",
    "doi": "https://doi.org/10.1145/1386118.1386124",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Panagiotis Karras; Nikos Mamoulis",
    "corresponding_authors": "",
    "abstract": "Hierarchical synopsis structures offer a viable alternative in terms of efficiency and flexibility in relation to traditional summarization techniques such as histograms. Previous research on such structures has mostly focused on a single model, based on the Haar wavelet decomposition. In previous work, we have introduced a more refined, wavelet-inspired hierarchical index structure for synopsis construction: the Haar + tree. The chief advantages of this structure are twofold. First, it achieves higher synopsis quality at the task of summarizing data sets with sharp discontinuities than state-of-the-art histogram and Haar wavelet techniques. Second, thanks to its search space delimitation capacity, Haar + synopsis construction operates in time linear in the size of the data set for any monotonic distributive error metric. Contemporaneous research has introduced another hierarchical synopsis structure, the compact hierarchical histogram (CHH). In this article, we elaborate on both these structures. First, we formally prove that the CHH, in its default binary-hierarchy form, is a simplified variant of a Haar + tree. We then focus on the summarization problem, with both these hierarchical synopsis structures, in which an error guarantee expressed by a maximum-error metric is required. We show that this problem is most efficiently solved through its dual, space-minimization counterpart, which can also achieve optimal quality . In this case, there is a benefit to be gained by specializing the algorithm for each structure; hence, our algorithm for optimal-quality maximum-error CHH requires low polynomial time; on the other hand, optimal-quality Haar + synopses for maximum-error metrics are constructed in exponential time; hence, we also develop a low-polynomial-time approximation scheme for the maximum-error Haar + case. Furthermore, we extend our approach for both general-error and maximum-error Haar + synopses to arbitrary dimensionality. In our experimental study, (i) we confirm the theoretically expected superiority of Haar + synopses over Haar wavelet methods in both construction time and achieved quality for representative error metrics; (ii) we demonstrate that Haar + synopses are also constructed faster than optimal plain histograms, and, moreover, achieve higher synopsis quality with highly discontinuous data sets; such an advantage of a hierarchical synopsis structure over a histogram had been intuitively expressed, but never experimentally verified; and (iii) we show that Haar + synopsis quality supersedes that of a CHH.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2135134185",
    "type": "article"
  },
  {
    "title": "Small synopses for group-by query verification on outsourced data streams",
    "doi": "https://doi.org/10.1145/1567274.1567277",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Ke Yi; Li Fei-Fei; Graham Cormode; Marios Hadjieleftheriou; George Kollios; Divesh Srivastava",
    "corresponding_authors": "",
    "abstract": "Due to the overwhelming flow of information in many data stream applications, data outsourcing is a natural and effective paradigm for individual businesses to address the issue of scale. In the standard data outsourcing model, the data owner outsources streaming data to one or more third-party servers, which answer queries posed by a potentially large number of clients on the data owner's behalf. Data outsourcing intrinsically raises issues of trust, making outsourced query assurance on data streams a problem with important practical implications. Existing solutions proposed in this model all build upon cryptographic primitives such as signatures and collision-resistant hash functions, which only work for certain types of queries, for example, simple selection/aggregation queries. In this article, we consider another common type of queries, namely, “GROUP BY, SUM” queries, which previous techniques fail to support. Our new solutions are not based on cryptographic primitives, but instead use algebraic and probabilistic techniques to compute a small synopsis on the true query result, which is then communicated to the client so as to verify the correctness of the query result returned by the server. The synopsis uses a constant amount of space irrespective of the result size, has an extremely small probability of failure, and can be maintained using no extra space when the query result changes as elements stream by. We then generalize our synopsis to allow some tolerance on the number of erroneous groups, in order to support semantic load shedding on the server. When the number of erroneous groups is indeed tolerable, the synopsis can be strengthened so that we can locate and even correct these errors. Finally, we implement our techniques and perform an empirical evaluation using live network traffic.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1964376941",
    "type": "article"
  },
  {
    "title": "Databsse system approach the management decision support",
    "doi": "https://doi.org/10.1145/320493.320500",
    "publication_date": "1976-12-01",
    "publication_year": 1976,
    "authors": "John J. Donovan",
    "corresponding_authors": "John J. Donovan",
    "abstract": "Traditional intuitive methods of decision-making are no longer adequate to deal with the complex problems faced by the modern policymaker. Thus systems must be developed to provide the information and analysis necessary for the decisions which must be made. These systems are called decision support systems. Although database systems provide a key ingredient to decision support systems, the problems now facing the policymaker are different from those problems to which database systems have been applied in the past. The problems are usually not known in advance, they are constantly changing, and answers are needed quickly. Hence additional technologies, methodologies, and approaches must expand the traditional areas of database and operating systems research (as well as other software and hardware research) in order for them to become truly effective in supporting policymakers. This paper describes recent work in this area and indicates where future work is needed. Specifically the paper discusses: (1) why there exists a vital need for decision support systems; (2) examples from work in the field of energy which make explicit the characteristics which distinguish these decision support systems from traditional operational and managerial systems; (3) how an awareness of decision support systems has evolved, including a brief review of work done by others and a statement of the computational needs of decision support systems which are consistent with contemporary technology; (4) an approach which has been made to meet many of these computational needs through the development and implementation of a computational facility, the Generalized Management Information System (GMIS); and (5) the application of this computational facility to a complex and important energy problem facing New England in a typical study within the New England Energy Management Information System (NEEMIS) Project.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4253674676",
    "type": "article"
  },
  {
    "title": "Return specification inference and result clustering for keyword search on XML",
    "doi": "https://doi.org/10.1145/1735886.1735889",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Ziyang Liu; Yi Chen",
    "corresponding_authors": "",
    "abstract": "Keyword search enables Web users to easily access XML data without the need to learn a structured query language and to study possibly complex data schemas. Existing work has addressed the problem of selecting qualified data nodes that match keywords and connecting them in a meaningful way, in the spirit of inferring the where clause in XQuery. However, how to infer the return clause for keyword searches is an open problem. To address this challenge, we present a keyword search engine for data-centric XML, XSeek, to infer the semantics of the search and identify return nodes effectively. XSeek recognizes possible entities and attributes inherently represented in the data. It also distinguishes between predicates and return specifications in query keywords. Then based on the analysis of both XML data structures and keyword patterns, XSeek generates return nodes. Furthermore, when the query is ambiguous and it is hard or impossible to determine the desirable return information, XSeek clusters the query results according to their semantics based on the user-specified granularity, and enables the user to easily browse and select the desired ones. Extensive experimental studies show the effectiveness and efficiency of XSeek.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2044660360",
    "type": "article"
  },
  {
    "title": "Anonymization-based attacks in privacy-preserving data publishing",
    "doi": "https://doi.org/10.1145/1538909.1538910",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Raymond Chi-Wing Wong; Ada Wai-Chee Fu; Ke Wang; Jian Pei",
    "corresponding_authors": "",
    "abstract": "Data publishing generates much concern over the protection of individual privacy. Recent studies consider cases where the adversary may possess different kinds of knowledge about the data. In this article, we show that knowledge of the mechanism or algorithm of anonymization for data publication can also lead to extra information that assists the adversary and jeopardizes individual privacy. In particular, all known mechanisms try to minimize information loss and such an attempt provides a loophole for attacks. We call such an attack a minimality attack. In this article, we introduce a model called m -confidentiality which deals with minimality attacks, and propose a feasible solution. Our experiments show that minimality attacks are practical concerns on real datasets and that our algorithm can prevent such attacks with very little overhead and information loss.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2104120015",
    "type": "article"
  },
  {
    "title": "Capturing continuous data and answering aggregate queries in probabilistic XML",
    "doi": "https://doi.org/10.1145/2043652.2043658",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Serge Abiteboul; T-H. Hubert Chan; Evgeny Kharlamov; Werner Nutt; Pierre Senellart",
    "corresponding_authors": "",
    "abstract": "Sources of data uncertainty and imprecision are numerous. A way to handle this uncertainty is to associate probabilistic annotations to data. Many such probabilistic database models have been proposed, both in the relational and in the semi-structured setting. The latter is particularly well adapted to the management of uncertain data coming from a variety of automatic processes. An important problem, in the context of probabilistic XML databases, is that of answering aggregate queries (count, sum, avg, etc.), which has received limited attention so far. In a model unifying the various (discrete) semi-structured probabilistic models studied up to now, we present algorithms to compute the distribution of the aggregation values (exploiting some regularity properties of the aggregate functions) and probabilistic moments (especially expectation and variance) of this distribution. We also prove the intractability of some of these problems and investigate approximation techniques. We finally extend the discrete model to a continuous one, in order to take into account continuous data values, such as measurements from sensor networks, and extend our algorithms and complexity results to the continuous case.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2012548906",
    "type": "article"
  },
  {
    "title": "WHAM",
    "doi": "https://doi.org/10.1145/2389241.2389247",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Yinan Li; Jignesh M. Patel; Allison Terrell",
    "corresponding_authors": "",
    "abstract": "Over the last decade, the cost of producing genomic sequences has dropped dramatically due to the current so-called next-generation sequencing methods. However, these next-generation sequencing methods are critically dependent on fast and sophisticated data processing methods for aligning a set of query sequences to a reference genome using rich string matching models. The focus of this work is on the design, development and evaluation of a data processing system for this crucial “short read alignment” problem. Our system, called WHAM, employs hash-based indexing methods and bitwise operations for sequence alignments. It allows rich match models and it is significantly faster than the existing state-of-the-art methods. In addition, its relative speedup over the existing method is poised to increase in the future in which read sequence lengths will increase.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2048971142",
    "type": "article"
  },
  {
    "title": "Reverse data exchange",
    "doi": "https://doi.org/10.1145/1966385.1966389",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Ronald Fagin; Phokion G. Kolaitis; Lucian Popa; Wang-Chiew Tan",
    "corresponding_authors": "",
    "abstract": "An inverse of a schema mapping M is intended to undo what M does, thus providing a way to perform reverse data exchange. In recent years, three different formalizations of this concept have been introduced and studied, namely the notions of an inverse of a schema mapping, a quasi-inverse of a schema mapping, and a maximum recovery of a schema mapping. The study of these notions has been carried out in the context in which source instances are restricted to consist entirely of constants, while target instances may contain both constants and labeled nulls. This restriction on source instances is crucial for obtaining some of the main technical results about these three notions, but, at the same time, limits their usefulness, since reverse data exchange naturally leads to source instances that may contain both constants and labeled nulls. We develop a new framework for reverse data exchange that supports source instances that may contain nulls, and we thereby overcome the semantic mismatch between source and target instances of the previous formalizations. The development of this new framework requires a careful reformulation of all the important notions, including the notions of the identity schema mapping, inverse, and maximum recovery. To this effect, we introduce the notions of extended identity schema mapping, extended inverse, and maximum extended recovery, by making systematic use of the homomorphism relation on instances. We give results concerning the existence of extended inverses and of maximum extended recoveries, and results concerning their applications to reverse data exchange and query answering. Moreover, we show that maximum extended recoveries can be used to capture in a quantitative way, the amount of information loss embodied in a schema mapping specified by source-to-target tuple-generating dependencies.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1995915698",
    "type": "article"
  },
  {
    "title": "RFID-data compression for supporting aggregate queries",
    "doi": "https://doi.org/10.1145/2487259.2487263",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Bettina Fazzinga; Sergio Flesca; Filippo Furfaro; Elio Masciari",
    "corresponding_authors": "",
    "abstract": "RFID-based systems for object tracking and supply chain management have been emerging since the RFID technology proved effective in monitoring movements of objects. The monitoring activity typically results in huge numbers of readings, thus making the problem of efficiently retrieving aggregate information from the collected data a challenging issue. In fact, tackling this problem is of crucial importance, as fast answers to aggregate queries are often mandatory to support the decision making process. In this regard, a compression technique for RFID data is proposed, and used as the core of a system supporting the efficient estimation of aggregate queries. Specifically, this technique aims at constructing a lossy synopsis of the data over which aggregate queries can be estimated, without accessing the original data. Owing to the lossy nature of the compression, query estimates are approximate, and are returned along with intervals that are guaranteed to contain the exact query answers. The effectiveness of the proposed approach has been experimentally validated, showing a remarkable trade-off between the efficiency and the accuracy of the query estimation.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2033055840",
    "type": "article"
  },
  {
    "title": "Attribute and object selection queries on objects with probabilistic attributes",
    "doi": "https://doi.org/10.1145/2109196.2109199",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Rabia Nuray-Turan; Dmitri V. Kalashnikov; Sharad Mehrotra; Yaming Yu",
    "corresponding_authors": "",
    "abstract": "Modern data processing techniques such as entity resolution, data cleaning, information extraction, and automated tagging often produce results consisting of objects whose attributes may contain uncertainty. This uncertainty is frequently captured in the form of a set of multiple mutually exclusive value choices for each uncertain attribute along with a measure of probability for alternative values. However, the lay end-user, as well as some end-applications, might not be able to interpret the results if outputted in such a form. Thus, the question is how to present such results to the user in practice, for example, to support attribute-value selection and object selection queries the user might be interested in. Specifically, in this article we study the problem of maximizing the quality of these selection queries on top of such a probabilistic representation. The quality is measured using the standard and commonly used set-based quality metrics. We formalize the problem and then develop efficient approaches that provide high-quality answers for these queries. The comprehensive empirical evaluation over three different domains demonstrates the advantage of our approach over existing techniques.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2061229960",
    "type": "article"
  },
  {
    "title": "Optimal Location Queries in Road Networks",
    "doi": "https://doi.org/10.1145/2818179",
    "publication_date": "2015-10-23",
    "publication_year": 2015,
    "authors": "Zitong Chen; Yubao Liu; Raymond Chi-Wing Wong; Jiamin Xiong; Ganglin Mai; Cheng Long",
    "corresponding_authors": "",
    "abstract": "In this article, we study an optimal location query based on a road network. Specifically, given a road network containing clients and servers, an optimal location query finds a location on the road network such that when a new server is set up at this location, a certain cost function computed based on the clients and servers (including the new server) is optimized. Two types of cost functions, namely, MinMax and MaxSum, have been used for this query. The optimal location query problem with MinMax as the cost function is called the MinMax query, which finds a location for setting up a new server such that the maximum cost of a client being served by his/her closest server is minimized. The optimal location query problem with MaxSum as the cost function is called the MaxSum query, which finds a location for setting up a new server such that the sum of the weights of clients attracted by the new server is maximized. The MinMax query and the MaxSum query correspond to two types of optimal location query with the objectives defined from the clients' perspective and from the new server's perspective, respectively. Unfortunately, the existing solutions for the optimal query problem are not efficient. In this article, we propose an efficient algorithm, namely, MinMax-Alg ( MaxSum-Alg ), for the MinMax (MaxSum) query, which is based on a novel idea of nearest location component . We also discuss two extensions of the optimal location query, namely, the optimal multiple-location query and the optimal location query on a 3D road network. Extensive experiments were conducted, showing that our algorithms are faster than the state of the art by at least an order of magnitude on large real benchmark datasets. For example, in our largest real datasets, the state of the art ran for more than 10 (12) hours while our algorithm ran within 3 (2) minutes only for the MinMax (MaxSum) query, that is, our algorithm ran at least 200 (600) times faster than the state of the art.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1986384340",
    "type": "article"
  },
  {
    "title": "Domination in the Probabilistic World",
    "doi": "https://doi.org/10.1145/2602135",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Ilaria Bartolini; Paolo Ciaccia; Marco Patella",
    "corresponding_authors": "",
    "abstract": "In a probabilistic database, deciding if a tuple u is better than another tuple v has not a univocal solution, rather it depends on the specific Probabilistic Ranking Semantics (PRS) one wants to adopt so as to combine together tuples' scores and probabilities. In deterministic databases it is known that skyline queries are a remarkable alternative to (top- k ) ranking queries, because they remove from the user the burden of specifying a scoring function that combines values of different attributes into a single score. The skyline of a deterministic relation R is the set of undominated tuples in R -- tuple u dominates tuple v iff on all the attributes of interest u is better than or equal to v and strictly better on at least one attribute. Domination is equivalent to having s ( u ) ≥ s ( v ) for all monotone scoring functions s (). The skyline of a probabilistic relation R p can be similarly defined as the set of P-undominated tuples in R p , where now u P-dominates v iff, whatever monotone scoring function one would use to combine the skyline attributes, u is reputed better than v by the PRS at hand. This definition, which is applicable to arbitrary ranking semantics and probabilistic correlation models, is parametric in the adopted PRS, thus it ensures that ranking and skyline queries will always return consistent results. In this article we provide an overall view of the problem of computing the skyline of a probabilistic relation. We show how, under mild conditions that indeed hold for all known PRSs, checking P-domination can be cast into an optimization problem, whose complexity we characterize for a variety of combinations of ranking semantics and correlation models. For each analyzed case we also provide specific P-domination rules , which are exploited by the algorithm we detail for the case where the probabilistic model is known to the query processor. We also consider the case in which the probability of tuple events can only be obtained through an oracle, and describe another skyline algorithm for this loosely integrated scenario. Our experimental evaluation of P-domination rules and skyline algorithms confirms the theoretical analysis.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1989627675",
    "type": "article"
  },
  {
    "title": "Efficient range searching for categorical and plain data",
    "doi": "https://doi.org/10.1145/2543924",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Yakov Nekrich",
    "corresponding_authors": "Yakov Nekrich",
    "abstract": "In the orthogonal range-searching problem, we store a set of input points S in a data structure; the answer to a query Q is a piece of information about points in Q ∩ S , for example, the list of all points in Q ∩ S or the number of points in Q . In the colored (or categorical) range-searching problem, the set of input points is partitioned into categories; the answer to a query is a piece of information about categories of points in a query range. In this article, we describe several new results for one- and two-dimensional range-searching problems. We obtain an optimal adaptive data structure for counting the number of objects in a three-sided range and for counting categories of objects in a one-dimensional range. We also obtain new results on color range reporting in two dimensions, approximate color counting in one dimension, and some other related problems.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1994664542",
    "type": "article"
  },
  {
    "title": "Privacy-Preserving Ad-Hoc Equi-Join on Outsourced Data",
    "doi": "https://doi.org/10.1145/2629501",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "HweeHwa Pang; Xuhua Ding",
    "corresponding_authors": "",
    "abstract": "In IT outsourcing, a user may delegate the data storage and query processing functions to a third-party server that is not completely trusted. This gives rise to the need to safeguard the privacy of the database as well as the user queries over it. In this article, we address the problem of running ad hoc equi-join queries directly on encrypted data in such a setting. Our contribution is the first solution that achieves constant complexity per pair of records that are evaluated for the join. After formalizing the privacy requirements pertaining to the database and user queries, we introduce a cryptographic construct for securely joining records across relations. The construct protects the database with a strong encryption scheme. Moreover, information disclosure after executing an equi-join is kept to the minimum—that two input records combine to form an output record if and only if they share common join attribute values. There is no disclosure on records that are not part of the join result. Building on this construct, we then present join algorithms that optimize the join execution by eliminating the need to match every record pair from the input relations. We provide a detailed analysis of the cost of the algorithms and confirm the analysis through extensive experiments with both synthetic and benchmark workloads. Through this evaluation, we tease out useful insights on how to configure the join algorithms to deliver acceptable execution time in practice.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2059160950",
    "type": "article"
  },
  {
    "title": "On the Complexity of Query Result Diversification",
    "doi": "https://doi.org/10.1145/2602136",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Ting Deng; Wenfei Fan",
    "corresponding_authors": "",
    "abstract": "Query result diversification is a bi-criteria optimization problem for ranking query results. Given a database D , a query Q , and a positive integer k , it is to find a set of k tuples from Q ( D ) such that the tuples are as relevant as possible to the query, and at the same time, as diverse as possible to each other. Subsets of Q ( D ) are ranked by an objective function defined in terms of relevance and diversity. Query result diversification has found a variety of applications in databases, information retrieval, and operations research. This article investigates the complexity of result diversification for relational queries. (1) We identify three problems in connection with query result diversification, to determine whether there exists a set of k tuples that is ranked above a bound with respect to relevance and diversity, to assess the rank of a given k -element set, and to count how many k -element sets are ranked above a given bound based on an objective function. (2) We study these problems for a variety of query languages and for the three objective functions proposed in Gollapudi and Sharma [2009]. We establish the upper and lower bounds of these problems, all matching , for both combined complexity and data complexity. (3) We also investigate several special settings of these problems, identifying tractable cases. Moreover, (4) we reinvestigate these problems in the presence of compatibility constraints commonly found in practice, and provide their complexity in all these settings.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2163508424",
    "type": "article"
  },
  {
    "title": "Exploiting Integrity Constraints for Cleaning Trajectories of RFID-Monitored Objects",
    "doi": "https://doi.org/10.1145/2939368",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Bettina Fazzinga; Sergio Flesca; Filippo Furfaro; Francesco Parisi",
    "corresponding_authors": "",
    "abstract": "A probabilistic framework for cleaning the data collected by Radio-Frequency IDentification (RFID) tracking systems is introduced. What has to be cleaned is the set of trajectories that are the possible interpretations of the readings: a trajectory in this set is a sequence whose generic element is a location covered by the reader(s) that made the detection at the corresponding time point. The cleaning is guided by integrity constraints and consists of discarding the inconsistent trajectories and assigning to the others a suitable probability of being the actual one. The probabilities are evaluated by adopting probabilistic conditioning that logically consists of the following steps. First, the trajectories are assigned a priori probabilities that rely on the independence assumption between the time points. Then, these probabilities are revised according to the spatio-temporal correlations encoded by the constraints. This is done by conditioning the a priori probability of each trajectory to the event that the constraints are satisfied: this means taking the ratio of this a priori probability to the sum of the a priori probabilities of all the consistent trajectories. Instead of performing these steps by materializing all the trajectories and their a priori probabilities (which is infeasible, owing to the typically huge number of trajectories), our approach exploits a data structure called conditioned trajectory graph (ct-graph) that compactly represents the trajectories and their conditioned probabilities, and an algorithm for efficiently constructing the ct-graph, which progressively builds it while avoiding the construction of components encoding inconsistent trajectories.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2548927174",
    "type": "article"
  },
  {
    "title": "Incremental and Approximate Computations for Accelerating Deep CNN Inference",
    "doi": "https://doi.org/10.1145/3397461",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Supun Nakandala; Kabir Nagrecha; Arun Kumar; Yannis Papakonstantinou",
    "corresponding_authors": "",
    "abstract": "Deep learning now offers state-of-the-art accuracy for many prediction tasks. A form of deep learning called deep convolutional neural networks (CNNs) are especially popular on image, video, and time series data. Due to its high computational cost, CNN inference is often a bottleneck in analytics tasks on such data. Thus, a lot of work in the computer architecture, systems, and compilers communities study how to make CNN inference faster. In this work, we show that by elevating the abstraction level and re-imagining CNN inference as queries , we can bring to bear database-style query optimization techniques to improve CNN inference efficiency. We focus on tasks that perform CNN inference repeatedly on inputs that are only slightly different . We identify two popular CNN tasks with this behavior: occlusion-based explanations (OBE) and object recognition in videos (ORV). OBE is a popular method for “explaining” CNN predictions. It outputs a heatmap over the input to show which regions (e.g., image pixels) mattered most for a given prediction. It leads to many re-inference requests on locally modified inputs. ORV uses CNNs to identify and track objects across video frames. It also leads to many re-inference requests. We cast such tasks in a unified manner as a novel instance of the incremental view maintenance problem and create a comprehensive algebraic framework for incremental CNN inference that reduces computational costs. We produce materialized views of features produced inside a CNN and connect them with a novel multi-query optimization scheme for CNN re-inference. Finally, we also devise novel OBE-specific and ORV-specific approximate inference optimizations exploiting their semantics. We prototype our ideas in Python to create a tool called Krypton that supports both CPUs and GPUs. Experiments with real data and CNNs show that Krypton reduces runtimes by up to 5× (respectively, 35×) to produce exact (respectively, high-quality approximate) results without raising resource requirements.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3041449006",
    "type": "article"
  },
  {
    "title": "A Scalable Lock Manager for Multicores",
    "doi": "https://doi.org/10.1145/2691190.2691192",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Hyungsoo Jung; Hyuck Han; Alan Fekete; Gernot Heiser; Heon Y. Yeom",
    "corresponding_authors": "",
    "abstract": "Modern implementations of DBMS software are intended to take advantage of high core counts that are becoming common in high-end servers. However, we have observed that several database platforms, including MySQL, Shore-MT, and a commercial system, exhibit throughput collapse as load increases into oversaturation (where there are more request threads than cores), even for a workload with little or no logical contention for locks, such as a read-only workload. Our analysis of MySQL identifies latch contention within the lock manager as the bottleneck responsible for this collapse. We design a lock manager with reduced latching, implement it in MySQL, and show that it avoids the collapse and generally improves performance. Our efficient implementation of a lock manager is enabled by a staged allocation and deallocation of locks. Locks are preallocated in bulk, so that the lock manager only has to perform simple list manipulation operations during the acquire and release phases of a transaction. Deallocation of the lock data structures is also performed in bulk, which enables the use of fast implementations of lock acquisition and release as well as concurrent deadlock checking.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4241097186",
    "type": "article"
  },
  {
    "title": "Succinct Range Filters",
    "doi": "https://doi.org/10.1145/3375660",
    "publication_date": "2020-06-21",
    "publication_year": 2020,
    "authors": "Huanchen Zhang; Hyeontaek Lim; Viktor Leis; David G. Andersen; Michael Kaminsky; Kimberly Keeton; Andrew Pavlo",
    "corresponding_authors": "",
    "abstract": "We present the Succinct Range Filter (SuRF), a fast and compact data structure for approximate membership tests. Unlike traditional Bloom filters, SuRF supports both single-key lookups and common range queries: open-range queries, closed-range queries, and range counts. SuRF is based on a new data structure called the Fast Succinct Trie (FST) that matches the point and range query performance of state-of-the-art order-preserving indexes, while consuming only 10 bits per trie node. The false-positive rates in SuRF for both point and range queries are tunable to satisfy different application needs. We evaluate SuRF in RocksDB as a replacement for its Bloom filters to reduce I/O by filtering requests before they access on-disk data structures. Our experiments on a 100-GB dataset show that replacing RocksDB’s Bloom filters with SuRFs speeds up open-seek (without upper-bound) and closed-seek (with upper-bound) queries by up to 1.5× and 5× with a modest cost on the worst-case (all-missing) point query throughput due to slightly higher false-positive rate.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3035954614",
    "type": "article"
  },
  {
    "title": "A Formal Framework for Complex Event Recognition",
    "doi": "https://doi.org/10.1145/3485463",
    "publication_date": "2021-12-08",
    "publication_year": 2021,
    "authors": "Alejandro Grez; Cristian Riveros; Martín Ugarte; Stijn Vansummeren",
    "corresponding_authors": "",
    "abstract": "Complex event recognition (CER) has emerged as the unifying field for technologies that require processing and correlating distributed data sources in real time. CER finds applications in diverse domains, which has resulted in a large number of proposals for expressing and processing complex events. Existing CER languages lack a clear semantics, however, which makes them hard to understand and generalize. Moreover, there are no general techniques for evaluating CER query languages with clear performance guarantees. In this article, we embark on the task of giving a rigorous and efficient framework to CER. We propose a formal language for specifying complex events, called complex event logic (CEL), that contains the main features used in the literature and has a denotational and compositional semantics. We also formalize the so-called selection strategies, which had only been presented as by-design extensions to existing frameworks. We give insight into the language design trade-offs regarding the strict sequencing operators of CEL and selection strategies. With a well-defined semantics at hand, we discuss how to efficiently process complex events by evaluating CEL formulas with unary filters. We start by introducing a formal computational model for CER, called complex event automata (CEA), and study how to compile CEL formulas with unary filters into CEA. Furthermore, we provide efficient algorithms for evaluating CEA over event streams using constant time per event followed by output-linear delay enumeration of the results.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4200280487",
    "type": "article"
  },
  {
    "title": "Scotty",
    "doi": "https://doi.org/10.1145/3433675",
    "publication_date": "2021-03-27",
    "publication_year": 2021,
    "authors": "Jonas Traub; Philipp M. Grulich; Alejandro Rodríguez Cuéllar; Sebastian Breß; Asterios Katsifodimos; Tilmann Rabl; Volker Markl",
    "corresponding_authors": "",
    "abstract": "Window aggregation is a core operation in data stream processing. Existing aggregation techniques focus on reducing latency, eliminating redundant computations, or minimizing memory usage. However, each technique operates under different assumptions with respect to workload characteristics, such as properties of aggregation functions (e.g., invertible, associative), window types (e.g., sliding, sessions), windowing measures (e.g., time- or count-based), and stream (dis)order. In this article, we present Scotty , an efficient and general open-source operator for sliding-window aggregation in stream processing systems, such as Apache Flink, Apache Beam, Apache Samza, Apache Kafka, Apache Spark, and Apache Storm. One can easily extend Scotty with user-defined aggregation functions and window types. Scotty implements the concept of general stream slicing and derives workload characteristics from aggregation queries to improve performance without sacrificing its general applicability. We provide an in-depth view on the algorithms of the general stream slicing approach. Our experiments show that Scotty outperforms alternative solutions.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3150719313",
    "type": "article"
  },
  {
    "title": "Reversible Database Watermarking Based on Order-preserving Encryption for Data Sharing",
    "doi": "https://doi.org/10.1145/3589761",
    "publication_date": "2023-04-17",
    "publication_year": 2023,
    "authors": "Donghui Hu; Qing Wang; Song Y. Yan; Xiaojun Liu; Meng Li; Shuli Zheng",
    "corresponding_authors": "",
    "abstract": "In the era of big data, data sharing not only boosts the economy of the world but also brings about problems of privacy disclosure and copyright infringement. The collected data may contain users’ sensitive information; thus, privacy protection should be applied to the data prior to them being shared. Moreover, the shared data may be re-shared to third parties without the consent or awareness of the original data providers. Therefore, there is an urgent need for copyright tracking. There are few works satisfying the requirements of both privacy protection and copyright tracking. The main challenge is how to protect the shared data and realize copyright tracking while not undermining the utility of the data. In this article, we propose a novel solution of a reversible database watermarking scheme based on order-preserving encryption. First, we encrypt the data using order-preserving encryption and adjust an encryption parameter within an appropriate interval to generate a ciphertext with redundant space. Then, we leverage the redundant space to embed robust reversible watermarking. We adopt grouping and K-means to improve the embedding capacity and the robustness of the watermark. Formal theoretical analysis proves that the proposed scheme guarantees correctness and security. Results of extensive experiments show that OPEW has 100% data utility, and the robustness and efficiency of OPEW are better than existing works.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4366087555",
    "type": "article"
  },
  {
    "title": "Semantics for null extended nested relations",
    "doi": "https://doi.org/10.1145/155271.155275",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Mark Levene; George Loizou",
    "corresponding_authors": "",
    "abstract": "The nested relational model extends the flat relational model by relaxing the first normal form assumption in order to allow the modeling of complex objects. Much of the previous work on the nested relational model has concentrated on defining the data structures and query language for the model. The work done on integrity constraints in nested relations has mainly focused on characterizing subclasses of nested relations and defining normal forms for nested relations with certain desirable properties. In this paper we define the semantics of nested relations, which may contain null values, in terms of integrity constraints, called null extended data dependencies , which extend functional dependencies and join dependencies encountered in flat relational database theory. We formalize incomplete information in nested relations by allowing only one unmarked generic null value , whose semantics we do not further specify. The motivation for the choice of a generic null is our desire to investigate only fundamental semantics which are common to all unmarked null types. This lead us to define a preorder on nested relations, which allows us to measure the relative information content of nested relations. We also define a procedure, called the extended chase procedure , for testing satisfaction of null extended data dependencies and for making inferences by using these null extended data dependencies. The extended chase procedure is shown to generalize the classical chase procedure, which is of major importance in flat relational database theory. As a consequence of our approach we are able to capture the novel notion of losslessness in nested relations, called herein null extended lossless decomposition . Finally, we show that the semantics of nested relations are a natural extension of the semantics of flat relations.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1971254064",
    "type": "article"
  },
  {
    "title": "Applying formal methods to semantic-based decomposition of transactions",
    "doi": "https://doi.org/10.1145/249978.249981",
    "publication_date": "1997-06-01",
    "publication_year": 1997,
    "authors": "Paul Ammann; Sushil Jajodia; Indrakshi Ray",
    "corresponding_authors": "",
    "abstract": "In some database applications the traditional approach of seerializability, in which transactions appear to execute atomically and in isolation on a consistent database state, fails to satisfy performance requirements. Although many researchers have investigated the process of decomposing transactions into steps to increase concurrency, such research typically focuses on providing algorithms necessary to implement a decomposition supplied by the database application developer and pays relatively little attention to what constitutess a desirable decomposition or how the developer should obtain one. We focus onthe decomposition itself. A decomposition generates proof obligations whose descharge ensures desirable properties with respect to the original collection of transactions. We introduce the notion of semantic histories to formulate and prove the necessary properties, and the notion of successor sets to describe efficiently the correct interleavings of steps. The successor set constraints use information about conflicts between steps so as to take full advantage of conflict serializability at the level of steps. We propose a mechanism based on two-phase locking to generate correct stepwise serializable histories.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2008660935",
    "type": "article"
  },
  {
    "title": "A compressed accessibility map for XML",
    "doi": "https://doi.org/10.1145/1005566.1005570",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Ting Yu; Divesh Srivastava; Laks V. S. Lakshmanan; H. V. Jagadish",
    "corresponding_authors": "",
    "abstract": "XML is the undisputed standard for data representation and exchange. As companies transact business over the Internet, letting authorized customers directly access, and even modify, XML data offers many advantages in terms of cost, accuracy, and timeliness. Given the complex business relationships between companies, and the sensitive nature of information, access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to an XML data item can be extremely inefficient. The alternative of fully materializing, for each data item, the users authorized to access it can be space-inefficient. In this article, we introduce a compressed accessibility map (CAM) as a space- and time-efficient solution to the access control problem for XML data. A CAM compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data. We present a CAM lookup algorithm for determining if a user has access to a data item that takes time proportional to the product of the depth of the item in the XML data and logarithm of the CAM size. We develop an algorithm for building an optimal size CAM that takes time linear in the size of the XML data set. While optimality cannot be preserved incrementally under data item updates, we provide an algorithm for incrementally maintaining near-optimality. Finally, we experimentally demonstrate the effectiveness of the CAM for multiple users on a variety of real and synthetic data sets.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1971512992",
    "type": "article"
  },
  {
    "title": "Algorithms for trie compaction",
    "doi": "https://doi.org/10.1145/329.295",
    "publication_date": "1984-06-03",
    "publication_year": 1984,
    "authors": "Muhammad H. Alsuwaiyel; Ellis Horowitz",
    "corresponding_authors": "",
    "abstract": "The trie data structure has many properties which make it especially attractive for representing large files of data. These properties include fast retrieval time, quick unsuccessful search determination, and finding the longest match to a given identifier. The main drawback is the space requirement. In this paper the concept of trie compaction is formalized. An exact algorithm for optimal trie compaction and three algorithms for approximate trie compaction are given, and an analysis of the three algorithms is done. The analysis indicate that for actual tries, reductions of around 70 percent in the space required by the uncompacted trie can be expected. The quality of the compaction is shown to be insensitive to the number of nodes, while a more relevant parameter is the alphabet size of the key.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2002320078",
    "type": "article"
  },
  {
    "title": "Tight upper bounds on the number of candidate patterns",
    "doi": "https://doi.org/10.1145/1071610.1071611",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Floris Geerts; Bart Goethals; Jan Van den Bussche",
    "corresponding_authors": "",
    "abstract": "In the context of mining for frequent patterns using the standard levelwise algorithm, the following question arises: given the current level and the current set of frequent patterns, what is the maximal number of candidate patterns that can be generated on the next level? We answer this question by providing tight upper bounds, derived from a combinatorial result from the sixties by Kruskal and Katona. Our result is useful to secure existing algorithms from a combinatorial explosion of the number of candidate patterns.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2098682382",
    "type": "article"
  },
  {
    "title": "Sequencing XML data and query twigs for fast pattern matching",
    "doi": "https://doi.org/10.1145/1132863.1132871",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Praveen Rao; Bongki Moon",
    "corresponding_authors": "",
    "abstract": "We propose a new way of indexing XML documents and processing twig patterns in an XML database. Every XML document in the database can be transformed into a sequence of labels by prüfer's method that constructs a one-to-one correspondence between trees and sequences. During query processing, a twig pattern is also transformed into its Prüfer sequence. By performing subsequence matching on the set of sequences in the database and performing a series of refinement phases that we have developed, we can find all the occurrences of a twig pattern in the database. Our approach allows holistic processing of a twig pattern without breaking the twig into root-to-leaf paths and processing these paths individually. Furthermore, we show in the article that all correct answers are found without any false dismissals or false alarms. Experimental results demonstrate the performance benefits of our proposed techniques.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2104644740",
    "type": "article"
  },
  {
    "title": "The Sort-Merge-Shrink join",
    "doi": "https://doi.org/10.1145/1189769.1189775",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Christopher Jermaine; Alin Dobra; Subramanian Arumugam; Shantanu Joshi; Abhijit Pol",
    "corresponding_authors": "",
    "abstract": "One of the most common operations in analytic query processing is the application of an aggregate function to the result of a relational join. We describe an algorithm called the Sort-Merge-Shrink (SMS) Join for computing the answer to such a query over large, disk-based input tables. The key innovation of the SMS join is that if the input data are clustered in a statistically random fashion on disk, then at all times, the join provides an online, statistical estimator for the eventual answer to the query as well as probabilistic confidence bounds. Thus, a user can monitor the progress of the join throughout its execution and stop the join when satisfied with the estimate's accuracy or run the algorithm to completion with a total time requirement that is not much longer than that of other common join algorithms. This contrasts with other online join algorithms, which either do not offer such statistical guarantees or can only offer guarantees so long as the input data can fit into main memory.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2136014429",
    "type": "article"
  },
  {
    "title": "An efficient I/O interface for optical disks",
    "doi": "https://doi.org/10.1145/3857.3862",
    "publication_date": "1985-06-01",
    "publication_year": 1985,
    "authors": "Jeffrey Scott Vitter",
    "corresponding_authors": "Jeffrey Scott Vitter",
    "abstract": "We introduce the notion of an I/O interface for optical digital (write-once) disks, which is quite different from earlier research. The purpose of an I/O interface is to allow existing operating systems and application programs that use magnetic disks to use optical disks instead, with minimal change. We define what it means for an I/O interface to be disk-efficient. We demonstrate a practical disk- efficient I/O interface and show that its I/O performance in many cases is optimum, up to a constant factor, among all disk-efficient interfaces. The interface is most effective for applications that are not update-intensive. An additional capability is a built-in history mechanism that provides software support for accessing previous versions of records. Even if not implemented, the I/O interface can be used as a programming tool to develop efficient special purpose applications for use with optical disks.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2022663994",
    "type": "article"
  },
  {
    "title": "A parallel pipelined relational query processor",
    "doi": "https://doi.org/10.1145/329.332",
    "publication_date": "1984-06-03",
    "publication_year": 1984,
    "authors": "Won Bae Kim; Daniel D. Gajski; David J. Kuck",
    "corresponding_authors": "",
    "abstract": "This paper presents the design of a relational query processor. The query processor consists of only four processing PIPEs and a number of random-access memory modules. Each PIPE processes tuples of relations in a bit-serial, tuple-parallel manner for each of the primitive database operations which comprise a complex relational query. The design of the query processor meets three major objectives: the query processor must be manufacturable using existing and near-term LSI (VLSI) technology; it must support in a uniform manner both the numeric and nonnumeric processing requirements a high-level user interface like SQL presents; and it must support the query-processing strategy derived in the query optimizer to satisfy certain system-wide performance optimality criteria.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2077864660",
    "type": "article"
  },
  {
    "title": "Incremental maintenance of shortest distance and transitive closure in first-order logic and SQL",
    "doi": "https://doi.org/10.1145/1093382.1093384",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Chaoyi Pang; Guozhu Dong; Kotagiri Ramamohanarao",
    "corresponding_authors": "",
    "abstract": "Given a database, the view maintenance problem is concerned with the efficient computation of the new contents of a given view when updates to the database happen. We consider the view maintenance problem for the situation when the database contains a weighted graph and the view is either the transitive closure or the answer to the all-pairs shortest-distance problem ( APSD ). We give incremental algorithms for APSD , which support both edge insertions and deletions. For transitive closure, the algorithm is applicable to a more general class of graphs than those previously explored. Our algorithms use first-order queries, along with addition (+) and less-than (&lt;) operations ( FO (+,&lt;)); they store O ( n 2 ) number of tuples, where n is the number of vertices, and have AC 0 data complexity for integer weights. Since FO (+,&lt;) is a sublanguage of SQL and is supported by almost all current database systems, our maintenance algorithms are more appropriate for database applications than nondatabase query types of maintenance algorithms.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1996982345",
    "type": "article"
  },
  {
    "title": "Integrating XML data sources using approximate joins",
    "doi": "https://doi.org/10.1145/1132863.1132868",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Sudipto Guha; H. V. Jagadish; Nick Koudas; Divesh Srivastava; Ting Yu",
    "corresponding_authors": "",
    "abstract": "XML is widely recognized as the data interchange standard of tomorrow because of its ability to represent data from a variety of sources. Hence, XML is likely to be the format through which data from multiple sources is integrated. In this article, we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently, an approximate match in structure, in addition to content, has to be folded into the join operation. We quantify an approximate match in structure and content for pairs of XML documents using well defined notions of distance. We show how notions of distance that have metric properties can be incorporated in a framework for joins between XML data sources and introduce the idea of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set, and we propose sampling-based algorithms to identify them. We then instantiate our join framework using the tree edit distance between a pair of trees. We next turn our attention to utilizing well known index structures to improve the performance of approximate XML join operations. We present a methodology enabling adaptation of index structures for this problem, and we instantiate it in terms of the R-tree. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets, varying parameters of interest, and highlighting the performance benefits of our approach.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2010916971",
    "type": "article"
  },
  {
    "title": "Use of graph-theoretic models for optimal relational database accesses to perform join",
    "doi": "https://doi.org/10.1145/3148.3325",
    "publication_date": "1985-03-01",
    "publication_year": 1985,
    "authors": "Sakti Pramanik; David Ittner",
    "corresponding_authors": "",
    "abstract": "A graph model is presented to analyze the performance of a relational join. The amount of page reaccesses, the page access sequence, and the amount of buffer needed are represented in terms of graph parameters. By using the graph model formed from the index on the join attributes, we determine the relationships between these parameters. Two types of buffer allocation strategies are studied, and the upper bound on the buffer size with no page reaccess is given. This bound is shown to be the maximum cut value of a graph. Hence, the problem of computing this upper bound is NP-hard. We also give algorithms to determine a page access sequence requiring a near optimal buffer size with no page reaccess. The optimal page access sequence for a fixed buffer size has also been considered.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2023467128",
    "type": "article"
  },
  {
    "title": "Dynamic indexing for multidimensional non-ordered discrete data spaces using a data-partitioning approach",
    "doi": "https://doi.org/10.1145/1138394.1138395",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Gang Qian; Qiang Zhu; Qiang Xue; Sakti Pramanik",
    "corresponding_authors": "",
    "abstract": "Similarity searches in multidimensional Non-ordered Discrete Data Spaces (NDDS) are becoming increasingly important for application areas such as bioinformatics, biometrics, data mining and E-commerce. Efficient similarity searches require robust indexing techniques. Unfortunately, existing indexing methods developed for multidimensional (ordered) Continuous Data Spaces (CDS) such as the R-tree cannot be directly applied to an NDDS. This is because some essential geometric concepts/properties such as the minimum bounding region and the area of a region in a CDS are no longer valid in an NDDS. Other indexing methods based on metric spaces such as the M-tree and the Slim-trees are too general to effectively utilize the special characteristics of NDDSs, resulting in nonoptimized performance. In this article, we propose a new dynamic data-partitioning-based indexing technique, called the ND-tree, to support efficient similarity searches in an NDDS. The key idea is to extend the relevant geometric concepts as well as some indexing strategies used in CDSs to NDDSs. Efficient algorithms for ND-tree construction and techniques to solve relevant issues such as handling dimensions with different alphabets in an NDDS are presented. Our experimental results on synthetic data and real genome sequence data demonstrate that the ND-tree outperforms the linear scan, the M-tree and the Slim-trees for similarity searches in multidimensional NDDSs. A theoretical model is also developed to predict the performance of the ND-tree for random data.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2123837096",
    "type": "article"
  },
  {
    "title": "A note on associative processors for data management",
    "doi": "https://doi.org/10.1145/320251.320254",
    "publication_date": "1978-06-01",
    "publication_year": 1978,
    "authors": "Glen G. Langdon",
    "corresponding_authors": "Glen G. Langdon",
    "abstract": "Associative “logic-per-track” processors for data management are examined from a technological and engineering point of view. Architectural and design decisions are discussed. Some alternatives to the design of comparators, garbage collection, and domain extraction for architectures like the Relational Associative Processor (RAP) are offered.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1963657448",
    "type": "article"
  },
  {
    "title": "Pseudo-random number generation for sketch-based estimations",
    "doi": "https://doi.org/10.1145/1242524.1242528",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Florin Rusu; Alin Dobra",
    "corresponding_authors": "",
    "abstract": "The exact computation of aggregate queries, like the size of join of two relations, usually requires large amounts of memory (constrained in data-streaming) or communication (constrained in distributed computation) and large processing times. In this situation, approximation techniques with provable guarantees, like sketches, are one possible solution. The performance of sketches depends crucially on the ability to generate particular pseudo-random numbers. In this article we investigate both theoretically and empirically the problem of generating k -wise independent pseudo-random numbers and, in particular, that of generating 3- and 4-wise independent pseudo-random numbers that are fast range-summable (i.e., they can be summed in sublinear time). Our specific contributions are: (a) we provide a thorough comparison of the various pseudo-random number generating schemes; (b) we study both theoretically and empirically the fast range-summation property of 3- and 4-wise independent generating schemes; (c) we provide algorithms for the fast range-summation of two 3-wise independent schemes, BCH and extended Hamming; and (d) we show convincing theoretical and empirical evidence that the extended Hamming scheme performs as well as any 4-wise independent scheme for estimating the size of join of two relations using AMS sketches, even though it is only 3-wise independent. We use this scheme to generate estimators that significantly outperform state-of-the-art solutions for two problems, namely, size of spatial joins and selectivity estimation .",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2010016018",
    "type": "article"
  },
  {
    "title": "Experiments on the determination of the relationships between terms",
    "doi": "https://doi.org/10.1145/320071.320081",
    "publication_date": "1979-06-01",
    "publication_year": 1979,
    "authors": "Vijay V. Raghavan; C. Yu",
    "corresponding_authors": "",
    "abstract": "The retrieval effectiveness of an automatic method that uses relevance judgments for the determination of positive as well as negative relationships between terms is evaluated. The term relationships are incorporated into the retrieval process by using a generalized similarity function that has a term match component, a positive term relationship component, and a negative term relationship component. Two strategies, query partitioning and query clustering, for the evaluation of the effectiveness of the term relationships are investigated. The latter appears to be more attractive from linguistic as well as economic points of view. The positive and the negative relationships are verified to be effective both when used individually, and in combination. The importance attached to the term relationship components relative to that of term match component is found to have a substantial effect on the retrieval performance. The usefulness of discriminant analysis as a technique for determining the relative importance of these components is investigated.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2074876593",
    "type": "article"
  },
  {
    "title": "An information-theoretic analysis of worst-case redundancy in database design",
    "doi": "https://doi.org/10.1145/1670243.1670248",
    "publication_date": "2008-02-15",
    "publication_year": 2008,
    "authors": "Solmaz Kolahi; Leonid Libkin",
    "corresponding_authors": "",
    "abstract": "Normal forms that guide the process of database schema design have several key goals such as elimination of redundancies and preservation of integrity constraints, such as functional dependencies. It has long been known that complete elimination of redundancies and complete preservation of constraints cannot be achieved simultaneously. In this article, we use a recently introduced information-theoretic framework, and provide a quantitative analysis of the redundancy/integrity preservation trade-off, and give techniques for comparing different schema designs in terms of the amount of redundancy they carry. The main notion of the information-theoretic framework is that of an information content of each datum in an instance (which is a number in [0,1]): the closer to 1, the less redundancy it carries. We start by providing a combinatorial criterion that lets us calculate, for a relational schema with functional dependencies, the lowest information content in its instances. This indicates how good the schema design is in terms of allowing redundant information. We then study the normal form 3NF, which tolerates some redundancy to guarantee preservation of functional dependencies. The main result provides a formal justification for normal form 3NF by showing that this normal form pays the smallest possible price, in terms of redundancy, for achieving dependency preservation. We also give techniques for quantitative comparison of different normal forms based on the redundancy they tolerate.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1998597078",
    "type": "article"
  },
  {
    "title": "From XQuery to relational logics",
    "doi": "https://doi.org/10.1145/1620585.1620592",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Michael Benedikt; Christoph Koch",
    "corresponding_authors": "",
    "abstract": "Predicate logic has long been seen as a good foundation for querying relational data. This is embodied in the correspondence between relational calculus and first-order logic, and can also be seen in mappings from fragments of the standard relational query language SQL to extensions of first-order logic (e.g. with counting). A key question is what is the analog to this correspondence for querying tree-structured data, as seen, for example, in XML documents. We formalize this as the question of the appropriate logical query language for defining transformations on tree-structured data. The predominant practitioner paradigm for defining such transformations is top-down tree building . This is embodied by the XQuery query language, which builds the output tree in parallel starting at the root, based on variable bindings and nodeset queries in the XPath language. The goal of this article is to compare the expressiveness of top-down tree-building languages based on a benchmark of predicate logic. We start by giving a formalized XQuery XQ that can serve as a representative of the top-down approach. We show that all queries in XQ with only atomic equality are equivalent to first-order interpretations, an analog to first-order logic (FO) in the setting of transformations of tree-structured data. We then consider fragments of atomic XQ . We identify a fragment that maps efficiently into first-order, a fragment that maps into existential first-order logic, and a fragment that maps into the navigationally two-variable fragment of first-order logic—an analog of two-variable logic in the setting where data values are unbounded. When XQ is considered with deep equality, we find that queries can be translated into FO with counting ( FO (Cnt)). Translations from XQ to logical languages on relations have a number of consequences. We use them to derive complexity bounds for XQ fragments, and to bound the Boolean expressiveness of XQ fragments.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2021964585",
    "type": "article"
  },
  {
    "title": "Designing fast architecture-sensitive tree search on modern multicore/many-core processors",
    "doi": "https://doi.org/10.1145/2043652.2043655",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Changkyu Kim; Jatin Chhugani; Nadathur Satish; Eric Sedlar; Anthony D. Nguyen; Tim Kaldewey; Victor W. Lee; Scott Brandt; Pradeep Dubey",
    "corresponding_authors": "",
    "abstract": "In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join, and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this article, we present FAST, an extremely fast architecture-sensitive layout of the index tree. FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and Single Instruction Multiple Data (SIMD) width of the underlying hardware. FAST eliminates the impact of memory latency, and exploits thread-level and data-level parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second for large trees of 64M elements, with even better results on smaller trees. These are 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures. We also evaluated FAST on the Intel$^\\tiny\\textregistered$ Many Integrated Core architecture (Intel$^\\tiny\\textregistered$ MIC), showing a speedup of 2.4X--3X over CPU and 1.8X--4.4X over GPU. FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64M keys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2054816578",
    "type": "article"
  },
  {
    "title": "Online subspace skyline query processing using the compressed skycube",
    "doi": "https://doi.org/10.1145/2188349.2188357",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Tian Xia; Donghui Zhang; Fang Zheng; Cindy Chen; Jie Wang",
    "corresponding_authors": "",
    "abstract": "The skyline query can help identify the “best” objects in a multi-attribute dataset. During the past decade, this query has received considerable attention in the database research community. Most research focused on computing the “skyline” of a dataset, or the set of “skyline objects” that are not dominated by any other object. Such algorithms are not appropriate in an online system, which should respond in real time to skyline query requests with arbitrary subsets of the attributes (also called subspaces). To guarantee real-time response, an online system should precompute the skylines for all subspaces, and look up a skyline upon query. Unfortunately, because the number of subspaces is exponential to the number of attributes, such pre computation has very expensive storage cost and update cost. We propose the Compressed SkyCube (CSC) that is much more compact, yet can still return the skyline of any subspace without consulting the base table. The CSC therefore combines the advantage of precomputation in that it can respond to queries in real time, and the advantage of no-precomputation in that it has efficient space cost and update cost. This article presents the CSC data structures, the CSC query algorithm, the CSC update algorithm, and the CSC initial computation scheme. A solution to extend to high-dimensional data is also proposed.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2166583424",
    "type": "article"
  },
  {
    "title": "Answering FO+MOD Queries under Updates on Bounded Degree Databases",
    "doi": "https://doi.org/10.1145/3232056",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Christoph Berkholz; Jens Keppeler; Nicole Schweikardt",
    "corresponding_authors": "",
    "abstract": "We investigate the query evaluation problem for fixed queries over fully dynamic databases, where tuples can be inserted or deleted. The task is to design a dynamic algorithm that immediately reports the new result of a fixed query after every database update. We consider queries in first-order logic (FO) and its extension with modulo-counting quantifiers (FO+MOD) and show that they can be efficiently evaluated under updates, provided that the dynamic database does not exceed a certain degree bound. In particular, we construct a data structure that allows us to answer a Boolean FO+MOD query and to compute the size of the result of a non-Boolean query within constant time after every database update. Furthermore, after every database update, we can update the data structure in constant time such that afterwards we are able to test within constant time for a given tuple whether or not it belongs to the query result, to enumerate all tuples in the new query result, and to enumerate the difference between the old and the new query result with constant delay between the output tuples. The preprocessing time needed to build the data structure is linear in the size of the database. Our results extend earlier work on the evaluation of first-order queries on static databases of bounded degree and rely on an effective Hanf normal form for FO+MOD recently obtained by Heimberg, Kuske, and Schweikardt (LICS 2016).",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2888177241",
    "type": "article"
  },
  {
    "title": "Optimality of Clustering Properties of Space-Filling Curves",
    "doi": "https://doi.org/10.1145/2556686",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Pan Xu; Srikanta Tirthapura",
    "corresponding_authors": "",
    "abstract": "Space-filling curves have been used in the design of data structures for multidimensional data for many decades. A fundamental quality metric of a space-filling curve is its “clustering number” with respect to a class of queries, which is the average number of contiguous segments on the space-filling curve that a query region can be partitioned into. We present a characterization of the clustering number of a general class of space-filling curves, as well as the first nontrivial lower bounds on the clustering number for any space-filling curve. Our results answer questions that have been open for more than 15 years.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1970805642",
    "type": "article"
  },
  {
    "title": "<i>XLynx</i> —An FPGA-based XML filter for hybrid XQuery processing",
    "doi": "https://doi.org/10.1145/2536800",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Jens Teubner; Louis Woods; Chongling Nie",
    "corresponding_authors": "",
    "abstract": "While offering unique performance and energy-saving advantages, the use of Field-Programmable Gate Arrays (FPGAs) for database acceleration has demanded major concessions from system designers. Either the programmable chips have been used for very basic application tasks (such as implementing a rigid class of selection predicates) or their circuit definition had to be completely recompiled at runtime—a very CPU-intensive and time-consuming effort. This work eliminates the need for such concessions. As part of our XLynx implementation—an FPGA-based XML filter—we present skeleton automata , which is a design principle for data-intensive hardware circuits that offers high expressiveness and quick reconfiguration at the same time. Skeleton automata provide a generic implementation for a class of finite-state automata . They can be parameterized to any particular automaton instance in a matter of microseconds or less (as opposed to minutes or hours for complete recompilation). We showcase skeleton automata based on XML projection [Marian and Siméon 2003], a filtering technique that illustrates the feasibility of our strategy for a real-world and challenging task. By performing XML projection in hardware and filtering data in the network, we report on performance improvements of several factors while remaining nonintrusive to the back-end XML processor (we evaluate XLynx using the Saxon engine).",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2010090589",
    "type": "article"
  },
  {
    "title": "TriAL",
    "doi": "https://doi.org/10.1145/3154385",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Leonid Libkin; Juan L. Reutter; Adrián Soto; Domagoj Vrgoč",
    "corresponding_authors": "",
    "abstract": "Navigational queries over RDF data are viewed as one of the main applications of graph query languages, and yet the standard model of graph databases—essentially labeled graphs—is different from the triples-based model of RDF. While encodings of RDF databases into graph data exist, we show that even the most natural ones are bound to lose some functionality when used in conjunction with graph query languages. The solution is to work directly with triples, but then many properties taken for granted in the graph database context (e.g., reachability) lose their natural meaning. Our goal is to introduce languages that work directly over triples and are closed, i.e., they produce sets of triples, rather than graphs. Our basic language is called TriAL, or Triple Algebra: it guarantees closure properties by replacing the product with a family of join operations. We extend TriAL with recursion and explain why such an extension is more intricate for triples than for graphs. We present a declarative language, namely a fragment of datalog, capturing the recursive algebra. For both languages, the combined complexity of query evaluation is given by low-degree polynomials. We compare our language with previously studied graph query languages such as adaptations of XPath, regular path queries, and nested regular expressions; many of these languages are subsumed by the recursive triple algebra. We also provide an implementation of recursive TriAL on top of a relational query engine, and we show its usefulness by running a wide array of navigational queries over real-world RDF data, while at the same time testing how our implementation compares to existing RDF systems.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2766594194",
    "type": "article"
  },
  {
    "title": "Dichotomies for Evaluating Simple Regular Path Queries",
    "doi": "https://doi.org/10.1145/3331446",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Wim Martens; Tina Trautner",
    "corresponding_authors": "",
    "abstract": "Regular path queries (RPQs) are a central component of graph databases. We investigate decision and enumeration problems concerning the evaluation of RPQs under several semantics that have recently been considered: arbitrary paths, shortest paths, paths without node repetitions (simple paths), and paths without edge repetitions (trails). Whereas arbitrary and shortest paths can be dealt with efficiently, simple paths and trails become computationally difficult already for very small RPQs. We study RPQ evaluation for simple paths and trails from a parameterized complexity perspective and define a class of simple transitive expressions that is prominent in practice and for which we can prove dichotomies for the evaluation problem. We observe that, even though simple path and trail semantics are intractable for RPQs in general, they are feasible for the vast majority of RPQs that are used in practice. At the heart of this study is a result of independent interest: the two disjoint paths problem in directed graphs is W[1]-hard if parameterized by the length of one of the two paths.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2998210606",
    "type": "article"
  },
  {
    "title": "The Exact Complexity of the First-Order Logic Definability Problem",
    "doi": "https://doi.org/10.1145/2886095",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Marcelo Arenas; Gonzalo I. Diaz",
    "corresponding_authors": "",
    "abstract": "We study the definability problem for first-order logic, denoted by FO-D ef . The input of FO-D ef is a relational database instance I and a relation R ; the question to answer is whether there exists a first-order query Q (or, equivalently, a relational algebra expression Q ) such that Q evaluated on I gives R as an answer. Although the study of FO-D ef dates back to 1978, when the decidability of this problem was shown, the exact complexity of FO-D ef remains as a fundamental open problem. In this article, we provide a polynomial-time algorithm for solving FO-D ef that uses calls to a graph-isomorphism subroutine (or oracle). As a consequence, the first-order definability problem is found to be complete for the class GI of all problems that are polynomial-time Turing reducible to the graph isomorphism problem, thus closing the open question about the exact complexity of this problem. The technique used is also applied to a generalized version of the problem that accepts a finite set of relation pairs, and whose exact complexity was also open; this version is also found to be GI -complete.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2347545322",
    "type": "article"
  },
  {
    "title": "Building a Hybrid Warehouse",
    "doi": "https://doi.org/10.1145/2972950",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Yuanyuan Tian; Fatma Özcan; Tao Zou; Romulo Gonçalves; Hamid Pirahesh",
    "corresponding_authors": "",
    "abstract": "The Hadoop Distributed File System (HDFS) has become an important data repository in the enterprise as the center for all business analytics, from SQL queries and machine learning to reporting. At the same time, enterprise data warehouses (EDWs) continue to support critical business analytics. This has created the need for a new generation of a special federation between Hadoop-like big data platforms and EDWs, which we call the hybrid warehouse . There are many applications that require correlating data stored in HDFS with EDW data, such as the analysis that associates click logs stored in HDFS with the sales data stored in the database. All existing solutions reach out to HDFS and read the data into the EDW to perform the joins, assuming that the Hadoop side does not have efficient SQL support. In this article, we show that it is actually better to do most data processing on the HDFS side, provided that we can leverage a sophisticated execution engine for joins on the Hadoop side. We identify the best hybrid warehouse architecture by studying various algorithms to join database and HDFS tables. We utilize Bloom filters to minimize the data movement and exploit the massive parallelism in both systems to the fullest extent possible. We describe a new zigzag join algorithm and show that it is a robust join algorithm for hybrid warehouses that performs well in almost all cases. We further develop a sophisticated cost model for the various join algorithms and show that it can facilitate query optimization in the hybrid warehouse to correctly choose the right algorithm under different predicate and join selectivities.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2548933032",
    "type": "article"
  },
  {
    "title": "BonXai",
    "doi": "https://doi.org/10.1145/3105960",
    "publication_date": "2017-08-24",
    "publication_year": 2017,
    "authors": "Wim Martens; Frank Neven; Matthias Niewerth; Thomas Schwentick",
    "corresponding_authors": "",
    "abstract": "While the migration from DTD to XML Schema was driven by a need for increased expressivity and flexibility, the latter was also significantly more complex to use and understand. Whereas DTDs are characterized by their simplicity, XML Schema Documents are notoriously difficult. In this article, we introduce the XML specification language BonXai, which incorporates many features of XML Schema but is arguably almost as easy to use as DTDs. In brief, the latter is achieved by sacrificing the explicit use of types in favor of simple patterns expressing contexts for elements. The goal of BonXai is not to replace XML Schema but rather to provide a simpler alternative for users who want to go beyond the expressiveness and features of DTD but do not need the explicit use of types. Furthermore, XML Schema processing tools can be used as a back-end for BonXai, since BonXai can be automatically converted into XML Schema. A particularly strong point of BonXai is its solid foundation rooted in a decade of theoretical work around pattern-based schemas. We present a formal model for a core fragment of BonXai and the translation algorithms to and from a core fragment of XML Schema. We prove that BonXai and XML Schema can be converted back-and-forth on the level of tree languages and we formally study the size trade-offs between the two languages.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2748192063",
    "type": "article"
  },
  {
    "title": "Negative Factor",
    "doi": "https://doi.org/10.1145/2847525",
    "publication_date": "2016-01-20",
    "publication_year": 2016,
    "authors": "Xiaochun Yang; Tao Qiu; Bin Wang; Baihua Zheng; Yaoshu Wang; Chen Li",
    "corresponding_authors": "",
    "abstract": "The problem of finding matches of a regular expression (RE) on a string exists in many applications, such as text editing, biosequence search, and shell commands. Existing techniques first identify candidates using substrings in the RE, then verify each of them using an automaton. These techniques become inefficient when there are many candidate occurrences that need to be verified. In this article, we propose a novel technique that prunes false negatives by utilizing negative factors , which are substrings that cannot appear in an answer. A main advantage of the technique is that it can be integrated with many existing algorithms to improve their efficiency significantly. We present a detailed description of this technique. We develop an efficient algorithm that utilizes negative factors to prune candidates, then improve it by using bit operations to process negative factors in parallel. We show that negative factors, when used with necessary factors (substrings that must appear in each answer), can achieve much better pruning power. We analyze the large number of negative factors, and develop an algorithm for finding a small number of high-quality negative factors. We conducted a thorough experimental study of this technique on real datasets, including DNA sequences, proteins, and text documents, and show significant performance improvement of the state-of-the-art tools by an order of magnitude.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2302109632",
    "type": "article"
  },
  {
    "title": "Private and Scalable Execution of SQL Aggregates on a Secure Decentralized Architecture",
    "doi": "https://doi.org/10.1145/2894750",
    "publication_date": "2016-08-08",
    "publication_year": 2016,
    "authors": "Quoc-Cuong To; Benjamin Nguyen; Philippe Pucheral",
    "corresponding_authors": "",
    "abstract": "Current applications, from complex sensor systems (e.g., quantified self) to online e-markets, acquire vast quantities of personal information that usually end up on central servers where they are exposed to prying eyes. Conversely, decentralized architectures that help individuals keep full control of their data complexify global treatments and queries, impeding the development of innovative services. This article aims precisely at reconciling individual's privacy on one side and global benefits for the community and business perspectives on the other. It promotes the idea of pushing the security to secure hardware devices controlling the data at the place of their acquisition. Thanks to these tangible physical elements of trust, secure distributed querying protocols can reestablish the capacity to perform global computations, such as Structured Query Language (SQL) aggregates, without revealing any sensitive information to central servers. This article studies how to secure the execution of such queries in the presence of honest-but-curious and malicious attackers. It also discusses how the resulting querying protocols can be integrated in a concrete decentralized architecture. Cost models and experiments on SQL/Asymmetric Architecture (AA), our distributed prototype running on real tamper-resistant hardware, demonstrate that this approach can scale to nationwide applications.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2515154381",
    "type": "article"
  },
  {
    "title": "Stream Data Cleaning under Speed and Acceleration Constraints",
    "doi": "https://doi.org/10.1145/3465740",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Shaoxu Song; Fei Gao; Aoqian Zhang; Jianmin Wang; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Stream data are often dirty, for example, owing to unreliable sensor reading or erroneous extraction of stock prices. Most stream data cleaning approaches employ a smoothing filter, which may seriously alter the data without preserving the original information. We argue that the cleaning should avoid changing those originally correct/clean data, a.k.a. the minimum modification rule in data cleaning. To capture the knowledge about what is clean , we consider the (widely existing) constraints on the speed and acceleration of data changes, such as fuel consumption per hour, daily limit of stock prices, or the top speed and acceleration of a car. Guided by these semantic constraints, in this article, we propose the constraint-based approach for cleaning stream data. It is notable that existing data repair techniques clean (a sequence of) data as a whole and fail to support stream computation. To this end, we have to relax the global optimum over the entire sequence to the local optimum in a window. Rather than the commonly observed NP-hardness of general data repairing problems, our major contributions include (1) polynomial time algorithm for global optimum, (2) linear time algorithm towards local optimum under an efficient median-based solution , and (3) experiments on real datasets demonstrate that our method can show significantly lower L1 error than the existing approaches such as smoother.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3202226255",
    "type": "article"
  },
  {
    "title": "Efficient Sorting, Duplicate Removal, Grouping, and Aggregation",
    "doi": "https://doi.org/10.1145/3568027",
    "publication_date": "2022-11-10",
    "publication_year": 2022,
    "authors": "Thanh Do; Goetz Graefe; Jeffrey F. Naughton",
    "corresponding_authors": "",
    "abstract": "Database query processing requires algorithms for duplicate removal, grouping, and aggregation. Three algorithms exist: in-stream aggregation is most efficient by far but requires sorted input; sort-based aggregation relies on external merge sort; and hash aggregation relies on an in-memory hash table plus hash partitioning to temporary storage. Cost-based query optimization chooses which algorithm to use based on several factors, including the sort order of the input, input and output sizes, and the need for sorted output. For example, hash-based aggregation is ideal for output smaller than the available memory (e.g., Query 1 of TPC-H), whereas sorting the entire input and aggregating after sorting are preferable when both aggregation input and output are large and the output needs to be sorted for a subsequent operation such as a merge join. Unfortunately, the size information required for a sound choice is often inaccurate or unavailable during query optimization, leading to sub-optimal algorithm choices. In response, this article introduces a new algorithm for sort-based duplicate removal, grouping, and aggregation. The new algorithm always performs at least as well as both traditional hash-based and traditional sort-based algorithms. It can serve as a system’s only aggregation algorithm for unsorted inputs, thus preventing erroneous algorithm choices. Furthermore, the new algorithm produces sorted output that can speed up subsequent operations. Google’s F1 Query uses the new algorithm in production workloads that aggregate petabytes of data every day.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4308883303",
    "type": "article"
  },
  {
    "title": "Declarative updates of relational databases",
    "doi": "https://doi.org/10.1145/202106.202110",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Weidong Chen",
    "corresponding_authors": "Weidong Chen",
    "abstract": "This article presents a declarative language, called update calculus , of relational database updates. A formula in update calculus involves conditions for the current database, as well as assertions about a new database. Logical connectives and quantifiers become constructors of complex updates, offering flexible specifications of database transformations. Update calculus can express all nondeterministic database transformations that are polynomial time. For set-at-a-time evaluation of updates, we present a corresponding update algebra . Existing techniques of query processing can be incorporated into update evaluation. We show that updates in update calculus can be translated into expressions in update algebra and vice versa.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2014409122",
    "type": "article"
  },
  {
    "title": "Type-checking OQL queries in the ODMG type systems",
    "doi": "https://doi.org/10.1145/328939.328943",
    "publication_date": "1999-09-01",
    "publication_year": 1999,
    "authors": "Suad Alagić",
    "corresponding_authors": "Suad Alagić",
    "abstract": "Several negative results are proved about the ability to type-check queries in the only existing proposed standard for object-oriented databases. The first of these negative results is that it is not possible to type-check OQL queries in the type system underlying the ODMG object model and its definition language ODL. The second negative result is that OQL queries cannot be type-checked in the type system of the Java binding of the ODMG standard either. A solution proposed in this paper is to extend the ODMG object model with explicit support for parametric polymorphism (universal type quantification). These results show that Java cannot be a viable database programming language unless extended with parametric polymorphism. This is why type-checking OQL queries presents no problem for the type system of the C++ binding of the ODMG standard. However, a type system that is strictly more powerful than any of the type systems of the ODMG standard is required in order to properly type ordered collectgions and indices. The required form of polymorphism is bounded type quantification (constrained genericity) and even F-bounded polymorphism. A further result is that neither static nor the standard dynamic object-oriented type-checking is possible for Java OQL, in spite of the fact that Java OQL combines features of two strongly and mostly statically-typed languages. Contrary to one of the promises of object-oriented database technology, this result shows that the impedance mismatch does not disappear in the ODMG standard. A type-safe reflective technique is proposed for overcoming this mismatch.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2021365199",
    "type": "article"
  },
  {
    "title": "Database design for incomplete relations",
    "doi": "https://doi.org/10.1145/310701.310712",
    "publication_date": "1999-03-01",
    "publication_year": 1999,
    "authors": "Mark Levene; George Loizou",
    "corresponding_authors": "",
    "abstract": "Although there has been a vast amount of research in the area of relational database design, to our knowledge, there has been very little work that considers whether this theory is still valid when relations in the database may be incomplete. When relations are incomplete and thus contain null values the problem of whether satisfaction is additive arises. Additivity is the property of the equivalence of the satisfaction of a set of functional dependencies (FDs) F with the individual satisfaction of each member of F in an incomplete relation. It is well known that in general, satisfaction of FDs is not additive. Previously we have shown that satisfaction is additive if and only if the set of FDs is monodependent. We conclude that monodependence is a fundamental desirable property of a set of FDs when considering incomplete information in relational database design. We show that, when the set of FDs F either satifies the intersection property or the split-freeness property, then the problem of finding an optimum cover of F can be solved in polynomial time in the size of F; in general, this problem is known to be NP-complete. We also show that when F satisfies the split-freeness property then deciding whether there is a superkey of cardinality k or less can be solved in polynomial time in the size of F, since all the keys have the same cardinality. If F only satisfies the intersection property then this problem is NP-complete, as in the general case. Moreover, we show that when F either satisfies the intersection property or the split-freeness property then deciding whether an attribute is prime can be solved in polynomial time in the size of F; in general, this problem is known to be NP-complete. Assume that a relation schema R is an appropriate normal form with respect to a set of FDs F. We show that when F satisfies the intersection property then the notions of second normal form and third normal form are equivalent. We also show that when R is in Boyce-Codd Normal Form (BCNF), then F is monodependent if and only if either there is a unique key for R, or for all keys X for R, the cardinality of X is one less than the number of attributes associated with R. Finally, we tackle a long-standing problem in relational database theory by showing that when a set of FDs F over R satisfies the intersection property, it also satisfies the split-freeness property (i.e., is monodependent), if and only if every lossless join decomposition of R with respect to F is also dependecy preserving. As a corollary of this result we are able to show that when F satisfies the intersection property, it also satisfies the intersection property, it also satisfies the split-freeness property(i.e., is monodependent), if and only if every lossless join decomposition of R, which is in BCNF, is also dependency preserving. Our final result is that when F is monodependent, then there exists a unique optimum lossless join decomposition of R, which is in BCNF, and is also dependency preserving. Furthermore, this ultimate decomposition can be attained in polynomial time in the size of F.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2027940820",
    "type": "article"
  },
  {
    "title": "The deductive synthesis of database transactions",
    "doi": "https://doi.org/10.1145/169725.169716",
    "publication_date": "1993-12-01",
    "publication_year": 1993,
    "authors": "Xiaolei Qian",
    "corresponding_authors": "Xiaolei Qian",
    "abstract": "article Free Access Share on The deductive synthesis of database transactions Author: Xiaolei Qian Stanford University Stanford UniversityView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 18Issue 4Dec. 1993 pp 626–677https://doi.org/10.1145/169725.169716Published:01 December 1993Publication History 15citation484DownloadsMetricsTotal Citations15Total Downloads484Last 12 Months13Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2143661363",
    "type": "article"
  },
  {
    "title": "File organization using composite perfect hashing",
    "doi": "https://doi.org/10.1145/63500.63521",
    "publication_date": "1989-06-01",
    "publication_year": 1989,
    "authors": "M. Ramakrishna; Per-Åke Larson",
    "corresponding_authors": "",
    "abstract": "Perfect hashing refers to hashing with no overflows. We propose and analyze a composite perfect hashing scheme for large external files. The scheme guarantees retrieval of any record in a single disk access. Insertions and deletions are simple, and the file size may vary considerably without adversely affecting the performance. A simple variant of the scheme supports efficient range searches in addition to being a completely dynamic file organization scheme. These advantages are achieved at the cost of a small amount of additional internal storage and increased cost of insertions.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2047704707",
    "type": "article"
  },
  {
    "title": "On compile-time query optimization in deductive databases by means of static filtering",
    "doi": "https://doi.org/10.1145/88636.87121",
    "publication_date": "1990-09-01",
    "publication_year": 1990,
    "authors": "Michael Kifer; Eliezer L. Lozinskii",
    "corresponding_authors": "",
    "abstract": "We extend the query optimization techniques known as algebric manipulations with relational expressions [48] to work with deductive databases. In particular, we propose a method for moving data-independent selections and projections into recursive axioms, which extends all other known techniques for performing that task [2, 3, 9, 18, 20]. We also show that, in a well-defined sense, our algorithm is optimal among the algorithms that propagate data-independent selections through recursion.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2139298964",
    "type": "article"
  },
  {
    "title": "Federated database systems for managing distributed, heterogeneous, and autonomous databases",
    "doi": null,
    "publication_date": "1990-01-01",
    "publication_year": 1990,
    "authors": "Amit Sheth",
    "corresponding_authors": "Amit Sheth",
    "abstract": "",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2279590846",
    "type": "article"
  },
  {
    "title": "A clustered search algorithm incorporating arbitrary term dependencies",
    "doi": "https://doi.org/10.1145/319732.319756",
    "publication_date": "1982-09-01",
    "publication_year": 1982,
    "authors": "K.C. Lam; C. Yu",
    "corresponding_authors": "",
    "abstract": "The documents in a database are organized into clusters, where each cluster contains similar documents and a representative of these documents. A user query is compared with all the representatives of the clusters, and on the basis of such comparisons, those clusters having many close neighbors with respect to the query are selected for searching. This paper presents an estimation of the number of close neighbors in a cluster in relation to the given query. The estimation takes into consideration the dependencies between terms. It is demonstrated by experiments that the estimate is accurate and the time to generate the estimate is small.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2012893546",
    "type": "article"
  },
  {
    "title": "Logical, internal, and physical reference behavior in CODASYL database systems",
    "doi": "https://doi.org/10.1145/329.331",
    "publication_date": "1984-06-03",
    "publication_year": 1984,
    "authors": "Wolfgang Effelsberg; Mary E. S. Loomis",
    "corresponding_authors": "",
    "abstract": "This work investigates one aspect of the performance of CODASYL database systems: the data reference behavior. We introduce a model of database traversals at three levels: the logical, internal, and physical levels. The mapping between the logical and internal levels is defined by the internal schema, whereas the mapping between the internal and the physical levels depends on cluster properties of the database. Our model explains the physical reference behavior for a given sequence of DML statements at the logical level. Software has been implemented to monitor references in two selected CODASYL DBMS applications. In a series of experiments the physical reference behavior was observed for varying internal schemas and cluster properties of the database. The measurements were limited to retrieval transactions, so that a variety of queries could be analyzed for the same well-known state of the database. Also, all databases were relatively small in order to allow fast reloading with varying internal schema parameters. In all cases, the database transactions showed less locality of reference than do programs under virtual memory operating systems; some databases showed no locality at all. No evidence of physical sequentiality was found. This suggests that standard page replacement strategies are not optimal for CODASYL database buffer management; instead, replacement decisions in a database buffer should be based on specific knowledge available from higher system layers.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2042138964",
    "type": "article"
  },
  {
    "title": "Compromising statistical databases responding to queries about means",
    "doi": "https://doi.org/10.1145/319830.319834",
    "publication_date": "1983-03-01",
    "publication_year": 1983,
    "authors": "Wiebren de Jonge",
    "corresponding_authors": "Wiebren de Jonge",
    "abstract": "This paper describes how to compromise a statistical database which only answers queries about arithmetic means for query sets whose cardinality falls in the range [ k, N - k ], for some k &gt; 0, where N ≥ 2 k is the number of records in the database. The compromise is shown to be easy and to require only a little preknowledge; knowing the cardinality of just one nonempty query set is usually sufficient. This means that not only count and sum queries, but also queries for arithmetic means can be extremely dangerous for the security of a statistical database, and that this threat must be taken into account explicitly by protective measures. This seems quite important from a practical standpoint: while arithmetic means were known for some time to be not altogether harmless, the (perhaps surprising) extent of the threat is now shown.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2048939721",
    "type": "article"
  },
  {
    "title": "A formal approach to the definition and the design of conceptual schemata for databased systems",
    "doi": "https://doi.org/10.1145/319682.319695",
    "publication_date": "1982-03-01",
    "publication_year": 1982,
    "authors": "Carlo Zaniolo; Michel A. Melkaoff",
    "corresponding_authors": "",
    "abstract": "A formal approach is proposed to the definition and the design of conceptual database diagrams to be used as conceptual schemata in a system featuring a multilevel schema architecture, and as an aid for the design of other forms of schemata. We consider E-R (entity-relationship) diagrams, and we introduce a new representation called CAZ -graphs. A rigorous connection is established between these diagrams and some formal constraints used to describe relationships in the framework of the relational data model. These include functional and multivalued dependencies of database relations. The basis for our schemata is a combined representation for two fundamental structures underlying every relation: the first defined by its minimal atomic decompositions, the second by its elementary functional dependencies. The interaction between these two structures is explored, and we show that, jointly, they can represent a wide spectrum of database relationships, of which the well-known one-to-one, one-to-many, and many-to-many associations constitute only a small subset. It is suggested that a main objective in conceptual schema design is to ensure a complete representation of these two structures. A procedure is presented to design schemata which obtain this objective while eliminating redundancy. A simple correspondence between the topological properties of these schemata and the structure of multivalued dependencies of the original relation is established. Various applications are discussed and a number of illustrative examples are given.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2105155294",
    "type": "article"
  },
  {
    "title": "Performance enhancements to a relational database system",
    "doi": "https://doi.org/10.1145/319983.319984",
    "publication_date": "1983-06-01",
    "publication_year": 1983,
    "authors": "Michael Stonebraker; J. Woodfill; Jeff Ranstrom; Marguerite C. Murphy; Marc De Meyer; Eric Allman",
    "corresponding_authors": "",
    "abstract": "In this paper we examine four performance enhancements to a database management system: dynamic compilation, microcoded routines, a special-purpose file system, and a special-purpose operating system. All were examined in the context of the INGRES database management system. Benchmark timings that are included suggest the attractiveness of dynamic compilation and a special-purpose file system. Microcode and a special-purpose operating system are analyzed and appear to be of more limited utility in the INGRES context.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2150227404",
    "type": "article"
  },
  {
    "title": "On interpretations of relational languages and solutions to the implied constraint problem",
    "doi": "https://doi.org/10.1145/319702.319730",
    "publication_date": "1982-06-01",
    "publication_year": 1982,
    "authors": "Barry E. Jacobs; Anthony C. Klug; Alan R. Aronson",
    "corresponding_authors": "",
    "abstract": "The interconnection between conceptual and external levels of a relational database is made precise in terms of the notion of “interpretation” between first-order languages. This is then used to obtain a methodology for discovering constraints at the external level that are “implied” by constraints at the conceptual level and by conceptual-to-external mappings. It is also seen that these concepts are important in other database issues, namely, automatic program conversion, database design, and compile-time error checking of embedded database languages. Although this the deals exclusively with the relational approach, it also discusses how these ideas can be extended to hierarchical and network databases.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2094875247",
    "type": "article"
  },
  {
    "title": "Forward node-selecting queries over trees",
    "doi": "https://doi.org/10.1145/1206049.1206052",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Dan Olteanu",
    "corresponding_authors": "Dan Olteanu",
    "abstract": "Node-selecting queries over trees lie at the core of several important XML languages for the web, such as the node-selection language XPath, the query language XQuery, and the transformation language XSLT. The main syntactic constructs of such queries are the backward predicates, for example, ancestor and preceding, and the forward predicates, for example, descendant and following. Forward predicates are included in the depth-first, left-to-right preorder relation associated with the input tree, whereas backward predicates are included in the inverse of this preorder relation. This work is devoted to an expressiveness study of node-selecting queries with proven theoretical and practical applicability, especially in the field of query evaluation against XML streams. The main question it answers positively is whether, for each input query with forward and backward predicates, there exists an equivalent forward-only output query. This question is then positively answered for input and output queries of varying structural complexity, using LOGLIN and PSPACE reductions. Various existing applications based on the results of this work are reported, including query optimization and streamed evaluation.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2095120159",
    "type": "article"
  },
  {
    "title": "On user criteria for data model evaluation",
    "doi": "https://doi.org/10.1145/320493.320504",
    "publication_date": "1976-12-01",
    "publication_year": 1976,
    "authors": "W. C. McGee",
    "corresponding_authors": "W. C. McGee",
    "abstract": "The emergence of a database technology in recent years has focused interest on the subject of data models. A data model is the class of logical data structures which a computer system or language makes available to the user for the purpose of formulating data processing applications. The diversity of computer systems and languages has resulted in a corresponding diversity of data models, and has created a problem for the user in selecting a data model which is in some sense appropriate to a given application. An evaluation procedure is needed which will allow the user to evaluate alternative models in the context of a specific set of applications. This paper takes a first step toward such a procedure by identifying the attributes of a data model which can be used as criteria for evaluating the model. Two kinds of criteria are presented: use criteria, which measure the usability of the model; and implementation criteria, which measure the implementability of the model and the efficiency of the resulting implementation. The use of the criteria is illustrated by applying them to three specific models: an n -ary relational model, a hierarchic model, and a network model.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2058569229",
    "type": "article"
  },
  {
    "title": "Exporting and interactively querying Web service-accessed sources",
    "doi": "https://doi.org/10.1145/1292609.1292612",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Michalis Petropoulos; Alin Deutsch; Yannis Papakonstantinou; Yannis Katsis",
    "corresponding_authors": "",
    "abstract": "The CLIDE System assists the owners of sources that participate in Web service-based data publishing systems to publish a restricted set of parameterized queries over the schema of their sources and package them as WSDL services. The sources may be relational databases, which naturally have a schema, or ad hoc information/application systems whereas the owner publishes a virtual schema. CLIDE allows information clients to pose queries over the published schema and utilizes prior work on answering queries using views to answer queries that can be processed by combining and processing the results of one or more Web service calls. These queries are called feasible . Contrary to prior work, where infeasible queries are rejected without an explanatory feedback, leading the user into a frustrating trial-and-error cycle, CLIDE features a query formulation interface, which extends the QBE-like query builder of Microsoft's SQL Server with a color scheme that guides the user toward formulating feasible queries. CLIDE guarantees that the suggested query edit actions are complete (i.e., each feasible query can be built by following only suggestions), rapidly convergent (the suggestions are tuned to lead to the closest feasible completions of the query), and suitably summarized (at each interaction step, only a minimal number of actions needed to preserve completeness are suggested). We present the algorithms, implementation, and performance evaluation showing that CLIDE is a viable on-line tool.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2152739188",
    "type": "article"
  },
  {
    "title": "Optimal algorithms for evaluating rank joins in database systems",
    "doi": "https://doi.org/10.1145/1670243.1670249",
    "publication_date": "2008-02-15",
    "publication_year": 2008,
    "authors": "Karl Schnaitter; Neoklis Polyzotis",
    "corresponding_authors": "",
    "abstract": "In the rank join problem, we are given a set of relations and a scoring function, and the goal is to return the join results with the top k scores. It is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic. These conditions allow for efficient algorithms that solve the rank join problem without reading all of the input. In this article, we present a thorough analysis of such rank join algorithms. A strong point of our analysis is that it is based on a more general problem statement than previous work, making it more relevant to the execution model that is employed by database systems. One of our results indicates that the well-known HRJN algorithm has shortcomings, because it does not stop reading its input as soon as possible. We find that it is NP-hard to overcome this weakness in the general case, but cases of limited query complexity are tractable. We prove the latter with an algorithm that infers provably tight bounds on the potential benefit of reading more input in order to stop as soon as possible. As a result, the algorithm achieves a cost that is within a constant factor of optimal.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2029985800",
    "type": "article"
  },
  {
    "title": "TuG synopses for approximate query answering",
    "doi": "https://doi.org/10.1145/1508857.1508860",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "Joshua Spiegel; Neoklis Polyzotis",
    "corresponding_authors": "",
    "abstract": "This article introduces the Tuple Graph (TuG) synopses, a new class of data summaries that enable accurate approximate answers for complex relational queries. The proposed summarization framework adopts a “semi-structured” view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals, respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to approximate the answer to a query by performing the corresponding traversal over the summarized graph. We detail the (TuG) synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate (TuG) within a specific storage budget. We validate the performance of (TuG) with an extensive experimental study on real-life and synthetic datasets. Our results verify the effectiveness of (TuG) in generating accurate approximate answers for complex join queries, and demonstrate their benefits over existing summarization techniques.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1995034419",
    "type": "article"
  },
  {
    "title": "Incorporating constraints in probabilistic XML",
    "doi": "https://doi.org/10.1145/1567274.1567280",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Sara Cohen; Benny Kimelfeld; Yehoshua Sagiv",
    "corresponding_authors": "",
    "abstract": "Constraints are important, not only for maintaining data integrity, but also because they capture natural probabilistic dependencies among data items. A probabilistic XML database (PXDB) is the probability subspace comprising the instances of a p-document that satisfy a set of constraints. In contrast to existing models that can express probabilistic dependencies, it is shown that query evaluation is tractable in PXDBs. The problems of sampling and determining well-definedness (i.e., whether the aforesaid subspace is nonempty) are also tractable. Furthermore, queries and constraints can include the aggregate functions count, max, min, and ratio. Finally, this approach can be easily extended to allow a probabilistic interpretation of constraints.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2049365209",
    "type": "article"
  },
  {
    "title": "Collaborative data sharing via update exchange and provenance",
    "doi": "https://doi.org/10.1145/2508020.2500127",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Grigoris Karvounarakis; Todd J. Green; Zachary G. Ives; Val Tannen",
    "corresponding_authors": "",
    "abstract": "Recent work [Ives et al. 2005] proposed a new class of systems for supporting data sharing among scientific and other collaborations: this new collaborative data sharing system connects heterogeneous logical peers using a network of schema mappings. Each peer has a locally controlled and edited database instance, but wants to incorporate related data from other peers as well. To achieve this, every peer's data and updates propagate along the mappings to the other peers. However, this operation, termed update exchange, is filtered by trust conditions—expressing what data and sources a peer judges to be authoritative—which may cause a peer to reject another's updates. In order to support such filtering, updates carry provenance information.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4247107899",
    "type": "article"
  },
  {
    "title": "Asymmetric signature schemes for efficient exact edit similarity query processing",
    "doi": "https://doi.org/10.1145/2508020.2508023",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Jianbin Qin; Wei Wang; Chuan Xiao; Yifei Lu; Xuemin Lin; Haixun Wang",
    "corresponding_authors": "",
    "abstract": "Given a query string Q , an edit similarity search finds all strings in a database whose edit distance with Q is no more than a given threshold τ. Most existing methods answering edit similarity queries employ schemes to generate string subsequences as signatures and generate candidates by set overlap queries on query and data signatures. In this article, we show that for any such signature scheme, the lower bound of the minimum number of signatures is τ + 1, which is lower than what is achieved by existing methods. We then propose several asymmetric signature schemes, that is, extracting different numbers of signatures for the data and query strings, which achieve this lower bound. A basic asymmetric scheme is first established on the basis of matching q -chunks and q -grams between two strings. Two efficient query processing algorithms (IndexGram and IndexChunk) are developed on top of this scheme. We also propose novel candidate pruning methods to further improve the efficiency. We then generalize the basic scheme by incorporating novel ideas of floating q -chunks, optimal selection of q -chunks, and reducing the number of signatures using global ordering. As a result, the Super and Turbo families of schemes are developed together with their corresponding query processing algorithms. We have conducted a comprehensive experimental study using the six asymmetric algorithms and nine previous state-of-the-art algorithms. The experiment results clearly showcase the efficiency of our methods and demonstrate space and time characteristics of our proposed algorithms.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2044312046",
    "type": "article"
  },
  {
    "title": "Representations and Optimizations for Embedded Parallel Dataflow Languages",
    "doi": "https://doi.org/10.1145/3281629",
    "publication_date": "2019-01-29",
    "publication_year": 2019,
    "authors": "A. Alexandrov; Georgi Krastev; Volker Markl",
    "corresponding_authors": "",
    "abstract": "Parallel dataflow engines such as Apache Hadoop, Apache Spark, and Apache Flink are an established alternative to relational databases for modern data analysis applications. A characteristic of these systems is a scalable programming model based on distributed collections and parallel transformations expressed by means of second-order functions such as map and reduce. Notable examples are Flink’s DataSet and Spark’s RDD programming abstractions. These programming models are realized as EDSLs—domain specific languages embedded in a general-purpose host language such as Java, Scala, or Python. This approach has several advantages over traditional external DSLs such as SQL or XQuery. First, syntactic constructs from the host language (e.g., anonymous functions syntax, value definitions, and fluent syntax via method chaining) can be reused in the EDSL. This eases the learning curve for developers already familiar with the host language. Second, it allows for seamless integration of library methods written in the host language via the function parameters passed to the parallel dataflow operators. This reduces the effort for developing analytics dataflows that go beyond pure SQL and require domain-specific logic. At the same time, however, state-of-the-art parallel dataflow EDSLs exhibit a number of shortcomings. First, one of the main advantages of an external DSL such as SQL—the high-level, declarative Select-From-Where syntax—is either lost completely or mimicked in a non-standard way. Second, execution aspects such as caching, join order, and partial aggregation have to be decided by the programmer. Optimizing them automatically is very difficult due to the limited program context available in the intermediate representation of the DSL. In this article, we argue that the limitations listed above are a side effect of the adopted type-based embedding approach. As a solution, we propose an alternative EDSL design based on quotations. We present a DSL embedded in Scala and discuss its compiler pipeline, intermediate representation, and some of the enabled optimizations. We promote the algebraic type of bags in union representation as a model for distributed collections and its associated structural recursion scheme and monad as a model for parallel collection processing. At the source code level, Scala’s comprehension syntax over a bag monad can be used to encode Select-From-Where expressions in a standard way. At the intermediate representation level, maintaining comprehensions as a first-class citizen can be used to simplify the design and implementation of holistic dataflow optimizations that accommodate for nesting and control-flow. The proposed DSL design therefore reconciles the benefits of embedded parallel dataflow DSLs with the declarativity and optimization potential of external DSLs like SQL.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2912256776",
    "type": "article"
  },
  {
    "title": "Designing a Query Language for RDF",
    "doi": "https://doi.org/10.1145/3129247",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Marcelo Arenas; Martín Ugarte",
    "corresponding_authors": "",
    "abstract": "When querying an Resource Description Framework (RDF) graph, a prominent feature is the possibility of extending the answer to a query with optional information. However, the definition of this feature in SPARQL—the standard RDF query language—has raised some important issues. Most notably, the use of this feature increases the complexity of the evaluation problem, and its closed-world semantics is in conflict with the underlying open-world semantics of RDF. Many approaches for fixing such problems have been proposed, the most prominent being the introduction of the semantic notion of weakly monotone SPARQL query. Weakly monotone SPARQL queries have shaped the class of queries that conform to the open-world semantics of RDF. Unfortunately, finding an effective way of restricting SPARQL to the fragment of weakly monotone queries has proven to be an elusive problem. In practice, the most widely adopted fragment for writing SPARQL queries is based on the syntactic notion of well designedness. This notion has proven to be a good approach for writing SPARQL queries, but its expressive power has yet to be fully understood. The starting point of this article is to understand the relation between well-designed queries and the semantic notion of weak monotonicity. It is known that every well-designed SPARQL query is weakly monotone; as our first contribution we prove that the converse does not hold, even if an extension of this notion based on the use of disjunction is considered. Given this negative result, we embark on the task of defining syntactic fragments that are weakly monotone and have higher expressive power than the fragment of well-designed queries. To this end, we move to a more general scenario where infinite RDF graphs are also allowed, so interpolation techniques studied for first-order logic can be applied. With the use of these techniques, we are able to define a new operator for SPARQL that gives rise to a query language with the desired properties (over finite and infinite RDF graphs). It should be noticed that every query in this fragment is weakly monotone if we restrict the semantics to finite RDF graphs. Moreover, we use this result to provide a simple characterization of the class of monotone CONSTRUCT queries, that is, the class of SPARQL queries that produce RDF graphs as output. Finally, we pinpoint the complexity of the evaluation problem for the query languages identified in the article.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2972708100",
    "type": "article"
  },
  {
    "title": "Generalizing database forensics",
    "doi": "https://doi.org/10.1145/2487259.2487264",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Kyriacos E. Pavlou; Richard T. Snodgrass",
    "corresponding_authors": "",
    "abstract": "In this article we present refinements on previously proposed approaches to forensic analysis of database tampering. We significantly generalize the basic structure of these algorithms to admit new characterizations of the “where” axis of the corruption diagram. Specifically, we introduce page-based partitioning as well as attribute-based partitioning along with their associated corruption diagrams. We compare the structure of all the forensic analysis algorithms and discuss the various design choices available with respect to forensic analysis. We characterize the forensic cost of the newly introduced algorithms, compare their forensic cost, and give our recommendations. We then introduce a comprehensive taxonomy of the types of possible corruption events, along with an associated forensic analysis protocol that consolidates all extant forensic algorithms and the corresponding type(s) of corruption events they detect. The result is a generalization of these algorithms and an overarching characterization of the process of database forensic analysis, thus providing a context within the overall operation of a DBMS for all existing forensic analysis algorithms.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1990891840",
    "type": "article"
  },
  {
    "title": "Finding Robust Itemsets under Subsampling",
    "doi": "https://doi.org/10.1145/2656261",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Nikolaj Tatti; Fabian Moerchen; Toon Calders",
    "corresponding_authors": "",
    "abstract": "Mining frequent patterns is plagued by the problem of pattern explosion, making pattern reduction techniques a key challenge in pattern mining. In this article we propose a novel theoretical framework for pattern reduction by measuring the robustness of a property of an itemset such as closedness or nonderivability. The robustness of a property is the probability that this property holds on random subsets of the original data. We study four properties, namely an itemset being closed, free, non-derivable, or totally shattered, and demonstrate how to compute the robustness analytically without actually sampling the data. Our concept of robustness has many advantages: Unlike statistical approaches for reducing patterns, we do not assume a null hypothesis or any noise model and, in contrast to noise-tolerant or approximate patterns, the robust patterns for a given property are always a subset of the patterns with this property. If the underlying property is monotonic then the measure is also monotonic, allowing us to efficiently mine robust itemsets. We further derive a parameter-free technique for ranking itemsets that can be used for top- k approaches. Our experiments demonstrate that we can successfully use the robustness measure to reduce the number of patterns and that ranking yields interesting itemsets.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2139963538",
    "type": "article"
  },
  {
    "title": "Closing the Gap",
    "doi": "https://doi.org/10.1145/2757217",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Kaustubh Beedkar; Klaus Berberich; Rainer Gemulla; Iris Miliaraki",
    "corresponding_authors": "",
    "abstract": "Frequent sequence mining is one of the fundamental building blocks in data mining. While the problem has been extensively studied, few of the available techniques are sufficiently scalable to handle datasets with billions of sequences; such large-scale datasets arise, for instance, in text mining and session analysis. In this article, we propose MG-FSM, a scalable algorithm for frequent sequence mining on MapReduce. MG-FSM can handle so-called “gap constraints”, which can be used to limit the output to a controlled set of frequent sequences. Both positional and temporal gap constraints, as well as appropriate maximality and closedness constraints, are supported. At its heart, MG-FSM partitions the input database in a way that allows us to mine each partition independently using any existing frequent sequence mining algorithm. We introduce the notion of ω-equivalency, which is a generalization of the notion of a “projected database” used by many frequent pattern mining algorithms. We also present a number of optimization techniques that minimize partition size, and therefore computational and communication costs, while still maintaining correctness. Our experimental study in the contexts of text mining and session analysis suggests that MG-FSM is significantly more efficient and scalable than alternative approaches.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2259693656",
    "type": "article"
  },
  {
    "title": "Robust Distributed Query Processing for Streaming Data",
    "doi": "https://doi.org/10.1145/2602138",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Chuan Lei; Elke A. Rundensteiner",
    "corresponding_authors": "",
    "abstract": "Distributed stream processing systems must function efficiently for data streams that fluctuate in their arrival rates and data distributions. Yet repeated and prohibitively expensive load reallocation across machines may make these systems ineffective, potentially resulting in data loss or even system failure. To overcome this problem, we propose a comprehensive solution, called the Robust Load Distribution (RLD) strategy, that is resilient under data fluctuations. RLD provides ϵ-optimal query performance under an expected range of load fluctuations without suffering from the performance penalty caused by load migration. RLD is based on three key strategies. First, we model robust distributed stream processing as a parametric query optimization problem in a parameter space that captures the stream fluctuations. The notions of both robust logical and robust physical plans that work together to proactively handle all ranges of expected fluctuations in parameters are abstracted as overlays of this parameter space. Second, our Early-terminated Robust Partitioning ( ERP ) finds a combination of robust logical plans that together cover the parameter space, while minimizing the number of prohibitively expensive optimizer calls with a probabilistic bound on the space coverage. Third, we design a family of algorithms for physical plan generation. Our GreedyPhy exploits a probabilistic model to efficiently find a robust physical plan that sustains most frequently used robust logical plans at runtime. Our CorPhy algorithm exploits operator correlations for the robust physical plan optimization. The resulting physical plan smooths the workload on each node under all expected fluctuations. Our OptPrune algorithm, using CorPhy as baseline, is guaranteed to find the optimal physical plan that maximizes the parameter space coverage with a practical increase in optimization time. Lastly, we further expand the capabilities of our proposed RLD framework to also appropriately react under so-called “space drifts”, that is, a space drift is a change of the parameter space where the observed runtime statistics deviate from the expected optimization-time statistics. Our RLD solution is capable of adjusting itself to the unexpected yet significant data fluctuations beyond those planned for via covering the parameter space. Our experimental study using stock market and sensor network streams demonstrates that our RLD methodology consistently outperforms state-of-the-art solutions in terms of efficiency and effectiveness in highly fluctuating data stream environments.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2059455635",
    "type": "article"
  },
  {
    "title": "Plan Bouquets",
    "doi": "https://doi.org/10.1145/2901738",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Anshuman Dutt; Jayant R. Haritsa",
    "corresponding_authors": "",
    "abstract": "Identifying efficient execution plans for declarative OLAP queries typically entails estimation of several predicate selectivities. In practice, these estimates often differ significantly from the values actually encountered during query execution, leading to poor plan choices and grossly inflated response times. We propose here a conceptually new approach to address this classical problem, wherein the compile-time estimation process is completely eschewed for error-prone selectivities. Instead, from the set of optimal plans in the query’s selectivity error space, a limited subset, called the “plan bouquet,” is selected such that at least one of the bouquet plans is 2-optimal at each location in the space. Then, at run time, a sequence of cost-budgeted executions from the plan bouquet is carried out, eventually finding a plan that executes to completion within its assigned budget. The duration and switching of these executions is controlled by a graded progression of isosurfaces projected onto the optimal performance profile. We prove that this construction results, for the first time, in guarantees on worst-case performance sub-optimality. Moreover, it ensures repeatable execution strategies across different invocations of a query. We then present a suite of enhancements to the basic plan bouquet algorithm, including randomized variants, that result in significantly stronger performance guarantees. An efficient isosurface identification algorithm is also introduced to curtail the bouquet construction overheads. The plan bouquet approach has been empirically evaluated on both PostgreSQL and a commercial DBMS, over the TPC-H and TPC-DS benchmark environments. Our experimental results indicate that it delivers substantial improvements in the worst-case behavior, without impairing the average-case performance, as compared to the native optimizers of these systems. Moreover, it can be implemented using existing optimizer infrastructure, making it relatively easy to incorporate in current database engines. Overall, the plan bouquet approach provides novel performance guarantees that open up new possibilities for robust query processing.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2354834590",
    "type": "article"
  },
  {
    "title": "DBMS Metrology",
    "doi": "https://doi.org/10.1145/2996454",
    "publication_date": "2016-11-09",
    "publication_year": 2016,
    "authors": "Sabah Currim; Richard T. Snodgrass; Young‐Kyoon Suh; Rui Zhang",
    "corresponding_authors": "",
    "abstract": "It is surprisingly hard to obtain accurate and precise measurements of the time spent executing a query because there are many sources of variance. To understand these sources, we review relevant per-process and overall measures obtainable from the Linux kernel and introduce a structural causal model relating these measures. A thorough correlational analysis provides strong support for this model. We attempted to determine why a particular measurement wasn’t repeatable and then to devise ways to eliminate or reduce that variance. This enabled us to articulate a timing protocol that applies to proprietary DBMSes, that ensures the repeatability of a query, and that obtains a quite accurate query execution time while dropping very few outliers. This resulting query time measurement procedure, termed the Tucson Timing Protocol Version 2 (TTPv2), consists of the following steps: (i) perform sanity checks to ensure data validity; (ii) drop some query executions via clearly motivated predicates; (iii) drop some entire queries at a cardinality, again via clearly motivated predicates; (iv) for those that remain, compute a single measured time by a carefully justified formula over the underlying measures of the remaining query executions; and (v) perform post-analysis sanity checks. The result is a mature, general, robust, self-checking protocol that provides a more precise and more accurate timing of the query. The protocol is also applicable to other operating domains in which measurements of multiple processes each doing computation and I/O is needed.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2553994547",
    "type": "article"
  },
  {
    "title": "A Family of Centrality Measures for Graph Data Based on Subgraphs",
    "doi": "https://doi.org/10.1145/3649134",
    "publication_date": "2024-02-23",
    "publication_year": 2024,
    "authors": "Sebastián Bugedo; Cristian Riveros; Jorge Salas",
    "corresponding_authors": "",
    "abstract": "We present the theoretical foundations and first experimental study of a new approach in centrality measures for graph data. The main principle is straightforward: the more relevant subgraphs around a vertex, the more central it is in the network. We formalize the notion of “relevant subgraphs” by choosing a family of subgraphs that, given a graph G and a vertex v , assigns a subset of connected subgraphs of G that contains v . Any of such families defines a measure of centrality by counting the number of subgraphs assigned to the vertex, i.e., a vertex will be more important for the network if it belongs to more subgraphs in the family. We show several examples of this approach. In particular, we propose the All-Subgraphs (All-Trees) centrality, a centrality measure that considers every subgraph (tree). We study fundamental properties over families of subgraphs that guarantee desirable properties over the centrality measure. Interestingly, All-Subgraphs and All-Trees satisfy all these properties, showing their robustness as centrality notions. To conclude the theoretical analysis, we study the computational complexity of counting certain families of subgraphs and show a linear time algorithm to compute the All-Subgraphs and All-Trees centrality for graphs with bounded treewidth. Finally, we implemented these algorithms and computed these measures over more than one hundred real-world networks. With this data, we present an empirical comparison between well-known centrality measures and those proposed in this work.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392121217",
    "type": "article"
  },
  {
    "title": "Declustering of key-based partitioned signature files",
    "doi": "https://doi.org/10.1145/232753.232755",
    "publication_date": "1996-09-01",
    "publication_year": 1996,
    "authors": "Paolo Ciaccia; Paolo Tiberio; Pavel Zezula",
    "corresponding_authors": "",
    "abstract": "Access methods based on signature files can largely benefit from possibilities offered by parallel environments. To this end, an effective declustering strategy that would distribute signatures over a set of parallel independent disks has to be combined with a synergic clustering which is employed to avoid searching the whole signature file while executing a query. This article proposes two parallel signature file organizations, Hamming Filter ( HF ) and Hamming + Filter ( H + F ), whose common declustering strategy is based on error correcting codes , and where clustering is achieved by organizing signatures into fixed-size buckets, each containing signatures sharing the same key value. HF allocates signatures on disks in a static way and works well if a correct relationship holds between the parameters of the code and the size of the file. H + F is a generalization of HF suitable to manage highly dynamic files. It uses a dynamic declustering, obtained through a sequence of codes, and organizes a smooth migration of signatures between disks so that high performance levels are retained regardless of current file size. Theoretical analysis characterizes the best-case, expected, and worst-case behaviors of these organizations. Analytical results are verified by experiments on prototype systems.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2086750463",
    "type": "article"
  },
  {
    "title": "Optimal weight assignment for signature generation",
    "doi": "https://doi.org/10.1145/128903.128907",
    "publication_date": "1992-06-01",
    "publication_year": 1992,
    "authors": "Chun-Wu Roger Leng; Dik Lun Lee",
    "corresponding_authors": "",
    "abstract": "Previous work on superimposed coding has been characterized by two aspects. First, it is generally assumed that signatures are generated from logical text blocks of the same size; that is, each block contains the same number of unique terms after stopword and duplicate removal. We call this approach the fixed-size block (FSB) method, since each text block has the same size, as measured by the number of unique terms contained in it. Second, with only a few exceptions [6,7,8,9,17], most previous work has assumed that each term in the text contributes the same number of ones to the signature (i.e., the weight of the term signatures is fixed). The main objective of this paper is to derive an optimal weight assignment that assigns weights to document terms according to their occurrence and query frequencies in order to minimize the false-drop probability. The optimal scheme can account for both uniform and nonuniform occurence and query frequencies, and the signature generation method is still based on hashing rather than on table lookup. Furthermore, a new way of generating signatures, the fixed-weight block (FWB) method, is introduced. FWB controls the weight of every signature to a constant, whereas in FSB, only the expected signature weight is constant. We have shown that FWB has a lower false-drop probability than that of the FSB method, but its storage overhead is slightly higher. Other advantages of FWB are that the optimal weight assignment can be obtained analytically without making unrealistic assumptions and that the formula for computing the term signature weights is simple and efficient.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2095423597",
    "type": "article"
  },
  {
    "title": "A logical foundation for deductive object-oriented databases",
    "doi": "https://doi.org/10.1145/507234.507237",
    "publication_date": "2002-03-01",
    "publication_year": 2002,
    "authors": "Mengchi Liu; Gillian Dobbie; Tok Wang Ling",
    "corresponding_authors": "",
    "abstract": "Over the past decade, a large number of deductive object-oriented database languages have been proposed. The earliest of these languages had few object-oriented features, and more and more features have systematically been incorporated in successive languages. However, a language with a clean logical semantics that naturally accounts for all the key object-oriented features, is still missing from the literature. This article takes us another step towards solving this problem. Two features that are currently missing are the encapsulation of rule-based methods in classes, and nonmonotonic structural and behavioral inheritance with overriding, conflict resolution and blocking. This article introduces the syntax of a language with these features. The language is restricted in the sense that we have omitted other object-oriented and deductive features that are now well understood, in order to make our contribution clearer. It then defines a class of databases, called well-defined databases , that have an intuitive meaning and develops a direct logical semantics for this class of databases. The semantics is based on the well-founded semantics from logic programming. The work presented in this article establishes a firm logical foundation for deductive object-oriented databases.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2051445745",
    "type": "article"
  },
  {
    "title": "Serializability with constraints",
    "doi": "https://doi.org/10.1145/27629.214284",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Toshihide Ibaraki; Tiko Kameda; Toshimi Minoura",
    "corresponding_authors": "",
    "abstract": "This paper deals with the serializability theory for single-version and multiversion database systems. We first introduce the concept of disjoint-interval topological sort ( DITS , for short) of an arc-labeled directed acyclic graph. It is shown that a history is serializable if and only if its transaction IO graph has a DITS. We then define several subclasses of serializable histories, based on the constraints imposed by write-write, write-read, read-write, or read-read conflicts, and investigate inclusion relationships among them. In terms of DITS, we give a sufficient condition for a class of serializable histories to be polynomially recognizable, which is then used to show that a new class of histories, named WRW, can be recognized in polynomial time. We also present NP-completeness results for the problem of testing membership in some other classes. In the second half of this paper, we extend these results to multiversion database systems. The inclusion relationships among multiversion classes defined by constraints, such as write-write and write-read, are investigated. One such class coincides with class DMVSR, introduced by Papadimitriou and Kanellakis, and gives a simple characterization of this class. It is shown that for most constraints, multiversion classes properly contain the corresponding single-version classes. Complexity results for the membership testing are also discussed.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2050973321",
    "type": "article"
  },
  {
    "title": "Necessary and sufficient conditions to linearize doubly recursive programs in logic databases",
    "doi": "https://doi.org/10.1145/88636.89237",
    "publication_date": "1990-09-01",
    "publication_year": 1990,
    "authors": "Weining Zhang; Clement Yu; Daniel Troy",
    "corresponding_authors": "",
    "abstract": "Linearization of nonlinear recursive programs is an important issue in logic databases for both practical and theoretical reasons. If a nonlinear recursive program can be transformed into an equivalent linear recursive program, then it may be computed more efficiently than when the tranformation is not possible. We provide a set of necessary and sufficient conditions for a simple doubly recursive program to be equivalent to a simple linear recursive program. The necessary and sufficient conditions can be verified effectively.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2105741963",
    "type": "article"
  },
  {
    "title": "Exchanging intensional XML data",
    "doi": "https://doi.org/10.1145/1061318.1061319",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Tova Milo; Serge Abiteboul; Bernd Amann; Omar Benjelloun; Fred Dang Ngoc",
    "corresponding_authors": "",
    "abstract": "XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process.We argue that---like for regular XML data---schemas (à la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4300845674",
    "type": "article"
  },
  {
    "title": "Expressions for batched searching of sequential and hierarchical files",
    "doi": "https://doi.org/10.1145/3148.3326",
    "publication_date": "1985-03-01",
    "publication_year": 1985,
    "authors": "Prashant Palvia",
    "corresponding_authors": "Prashant Palvia",
    "abstract": "Batching yields significant savings in access costs in sequential, tree structured, and random files. A direct and simple expression is developed for computing the average number of records/pages accessed to satisfy a batched query of a sequential tile. The advantages of batching for sequential and random files are discussed. A direct equation is provided for the number of nodes accessed in unhatched queries of hierarchical files. An exact recursive expression is developed for node accesses in batched queries of hierarchical files. In addition to the recursive relationship, good, closed-form upper- and lower-bound approximations are provided for the case of batched queries of hierarchical files.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2027950744",
    "type": "article"
  },
  {
    "title": "Semantics of query languages for network databases",
    "doi": "https://doi.org/10.1145/3979.214293",
    "publication_date": "1985-09-01",
    "publication_year": 1985,
    "authors": "Kazimierz Subieta",
    "corresponding_authors": "Kazimierz Subieta",
    "abstract": "Semantics determines the meaning of language constructs; hence it says much more than syntax does about implementing the language. The main purpose of this paper is a formal presentation of the meaning of basic language constructs employed in many database languages (sublanguages). Therefore, stylized query languages SSL (Sample Selection Language) and J (Joins) are introduced, wherein most of the typical entries present in other query languages are collected. The semantics of SSL and J are defined by means of the denotational method and explained informally. In SSL and J, four types of expressions are introduced: a selector (denotes a set of addresses), a term (denotes a set of values), a formula (denotes a truth value), and a join (denotes a set of n-tuples of addresses or values). In many cases alternative semantics are given and discussed. In order to obtain more general properties of the proposed languages, a new database access model is introduced, intended to be a tool for the description of the logical access paths to data. In particular, the access paths of the network and relational models can be described. SSL and J expressions may be addressed to both data structures. In the case of the relational model, expressions of J are similar to SQL or QUEL statements. Thus J may be considered a generalization of relational query languages for the network model. Finally, a programming language, based on SSL and J, is outlined, and the issues of SSL and J implementation are considered.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2030353901",
    "type": "article"
  },
  {
    "title": "PATAXÓ",
    "doi": "https://doi.org/10.1145/1166074.1166078",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Vanessa Braganholo; Susan B. Davidson; Carlos A. Heuser",
    "corresponding_authors": "",
    "abstract": "XML has become an important medium for data exchange, and is frequently used as an interface to (i.e., a view of) a relational database. Although a lot of work has been done on querying relational databases through XML views, the problem of updating relational databases through XML views has not received much attention. In this work, we map XML views expressed using a subset of XQuery to a corresponding set of relational views. Thus, we transform the problem of updating relational databases through XML views into a classical problem of updating relational databases through relational views. We then show how updates on the XML view are mapped to updates on the corresponding relational views. Existing work on updating relational views can then be leveraged to determine whether or not the relational views are updatable with respect to the relational updates, and if so, to translate the updates to the underlying relational database.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2104877771",
    "type": "article"
  },
  {
    "title": "An extension of the performance of a database manager in a virtual memory system using partially locked virtual buffers",
    "doi": "https://doi.org/10.1145/320544.320556",
    "publication_date": "1977-06-01",
    "publication_year": 1977,
    "authors": "Richard Brice; Stephen W. Sherman",
    "corresponding_authors": "",
    "abstract": "Buffer pools are created and managed in database systems in order to reduce the total number of accesses to the I/O devices. In systems using virtual memory, any reduction in I/O accesses may be accompanied by an increase in paging. The effects of these factors on system performance are quantified, where system performance is a function of page faults and database accesses to the I/O devices. A previous study of this phenomenon is extended through the analysis of empirical data gathered in a multifactor experiment. In this study memory is partitioned between the program and the buffer so that the impact of the controlled factors can be more effectively evaluated. It is possible to improve system performance through the use of different paging algorithms in the program partition and the buffer partition. Also, the effects on system performance as the virtual buffer size is increased beyond the real memory allocated to the buffer partition are investigated.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2070884996",
    "type": "article"
  },
  {
    "title": "Commutativity analysis for XML updates",
    "doi": "https://doi.org/10.1145/1412331.1412341",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Giorgio Ghelli; Kristoffer H. Rose; Jérǒme Simèon",
    "corresponding_authors": "",
    "abstract": "An effective approach to support XML updates is to use XQuery extended with update operations. This approach results in very expressive languages which are convenient for users but are difficult to optimize or reason about. A crucial question underlying many static analysis problems for such languages, from optimization to view maintenance, is whether two expressions commute. Unfortunately, commutativity is undecidable for most existing XML update languages. In this article, we propose a conservative analysis for an expressive XML update language that can be used to determine commutativity. The approach relies on a form of path analysis that computes upper bounds for the nodes that are accessed or modified in a given expression. Our main result is a theorem that can be used to identify commuting expressions. We illustrate how the technique applies to concrete examples of query optimization in the presence of updates.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2087404282",
    "type": "article"
  },
  {
    "title": "On the encipherment of search trees and random access files",
    "doi": "https://doi.org/10.1145/320434.320445",
    "publication_date": "1976-03-01",
    "publication_year": 1976,
    "authors": "Raymond Bayer; Jeremiah Metzger",
    "corresponding_authors": "",
    "abstract": "The securing of information in indexed, random access files by means of privacy transformations must be considered as a problem distinct from that for sequential files. Not only must processing overhead due to encrypting be considered, but also threats to encipherment arising from updating and the file structure itself must be countered. A general encipherment scheme is proposed for files maintained in a paged structure in secondary storage. This is applied to the encipherment of indexes organized as B -trees; a B -tree is a particular type of multiway search tree. Threats to the encipherment of B -trees, especially relating to updating, are examined, and countermeasures are proposed for each. In addition, the effect of encipherment on file access and update, on paging mechanisms, and on files related to the enciphered index are discussed. Many of the concepts presented may be readily transferred to other forms of multiway index trees and to binary search trees.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4229874230",
    "type": "article"
  },
  {
    "title": "SQL query optimization through nested relational algebra",
    "doi": "https://doi.org/10.1145/1272743.1272748",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Bin Cao; Antonio Badia",
    "corresponding_authors": "",
    "abstract": "Most research work on optimization of nested queries focuses on aggregate subqueries. In this article, we show that existing approaches are not adequate for nonaggregate subqueries, especially for those having multiple subqueries and certain comparison operators. We then propose a new efficient approach, the nested relational approach, based on the nested relational algebra. The nested relational approach treats all subqueries in a uniform manner, being able to deal with nested queries of any type and any level. We report on experimental work that confirms that existing approaches have difficulties dealing with nonaggregate subqueries, and that the nested relational approach offers better performance. We also discuss algebraic optimization rules for further optimizing the nested relational approach and the issue of integrating it into relational database systems.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1979093255",
    "type": "article"
  },
  {
    "title": "Modeling and managing changes in text databases",
    "doi": "https://doi.org/10.1145/1272743.1272744",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Panagiotis G. Ipeirotis; Alexandros Ntoulas; Junghoo Cho; Luis Gravano",
    "corresponding_authors": "",
    "abstract": "Large amounts of (often valuable) information are stored in web-accessible text databases. “Metasearchers” provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the database contents to select the best databases for each query. So far, database selection research has largely assumed that databases are static, so the associated statistical summaries do not evolve over time. However, databases are rarely static and the statistical summaries that describe their contents need to be updated periodically to reflect content changes. In this article, we first report the results of a study showing how the content summaries of 152 real web databases evolved over a period of 52 weeks. Then, we show how to use “survival analysis” techniques in general, and Cox's proportional hazards regression in particular, to model database changes over time and predict when we should update each content summary. Finally, we exploit our change model to devise update schedules that keep the summaries up to date by contacting databases only when needed, and then we evaluate the quality of our schedules experimentally over real web databases.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2036074375",
    "type": "article"
  },
  {
    "title": "Confidence bounds for sampling-based group by estimates",
    "doi": "https://doi.org/10.1145/1386118.1386122",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Fei Xu; Christopher Jermaine; Alin Dobra",
    "corresponding_authors": "",
    "abstract": "Sampling is now a very important data management tool, to such an extent that an interface for database sampling is included in the latest SQL standard. In this article we reconsider in depth what at first may seem like a very simple problem—computing the error of a sampling-based guess for the answer to a GROUP BY query over a multitable join. The difficulty when sampling for the answer to such a query is that the same sample will be used to guess the result of the query for each group, which induces correlations among the estimates. Thus, from a statistical point-of-view it is very problematic and even dangerous to use traditional methods such as confidence intervals for communicating estimate accuracy to the user. We explore ways to address this problem, and pay particular attention to the computational aspects of computing “safe” confidence intervals.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2066179253",
    "type": "article"
  },
  {
    "title": "Keyword search over relational tables and streams",
    "doi": "https://doi.org/10.1145/1567274.1567279",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Alexander Markowetz; Yin Yang; Dimitris Papadias",
    "corresponding_authors": "",
    "abstract": "Relational Keyword Search (R-KWS) provides an intuitive way to query relational data without requiring SQL, or knowledge of the underlying schema. In this article we describe a comprehensive framework for R-KWS covering snapshot queries on conventional tables and continuous queries on relational streams. Our contributions are summarized as follows: (i) We provide formal semantics, addressing the temporal validity and order of results, spanning uniformly over tables and streams; (ii) we investigate two general methodologies for query processing, graph based and operator based , that resolve several problems of previous approaches; and (iii) we develop a range of algorithms and optimizations covering both methodologies. We demonstrate the effectiveness of R-KWS, as well as the significant performance benefits of the proposed techniques, through extensive experiments with static and streaming datasets.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1995451707",
    "type": "article"
  },
  {
    "title": "Towards a theory of search queries",
    "doi": "https://doi.org/10.1145/1862919.1862925",
    "publication_date": "2010-10-12",
    "publication_year": 2010,
    "authors": "George Fletcher; Jan Van den Bussche; Dirk Van Gucht; Stijn Vansummeren",
    "corresponding_authors": "",
    "abstract": "The need to manage diverse information sources has triggered the rise of very loosely structured data models, known as dataspace models. Such information management systems must allow querying in simple ways, mostly by a form of searching. Motivated by these developments, we propose a theory of search queries in a general model of dataspaces. In this model, a dataspace is a collection of data objects, where each data object is a collection of data items. Basic search queries are expressed using filters on data items, following the basic model of Boolean search in information retrieval. We characterize semantically the class of queries that can be expressed by searching. We apply our theory to classical relational databases, where we connect search queries to the known class of fully generic queries, and to dataspaces where data items are formed by attribute-value pairs. We also extend our theory to a more powerful, associative form of searching, where one can ask for objects that are similar to objects satisfying given search conditions. Such associative search queries are shown to correspond to a very limited kind of joins. We show that the basic search language extended with associative search can exactly define the queries definable in a restricted fragment of the semijoin algebra working on an explicit relational representation of the dataspace.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1993572358",
    "type": "article"
  },
  {
    "title": "Continuous online index tuning in moving object databases",
    "doi": "https://doi.org/10.1145/1806907.1806909",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Chen Su; Mário A. Nascimento; Beng Chin Ooi; Kian‐Lee Tan",
    "corresponding_authors": "",
    "abstract": "In a Moving Object Database (MOD), the dataset, for example, the location of objects and their distribution, and the workload change frequently. Traditional static indexes are not able to cope well with such changes, that is, their effectiveness and efficiency are seriously affected. This calls for the development of novel indexes that can be reconfigured automatically based on the state of the system. In this article, we design and present the ST 2 B-tree, a S elf- T unable S patio- T emporal B + -tree index for MODs. In ST 2 B-tree, the data space is partitioned into regions of different density with respect to a set of reference points. Based on the density, objects in a region are managed using a grid of appropriate granularity; intuitively, a dense region employs a grid with fine granularity, while a sparse region uses a grid with coarse granularity. In this way, the ST 2 B-tree adapts itself to workload diversity in space. To enable online tuning, the ST 2 B-tree employs a “multitree” indexing technique. The underlying B + -tree is logically divided into two subtrees. Objects are dispatched to either subtree depending on their last update time. The two subtrees are rebuilt periodically and alternately. Whenever a subtree is rebuilt, it is tuned to optimize performance by picking an appropriate setting (e.g., the set of reference points and grid granularity) based on the most recent data and workload. To cut down the overhead of rebuilding, we propose an eager update technique to construct the subtree. Finally, we present a tuning framework for the ST 2 B-tree, where the tuning is conducted online and automatically without human intervention, and without interfering with the regular functions of the MOD. We have implemented the tuning framework and the ST 2 B-tree, and conducted extensive performance evaluations. The results show that the self-tuning mechanism minimizes the degradation of performance caused by workload changes without any noticeable overhead.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2080555007",
    "type": "article"
  },
  {
    "title": "On Provenance Minimization",
    "doi": "https://doi.org/10.1145/2389241.2389249",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Yael Amsterdamer; Daniel Deutch; Tova Milo; Val Tannen",
    "corresponding_authors": "",
    "abstract": "Provenance information has been proved to be very effective in capturing the computational process performed by queries, and has been used extensively as the input to many advanced data management tools (e.g., view maintenance, trust assessment, or query answering in probabilistic databases). We observe here that while different (set-)equivalent queries may admit different provenance expressions when evaluated on the same database, there is always some part of these expressions that is common to all. We refer to this part as the core provenance. In addition to being informative, the core provenance is also useful as a compact input to the aforementioned data management tools. We formally define the notion of core provenance. We study algorithms that, given a query, compute an equivalent (called p-minimal) query that for every input database, the provenance of every result tuple is the core provenance. We study such algorithms for queries of varying expressive power (namely conjunctive queries with disequalities and unions thereof). Finally, we observe that, in general, one would not want to require database systems to execute a specific p-minimal query, but instead to be able to find, possibly off-line, the core provenance of a given tuple in the output (computed by an arbitrary equivalent query), without reevaluating the query. We provide algorithms for such direct computation of the core provenance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2062282899",
    "type": "article"
  },
  {
    "title": "Instant anonymization",
    "doi": "https://doi.org/10.1145/1929934.1929936",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "Mehmet Ercan Nergiz; Acar Tamersoy; Yücel Saygın",
    "corresponding_authors": "",
    "abstract": "Anonymization-based privacy protection ensures that data cannot be traced back to individuals. Researchers working in this area have proposed a wide variety of anonymization algorithms, many of which require a considerable number of database accesses. This is a problem of efficiency, especially when the released data is subject to visualization or when the algorithm needs to be run many times to get an acceptable ratio of privacy/utility. In this paper, we present two instant anonymization algorithms for the privacy metrics k -anonymity and ℓ-diversity. Proposed algorithms minimize the number of data accesses by utilizing the summary structure already maintained by the database management system for query selectivity. Experiments on real data sets show that in most cases our algorithm produces an optimal anonymization, and it requires a single scan of data as opposed to hundreds of scans required by the state-of-the-art algorithms.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2293371736",
    "type": "article"
  },
  {
    "title": "Efficient SimRank-Based Similarity Join",
    "doi": "https://doi.org/10.1145/3083899",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Weiguo Zheng; Lei Zou; Lei Chen; Dongyan Zhao",
    "corresponding_authors": "",
    "abstract": "Graphs have been widely used to model complex data in many real-world applications. Answering vertex join queries over large graphs is meaningful and interesting, which can benefit friend recommendation in social networks and link prediction, and so on. In this article, we adopt “SimRank” [13] to evaluate the similarity between two vertices in a large graph because of its generality. Note that “Simank” is purely structure dependent, and it does not rely on the domain knowledge. Specifically, we define a S im R ank-based j oin ( SRJ ) query to find all vertex pairs satisfying the threshold from two sets of vertices U and V . To reduce the search space, we propose a shortest-path-distance-based upper bound for SimRank scores to prune unpromising vertex pairs. In the verification, we propose a novel index, called h-go cover + , to efficiently compute the SimRank score of any single vertex pair. Given a graph G , we only materialize the SimRank scores of a small proportion of vertex pairs (i.e., the h-go cover + vertex pairs), based on which the SimRank score of any vertex pair can be computed easily. To find the h-go cover + vertex pairs, we propose an efficient method without building the vertex-pair graph. Hence, large graphs can be dealt with easily. Extensive experiments over both real and synthetic datasets confirm the efficiency of our solution.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2740100068",
    "type": "article"
  },
  {
    "title": "Mining order-preserving submatrices from probabilistic matrices",
    "doi": "https://doi.org/10.1145/2533712",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Qiong Fang; Wilfred Ng; Jianlin Feng; Yuliang Li",
    "corresponding_authors": "",
    "abstract": "Order-preserving submatrices (OPSMs) capture consensus trends over columns shared by rows in a data matrix. Mining OPSM patterns discovers important and interesting local correlations in many real applications, such as those involving biological data or sensor data. The prevalence of uncertain data in various applications, however, poses new challenges for OPSM mining, since data uncertainty must be incorporated into OPSM modeling and the algorithmic aspects. In this article, we define new probabilistic matrix representations to model uncertain data with continuous distributions. A novel probabilistic order-preserving submatrix (POPSM) model is formalized in order to capture similar local correlations in probabilistic matrices. The POPSM model adopts a new probabilistic support measure that evaluates the extent to which a row belongs to a POPSM pattern. Due to the intrinsic high computational complexity of the POPSM mining problem, we utilize the anti-monotonic property of the probabilistic support measure and propose an efficient Apriori-based mining framework called ProbApri to mine POPSM patterns. The framework consists of two mining methods, UniApri and NormApri , which are developed for mining POPSM patterns, respectively, from two representative types of probabilistic matrices, the UniDist matrix (assuming uniform data distributions) and the NormDist matrix (assuming normal data distributions). We show that the NormApri method is practical enough for mining POPSM patterns from probabilistic matrices that model more general data distributions. We demonstrate the superiority of our approach by two applications. First, we use two biological datasets to illustrate that the POPSM model better captures the characteristics of the expression levels of biologically correlated genes and greatly promotes the discovery of patterns with high biological significance. Our result is significantly better than the counterpart OPSMRM (OPSM with repeated measurement) model which adopts a set-valued matrix representation to capture data uncertainty. Second, we run the experiments on an RFID trace dataset and show that our POPSM model is effective and efficient in capturing the common visiting subroutes among users.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1967025868",
    "type": "article"
  },
  {
    "title": "Schema matching and embedded value mapping for databases with opaque column names and mixed continuous and discrete-valued data fields",
    "doi": "https://doi.org/10.1145/2445583.2445585",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Anuj Jaiswal; David J. Miller; Prasenjit Mitra",
    "corresponding_authors": "",
    "abstract": "Schema matching and value mapping across two information sources, such as databases, are critical information aggregation tasks. Before data can be integrated from multiple tables, the columns and values within the tables must be matched. The complexities of both these problems grow quickly with the number of attributes to be matched and due to multiple semantics of data values. Traditional research has mostly tackled schema matching and value mapping independently, and for categorical (discrete-valued) attributes. We propose novel methods that leverage value mappings to enhance schema matching in the presence of opaque column names for schemas consisting of both continuous and discrete-valued attributes. An additional source of complexity is that a discrete-valued attribute in one schema could in fact be a quantized, encoded version of a continuous-valued attribute in the other schema. In our approach, which can tackle both “onto” and bijective schema matching, the fitness objective for matching a pair of attributes from two schemas exploits the statistical distribution over values within the two attributes. Suitable fitness objectives are based on Euclidean-distance and the data log-likelihood, both of which are applied in our experimental study. A heuristic local descent optimization strategy that uses two-opt switching to optimize attribute matches, while simultaneously embedding value mappings, is applied for our matching methods. Our experiments show that the proposed techniques matched mixed continuous and discrete-valued attribute schemas with high accuracy and, thus, should be a useful addition to a framework of (semi) automated tools for data alignment.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2084988263",
    "type": "article"
  },
  {
    "title": "Online Updates on Data Warehouses via Judicious Use of Solid-State Storage",
    "doi": "https://doi.org/10.1145/2699484",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Manos Athanassoulis; Shimin Chen; Anastasia Ailamaki; Philip B. Gibbons; Radu Stoica",
    "corresponding_authors": "",
    "abstract": "Data warehouses have been traditionally optimized for read-only query performance, allowing only offline updates at night, essentially trading off data freshness for performance. The need for 24x7 operations in global markets and the rise of online and other quickly reacting businesses make concurrent online updates increasingly desirable. Unfortunately, state-of-the-art approaches fall short of supporting fast analysis queries over fresh data. The conventional approach of performing updates in place can dramatically slow down query performance, while prior proposals using differential updates either require large in-memory buffers or may incur significant update migration cost. This article presents a novel approach for supporting online updates in data warehouses that overcomes the limitations of prior approaches by making judicious use of available SSDs to cache incoming updates. We model the problem of query processing with differential updates as a type of outer join between the data residing on disks and the updates residing on SSDs. We present MaSM algorithms for performing such joins and periodic migrations, with small memory footprints, low query overhead, low SSD writes, efficient in-place migration of updates, and correct ACID support. We present detailed modeling of the proposed approach, and provide proofs regarding the fundamental properties of the MaSM algorithms. Our experimentation shows that MaSM incurs only up to 7% overhead both on synthetic range scans (varying range size from 4KB to 100GB) and in a TPC-H query replay study, while also increasing the update throughput by orders of magnitude.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2089527922",
    "type": "article"
  },
  {
    "title": "Verification of Hierarchical Artifact Systems",
    "doi": "https://doi.org/10.1145/3321487",
    "publication_date": "2019-06-05",
    "publication_year": 2019,
    "authors": "Alin Deutsch; Yuliang Li; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "Data-driven workflows, of which IBM’s Business Artifacts are a prime exponent, have been successfully deployed in practice, adopted in industrial standards, and have spawned a rich body of research in academia, focused primarily on static analysis. The present work represents a significant advance on the problem of artifact verification by considering a much richer and more realistic model than in previous work, incorporating core elements of IBM’s successful Guard-Stage-Milestone model. In particular, the model features task hierarchy, concurrency, and richer artifact data. It also allows database key and foreign key dependencies, as well as arithmetic constraints. The results show decidability of verification and establish its complexity, making use of novel techniques including a hierarchy of Vector Addition Systems and a variant of quantifier elimination tailored to our context.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2950191488",
    "type": "article"
  },
  {
    "title": "On the Expressive Power of Query Languages for Matrices",
    "doi": "https://doi.org/10.1145/3331445",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Robert Brijder; Floris Geerts; Jan Van den Bussche; Timmy Weerwag",
    "corresponding_authors": "",
    "abstract": "We investigate the expressive power of MATLANG, a formal language for matrix manipulation based on common matrix operations and linear algebra. The language can be extended with the operation inv for inverting a matrix. In MATLANG + inv, we can compute the transitive closure of directed graphs, whereas we show that this is not possible without inversion. Indeed, we show that the basic language can be simulated in the relational algebra with arithmetic operations, grouping, and summation. We also consider an operation eigen for diagonalizing a matrix. It is defined such that for each eigenvalue a set of mutually orthogonal eigenvectors is returned that span the eigenspace of that eigenvalue. We show that inv can be expressed in MATLANG + eigen. We put forward the open question whether there are Boolean queries about matrices, or generic queries about graphs, expressible in MATLANG + eigen but not in MATLANG + inv. Finally, the evaluation problem for MATLANG + eigen is shown to be complete for the complexity class ∃ R.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2980953914",
    "type": "article"
  },
  {
    "title": "Adaptive Asynchronous Parallelization of Graph Algorithms",
    "doi": "https://doi.org/10.1145/3397491",
    "publication_date": "2020-06-30",
    "publication_year": 2020,
    "authors": "Wenfei Fan; Ping Lü; Wenyuan Yu; Jingbo Xu; Qiang Yin; Xiaojian Luo; Jingren Zhou; Ruochun Jin",
    "corresponding_authors": "",
    "abstract": "This article proposes an Adaptive Asynchronous Parallel (AAP) model for graph computations. As opposed to Bulk Synchronous Parallel (BSP) and Asynchronous Parallel (AP) models, AAP reduces both stragglers and stale computations by dynamically adjusting relative progress of workers. We show that BSP, AP, and Stale Synchronous Parallel model (SSP) are special cases of AAP. Better yet, AAP optimizes parallel processing by adaptively switching among these models at different stages of a single execution. Moreover, employing the programming model of GRAPE, AAP aims to parallelize existing sequential algorithms based on simultaneous fixpoint computation with partial and incremental evaluation. Under a monotone condition, AAP guarantees to converge at correct answers if the sequential algorithms are correct. Furthermore, we show that AAP can optimally simulate MapReduce, PRAM, BSP, AP, and SSP. Using real-life and synthetic graphs, we experimentally verify that AAP outperforms BSP, AP, and SSP for a variety of graph computations.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3039546816",
    "type": "article"
  },
  {
    "title": "Weaker Forms of Monotonicity for Declarative Networking",
    "doi": "https://doi.org/10.1145/2809784",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Tom J. Ameloot; Bas Ketsman; Frank Neven; Daniel Zinn",
    "corresponding_authors": "",
    "abstract": "The CALM-conjecture, first stated by Hellerstein [2010] and proved in its revised form by Ameloot et al. [2013] within the framework of relational transducer networks, asserts that a query has a coordination-free execution strategy if and only if the query is monotone. Zinn et al. [2012] extended the framework of relational transducer networks to allow for specific data distribution strategies and showed that the nonmonotone win-move query is coordination-free for domain-guided data distributions. In this article, we extend the story by equating increasingly larger classes of coordination-free computations with increasingly weaker forms of monotonicity and present explicit Datalog variants that capture each of these classes. One such fragment is based on stratified Datalog where rules are required to be connected with the exception of the last stratum. In addition, we characterize coordination-freeness as those computations that do not require knowledge about all other nodes in the network, and therefore, can not globally coordinate. The results in this article can be interpreted as a more fine-grained answer to the CALM-conjecture.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2658699200",
    "type": "article"
  },
  {
    "title": "Graph Indexing for Efficient Evaluation of Label-constrained Reachability Queries",
    "doi": "https://doi.org/10.1145/3451159",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Yangjun Chen; Gagandeep Singh",
    "corresponding_authors": "",
    "abstract": "Given a directed edge labeled graph G , to check whether vertex v is reachable from vertex u under a label set S is to know if there is a path from u to v whose edge labels across the path are a subset of S . Such a query is referred to as a label-constrained reachability ( LCR ) query. In this article, we present a new approach to store a compressed transitive closure of G in the form of intervals over spanning trees (forests). The basic idea is to associate each vertex v with two sequences of some other vertices: one is used to check reachability from v to any other vertex, by using intervals, while the other is used to check reachability to v from any other vertex. We will show that such sequences are in general much shorter than the number of vertices in G. Extensive experiments have been conducted, which demonstrates that our method is much better than all the previous methods for this problem in all the important aspects, including index construction times, index sizes, and query times.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3172623769",
    "type": "article"
  },
  {
    "title": "Answering (Unions of) Conjunctive Queries using Random Access and Random-Order Enumeration",
    "doi": "https://doi.org/10.1145/3531055",
    "publication_date": "2022-06-25",
    "publication_year": 2022,
    "authors": "Nofar Carmeli; Shai Zeevi; Christoph Berkholz; Alessio Conte; Benny Kimelfeld; Nicole Schweikardt",
    "corresponding_authors": "",
    "abstract": "As data analytics becomes more crucial to digital systems, so grows the importance of characterizing the database queries that admit a more efficient evaluation. We consider the tractability yardstick of answer enumeration with a polylogarithmic delay after a linear-time preprocessing phase. Such an evaluation is obtained by constructing, in the preprocessing phase, a data structure that supports polylogarithmic-delay enumeration. In this article, we seek a structure that supports the more demanding task of a “random permutation”: polylogarithmic-delay enumeration in truly random order. Enumeration of this kind is required if downstream applications assume that the intermediate results are representative of the whole result set in a statistically meaningful manner. An even more demanding task is that of “random access”: polylogarithmic-time retrieval of an answer whose position is given. We establish that the free-connex acyclic CQs are tractable in all three senses: enumeration, random-order enumeration, and random access; and in the absence of self-joins, it follows from past results that every other CQ is intractable by each of the three (under some fine-grained complexity assumptions). However, the three yardsticks are separated in the case of a union of CQs (UCQ ): while a union of free-connex acyclic CQs has a tractable enumeration, it may (provably) admit no random access. We identify a fragment of such UCQs where we can guarantee random access with polylogarithmic access time (and linear-time preprocessing) and a more general fragment where we can guarantee tractable random permutation. For general unions of free-connex acyclic CQs, we devise two algorithms with relaxed guarantees: one has logarithmic delay in expectation, and the other provides a permutation that is almost uniformly distributed. Finally, we present an implementation and an empirical study that show a considerable practical superiority of our random-order enumeration approach over state-of-the-art alternatives.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2994853131",
    "type": "article"
  },
  {
    "title": "Conjunctive Queries: Unique Characterizations and Exact Learnability",
    "doi": "https://doi.org/10.1145/3559756",
    "publication_date": "2022-08-31",
    "publication_year": 2022,
    "authors": "Balder ten Cate; Víctor Dalmau",
    "corresponding_authors": "",
    "abstract": "We answer the question of which conjunctive queries are uniquely characterized by polynomially many positive and negative examples and how to construct such examples efficiently. As a consequence, we obtain a new efficient exact learning algorithm for a class of conjunctive queries. At the core of our contributions lie two new polynomial-time algorithms for constructing frontiers in the homomorphism lattice of finite structures. We also discuss implications for the unique characterizability and learnability of schema mappings and of description logic concepts.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3049407862",
    "type": "article"
  },
  {
    "title": "Optimal Joins Using Compressed Quadtrees",
    "doi": "https://doi.org/10.1145/3514231",
    "publication_date": "2022-02-23",
    "publication_year": 2022,
    "authors": "Diego Arroyuelo; Gonzalo Navarro; Juan L. Reutter; Javiel Rojas-Ledesma",
    "corresponding_authors": "",
    "abstract": "Worst-case optimal join algorithms have gained a lot of attention in the database literature. We now count several algorithms that are optimal in the worst case, and many of them have been implemented and validated in practice. However, the implementation of these algorithms often requires an enhanced indexing structure: to achieve optimality one either needs to build completely new indexes or must populate the database with several instantiations of indexes such as B \\( + \\) -trees. Either way, this means spending an extra amount of storage space that is typically one or two orders of magnitude more than what is required to store the raw data. We show that worst-case optimal algorithms can be obtained directly from a representation that regards the relations as point sets in variable-dimensional grids, without the need of any significant extra storage. Our representation is a compressed quadtreefor the static indexes and a quadtreebuilt on the fly that shares subtrees (which we dub a qdag) for intermediate results. We develop a compositional algorithm to process full join queries under this representation, which simulates navigation of the quadtreeof the output, and show that the running time of this algorithm is worst-case optimal in data complexity. We implement our index and compare it experimentally with state-of-the-art alternatives. Our experiments show that our index uses even less space than what is needed to store the data in raw form (and replaces it) and one or two orders of magnitude less space than the other indexes. At the same time, our query algorithm is competitive in time, even sharply outperforming other indexes in various cases. Finally, we extend our framework to evaluate more expressive queries from relational algebra, including not only joins and intersections but also unions and negations. To obtain optimality on those more complex formulas, we introduce a lazy version of qdagswe dub lqdags, which allow us navigate over the quadtreerepresenting the output of a formula while only evaluating what is needed from its components. We show that the running time of our query algorithms on this extended set of operations is worst-case optimal under some constraints. Moving to full relational algebra, we also show that lqdagscan handle selections and projections. While worst-case optimality is no longer guaranteed, we introduce a partial materialization scheme that extends results from Deep and Koutris regarding compressed representation of query results.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4213422744",
    "type": "article"
  },
  {
    "title": "Synchronizing Disaggregated Data Structures with One-Sided RDMA: Pitfalls, Experiments and Design Guidelines",
    "doi": "https://doi.org/10.1145/3716377",
    "publication_date": "2025-02-14",
    "publication_year": 2025,
    "authors": "Matthias Jasny; Tobias Ziegler; Jacob Nelson-Slivon; Viktor Leis; Carsten Binnig",
    "corresponding_authors": "",
    "abstract": "Remote data structures built with one-sided Remote Direct Memory Access (RDMA) are at the heart of many disaggregated database management systems today. Concurrent access to these data structures by thousands of remote workers necessitates a highly efficient synchronization scheme. Remarkably, our investigation reveals that existing synchronization schemes display substantial variations in performance and scalability. Even worse, some schemes do not correctly synchronize, resulting in rare and hard-to-detect data corruption. Motivated by these observations, we conduct the first comprehensive analysis of one-sided synchronization techniques and provide general principles for correct synchronization using one-sided RDMA. Our research demonstrates that adherence to these principles not only guarantees correctness but also results in substantial performance enhancements. This paper is an extended version of [73] in which we investigate modern 400G NICs. Our findings reveal that the challenges persist even with new generations of NICs. Consequently, we turn our attention to alternative networking hardware, such as smart switches, to address some of the limitations associated with one-sided synchronization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407583791",
    "type": "article"
  },
  {
    "title": "Cost-effective Missing Value Imputation for Data-effective Machine Learning",
    "doi": "https://doi.org/10.1145/3716376",
    "publication_date": "2025-03-08",
    "publication_year": 2025,
    "authors": "Chengliang Chai; Kaisen Jin; Nan Tang; Ju Fan; Dongjing Miao; Jiayi Wang; Yuyu Luo; Guoliang Li; Ye Yuan; Guoren Wang",
    "corresponding_authors": "",
    "abstract": "Given a dataset with incomplete data (e.g., missing values), training a machine learning model over the incomplete data requires two steps. First, it requires a data-effective step that cleans the data in order to improve the data quality (and the model quality on the cleaned data). Second, it requires a data-efficient step that selects a core subset of the data (called coreset) such that the trained models on the entire data and the coreset have similar model quality, in order to save the computational cost of training. The first-data-effective-then-data-efficient methods are too costly, because they are expensive to clean the whole data; while the first-data-efficient-then-data-effective methods have low model quality, because they cannot select high-quality coreset for incomplete data. In this paper, we investigate the problem of coreset selection over incomplete data for data-effective and data-efficient machine learning. The essential challenge is how to model the incomplete data for selecting high-quality coreset. To this end, we propose the GoodCore framework towards selecting a good coreset over incomplete data with low cost. To model the unknown complete data, we utilize the combinations of possible repairs as possible worlds of the incomplete data. Based on possible worlds, GoodCore selects an expected optimal coreset through gradient approximation without training ML models. We formally define the expected optimal coreset selection problem, prove its NP-hardness, and propose a greedy algorithm with an approximation ratio. To make GoodCore more efficient, we propose optimization methods that incorporate human-in-the-loop imputation or automatic imputation method into our framework. Moreover, a group-based strategy is utilized to further accelerate the coreset selection with incomplete data given large datasets. Experimental results show the effectiveness and efficiency of our framework with low cost.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408256843",
    "type": "article"
  },
  {
    "title": "Scaling and Load-Balancing Equi-Joins",
    "doi": "https://doi.org/10.1145/3722102",
    "publication_date": "2025-03-08",
    "publication_year": 2025,
    "authors": "Ahmed Metwally",
    "corresponding_authors": "Ahmed Metwally",
    "abstract": "The task of joining two tables is fundamental for querying databases. In this paper, we focus on the equi-join problem, where a pair of records from the two joined tables are part of the join results if equality holds between their values in the join column(s). While this is a tractable problem when the number of records in the joined tables is relatively small, it becomes very challenging as the table sizes increase, especially if hot keys (join column values with a large number of records) exist in both joined tables. This paper, an extended version of [60], proposes Adaptive-Multistage-Join (AM-Join) for scalable and fast equi-joins in distributed shared-nothing architectures. AM-Join utilizes (a) Tree-Join, a proposed novel algorithm that scales well when the joined tables share hot keys, and (b) Broadcast-Join, the fastest known algorithm when joining keys that are hot in only one table. Unlike the state-of-the-art algorithms, AM-Join (a) holistically solves the join-skew problem by achieving load balancing throughout the join execution, and (b) supports all outer-join variants without record deduplication or custom table partitioning. For the fastest AM-Join outer-join performance, we propose the Index-Broadcast-Join (IB-Join) family of algorithms for Small-Large joins, where one table fits in memory and the other can be up to orders of magnitude larger. The outer-join variants of IB-Join improves on the state-of-the-art Small-Large outer-join algorithms. The proposed algorithms can be adopted in any shared-nothing architecture. We implemented a MapReduce version using Spark. Our evaluation shows the proposed algorithms execute significantly faster and scale to more skewed and orders-of-magnitude bigger tables when compared to the state-of-the-art algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408256854",
    "type": "article"
  },
  {
    "title": "Better Differentially Private Approximate Histograms and Heavy Hitters using the Misra-Gries Sketch",
    "doi": "https://doi.org/10.1145/3716375",
    "publication_date": "2025-03-21",
    "publication_year": 2025,
    "authors": "Christian Janos Lebeda; Jakub Tětek",
    "corresponding_authors": "",
    "abstract": "We consider the problem of computing differentially private approximate histograms and heavy hitters in a stream of elements. In the non-private setting, this is often done using the sketch of Misra and Gries [Science of Computer Programming, 1982]. Chan, Li, Shi, and Xu [PETS 2012] describe a differentially private version of the Misra-Gries sketch, but the amount of noise it adds can be large and scales linearly with the size of the sketch; the more accurate the sketch is, the more noise this approach has to add. We present a better mechanism for releasing a Misra-Gries sketch under (ε, δ )-differential privacy. It adds noise with magnitude independent of the size of the sketch; in fact, the maximum error coming from the noise is the same as the best known in the private non-streaming setting, up to a constant factor. Our mechanism is simple and likely to be practical. We also give a simple post-processing step of the Misra-Gries sketch that does not increase the worst-case error guarantee. It is sufficient to add noise to this new sketch with less than twice the magnitude of the non-streaming setting. This improves on the previous result for ϵ-differential privacy where the noise scales linearly to the size of the sketch. Finally, we consider a general setting where users can contribute multiple distinct elements. We present a new sketch with maximum error matching the Misra-Gries sketch. For many parameters in this setting our sketch can be released with less noise under (ε, δ )-differential privacy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408688819",
    "type": "article"
  },
  {
    "title": "Allocating Isolation Levels to Transactions in a Multiversion Setting",
    "doi": "https://doi.org/10.1145/3716374",
    "publication_date": "2025-04-08",
    "publication_year": 2025,
    "authors": "Brecht Vandevoort; Bas Ketsman; Frank Neven",
    "corresponding_authors": "",
    "abstract": "A serializable concurrency control mechanism ensures consistency for OLTP systems at the expense of a reduced transaction throughput. A DBMS therefore usually offers the possibility to allocate lower isolation levels for some transactions when it is safe to do so. However, such trading of consistency for efficiency does not come with any safety guarantees. In this paper, we study the mixed robustness problem which asks whether, for a given set of transactions and a given allocation of isolation levels, every possible interleaved execution of those transactions that is allowed under the provided allocation is always serializable. That is, whether the given allocation is indeed safe. While robustness has already been studied in the literature for the homogeneous setting where all transactions are allocated the same isolation level, the heterogeneous setting that we consider in this paper, despite its practical relevance, has largely been ignored. We focus on multiversion concurrency control and consider the isolation levels that are available in PostgreSQL and Oracle: read committed (RC), snapshot isolation (SI) and serializable snapshot isolation (SSI). We show that the mixed robustness problem can be decided in polynomial time. In addition, we provide a polynomial time algorithm for computing the optimal robust allocation for a given set of transactions, prioritizing lower over higher isolation levels. The present results therefore establish the groundwork to automate isolation level allocation within existing databases supporting multiversion concurrency control.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409269388",
    "type": "article"
  },
  {
    "title": "Graph Based K-Nearest Neighbor Search Revisited",
    "doi": "https://doi.org/10.1145/3736716",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Jiadong Xie; Jeffrey Xu Yu; Yingfan Liu",
    "corresponding_authors": "",
    "abstract": "The problem of k -nearest neighbor ( k -NN) search is a fundamental problem to find the exact k nearest neighbor points for a user-given query point q in a d -dimensional large dataset D with n points, and the approximate k -NN ( k -ANN) search problem is to find the approximate k -NN. Both are extensively studied to support real applications. Among all approaches, the graph-based approaches have been seen as the best to support k -NN/ANN in recent studies. The state-of-the-art graph-based approach, τ -MG, finds 1-NN, \\(\\bar{p}_1 \\) , over a graph index G τ constructed for D based on a predetermined parameter τ where the distance between \\(\\bar{p}_1 \\) and q is less than τ , and finds k -ANN based on the approach taken for 1-NN. There are some main issues in τ -MG and other graph-based approaches. One is that it is difficult to predetermine τ which can ensure to find 1-NN and can do it efficiently. This is because the accuracy/efficiency is related to the size of the graph index G τ constructed. To achieve high accuracy is at the expense of efficiency. In addition, like all the other existing graph-based approaches, it does not have a theoretical guarantee to ensure k -NN for the same reason to use the same graph index, G τ , for both 1-NN and k -NN ( k &gt; 1). In this paper, we propose a new graph-based approach for k -NN with a theoretical guarantee. We construct a labeled graph, \\(\\mathcal {G} \\) , and we do not need to predetermine τ . Instead, we find 1-NN over a subgraph, \\(\\mathcal {G}_{\\dot{\\tau }} \\) , of \\(\\mathcal {G} \\) , virtually constructed in a dynamic manner. Here, \\(\\dot{\\tau } \\) we use is query-dependent and can be smaller than τ , and the subgraph \\(\\mathcal {G}_{\\dot{\\tau }} \\) is smaller than G τ when \\(\\dot{\\tau } = \\tau \\) . We find k -NN in two phases. In the navigation phase, we find 1-NN, \\(\\bar{p}_1 \\) , of q over \\(\\mathcal {G}_{\\dot{\\tau }} \\) . In the second refinement phase, for k &gt; 1, we explore the neighbors within the vicinity region of \\(\\bar{p}_1 \\) in \\(\\mathcal {G} \\) . Based on our solution for k -NN in theory, we propose new algorithms to support k -ANN efficiently in practice. We conduct extensive performance studies and confirm the effectiveness and efficiency of our new approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410564860",
    "type": "article"
  },
  {
    "title": "Efficient Parallel Boolean Expression Matching",
    "doi": "https://doi.org/10.1145/3736756",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "Shuping Ji; Jianguo Yao; Wei Wang; Jun Wei; Hans‐Arno Jacobsen",
    "corresponding_authors": "",
    "abstract": "Boolean expression matching plays an important role in many applications. However, existing solutions still show efficiency and scalability limitations. For example, existing solutions often exhibit degraded performance when applied to high-dimensional and diverse workloads, and existing algorithms rarely consider supporting concurrent matching and index updating under multicore environments. To overcome these limitations, in this article, we first design the PS-Tree data structure to efficiently index Boolean expressions in one dimension. By dividing predicates into disjoint predicate spaces, PS-Tree achieves high matching performance and good expressiveness. Based on the PS-Tree , we propose a Boolean expression matching algorithm called PSTDynamic . By dynamically adjusting the index and efficiently filtering out a large proportion of unmatching expressions, PSTDynamic achieves high matching performance under high-dimensional and diverse workloads. For multicore environment, we further extend the PSTDynamic algorithm to PSTParallel to achieve scalability with lower matching latency and higher matching throughput. We run experiments on both synthetic and real-world datasets. The experiments verify that our proposed algorithms show high efficiency and parallelism. Moreover, they also achieve fast index construction and a small memory footprint. Comprehensive experiments show that our solutions drastically outperform state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410830018",
    "type": "article"
  },
  {
    "title": "Degree Sequence Bounds",
    "doi": "https://doi.org/10.1145/3716378",
    "publication_date": "2025-05-29",
    "publication_year": 2025,
    "authors": "Kyle Deeds; Dan Suciu; Magdalena Balazinska; Walter Cai",
    "corresponding_authors": "",
    "abstract": "Recent work has demonstrated the catastrophic effects of poor cardinality estimates on query processing time. In particular, underestimating query cardinality can result in overly optimistic query plans which take orders of magnitude longer to complete than one generated with the true cardinality. Cardinality bounding avoids this pitfall by computing an upper bound on the query’s output size using statistics about the database such as table sizes and degrees, i.e. value frequencies. In this paper, we extend this line of work by proving a novel bound called the Degree Sequence Bound which takes into account the full degree sequences and the max tuple multiplicity. This work focuses on the important class of Berge-Acyclic queries for which the Degree Sequence Bound is tight and provably improves on prior work. We further describe how to practically compute this bound using a functional approximation of the true degree sequences and prove that even this functional form improves upon previous bounds. Lastly, we outline the challenges of implementing this in a real system and some techniques for overcoming these challenges.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410863843",
    "type": "article"
  },
  {
    "title": "The Bounded Cardinality Normal Form for the Logical Design of Relational Database Schemata",
    "doi": "https://doi.org/10.1145/3744897",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Ziheng Wei; Sebastian Link",
    "corresponding_authors": "",
    "abstract": "The goal of classical normalization is to maintain data consistency under updates, with a minimum level of effort. Given functional dependencies (FDs) alone, this goal is only achievable in the special case an FD-preserving Boyce–Codd Normal Form (BCNF) decomposition exists. As we show, in all other cases the level of effort can be neither controlled nor quantified. In response, we establish the ℓ-Bounded Cardinality Normal Form, parameterized by a positive integer ℓ. For every ℓ, the normal form condition requires from every instance that every value combination over the left-hand side of every non-trivial FD does not occur in more than ℓ tuples. BCNF is captured when ℓ =1. We show that schemata in ℓ-Bounded Cardinality Normal Form characterize instances in which updates to at most ℓ occurrences of any redundant data value are sufficient to maintain data consistency. In fact, for the smallest ℓ in which a schema is in ℓ-Bounded Cardinality Normal Form we capture an equilibrium between worst-case update inefficiency and best-case join efficiency, where some redundant data value can be joined with up to ℓ other data values. We then establish algorithms that compute schemata in ℓ-Bounded Cardinality Normal Form for the smallest level ℓ attainable across all lossless, FD-preserving decompositions. Additional algorithms (i) attain even smaller levels of effort based on the loss of some FDs, and (ii) decompose schemata based on prioritized FDs that cause high levels of effort. Our framework informs de-normalization already during logical design. In particular, every materialized view exhibits an equilibrium level ℓ that quantifies its worst-case incremental maintenance cost and its best-case support for join queries. Experiments with synthetic and real-world data illustrate which properties the schemata have that result from our algorithms, and how these properties predict the performance of update and query operations on instances over the schemata, without and with materialized views. We further demonstrate how our framework can automate the design of data warehouses by mining data for dimensions that exhibit high levels of data redundancy. In an effort to align data and the business rules that govern them, we use lattice theory to characterize ℓ-Bounded Cardinality Normal Form on database instances and schemata. As a consequence, any difference in constraints observed at instance and schema levels provides an opportunity to improve data quality, insight derived from analytics, and database performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411378076",
    "type": "article"
  },
  {
    "title": "Any-k Algorithms for Enumerating Ranked Answers to Conjunctive Queries",
    "doi": "https://doi.org/10.1145/3734517",
    "publication_date": "2025-06-25",
    "publication_year": 2025,
    "authors": "Nikolaos Tziavelis; Wolfgang Gatterbauer; Mirek Riedewald",
    "corresponding_authors": "",
    "abstract": "We study ranked enumeration for Conjunctive Queries (CQs) where the answers are ordered by a given ranking function (e.g., an ORDER BY clause in SQL). We develop “ any-k ” algorithms, which, without knowing the number k of desired answers, push down the ranking into joins by carefully ordering the computation of intermediate tuples and avoiding materialization of join answers until they are needed. For this to be possible, the ranking function needs to obey a certain type of monotonicity. Supported ranking functions include the common sum-of-weights, where answers are compared by the sum of input-tuple weights, as well as any commutative selective dioid. Our results extend a well-known unranked-enumeration dichotomy, which states that only free-connex CQs are tractable (under certain hardness hypotheses and for CQs without self-joins). For this class of queries and appropriately monotone ranking functions, the data complexity of our ranked enumeration approach for the time to the \\(k^\\textrm {th} \\) CQ answer is \\(\\mathcal {O}(n + k \\log k) \\) with n denoting the size of the input, which is only a logarithmic factor slower than the \\(\\mathcal {O}(n + k) \\) unranked-enumeration guarantee. A core insight of our work is that ranked enumeration for CQs is closely related to Dynamic Programming and the fundamental task of path enumeration in a weighted DAG . We uncover a previously unknown tradeoff, both for this problem and for CQs, under the lens of combined complexity where the query size is not considered a constant: one any- k algorithm has lower complexity when the number of returned answers is small, the other when their number is large. This tradeoff is eliminated under a stricter monotonicity property that we define and exploit for a novel algorithm that asymptotically dominates all previously known alternatives , including Eppstein’s algorithm for sum-of-weights path enumeration. We empirically demonstrate the findings of our theoretical analysis in an experimental study that highlights the superiority of our approach over the join-then-rank approach that existing database systems follow.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411652453",
    "type": "article"
  },
  {
    "title": "Editorial: A Message from the New Editor-in-Chief",
    "doi": "https://doi.org/10.1145/3736110",
    "publication_date": "2025-06-30",
    "publication_year": 2025,
    "authors": "Yufei Tao",
    "corresponding_authors": "Yufei Tao",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411817760",
    "type": "editorial"
  },
  {
    "title": "BISLearner: Block-Aware Index Selection using Attention-Based Reinforcement Learning for Data Analytics",
    "doi": "https://doi.org/10.1145/3760773",
    "publication_date": "2025-08-15",
    "publication_year": 2025,
    "authors": "Yulai Tong; Fengrui Liu; Jinhui Xu; Hua Wang; Ke Zhou; Junxiang Miao; Cheng Wang; Rongfeng He",
    "corresponding_authors": "",
    "abstract": "The development of data analytics services has fueled many optimizations in data scans, and indexes are one of the most important techniques to improve scan efficiency. Meanwhile, block-based data organization has become standard practice in these services, providing an opportunity for more fine-grained index selection at the block level. However, today’s systems ignore data distribution differences among blocks and usually tune indexes over the entire database table, leading to unnecessary storage costs and potential degradation in query performance. To bridge this gap, we propose BISLearner, a fast, block-aware index selecting approach based on reinforcement learning. One major challenge lies in differentiating the data distribution among data blocks. To solve this problem, BISLearner maintains simplified histograms that represent the data distribution of each block. When a query is issued, BISLearner leverages the query predicate and histogram-based block summaries to generate a specific workload representation for each block. However, such block-aware workload representation leads to an excessive number of input features, resulting in a slow or even incorrect convergence of neural networks. Inspired by the human-learning process, where more attention is devoted to the important parts of data, we design an attention-based neural model to efficiently handle the high volume of input features caused by table partitioning and select the best-suited index combinations at the block level. Additionally, to handle the expensive search space caused by attribute combinations and data partitioning, we employ heuristic-based invalid action masking at the block level to accelerate the training process. Our evaluation using PostgreSQL and Greenplum database systems demonstrates BISLearner is able to reduce job completion time by up to 28.45% compared to its best counterparts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413381037",
    "type": "article"
  },
  {
    "title": "Conjunctive Queries with Comparisons",
    "doi": "https://doi.org/10.1145/3769424",
    "publication_date": "2025-09-25",
    "publication_year": 2025,
    "authors": "Qichen Wang; Ke Yi",
    "corresponding_authors": "",
    "abstract": "Conjunctive queries with predicates in the form of comparisons that span multiple relations have regained interest recently, due to their relevance in OLAP queries, spatiotemporal databases, and machine learning over relational data. The standard technique, predicate pushdown, has limited efficacy on such comparisons. A technique by Willard can be used to process short comparisons that are adjacent in the join tree in time linear in the input size plus output size. In this paper, we describe a new algorithm for evaluating conjunctive queries with both short and long comparisons, and identify an acyclic condition under which linear time can be achieved. We have also implemented the new algorithm on top of Spark, and our experimental results demonstrate order-of-magnitude speedups over SparkSQL on a variety of graph pattern and analytical queries.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414490952",
    "type": "article"
  },
  {
    "title": "Efficient Path Oracles for Proximity Queries on Point Clouds",
    "doi": "https://doi.org/10.1145/3770577",
    "publication_date": "2025-10-02",
    "publication_year": 2025,
    "authors": "Yinzhao Yan; Raymond Chi-Wing Wong",
    "corresponding_authors": "",
    "abstract": "The prevalence of computer graphics technology boosts the development of point clouds, which offer advantages over T riangular I rregular N etworks , i.e., TIN s, in proximity queries. All existing on-the-fly shortest path query algorithms and oracles on a TIN are expensive, and no algorithms can answer shortest path queries on a point cloud directly. Thus, we propose two types of efficient shortest path oracles on a point cloud. They answer the shortest path query between (1) a pair of P oints- O f- I nterests ( POIs ), and (2) any point and a POI, respectively. We propose four adaptations of them to answer the query between any point and a POI (or any point if no POIs are given). We also propose two efficient proximity query algorithms using these oracles. Our two oracles and their proximity query algorithms outperform the best-known adapted oracle by 12 to 42,000 times in terms of the oracle construction time, oracle size and proximity query time, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414747296",
    "type": "article"
  },
  {
    "title": "Saga++: A Scalable Framework for Optimizing Data Cleaning Pipelines for Machine Learning Applications",
    "doi": "https://doi.org/10.1145/3771766",
    "publication_date": "2025-10-14",
    "publication_year": 2025,
    "authors": "Shafaq Siddiqi; Arnab Phani; Roman Kern; Matthias Böehm",
    "corresponding_authors": "",
    "abstract": "In the exploratory data science lifecycle, data scientists often spent the majority of their time finding, integrating, validating and cleaning relevant datasets. Despite recent work on data validation, and numerous error detection and correction algorithms, in practice, data cleaning for ML remains largely a manual, unpleasant, and labor-intensive trial and error process, especially in large-scale, distributed computation settings. The target ML application—such as classification or regression models—can be used as a signal of valuable feedback though, for selecting effective data cleaning strategies. In this paper, we introduce Saga++ , a framework for automatically generating the top-K most effective data cleaning pipelines. Saga++ adopts ideas from Auto-ML, feature selection, and hyper-parameter tuning. Our framework is extensible for user-provided constraints, new data cleaning primitives, and ML applications; automatically generates hybrid runtime plans of local and distributed operations; and performs pruning by interesting properties (e.g., monotonicity). Furthermore, we exploit guided sampling on the input dataset to enable enumeration on a smaller subset, reducing the time required to discover the top-K pipelines. As a post-processing step, we also perform pipeline pruning on the selected top-K pipelines, removing redundant and less effective cleaning primitives. Instead of full automation—which is rather unrealistic— Saga++ simplifies the mechanical aspects of data cleaning. Our experiments show that Saga++ yields robust accuracy improvements over state-of-the-art, and good scalability regarding increasing data sizes and number of evaluated pipelines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415148192",
    "type": "article"
  },
  {
    "title": "T-FSM: A Scalable Distributed Task-Based System for Frequent Subgraph Pattern Mining from a Big Graph",
    "doi": "https://doi.org/10.1145/3771994",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Lyuheng Yuan; Da Yan; Dingwen Tao; Jiao Han; Saugat Adhikari; Cheng Long; Yang Zhou",
    "corresponding_authors": "",
    "abstract": "Finding frequent subgraph patterns in a big graph is an important problem with many applications such as classifying chemical compounds and building indexes to speed up graph queries. Since this problem is NP-hard, some recent parallel and distributed systems have been developed to accelerate the mining. However, they often have a huge memory cost, very long running time, suboptimal load balancing, poor scale-out capability, and possibly inaccurate results. In this paper, we propose an efficient system called T-FSM for parallel mining of frequent subgraph patterns in a big graph. T-FSM supports a new anti-monotonic frequentness measure called Fraction-Score, which is more accurate than the widely used MNI measure. The execution engine of T-FSM supports both intra-machine parallelism and inter-machine parallelism. For intra-machine parallelism, T-FSM adopts a novel task-based execution model to ensure high multithreading concurrency, bounded memory consumption, and effective load balancing. For inter-machine parallelism, T-FSM ensures good scale-out performance with a lightweight pattern rebalancing approach that reduces workload skewness of pattern evaluations among machines. To avoid recomputing the contexts for migrated patterns, we design a novel context cache table to support concurrent and asynchronous requesting and caching of remote context data, which can timely evict and garbage collect used pattern contexts that are no longer needed to keep memory consumption bounded. Extensive experiments show that T-FSM is orders of magnitude faster than existing state-of-the-art parallel systems (more than 10 ×, 51 ×, 131 ×, 55 × speedup over ScaleMine, DistGraph, Pangolin and Peregrine, respectively) and distributed systems (more than 42 × and 88 × over ScaleMine and DistGraph, respectively) for frequent subgraph pattern mining, and it scales out satisfactorily to 512 CPU cores on the Polaris supercomputer at Argonne National Laboratory.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415243163",
    "type": "article"
  },
  {
    "title": "Magic conditions",
    "doi": "https://doi.org/10.1145/227604.227624",
    "publication_date": "1996-03-01",
    "publication_year": 1996,
    "authors": "Inderpal Singh Mumick; Sheldon J. Finkelstein; Hamid Pirahesh; Raghu Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Much recent work has focused on the bottom-up evaluation of Datalog programs [Bancilhon and Ramakrishnan 1988]. One approach, called magic-sets, is based on rewriting a logic program so that bottom-up fixpoint evaluation of the program avoids generation of irrelevant facts [Bancilhon et al. 1986; Beeri and Ramakrishnan 1987; Ramakrishnan 1991]. It was widely believed for some time that the principal application of the magic-sets technique is to restrict computation in recursive queries using equijoin predicates. We extend the magic-sets transformation to use predicates other than equality ( X &gt;10, for example) in restricting computation. The resulting ground magic-sets transformation is an important step in developing an extended magic-sets transformation that has practical utility in “real” relational databases, not only for recursive queries, but for nonrecursive queries as well [Mumick et al. 1990b; Mumick 1991].",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2295651970",
    "type": "article"
  },
  {
    "title": "Model and verification of a data manager based on ARIES",
    "doi": "https://doi.org/10.1145/236711.236712",
    "publication_date": "1996-12-01",
    "publication_year": 1996,
    "authors": "Dean Kuo",
    "corresponding_authors": "Dean Kuo",
    "abstract": "In this article, we model and verify a data manager whose algorithm is based on ARIES. The work uses the I/O automata method as the formal model and the definition of correctness is defined on the interface between the scheduler and the data manager.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2053200599",
    "type": "article"
  },
  {
    "title": "AGM: a dataflow database machine",
    "doi": "https://doi.org/10.1145/62032.62037",
    "publication_date": "1989-03-01",
    "publication_year": 1989,
    "authors": "Lubomir Bic; Robert L. Hartmann",
    "corresponding_authors": "",
    "abstract": "In recent years, a number of database machines consisting of large numbers of parallel processing elements have been proposed. Unfortunately, there are two main limitations in database processing that prevent a high degree of parallelism; these are the available I/O bandwidth of the underlying storage devices and the concurrency control mechanisms necessary to guarantee data integrity. The main problem with conventional approaches is the lack of a computational model capable of utilizing the potential of any significant number of processing elements and storage devices and, at the same time, preserving the integrity of the database. This paper presents a database model and its associated architecture, which is based on the principles of data-driven computation. According to this model, the database is represented as a network in which each node is conceptually an independent, asynchronous processing element, capable of communicating with other nodes by exchanging messages along the network arcs. To answer a query, one or more such messages, called tokens, are created and injected into the network. These then propagate asynchronously through the network in search of results satisfying the given query. The asynchronous nature of processing permits the model to be mapped onto a computer architecture consisting of large numbers of independent disk units and processing elements. This increases both the available I/O bandwidth as well as the processing potential of the machine. At the same time, new concurrency control and error recovery mechanisms are necessary to cope with the resulting parallelism.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1969054202",
    "type": "article"
  },
  {
    "title": "Computation-tuple sequences and object histories",
    "doi": "https://doi.org/10.1145/5922.5924",
    "publication_date": "1986-06-01",
    "publication_year": 1986,
    "authors": "Seymour Ginsburg; Katsumi Tanaka",
    "corresponding_authors": "",
    "abstract": "A record-based, algebraically-oriented model is introduced for describing data for “object histories” (with computation), such as checking accounts, credit card accounts, taxes, schedules, and so on. The model consists of sequences of computation tuples defined by a computation-tuple sequence scheme (CSS). The CSS has three major features (in addition to input data): computation (involving previous computation tuples), “uniform” constraints (whose satisfaction by a computation-tuple sequence u implies satisfaction by every interval of u ), and specific sequences with which to start the valid computation-tuple sequences. A special type of CSS, called “local,” is singled out for its relative simplicity in maintaining the validity of a computation-tuple sequence. A necessary and sufficient condition for a CSS to be equivalent to at least one local CSS is given. Finally, the notion of “local bisimulatability” is introduced for regarding two CSS as conveying the same information, and two results on local bisimulatability in connection with local CSS are established.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1983236753",
    "type": "article"
  },
  {
    "title": "A framework for effective retrieval",
    "doi": "https://doi.org/10.1145/63500.63519",
    "publication_date": "1989-06-01",
    "publication_year": 1989,
    "authors": "C. Yu; Wei Meng; S. Park",
    "corresponding_authors": "",
    "abstract": "The aim of an effective retrieval system is to yield high recall and precision (retrieval effectiveness). The nonbinary independence model, which takes into consideration the number of occurrences of terms in documents, is introduced. It is shown to be optimal under the assumption that terms are independent. It is verified by experiments to yield significant improvement over the binary independence model. The nonbinary model is extended to normalized vectors and is applicable to more general queries. Various ways to alleviate the consequences of the term independence assumption are discussed. Estimation of parameters required for the nonbinary independence model is provided, taking into consideration that a term may have different meanings.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1989879765",
    "type": "article"
  },
  {
    "title": "A problem-oriented inferential database system",
    "doi": "https://doi.org/10.1145/6314.6419",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Eliezer L. Lozinskii",
    "corresponding_authors": "Eliezer L. Lozinskii",
    "abstract": "Recently developed inferential database systems face some common problems: a very fast growth of search space and difficulties in recognizing inference termination (especially for recursive axioms). These shortcomings stem mainly from the fact that the inference process is usually separated from database operations. A problem-oriented inferential system i8 described which refers to the database prior to query (or subquery) processing, so that the inference from the very beginning is directed by data relevant to the query. A multiprocessor implementation of the system is presented based on a computer network conforming to database relations and axioms. The system provides an efficient indication of query termination, and is complete in the sense that it produces all correct answers to a query in a finite time.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2033906060",
    "type": "article"
  },
  {
    "title": "On estimating the cardinality of the projection of a database relation",
    "doi": "https://doi.org/10.1145/62032.62034",
    "publication_date": "1989-03-01",
    "publication_year": 1989,
    "authors": "Rafiul Ahad; K. V. Bapa; Dennis McLeod",
    "corresponding_authors": "",
    "abstract": "We present an analytical formula for estimating the cardinality of the projection on certain attributes of a subset of a relation in a relational database. This formula takes into account a priori knowledge of the semantics of the real-world objects and relationships that the database is intended to represent. Experimental testing of the formula shows that it has an acceptably low percentage error, and that its worst-case error is smaller than the best-known formula. Furthermore, the formula presented here has the advantage that it does not require a scan of the relation.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1991782662",
    "type": "article"
  },
  {
    "title": "Deadlock freedom using edge locks",
    "doi": "https://doi.org/10.1145/319758.319772",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "Henry F. Korth",
    "corresponding_authors": "Henry F. Korth",
    "abstract": "We define a series of locking protocols for database systems that all have three main features: freedom from deadlock, multiple granularity, and support for general collections of locking primitives. A rooted directed acyclic graph is used to represent multiple granularities, as in System R. Deadlock freedom is guaranteed by extending the System R protocol to require locks on edges of the graph in addition to the locks required on nodes.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2049461375",
    "type": "article"
  },
  {
    "title": "Self-tuning cost modeling of user-defined functions in an object-relational DBMS",
    "doi": "https://doi.org/10.1145/1093382.1093387",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Zhen He; Byung Suk Lee; Robert R. Snapp",
    "corresponding_authors": "",
    "abstract": "Query optimizers in object-relational database management systems typically require users to provide the execution cost models of user-defined functions (UDFs). Despite this need, however, there has been little work done to provide such a model. The existing approaches are static in that they require users to train the model a priori with pregenerated UDF execution cost data. Static approaches can not adapt to changing UDF execution patterns and thus degrade in accuracy when the UDF executions used for generating training data do not reflect the patterns of those performed during operation. This article proposes a new approach based on the recent trend of self-tuning DBMS by which the cost model is maintained dynamically and incrementally as UDFs are being executed online. In the context of UDF cost modeling, our approach faces a number of challenges, that is, it should work with limited memory, work with limited computation time, and adjust to the fluctuations in the execution costs (e.g., caching effect). In this article, we first provide a set of guidelines for developing techniques that meet these challenges, while achieving accurate and fast cost prediction with small overheads. Then, we present two concrete techniques developed under the guidelines. One is an instance-based technique based on the conventional k -nearest neighbor (KNN) technique which uses a multidimensional index like the R*-tree. The other is a summary-based technique which uses the quadtree to store summary values at multiple resolutions. We have performed extensive performance evaluations comparing these two techniques against existing histogram-based techniques and the KNN technique, using both real and synthetic UDFs/data sets. The results show our techniques provide better performance in most situations considered.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2066140355",
    "type": "article"
  },
  {
    "title": "Concise descriptions of subsets of structured sets",
    "doi": "https://doi.org/10.1145/1061318.1061324",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Ken Q. Pu; Alberto O. Mendelzon",
    "corresponding_authors": "",
    "abstract": "We study the problem of economical representation of subsets of structured sets, which are sets equipped with a set cover or a family of preorders. Given a structured set U , and a language L whose expressions define subsets of U , the problem of minimum description length in L (L-MDL) is: “given a subset V of U , find a shortest string in L that defines V .” Depending on the structure and the language, the MDL-problem is in general intractable. We study the complexity of the MDL-problem for various structures and show that certain specializations are tractable. The families of focus are hierarchy, linear order, and their multidimensional extensions; these are found in the context of statistical and OLAP databases. In the case of general OLAP databases, data organization is a mixture of multidimensionality, hierarchy, and ordering, which can also be viewed naturally as a cover-structured ordered set. Efficient algorithms are provided for the MDL-problem for hierarchical and linearly ordered structures, and we prove that the multidimensional extensions are NP-complete. Finally, we illustrate the application of the theory to summarization of large result sets and (multi) query optimization for ROLAP queries.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2002932370",
    "type": "article"
  },
  {
    "title": "Analysis of index-sequential files with overflow chaining",
    "doi": "https://doi.org/10.1145/319628.319665",
    "publication_date": "1981-12-01",
    "publication_year": 1981,
    "authors": "Per-Åke Larson",
    "corresponding_authors": "Per-Åke Larson",
    "abstract": "The gradual performance deterioration caused by deletions from and insertions into an index-sequential file after loading is analyzed. The model developed assumes that overflow records are handled by chaining. Formulas for computing the expected number of overflow records and the expected number of additional accesses caused by the overflow records for both successful and unsuccessful searches are derived.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2019945163",
    "type": "article"
  },
  {
    "title": "Analysis of a heuristic for full trie minimization",
    "doi": "https://doi.org/10.1145/319587.319618",
    "publication_date": "1981-09-01",
    "publication_year": 1981,
    "authors": "Douglas E. Comer",
    "corresponding_authors": "Douglas E. Comer",
    "abstract": "A trie is a distributed-key search tree in which records from a file correspond to leaves in the tree. Retrieval consists of following a path from one root to a leaf, where the choice of edge at each node is determined by attribute values of the key. For full tries, those in which all leaves lie at the same depth, the problem of finding an ordering of attributes which yields a minimum size trie is NP-complete. This paper considers a “greedy” heuristic for constructing low-cost tries. It presents simulation experiments which show that the greedy method tends to produce tries with small size, and analysis leading to a worst case bound on approximations produced by the heuristic. It also shows a class of files for which the greedy method may perform badly, producing tries of high cost.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2033689951",
    "type": "article"
  },
  {
    "title": "Design of an external schema facility to define and process recursive structures",
    "doi": "https://doi.org/10.1145/319566.319576",
    "publication_date": "1981-06-01",
    "publication_year": 1981,
    "authors": "﻿Eric K. Clemons",
    "corresponding_authors": "﻿Eric K. Clemons",
    "abstract": "The role of the external schema is to support user views of data and thus to provide programmers with easier data access. This author believes that an external schema facility is best based on hierarchies, both simple and recursive. After a brief introduction to an external schema facility to support simple hierarchical user views, the requirements for a facility for recursive hierarchies are listed and the necessary extensions to the external schema definition language are offered. Functions that must be provided for generality in definition are node specification and node control. Tree traversal functions must be provided for processing. Definitions of each and examples of use are presented.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2034951428",
    "type": "article"
  },
  {
    "title": "Mathematical models of database degradation",
    "doi": "https://doi.org/10.1145/319758.319771",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "Daniel P. Heyman",
    "corresponding_authors": "Daniel P. Heyman",
    "abstract": "As data are updated, the initial physical structure of a database is changed and retrieval of specific pieces of data becomes more time consuming. This phenomenon is called database degradation. In this paper two models of database degradation are described. Each model refers to a different aspect of the problem. It is assumed that transactions are statistically independent and either add, delete, or update data. The first model examines the time during which a block of data is filling up. The second model examines the overflows from a block of data, which essentially describes the buildup of disorganization. Analytical results are obtained for both models. In addition, several numerical examples are presented which show that the mean number of overflows grows approximately linearly with time. This approximation is used to devise a simple formula for the optimal time to reorganize a stochastically growing database.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2039076322",
    "type": "article"
  },
  {
    "title": "Transformation of data traversals and operations in application programs to account for semantic changes of databases",
    "doi": "https://doi.org/10.1145/319566.319573",
    "publication_date": "1981-06-01",
    "publication_year": 1981,
    "authors": "Stanley Y. W. Su; Herman Lam; Der Her Lo",
    "corresponding_authors": "",
    "abstract": "This paper addresses the problem of application program conversion to account for changes in database semantics that result in changes in the schema and database contents. With the observation that the existing data models can be viewed as alternative ways of modeling the same database semantics, a methodology of application program analysis and conversion based on an existing-DBMS-model-and schema-independent representation of both the database and programs is presented. In this methodology, the source and target databases are described in terms of the association types of a semantic association model. The structural properties, the integrity constraints, and the operational characteristics (storage operation behaviors) of the association types are more explicitly defined to reveal the semantics that is generally hidden in application programs. The explicit descriptions of the source and target databases are used as the basis for program analysis and conversion. Application programs are described in terms of a small number of “access patterns” which define the data traversals and operations of the programs. In addition to the methodology, this paper (1) describes a model of a generalized application program conversion system that serves as a framework for research, (2) presents an analysis of access patterns that serve as the primitives for program description, (3) delineates some meaningful semantic changes to databases and their corresponding transformation rules for program conversion, (4) illustrates the application of these rules to two different approaches to program conversion problems, and (5) reports on the development effort undertaken at the University of Florida.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1966676907",
    "type": "article"
  },
  {
    "title": "Heuristics for trie index minimization",
    "doi": "https://doi.org/10.1145/320083.320102",
    "publication_date": "1979-09-01",
    "publication_year": 1979,
    "authors": "Douglas E. Comer",
    "corresponding_authors": "Douglas E. Comer",
    "abstract": "A trie is a digital search tree in which leaves correspond to records in a file. Searching proceeds from the root to a leaf, where the edge taken at each node depends on the value of an attribute in the query. Trie implementations have the advantage of being fast, but the disadvantage of achieving that speed at great expense in storage space. Of primary concern in making a trie practical, therefore, is the problem of minimizing storage requirements. One method for reducing the space required is to reorder attribute testing. Unfortunately, the problem of finding an ordering which guarantees a minimum-size trie is NP-complete. In this paper we investigate several heuristics for reordering attributes, and derive bounds on the sizes of the worst tries produced by them in terms of the underlying file. Although the analysis is presented for a binary file, extensions to files of higher degree are shown. Another alternative for reducing the space required by a trie is an implementation, called an Ο-trie, in which the order of attribute testing is contained in the trie itself. We show that for most applications, Ο-tries are smaller than other implementations of tries, even when heuristics for improving storage requirements are employed.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1986404066",
    "type": "article"
  },
  {
    "title": "Joined normal form",
    "doi": "https://doi.org/10.1145/319758.319768",
    "publication_date": "1982-12-01",
    "publication_year": 1982,
    "authors": "E. Babb",
    "corresponding_authors": "E. Babb",
    "abstract": "A new on-line query language and storage structure for a database machine is presented. By including a mathematical model in the interpreter the query language has been substantially simplified so that no reference to relation names is necessary. By storing the model as a single joined normal form (JNF) file, it has been possible to exploit the powerful search capability of the Content Addressable File Store (CAFS®; CAFS is a registered trademark of International Computers Limited) database machine.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2027009365",
    "type": "article"
  },
  {
    "title": "High-level programming features for improving the efficiency of a relational database system",
    "doi": "https://doi.org/10.1145/319587.319604",
    "publication_date": "1981-09-01",
    "publication_year": 1981,
    "authors": "Reind P. van de Riet; Martin Kersten; Wiebren de Jonge; Anthony I. Wasserman",
    "corresponding_authors": "",
    "abstract": "This paper discusses some high-level language programming constructs that can be used to manipulate the relations of a relational database system efficiently. Three different constructs are described: (1) tuple identifiers that directly reference tuples of a relation; (2) cursors that may iterate over the tuples of a relation; and (3) markings, a form of temporary relation consisting of a set of tuple identifiers. In each case, attention is given to syntactic, semantic, and implementation considerations. The use of these features is first presented within the context of the programming language PLAIN, and it is then shown how these features could be used more generally to provide database manipulation capabilities in a high-level programming language. Consideration is also given to issues of programming methodology, with an important goal being the achievement of a balance between the enforcement of good programming practices and the ability to write efficient programs.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2035968222",
    "type": "article"
  },
  {
    "title": "Optimization of query streams using semantic prefetching",
    "doi": "https://doi.org/10.1145/1114244.1114250",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Ivan T. Bowman; Kenneth Salem",
    "corresponding_authors": "",
    "abstract": "Streams of relational queries submitted by client applications to database servers contain patterns that can be used to predict future requests. We present the Scalpel system, which detects these patterns and optimizes request streams using context-based predictions of future requests. Scalpel uses its predictions to provide a form of semantic prefetching, which involves combining a predicted series of requests into a single request that can be issued immediately. Scalpel's semantic prefetching reduces not only the latency experienced by the application but also the total cost of query evaluation. We describe how Scalpel learns to predict optimizable request patterns by observing the application's request stream during a training phase. We also describe the types of query pattern rewrites that Scalpels cost-based optimizer considers. Finally, we present empirical results that show the costs and benefits of Scalpel's optimizations.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4245823248",
    "type": "article"
  },
  {
    "title": "Algorithms for parsing search queries in systems with inverted file organization",
    "doi": "https://doi.org/10.1145/320493.320490",
    "publication_date": "1976-12-01",
    "publication_year": 1976,
    "authors": "Jane W. S. Liu",
    "corresponding_authors": "Jane W. S. Liu",
    "abstract": "In an inverted file system a query is in the form of a Boolean expression of index terms. In response to a query the system accesses the inverted lists corresponding to the index terms, merges them, and selects from the merged list those records that satisfy the search logic. Considered in this paper is the problem of determining a Boolean expression which leads to the minimum total merge time among all Boolean expressions that are equivalent to the expression given in the query. This problem is the same as finding an optimal merge tree among all trees that realize the truth function determined by the Boolean expression in the query. Several algorithms are described which generate optimal merge trees when the sizes of overlaps between different lists are small compared with the length of the lists.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2022927551",
    "type": "article"
  },
  {
    "title": "Interval hierarchies and their application to predicate files",
    "doi": "https://doi.org/10.1145/320557.320562",
    "publication_date": "1977-09-01",
    "publication_year": 1977,
    "authors": "K.C. Wong; Murray Edelberg",
    "corresponding_authors": "",
    "abstract": "Predicates are used extensively in modern database systems for purposes ranging from user specification of associative accesses to data, to user-invisible system control functions such as concurrency control and data distribution. Collections of predicates, or predicate files, must be maintained and accessed efficiently. This paper describes a dynamic index, called an interval hierarchy, which supports several important retrieval operations on files of simple conjunctive predicates. Search and maintenance algorithms for interval hierarchies are given. For a file of n predicates, typical of the kind expected in practice, these algorithms require time equal to Ο (log n ).",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2083318526",
    "type": "article"
  },
  {
    "title": "Performance evaluation of attribute-based tree organization",
    "doi": "https://doi.org/10.1145/320128.320135",
    "publication_date": "1980-03-01",
    "publication_year": 1980,
    "authors": "V. Gopalakrishna; C. E. Veni Madhavan",
    "corresponding_authors": "",
    "abstract": "A modified version of the multiple attribute tree (MAT) database organization, which uses a compact directory, is discussed. An efficient algorithm to process the directory for carrying out the node searches is presented. Statistical procedures are developed to estimate the number of nodes searched and the number of data blocks retrieved for most general and complex queries. The performance of inverted file and modified MAT organizations are compared using six real-life databases and four types of query complexities. Careful tradeoffs are established in terms of storage and access times for directory and data, query complexities, and database characteristics.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1972093965",
    "type": "article"
  },
  {
    "title": "TODS---the first three years (1976--1978)",
    "doi": "https://doi.org/10.1145/320610.320611",
    "publication_date": "1980-12-01",
    "publication_year": 1980,
    "authors": "David K. Hsiao",
    "corresponding_authors": "David K. Hsiao",
    "abstract": "article Free AccessTODS---the first three years (1976--1978) Share on Editor: David K. Hsiao TODS, The Ohio State University TODS, The Ohio State UniversityView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 5Issue 4Dec. 1980 pp 385–403https://doi.org/10.1145/320610.320611Online:01 December 1980Publication History 1citation385DownloadsMetricsTotal Citations1Total Downloads385Last 12 Months19Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2053618138",
    "type": "article"
  },
  {
    "title": "Specialized merge processor networks for combining sorted lists",
    "doi": "https://doi.org/10.1145/320263.320281",
    "publication_date": "1978-09-01",
    "publication_year": 1978,
    "authors": "Lee A. Hollaar",
    "corresponding_authors": "Lee A. Hollaar",
    "abstract": "In inverted file database systems, index lists consisting of pointers to items within the database are combined to form a list of items which potentially satisfy a user's query. This list merging is similar to the common data processing operation of combining two or more sorted input files to form a sorted output file, and generally represents a large percentage of the computer time used by the retrieval system. Unfortunately, a general purpose digital computer is better suited for complicated numeric processing rather than the simple combining of data. The overhead of adjusting and checking pointers, aligning data, and testing for completion of the operation overwhelm the processing of the data. A specialized processor can perform most of these overhead operations in parallel with the processing of the data, thereby offering speed increases by a factor from 10 to 100 over conventional computers, depending on whether a higher speed memory is used for storing the lists. These processors can also be combined into networks capable of directly forming the result of a complex expression, with another order of magnitude speed increase possible. The programming and operation of these processors and networks is discussed, and comparisons are made with the speed and efficiency of conventional general purpose computers.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2061281586",
    "type": "article"
  },
  {
    "title": "Differentiating search results on structured data",
    "doi": "https://doi.org/10.1145/2109196.2109200",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Ziyang Liu; Yi Chen",
    "corresponding_authors": "",
    "abstract": "Studies show that about 50% of Web search is for information exploration purposes, where a user would like to investigate, compare, evaluate, and synthesize multiple relevant results. Due to the absence of general tools that can effectively analyze and differentiate multiple results, a user has to manually read and comprehend potential large results in an exploratory search. Such a process is time consuming, labor intensive and error prone. Interestingly, we find that the metadata information embedded in structured data provides a potential for automating or semi-automating the comparison of multiple results. In this article we present an approach for structured data search result differentiation. We define the differentiability of query results and quantify the degree of difference. Then we define the problem of identifying a limited number of valid features in a result that can maximally differentiate this result from the others, which is proved NP-hard. We propose two local optimality conditions, namely single-swap and multi-swap, and design efficient algorithms to achieve local optimality. We then present a feature type-based approach, which further improves the quality of the features identified for result differentiation. To show the usefulness of our approach, we implemented a system CompareIt, which can be used to compare structured search results as well as any objects. Our empirical evaluation verifies the effectiveness and efficiency of the proposed approach.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2058988540",
    "type": "article"
  },
  {
    "title": "Collaborative personalized top-k processing",
    "doi": "https://doi.org/10.1145/2043652.2043659",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Xiao Bai; Rachid Guerraoui; Anne-Marie Kermarrec; Vincent Leroy",
    "corresponding_authors": "",
    "abstract": "This article presents P4Q, a fully decentralized gossip-based protocol to personalize query processing in social tagging systems. P4Q dynamically associates each user with social acquaintances sharing similar tagging behaviors. Queries are gossiped among such acquaintances, computed on-the-fly in a collaborative, yet partitioned manner, and results are iteratively refined and returned to the querier. Analytical and experimental evaluations convey the scalability of P4Q for top- k query processing, as well its inherent ability to cope with users updating profiles and departing.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2081711937",
    "type": "article"
  },
  {
    "title": "Processing Top-k Dominating Queries in Metric Spaces",
    "doi": "https://doi.org/10.1145/2847524",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Eleftherios Tiakas; George Valkanas; Apostolos N. Papadopoulos; Yannis Manolopoulos; Dimitrios Gunopulos",
    "corresponding_authors": "",
    "abstract": "Top - k dominating queries combine the natural idea of selecting the k best items with a comprehensive “goodness” criterion based on dominance. A point p 1 dominates p 2 if p 1 is as good as p 2 in all attributes and is strictly better in at least one. Existing works address the problem in settings where data objects are multidimensional points. However, there are domains where we only have access to the distance between two objects. In cases like these, attributes reflect distances from a set of input objects and are dynamically generated as the input objects change. Consequently, prior works from the literature cannot be applied, despite the fact that the dominance relation is still meaningful and valid. For this reason, in this work, we present the first study for processing top- k dominating queries over distance-based dynamic attribute vectors, defined over a metric space . We propose four progressive algorithms that utilize the properties of the underlying metric space to efficiently solve the problem and present an extensive, comparative evaluation on both synthetic and real-world datasets.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2317824144",
    "type": "article"
  },
  {
    "title": "Capturing Missing Tuples and Missing Values",
    "doi": "https://doi.org/10.1145/2901737",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Ting Deng; Wenfei Fan; Floris Geerts",
    "corresponding_authors": "",
    "abstract": "Databases in real life are often neither entirely closed-world nor entirely open-world. Databases in an enterprise are typically partially closed , in which a part of the data is constrained by master data that contains complete information about the enterprise in certain aspects. It has been shown that, despite missing tuples, such a database may turn out to have complete information for answering a query. This article studies partially closed databases from which both tuples and attribute values may be missing. We specify such a database in terms of conditional tables constrained by master data, referred to as c -instances. We first propose three models to characterize whether a c -instance T is complete for a query Q relative to master data. That is, depending on how missing values in T are instantiated, the answer to Q in T remains unchanged when new tuples are added. We then investigate three problems, to determine (a) whether a given c -instance is complete for a query Q , (b) whether there exists a c -instance that is complete for Q relative to master data available, and (c) whether a c -instance is a minimal-size database that is complete for Q . We establish matching lower and upper bounds on these problems for queries expressed in a variety of languages in each of the three models for specifying relative completeness.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4245179829",
    "type": "article"
  },
  {
    "title": "Probabilistic Truss Decomposition on Uncertain Graphs: Indexing and Dynamic Maintenance",
    "doi": "https://doi.org/10.1145/3721428",
    "publication_date": "2025-03-03",
    "publication_year": 2025,
    "authors": "Zitan Sun; Xin Huang; Jianliang Xu; Francesco Bonchi; Lijun Chang",
    "corresponding_authors": "",
    "abstract": "Networks in many real-world applications come with an inherent uncertainty in their structure, due to e.g., noisy measurements, inference and prediction models, or for privacy purposes. Modeling and analyzing uncertain graphs have attracted a great deal of attention. Among the various graph analytic tasks studied, the extraction of dense substructures, such as cores or trusses, has a central role. In this paper, we study the problem of ( k , γ )-truss indexing and querying over an uncertain graph \\(\\mathcal {G} \\) . A ( k , γ )-truss is the largest subgraph of \\(\\mathcal {G} \\) , such that the probability of each edge being contained in at least k − 2 triangles is no less than γ . Our first proposal, \\(\\mathsf {CPT}-\\mathsf {index} \\) , keeps all the ( k , γ )-trusses: retrieval for any given k and γ can be executed in an optimal linear time w.r.t. the graph size of the queried ( k , γ )-truss. We develop a bottom-up \\(\\mathsf {CPT}-\\mathsf {index} \\) construction scheme and an improved algorithm for fast \\(\\mathsf {CPT}-\\mathsf {index} \\) construction using top-down graph partitions. For trading off between ( k , γ )-truss offline indexing and online querying, we further develop an approximate indexing approach (ϵ, Δ r )- APX equipped with two parameters, ϵ and Δ r , that govern tolerated errors. In addition, we further investigate the problem of maintaining ( k , γ )-truss indexes over dynamic uncertain graphs, where the update of vertex/edge insertions/deletions and also edge probability increments/decrements may frequently occur. We propose a comprehensive solution for \\(\\mathsf {CPT}-\\mathsf {index} \\) and (ϵ, Δ r )- APX maintenance by addressing one fundamental task of one edge’s probability increment/decrement. To reduce the scope of affected edges that have trussness changed, we categorize three types of candidate edges and propose tight lower/upper bounds for trussness refinement, which can efficiently accomplish \\(\\mathsf {CPT}-\\mathsf {index} \\) maintenance in a local update scheme. Our proposed techniques for one single edge change can also be extended to handle a batch update of multiple edges. Extensive experiments using large-scale uncertain graphs with 261 million edges validate the efficiency of our proposed indexing and querying algorithms, as well as our ( k , γ )-truss index maintenance algorithms, against state-of-the-art methods. Case studies on real-world graphs demonstrate the significant efficiency improvement by our proposed solutions as well as interesting discoveries.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408116855",
    "type": "article"
  },
  {
    "title": "Expressiveness within Sequence Datalog",
    "doi": "https://doi.org/10.1145/3732283",
    "publication_date": "2025-05-01",
    "publication_year": 2025,
    "authors": "Heba Aamer; Jan Hidders; Jan Paredaens; Jan Van den Bussche",
    "corresponding_authors": "",
    "abstract": "Motivated by old and new applications, we investigate Datalog as a language for sequence databases. We reconsider classical features of Datalog programs, such as negation, recursion, intermediate predicates, and relations of higher arities. We also consider new features that are useful for sequences, notably, equations between path expressions, and “packing”. Our goal is to clarify the relative expressiveness of all these different features, in the context of sequences. Towards our goal, we establish a number of redundancy and primitivity results, showing that certain features can, or cannot, be expressed in terms of other features. These results paint a complete picture of the expressiveness relationships among all possible Sequence Datalog fragments that can be formed using the six features that we consider.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410004955",
    "type": "article"
  },
  {
    "title": "High-dimensional Data Cubes",
    "doi": "https://doi.org/10.1145/3716373",
    "publication_date": "2025-05-02",
    "publication_year": 2025,
    "authors": "Sachin Basil John; Christoph Koch; Peter Lindner",
    "corresponding_authors": "",
    "abstract": "We introduce an approach to supporting high-dimensional data cubes at interactive query speeds and moderate storage cost. Our approach is based on binary(-domain) data cubes that are judiciously partially materialized; the missing information can be quickly approximated using statistical or linear programming techniques. This enables new applications such as exploratory data analysis for feature engineering and other fields of data science. Moreover, it removes the need to compromise when building a data cube – all columns we might ever wish to use can be included as dimensions. Our approach also speeds up certain dice, roll-up, and drill-down operations on data cubes with hierarchical dimensions compared to traditional data cubes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410039305",
    "type": "article"
  },
  {
    "title": "Space-Time Tradeoffs for Conjunctive Queries with Access Patterns",
    "doi": "https://doi.org/10.1145/3743130",
    "publication_date": "2025-06-08",
    "publication_year": 2025,
    "authors": "Hangdong Zhao; Shaleen Deep; Paraschos Koutris",
    "corresponding_authors": "",
    "abstract": "In this paper, we investigate space-time tradeoffs for answering conjunctive queries with access patterns (CQAPs). The goal is to create a space-efficient data structure in an initial preprocessing phase and use it for answering (multiple) queries in an online phase. Previous work has developed data structures that trades off space usage for answering time for queries of practical interest, such as the path and triangle query. However, these approaches lack a comprehensive framework and are not generalizable. Our main contribution is a general algorithmic framework for obtaining space-time tradeoffs for any CQAP. Our framework builds upon the PANDA algorithm and tree decomposition techniques. We demonstrate that our framework captures all state-of-the-art tradeoffs that were independently produced for various queries. Further, we show surprising improvements over the state-of-the-art tradeoffs known in the existing literature for reachability queries.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411123944",
    "type": "article"
  },
  {
    "title": "Unveiling Logic Bugs in SPJG Query Optimizations within DBMS",
    "doi": "https://doi.org/10.1145/3764583",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Xiu Tang; Shi-Jie Yang; Sai Wu; Dongxiang Zhang; Wenchao Zhou; Feifei Li; Gang Chen",
    "corresponding_authors": "",
    "abstract": "Generation-based testing techniques have proven effective in detecting logic bugs in DBMS, often stemming from the improper implementation of query optimizers. However, existing generation-based debugging tools predominantly rely on random testing, which tends to overlook critical error-prone areas such as multi-table joining and grouped aggregation. In this paper, we propose TQS, a novel testing framework targeted at detecting logic bugs arising from SPJG (Select-Project-Join-Group By) query optimizations. Given a target DBMS, TQS achieves the goal with two key components: Data-guided Schema and Query Generation (DSG) and Knowledge-guided Query Space Exploration (KQE). DSG addresses the key challenge of multi-table query debugging: how to generate ground-truth (query, result) pairs for verification. DSG utilizes data derived from dimensionally aggregated data cubes, which store data of grouped metric columns. It maps data from data cubes to a wide table, applies database normalization techniques to the wide table to generate a testing schema and maintains a bitmap index for result tracking. To improve debug efficiency, DSG also artificially inserts some noises into the generated data. To avoid repetitive query space search, KQE guides the generation of error-prone cubes, and forms the problem as isomorphic graph set discovery and combines the graph embedding and weighted random walk for query generation. We evaluated TQS on four popular DBMSs: MySQL, MariaDB, TiDB and PolarDB. Experimental results show that TQS is effective in finding logic bugs of SPJG query optimization in database management systems. It successfully detected 226 bugs within 24 hours, including 63 bugs in MySQL, 52 in MariaDB, 68 in TiDB, and 43 in PolarDB respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413792304",
    "type": "article"
  },
  {
    "title": "From Global to Query-Dependent: Summarization of Large Hierarchical DAGs",
    "doi": "https://doi.org/10.1145/3769079",
    "publication_date": "2025-09-20",
    "publication_year": 2025,
    "authors": "Xuliang Zhu; Xin Huang; Kai Wang; Jianliang Xu; Xuemin Lin",
    "corresponding_authors": "",
    "abstract": "Hierarchical directed acyclic graph (DAG) is an essential model for representing terminologies and their hierarchical relationships, such as Disease Ontology and ImageNet categories. Due to the vast number of terminologies and complex structures in a large DAG, it becomes challenging for humans to effectively analyze and explore the hierarchical relationships they encode. Therefore, summarizing hierarchical DAGs is essential for enhancing the interpretability and visualization of the underlying hierarchy. Beyond visual data exploration, hierarchical DAG summarization also supports a range of applications, such as biomedical ontology analytics, snippet generation for information search, and summarized recommendation. In this paper, we address a new problem of finding k representative vertices to summarize a hierarchical DAG. To capture diverse summarization and identify important vertices, we design a summary score function that reflects vertices diversity coverage and structure correlation. The studied problem is theoretically proven to be NP-hard. To tackle it efficiently, we propose a greedy algorithm with an approximation guarantee that iteratively adds vertices with significant summary contributions to the answers. To further enhance the answer quality, we introduce a subtree extraction-based method that is proven to achieve higher-quality answers. Additionally, we develop a scalable algorithm, k-PCGS, which employs candidate pruning and DAG compression for large-scale hierarchical DAGs. For the query-dependent problem, we propose an index-based method and several optimization techniques to improve efficiency. Extensive experiments on large real-world datasets demonstrate the effectiveness and efficiency of our proposed algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414374716",
    "type": "article"
  },
  {
    "title": "Implementing deductive databases by mixed integer programming",
    "doi": "https://doi.org/10.1145/232616.232691",
    "publication_date": "1996-06-01",
    "publication_year": 1996,
    "authors": "Colin E. Bell; Anil Nerode; Raymond T. Ng; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "Existing and past generations of Prolog compilers have left deduction to run-time and this may account for the poor run-time performance of existing Prolog systems. Our work tries to minimize run-time deduction by shifting the deductive process to compile-time. In addition, we offer an alternative inferencing procedure based on translating logic to mixed integer programming. This makes available for research and implementation in deductive databases, all the theorems, algorithms, and software packages developed by the operations research community over the past 50 years. The method keeps the same query language as for disjunctive deductive databases, only the inferencing procedure changes. The language is purely declarative, independent of the order of rules in the program, and independent of the order in which literals occur in clause bodies. The technique avoids Prolog's problem of infinite looping. It saves run-time by doing primary inferencing at compile-time. Furthermore, it is incremental in nature. The first half of this article translates disjunctive clauses, integrity constraints, and database facts into Boolean equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute equations, and develops procedures to use mixed integer programming methods to compute —least models of definite deductive databases, and —minimal models and the Generalized Closed World Assumption of disjunctive databases.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1994801292",
    "type": "article"
  },
  {
    "title": "A software tool for modular database design",
    "doi": "https://doi.org/10.1145/114325.103711",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Marco A. Casanova; Antônio L. Furtado; Luiz Tucherman",
    "corresponding_authors": "",
    "abstract": "A modularization discipline for database schemas is first described. The dicipline incorporates both a strategy for enforcing integrity constraints and a tactic for organizing large sets of database structures, integrity constraints, and operations. A software tool that helps the development and maintenance of database schemas modularized according to the discipline is then presented. It offers a user-friendly interface that guides the designer through the various stages of the creation of a new module or through the process of changing objects of existing modules. The tool incorporates, in a declarative style, a description of the design and redesign rules behind the modularization discipline, hence facilitating the incremental addition of new expertise about database design.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1983117661",
    "type": "article"
  },
  {
    "title": "Explaining ambiguity in a formal query language",
    "doi": "https://doi.org/10.1145/78922.78923",
    "publication_date": "1990-06-01",
    "publication_year": 1990,
    "authors": "Joseph A. Wald; Paul Sorenson",
    "corresponding_authors": "",
    "abstract": "The problem of generating reasonable natural language-like responses to queries formulated in nonnavigational query languages with logical data independence is addressed. An extended ER model, the Entity-Relationship-Involvement model, is defined which assists in providing a greater degree of logical data independence and the generation of natural language explanations of a query processor's interpretation of a query. These are accomplished with the addition of the concept of an involvement to the model. Based on involvement definitions in a formally defined data definition language, DDL, an innovative strategy for generating explanations is outlined and exemplified. In the conclusion, possible extensions to the approach are given.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2003507569",
    "type": "article"
  },
  {
    "title": "Translation with optimization from relational calculus to relational algebra having aggregate functions",
    "doi": "https://doi.org/10.1145/99935.99943",
    "publication_date": "1990-12-01",
    "publication_year": 1990,
    "authors": "Ryohei Nakano",
    "corresponding_authors": "Ryohei Nakano",
    "abstract": "Most of the previous translations of relational calculus to relational algebra aimed at proving that the two languages have the equivalent expressive power, thereby generating very complicated relational algebra expressions, especially when aggregate functions are introduced. This paper presents a rule-based translation method from relational calculus expressions having both aggregate functions and null values to optimized relational algebra expressions. Thus, logical optimization is carried out through translation. The translation method comprises two parts: the translational of the relational calculus kernel and the translation of aggregate functions. The former uses the familiar step-wise rewriting strategy, while the latter adopts a two-phase rewriting strategy via standard aggregate expressions. Each translation proceeds by applying a heuristic rewriting rule in preference to a basic rewriting rule. After introducing SQL-type null values, their impact on the translation is thoroughly investigated, resulting in several extensions of the translation. A translation experiment with many queries shows that the proposed translation method generates optimized relational algebra expressions. It is shown that heuristic rewriting rules play an essential role in the optimization. The correctness of the present translation is also shown. Each translation proceeds by applying a heuristic rewriting rule in preference to a basic rewriting rule. After introducing SQL-type null values, their impact on the translation is thoroughly investigated, resulting in several extensions of the translation. A translation experiment with many queries shows that the proposed translation method generates optimized relational",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2003815743",
    "type": "article"
  },
  {
    "title": "EAS-E",
    "doi": "https://doi.org/10.1145/319996.320003",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "A. Malhotra; Harry M. Markowitz; Donald P. Pazel",
    "corresponding_authors": "",
    "abstract": "EAS-E (pronounced EASY) is an experimental programming language integrated with a database management system now running on VM/370 at the IBM Thomas J. Watson Research Center. The EAS-E programming language is built around the entity, attribute, and set ( EAS ) view of application development. It provides a means for translating operations on EAS structures directly into executable code. EAS-E commands have an English-like syntax, and thus EAS-E programs are easy to read and understand. EAS-E programs are also more compact than equivalent programs in other database languages. The EAS-E database management system allows many users simultaneous access to the database. It supports locking and deadlock detection and is capable of efficiently supporting network databases of various sizes including very large databases, consisting of several millions of entities stored on multiple DASD extends. Also available is a nonprocedural facility that allows a user to browse and update the database without writing programs.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2010870488",
    "type": "article"
  },
  {
    "title": "On the selection of efficient record segmentations and backup strategies for large shared databases",
    "doi": "https://doi.org/10.1145/1270.1481",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Salvatore T. March; Gary D. Scudder",
    "corresponding_authors": "",
    "abstract": "In recent years the information processing requirements of business organizations have expanded tremendously. With this expansion, the design of databases to efficiently manage and protect business information has become critical. We analyze the impacts of record segmentation (the assignment of data items to segments defining subfiles), an efficiency-oriented design technique, and of backup and recovery strategies , a data protection technique, on the overall process of database design. A combined record segmentation/backup and recovery procedure is presented and an application of the procedure is discussed. Results in which problem characteristics are varied along three dimensions: update frequencies, available types of access paths, and the predominant type of data retrieval that must be supported by the database, are presented.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2020966700",
    "type": "article"
  },
  {
    "title": "Response Time Analysis of Multiprocessor Computers for Database Support",
    "doi": "https://doi.org/10.1145/348.318589",
    "publication_date": "1984-03-23",
    "publication_year": 1984,
    "authors": "Roger K. Shultz; Roy J. Zingg",
    "corresponding_authors": "",
    "abstract": "Comparison of three multiprocessor computer architectures for database support is made possible through evaluation of response time expressions. These expressions are derived by parameterizing algorithms performed by each machine to execute a relational algebra query. Parameters represent properties of the database and components of the machines. Studies of particular parameter values exhibit response times for conventional machine technology, for low selectivity, high duplicate occurrence, and parallel disk access, increasing the number of processors, and improving communication and processing technology.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2071096274",
    "type": "article"
  },
  {
    "title": "On Kent's “Consequences of assuming a universal relation” (Technical correspondance)",
    "doi": "https://doi.org/10.1145/319996.320017",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "Jeffrey D. Ullman",
    "corresponding_authors": "Jeffrey D. Ullman",
    "abstract": "article Free AccessOn Kent's “Consequences of assuming a universal relation” (Technical correspondance) Author: Jeffrey D. Ullman Dept. of Computer Science, Stanford University, Stanford, CA Dept. of Computer Science, Stanford University, Stanford, CAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 8Issue 4Dec. 1983 pp 637–643https://doi.org/10.1145/319996.320017Published:01 December 1983Publication History 12citation441DownloadsMetricsTotal Citations12Total Downloads441Last 12 Months16Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1984089690",
    "type": "article"
  },
  {
    "title": "Frame memory",
    "doi": "https://doi.org/10.1145/319587.319598",
    "publication_date": "1981-09-01",
    "publication_year": 1981,
    "authors": "Salvatore T. March; Dennis G. Severance; Michael Wilens",
    "corresponding_authors": "",
    "abstract": "Frame memory is a virtual view of secondary storage that can be implemented with reasonable overhead to support database record storage and accessing requirements. Frame memory is designed so that its operating characteristics can be easily manipulated by either designers or design algorithms, while performance effects of such changes can be accurately predicted. Automated design procedures exist to generate and evaluate alternative database designs built upon frame memory, and the existence of these procedures establishes frames as an attractive memory management architecture for future database management systems.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2051837050",
    "type": "article"
  },
  {
    "title": "An extension of conflict-free multivalued dependency sets",
    "doi": "https://doi.org/10.1145/329.354",
    "publication_date": "1984-06-03",
    "publication_year": 1984,
    "authors": "Hirofumi Katsuno",
    "corresponding_authors": "Hirofumi Katsuno",
    "abstract": "Several researchers (Beeri, Bernstein, Chiu, Fagin, Goodman, Maier, Mendelzon, Ullman, and Yannakakis) have introduced a special class of database schemes, called acyclic or tree schemes. Beeri et al. have shown that an acyclic join dependency, naturally defined by an acyclic database scheme, has several desirable properties, and that an acyclic join dependency is equivalent to a conflict-free set of multivalued dependencies. However, since their results are confined to multivalued and join dependencies, it is not clear whether we can handle functional dependencies independently of other dependencies. In the present paper we define an extension of a conflict-free set, called an extended conflict-free set , including multivalued dependencies and functional dependencies, and show the following two properties of an extended conflict-free set: There are three equivalent definitions of an extended conflict-free set. One of them is defined as a set including an acyclic joint dependency and a set of functional dependencies such that the left and right sides of each functional dependency are included in one of the attribute sets that construct the acyclic join dependency. For a relation scheme with an extended conflict-free set, there is a decomposition into third normal form with a lossless join and preservation of dependencies.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2132873892",
    "type": "article"
  },
  {
    "title": "Partial-match hash coding",
    "doi": "https://doi.org/10.1145/320071.320079",
    "publication_date": "1979-06-01",
    "publication_year": 1979,
    "authors": "Walter A. Burkhard",
    "corresponding_authors": "Walter A. Burkhard",
    "abstract": "File designs suitable for retrieval from a file of k -field records when queries may be partially specified are examined. Storage redundancy is introduced to obtain improved worst-case and average-case performances. The resulting storage schemes are appropriate for replicated distributed database environments; it is possible to improve the overall average and worst-case behavior for query response as well as provide an environment with very high reliability. Within practical systems it will be possible to improve the query response time performance as well as reliability over comparable systems without replication.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1972988885",
    "type": "article"
  },
  {
    "title": "Generating sound workflow views for correct provenance analysis",
    "doi": "https://doi.org/10.1145/1929934.1929940",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "Ziyang Liu; Susan B. Davidson; Yi Chen",
    "corresponding_authors": "",
    "abstract": "Workflow views abstract groups of tasks in a workflow into high level composite tasks, in order to reuse subworkflows and facilitate provenance analysis. However, unless a view is carefully designed, it may not preserve the dataflow between tasks in the workflow, that is, it may not be sound . Unsound views can be misleading and cause incorrect provenance analysis. This article studies the problem of efficiently identifying and correcting unsound workflow views with minimal changes, and constructing minimal sound and elucidative workflow views with a set of user-specified relevant tasks. In particular, two related problems are investigated. First, given a workflow view, we wish to split each unsound composite task into the minimal number of tasks, such that the resulting view is sound. Second, given a workflow and a set of user specified relevant tasks, we generate a sound view, such that each composite task contains at most one relevant task, and the total number of tasks is minimized. We prove that both problems are NP-hard by reduction from independent set. We then propose two local optimality conditions (weak and strong) for each problem, and design polynomial time algorithms for both problems to meet these conditions. Experiments show that our proposed algorithms are reasonably effective and efficient. The proposed techniques are useful for view analysis/construction for not only workflows, but general networks as well.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2006638312",
    "type": "article"
  },
  {
    "title": "Improving XML search by generating and utilizing informative result snippets",
    "doi": "https://doi.org/10.1145/1806907.1806911",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Ziyang Liu; Yu Huang; Yi Chen",
    "corresponding_authors": "",
    "abstract": "Snippets are used by almost every text search engine to complement the ranking scheme in order to effectively handle user searches, which are inherently ambiguous and whose relevance semantics are difficult to assess. Despite the fact that XML is a standard representation format of Web data, research on generating result snippets for XML search remains limited. To tackle this important yet open problem, in this article, we present a system eXtract which generates snippets for XML search results. We identify that a good XML result snippet should be a meaningful information unit of a small size that effectively summarizes this query result and differentiates it from others, according to which users can quickly assess the relevance of the query result. We have designed and implemented a novel algorithm to satisfy these requirements. Furthermore, we propose to cluster the query results based on their snippets. Since XML result clustering can only be done at query time, snippet-based clustering significantly improves the efficiency while compromising little clustering accuracy. We verified the efficiency and effectiveness of our approach through experiments.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2056464377",
    "type": "article"
  },
  {
    "title": "Scalable Analytics on Fast Data",
    "doi": "https://doi.org/10.1145/3283811",
    "publication_date": "2019-01-22",
    "publication_year": 2019,
    "authors": "Andreas Kipf; Varun Pandey; Jan P. Böttcher; Lucas Braun; Thomas Neumann; Alfons Kemper",
    "corresponding_authors": "",
    "abstract": "Today’s streaming applications demand increasingly high event throughput rates and are often subject to strict latency constraints. To allow for more complex workloads, such as window-based aggregations, streaming systems need to support stateful event processing. This introduces new challenges for streaming engines as the state needs to be maintained in a consistent and durable manner and simultaneously accessed by complex queries for real-time analytics. Modern streaming systems, such as Apache Flink, do not allow for efficiently exposing the state to analytical queries. Thus, data engineers are forced to keep the state in external data stores, which significantly increases the latencies until events become visible to analytical queries. Proprietary solutions have been created to meet data freshness constraints. These solutions are expensive, error-prone, and difficult to maintain. Main-memory database systems, such as HyPer, achieve extremely low query response times while maintaining high update rates, which makes them well-suited for analytical streaming workloads. In this article, we explore extensions to database systems to match the performance and usability of streaming systems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2912423768",
    "type": "article"
  },
  {
    "title": "ChronicleDB",
    "doi": "https://doi.org/10.1145/3342357",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Marc Seidemann; Nikolaus Glombiewski; Michael Körber; Bernhard Seeger",
    "corresponding_authors": "",
    "abstract": "Reactive security monitoring, self-driving cars, the Internet of Things (IoT), and many other novel applications require systems for both writing events arriving at very high and fluctuating rates to persistent storage as well as supporting analytical ad hoc queries. As standard database systems are not capable of delivering the required write performance, log-based systems, key-value stores, and other write-optimized data stores have emerged recently. However, the drawbacks of these systems are a fair query performance and the lack of suitable instant recovery mechanisms in case of system failures. In this article, we present ChronicleDB, a novel database system with a storage layout tailored for high write performance under fluctuating data rates and powerful indexing capabilities to support a variety of queries. In addition, ChronicleDB offers low-cost fault tolerance and instant recovery within milliseconds. Unlike previous work, ChronicleDB is designed either as a serverless library to be tightly integrated in an application or as a standalone database server. Our results of an experimental evaluation with real and synthetic data reveal that ChronicleDB clearly outperforms competing systems with respect to both write and query performance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2980657213",
    "type": "article"
  },
  {
    "title": "K-Regret Queries Using Multiplicative Utility Functions",
    "doi": "https://doi.org/10.1145/3230634",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Jianzhong Qi; Fei Zuo; Hanan Samet; Jia Yao",
    "corresponding_authors": "",
    "abstract": "The k -regret query aims to return a size- k subset S of a database D such that, for any query user that selects a data object from this size- k subset S rather than from database D , her regret ratio is minimized. The regret ratio here is modeled by the relative difference in the optimality between the locally optimal object in S and the globally optimal object in D . The optimality of a data object in turn is modeled by a utility function of the query user. Unlike traditional top- k queries, the k -regret query does not minimize the regret ratio for a specific utility function. Instead, it considers a family of infinite utility functions F , and aims to find a size- k subset that minimizes the maximum regret ratio of any utility function in F . Studies on k -regret queries have focused on the family of additive utility functions, which have limitations in modeling individuals’ preferences and decision-making processes, especially for a common observation called the diminishing marginal rate of substitution (DMRS). We introduce k -regret queries with multiplicative utility functions, which are more expressive in modeling the DMRS, to overcome those limitations. We propose a query algorithm with bounded regret ratios. To showcase the applicability of the algorithm, we apply it to a special family of multiplicative utility functions, the Cobb-Douglas family of utility functions, and a closely related family of utility functions, the Constant Elasticity of Substitution family of utility functions, both of which are frequently used utility functions in microeconomics. After a further study of the query properties, we propose a heuristic algorithm that produces even smaller regret ratios in practice. Extensive experiments on the proposed algorithms confirm that they consistently achieve small maximum regret ratios.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3125176478",
    "type": "article"
  },
  {
    "title": "Shared execution strategy for neighbor-based pattern mining requests over streaming windows",
    "doi": "https://doi.org/10.1145/2109196.2109201",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Di Yang; Elke A. Rundensteiner; Matthew O. Ward",
    "corresponding_authors": "",
    "abstract": "In diverse applications ranging from stock trading to traffic monitoring, data streams are continuously monitored by multiple analysts for extracting patterns of interest in real time. These analysts often submit similar pattern mining requests yet customized with different parameter settings. In this work, we present shared execution strategies for processing a large number of neighbor-based pattern mining requests of the same type yet with arbitrary parameter settings. Such neighbor-based pattern mining requests cover a broad range of popular mining query types, including detection of clusters, outliers, and nearest neighbors. Given the high algorithmic complexity of the mining process, serving multiple such queries in a single system is extremely resource intensive. The naive method of detecting and maintaining patterns for different queries independently is often infeasible in practice, as its demands on system resources increase dramatically with the cardinality of the query workload. In order to maximize the efficiency of the system resource utilization for executing multiple queries simultaneously, we analyze the commonalities of the neighbor-based pattern mining queries, and identify several general optimization principles which lead to significant system resource sharing among multiple queries. In particular, as a preliminary sharing effort, we observe that the computation needed for the range query searches (the process of searching the neighbors for each object) can be shared among multiple queries and thus saves the CPU consumption. Then we analyze the interrelations between the patterns identified by queries with different parameters settings, including both pattern-specific and window-specific parameters. For that, we first introduce an incremental pattern representation, which represents the patterns identified by queries with different pattern-specific parameters within a single compact structure. This enables integrated pattern maintenance for multiple queries. Second, by leveraging the potential overlaps among sliding windows, we propose a metaquery strategy which utilizes a single query to answer multiple queries with different window-specific parameters. By combining these three techniques, namely the range query search sharing, integrated pattern maintenance, and metaquery strategy, our framework realizes fully shared execution of multiple queries with arbitrary parameter settings. It achieves significant savings of computational and memory resources due to shared execution. Our comprehensive experimental study, using real data streams from domains of stock trades and moving object monitoring, demonstrates that our solution is significantly faster than the independent execution strategy, while using only a small portion of memory space compared to the independent execution. We also show that our solution scales in handling large numbers of queries in the order of hundreds or even thousands under high input data rates.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2024319164",
    "type": "article"
  },
  {
    "title": "Boosting the Quality of Approximate String Matching by Synonyms",
    "doi": "https://doi.org/10.1145/2818177",
    "publication_date": "2015-10-23",
    "publication_year": 2015,
    "authors": "Jiaheng Lu; Chunbin Lin; Wei Wang; Chen Li; Xiaokui Xiao",
    "corresponding_authors": "",
    "abstract": "A string-similarity measure quantifies the similarity between two text strings for approximate string matching or comparison. For example, the strings “Sam” and “Samuel” can be considered to be similar. Most existing work that computes the similarity of two strings only considers syntactic similarities, for example, number of common words or q -grams. While this is indeed an indicator of similarity, there are many important cases where syntactically-different strings can represent the same real-world object. For example, “Bill” is a short form of “William,” and “Database Management Systems” can be abbreviated as “DBMS.” Given a collection of predefined synonyms, the purpose of this article is to explore such existing knowledge to effectively evaluate the similarity between two strings and efficiently perform similarity searches and joins, thereby boosting the quality of approximate string matching. In particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms. We then study efficient algorithms for similarity searches and joins by proposing two novel indexes, called SI-trees and QP-trees, which combine signature-filtering and length-filtering strategies. In order to improve the efficiency of our algorithms, we develop an estimator to estimate the size of candidates to enable an online selection of signature filters. This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice. Finally, the experimental results from a comprehensive study of the algorithms with three real datasets verify the effectiveness and efficiency of our approaches.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2024651132",
    "type": "article"
  },
  {
    "title": "Almost-linear inclusion for XML regular expression types",
    "doi": "https://doi.org/10.1145/2508020.2508022",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Dario Colazzo; Giorgio Ghelli; L Pardini; Carlo Sartiani",
    "corresponding_authors": "",
    "abstract": "Type inclusion is a fundamental operation in every type-checking compiler, but it is quite expensive for XML manipulation languages. A polynomial inclusion checking algorithm for an expressive family of XML type languages is known, but it runs in quadratic time both in the best and in the worst cases. We present here an algorithm that has a linear-time backbone, and resorts to the quadratic approach for some specific parts of the compared types. Our experiments show that the new algorithm is much faster than the quadratic one, and that it typically runs in linear time, hence it can be used as a building block for a practical type-checking compiler.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2078935356",
    "type": "article"
  },
  {
    "title": "The data cyclotron query processing scheme",
    "doi": "https://doi.org/10.1145/2043652.2043660",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Romulo Gonçalves; Martin Kersten",
    "corresponding_authors": "",
    "abstract": "A grand challenge of distributed query processing is to devise a self-organizing architecture which exploits all hardware resources optimally to manage the database hot set, minimize query response time, and maximize throughput without single point global coordination. The Data Cyclotron architecture [Goncalves and Kersten 2010] addresses this challenge using turbulent data movement through a storage ring built from distributed main memory and capitalizing on the functionality offered by modern remote-DMA network facilities. Queries assigned to individual nodes interact with the storage ring by picking up data fragments, which are continuously flowing around, that is, the hot set. The storage ring is steered by the Level Of Interest ( LOI ) attached to each data fragment, which represents the cumulative query interest as it passes around the ring multiple times. A fragment with LOI below a given threshold, inversely proportional to the ring load, is pulled out to free up resources. This threshold is dynamically adjusted in a fully distributed manner based on ring characteristics and locally observed query behavior. It optimizes resource utilization by keeping the average data access latency low. The approach is illustrated using an extensive and validated simulation study. The results underpin the fragment hot set management robustness in turbulent workload scenarios. A fully functional prototype of the proposed architecture has been implemented using modest extensions to MonetDB and runs within a multirack cluster equipped with Infiniband. Extensive experimentation using both microbenchmarks and high-volume workloads based on TPC-H demonstrates its feasibility. The Data Cyclotron architecture and experiments open a new vista for modern distributed database architectures with a plethora of new research challenges.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2157666539",
    "type": "article"
  },
  {
    "title": "Flexible Skylines",
    "doi": "https://doi.org/10.1145/3406113",
    "publication_date": "2020-12-10",
    "publication_year": 2020,
    "authors": "Paolo Ciaccia; Davide Martinenghi",
    "corresponding_authors": "",
    "abstract": "Skyline and ranking queries are two popular, alternative ways of discovering interesting data in large datasets. Skyline queries are simple to specify, as they just return the set of all non-dominated tuples, thereby providing an overall view of potentially interesting results. However, they are not equipped with any means to accommodate user preferences or to control the cardinality of the result set. Ranking queries adopt, instead, a specific scoring function to rank tuples, and can easily control the output size. While specifying a scoring function allows one to give different importance to different attributes by means of, e.g., weight parameters, choosing the “right” weights to use is known to be a hard problem. In this article, we embrace the skyline approach by introducing an original framework able to capture user preferences by means of constraints on the weights used in a scoring function, which is typically much easier than specifying precise weight values. To this end, we introduce the novel concept of F-dominance , i.e., dominance with respect to a family of scoring functions F : a tuple t is said to F -dominate tuple s when t is always better than or equal to s according to all the functions in F . Based on F -dominance, we present two flexible skyline (F-skyline) operators, both returning a subset of the skyline: nd , characterizing the set of non- F -dominated tuples; po , referring to the tuples that are also potentially optimal, i.e., best according to some function in F . While nd and po coincide and reduce to the traditional skyline when F is the family of all monotone scoring functions, their behaviors differ when subsets thereof are considered. We discuss the formal properties of these new operators, show how to implement them efficiently, and evaluate them on both synthetic and real datasets.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3111181114",
    "type": "article"
  },
  {
    "title": "Indexing for summary queries",
    "doi": "https://doi.org/10.1145/2508702",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Ke Yi; Lu Wang; Zhewei Wei",
    "corresponding_authors": "",
    "abstract": "Database queries can be broadly classified into two categories: reporting queries and aggregation queries. The former retrieves a collection of records from the database that match the query's conditions, while the latter returns an aggregate, such as count, sum, average, or max (min), of a particular attribute of these records. Aggregation queries are especially useful in business intelligence and data analysis applications where users are interested not in the actual records, but some statistics of them. They can also be executed much more efficiently than reporting queries, by embedding properly precomputed aggregates into an index. However, reporting and aggregation queries provide only two extremes for exploring the data. Data analysts often need more insight into the data distribution than what those simple aggregates provide, and yet certainly do not want the sheer volume of data returned by reporting queries. In this article, we design indexing techniques that allow for extracting a statistical summary of all the records in the query. The summaries we support include frequent items, quantiles, and various sketches, all of which are of central importance in massive data analysis. Our indexes require linear space and extract a summary with the optimal or near-optimal query cost. We illustrate the efficiency and usefulness of our designs through extensive experiments and a system demonstration.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2103112899",
    "type": "article"
  },
  {
    "title": "The Goal Behind the Action",
    "doi": "https://doi.org/10.1145/2934666",
    "publication_date": "2016-11-08",
    "publication_year": 2016,
    "authors": "Dimitra Papadimitriou; Georgia Koutrika; John Mylopoulos; Yannis Velegrakis",
    "corresponding_authors": "",
    "abstract": "Human activity is almost always intentional, be it in a physical context or as part of an interaction with a computer system. By understanding why user-generated events are happening and what purposes they serve, a system can offer a significantly improved and more engaging experience. However, goals cannot be easily captured. Analyzing user actions such as clicks and purchases can reveal patterns and behaviors, but understanding the goals behind these actions is a different and challenging issue. Our work presents a unified, multidisciplinary viewpoint for goal management that covers many different cases where goals can be used and techniques with which they can be exploited. Our purpose is to provide a common reference point to the concepts and challenging tasks that need to be formally defined when someone wants to approach a data analysis problem from a goal-oriented point of view. This work also serves as a springboard to discuss several open challenges and opportunities for goal-oriented approaches in data management, analysis, and sharing systems and applications.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2549996866",
    "type": "article"
  },
  {
    "title": "Database Repairing with Soft Functional Dependencies",
    "doi": "https://doi.org/10.1145/3651156",
    "publication_date": "2024-03-04",
    "publication_year": 2024,
    "authors": "Nofar Carmeli; Martin Grohe; Benny Kimelfeld; Ester Livshits; Muhammad Tibi",
    "corresponding_authors": "",
    "abstract": "A common interpretation of soft constraints penalizes the database for every violation of every constraint, where the penalty is the cost (weight) of the constraint. A computational challenge is that of finding an optimal subset: a collection of database tuples that minimizes the total penalty when each tuple has a cost of being excluded. When the constraints are strict (i.e., have an infinite cost), this subset is a \"cardinality repair\" of an inconsistent database; in soft interpretations, this subset corresponds to a \"most probable world\" of a probabilistic database, a \"most likely intention\" of a probabilistic unclean database, and so on. Within the class of functional dependencies, the complexity of finding a cardinality repair is thoroughly understood. Yet, very little is known about the complexity of this problem in the more general soft semantics. This paper makes a significant progress in this direction. In addition to general insights about the hardness and approximability of the problem, we present algorithms for two special cases: a single functional dependency, and a bipartite matching. The latter is the problem of finding an optimal \"almost matching\" of a bipartite graph where a penalty is paid for every lost edge and every violation of monogamy.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3091429779",
    "type": "article"
  },
  {
    "title": "Linking Entities across Relations and Graphs",
    "doi": "https://doi.org/10.1145/3639363",
    "publication_date": "2024-01-03",
    "publication_year": 2024,
    "authors": "Wenfei Fan; Ping Lü; K.K. Pang; Ruochun Jin; Wenyuan Yu",
    "corresponding_authors": "",
    "abstract": "This article proposes a notion of parametric simulation to link entities across a relational database 𝒟 and a graph G . Taking functions and thresholds for measuring vertex closeness, path associations, and important properties as parameters, parametric simulation identifies tuples t in 𝒟 and vertices v in G that refer to the same real-world entity, based on both topological and semantic matching. We develop machine learning methods to learn the parameter functions and thresholds. We show that parametric simulation is in quadratic-time by providing such an algorithm. Moreover, we develop an incremental algorithm for parametric simulation; we show that the incremental algorithm is bounded relative to its batch counterpart, i.e., it incurs the minimum cost for incrementalizing the batch algorithm. Putting these together, we develop HER , a parallel system to check whether ( t, v ) makes a match, find all vertex matches of t in G , and compute all matches across 𝒟 and G , all in quadratic-time; moreover, HER supports incremental computation of these in response to updates to 𝒟 and G . Using real-life and synthetic data, we empirically verify that HER is accurate with F-measure of 0.94 on average, and is able to scale with database 𝒟 and graph G for both batch and incremental computations.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390532727",
    "type": "article"
  },
  {
    "title": "Identifying the Root Causes of DBMS Suboptimality",
    "doi": "https://doi.org/10.1145/3636425",
    "publication_date": "2024-01-10",
    "publication_year": 2024,
    "authors": "Sabah Currim; Richard T. Snodgrass; Young‐Kyoon Suh",
    "corresponding_authors": "",
    "abstract": "The query optimization phase within a database management system (DBMS) ostensibly finds the fastest query execution plan from a potentially large set of enumerated plans, all of which correctly compute the same result of the specified query. Sometimes the cost-based optimizer selects a slower plan, for a variety of reasons. Previous work has focused on increasing the performance of specific components, often a single operator, within an individual DBMS. However, that does not address the fundamental question: from where does this suboptimality arise, across DBMSes generally? In particular, the contribution of each of many possible factors to DBMS suboptimality is currently unknown. To identify the root causes of DBMS suboptimality, we first introduce the notion of empirical suboptimality of a query plan chosen by the DBMS, indicated by the existence of a query plan that performs more efficiently than the chosen plan, for the same query. A crucial aspect is that this can be measured externally to the DBMS, and thus does not require access to its source code. We then propose a novel predictive model to explain the relationship between various factors in query optimization and empirical suboptimality. Our model associates suboptimality with the factors of complexity of the schema, of the underlying data on which the query is evaluated, of the query itself, and of the DBMS optimizer. The model also characterizes concomitant interactions among these factors. This model induces a number of specific hypotheses that were tested on multiple DBMSes. We performed a series of experiments that examined the plans for thousands of queries run on four popular DBMSes. We tested the model on over a million of these query executions, using correlational analysis, regression analysis, and causal analysis, specifically Structural Equation Modeling (SEM). We observed that the dependent construct of empirical suboptimality prevalence correlates positively with nine specific constructs characterizing four identified factors that explain in concert much of the variance of suboptimality of two extensive benchmarks, across these disparate DBMSes. This predictive model shows that it is the common aspects of these DBMSes that predict suboptimality, not the particulars embedded in the inordinate complexity of each of these DBMSes. This paper thus provides a new methodology to study mature query optimizers, identifies underlying DBMS-independent causes for the observed suboptimality, and quantifies the relative contribution of each of these causes to the observed suboptimality. This work thus provides a roadmap for fundamental improvements of cost-based query optimizers.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390769402",
    "type": "article"
  },
  {
    "title": "Accurate Sampling-Based Cardinality Estimation for Complex Graph Queries",
    "doi": "https://doi.org/10.1145/3689209",
    "publication_date": "2024-08-17",
    "publication_year": 2024,
    "authors": "Pan Hu; Boris Motik",
    "corresponding_authors": "",
    "abstract": "Accurately estimating the cardinality (i.e., the number of answers) of complex queries plays a central role in database systems. This problem is particularly difficult in graph databases, where queries often involve a large number of joins and self-joins. Recently, Park et al. [ 55 ] surveyed seven state-of-the-art cardinality estimation approaches for graph queries. The results of their extensive empirical evaluation show that a sampling method based on the WanderJoin online aggregation algorithm [ 47 ] consistently offers superior accuracy. We extended the framework by Park et al. [ 55 ] with three additional datasets and repeated their experiments. Our results showed that WanderJoin is indeed very accurate, but it can often take a large number of samples and thus be very slow. Moreover, when queries are complex and data distributions are skewed, it often fails to find valid samples and estimates the cardinality as zero. Finally, complex graph queries often go beyond simple graph matching and involve arbitrary nesting of relational operators such as disjunction, difference, and duplicate elimination. Neither of the methods considered by Park et al. [ 55 ] is applicable to such queries. In this article, we present a novel approach for estimating the cardinality of complex graph queries. Our approach is inspired by WanderJoin, but, unlike all approaches known to us, it can process complex queries with arbitrary operator nesting. Our estimator is strongly consistent, meaning that the average of repeated estimates converges with probability one to the actual cardinality. We present optimisations of the basic algorithm that aim to reduce the chance of producing zero estimates and improve accuracy. We show empirically that our approach is both accurate and quick on complex queries and large datasets. Finally, we discuss how to integrate our approach into a simple dynamic programming query planner, and we confirm empirically that our planner produces high-quality plans that can significantly reduce end-to-end query evaluation times.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401657016",
    "type": "article"
  },
  {
    "title": "Processing time-constrained aggregate queries in CASE-DB",
    "doi": "https://doi.org/10.1145/151634.151636",
    "publication_date": "1993-06-01",
    "publication_year": 1993,
    "authors": "Wen‐Chi Hou; Gültekin Özsoyoğlu",
    "corresponding_authors": "",
    "abstract": "In this paper, we present an algorithm to strictly control the time to process an estimator for an aggregate relational query. The algorithm implemented in a prototype database management system, called CASE-DB, iteratively samples from input relations, and evaluates the associated estimator until the time quota expires. In order to estimate the time cost of a query, CASE-DB uses adaptive time cost formulas. The formulas are adaptive in that the parameters of the formulas can be adjusted at runtime to better fit the characteristics of a query. To control the use of time quota, CASE-DB adopts the one-at-a-time-interval time control strategy to make a tradeoff between the risks of overspending and the overhead, finally, experimental evaluation of the methodology is presented.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1984802460",
    "type": "article"
  },
  {
    "title": "Solving queries by tree projections",
    "doi": "https://doi.org/10.1145/155271.155277",
    "publication_date": "1993-09-01",
    "publication_year": 1993,
    "authors": "Yehoshua Sagiv; Oded Shmueli",
    "corresponding_authors": "",
    "abstract": "Suppose a database schema D is extended to D¯ by adding new relation schemas, and states for D are extended to states for D¯ by applying joins and projections to existing relations. It is shown that certain desirable properties that D¯ has with respect to D . These properties amount to the ability to compute efficiently the join of all relations in a state for D from an extension of this state over D¯ . The equivalence is proved for unrestricted (i.e., both finite and infinite) databases. If D¯ is obtained from D by adding a set of new relation schemas that form a tree schema, then the equivalence also holds for finite databases. In this case there is also a polynomial time algorithm for testing the existence of a tree projection of D¯ with respect to D .",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2026838793",
    "type": "article"
  },
  {
    "title": "The building blocks for specifying communication behavior of complex objects",
    "doi": "https://doi.org/10.1145/232616.232622",
    "publication_date": "1996-06-01",
    "publication_year": 1996,
    "authors": "Ling Liu; Robert Meersman",
    "corresponding_authors": "",
    "abstract": "Communication behavior represents dynamic evolution and cooperation of a group of objects in accomplishing a task. It is an important feature in object-oriented systems. We propose the concept of activity as a basic building block for declarative specification of communication behavior in object-oriented database systems, including the temporal ordering of message exchanges within object communication and the behavioral relationships between activity executions. We formally introduce two kinds of activity composition mechanisms: activity specialization and activity aggregation for abstract implementation of communication behavior. The former is suited for behavioral refinement of existing activities into specialized activities. The latter is used for behavioral composition of simpler activities into complex activities, and ultimately, into the envisaged database system. We use first-order temporal logic as an underlying formalism for specification of communication constraints. The well known Air-traffic-control case is used as a running example to highlight the underlying concepts, to illustrate the usefulness, and to assess the effectiveness of the activity model for declarative specification of communication behavior in the relevant universe of discourse. We also propose a methodological framework for integrating activity schema with entity schema in an object-oriented design environment.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2077865798",
    "type": "article"
  },
  {
    "title": "Database concurrency control using data flow graphs",
    "doi": "https://doi.org/10.1145/42338.42345",
    "publication_date": "1988-06-01",
    "publication_year": 1988,
    "authors": "Margaret H. Eich; David L. Wells",
    "corresponding_authors": "",
    "abstract": "A specialized data flow graph, Database Flow Graph ( DBFG ) is introduced. DBFGs may be used for scheduling database operations, particularly in an MIMD database machine environment. A DBFG explicitly maintains intertransaction and intratransaction dependencies, and is constructed from the Transaction Flow Graphs (TFG) of active transactions. A TFG, in turn, is the generalization of a query tree used, for example, in DIRECT [15]. All DBFG schedules are serializable and deadlock free. Operations needed to create and maintain the DBFG structure as transactions are added or removed from the system are discussed. Simulation results show that DBFG scheduling performs as well as two-phase locking.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2044242284",
    "type": "article"
  },
  {
    "title": "Performance of a two-headed disk system when serving database queries under the scan policy",
    "doi": "https://doi.org/10.1145/68012.68017",
    "publication_date": "1989-09-01",
    "publication_year": 1989,
    "authors": "Yannis Manolopoulos; J. G. Kollias",
    "corresponding_authors": "",
    "abstract": "Disk drives with movable two-headed arms are now commercially available. The two heads are separated by a fixed number of cylinders. A major problem for optimizing disk head movement, when answering database requests, is the specification of the optimum number of cylinders separating the two heads. An earlier analytical study assumed a FCFS model and concluded that the optimum separation distance should be equal to 0.44657 of the number of cylinders N of the disk. This paper considers that the SCAN scheduling policy is used in file access, and it applies combinatorial analysis to derive exact formulas for the expected head movement. Furthermore, it is proven that the optimum separation distance is N/2 - 1 (⌈ N /2 - 1⌉ and ⌊ N /2 - 1⌋) if N is even (odd). In addition, a comparison with a single-headed disk system operating under the same scheduling policy shows that if the two heads are optimally spaced, then the mean seek distance is less than one-half of the value obtained with one head. In fact that the SCAN policy is used for many database applications (for example,batching and secondary key retrieval) demonstrates the potential of two-headed disk systems for improving the performance of database systems.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2083758719",
    "type": "article"
  },
  {
    "title": "The overhead of locking (and commit) protocols in distributed databases",
    "doi": "https://doi.org/10.1145/27629.28053",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Ouri Wolfson",
    "corresponding_authors": "Ouri Wolfson",
    "abstract": "The main purpose of a locking protocol is to ensure correct interleaving of actions executed by concurrent transactions. The locking protocol consists of a set of rules dictating how accessed entities should be locked and unlocked. As a result of obeying the rules, transactions in a distributed database incur an overhead. We propose three measures of evaluating this overhead, each most suitable to a different type of underlying communication network. Then, using a graph theoretic model, we analyze and compare three protocols according to each measure: two-phase locking, two-phase locking with a fixed order imposed on the database entities (ensuring deadlock freedom), and the tree protocol. In practice, a transaction also executes the two-phase commit protocol in order to guarantee atomicity. Therefore, the combined overhead of each locking protocol and the two-phase commit protocol is also determined.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1996422234",
    "type": "article"
  },
  {
    "title": "Comment on Bancilhon and Spyratos' “Update semantics and relational views”",
    "doi": "https://doi.org/10.1145/27629.214296",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Arthur M. Keller",
    "corresponding_authors": "Arthur M. Keller",
    "abstract": "article Free AccessComment on Bancilhon and Spyratos' “Update semantics and relational views” Author: Arthur M. Keller The University of Texas at Austin, Department of Computer Sciences, Austin, TX The University of Texas at Austin, Department of Computer Sciences, Austin, TXView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 12Issue 3Sept. 1987 pp 521–523https://doi.org/10.1145/27629.214296Published:01 September 1987Publication History 14citation339DownloadsMetricsTotal Citations14Total Downloads339Last 12 Months5Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2001623199",
    "type": "article"
  },
  {
    "title": "The universal relation revisited (technical correspondence)",
    "doi": "https://doi.org/10.1145/319996.320019",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "William Kent",
    "corresponding_authors": "William Kent",
    "abstract": "article Free Access Share on The universal relation revisited (technical correspondence) Author: William Kent IBM General Products Div., V83/H30, 555 Bailey Ave., San Jose, CA IBM General Products Div., V83/H30, 555 Bailey Ave., San Jose, CAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 8Issue 4pp 644–648https://doi.org/10.1145/319996.320019Published:01 December 1983Publication History 12citation478DownloadsMetricsTotal Citations12Total Downloads478Last 12 Months5Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2005822366",
    "type": "article"
  },
  {
    "title": "Decoupling partitioning and grouping",
    "doi": "https://doi.org/10.1145/1042046.1042052",
    "publication_date": "2004-12-12",
    "publication_year": 2004,
    "authors": "Hanan Samet",
    "corresponding_authors": "Hanan Samet",
    "abstract": "The principle of decoupling the partitioning and grouping processes that form the basis of most spatial indexing methods that use tree directories of buckets is explored. The decoupling is designed to overcome the following drawbacks of traditional solutions:(1) multiple postings in disjoint space decomposition methods that lead to balanced trees such as the hB-tree where a node split in the event of node overflow may be such that one of the children of the node that was split becomes a child of both of the nodes resulting from the split;(2) multiple coverage and nondisjointness of methods based on object hierarchies such as the R-tree which lead to nonunique search paths;(3) directory nodes with similarly-shaped hyper-rectangle bounding boxes with minimum occupancy in disjoint space decomposition methods such as those based on quadtrees and k-d trees that make use of regular decomposition.The first two drawbacks are shown to be overcome by the BV-tree where as a result of decoupling the partitioning and grouping processes, the union of the regions associated with the nodes at a given level of the directory does not necessarily contain all of the data points although all searches take the same amount of time. The BV-tree is not plagued by the third drawback. The third drawback is shown to be overcome by the PK-tree where the grouping process is based on ensuring that every node has at least k objects or blocks. The PK-tree is not plagued by the first two drawbacks as they are inapplicable to it. In both cases, the downside of decoupling the partitioning and grouping processes is that the resulting structure is not necessarily balanced, although, since the nodes have a relatively large fanout, the deviation from a balanced structure is relatively small.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2159936290",
    "type": "article"
  },
  {
    "title": "Design of a balanced multiple-valued file-organization scheme with the least redundancy",
    "doi": "https://doi.org/10.1145/320107.320123",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "Sumiyasu Yamamoto; Shinsei Tazawa; Kazuhiko Ushio; Hideto Ikeda",
    "corresponding_authors": "",
    "abstract": "A new balanced file-organization scheme of order two for multiple-valued records is presented. This scheme is called HUBMFS 2 (Hiroshima University Balanced Multiple-valued File-organization Scheme of order two). It is assumed that records are characterized by m attributes having n possible values each, and the query set consists of queries which specify values of two attributes. It is shown that the redundancy of the bucket (the probability of storing a record in the bucket) is minimized if and only if the structure of the bucket is a partite-claw. A necessary and sufficient condition for the existence of an HUBMFS 2 , which is composed exclusively of partite-claw buckets, is given. A construction algorithm is also given. The proposed HUBMFS 2 is superior to existing BMFS 2 (Balanced Multiple-valued File-organization Schemes of order two) in that it has the least redundancy among all possible BMFS 2 's having the same parameters and that it can be constructed for a less restrictive set of parameters.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1980448457",
    "type": "article"
  },
  {
    "title": "Advanced SQL modeling in RDBMS",
    "doi": "https://doi.org/10.1145/1061318.1061321",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Andrew Witkowski; Srikanth Bellamkonda; Tolga Bozkaya; Nathan Folkert; Abhinav Gupta; John J. Haydu; Lei Sheng; Sankar Subramanian",
    "corresponding_authors": "",
    "abstract": "Commercial relational database systems lack support for complex business modeling. ANSI SQL cannot treat relations as multidimensional arrays and define multiple, interrelated formulas over them, operations which are needed for business modeling. Relational OLAP (ROLAP) applications have to perform such tasks using joins, SQL Window Functions, complex CASE expressions, and the GROUP BY operator simulating the pivot operation. The designated place in SQL for calculations is the SELECT clause, which is extremely limiting and forces the user to generate queries with nested views, subqueries and complex joins. Furthermore, SQL query optimizers are preoccupied with determining efficient join orders and choosing optimal access methods and largely disregard optimization of multiple, interrelated formulas. Research into execution methods has thus far concentrated on efficient computation of data cubes and cube compression rather than on access structures for random, interrow calculations. This has created a gap that has been filled by spreadsheets and specialized MOLAP engines, which are good at specification of formulas for modeling but lack the formalism of the relational model, are difficult to coordinate across large user groups, exhibit scalability problems, and require replication of data between the tool and RDBMS. This article presents an SQL extension called SQL Spreadsheet , to provide array calculations over relations for complex modeling. We present optimizations, access structures, and execution models for processing them efficiently. Special attention is paid to compile time optimization for expensive operations like aggregation. Furthermore, ANSI SQL does not provide a good separation between data and computation and hence cannot support parameterization for SQL Spreadsheets models. We propose two parameterization methods for SQL. One parameterizes ANSI SQL view using subqueries and scalars, which allows passing data to SQL Spreadsheet. Another method presents parameterization of the SQL Spreadsheet formulas. This supports building stand-alone SQL Spreadsheet libraries. These models are then subject to the SQL Spreadsheet optimizations during model invocation time.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2116626927",
    "type": "article"
  },
  {
    "title": "Mining constraint violations",
    "doi": "https://doi.org/10.1145/1206049.1206055",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Stefano Ceri; Francesco Di Giunta; Pier Luca Lanzi",
    "corresponding_authors": "",
    "abstract": "In this article, we introduce pesudoconstraints , a novel data mining pattern aimed at identifying rare events in databases. At first, we formally define pesudoconstraints using a probabilistic model and provide a statistical test to identify pesudoconstraints in a database. Then, we focus on a specific class of pesudoconstraints, named cycle pesudoconstraints , which often occur in databases. We define cycle pesudoconstraints in the context of the ER model and present an automatic method for detecting cycle pesudoconstraints from a relational database. Finally, we present an experiment to show cycle pesudoconstraints “at work” on real data.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1966623674",
    "type": "article"
  },
  {
    "title": "Incremental XPath evaluation",
    "doi": "https://doi.org/10.1145/1862919.1862926",
    "publication_date": "2010-10-12",
    "publication_year": 2010,
    "authors": "Henrik Björklund; Wouter Gelade; Wim Martens",
    "corresponding_authors": "",
    "abstract": "Incremental view maintenance for XPath queries asks to maintain a materialized XPath view over an XML database. It assumes an underlying XML database D and a query Q . One is given a sequence of updates U to D , and the problem is to compute the result of Q ( U ( D )): the result of evaluating query Q on database D after having applied updates U . This article initiates a systematic study of the Boolean version of this problem. In the Boolean version, one only wants to know whether Q ( U ( D )) is empty or not. In order to quickly answer this question, we are allowed to maintain an auxiliary data structure. The complexity of the maintenance algorithms is measured in, (1) the size of the auxiliary data structure, (2) the worst-case time per update needed to compute Q ( U ( D )), and (3) the worst-case time per update needed to bring the auxiliary data structure up to date. We allow three kinds of updates: node insertion, node deletion, and node relabeling. Our main results are that downward XPath queries can be incrementally maintained in time O(depth( D )·poly(| Q |)) per update and conjunctive forward XPath queries in time O(depth( D ) · log(width( D ))·poly(| Q |)) per update, where | Q | is the size of the query, and depth( D ) and width( D ) are the nesting depth and maximum number of siblings in database D , respectively. The auxiliary data structures for maintenance are linear in | D | and polynomial in | Q | in all these cases.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2050453134",
    "type": "article"
  },
  {
    "title": "Distributed Joins and Data Placement for Minimal Network Traffic",
    "doi": "https://doi.org/10.1145/3241039",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Orestis Polychroniou; Wangda Zhang; Kenneth A. Ross",
    "corresponding_authors": "",
    "abstract": "Network communication is the slowest component of many operators in distributed parallel databases deployed for large-scale analytics. Whereas considerable work has focused on speeding up databases on modern hardware, communication reduction has received less attention. Existing parallel DBMSs rely on algorithms designed for disks with minor modifications for networks. A more complicated algorithm may burden the CPUs but could avoid redundant transfers of tuples across the network. We introduce track join, a new distributed join algorithm that minimizes network traffic by generating an optimal transfer schedule for each distinct join key. Track join extends the trade-off options between CPU and network. Track join explicitly detects and exploits locality, also allowing for advanced placement of tuples beyond hash partitioning on a single attribute. We propose a novel data placement algorithm based on track join that minimizes the total network cost of multiple joins across different dimensions in an analytical workload. Our evaluation shows that track join outperforms hash join on the most expensive queries of real workloads regarding both network traffic and execution time. Finally, we show that our data placement optimization approach is both robust and effective in minimizing the total network cost of joins in analytical workloads.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2901077120",
    "type": "article"
  },
  {
    "title": "Proximity measures for rank join",
    "doi": "https://doi.org/10.1145/2109196.2109198",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Davide Martinenghi; Marco Tagliasacchi",
    "corresponding_authors": "",
    "abstract": "We introduce the proximity rank join problem, where we are given a set of relations whose tuples are equipped with a score and a real-valued feature vector. Given a target feature vector, the goal is to return the K combinations of tuples with high scores that are as close as possible to the target and to each other, according to some notion of distance or dissimilarity. The setting closely resembles that of traditional rank join, but the geometry of the vector space plays a distinctive role in the computation of the overall score of a combination. Also, the input relations typically return their results either by distance from the target or by score. Because of these aspects, it turns out that traditional rank join algorithms, such as the well-known HRJN , have shortcomings in solving the proximity rank join problem, as they may read more input than needed. To overcome this weakness, we define a tight bound (used as a stopping criterion) that guarantees instance optimality, that is, an I/O cost is achieved that is always within a constant factor of optimal. The tight bound can also be used to drive an adaptive pulling strategy, deciding at each step which relation to access next. For practically relevant classes of problems, we show how to compute the tight bound efficiently. An extensive experimental study validates our results and demonstrates significant gains over existing solutions.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1988739216",
    "type": "article"
  },
  {
    "title": "A graph-theoretic approach to map conceptual designs to XML schemas",
    "doi": "https://doi.org/10.1145/2445583.2445589",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Massimo Franceschet; Donatella Gubiani; Angelo Montanari; Carla Piazza",
    "corresponding_authors": "",
    "abstract": "We propose a mapping from a database conceptual design to a schema for XML that produces highly connected and nested XML structures. We first introduce two alternative definitions of the mapping, one modeling entities as global XML elements and expressing relationships among them in terms of keys and key references (flat design), the other one encoding relationships by properly including the elements for some entities into the elements for other entities (nest design). Then we provide a benchmark evaluation of the two solutions showing that the nest approach, compared to the flat one, leads to improvements in both query and validation performances. This motivates us to systematically investigate the best way to nest XML structures. We identify two different nesting solutions: a maximum depth nesting, that keeps low the number of costly join operations that are necessary to reconstruct information at query time using the mapped schema, and a maximum density nesting, that minimizes the number of schema constraints used in the mapping of the conceptual schema, thus reducing the validation overhead. On the one hand, the problem of finding a maximum depth nesting turns out to be NP-complete and, moreover, it admits no constant ratio approximation algorithm. On the other hand, we devise a graph-theoretic algorithm, NiduX, that solves the maximum density problem in linear time. Interestingly, NiduX finds the optimal solution for the harder maximum depth problem whenever the conceptual design graph is either acyclic or complete. In randomly generated intermediate cases of the graph topology, we experimentally show that NiduX finds a good approximation of the optimal solution.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2070829911",
    "type": "article"
  },
  {
    "title": "Blazes",
    "doi": "https://doi.org/10.1145/3110214",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Peter Alvaro; Neil Conway; Joseph M. Hellerstein; David Maier",
    "corresponding_authors": "",
    "abstract": "Distributed consistency is perhaps the most-discussed topic in distributed systems today. Coordination protocols can ensure consistency, but in practice they cause undesirable performance unless used judiciously. Scalable distributed architectures avoid coordination whenever possible, but under-coordinated systems can exhibit behavioral anomalies under fault, which are often extremely difficult to debug. This raises significant challenges for distributed system architects and developers. In this article, we present B lazes , a cross-platform program analysis framework that (a) identifies program locations that require coordination to ensure consistent executions, and (b) automatically synthesizes application-specific coordination code that can significantly outperform general-purpose techniques. We present two case studies, one using annotated programs in the Twitter Storm system and another using the Bloom declarative language.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2766916843",
    "type": "article"
  },
  {
    "title": "A Unified Framework for Frequent Sequence Mining with Subsequence Constraints",
    "doi": "https://doi.org/10.1145/3321486",
    "publication_date": "2019-06-05",
    "publication_year": 2019,
    "authors": "Kaustubh Beedkar; Rainer Gemulla; Wim Martens",
    "corresponding_authors": "",
    "abstract": "Frequent sequence mining methods often make use of constraints to control which subsequences should be mined. A variety of such subsequence constraints has been studied in the literature, including length, gap, span, regular-expression, and hierarchy constraints. In this article, we show that many subsequence constraints—including and beyond those considered in the literature—can be unified in a single framework. A unified treatment allows researchers to study jointly many types of subsequence constraints (instead of each one individually) and helps to improve usability of pattern mining systems for practitioners. In more detail, we propose a set of simple and intuitive “pattern expressions” to describe subsequence constraints and explore algorithms for efficiently mining frequent subsequences under such general constraints. Our algorithms translate pattern expressions to succinct finite-state transducers, which we use as computational model, and simulate these transducers in a way suitable for frequent sequence mining. Our experimental study on real-world datasets indicates that our algorithms—although more general—are efficient and, when used for sequence mining with prior constraints studied in literature, competitive to (and in some cases superior to) state-of-the-art specialized methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2949802031",
    "type": "article"
  },
  {
    "title": "Sharing across Multiple MapReduce Jobs",
    "doi": "https://doi.org/10.1145/2560796",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Tomasz Nykiel; Michalis Potamias; Chaitanya Mishra; George Kollios; Nick Koudas",
    "corresponding_authors": "",
    "abstract": "Large-scale data analysis lies in the core of modern enterprises and scientific research. With the emergence of cloud computing, the use of an analytical query processing infrastructure can be directly associated with monetary cost. MapReduce has been a popular framework in the context of cloud computing, designed to serve long-running queries (jobs) which can be processed in batch mode. Taking into account that different jobs often perform similar work, there are many opportunities for sharing. In principle, sharing similar work reduces the overall amount of work, which can lead to reducing monetary charges for utilizing the processing infrastructure. In this article we present a sharing framework tailored to MapReduce, namely, &lt;tt&gt;MRShare&lt;/tt&gt;. Our framework, &lt;tt&gt;MRShare&lt;/tt&gt;, transforms a batch of queries into a new batch that will be executed more efficiently, by merging jobs into groups and evaluating each group as a single query. Based on our cost model for MapReduce, we define an optimization problem and we provide a solution that derives the optimal grouping of queries. Given the query grouping, we merge jobs appropriately and submit them to MapReduce for processing. A key property of &lt;tt&gt;MRShare&lt;/tt&gt; is that it is independent of the MapReduce implementation. Experiments with our prototype, built on top of Hadoop, demonstrate the overall effectiveness of our approach. &lt;tt&gt;MRShare&lt;/tt&gt; is primarily designed for handling I/O-intensive queries. However, with the development of high-level languages operating on top of MapReduce, user queries executed in this model become more complex and CPU intensive. Commonly, executed queries can be modeled as evaluating pipelines of CPU-expensive filters over the input stream. Examples of such filters include, but are not limited to, index probes, or certain types of joins. In this article we adapt some of the standard techniques for filter ordering used in relational and stream databases, propose their extensions, and implement them through &lt;tt&gt;MRAdaptiveFilter&lt;/tt&gt;, an extension of &lt;tt&gt;MRShare&lt;/tt&gt; for expensive filter ordering tailored to MapReduce, which allows one to handle both single- and batch-query execution modes. We present an experimental evaluation that demonstrates additional benefits of &lt;tt&gt;MRAdaptiveFilter&lt;/tt&gt;, when executing CPU-intensive queries in &lt;tt&gt;MRShare&lt;/tt&gt;.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2036787492",
    "type": "article"
  },
  {
    "title": "Analysis of Schemas with Access Restrictions",
    "doi": "https://doi.org/10.1145/2699500",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Michael Benedikt; Pierre Bourhis; Clemens Ley",
    "corresponding_authors": "",
    "abstract": "We study verification of systems whose transitions consist of accesses to a Web-based data source . An access is a lookup on a relation within a relational database, fixing values for a set of positions in the relation. For example, a transition can represent access to a Web form, where the user is restricted to filling in values for a particular set of fields. We look at verifying properties of a schema describing the possible accesses of such a system. We present a language where one can describe the properties of an access path and also specify additional restrictions on accesses that are enforced by the schema. Our main property language, AccessLTL, is based on a first-order extension of linear-time temporal logic, interpreting access paths as sequences of relational structures. We also present a lower-level automaton model, A-automata, into which AccessLTL specifications can compile. We show that AccessLTL and A-automata can express static analysis problems related to “querying with limited access patterns” that have been studied in the database literature in the past, such as whether an access is relevant to answering a query and whether two queries are equivalent in the accessible data they can return. We prove decidability and complexity results for several restrictions and variants of AccessLTL and explain which properties of paths can be expressed in each restriction.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2065464789",
    "type": "article"
  },
  {
    "title": "Efficient Processing of Skyline-Join Queries over Multiple Data Sources",
    "doi": "https://doi.org/10.1145/2699483",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Mithila Nagendra; K. Selçuk Candan",
    "corresponding_authors": "",
    "abstract": "Efficient processing of skyline queries has been an area of growing interest. Many of the earlier skyline techniques assumed that the skyline query is applied to a single data table. Naturally, these algorithms were not suitable for many applications in which the skyline query may involve attributes belonging to multiple data sources. In other words, if the data used in the skyline query are stored in multiple tables, then join operations would be required before the skyline can be searched. The task of computing skylines on multiple data sources has been coined as the skyline-join problem and various skyline-join algorithms have been proposed. However, the current proposals suffer several drawbacks: they often need to scan the input tables exhaustively in order to obtain the set of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive pairwise tuple-to-tuple comparisons. In this article, we aim to address these shortcomings by proposing two novel skyline-join algorithms, namely skyline-sensitive join (S 2 J) and symmetric skyline-sensitive join (S 3 J), to process skyline queries over two data sources. Our approaches compute the results using a novel layer/region pruning technique ( LR-pruning ) that prunes the join space in blocks as opposed to individual data points, thereby avoiding excessive pairwise point-to-point dominance checks. Furthermore, the S 3 J algorithm utilizes an early stopping condition in order to successfully compute the skyline results by accessing only a subset of the input tables. In addition to S 2 J and S 3 J, we also propose the S 2 J-M and S 3 J-M algorithms. These algorithms extend S 2 J's and S 3 J's two-way skyline-join ability to efficiently process skyline-join queries over more than two data sources. S 2 J-M and S 3 J-M leverage the extended concept of LR-pruning , called M -way LR-pruning , to compute multi-way skyline-joins in which more than two data sources are integrated during skyline processing. We report extensive experimental results that confirm the advantages of the proposed algorithms over state-of-the-art skyline-join techniques.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2294291508",
    "type": "article"
  },
  {
    "title": "Height Optimized Tries",
    "doi": "https://doi.org/10.1145/3506692",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Robert Binna; Eva Zangerle; Martin Pichl; Günther Specht; Viktor Leis",
    "corresponding_authors": "",
    "abstract": "We present the Height Optimized Trie (HOT), a fast and space-efficient in-memory index structure. The core algorithmic idea of HOT is to dynamically vary the number of bits considered at each node, which enables a consistently high fanout and thereby good cache efficiency. For a fixed maximum node fanout, the overall tree height is minimal and its structure is deterministically defined. Multiple carefully engineered node implementations using SIMD instructions or lightweight compression schemes provide compactness and fast search and optimize HOT structures for different usage scenarios. Our experiments, which use a wide variety of workloads and data sets, show that HOT outperforms other state-of-the-art index structures for string keys both in terms of search performance and memory footprint, while being competitive for integer keys.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4226174211",
    "type": "article"
  },
  {
    "title": "Enabling Timely and Persistent Deletion in LSM-Engines",
    "doi": "https://doi.org/10.1145/3599724",
    "publication_date": "2023-06-08",
    "publication_year": 2023,
    "authors": "Subhadeep Sarkar; Tarikul Islam Papon; Dimitris Staratzis; Zichen Zhu; Manos Athanassoulis",
    "corresponding_authors": "",
    "abstract": "Data-intensive applications have fueled the evolution of log-structured merge (LSM) based key-value engines that employ the out-of-place paradigm to support high ingestion rates with low read/write interference. These benefits, however, come at the cost of treating deletes as second-class citizens . A delete operation inserts a tombstone that invalidates older instances of the deleted key. State-of-the-art LSM-engines do not provide guarantees as to how fast a tombstone will propagate to persist the deletion . Further, LSM-engines only support deletion on the sort key. To delete on another attribute (e.g., timestamp), the entire tree is read and re-written, leading to undesired latency spikes and increasing the overall operational cost of a database. Efficient and persistent deletion is key to support: (i) streaming systems operating on a window of data, (ii) privacy with latency guarantees on data deletion, and (iii) en masse cloud deployment of data systems. Further, we document that LSM-based key-value engines perform suboptimally in the presence of deletes in a workload. Tombstone-driven logical deletes, by design, are unable to purge the deleted entries in a timely manner, and retaining the invalidated entries perpetually affects the overall performance of LSM-engines in terms of space amplification, write amplification, and read performance. Moreover, the potentially unbounded latency for persistent deletes brings in critical privacy concerns in light of the data privacy protection regulations, such as the right to be forgotten in EU’s GDPR, the right to delete in California’s CCPA and CPRA, and deletion right in Virginia’s VCDPA. Toward this, we introduce the delete design space for LSM-trees and highlight the performance implications of the different classes of delete operations. To address these challenges, in this article, we build a new key-value storage engine, Lethe + , that uses a very small amount of additional metadata, a set of new delete-aware compaction policies, and a new physical data layout that weaves the sort and the delete key order. We show that Lethe + supports any user-defined threshold for the delete persistence latency offering higher read throughput (1.17× -1.4×) and lower space amplification (2.1× -9.8×), with a modest increase in write amplification (between 4% and 25%) that can be further amortized to less than 1%. In addition, Lethe + supports efficient range deletes on a secondary delete key by dropping entire data pages without sacrificing read performance or employing a costly full tree merge.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4380050158",
    "type": "article"
  },
  {
    "title": "Cost-based Data Prefetching and Scheduling in Big Data Platforms over Tiered Storage Systems",
    "doi": "https://doi.org/10.1145/3625389",
    "publication_date": "2023-09-24",
    "publication_year": 2023,
    "authors": "Herodotos Herodotou; Elena Kakoulli",
    "corresponding_authors": "",
    "abstract": "The use of storage tiering is becoming popular in data-intensive compute clusters due to the recent advancements in storage technologies. The Hadoop Distributed File System, for example, now supports storing data in memory, SSDs, and HDDs, while OctopusFS and hatS offer fine-grained storage tiering solutions. However, current big data platforms (such as Hadoop and Spark) are not exploiting the presence of storage tiers and the opportunities they present for performance optimizations. Specifically, schedulers and prefetchers will make decisions only based on data locality information and completely ignore the fact that local data are now stored on a variety of storage media with different performance characteristics. This article presents Trident, a scheduling and prefetching framework that is designed to make task assignment, resource scheduling, and prefetching decisions based on both locality and storage tier information. Trident formulates task scheduling as a minimum cost maximum matching problem in a bipartite graph and utilizes two novel pruning algorithms for bounding the size of the graph, while still guaranteeing optimality. In addition, Trident extends YARN’s resource request model and proposes a new storage-tier-aware resource scheduling algorithm. Finally, Trident includes a cost-based data prefetching approach that coordinates with the schedulers for optimizing prefetching operations. Trident is implemented in both Spark and Hadoop and evaluated extensively using a realistic workload derived from Facebook traces as well as an industry-validated benchmark, demonstrating significant benefits in terms of application performance and cluster efficiency.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386998168",
    "type": "article"
  },
  {
    "title": "Quorum consensus in nested-transaction systems",
    "doi": "https://doi.org/10.1145/195664.195666",
    "publication_date": "1994-12-01",
    "publication_year": 1994,
    "authors": "Kenneth J. Goldman; Nancy Lynch",
    "corresponding_authors": "",
    "abstract": "Gifford's Quorum Consensus algorithm for data replication is studied in the context of nested transactions and transaction failures (aborts), and a fully developed reconfiguration strategy is presented. A formal description of the algorithm is presented using the Input/Output automaton model for nested-transaction systems due to Lynch and Merritt. In this description, the algorithm itself is described in terms of nested transactions. The formal description is used to construct a complete proof of correctness that uses standard assertional techniques, is based on a natural correctness condition, and takes advantage of modularity that arises from describing the algorithm as nested transactions. The proof is accomplished hierarchically, showing that a fully replicated reconfigurable system “simulates” an intermediate replicated system, and that the intermediate system simulates an unreplicated system. The presentation and proof treat issues of data replication entirely separately from issues of concurrency control and recovery.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1997291822",
    "type": "article"
  },
  {
    "title": "Incremental computation of nested relational query expressions",
    "doi": "https://doi.org/10.1145/210197.210198",
    "publication_date": "1995-06-01",
    "publication_year": 1995,
    "authors": "Lars Bækgaard; Leo Mark",
    "corresponding_authors": "",
    "abstract": "Efficient algorithms for incrementally computing nested query expressions do not exist. Nested query expressions are query expressions in which selection/join predicates contain subqueries. In order to respond to this problem, we propose a two-step strategy for incrementaly computing nested query expressions. In step (1), the query expression is transformed into an equivalent unnested flat query expression. In step (2), the flat query expression is incrementally computed. To support step (1), we have developed a very concise algebra-to-algebra transformation algorithm, and we have formally proved its correctness. The flat query expressions resulting from the transformation make intensive use of the relational set-difference operator. To support step (2), we present and analyze an efficient algorithm for incrementally computing set differences based on view pointer caches. When combined with existing incremental algorithms for SPJ queries, our incremental set-difference algorithm can be used to compute the unnested flat query expressions efficiently. It is important to notice that without our incremental set-difference algorithm the existing incremental algorithms for SPJ queries are useless for any query involving the set-difference operator, including queries that are not the result of unnesting nested queries.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2019846126",
    "type": "article"
  },
  {
    "title": "On robust transaction routing and load sharing",
    "doi": "https://doi.org/10.1145/111197.111210",
    "publication_date": "1991-09-01",
    "publication_year": 1991,
    "authors": "Philip S. Yu; Avraham Leff; Yann-Hang Lee",
    "corresponding_authors": "",
    "abstract": "In this paper we examine the issue of robust transaction routing in a locally distributed database environment where transaction characteristics such as reference locality imply that certain processing systems can be identified as being more suitable than others for a given transaction class. A response time based routing strategy can strike a balance between indiscriminate sharing of the load and routing based only on transaction affinity. Since response time estimates depend on workload and system parameters that may not be readily available, it is important to examine the robustness of routing decisions to information accuracy. We find that a strategy which strictly tries to minimize the response time of incoming transactions is sensitive to the accuracy of certain parameter values. On the other hand, naive strategies, that simply ignore the parameters in making routing decisions, have even worse performance. Three alternative strategies are therefore examined: threshold, discriminatory, and adaptive. Instead of just optimizing an incoming transaction's response time, the first two strategies pursue a strategy that is somewhat more oriented towards global optimization. This is achieved by being more restrictive on either the condition or the candidate for balancing the load. The third strategy, while trying to minimize the response time of individual incoming transactions, employs a feedback process to adaptively adjust future response time estimates. It monitors the discrepancy between the actual and estimated response times and introduces a correction factor based on regression analysis. All three strategies are shown to be robust with respect to the accuracy of workload and system parameters used in the response time estimation.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2010140684",
    "type": "article"
  },
  {
    "title": "A decision theoretic approach to information retrieval",
    "doi": "https://doi.org/10.1145/88636.88597",
    "publication_date": "1990-09-01",
    "publication_year": 1990,
    "authors": "James C. Moore; William B. Richmond; Andrew B. Whinston",
    "corresponding_authors": "",
    "abstract": "We present the file search problem in a decision-theoretic framework, and discuss a variation of it that we call the common index problem. The goal of the common index problem is to return the best available record in the file, where best is in terms of a class of user preferences. We use dynamic programming to construct an optimal algorithm using two different optimality criteria, and we develop sufficient conditions for obtaining complete information.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2086308611",
    "type": "article"
  },
  {
    "title": "Consistency of transactions and random batch",
    "doi": "https://doi.org/10.1145/7239.214287",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "Rudolf Bayer",
    "corresponding_authors": "Rudolf Bayer",
    "abstract": "A synchronization technique and scheduling strategy is described, which allows us to run a batch process simultaneously with on-line transactions. The batch process and the transactions are serialized in such a way that consistency level 3 is achieved.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2130147246",
    "type": "article"
  },
  {
    "title": "Partitioned two-phase locking",
    "doi": "https://doi.org/10.1145/7239.7477",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "Meichun Hsu; Arvola Chan",
    "corresponding_authors": "",
    "abstract": "In a large integrated database, there often exists an “information hierarchy,” where both raw data and derived data are stored and used together. Therefore, among update transactions, there will often be some that perform only read accesses from a certain (i.e., the “raw” data) portion of the database and write into another (i.e., the “derived” data) portion. A conventional concurrency control algorithm would have treated such transactions as regular update transactions and subjected them to the usual protocols for synchronizing update transactions. In this paper such transactions are examined more closely. The purpose is to devise concurrency control methods that allow the computation of derived information to proceed without interfering with the updating of raw data. The first part of the paper presents a proof method for correctness of concurrency control algorithms in a hierarchically decomposed database. The proof method provides a framework for understanding the intricacies in dealing with hierarchically decomposed databases. The second part of the paper is an application of the proof method to show the correctness of a two-phase-locking- based algorithm, called partitioned two-phase locking, for hierarchically decomposed databases. This algorithm is a natural extension to the Version Pool method proposed previously in the literature.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2167845441",
    "type": "article"
  },
  {
    "title": "Reduced MVDs and minimal covers",
    "doi": "https://doi.org/10.1145/27629.214286",
    "publication_date": "1987-09-01",
    "publication_year": 1987,
    "authors": "Z. Meral Özsoyoğlu; Li-Yan Yuan",
    "corresponding_authors": "",
    "abstract": "Multivalued dependencies (MVDs) are data dependencies that appear frequently in the “real world” and play an important role in designing relational database schemes. Given a set of MVDs to constrain a database scheme, it is desirable to obtain an equivalent set of MVDs that do not have any redundancies. In this paper we define such a set of MVDs, called reduced MVDs, and present an algorithm to obtain reduced MVDs. We also define a minimal cover of a set of MVDs, which is a set of reduced MVDs, and give an efficient method to find such a minimal cover. The significance and properties of reduced MVDs are also discussed in the context of database design (e.g., 4NF decomposition) and conflict-free MVDs.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2053506781",
    "type": "article"
  },
  {
    "title": "On the complexity of designing optimal partial-match retrieval systems",
    "doi": "https://doi.org/10.1145/319996.320004",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "Shlomo Moran",
    "corresponding_authors": "Shlomo Moran",
    "abstract": "We consider the problem of designing an information retrieval system on which partial match queries have to be answered. Each record in the system consists of a list of attributes , and a partial match query specifies the values of some of the attributes. The records are stored in buckets in a secondary memory, and in order to answer a partial match query all the buckets that may contain a record satisfying the specifications of that query must be retrieved. The bucket in which a given record is stored is found by a multiple key hashing function, which maps each attribute to a string of a fixed number of bits. The address of that bucket is then represented by the string obtained by concatenating the strings on which the various attributes were mapped. A partial match query may specify only part of the bits in the string representing the address, and the larger the number of bits specified, the smaller the number of buckets that have to be retrieved in order to answer the query. The optimization problem considered in this paper is that of deciding to how many bits each attribute should be mapped by the bashing function above, so that the expected number of buckets retrieved per query is minimized. Efficient solutions for special cases of this problem have been obtained in [1], [12], and [14]. It is shown that in general the problem is NP-hard, and that if P ≠ NP, it is also not fully approximable. Two heuristic algorithms for the problem are also given and compared.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1964803574",
    "type": "article"
  },
  {
    "title": "Hierarchical file organization and its application to similar-string matching",
    "doi": "https://doi.org/10.1145/319989.319994",
    "publication_date": "1983-09-01",
    "publication_year": 1983,
    "authors": "Tetsuro Ito; Makoto Kizawa",
    "corresponding_authors": "",
    "abstract": "The automatic correction of misspelled inputs is discussed from a viewpoint of similar-string matching. First a hierarchical file organization based on a linear ordering of records is presented for retrieving records highly similar to any input query. Then the spelling problem is attacked by constructing a hierarchical file for a set of strings in a dictionary of English words. The spelling correction steps proceed as follows: (1) find one of the best-match strings which are most similar to a query, (2) expand the search area for obtaining the good-match strings, and (3) interrupt the file search as soon as the required string is displayed. Computational experiments verify the performance of the proposed methods for similar-string matching under the UNIX™ time-sharing system.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1995796583",
    "type": "article"
  },
  {
    "title": "A declarative approach to optimize bulk loading into databases",
    "doi": "https://doi.org/10.1145/1005566.1005567",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Sihem Amer-Yahia; Sophie Cluet",
    "corresponding_authors": "",
    "abstract": "Applications, such as warehouse maintenance, need to load large data volumes regularly. The efficiency of loading depends on the resources that are available at the source and at the target systems. Our work aims to understand the performance criteria that are involved in bulk loading data into a database and to devise tailored optimization strategies.Unlike commercial systems and previous research on the same topic, our approach follows the fundamental database principle of physical-logical independence. A loading program is represented as a sequence of algebraic expressions. This abstraction enables the use of appropriate algebraic rewritings to optimize a loading program and of a cost model that takes into consideration efficiency criteria such as the processing times at the source and target systems and the bandwidth between them. A slow-loading program may be preferable if it does not slow down other applications by consuming too much memory. Thus, we view the problem of optimizing a loading program as finding a compromise between several efficiency criteria.The ability to represent loading programs in an algebra and performance criteria in a cost model has two very desirable properties: reusability and efficiency. Database programmers do not have to write loading programs by hand. In addition, tuning loading programs becomes easier since programmers have a better control on the performance criteria specified in the cost model. The algebra captures data transformations that would have been otherwise hardcoded in loading programs. Consequently, richer optimizations can be explored. Finally, our optimization techniques are not specific to one particular system. They can be used for loading data and from to any structured store (e.g., relational, structured files).We implemented our ideas in a complete environment for migrating ODBC-compliant databases into the O 2 object-oriented database system. This prototype provides a declarative view language to specify loading, an interface to specify directives, such as desired database physical organization and constraints on several criteria, such as resource and bandwidth consumption, an algebraic optimizer, a code generator, and an execution environment to control failures and guarantee incremental loading. Our experiments show that a tailored optimization is necessary when loading large data volumes into a database.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2016042910",
    "type": "article"
  },
  {
    "title": "Query-sensitive embeddings",
    "doi": "https://doi.org/10.1145/1242524.1242525",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Vassilis Athitsos; Marios Hadjieleftheriou; George Kollios; Stan Sclaroff",
    "corresponding_authors": "",
    "abstract": "A common problem in many types of databases is retrieving the most similar matches to a query object. Finding these matches in a large database can be too slow to be practical, especially in domains where objects are compared using computationally expensive similarity (or distance) measures. Embedding methods can significantly speed-up retrieval by mapping objects into a vector space, where distances can be measured rapidly using a Minkowski metric. In this article we present a novel way to improve embedding quality. In particular, we propose to construct embeddings that use a query-sensitive distance measure for the target space of the embedding. This distance measure is used to compare those vectors that the query and database objects are mapped to. The term “query-sensitive” means that the distance measure changes, depending on the current query object. We demonstrate theoretically that using a query-sensitive distance measure increases the modeling power of embeddings and allows them to capture more of the structure of the original space. We also demonstrate experimentally that query-sensitive embeddings can significantly improve retrieval performance. In experiments with an image database of handwritten digits and a time-series database, the proposed method outperforms existing state-of-the-art non-Euclidean indexing methods, meaning that it provides significantly better tradeoffs between efficiency and retrieval accuracy.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2020242055",
    "type": "article"
  },
  {
    "title": "Efficient estimation of joint queries from multiple OLAP databases",
    "doi": "https://doi.org/10.1145/1206049.1206051",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Elaheh Pourabbas; Arie Shoshani",
    "corresponding_authors": "",
    "abstract": "Given an OLAP query expressed over multiple source OLAP databases, we study the problem of estimating the resulting OLAP target database. The problem arises when it is not possible to derive the result from a single database. The method we use is linear indirect estimation, commonly used for statistical estimation. We examine two obvious computational methods for computing such a target database, called the full cross-product (F) and preaggregation (P) methods. We study the accuracy and computational cost of these methods. While the F method provides a more accurate estimate, it is more expensive computationally than P. Our contribution is in proposing a third, new method, called the partial preaggregation method (PP), which is significantly less expensive than F, but just as accurate. We prove formally that the PP method yields the same results as the F method, and provide analytical and experimental results on the accuracy and computational benefits of the PP method.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2078278619",
    "type": "article"
  },
  {
    "title": "Expressiveness and complexity of XML publishing transducers",
    "doi": "https://doi.org/10.1145/1412331.1412337",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Wenfei Fan; Floris Geerts; Frank Neven",
    "corresponding_authors": "",
    "abstract": "A number of languages have been developed for specifying XML publishing, that is, transformations of relational data into XML trees. These languages generally describe the behaviors of a middleware controller that builds an output tree iteratively, issuing queries to a relational source and expanding the tree with the query results at each step. To study the complexity and expressive power of XML publishing languages, this article proposes a notion of publishing transducers , which generate XML trees from relational data. We study a variety of publishing transducers based on what relational queries a transducer can issue, what temporary stores a transducer can use during tree generation, and whether or not some tree nodes are allowed to be virtual, that is, excluded from the output tree. We first show how existing XML publishing languages can be characterized by such transducers, and thus provide a synergy between theory and practice. We then study the membership, emptiness, and equivalence problems for various classes of transducers. We establish lower and upper bounds, all matching, ranging from PTIME to undecidable. Finally, we investigate the expressive power of these transducers and existing languages. We show that when treated as relational query languages, different classes of transducers capture either complexity classes (e.g., PSPACE) or fragments of datalog (e.g., linear datalog). For tree generation, we establish connections between publishing transducers and logical transductions, among other things.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2615048832",
    "type": "article"
  },
  {
    "title": "Comparing workflow specification languages",
    "doi": "https://doi.org/10.1145/2188349.2188352",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Serge Abiteboul; Pierre Bourhis; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "We address the problem of comparing the expressiveness of workflow specification formalisms using a notion of view of a workflow. Views allow to compare widely different workflow systems by mapping them to a common representation capturing the observables relevant to the comparison. Using this framework, we compare the expressiveness of several workflow specification mechanisms, including automata, temporal constraints, and pre-and postconditions, with XML and relational databases as underlying data models. One surprising result shows the considerable power of static constraints to simulate apparently much richer workflow control mechanisms.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2003812367",
    "type": "article"
  },
  {
    "title": "Efficient reasoning about data trees via integer linear programming",
    "doi": "https://doi.org/10.1145/2338626.2338632",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Claire David; Leonid Libkin; Tony Tan",
    "corresponding_authors": "",
    "abstract": "Data trees provide a standard abstraction of XML documents with data values: they are trees whose nodes, in addition to the usual labels, can carry labels from an infinite alphabet (data). Therefore, one is interested in decidable formalisms for reasoning about data trees. While some are known—such as the two-variable logic—they tend to be of very high complexity, and most decidability proofs are highly nontrivial. We are therefore interested in reasonable complexity formalisms as well as better techniques for proving decidability. Here we show that many decidable formalisms for data trees are subsumed—fully or partially—by the power of tree automata together with set constraints and linear constraints on cardinalities of various sets of data values. All these constraints can be translated into instances of integer linear programming, giving us an NP upper bound on the complexity of the reasoning tasks. We prove that this bound, as well as the key encoding technique, remain very robust, and allow the addition of features such as counting of paths and patterns, and even a concise encoding of constraints, without increasing the complexity. The NP bound is tight, as we also show that the satisfiability of a single set constraint is already NP-hard. We then relate our results to several reasoning tasks over XML documents, such as satisfiability of schemas and data dependencies and satisfiability of the two-variable logic. As a final contribution, we describe experimental results based on the implementation of some reasoning tasks using the SMT solver Z3.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2062179480",
    "type": "article"
  },
  {
    "title": "Exact and approximate algorithms for the most connected vertex problem",
    "doi": "https://doi.org/10.1145/2188349.2188354",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Cheng Sheng; Yufei Tao; Jianzhong Li",
    "corresponding_authors": "",
    "abstract": "An (edge) hidden graph is a graph whose edges are notexplicitly given. Detecting the presence of an edge requires an expensive edge probing query. We consider the k Most Connected Vertex ( k -MCV) problem on hidden bipartite graphs. Given a bipartite graph G with independent vertex sets B and W , the goal is to find the k vertices in B with the largest degrees using the minimum number of queries. This problem can be regarded as a top- k extension of semi-join, and is encountered in several applications in practice. If B and W have n and m vertices, respectively, the number of queries needed to solve the problem is nm in the worst case. This, however, is a pessimistic estimate on how many queries are necessary on practical data. In fact, on some inputs, the problem may be settled with only km + n queries, which is significantly lower than nm for k ≪ n . The huge difference between km + n and nm makes it interesting to design an adaptive algorithm that is guaranteed to achieve the best possible performance on every input G . For k ≤ n /2, we give an algorithm that is instance optimal among a broad class of solutions. This means that, for any G , our algorithm can perform more queries than the optimal solution (which is unknown) by only a constant factor, which can be shown at most 2. As a second step, we study an ε-approximate version of the k -MCV problem, where ε is a parameter satisfying 0 &lt; ε &lt; 1. The goal is to return k black vertices b 1 , …, b k such that the degree of b i ( i ≤ k ) can be smaller than t i by a factor of at most ε, where t i , …, t k (in nonascending order) are the degrees of the k most connected black vertices. We give an efficient randomized algorithm that successfully finds the correct answer with high probability. In particular, for a fixed ε and a fixed success probability, our algorithm performs o(nm) queries in expectation for t k = ω(log n ). In other words, whenever t k is greater than log n by more than a constant, our algorithm beats the Ω( nm ) lower bound for solving the k -MCV problem exactly. All the proposed algorithms, despite the complication of their underlying theory, are simple enough for easy implementation in practice. Extensive experiments have confirmed that their performance in reality agrees with our theoretical findings very well.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2108023620",
    "type": "article"
  },
  {
    "title": "Interactive Mapping Specification with Exemplar Tuples",
    "doi": "https://doi.org/10.1145/3321485",
    "publication_date": "2019-06-05",
    "publication_year": 2019,
    "authors": "Angela Bonifati; Ugo Comignani; Emmanuel Coquery; Romuald Thion",
    "corresponding_authors": "",
    "abstract": "While schema mapping specification is a cumbersome task for data curation specialists, it becomes unfeasible for non-expert users, who are unacquainted with the semantics and languages of the involved transformations. In this article, we present an interactive framework for schema mapping specification suited for non-expert users. The underlying key intuition is to leverage a few exemplar tuples to infer the underlying mappings and iterate the inference process via simple user interactions under the form of Boolean queries on the validity of the initial exemplar tuples. The approaches available so far are mainly assuming pairs of complete universal data examples, which can be solely provided by data curation experts, or are limited to poorly expressive mappings. We present a quasi-lattice-based exploration of the space of all possible mappings that satisfy arbitrary user exemplar tuples. Along the exploration, we challenge the user to retain the mappings that fit the user’s requirements at best and to dynamically prune the exploration space, thus reducing the number of user interactions. We prove that after the refinement process, the obtained mappings are correct and complete. We present an extensive experimental analysis devoted to measure the feasibility of our interactive mapping strategies and the inherent quality of the obtained mappings.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2952363528",
    "type": "article"
  },
  {
    "title": "Evaluation of Machine Learning Algorithms in Predicting the Next SQL Query from the Future",
    "doi": "https://doi.org/10.1145/3442338",
    "publication_date": "2021-03-18",
    "publication_year": 2021,
    "authors": "Venkata Vamsikrishna Meduri; Kanchan Chowdhury; Mohamed Sarwat",
    "corresponding_authors": "",
    "abstract": "Prediction of the next SQL query from the user, given her sequence of queries until the current timestep, during an ongoing interaction session of the user with the database, can help in speculative query processing and increased interactivity. While existing machine learning-- (ML) based approaches use recommender systems to suggest relevant queries to a user, there has been no exhaustive study on applying temporal predictors to predict the next user issued query. In this work, we experimentally compare ML algorithms in predicting the immediate next future query in an interaction workload, given the current user query or the sequence of queries in a user session thus far. As a part of this, we propose the adaptation of two powerful temporal predictors: (a) Recurrent Neural Networks (RNNs) and (b) a Reinforcement Learning approach called Q-Learning that uses Markov Decision Processes. We represent each query as a comprehensive set of fragment embeddings that not only captures the SQL operators, attributes, and relations but also the arithmetic comparison operators and constants that occur in the query. Our experiments on two real-world datasets show the effectiveness of temporal predictors against the baseline recommender systems in predicting the structural fragments in a query w.r.t. both quality and time. Besides showing that RNNs can be used to synthesize novel queries, we find that exact Q-Learning outperforms RNNs despite predicting the next query entirely from the historical query logs.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3138293303",
    "type": "article"
  },
  {
    "title": "MacroBase",
    "doi": "https://doi.org/10.1145/3276463",
    "publication_date": "2018-12-06",
    "publication_year": 2018,
    "authors": "Firas Abuzaid; Peter Bailis; Jialin Ding; Edward Gan; Samuel Madden; Deepak Narayanan; Kexin Rong; Sahaana Suri",
    "corresponding_authors": "",
    "abstract": "As data volumes continue to rise, manual inspection is becoming increasingly untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient, accurate, and modular analyses that highlight and aggregate important and unusual behavior, acting as a search engine for fast data. MacroBase is able to deliver order-of-magnitude speedups over alternatives by optimizing the combination of explanation (i.e., feature selection) and classification tasks and by leveraging a new reservoir sampler and heavy-hitters sketch specialized for fast data streams. As a result, MacroBase delivers accurate results at speeds of up to 2M events per second per query on a single core. The system has delivered meaningful results in production, including at a telematics company monitoring hundreds of thousands of vehicles.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4211256068",
    "type": "article"
  },
  {
    "title": "Sloth",
    "doi": "https://doi.org/10.1145/2894749",
    "publication_date": "2016-06-06",
    "publication_year": 2016,
    "authors": "Alvin Cheung; Samuel Madden; Armando Solar-Lezama",
    "corresponding_authors": "",
    "abstract": "Many web applications store persistent data in databases. During execution, such applications spend a significant amount of time communicating with the database for retrieval and storing of persistent data over the network. These network round-trips represent a significant fraction of the overall execution time for many applications (especially those that issue a lot of database queries) and, as a result, increase their latency. While there has been prior work that aims to eliminate round-trips by batching queries, they are limited by (1) a requirement that developers manually identify batching opportunities, or (2) the fact that they employ static program analysis techniques that cannot exploit many opportunities for batching, as many of these opportunities require knowing precise information about the state of the running program. In this article, we present S loth , a new system that extends traditional lazy evaluation to expose query batching opportunities during application execution, even across loops, branches, and method boundaries. Many such opportunities often require expensive and sophisticated static analysis to recognize from the application source code. Rather than doing so, S loth instead makes use of dynamic analysis to capture information about the program state and, based on that information, decides how to batch queries and when to issue them to the database. We formalize extended lazy evaluation and prove that it preserves program semantics when executed under standard semantics. Furthermore, we describe our implementation of S loth and our experience in evaluating S loth using over 100 benchmarks from two large-scale open-source applications, in which S loth achieved up to a 3 × reduction in page load time by delaying computation using extended lazy evaluation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4252081683",
    "type": "article"
  },
  {
    "title": "An execution model for limited ambiguity rules and its application to derived data update",
    "doi": "https://doi.org/10.1145/219035.219039",
    "publication_date": "1995-12-01",
    "publication_year": 1995,
    "authors": "I.-Min Amy Chen; Richard Hull; Dennis McLeod",
    "corresponding_authors": "",
    "abstract": "A novel execution model for rule application in active databases is developed and applied to the problem of updating derived data in a database represented using a semantic, object-based database model. The execution model is based on the use of “limited ambiguity rules” (LARs), which permit disjunction in rule actions. The execution model essentially performs a breadth-first exploration of alternative extensions of a user-requested update. Given an object-based database schema, both integrity constraints and specifications of derived classes and attributes are compiled into a family of limited ambiguity rules. A theoretical analysis shows that the approach is sound: the execution model returns all valid “completions” of a user-requested update, or terminates with an appropriate error notification. The complexity of the approach in connection with derived data update is considered.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1975438514",
    "type": "article"
  },
  {
    "title": "Semantics for update rule programs and implementation in a relational database management system",
    "doi": "https://doi.org/10.1145/236711.236714",
    "publication_date": "1996-12-01",
    "publication_year": 1996,
    "authors": "Louiqa Raschid; Jorge Lobo",
    "corresponding_authors": "",
    "abstract": "In this paper, we present our research on defining a correct semantics for a class of update rule (UR) programs, and discuss implemanting these programs in a DBMS environment. Update rules execute by updating relations in a database which may cause the further execution of rules. A correct semantics must guarantee that the execution of the rules will terminate and that it will produce a minimal updated database. The class of UR programs is syntactically identified, based upon a concept that is similar to stratification. We extend that strict definition of stratification and allow a relaxed criterion for partitioning of the rules in the UR program. This relaxation allows a limited degree of nondeterminism in rule execution. We define an execution semantics based upon a monotonic fixpoint operator T UR , resulting in a set of fixpoints for UR. The monotionicity of the operator is maintained nby explicitly representing the effect of asserting and retracting tuples in the database. A declarative semantics for the update rule program is obtained by associating a normal logic program UR to represent the UR program. We use the stable model semantics which characterize a normal logic program by a set of minimal models which are called stable models. We show the equivalence between the set of fixpoints for UR and the set of stable models for UR. We briefly discuss implementing the fixpoint semantics of the UR program in a DBMS environment. Relations that can be updated by the rules are updatable relations and they are extended with two flags. An update rule is represented by a database query, which queries the updatable relations as well as database relaions, i.e., those relations which are not update by rules. We describe an algorithm to process the queries and compute a fixpoint in the DBMS environment and obtain a final database.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1997526587",
    "type": "article"
  },
  {
    "title": "Simplification rules and complete axiomatization for relational update transactions",
    "doi": "https://doi.org/10.1145/111197.111208",
    "publication_date": "1991-09-01",
    "publication_year": 1991,
    "authors": "Dino Karabeg; Victor Vianu",
    "corresponding_authors": "",
    "abstract": "Relational update transactions consisting of line programs of inserts, deletes, and modifications are studied with respect to equivalence and simplification. A sound and complete set of axioms for proving transaction equivalence is exhibited. The axioms yield a set of simplification rules that can be used to optimize efficiently a large class of transactions of practical interest. The simplification rules are particularly well suited to a dynamic environment where transactions are presented in an on-line fashion, and where the time available for optimization may consist of arbitrarily short and sparse intervals.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2054514167",
    "type": "article"
  },
  {
    "title": "Functional dependencies in Horn clause queries",
    "doi": "https://doi.org/10.1145/103140.103142",
    "publication_date": "1991-03-01",
    "publication_year": 1991,
    "authors": "Alberto O. Mendelzon; Peter T. Wood",
    "corresponding_authors": "",
    "abstract": "When a database query is expressed as a set of Horn clauses whose execution is by top-down resolution of goals, there is a need to improve the backtracking behavior of the interpreter. Rather than putting on the programmer the onus of using extra-logical operators such as cut to improve performance, we show that some uses of the cut can be automated by inferring them from functional dependencies. This requires some knowledge of which variables are guaranteed to be bound at query execution time; we give a method for deriving such information using data flow analysis.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2063538282",
    "type": "article"
  },
  {
    "title": "Intelligent database caching through the use of page-answers and page-traces",
    "doi": "https://doi.org/10.1145/146931.146933",
    "publication_date": "1992-12-01",
    "publication_year": 1992,
    "authors": "Nabil Kamel; Roger King",
    "corresponding_authors": "",
    "abstract": "In this paper a new method to improve the utilization of main memory systems is presented. The new method is based on prestoring in main memory a number of query answers, each evaluated out of a single memory page. To this end, the ideas of page-answers and page-traces are formally described and their properties analyzed. The query model used here allows for selection, projection, join, recursive queries as well as arbitrary combinations. We also show how to apply the approach under update traffic. This concept is especially useful in managing the main memories of an important class of applications. This class includes the evaluation of triggers and alerters, performance improvement of rule-based systems, integrity constraint checking, and materialized views. These applications are characterized by the existence at compile time of a predetermined set of queries, by a slow but persistent update traffic, and by their need to repetitively reevaluate the query set. The new approach represents a new type of intelligent database caching, which contrasts with traditional caching primarily in that the cache elements are derived data and as a consequence, they overlap arbitrarily and do not have a fixed length. The contents of the main memory cache are selected based on the data distribution within the database, the set of fixed queries to preprocess, and the paging characteristics. Page-answers and page-traces are used as the smallest indivisible units in the cache. An efficient heuristic to select a near optimal set of page-answers and page-traces to populate the main memory has been developed, implemented, and tested. Finally, quantitative measurements of performance benefits are reported.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2085965844",
    "type": "article"
  },
  {
    "title": "Variable-depth trie index optimization: theory and experimental results",
    "doi": "https://doi.org/10.1145/62032.77249",
    "publication_date": "1989-03-01",
    "publication_year": 1989,
    "authors": "R. Ramesh; A.J.G. Babu; J. Peter Kincaid",
    "corresponding_authors": "",
    "abstract": "We develop an efficient approach to Trie index optimization. A Trie is a data structure used to index a file having a set of attributes as record identifiers. In the proposed methodology, a file is horizontally partitioned into subsets of records using a Trie index whose depth of indexing is allowed to vary. The retrieval of a record from the file proceeds by “stepping through” the index to identify a subset of records in the file in which a binary search is performed. This paper develops a taxonomy of optimization problems underlying variable-depth Trie index construction. All these problems are solvable in polynomial time, and their characteristics are studied. Exact algorithms and heuristics for their solution are presented. The algorithms are employed in CRES-an expert system for editing written narrative material, developed for the Department of the Navy. CRES uses several large-to-very-large dictionary files for which Trie indexes are constructed using these algorithms. Computational experience with CRES shows that search and retrieval using variable-depth Trie indexes can be as much as six times faster than pure binary search. The space requirements of the Tries are reasonable. The results show that the variable-depth Tries constructed according to the proposed algorithms are viable and efficient for indexing large-to-very-large files by attributes in practical applications.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2054350481",
    "type": "article"
  },
  {
    "title": "The average time until bucket overflow",
    "doi": "https://doi.org/10.1145/1270.318575",
    "publication_date": "1984-09-01",
    "publication_year": 1984,
    "authors": "Robert B. Cooper; Martin K. Solomon",
    "corresponding_authors": "",
    "abstract": "It is common for file structures to be divided into equal-length partitions, called buckets, into which records arrive for insertion and from which records are physically deleted. We give a simple algorithm which permits calculation of the average time until overflow for a bucket of capacity n records, assuming that record insertions and deletions can be modeled as a stochastic process in the usual manner of queueing theory. We present some numerical examples, from which we make some general observations about the relationships among insertion and deletion rates, bucket capacity, initial fill, and average time until overflow. In particular, we observe that it makes sense to define the stable point as the product of the arrival rate and the average residence time of the records; then a bucket tends to fill up to its stable point quickly, in an amount of time almost independent of the stable point, but the average time until overflow increases rapidly with the difference between the bucket capacity and the stable point.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2072181386",
    "type": "article"
  },
  {
    "title": "A parallel algorithm for record clustering",
    "doi": "https://doi.org/10.1145/99935.99947",
    "publication_date": "1990-12-01",
    "publication_year": 1990,
    "authors": "Edward Omiecinski; Peter Scheuermann",
    "corresponding_authors": "",
    "abstract": "We present an efficient heuristic algorithm for record clustering that can run on a SIMD machine. We introduce the P-tree, and its associated numbering scheme, which in the split phase allows each processor independently to compute the unique cluster number of a record satisfying an arbitrary query. We show that by restricting ourselves in the merge phase to combining only sibling clusters, we obtain a parallel algorithm whose speedup ratio is optimal in the number of processors used. Finally, we report on experiments showing that our method produces substantial savings in an enviornment with relatively little overlap among the queries.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2082929085",
    "type": "article"
  },
  {
    "title": "Associative hardware and software techniques for integrity control",
    "doi": "https://doi.org/10.1145/319587.319594",
    "publication_date": "1981-09-01",
    "publication_year": 1981,
    "authors": "Y. C. Hong; Stanley Y. W. Su",
    "corresponding_authors": "",
    "abstract": "This paper presents the integrity control mechanism of the associative processing system, CASSM. The mechanism takes advantage of the associative techniques, such as content and context addressing, tagging and marking data, parallel processing, automatic triggering of integrity control procedures, etc., for integrity control and as a result offers three significant advantages: (1) The problem of staging data in a main memory for integrity checking can be eliminated because database storage operations are verified at the place where the data are stored. (2) The backout or merging procedures are relatively easy and inexpensive in the associative system because modified copies can be substituted for the originals or may be discarded by merely changing their associated tags. (3) The database management system software is simplified because database integrity functions are handled by the associative processing system to which a mainframe computer is a front-end computer.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1983715982",
    "type": "article"
  },
  {
    "title": "Analysis of new variants of coalesced hashing",
    "doi": "https://doi.org/10.1145/1994.2205",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "Wen-Chin Chen; Jeffrey Scott Vitter",
    "corresponding_authors": "",
    "abstract": "The coalesced hashing method has been shown to be very fast for dynamic information storage and retrieval. This paper analyzes in a uniform way the performance of coalesced hashing and its variants, thus settling some open questions in the literature. In all the variants, the range of the hash function is called the address region , and extra space reserved for storing colliders is called the cellar . We refer to the unmodified method, which was analyzed previously, as late-insertion coalesced hashing. In this paper we analyze late insertion and two new variations called early insertion and varied insertion . When there is no cellar, the early-insertion method is better than late insertion; however, past experience has indicated that it might be worse when there is a cellar. Our analysis confirms that it is worse. The varied-insertion method was introduced as a means of combining the advantages of late insertion and early insertion. This paper shows that varied insertion requires fewer probes per search, on the average, than do the other variants. Each of these three coalesced hashing methods has a parameter that relates the sizes of the address region and the cellar. Techniques in this paper are designed for tuning the parameter in order to achieve optimum search times. We conclude with a list of open problems.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2017955951",
    "type": "article"
  },
  {
    "title": "Rosolving conflicts in global storage design through replication",
    "doi": "https://doi.org/10.1145/319830.319836",
    "publication_date": "1983-03-01",
    "publication_year": 1983,
    "authors": "Randy H. Katz; Eugene Wong",
    "corresponding_authors": "",
    "abstract": "We present a conceptual framework in which a database's intra- and interrecord set access requirements are specified as a constrained assignment of abstract characteristics (“evaluated,” “indexed,” “clustered,” “well-placed”) to logical access paths. We derive a physical schema by choosing an available storage structure that most closely provides the desired access characteristics. We use explicit replication of schema objects to reduce the access cost along certain paths, and analyze the trade-offs between increased update overhead and improved retrieval access. Finally, we given an algorithm to select storage structures for a CODASYL 78 DBTG schema, given its access requirements specification.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2050894976",
    "type": "article"
  },
  {
    "title": "Querying web metadata",
    "doi": "https://doi.org/10.1145/1042046.1042047",
    "publication_date": "2004-12-12",
    "publication_year": 2004,
    "authors": "Gültekin Özsoyoğlu; İsmail Sengör Altıngövde; Abdullah Al-Hamdani; Selma Ayşe Özel; Özgür Ulusoy; Z. Meral Özsoyoğlu",
    "corresponding_authors": "",
    "abstract": "In this article, we discuss the issues involved in adding a native score management system to object-relational databases, to be used in querying Web metadata (that describes the semantic content of Web resources). The Web metadata model is based on topics (representing entities), relationships among topics (called metalinks ), and importance scores (sideway values) of topics and metalinks. We extend database relations with scoring functions and importance scores. We add to SQL score-management clauses with well-defined semantics, and propose the sideway-value algebra (SVA), to evaluate the extended SQL queries. SQL extensions and the SVA algebra are illustrated through two Web resources, namely, the DBLP Bibliography and the SIGMOD Anthology.SQL extensions include clauses for propagating input tuple importance scores to output tuples during query processing, clauses that specify query stopping conditions, threshold predicates (a type of approximate similarity predicates for text comparisons), and user-defined-function-based predicates. The propagated importance scores are then used to rank and return a small number of output tuples. The query stopping conditions are propagated to SVA operators during query processing. We show that our SQL extensions are well-defined, meaning that, given a database and a query Q, under any query processing scheme, the output tuples of Q and their importance scores stay the same.To process the SQL extensions, we discuss two sideway value algebra operators, namely, sideway value algebra join and topic closure, give their implementation algorithms, and report their experimental evaluations.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2084075868",
    "type": "article"
  },
  {
    "title": "I/O efficient algorithms for serial and parallel suffix tree construction",
    "doi": "https://doi.org/10.1145/1862919.1862922",
    "publication_date": "2010-10-12",
    "publication_year": 2010,
    "authors": "Amol Ghoting; Konstantin Makarychev",
    "corresponding_authors": "",
    "abstract": "Over the past three decades, the suffix tree has served as a fundamental data structure in string processing. However, its widespread applicability has been hindered due to the fact that suffix tree construction does not scale well with the size of the input string. With advances in data collection and storage technologies, large strings have become ubiquitous, especially across emerging applications involving text, time series, and biological sequence data. To benefit from these advances, it is imperative that we have a scalable suffix tree construction algorithm. The past few years have seen the emergence of several disk-based suffix tree construction algorithms. However, construction times continue to be daunting—for example, indexing the entire human genome still takes over 30 hours on a system with 2 gigabytes of physical memory. In this article, we will empirically demonstrate and argue that all existing suffix tree construction algorithms have a severe limitation—to glean reasonable disk I/O efficiency, the input string being indexed must fit in main memory. This limitation is attributed to the poor locality exhibited by existing suffix tree construction algorithms and inhibits both sequential and parallel scalability. To deal with this limitation, we will show that through careful algorithm design, one of the simplest suffix tree construction algorithms can be rearchitected to build a suffix tree in a tiled manner, allowing the execution to operate within a fixed main memory budget when indexing strings of any size. We will also present a parallel extension of our algorithm that is designed for massively parallel systems like the IBM Blue Gene. An experimental evaluation will show that the proposed approach affords an improvement of several orders of magnitude in serial performance when indexing large strings. Furthermore, the proposed parallel extension is shown to be scalable—it is now possible to index the entire human genome on a 1024 processor IBM Blue Gene system in under 15 minutes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2044891228",
    "type": "article"
  },
  {
    "title": "Consistent thinning of large geographical data for map visualization",
    "doi": "https://doi.org/10.1145/2539032.2539034",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Anish Das Sarma; Hongrae Lee; Héctor González; Jayant Madhavan; Alon Halevy",
    "corresponding_authors": "",
    "abstract": "Large-scale map visualization systems play an increasingly important role in presenting geographic datasets to end-users. Since these datasets can be extremely large, a map rendering system often needs to select a small fraction of the data to visualize them in a limited space. This article addresses the fundamental challenge of thinning : determining appropriate samples of data to be shown on specific geographical regions and zoom levels. Other than the sheer scale of the data, the thinning problem is challenging because of a number of other reasons: (1) data can consist of complex geographical shapes, (2) rendering of data needs to satisfy certain constraints, such as data being preserved across zoom levels and adjacent regions, and (3) after satisfying the constraints, an optimal solution needs to be chosen based on objectives such as maximality , fairness , and importance of data. This article formally defines and presents a complete solution to the thinning problem. First, we express the problem as an integer programming formulation that efficiently solves thinning for desired objectives. Second, we present more efficient solutions for maximality, based on DFS traversal of a spatial tree. Third, we consider the common special case of point datasets, and present an even more efficient randomized algorithm. Fourth, we show that contiguous regions are tractable for a general version of maximality for which arbitrary regions are intractable. Fifth, we examine the structure of our integer programming formulation and show that for point datasets, our program is integral. Finally, we have implemented all techniques from this article in Google Maps [Google 2005] visualizations of fusion tables [Gonzalez et al. 2010], and we describe a set of experiments that demonstrate the trade-offs among the algorithms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2018155822",
    "type": "article"
  },
  {
    "title": "Validating XML documents in the streaming model with external memory",
    "doi": "https://doi.org/10.1145/2504590",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Christian Konrad; Frédéric Magniez",
    "corresponding_authors": "",
    "abstract": "We study the problem of validating XML documents of size N against general DTDs in the context of streaming algorithms. The starting point of this work is a well-known space lower bound. There are XML documents and DTDs for which p -pass streaming algorithms require Ω( N / p ) space. We show that when allowing access to external memory, there is a deterministic streaming algorithm that solves this problem with memory space O(log 2 N ), a constant number of auxiliary read/write streams, and O(log N ) total number of passes on the XML document and auxiliary streams. An important intermediate step of this algorithm is the computation of the First-Child-Next-Sibling (FCNS) encoding of the initial XML document in a streaming fashion. We study this problem independently, and we also provide memory-efficient streaming algorithms for decoding an XML document given in its FCNS encoding. Furthermore, validating XML documents encoding binary trees against any DTD in the usual streaming model without external memory can be done with sublinear memory. There is a one-pass algorithm using O(√ N log N ) space, and a bidirectional two-pass algorithm using O(log 2 N ) space which perform this task.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2157661096",
    "type": "article"
  },
  {
    "title": "Dynamic Complexity under Definable Changes",
    "doi": "https://doi.org/10.1145/3241040",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Thomas Schwentick; Nils Vortmeier; Thomas Zeume",
    "corresponding_authors": "",
    "abstract": "In the setting of dynamic complexity, the goal of a dynamic program is to maintain the result of a fixed query for an input database that is subject to changes, possibly using additional auxiliary relations. In other words, a dynamic program updates a materialized view whenever a base relation is changed. The update of query result and auxiliary relations is specified using first-order logic or, equivalently, relational algebra. The original framework by Patnaik and Immerman only considers changes to the database that insert or delete single tuples. This article extends the setting to definable changes , also specified by first-order queries on the database, and generalizes previous maintenance results to these more expressive change operations. More specifically, it is shown that the undirected reachability query is first-order maintainable under single-tuple changes and first-order defined insertions, likewise the directed reachability query for directed acyclic graphs is first-order maintainable under insertions defined by quantifier-free first-order queries. These results rely on bounded bridge properties , which basically say that, after an insertion of a defined set of edges, for each connected pair of nodes there is some path with a bounded number of new edges. While this bound can be huge, in general, it is shown to be small for insertion queries defined by unions of conjunctive queries. To illustrate that the results for this restricted setting could be practically relevant, they are complemented by an experimental study that compares the performance of dynamic programs with complex changes, dynamic programs with single changes, and with recomputation from scratch. The positive results are complemented by several inexpressibility results. For example, it is shown that—unlike for single-tuple insertions—dynamic programs that maintain the reachability query under definable, quantifier-free changes strictly need update formulas with quantifiers. Finally, further positive results unrelated to reachability are presented: it is shown that for changes definable by parameter-free first-order formulas, all LOGSPACE-definable (and even AC 1 -definable) queries can be maintained by first-order dynamic programs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2578121999",
    "type": "article"
  },
  {
    "title": "Lightweight Monitoring of Distributed Streams",
    "doi": "https://doi.org/10.1145/3226113",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Arnon Lazerson; Daniel Keren; Assaf Schuster",
    "corresponding_authors": "",
    "abstract": "As data becomes dynamic, large, and distributed, there is increasing demand for what have become known as distributed stream algorithms . Since continuously collecting the data to a central server and processing it there is infeasible, a common approach is to define local conditions at the distributed nodes, such that—as long as they are maintained—some desirable global condition holds. Previous methods derived local conditions focusing on communication efficiency. While proving very useful for reducing the communication volume, these local conditions often suffer from heavy computational burden at the nodes. The computational complexity of the local conditions affects both the runtime and the energy consumption. These are especially critical for resource-limited devices like smartphones and sensor nodes. Such devices are becoming more ubiquitous due to the recent trend toward smart cities and the Internet of Things. To accommodate for high data rates and limited resources of these devices, it is crucial that the local conditions be quickly and efficiently evaluated. Here we propose a novel approach, designated CB (for Convex/Concave Bounds). CB defines local conditions using suitably chosen convex and concave functions. Lightweight and simple, these local conditions can be rapidly checked on the fly. CB’s superiority over the state-of-the-art is demonstrated in its reduced runtime and power consumption, by up to six orders of magnitude in some cases. As an added bonus, CB also reduced communication overhead in all the tested application scenarios.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2886448534",
    "type": "article"
  },
  {
    "title": "Estimating the Impact of Unknown Unknowns on Aggregate Query Results",
    "doi": "https://doi.org/10.1145/3167970",
    "publication_date": "2018-03-06",
    "publication_year": 2018,
    "authors": "Yeounoh Chung; Michael Lind Mortensen; Carsten Binnig; Tim Kraska",
    "corresponding_authors": "",
    "abstract": "It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) Is the integrated data set complete? and (2) What is the impact of any unknown (i.e., unobserved) data on query results? In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns ) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution; we also propose a parametric model that can be used instead when the data sources are imbalanced. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2953288349",
    "type": "article"
  },
  {
    "title": "Design and Evaluation of an RDMA-aware Data Shuffling Operator for Parallel Database Systems",
    "doi": "https://doi.org/10.1145/3360900",
    "publication_date": "2019-12-12",
    "publication_year": 2019,
    "authors": "Feilong Liu; Lingyan Yin; Spyros Blanas",
    "corresponding_authors": "",
    "abstract": "The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space that includes (1) the number of open connections, (2) the contention for the shared network interface, (3) the RDMA transport function, and (4) how much memory should be reserved to exchange data between nodes during query processing. We contribute eight designs that capture salient tradeoffs in this design space as well as an adaptive algorithm to dynamically manage RDMA-registered memory. We comprehensively evaluate how transport-layer decisions impact the query performance of a database system for different generations of InfiniBand. We find that a shuffling operator that uses the RDMA Send/Receive transport function over the Unreliable Datagram transport service can transmit data up to 4× faster than an RDMA-capable MPI implementation in a 16-node cluster. The response time of TPC-H queries improves by as much as 2×.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3007587379",
    "type": "article"
  },
  {
    "title": "Catching Numeric Inconsistencies in Graphs",
    "doi": "https://doi.org/10.1145/3385031",
    "publication_date": "2020-06-27",
    "publication_year": 2020,
    "authors": "Wenfei Fan; Xueli Liu; Ping Lü; Chao Tian",
    "corresponding_authors": "",
    "abstract": "Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we extend graph functional dependencies with linear arithmetic expressions and built-in comparison predicates, referred to as numeric graph dependencies (NGDs). We study fundamental problems for NGDs. We show that their satisfiability, implication, and validation problems are Σ p 2 -complete, Π p 2 -complete, and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity. To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs in response to updates Δ G to G . We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in Δ G instead of the entire G . Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. In addition, to strike a balance between the efficiency and accuracy, we also develop polynomial-time parallel algorithms for detection and incremental detection of top-ranked inconsistencies. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3038423637",
    "type": "article"
  },
  {
    "title": "Flexible and extensible preference evaluation in database systems",
    "doi": "https://doi.org/10.1145/2508020.2493268",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Justin J. Levandoski; Ahmed Eldawy; Mohamed F. Mokbel; Mohamed E. Khalefa",
    "corresponding_authors": "",
    "abstract": "Personalized database systems give users answers tailored to their personal preferences. While numerous preference evaluation methods for databases have been proposed (e.g., skyline, top-k, k-dominance, k-frequency), the implementation of these methods at the core of a database system is a double-edged sword. Core implementation provides efficient query processing for arbitrary database queries, however, this approach is not practical since each existing (and future) preference method requires implementation within the database engine. To solve this problem, this article introduces FlexPref, a framework for extensible preference evaluation in database systems. FlexPref, implemented in the query processor, aims to support a wide array of preference evaluation methods in a single extensible code base. Integration with FlexPref is simple, involving the registration of only three functions that capture the essence of the preference method. Once integrated, the preference method “lives” at the core of the database, enabling the efficient execution of preference queries involving common database operations. This article also provides a query optimization framework for FlexPref, as well as a theoretical framework that defines the properties a preference method must exhibit to be implemented in FlexPref. To demonstrate the extensibility of FlexPref, this article also provides case studies detailing the implementation of seven state-of-the-art preference evaluation methods within FlexPref. We also experimentally study the strengths and weaknesses of an implementation of FlexPref in PostgreSQL over a range of single-table and multitable preference queries.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4244008011",
    "type": "article"
  },
  {
    "title": "Discovering XSD Keys from XML Data",
    "doi": "https://doi.org/10.1145/2638547",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Marcelo Arenas; Jonny Daenen; Frank Neven; Martín Ugarte; Jan Van den Bussche; Stijn Vansummeren",
    "corresponding_authors": "",
    "abstract": "A great deal of research into the learning of schemas from XML data has been conducted in recent years to enable the automatic discovery of XML schemas from XML documents when no schema or only a low-quality one is available. Unfortunately, and in strong contrast to, for instance, the relational model, the automatic discovery of even the simplest of XML constraints, namely XML keys, has been left largely unexplored in this context. A major obstacle here is the unavailability of a theory on reasoning about XML keys in the presence of XML schemas, which is needed to validate the quality of candidate keys. The present article embarks on a fundamental study of such a theory and classifies the complexity of several crucial properties concerning XML keys in the presence of an XSD, like, for instance, testing for consistency, boundedness, satisfiability, universality, and equivalence. Of independent interest, novel results are obtained related to cardinality estimation of XPath result sets. A mining algorithm is then developed within the framework of levelwise search. The algorithm leverages known discovery algorithms for functional dependencies in the relational model, but incorporates the properties mentioned before to assess and refine the quality of derived keys. An experimental study on an extensive body of real-world XML data evaluating the effectiveness of the proposed algorithm is provided.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4232743491",
    "type": "article"
  },
  {
    "title": "Open commit protocols tolerating commission failures",
    "doi": "https://doi.org/10.1145/151634.151638",
    "publication_date": "1993-06-01",
    "publication_year": 1993,
    "authors": "Kurt Rothermel; Stefan Pappe",
    "corresponding_authors": "",
    "abstract": "To ensure atomicity of transactions in distributed systems so-called 2-phase commit (2PC) protocols have been proposed. The basic assumption of these protocols is that the processing nodes involved in transactions are “sane,” i.e., they only fail with omission failures, and nodes eventually recover from failures. Unfortunately, this assumption is not realistic for so-called Open Distributed Systems (ODSs), in which nodes may have totally different reliability characteristics. In ODSs, nodes can be classified into trusted nodes (e.g., a banking server) and nontrusted nodes (e.g., a home PC requesting a remote banking service). While trusted nodes are assumed to be sane, nontrusted nodes may fail permanently and even cause commission failures to occur. In this paper, we propose a family of 2PC protocols that tolerate any number of omission failures at trusted nodes and any number of commission and omission failures at nontrusted nodes. The proposed protocols ensure that (at least) the trusted nodes participating in a transaction eventually terminate the transaction in a consistent manner. Unlike Byzantine commit protocols, our protocols do not incorporate mechanisms for achieving Byzantine agreement, which has advantages in terms of complexity: Our protocols have the same or only a slightly higher message complexity than traditional 2PC protocols.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1994749301",
    "type": "article"
  },
  {
    "title": "An efficient method for checking object-oriented database schema correctness",
    "doi": "https://doi.org/10.1145/293910.293152",
    "publication_date": "1998-09-01",
    "publication_year": 1998,
    "authors": "Anna Formica; Hans Dietmar Gröger; Michele Missikoff",
    "corresponding_authors": "",
    "abstract": "Inheritance is introducted in object-oriented systems to enhance code reuse and create more compact and readable software. Powerful object models adopt multiple inheritance, allowing a type (or class) definition to inherit from more than one supertype. Unfortunately, in applying this powerful modeling mechanism, inheritance conflicts may be generated, which arise when the same property or operation is defined in more than one supertype. Inheritance conflicts identification and resolution is the key issue of this article. In strongly typed object-oriented systems the resolutioin of inheritance conflicts depends on the compatibility of the types of the conflicting definitions. In case of incompatible types, a contradiction arises. This article focuses on object-oriented databases (ODBs), providing a method aimed at supporting the designer in the construction of correct ODB schemas. The first necessary condition for schema correctness is the absence of contradictions. A second cause of schema incorrectness is due to the presence of structurally recursive types that, when defined within certain hierarchical patterns, cause the nontermination of the inheritance process. In the article, after the formal definition of a correct schema, two graph-theoretic methods aimed at verifying ODB schema correctness are analyzed. Although the first method is intuitive but inefficient, the second allows schema correctness to be checked in polynomial time, in the size of the schema. The results of this study are included in the implementation of Mosaico, an environment for ODB application design.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2047993130",
    "type": "article"
  },
  {
    "title": "The complexity of operations on a fragmented relation",
    "doi": "https://doi.org/10.1145/103140.103143",
    "publication_date": "1991-03-01",
    "publication_year": 1991,
    "authors": "Carlo Meghini; C. Thanos",
    "corresponding_authors": "",
    "abstract": "Data fragmentation is an important aspect of distributed database design, in which portions of relations, tailored to the specific needs of local applications, are defined to be further allocated to the sites of the computer network supporting the database system. In this paper we present a theory of fragmentation with overlapping fragments to study the complexity of the problems involved in checking the completeness of a fragmentation schema and in querying and updating a fragmented relation. We analyze these problems from the complexity viewpoint and present sound and complete algorithms for their solution.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2050464335",
    "type": "article"
  },
  {
    "title": "Applying an update method to a set of receivers",
    "doi": "https://doi.org/10.1145/383734.383735",
    "publication_date": "2001-03-01",
    "publication_year": 2001,
    "authors": "Marc Andries; Luca Cabibbo; Jan Paredaens; Jan Van den Bussche",
    "corresponding_authors": "",
    "abstract": "In the context of object databases, we study the application of an update method to a collection of receivers rather than to a single one. The obvious strategy of applying the update to the receivers one after the other, in some arbitrary order, brings up the problem of order independence. On a very general level, we investigate how update behavior can be analyzed in terms of certain schema annotations, called colorings. We are able to characterize those colorings that always describe order-independedent updates. We also consider a more specific model of update methods implemented in the relational algebra. Order-independence of such algebraic methods is undecidable in general, but decidable if the expressions used are positive. Finally, we consider an alternative parallel strategy for set-oriented applications of algebraic update methods and compare and relate it to the sequential strategy.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1987984611",
    "type": "article"
  },
  {
    "title": "Functions in databases",
    "doi": "https://doi.org/10.1145/319830.319835",
    "publication_date": "1983-03-01",
    "publication_year": 1983,
    "authors": "Marc H. Graham",
    "corresponding_authors": "Marc H. Graham",
    "abstract": "We discuss the objectives of including functional dependencies in the definition of a relational database. We find two distinct objectives. The appearance of a dependency in the definition of a database indicates that the states of the database are to encode a function. A method based on the chase of calculating the function encoded by a particular state is given and compared to methods utilizing derivations of the dependency. A test for deciding whether the states of a schema may encode a nonempty function is presented as is a characterization of the class of schemas which are capable of encoding nonempty functions for all the dependencies in the definition. This class is the class of dependency preserving schemas as defined by Beeri et al. and is strictly larger than the class presented by Bernstein. The second objective of including a functional dependency in the definition of a database is that the dependency be capable of constraining the states of the database; that is, capable of uncovering input errors made by the users. We show that this capability is weaker than the first objective; thus, even dependencies whose functions are everywhere empty may still act as constraints. Bounds on the requirements for a dependency to act as a constraint are derived. These results are founded on the notion of a weak instance for a database state, which replaces the universal relation instance assumption and is both intuitively and computationally more nearly acceptable.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2011239742",
    "type": "article"
  },
  {
    "title": "Propeties of storage hierarchy systems with multiple page sizes and redundant data",
    "doi": "https://doi.org/10.1145/320083.320095",
    "publication_date": "1979-09-01",
    "publication_year": 1979,
    "authors": "C.W. Lam; Stuart Madnick",
    "corresponding_authors": "",
    "abstract": "The need for high performance, highly reliable storage for very large on-line databases, coupled with rapid advances in storage device technology, has made the study of generalized storage hierarchies an important area of research. This paper analyzes properties of a data storage hierarchy system specifically designed for handling very large on-line databases. To attain high performance and high reliability, the data storage hierarchy makes use of multiple page sizes in different storage levels and maintains multiple copies of the same information across the storage levels. Such a storage hierarchy system is currently being designed as part of the INFOPLEX database computer project. Previous studies of storage hierarchies have primarily focused on virtual memories for program storage and hierarchies with a single page size across all storage levels and/or a single copy of information in the hierarchy. In the INFOPLEX design, extensions to the least recently used (LRU) algorithm are used to manage the storage levels. The read-through technique is used to initially load a referenced page of the appropriate size into all storage levels above the one in which the page is found. Since each storage level is viewed as an extension of the immediate higher level, an overflow page from level i is always placed in level i + 1. Important properties of these algorithms are derived. It is shown that depending on the types of algorithms used and the relative sizes of the storage levels, it is not always possible to guarantee that the contents of a given storage level i is always a superset of the contents of its immediate higher storage level i - 1. The necessary and sufficient conditions for this property to hold are identified and proved. Furthermore, it is possible that increasing the size of intermediate storage levels may actually increase the number of references to lower storage levels, resulting in reduced performance. Conditions necessary to avoid such an anomaly are also identified and proved.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4235214584",
    "type": "article"
  },
  {
    "title": "Construction of relations in relational databases",
    "doi": "https://doi.org/10.1145/320141.320155",
    "publication_date": "1980-06-01",
    "publication_year": 1980,
    "authors": "Eliezer L. Lozinskii",
    "corresponding_authors": "Eliezer L. Lozinskii",
    "abstract": "Using a nonprocedural language for query formulation requires certain automatization of a query answering process. Given a query for creation of a new relation, the problem is to find an efficient procedure which produces this relation from a given relational database. We concentrate upon sequences of join operations which losslessly produce a relation required by a query. A new property of such sequences is analyzed which provides a basis for the presented algorithms that construct an efficient join procedure. The algorithms have polynomial complexity. A modified AND/OR graph is used for the display of a given set of dependencies and a collection of relations representing a database.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2058667541",
    "type": "article"
  },
  {
    "title": "Summarizing level-two topological relations in large spatial datasets",
    "doi": "https://doi.org/10.1145/1138394.1138398",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Xuemin Lin; Qing Liu; Yidong Yuan; Xiaofang Zhou; Hongjun Lü",
    "corresponding_authors": "",
    "abstract": "Summarizing topological relations is fundamental to many spatial applications including spatial query optimization. In this article, we present several novel techniques to effectively construct cell density based spatial histograms for range (window) summarizations restricted to the four most important level-two topological relations: contains, contained, overlap, and disjoint. We first present a novel framework to construct a multiscale Euler histogram in 2D space with the guarantee of the exact summarization results for aligned windows in constant time. To minimize the storage space in such a multiscale Euler histogram, an approximate algorithm with the approximate ratio 19/12 is presented, while the problem is shown NP-hard generally. To conform to a limited storage space where a multiscale histogram may be allowed to have only k Euler histograms, an effective algorithm is presented to construct multiscale histograms to achieve high accuracy in approximately summarizing aligned windows. Then, we present a new approximate algorithm to query an Euler histogram that cannot guarantee the exact answers; it runs in constant time. We also investigate the problem of nonaligned windows and the problem of effectively partitioning the data space to support nonaligned window queries. Finally, we extend our techniques to 3D space. Our extensive experiments against both synthetic and real world datasets demonstrate that the approximate multiscale histogram techniques may improve the accuracy of the existing techniques by several orders of magnitude while retaining the cost efficiency, and the exact multiscale histogram technique requires only a storage space linearly proportional to the number of cells for many popular real datasets.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2046584324",
    "type": "article"
  },
  {
    "title": "Bag Query Containment and Information Theory",
    "doi": "https://doi.org/10.1145/3472391",
    "publication_date": "2021-09-28",
    "publication_year": 2021,
    "authors": "Mahmoud Abo Khamis; Phokion G. Kolaitis; Hung Q. Ngo; Dan Suciu",
    "corresponding_authors": "",
    "abstract": "The query containment problem is a fundamental algorithmic problem in data management. While this problem is well understood under set semantics, it is by far less understood under bag semantics. In particular, it is a long-standing open question whether or not the conjunctive query containment problem under bag semantics is decidable. We unveil tight connections between information theory and the conjunctive query containment under bag semantics. These connections are established using information inequalities, which are considered to be the laws of information theory. Our first main result asserts that deciding the validity of a generalization of information inequalities is many-one equivalent to the restricted case of conjunctive query containment in which the containing query is acyclic; thus, either both these problems are decidable or both are undecidable. Our second main result identifies a new decidable case of the conjunctive query containment problem under bag semantics. Specifically, we give an exponential-time algorithm for conjunctive query containment under bag semantics, provided the containing query is chordal and admits a simple junction tree.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3202852140",
    "type": "article"
  },
  {
    "title": "Understanding cardinality estimation using entropy maximization",
    "doi": "https://doi.org/10.1145/2109196.2109202",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Christopher Ré; Dan Suciu",
    "corresponding_authors": "",
    "abstract": "Cardinality estimation is the problem of estimating the number of tuples returned by a query; it is a fundamentally important task in data management, used in query optimization, progress estimation, and resource provisioning. We study cardinality estimation in a principled framework: given a set of statistical assertions about the number of tuples returned by a fixed set of queries, predict the number of tuples returned by a new query. We model this problem using the probability space, over possible worlds, that satisfies all provided statistical assertions and maximizes entropy. We call this the Entropy Maximization model for statistics (MaxEnt). In this article we develop the mathematical techniques needed to use the MaxEnt model for predicting the cardinality of conjunctive queries.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2021993549",
    "type": "article"
  },
  {
    "title": "Querying XML data sources that export very large sets of views",
    "doi": "https://doi.org/10.1145/1929934.1929939",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "Bogdan Cautis; Alin Deutsch; Nicola Onose; Vasilis Vassalos",
    "corresponding_authors": "",
    "abstract": "We study the problem of querying XML data sources that accept only a limited set of queries, such as sources accessible by Web services which can implement very large (potentially infinite) families of XPath queries. To compactly specify such families of queries we adopt the Query Set Specifications, a formalism close to context-free grammars. We say that query Q is expressible by the specification P if it is equivalent to some expansion of P. Q is supported by P if it has an equivalent rewriting using some finite set of P's expansions. We study the complexity of expressibility and support and identify large classes of XPath queries for which there are efficient (PTIME) algorithms. Our study considers both the case in which the XML nodes in the results of the queries lose their original identity and the one in which the source exposes persistent node ids.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2024805781",
    "type": "article"
  },
  {
    "title": "The Complexity of Learning Tree Patterns from Example Graphs",
    "doi": "https://doi.org/10.1145/2890492",
    "publication_date": "2016-05-11",
    "publication_year": 2016,
    "authors": "Sara Cohen; Yaacov Y. Weiss",
    "corresponding_authors": "",
    "abstract": "This article investigates the problem of learning tree patterns that return nodes with a given set of labels, from example graphs provided by the user. Example graphs are annotated by the user as being either positive or negative . The goal is then to determine whether there exists a tree pattern returning tuples of nodes with the given labels in each of the positive examples, but in none of the negative examples, and furthermore, to find one such pattern if it exists. These are called the satisfiability and learning problems, respectively. This article thoroughly investigates the satisfiability and learning problems in a variety of settings. In particular, we consider example sets that (1) may contain only positive examples, or both positive and negative examples, (2) may contain directed or undirected graphs, and (3) may have multiple occurrences of labels or be uniquely labeled (to some degree). In addition, we consider tree patterns of different types that can allow, or prohibit, wildcard labeled nodes and descendant edges. We also consider two different semantics for mapping tree patterns to graphs. The complexity of satisfiability is determined for the different combinations of settings. For cases in which satisfiability is polynomial, it is also shown that learning is polynomial. (This is nontrivial as satisfying patterns may be exponential in size.) Finally, the minimal learning problem, that is, that of finding a minimal-sized satisfying pattern, is studied for cases in which satisfiability is polynomial.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2380715658",
    "type": "article"
  },
  {
    "title": "Skycube Materialization Using the Topmost Skyline or Functional Dependencies",
    "doi": "https://doi.org/10.1145/2955092",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Sofian Maabout; Carlos Ordońẽz; Patrick Kamnang Wanko; Nicolas Hanusse",
    "corresponding_authors": "",
    "abstract": "Given a table T ( Id , D 1 , …, D d ), the skycube of T is the set of skylines with respect to to all nonempty subsets (subspaces) of the set of all dimensions { D 1 , …, D d }. To optimize the evaluation of any skyline query, the solutions proposed so far in the literature either (i) precompute all of the skylines or (ii) use compression techniques so that the derivation of any skyline can be done with little effort. Even though solutions (i) are appealing because skyline queries have optimal execution time, they suffer from time and space scalability because the number of skylines to be materialized is exponential with respect to d . On the other hand, solutions (ii) are attractive in terms of memory consumption, but as we show, they also have a high time complexity. In this article, we make contributions to both kinds of solutions. We first observe that skyline patterns are monotonic. This property leads to a simple yet efficient solution for full and partial skycube materialization when the skyline with respect to all dimensions, the topmost skyline, is small. On the other hand, when the topmost skyline is large relative to the size of the input table, it turns out that functional dependencies, a fundamental concept in databases, uncover a monotonic property between skylines. Equipped with this information, we show that closed attributes sets are fundamental for partial and full skycube materialization. Extensive experiments with real and synthetic datasets show that our solutions generally outperform state-of-the-art algorithms.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2548069050",
    "type": "article"
  },
  {
    "title": "The Space-Efficient Core of Vadalog",
    "doi": "https://doi.org/10.1145/3488720",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Gérald Berger; Georg Gottlob; Andréas Pieris; Emanuel Sallinger",
    "corresponding_authors": "",
    "abstract": "Vadalog is a system for performing complex reasoning tasks such as those required in advanced knowledge graphs. The logical core of the underlying Vadalog language is the warded fragment of tuple-generating dependencies (TGDs). This formalism ensures tractable reasoning in data complexity, while a recent analysis focusing on a practical implementation led to the reasoning algorithm around which the Vadalog system is built. A fundamental question that has emerged in the context of Vadalog is whether we can limit the recursion allowed by wardedness in order to obtain a formalism that provides a convenient syntax for expressing useful recursive statements, and at the same time achieves space-efficiency. After analyzing several real-life examples of warded sets of TGDs provided by our industrial partners, as well as recent benchmarks, we observed that recursion is often used in a restricted way: the body of a TGD contains at most one atom whose predicate is mutually recursive with a predicate in the head. We show that this type of recursion, known as piece-wise linear in the Datalog literature, is the answer to our main question. We further show that piece-wise linear recursion alone, without the wardedness condition, is not enough as it leads to undecidability. We also study the relative expressiveness of the query languages based on (piece-wise linear) warded sets of TGDs. Finally, we give preliminary experimental evidence for the practical effect of piece-wise linearity on Vadalog.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2890954631",
    "type": "article"
  },
  {
    "title": "Efficiently Cleaning Structured Event Logs: A Graph Repair Approach",
    "doi": "https://doi.org/10.1145/3571281",
    "publication_date": "2022-11-17",
    "publication_year": 2022,
    "authors": "Ruihong Huang; Jianmin Wang; Shaoxu Song; Xuemin Lin; Xiaochen Zhu; Jian Pei",
    "corresponding_authors": "",
    "abstract": "Event data are often dirty owing to various recording conventions or simply system errors. These errors may cause serious damage to real applications, such as inaccurate provenance answers, poor profiling results, or concealing interesting patterns from event data. Cleaning dirty event data is strongly demanded. While existing event data cleaning techniques view event logs as sequences, structural information does exist among events, such as the task passing relationships between staffs in workflow or the invocation relationships among different micro-services in monitoring application performance. We argue that such structural information enhances not only the accuracy of repairing inconsistent events but also the computation efficiency. It is notable that both the structure and the names (labeling) of events could be inconsistent. In real applications, while an unsound structure is not repaired automatically (which requires manual effort from business actors to handle the structure error), it is highly desirable to repair the inconsistent event names introduced by recording mistakes. In this article, we first prove that the inconsistent label repairing problem is NP-complete. Then, we propose a graph repair approach for (1) detecting unsound structures, and (2) repairing inconsistent event names. Efficient pruning techniques together with two heuristic solutions are also presented. Extensive experiments over real and synthetic datasets demonstrate both the effectiveness and efficiency of our proposal.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4309617049",
    "type": "article"
  },
  {
    "title": "Ad Hoc Transactions through the Looking Glass: An Empirical Study of Application-Level Transactions in Web Applications",
    "doi": "https://doi.org/10.1145/3638553",
    "publication_date": "2023-12-23",
    "publication_year": 2023,
    "authors": "Zhaoguo Wang; Chuzhe Tang; Xiaodong Zhang; Qianmian Yu; Binyu Zang; Haibing Guan; Haibo Chen",
    "corresponding_authors": "",
    "abstract": "Many transactions in web applications are constructed ad hoc in the application code. For example, developers might explicitly use locking primitives or validation procedures to coordinate critical code fragments. We refer to database operations coordinated by application code as ad hoc transactions . Until now, little is known about them. This paper presents the first comprehensive study on ad hoc transactions. By studying 91 ad hoc transactions among eight popular open-source web applications, we found that (i) every studied application uses ad hoc transactions (up to 16 per application), 71 of which play critical roles; (ii) compared with database transactions, concurrency control of ad hoc transactions is much more flexible; (iii) ad hoc transactions are error-prone—53 of them have correctness issues, and 33 of them are confirmed by developers; and (iv) ad hoc transactions have the potential for improving performance in contentious workloads by utilizing application semantics such as access patterns. Based on these findings, we discuss the implications of ad hoc transactions to the database research community.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390145145",
    "type": "article"
  },
  {
    "title": "Ensuring consistency in multidatabases by preserving two-level serializability",
    "doi": "https://doi.org/10.1145/292481.277629",
    "publication_date": "1998-06-01",
    "publication_year": 1998,
    "authors": "Sharad Mehrotra; Rajeev Rastogi; Henry F. Korth; Abraham Silberschatz",
    "corresponding_authors": "",
    "abstract": "The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency. In this article, we introduce a systematic approach to relaxing the serializability requirement in MDBS environments. Our approach exploits the structure of the integrity constraints and the nature of transaction programs to ensure consistency without requiring executions to be serializable. We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2064505593",
    "type": "article"
  },
  {
    "title": "The <i>five color</i> concurrency control protocol: non-two-phase locking in general databases",
    "doi": "https://doi.org/10.1145/78922.78927",
    "publication_date": "1990-06-01",
    "publication_year": 1990,
    "authors": "Partha Dasgupta; Zvi M. Kedem",
    "corresponding_authors": "",
    "abstract": "Concurrency control protocols based on two-phase locking are a popular family of locking protocols that preserve serializability in general (unstructured) database systems. A concurrency control algorithm (for databases with no inherent structure) is presented that is practical, non two-phase, and allows varieties of serializable logs not possible with any commonly known locking schemes. All transactions are required to predeclare the data they intend to read or write. Using this information, the protocol anticipates the existence (or absence) of possible conflicts and hence can allow non-two-phase locking. It is well known that serializability is characterized by acyclicity of the conflict graph representation of interleaved executions. The two-phase locking protocols allow only forward growth of the paths in the graph. The Five Color protocol allows the conflict graph to grow in any direction (avoiding two-phase constraints) and prevents cycles in the graph by maintaining transaction access information in the form of data-item markers. The read and write set information can also be used to provide relative immunity from deadlocks.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2138769573",
    "type": "article"
  },
  {
    "title": "Height-balanced trees of order (β, γ, δ)",
    "doi": "https://doi.org/10.1145/3857.3858",
    "publication_date": "1985-06-01",
    "publication_year": 1985,
    "authors": "Shou‐Hsuan Stephen Huang",
    "corresponding_authors": "Shou‐Hsuan Stephen Huang",
    "abstract": "We study restricted classes of B-trees, called H(β, γ, δ) trees. A class is defined by three parameters: β, the size of a node; γ, the minimal number of grandsons a node must have; and δ, the minimal number of leaves bottom nodes must have. This generalizes the brother condition of 2-3 brother trees in a uniform way to B-trees of higher order. The class of B-trees of order m is obtained by choosing β = m, γ = (m/2) 2 and δ = m/2. An algorithm to construct H-trees for any given number of keys is given in Section 1. Insertion and deletion algorithms are given in Section 2. The costs of these algorithms increase smoothly as the parameters are increased. Furthermore, it is proved that the insertion can be done in time O(β + log N), where N is the number of nodes in the tree. Deletion can also be accomplished without reconstructing the entire tree. Properties of H-trees are given in Section 3. It is shown that the height of H-trees decreases as γ increases, and the storage utilization increases significantly as δ increases. Finally, comparisons with other restricted classes of B-trees are given in Section 4 to show the attractiveness of H-trees.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2009884588",
    "type": "article"
  },
  {
    "title": "Further results on the security of partitioned dynamic statistical databases",
    "doi": "https://doi.org/10.1145/62032.62036",
    "publication_date": "1989-03-01",
    "publication_year": 1989,
    "authors": "Mary McLeish",
    "corresponding_authors": "Mary McLeish",
    "abstract": "Partitioning is a highly secure approach to protecting statistical databases. When updates are introduced, security depends on putting restrictions on the sizes of partition sets which may be queried. To overcome this problem, attempts have been made to add “dummy” records. Recent work has shown that this leads to high information loss. This paper reconsiders the restrictions on the size of partitioning sets required to achieve a high level of security. Updates of two records at a time were studied earlier, and security was found to hold if the sizes of the partition sets were kept even. In this paper an extended model is presented, allowing very general updates to be performed. The security problem is thoroughly studied, giving if and only if conditions. The earlier result is shown to be part of a corollary to the main theorem of this paper. Alternatives to adding dummy records are presented and the practical implications of the theory for the database manager are discussed.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2134516159",
    "type": "article"
  },
  {
    "title": "Papers from the International Conference on Very Large Data Bases, September 22-24, 1975, Framingham, Massachusetts.",
    "doi": null,
    "publication_date": "1976-01-01",
    "publication_year": 1976,
    "authors": "R. Stockton Gaines; David K. Hsiao",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W210045504",
    "type": "article"
  },
  {
    "title": "Synopses for query optimization",
    "doi": "https://doi.org/10.1145/1114244.1114251",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Raghav Kaushik; Jeffrey F. Naughton; Raghu Ramakrishnan; Venkatesan Chakravarthy",
    "corresponding_authors": "",
    "abstract": "Database systems use precomputed synopses of data to estimate the cost of alternative plans during query optimization. A number of alternative synopsis structures have been proposed, but histograms are by far the most commonly used. While histograms have proved to be very effective in (cost estimation for) single-table selections, queries with joins have long been seen as a challenge; under a model where histograms are maintained for individual tables, a celebrated result of Ioannidis and Christodoulakis [1991] observes that errors propagate exponentially with the number of joins in a query.In this article, we make two main contributions. First, we study the space complexity of using synopses for query optimization from a novel information-theoretic perspective. In particular, we offer evidence in support of histograms for single-table selections, including an analysis over data distributions known to be common in practice, and illustrate their limitations for join queries. Second, for a broad class of common queries involving joins (specifically, all queries involving only key-foreign key joins) we show that the strategy of storing a small precomputed sample of the database yields probabilistic guarantees that are almost space-optimal, which is an important property if these samples are to be used as database statistics. This is the first such optimality result, to our knowledge, and suggests that precomputed samples might be an effective way to circumvent the error propagation problem for queries with key-foreign key joins. We support this result empirically through an experimental study that demonstrates the effectiveness of precomputed samples, and also shows the increasing difference in the effectiveness of samples versus multidimensional histograms as the number of joins in the query grows.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3083806692",
    "type": "article"
  },
  {
    "title": "Input-sensitive scalable continuous join query processing",
    "doi": "https://doi.org/10.1145/1567274.1567275",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Pankaj K. Agarwal; Junyi Xie; Jun Yang; Hai Yu",
    "corresponding_authors": "",
    "abstract": "This article considers the problem of scalably processing a large number of continuous queries. Our approach, consisting of novel data structures and algorithms and a flexible processing framework, advances the state-of-the-art in several ways. First, our approach is query sensitive in the sense that it exploits potential overlaps in query predicates for efficient group processing. We partition the collection of continuous queries into groups based on the clustering patterns of the query predicates, and apply specialized processing strategies to heavily clustered groups (or hotspots ). We show how to maintain the hotspots efficiently, and use them to scalably process continuous select-join, band-join, and window-join queries. Second, our approach is also data sensitive, in the sense that it makes cost-based decisions on how to process each incoming tuple based on its characteristics. Experiments demonstrate that our approach can improve the processing throughput by orders of magnitude.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2060846685",
    "type": "article"
  },
  {
    "title": "A relational approach to functional decomposition of logic circuits",
    "doi": "https://doi.org/10.1145/1966385.1966391",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Tony T. Lee; Tong Ye",
    "corresponding_authors": "",
    "abstract": "Functional decomposition of Boolean functions has a profound influence on all quality aspects of cost-effectively implementing modern digital systems and data-mining. The relational databases are multivalued tables, which include any truth tables of logic functions as special cases. In this article, we propose a relational database approach to the decomposition of logic circuits. The relational algebra consists of a set of well-defined algebraic operations that can be performed on multivalued tables. Our approach shows that the functional decomposition of logic circuits is similar to the normalization of relational databases; they are governed by the same concepts of functional dependency (FD) and multivalued dependency (MVD). The completeness of relational algebra demonstrated by our approach to functional decomposition reveals that the relational database is a fundamental computation model, the same as the Boolean logic circuit.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2049150522",
    "type": "article"
  },
  {
    "title": "Optimizing XML querying using type-based document projection",
    "doi": "https://doi.org/10.1145/2445583.2445587",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Véronique Benzaken; Giuseppe Castagna; Dario Colazzo; Kim Nguy ̃ˆen",
    "corresponding_authors": "",
    "abstract": "XML data projection (or pruning) is a natural optimization for main memory query engines: given a query Q over a document D , the subtrees of D that are not necessary to evaluate Q are pruned, thus producing a smaller document D' ; the query Q is then executed on D' , hence avoiding to allocate and process nodes that will never be reached by Q . In this article, we propose a new approach, based on types, that greatly improves current solutions. Besides providing comparable or greater precision and far lesser pruning overhead, our solution—unlike current approaches—takes into account backward axes, predicates, and can be applied to multiple queries rather than just to single ones. A side contribution is a new type system for XPath able to handle backward axes. The soundness of our approach is formally proved. Furthermore, we prove that the approach is also complete (i.e., yields the best possible type-driven pruning) for a relevant class of queries and Schemas. We further validate our approach using the XMark and XPathMark benchmarks and show that pruning not only improves the main memory query engine's performances (as expected) but also those of state of the art native XML databases.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2080477458",
    "type": "article"
  },
  {
    "title": "Flexible and extensible preference evaluation in database systems",
    "doi": "https://doi.org/10.1145/2493268",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Justin J. Levandoski; Ahmed Eldawy; Mohamed F. Mokbel; Mohamed E. Khalefa",
    "corresponding_authors": "",
    "abstract": "Personalized database systems give users answers tailored to their personal preferences. While numerous preference evaluation methods for databases have been proposed (e.g., skyline, top-k, k-dominance, k-frequency), the implementation of these methods at the core of a database system is a double-edged sword. Core implementation provides efficient query processing for arbitrary database queries, however, this approach is not practical since each existing (and future) preference method requires implementation within the database engine. To solve this problem, this article introduces FlexPref, a framework for extensible preference evaluation in database systems. FlexPref, implemented in the query processor, aims to support a wide array of preference evaluation methods in a single extensible code base. Integration with FlexPref is simple, involving the registration of only three functions that capture the essence of the preference method. Once integrated, the preference method “lives” at the core of the database, enabling the efficient execution of preference queries involving common database operations. This article also provides a query optimization framework for FlexPref, as well as a theoretical framework that defines the properties a preference method must exhibit to be implemented in FlexPref. To demonstrate the extensibility of FlexPref, this article also provides case studies detailing the implementation of seven state-of-the-art preference evaluation methods within FlexPref. We also experimentally study the strengths and weaknesses of an implementation of FlexPref in PostgreSQL over a range of single-table and multitable preference queries.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2091814660",
    "type": "article"
  },
  {
    "title": "Linear Time Membership in a Class of Regular Expressions with Counting, Interleaving, and Unordered Concatenation",
    "doi": "https://doi.org/10.1145/3132701",
    "publication_date": "2017-11-13",
    "publication_year": 2017,
    "authors": "Dario Colazzo; Giorgio Ghelli; Carlo Sartiani",
    "corresponding_authors": "",
    "abstract": "Regular Expressions (REs) are ubiquitous in database and programming languages. While many applications make use of REs extended with interleaving ( shuffle ) and unordered concatenation operators, this extension badly affects the complexity of basic operations, and, especially, makes membership NP-hard, which is unacceptable in most practical scenarios. In this article, we study the problem of membership checking for a restricted class of these extended REs, called conflict-free REs , which are expressive enough to cover the vast majority of real-world applications. We present several polynomial algorithms for membership checking over conflict-free REs. The algorithms are all polynomial and differ in terms of adopted optimization techniques and in the kind of supported operators. As a particular application, we generalize the approach to check membership of Extensible Markup Language trees into a class of EDTDs (Extended Document Type Definitions) that models the crucial aspects of DTDs (Document Type Definitions) and XSD (XML Schema Definitions) schemas. Results about an extensive experimental analysis validate the efficiency of the presented membership checking techniques.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2768764221",
    "type": "article"
  },
  {
    "title": "BEVA",
    "doi": "https://doi.org/10.1145/2877201",
    "publication_date": "2016-03-18",
    "publication_year": 2016,
    "authors": "Xiaoling Zhou; Jianbin Qin; Chuan Xiao; Wei Wang; Xuemin Lin; Yoshiharu Ishikawa",
    "corresponding_authors": "",
    "abstract": "Query autocompletion has become a standard feature in many search applications, especially for search engines. A recent trend is to support the error-tolerant autocompletion , which increases the usability significantly by matching prefixes of database strings and allowing a small number of errors. In this article, we systematically study the query processing problem for error-tolerant autocompletion with a given edit distance threshold. We propose a general framework that encompasses existing methods and characterizes different classes of algorithms and the minimum amount of information they need to maintain under different constraints. We then propose a novel evaluation strategy that achieves the minimum active node size by eliminating ancestor-descendant relationships among active nodes entirely. In addition, we characterize the essence of edit distance computation by a novel data structure named edit vector automaton (EVA). It enables us to compute new active nodes and their associated states efficiently by table lookups. In order to support large distance thresholds, we devise a partitioning scheme to reduce the size and construction cost of the automaton, which results in the universal partitioned EVA (UPEVA) to handle arbitrarily large thresholds. Our extensive evaluation demonstrates that our proposed method outperforms existing approaches in both space and time efficiencies.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2299127550",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2946798",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "editorial Free Access Share on Editorial: The Dark Citations of TODS Papers and What to Do about It—or: Cite the Journal Paper Editor: Christian S. Jensen View Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 41Issue 2June 2016 Article No.: 8epp 1–3https://doi.org/10.1145/2946798Published:30 June 2016Publication History 0citation264DownloadsMetricsTotal Citations0Total Downloads264Last 12 Months18Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2464780731",
    "type": "editorial"
  },
  {
    "title": "Updating relational databases through weak instance interfaces",
    "doi": "https://doi.org/10.1145/146931.146936",
    "publication_date": "1992-12-01",
    "publication_year": 1992,
    "authors": "Paolo Atzeni; Riccardo Torlone",
    "corresponding_authors": "",
    "abstract": "The problem of updating databases through interfaces based on the weak instance model is studied, thus extending previous proposals that considered them only from the query point of view. Insertions and deletions of tuples are considered. As a preliminary tool, a lattice on states is defined, based on the information content of the various states. Potential results of an insertion are states that contain at least the information in the original state and that in the new tuple. Sometimes there is no potential result, and in the other cases there may be many of them. We argue that the insertion is deterministic if the state that contains the information common to all the potential results (the greatest lower bound, in the lattice framework) is a potential result itself. Effective characterizations for the various cases exist. A symmetric approach is followed for deletions, with fewer cases, since there are always potential results; determinism is characterized as a consequence.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2052743207",
    "type": "article"
  },
  {
    "title": "Distributive join",
    "doi": "https://doi.org/10.1145/115302.115299",
    "publication_date": "1991-12-01",
    "publication_year": 1991,
    "authors": "Mauro Negri; Giuseppe Pelagatti",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Distributive join: a new algorithm for joining relations Authors: M. Negri Univ. di Brescia, Brescia, Italy Univ. di Brescia, Brescia, ItalyView Profile , G. Pelagatti Politecnico di Milano, Milan, Italy Politecnico di Milano, Milan, ItalyView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 16Issue 4pp 655–669https://doi.org/10.1145/115302.115299Published:01 December 1991Publication History 9citation435DownloadsMetricsTotal Citations9Total Downloads435Last 12 Months9Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2005272677",
    "type": "article"
  },
  {
    "title": "Safe stratified Datalog with integer order does not have syntax",
    "doi": "https://doi.org/10.1145/288086.288089",
    "publication_date": "1998-03-01",
    "publication_year": 1998,
    "authors": "Alexei P. Stolboushkin; Michael A. Taitslin",
    "corresponding_authors": "",
    "abstract": "Stratified Datalog with integer (gap)-order (or Datalog ¬,&lt; z ) is considered. A Datalog ¬,&lt; z -program is said to be safe if its bottom-up processing terminates on all valid inputs. We prove that safe Datalog ¬,&lt; z programs do not have effective syntax in the sense that there is no recursively enumerable set S of safe Datalog ¬,&lt; z programs such that every safe Datalog ¬,&lt; z program is equivalent to a program in S .",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2052345856",
    "type": "article"
  },
  {
    "title": "Constant-time maintainability",
    "doi": "https://doi.org/10.1145/128903.128904",
    "publication_date": "1992-06-01",
    "publication_year": 1992,
    "authors": "Ke Wang; Marc H. Graham",
    "corresponding_authors": "",
    "abstract": "The maintenance problem of a database scheme is the following decision problem: Given a consistent database state ρ and a new tuple u over some relation scheme of ρ, is the modified state ρ ∪ { u } still consistent? A database scheme is said to be constant-time-maintainable(ctm) if there exists an algorithm that solves its maintenance problem by making a fixed number of tuple retrievals. We present a practically useful algorithm, called the canonical maintenance algorithm , that solves the maintenance problem of all ctm database schemes within a \"not too large\" bound. A number of interesting properties are shown for ctm database schemes, among them that non-ctm database schemes are not maintainable in less than a linear time in the state size. A test method is given when only cover embedded functional dependencies (fds) appear. When the given dependencies consist of fds and the join dependency (jd) ⋈ R of the database scheme, testing whether a database scheme is ctm is reduced to the case of cover embedded fds. When dependency-preserving database schemes with only equality-generating dependencies (egds) are considered, it is shown that every ctm database scheme has a set of dependencies that is equivalent to a set of embedded fds, and thus, our test method for the case of embedded fds can be applied. In particular, this includes the important case of lossless database schemes with only egds.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2133566171",
    "type": "article"
  },
  {
    "title": "Efficient optimization of simple chase join expressions",
    "doi": "https://doi.org/10.1145/63500.63520",
    "publication_date": "1989-06-01",
    "publication_year": 1989,
    "authors": "Paolo Atzeni; Edward P. F. Chan",
    "corresponding_authors": "",
    "abstract": "Simple chase join expressions are relational algebra expressions, involving only projection and join operators, defined on the basis of the functional dependencies associated with the database scheme. They are meaningful in the weak instance model, because for certain classes of schemes, including independent schemes, the total projections of the representative instance can be computed by means of unions of simple chase join expressions. We show how unions of simple chase join expressions can be optimized efficiently, without constructing and chasing the corresponding tableaux. We also present efficient algorithms for testing containment and equivalence, and for optimizing individual simple chase join expressions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2017293469",
    "type": "article"
  },
  {
    "title": "MULTISAFE—a modular multiprocessing approach to secure database management",
    "doi": "https://doi.org/10.1145/319989.319993",
    "publication_date": "1983-09-01",
    "publication_year": 1983,
    "authors": "Robert P. Trueblood; H. Rex Hartson; Johannes J. Martin",
    "corresponding_authors": "",
    "abstract": "This paper describes the configuration and intermodule communication of a MULTImodule system for supporting Secure Authorization with Full Enforcement (MULTISAFE) for database management. A modular architecture is described which provides secure, controlled access to shared data in a multiuser environment, with low performance penalties, even for complex protection policies. The primary mechanisms are structured and verifiable. The entire approach is immediately extendible to distributed protection of distributed data. The system includes a user and applications module (UAM), a data storage and retrieval module (SRM), and a protection and security module (PSM). The control of intermodule communication is based on a data abstraction approach, initially described in terms of function invocations. An implementation within a formal message system is then described. The discussion of function invocations begins with the single terminal case and extends to the multiterminal case. Some physical implementation aspects are also discussed, and some examples of message sequences are given.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2028093895",
    "type": "article"
  },
  {
    "title": "Comments on batched searching of sequential and tree-structured files",
    "doi": "https://doi.org/10.1145/3857.214294",
    "publication_date": "1985-06-01",
    "publication_year": 1985,
    "authors": "M. Piwowarski",
    "corresponding_authors": "M. Piwowarski",
    "abstract": "Exact formulas for the expected cost savings from batching requests against two types of j-ary trees are given. Approximate expressions are also presented.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2028743993",
    "type": "article"
  },
  {
    "title": "Organization of clustered files for consecutive retrieval",
    "doi": "https://doi.org/10.1145/1994.2208",
    "publication_date": "1984-12-05",
    "publication_year": 1984,
    "authors": "Jitender S. Deogun; Vijay V. Raghavan; Teng-Yuan Tsou",
    "corresponding_authors": "",
    "abstract": "This paper studies the problem of storing single-level and multilevel clustered files. Necessary and sufficient conditions for a single-level clustered file to have the consecutive retrieval property (CRP) are developed. A linear time algorithm to test the CRP for a given clustered file and to identify the proper arrangement of objects, if CRP exists, is presented. For the single-level clustered files that do not have CRP, it is shown that the problem of identifying a storage organization with minimum redundancy is NP-complete. Consequently, an efficient heuristic algorithm to generate a good storage organization for such files is developed. Furthermore, it is shown that, for certain types of multilevel clustered files, there exists a storage organization such that the objects in each cluster, for all clusters in each level of the clustering, appear in consecutive locations.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2054369500",
    "type": "article"
  },
  {
    "title": "Associative searching in multiple storage units",
    "doi": "https://doi.org/10.1145/12047.12048",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "Chao Wu; Walter A. Burkhard",
    "corresponding_authors": "",
    "abstract": "A file maintenance model, called the multiple random access storage units model, is introduced. Storage units can be accessed simultaneously, and the parallel processing of an associative query is achieved by distributing data evenly among the storage units. Maximum parallelism is obtained when data satisfying an associative query are evenly distributed for every possible query. An allocation scheme called M -cycle allocation is proposed to maintain large files of data on multiple random access storage units. The allocation scheme provides an efficient and straightforward indexing over multidimensional key spaces and supports the parallel processing of orthogonal range queries. Our analysis shows that M -cycle allocation achieves the near-optimum parallelism for processing the orthogonal range queries. Moreover, there is no duplication of records and no increase in insertion/deletion cost.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2073220078",
    "type": "article"
  },
  {
    "title": "Analysis of dynamic hashing with deferred splitting",
    "doi": "https://doi.org/10.1145/3148.318987",
    "publication_date": "1985-03-01",
    "publication_year": 1985,
    "authors": "Eugene Veklerov",
    "corresponding_authors": "Eugene Veklerov",
    "abstract": "Dynamic hashing with deferred splitting is a file organization scheme which increases storage utilization, as compared to standard dynamic hashing. In this scheme, splitting of a bucket is deferred if the bucket is full but its brother can accommodate new records. The performance of the scheme is analyzed. In a typical case the expected storage utilization increases from 69 to 76 percent.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2171418521",
    "type": "article"
  },
  {
    "title": "Triggers over nested views of relational data",
    "doi": "https://doi.org/10.1145/1166074.1166080",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Feng Shao; Antal Novak; Jayavel Shanmugasundaram",
    "corresponding_authors": "",
    "abstract": "Current systems that publish relational data as nested (XML) views are passive in the sense that they can only respond to user-initiated queries over the nested views. In this article, we propose an active system whereby users can place triggers on (unmaterialized) nested views of relational data. In this architecture, we present scalable and efficient techniques for processing triggers over nested views by leveraging existing support for SQL triggers over flat relations in commercial relational databases. We have implemented our proposed techniques in the context of the Quark XML middleware system. Our performance results indicate that our proposed techniques are a feasible approach to supporting triggers over nested views of relational data.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2047453522",
    "type": "article"
  },
  {
    "title": "Efficient Evaluation and Static Analysis for Well-Designed Pattern Trees with Projection",
    "doi": "https://doi.org/10.1145/3233983",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Pablo Barceló; Markus Kröll; Reinhard Pichler; Sebastian Skritek",
    "corresponding_authors": "",
    "abstract": "Conjunctive queries (CQs) fail to provide an answer when the pattern described by the query does not exactly match the data. CQs might thus be too restrictive as a querying mechanism when data is semistructured or incomplete. The semantic web therefore provides a formalism—known as (projected) well-designed pattern trees (pWDPTs)—that tackles this problem: pWDPTs allow us to formulate queries that match parts of the query over the data if available, but do not ignore answers of the remaining query otherwise. Here we abstract away the specifics of semantic web applications and study pWDPTs over arbitrary relational schemas. Since the language of pWDPTs subsumes CQs, their evaluation problem is intractable. We identify structural properties of pWDPTs that lead to (fixed-parameter) tractability of various variants of the evaluation problem. We also show that checking if a pWDPT is equivalent to one in our tractable class is in 2EXPTIME. As a corollary, we obtain fixed-parameter tractability of evaluation for pWDPTs with such good behavior. Our techniques also allow us to develop a theory of approximations for pWDPTs.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2888755732",
    "type": "article"
  },
  {
    "title": "A Relational Framework for Classifier Engineering",
    "doi": "https://doi.org/10.1145/3268931",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Benny Kimelfeld; Christopher Ré",
    "corresponding_authors": "",
    "abstract": "In the design of analytical procedures and machine learning solutions, a critical and time-consuming task is that of feature engineering, for which various recipes and tooling approaches have been developed. In this article, we embark on the establishment of database foundations for feature engineering. We propose a formal framework for classification in the context of a relational database. The goal of this framework is to open the way to research and techniques to assist developers with the task of feature engineering by utilizing the database’s modeling and understanding of data and queries and by deploying the well-studied principles of database management. As a first step, we demonstrate the usefulness of this framework by formally defining three key algorithmic challenges. The first challenge is that of separability, which is the problem of determining the existence of feature queries that agree with the training examples. The second is that of evaluating the VC dimension of the model class with respect to a given sequence of feature queries. The third challenge is identifiability, which is the task of testing for a property of independence among features that are represented as database queries. We give preliminary results on these challenges for the case where features are defined by means of conjunctive queries, and, in particular, we study the implication of various traditional syntactic restrictions on the inherent computational complexity.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2898639466",
    "type": "article"
  },
  {
    "title": "On the Language of Nested Tuple Generating Dependencies",
    "doi": "https://doi.org/10.1145/3369554",
    "publication_date": "2020-06-30",
    "publication_year": 2020,
    "authors": "Phokion G. Kolaitis; Reinhard Pichler; Emanuel Sallinger; Vadim Savenkov",
    "corresponding_authors": "",
    "abstract": "During the past 15 years, schema mappings have been extensively used in formalizing and studying such critical data interoperability tasks as data exchange and data integration. Much of the work has focused on GLAV mappings, i.e., schema mappings specified by source-to-target tuple-generating dependencies (s-t tgds), and on schema mappings specified by second-order tgds (SO tgds), which constitute the closure of GLAV mappings under composition. In addition, nested GLAV mappings have also been considered, i.e., schema mappings specified by nested tgds, which have expressive power intermediate between s-t tgds and SO tgds. Even though nested GLAV mappings have been used in data exchange systems, such as IBM’s Clio, no systematic investigation of this class of schema mappings has been carried out so far. In this article, we embark on such an investigation by focusing on the basic reasoning tasks, algorithmic problems, and structural properties of nested GLAV mappings. One of our main results is the decidability of the implication problem for nested tgds. We also analyze the structure of the core of universal solutions with respect to nested GLAV mappings and develop useful tools for telling apart SO tgds from nested tgds. By discovering deeper structural properties of nested GLAV mappings, we show that also the following problem is decidable: Given a nested GLAV mapping, is it logically equivalent to a GLAV mapping?",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3041663374",
    "type": "article"
  },
  {
    "title": "Entangled queries",
    "doi": "https://doi.org/10.1145/2338626.2338629",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Nitin Gupta; Łucja Kot; Sudip Roy; Gabriel Bender; Johannes Gehrke; Christoph Koch",
    "corresponding_authors": "",
    "abstract": "Many data-driven social and Web applications involve collaboration and coordination. The vision of Declarative Data-Driven Coordination (D3C), proposed in Kot et al. [2010], is to support coordination in the spirit of data management: to make it data-centric and to specify it using convenient declarative languages. This article introduces entangled queries , a language that extends SQL by constraints that allow for the coordinated choice of result tuples across queries originating from different users or applications. It is nontrivial to define a declarative coordination formalism without arriving at the general (NP-complete) Constraint Satisfaction Problem from AI. In this article, we propose an efficiently enforceable syntactic safety condition that we argue is at the sweet spot where interesting declarative power meets applicability in large-scale data management systems and applications. The key computational problem of D3C is to match entangled queries to achieve coordination. We present an efficient matching algorithm which statically analyzes query workloads and merges coordinating entangled queries into compound SQL queries. These can be sent to a standard database system and return only coordinated results. We present the overall architecture of an implemented system that contains our evaluation algorithm. We also describe a proof-of-concept Facebook application we have built on top of this system to allow friends to coordinate flight plans. Finally, we evaluate the performance of the matching algorithm experimentally on realistic coordination workloads.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1994156295",
    "type": "article"
  },
  {
    "title": "Revisiting answering tree pattern queries using views",
    "doi": "https://doi.org/10.1145/2338626.2338631",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Junhu Wang; Jeffrey Xu Yu",
    "corresponding_authors": "",
    "abstract": "We revisit the problem of answering tree pattern queries using views. We first show that, for queries and views that do not have nodes labeled with the wildcard *, there is an approach which does not require us to find any rewritings explicitly, yet which produces the same answers as the maximal contained rewriting. Then, using the new approach, we give simple conditions and a corresponding algorithm for identifying redundant view answers, which are view answers that can be ignored when evaluating the maximal contained rewriting. We also consider redundant view answers in the case where there are multiple views, the relationship between redundant views and redundant view answers, and discuss how to combine the removal of redundant view answers and redundant rewritings. We show that the aforesaid results can be extended to a number of other special cases. Finally, for arbitrary queries and views in P {/,//,.,[]} , we provide a method to find the maximal contained rewriting and show how to answer the query using views without explicitly finding the rewritings.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2018577048",
    "type": "article"
  },
  {
    "title": "Reducing Layered Database Applications to their Essence through Vertical Integration",
    "doi": "https://doi.org/10.1145/2818180",
    "publication_date": "2015-10-23",
    "publication_year": 2015,
    "authors": "Kristian F. D. Rietveld; Harry A. G. Wijshoff",
    "corresponding_authors": "",
    "abstract": "In the last decade, improvements on single-core performance of CPUs has stagnated. Consequently, methods for the development and optimization of software for these platforms have to be reconsidered. Software must be optimized such that the available single-core performance is exploited more effectively. This can be achieved by reducing the number of instructions that need to be executed. In this article, we show that layered database applications execute many redundant, nonessential, instructions that can be eliminated without affecting the course of execution and the output of the application. This elimination is performed using a vertical integration process which breaks down the different layers of layered database applications. By doing so, applications are being reduced to their essence, and as a consequence, transformations can be carried out that affect both the application code and the data access code which were not possible before. We show that this vertical integration process can be fully automated and, as such, be integrated in an operational workflow. Experimental evaluation of this approach shows that up to 95% of the instructions can be eliminated. The reduction of instructions leads to a more efficient use of the available hardware resources. This results in greatly improved performance of the application and a significant reduction in energy consumption.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2039877226",
    "type": "article"
  },
  {
    "title": "Bag equivalence of tree patterns",
    "doi": "https://doi.org/10.1145/2043652.2043657",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Sara Cohen; Yaacov Y. Weiss",
    "corresponding_authors": "",
    "abstract": "When a query is evaluated under bag semantics, each answer is returned as many times as it has derivations. Bag semantics has long been recognized as important, especially when aggregation functions will be applied to query results. This article is the first to focus on bag semantics for tree pattern queries. In particular, the problem of bag equivalence of a large class of tree pattern queries (which can be used to model XPath) is explored. The queries can contain unions, branching, label wildcards, the vertical child and descendant axes, the horizontal following and following-sibling axes, as well as positional (i.e., first and last) axes. Equivalence characterizations are provided, and their complexity is analyzed. As the descendant axis involves a recursive relationship, this article is also the first to address bag equivalence over recursive queries, in any setting.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2091569260",
    "type": "article"
  },
  {
    "title": "Sampling a Near Neighbor in High Dimensions — Who is the Fairest of Them All?",
    "doi": "https://doi.org/10.1145/3502867",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Martin Aumüller; Sariel Har-Peled; Sepideh Mahabadi; Rasmus Pagh; Francesco Silvestri",
    "corresponding_authors": "",
    "abstract": "Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. Given a set of points S and a radius parameter r > 0, the r-near neighbor (r-NN) problem asks for a data structure that, given any query point q, returns a point p within distance at most r from q. In this paper, we study the r-NN problem in the light of individual fairness and providing equal opportunities: all points that are within distance r from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee.In this work, we show that LSH based algorithms can be made fair, without a significant loss in efficiency. We propose several efficient data structures for the exact and approximate variants of the fair NN problem. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. We also develop a data structure for fair similarity search under inner product that requires nearly-linear space and exploits locality sensitive filters. The paper concludes with an experimental evaluation that highlights the unfairness of state-of-the-art NN data structures and shows the performance of our algorithms on real-world datasets.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210512186",
    "type": "article"
  },
  {
    "title": "Instance-optimal Truncation for Differentially Private Query Evaluation with Foreign Keys",
    "doi": "https://doi.org/10.1145/3697831",
    "publication_date": "2024-09-26",
    "publication_year": 2024,
    "authors": "Wei Dong; Juanru Fang; Ke Yi; Yuchao Tao; Ashwin Machanavajjhala",
    "corresponding_authors": "",
    "abstract": "Answering SPJA queries under differential privacy (DP), including graph pattern counting under node-DP as an important special case, has received considerable attention in recent years. The dual challenge of foreign-key constraints combined with self-joins is particularly tricky to deal with, and no existing DP mechanisms can correctly handle both. For the special case of graph pattern counting under node-DP, the existing mechanisms are correct (i.e., satisfy DP), but they do not offer nontrivial utility guarantees or are very complicated and costly. In this article, we propose two mechanisms for solving this problem with both efficiency and strong utility guarantees. The first mechanism, called R2T , is simple and efficient, while achieving down-neighborhood optimality with a logarithmic optimality ratio. Down-neighborhood optimality is a new notion of optimality that we introduce for measuring the utilities of DP mechanisms, which can be considered as a natural relaxation of instance optimality, and it is especially suitable for functions with a large or unbounded sensitivity. Our second mechanism further reduces the optimality ratio to a double logarithm, which is also known to be optimal, thus we call this mechanism OPT 2 . While OPT 2 also runs in polynomial time, it does have a higher computational cost than R2T in practice. Both R2T and OPT 2 are simple enough that they can be easily implemented on top of any RDBMS and an LP solver. Experimental results show that they offer order-of-magnitude improvements in terms of utility over existing techniques, even those specifically designed for graph pattern counting.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402870446",
    "type": "article"
  },
  {
    "title": "Making It Tractable to Detect and Correct Errors in Graphs",
    "doi": "https://doi.org/10.1145/3702315",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Wenfei Fan; K.K. Pang; Ping Lü; Chao Tian",
    "corresponding_authors": "",
    "abstract": "This paper develops \\(\\mathsf {Hercules} \\) , a system for entity resolution (ER), conflict resolution (CR), timeliness deduction (TD) and missing value/link imputation (MI) in graphs. It proposes \\(\\mathsf {GCR^{+}\\!s} \\) , a class of graph cleaning rules that support not only predicates for ER and CR, but also temporal orders to deduce timeliness and data extraction to impute missing data. As opposed to previous graph rules, \\(\\mathsf {GCR^{+}\\!s} \\) are defined with a dual graph pattern to accommodate irregular structures of schemaless graphs, and adopt patterns of a star form to reduce the complexity. We show that while the implication and satisfiability problems are intractable for \\(\\mathsf {GCR^{+}\\!s} \\) , it is in PTIME to detect and correct errors with \\(\\mathsf {GCR^{+}\\!s} \\) . Underlying \\(\\mathsf {Hercules} \\) , we train a ranking model to predict the temporal orders on attributes, and embed it as a predicate of \\(\\mathsf {GCR^{+}\\!s} \\) . We provide an algorithm for discovering \\(\\mathsf {GCR^{+}\\!s} \\) by combining the generations of patterns and predicates. We also develop a method for conducting ER, CR, TD and MI in the same process to improve the overall quality of graphs, by leveraging their interactions and chasing with \\(\\mathsf {GCR^{+}\\!s} \\) ; we show that the method has the Church-Rosser property under certain conditions. Using real-life and synthetic graphs, we empirically verify that \\(\\mathsf {Hercules} \\) is 53% more accurate than the state-of-the-art graph cleaning systems, and performs comparably in efficiency and scalability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404006790",
    "type": "article"
  },
  {
    "title": "A Caching-based Framework for Scalable Temporal Graph Neural Network Training",
    "doi": "https://doi.org/10.1145/3705894",
    "publication_date": "2024-11-25",
    "publication_year": 2024,
    "authors": "Yiming Li; Yanyan Shen; Lei Chen; Mingxuan Yuan",
    "corresponding_authors": "",
    "abstract": "Representation learning over dynamic graphs is critical for many real-world applications such as social network services and recommender systems. Temporal graph neural networks (T-GNNs) are powerful representation learning methods and have demonstrated remarkable effectiveness on continuous-time dynamic graphs. However, T-GNNs still suffer from high time complexity, which increases linearly with the number of timestamps and grows exponentially with the model depth, making them not scalable to large dynamic graphs. To address the limitations, we propose Orca , a novel framework that accelerates T-GNN training by caching and reusing intermediate embeddings. We design an optimal caching policy, named MRD , for the uniform cache replacement problem, where embeddings at different intermediate layers have identical dimensions and recomputation costs. MRD not only improves the efficiency of training T-GNNs by maximizing the number of cache hits but also reduces the approximation errors by avoiding keeping and reusing extremely stale embeddings. For the general cache replacement problem, where embeddings at different intermediate layers can have different dimensions and recomputation costs, we solve this NP-hard problem by presenting a novel two-stage framework with approximation guarantees on the achieved benefit of caching. Furthermore, we have developed profound theoretical analyses of the approximation errors introduced by reusing intermediate embeddings, providing a thorough understanding of the impact of our caching and reuse schemes on model outputs. We also offer rigorous convergence guarantees for model training, adding to the reliability and validity of our Orca framework. Extensive experiments have validated that Orca can obtain two orders of magnitude speedup over state-of-the-art T-GNNs while achieving higher precision on various dynamic graphs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404700191",
    "type": "article"
  },
  {
    "title": "Multiview access protocols for large-scale replication",
    "doi": "https://doi.org/10.1145/292481.277628",
    "publication_date": "1998-06-01",
    "publication_year": 1998,
    "authors": "Xiangning Liu; Sumi Helal; Weimin Du",
    "corresponding_authors": "",
    "abstract": "The article proposes a scalable protocol for replication management in large-scale replicated systems. The protocol organizes sites and data replicas into a tree-structured, hierarchical cluster architecture. The basic idea of the protocol is to accomplish the complex task of updating replicated data with a very large number of replicas by a set of related but independently committed transactions. Each transaction is responsible for updating replicas in exactly one cluster and invoking additional transactions for member clusters. Primary copies (one from each cluster) are updated by a cross-cluster transaction. Then each cluster is independently updated by a separate transaction. This decoupled update propagation process results in possible multiple views of replicated data in a cluster. Compared to other replicated data management protocols, the proposed protocol has several unique advantages. First, thanks to a smaller number of replicas each transaction needs to atomically update in a cluster, the protocol significantly reduces the transaction abort rate, which tends to soar in large transactional systems. Second, the protocol improves user-level transaction response time as top-level update transactions are allowed to commit before all replicas have been updated. Third, read-only queries have the flexibility to see database views of different degrees of consistency and data currency. This ranges from global, most up to date, and consistent views, to local, consistent, but potentially old views, to local, nearest to users but potentially inconsistent views. Fourth, the protocol maintains its scalability by allowing dynamic system reconfiguration as it grows by splitting a cluster into two or more smaller ones. Fifth, autonomy of the clusters is preserved as no specific protocol is required to update replicas within the same cluster. Clusters are, therefore, free to use any valid replication or concurrency control protocols.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2029015022",
    "type": "article"
  },
  {
    "title": "Imprecise schema: a rationale for relations with embedded subrelations",
    "doi": "https://doi.org/10.1145/76902.76903",
    "publication_date": "1989-12-01",
    "publication_year": 1989,
    "authors": "Howard M. Dreizen; Shi‐Kuo Chang",
    "corresponding_authors": "",
    "abstract": "Exceptional conditions are anomalous data which meet the intent of a schema but not the schema definition, represent a small proportion of the database extension, and may become known only after the schema is in use. Admission of exceptional conditions is argued to suggest a representation that locally stretches the schema definition by use of relations with embedded subrelations. Attempted normalization of these relations to 1NF does not yield the static schema typically associated with such transformations. A class of relations, termed Exceptional Condition Nested Form (ECNF), is defined which allows the necessary representation of exceptional conditions while containing sufficient restrictions to prevent arbitrary and chaotic inclusion of embedded subrelations. Queries on a subset of exceptional conditions, the exceptional constraints , are provided an interpretation via an algorithm that transforms ECNF relations into 1NF relations containing two types of null values. Extensions of relational algebraic operators, suitable for interactive query navigation, are defined for use with ECNF relations containing all forms of exceptional conditions.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1976861191",
    "type": "article"
  },
  {
    "title": "A unified analysis of batched searching of sequential and tree-structured files",
    "doi": "https://doi.org/10.1145/76902.76908",
    "publication_date": "1989-12-01",
    "publication_year": 1989,
    "authors": "Sheau-Dong Lang; James R. Driscoll; Jiann H. Jou",
    "corresponding_authors": "",
    "abstract": "A direct and unified approach is used to analyze the efficiency of batched searching of sequential and tree-structured files. The analysis is applicable to arbitrary search distributions, and closed-form expressions are obtained for the expected batched searching cost and savings. In particular, we consider a search distribution satisfying Zipf's law for sequential files and four types of uniform (random) search distribution for sequential and tree-structured files. These results unify and extend earlier research on batched searching and estimating block accesses for database systems.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2007013915",
    "type": "article"
  },
  {
    "title": "Data replicas in distributed information services",
    "doi": "https://doi.org/10.1145/62032.62035",
    "publication_date": "1989-03-01",
    "publication_year": 1989,
    "authors": "H. M. Gladney",
    "corresponding_authors": "H. M. Gladney",
    "abstract": "In an information distribution network in which records are repeatedly read, it is cost-effective to keep read-only copies in work locations. This paper presents a method of updating replicas that need not be immediately synchronized with the source data or with each other. The method allows an arbitrary mapping from source records to replica records. It is fail-safe, maximizes workstation autonomy, and is well suited to a network with slow, unreliable, and/or expensive communications links. The algorithm is a manipulation of queries, which are represented as short encodings. When a response is generated, we record which portion of the source database was used. Later, when the source data are updated, this information is used to identify obsolete replicas. For each workstation, the identity of obsolete replicas is saved until a workstation process asks for this information. This workstation process deletes each obsolete replica, and replaces it by an up-to-date version either promptly or the next time the application asks for this particular item. Throughout, queries are grouped so that the impact of each source update transaction takes effect atomically at each workstation. Optimizations of the basic algorithm are outlined. These overlap change dissemination with user service, allow the mechanism to be hidden within the data delivery subsystem, and permit very large networks.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2011842080",
    "type": "article"
  },
  {
    "title": "A dynamic framework for object projection views",
    "doi": "https://doi.org/10.1145/42201.42202",
    "publication_date": "1988-03-01",
    "publication_year": 1988,
    "authors": "Victor Vianu",
    "corresponding_authors": "Victor Vianu",
    "abstract": "User views in a relational database obtained through a single projection (\"projection views\") are considered in a new framework. Specifically, such views, where each tuple in the view represents an object (\"object-projection views\"), are studied using the dynamic relational model, which captures the evolution of the database through consecutive updates. Attribute sets that yield object-projection views are characterized using the static and dynamic functional dependencies satisfied by the database. Object-projection views are then described using the static and dynamic functional dependencies “inherited” from the original database. Finally, the impact of dynamic constraints on the view update problem is studied in a limited context. This paper demonstrates that new, useful information about views can be obtained by looking at the evolution of the database as captured by the dynamic relational model.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2069400400",
    "type": "article"
  },
  {
    "title": "Understanding the global semantics of referential actions using logic rules",
    "doi": "https://doi.org/10.1145/582410.582411",
    "publication_date": "2002-12-01",
    "publication_year": 2002,
    "authors": "Wolfgang May; Bertram Ludäscher",
    "corresponding_authors": "",
    "abstract": "Referential actions are specialized triggers for automatically maintaining referential integrity in databases. While the local effects of referential actions can be grasped easily, it is far from obvious what the global semantics of a set of interacting referential actions should be. In particular, when using procedural execution models, ambiguities due to the execution ordering can occur. No global, declarative semantics of referential actions has yet been defined.We show that the well-known logic programming semantics provide a natural global semantics of referential actions that is based on their local characterization: To capture the global meaning of a set RA of referential actions, we first define their abstract (but non-constructive) intended semantics . Next, we formalize RA as a logic program P RA . The declarative, logic programming semantics of P RA then provide the constructive, global semantics of the referential actions. So, we do not define a semantics for referential actions, but we show that there exists a unique natural semantics if one is ready to accept (i) the intuitive local semantics of local referential actions, (ii) the formalization of those and of the local \"effect-propagating\" rules, and (iii) the well-founded or stable model semantics from logic programming as \"reasonable\" global semantics for local rules.We first focus on the subset of referential actions for deletions only. We prove the equivalence of the logic programming semantics and the abstract semantics via a game-theoretic characterization, which provides additional insight into the meaning of interacting referential actions. In this case a unique maximal admissible solution exists , computable by a ptime algorithm.Second, we investigate the general case---including modifications. We show that in this case there can be multiple maximal admissible subsets and that all maximal admissible subsets can be characterized as 3-valued stable models of P RA . We show that for a given set of user requests, in the presence of referential actions of the form ON UPDATE CASCADE, the admissibility check and the computation of the subsequent database state, and (for non-admissible updates) the derivation of debugging hints all are in ptime. Thus, full referential actions can be implemented efficiently.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2153714213",
    "type": "article"
  },
  {
    "title": "Pipelining",
    "doi": "https://doi.org/10.1145/320107.320118",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "Barron C. Housel",
    "corresponding_authors": "Barron C. Housel",
    "abstract": "In the past several years much attention has been given to the problem of data translation. The focus has been mainly on methodologies and specification languages for accomplishing this task. Recently, several prototype systems have emerged, and now the issues of implementation and performance must be addressed. In general, a data restructuring specification may contain multiple source and target files. This specification can be viewed as a “process graph” which is a network of restructuring operations subject to precedence constraints. One technique used to achieve good performance is that of pipelining data in the process graph. In this paper we address a number of issues pertinent to a pipelining architecture. Specifically, we give algorithms for resolving deadlock situations which can arise, and partitioning the process graph to achieve an optimal schedule for executing the restructuring steps. In addition, we discuss how pipelining has influenced the design of the restructuring operations and the file structures used in an actual system.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2032952145",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2747020",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1968416537",
    "type": "editorial"
  },
  {
    "title": "A Join-Like Operator to Combine Data Cubes and Answer Queries from Multiple Data Cubes",
    "doi": "https://doi.org/10.1145/2638545",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Francesco M. Malvestuto",
    "corresponding_authors": "Francesco M. Malvestuto",
    "abstract": "In order to answer a “joint” query from multiple data cubes, Pourabass and Shoshani [2007] distinguish the data cube on the measure of interest (called the “primary” data cube) from the other data cubes (called “proxy” data cubes) that are used to involve the dimensions (in the query) not in the primary data cube. They demonstrate in study cases that, if the measures of the primary and proxy data cubes are correlated, then the answer to a joint query is an accurate estimate of its true value. Needless to say, for two or more proxy data cubes, the result depends upon the way the primary and proxy data cubes are combined together; however, for certain combination schemes Pourabass and Shoshani provide a sufficient condition , that they call proxy noncommonality , for the invariance of the result. In this article, we introduce: (1) a merge operator combining the contents of a primary data cube with the contents of a proxy data cube, (2) merge expressions for general combination schemes, and (3) an equivalence relation between merge expressions having the same pattern. Then, we prove that proxy noncommonality characterizes patterns for which every two merge expressions are equivalent. Moreover, we provide an efficient procedure for answering joint queries in the special case of perfect merge expressions. Finally, we show that our results apply to data cubes in which measures are obtained from unaggregated data using the aggregate functions SUM, COUNT, MAX, and MIN, and a lot more.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2024478485",
    "type": "article"
  },
  {
    "title": "Deletion without rebalancing in multiway search trees",
    "doi": "https://doi.org/10.1145/2540068",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Siddhartha Sen; Robert E. Tarjan",
    "corresponding_authors": "",
    "abstract": "Some database systems that use a form of B-tree for the underlying data structure do not do rebalancing on deletion. This means that a bad sequence of deletions can create a very unbalanced tree. Yet such databases perform well in practice. Avoidance of rebalancing on deletion has been justified empirically and by average-case analysis, but to our knowledge, no worst-case analysis has been done. We do such an analysis. We show that the tree height remains logarithmic in the number of insertions, independent of the number of deletions. Furthermore, the amortized time for an insertion or deletion, excluding the search time, is O (1), and nodes are modified by insertions and deletions with a frequency that is exponentially small in their height. The latter results do not hold for standard B-trees. By adding periodic rebuilding of the tree, we obtain a data structure that is theoretically superior to standard B-trees in many ways. Our results suggest that rebalancing on deletion not only is unnecessary but may be harmful.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2055701211",
    "type": "article"
  },
  {
    "title": "Technical Correspondence",
    "doi": "https://doi.org/10.1145/2757214",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Millist W. Vincent; Jixue Liu; Hong-Cheu Liu; Sebastian Link",
    "corresponding_authors": "",
    "abstract": "To address the frequently occurring situation where data is inexact or imprecise, a number of extensions to the classical notion of a functional dependency (FD) integrity constraint have been proposed in recent years. One of these extensions is the notion of a differential dependency (DD), introduced in the recent article “Differential Dependencies: Reasoning and Discovery” by Song and Chen in the March 2011 edition of this journal. A DD generalises the notion of an FD by requiring only that the values of the attribute from the RHS of the DD satisfy a distance constraint whenever the values of attributes from the LHS of the DD satisfy a distance constraint. In contrast, an FD requires that the values from the attributes in the RHS of an FD be equal whenever the values of the attributes from the LHS of the FD are equal. The article “Differential Dependencies: Reasoning and Discovery” investigated a number of aspects of DDs, the most important of which, since they form the basis for the other topics investigated, were the consistency problem (determining whether there exists a relation instance that satisfies a set of DDs) and the implication problem (determining whether a set of DDs logically implies another DD). Concerning these problems, a number of results were claimed in “Differential Dependencies: Reasoning and Discovery”. In this article we conduct a detailed analysis of the correctness of these results. The outcomes of our analysis are that, for almost every claimed result, we show there are either fundamental errors in the proof or the result is false. For some of the claimed results we are able to provide corrected proofs, but for other results their correctness remains open.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2240640875",
    "type": "article"
  },
  {
    "title": "ENFrame",
    "doi": "https://doi.org/10.1145/2877205",
    "publication_date": "2016-03-18",
    "publication_year": 2016,
    "authors": "Dan Olteanu; Sebastiaan J. van Schaik",
    "corresponding_authors": "",
    "abstract": "This article introduces ENFrame, a framework for processing probabilistic data. Using ENFrame, users can write programs in a fragment of Python with constructs such as loops, list comprehension, aggregate operations on lists, and calls to external database engines. Programs are then interpreted probabilistically by ENFrame. We exemplify ENFrame on three clustering algorithms ( k -means, k -medoids, and Markov clustering) and one classification algorithm ( k -nearest-neighbour). A key component of ENFrame is an event language to succinctly encode correlations, trace the computation of user programs, and allow for computation of discrete probability distributions for program variables. We propose a family of sequential and concurrent, exact, and approximate algorithms for computing the probability of interconnected events. Experiments with k -medoids clustering and k -nearest-neighbour show orders-of-magnitude improvements of exact processing using ENFrame over naïve processing in each possible world, of approximate over exact, and of concurrent over sequential processing.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2299384458",
    "type": "article"
  },
  {
    "title": "B-Trees and Cache-Oblivious B-Trees with Different-Sized Atomic Keys",
    "doi": "https://doi.org/10.1145/2907945",
    "publication_date": "2016-07-18",
    "publication_year": 2016,
    "authors": "Michael A. Bender; Roozbeh Ebrahimi; Haodong Hu; Bradley C. Kuszmaul",
    "corresponding_authors": "",
    "abstract": "Most B-tree articles assume that all N keys have the same size K , that f = B / K keys fit in a disk block, and therefore that the search cost is O (log f + 1 N ) block transfers. When keys have variable size, B-tree operations have no nontrivial performance guarantees, however. This article provides B-tree-like performance guarantees on dictionaries that contain keys of different sizes in a model in which keys must be stored and compared as opaque objects. The resulting atomic-key dictionaries exhibit performance bounds in terms of the average key size and match the bounds when all keys are the same size. Atomic-key dictionaries can be built with minimal modification to the B-tree structure, simply by choosing the pivot keys properly. This article describes both static and dynamic atomic-key dictionaries. In the static case, if there are N keys with average size K , the search cost is O (⌈ K / B ⌉log 1 + ⌈ B / K ⌉ N ) expected transfers. It is not possible to transform these expected bounds into worst-case bounds. The cost to build the tree is O ( NK ) operations and O ( NK/B ) transfers if all keys are presented in sorted order. If not, the cost is the sorting cost. For the dynamic dictionaries, the amortized cost to insert a key κ of arbitrary length at an arbitrary rank is dominated by the cost to search for κ. Specifically, the amortized cost to insert a key κ of arbitrary length and random rank is O (⌈ K / B ⌉log 1 + ⌈ B / K ⌉ N + |κ|/ B ) transfers. A dynamic-programming algorithm is shown for constructing a search tree with minimal expected cost. This article also gives a cache-oblivious static atomic-key B-tree, which achieves the same asymptotic performance as the static B-tree dictionary, mentioned previously. A cache-oblivious data structure or algorithm is not parameterized by the block size B or memory size M in the memory hierarchy; rather, it is universal, working simultaneously for all possible values of B or M . On a machine with block size B , if there are N keys with average size K , search operations costs O (⌈ K / B ⌉log 1 + ⌈ B / K ⌉ N ) block transfers in expectation. This cache-oblivious layout can be built in O ( N log( NK )) processor operations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2499700696",
    "type": "article"
  },
  {
    "title": "DomainNet: Homograph Detection and Understanding in Data Lake Disambiguation",
    "doi": "https://doi.org/10.1145/3612919",
    "publication_date": "2023-08-05",
    "publication_year": 2023,
    "authors": "Aristotelis Leventidis; Laura Rocco; Wolfgang Gatterbauer; Renée J. Miller; Mirek Riedewald",
    "corresponding_authors": "",
    "abstract": "Modern data lakes are heterogeneous in the vocabulary that is used to describe data. We study a problem of disambiguation in data lakes: How can we determine if a data value occurring more than once in the lake has different meanings and is therefore a homograph? While word and entity disambiguation have been well studied in computational linguistics, data management, and data science, we show that data lakes provide a new opportunity for disambiguation of data values, because tables implicitly define a massive network of interconnected values. We introduce DomainNet , which efficiently represents this network, and investigate to what extent it can be used to disambiguate values without requiring any supervision. DomainNet leverages network-centrality measures on a bipartite graph whose nodes represent data values and attributes to determine if a value is a homograph. A thorough experimental evaluation demonstrates that state-of-the-art techniques in domain discovery cannot be re-purposed to compete with our method. Specifically, using a domain discovery method to identify homographs achieves an F1-score of 0.38 versus 0.69 for DomainNet , which separates homographs well from data values that have a unique meaning. On a real data lake, our top-100 precision is 93%. Given a homograph, we also present a novel method for determining the number of meanings of the homograph and for assigning its data lake attributes to a meaning. We show the influence of homographs on two downstream tasks: entity-matching and domain discovery.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4385607102",
    "type": "article"
  },
  {
    "title": "Partial Order Multiway Search",
    "doi": "https://doi.org/10.1145/3626956",
    "publication_date": "2023-10-09",
    "publication_year": 2023,
    "authors": "Shangqi Lu; Wim Martens; Matthias Niewerth; Yufei Tao",
    "corresponding_authors": "",
    "abstract": "Partial order multiway search (POMS) is a fundamental problem that finds applications in crowdsourcing, distributed file systems, software testing, and more. This problem involves an interaction between an algorithm 𝒜 and an oracle, conducted on a directed acyclic graph 𝒢 known to both parties. Initially, the oracle selects a vertex t in 𝒢 called the target . Subsequently, 𝒜 must identify the target vertex by probing reachability. In each probe , 𝒜 selects a set Q of vertices in 𝒢, the number of which is limited by a pre-agreed value k . The oracle then reveals, for each vertex q ∈ Q , whether q can reach the target in 𝒢. The objective of 𝒜 is to minimize the number of probes. We propose an algorithm to solve POMS in \\(O(\\log _{1+k} n + \\frac{d}{k} \\log _{1+d} n)\\) probes, where n represents the number of vertices in 𝒢, and d denotes the largest out-degree of the vertices in 𝒢. The probing complexity is asymptotically optimal. Our study also explores two new POMS variants: The first one, named taciturn POMS , is similar to classical POMS but assumes a weaker oracle, and the second one, named EM POMS , is a direct extension of classical POMS to the external memory (EM) model. For both variants, we introduce algorithms whose performance matches or nearly matches the corresponding theoretical lower bounds.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387460439",
    "type": "article"
  },
  {
    "title": "Fast Parallel Hypertree Decompositions in Logarithmic Recursion Depth",
    "doi": "https://doi.org/10.1145/3638758",
    "publication_date": "2023-12-30",
    "publication_year": 2023,
    "authors": "Georg Gottlob; Matthias Lanzinger; Cem Okulmus; Reinhard Pichler",
    "corresponding_authors": "",
    "abstract": "Various classic reasoning problems with natural hypergraph representations are known to be tractable if a hypertree decomposition (HD) of low width exists. The resulting algorithms are attractive for practical use in fields like databases and constraint satisfaction. However, algorithmic use of HDs relies on the difficult task of first computing a decomposition of the hypergraph underlying a given problem instance, which is then used to guide the algorithm for this particular instance. The performance of purely sequential methods for computing HDs is inherently limited, yet the problem is, theoretically, amenable to parallelisation. In this article, we propose the first algorithm for computing hypertree decompositions that is well suited for parallelisation. The newly proposed algorithm log- k -decomp requires only a logarithmic number of recursion levels and additionally allows for highly parallelised pruning of the search space by restriction to so-called balanced separators. We provide a detailed experimental evaluation over the HyperBench benchmark and demonstrate that log- k -decomp outperforms the current state of the art significantly.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390450394",
    "type": "article"
  },
  {
    "title": "Stochastic query optimization in distributed databases",
    "doi": "https://doi.org/10.1145/151634.151637",
    "publication_date": "1993-06-01",
    "publication_year": 1993,
    "authors": "P. E. Drenick; Edward Smith",
    "corresponding_authors": "",
    "abstract": "Many algorithms have been devised for minimizing the costs associated with obtaining the answer to a single, isolated query in a distributed database system. However, if more than one query may be processed by the system at the same time and if the arrival times of the queries are unknown, the determination of optimal query-processing strategies becomes a stochastic optimization problem. In order to cope with such problems, a theoretical state-transition model is presented that treats the system as one operating under a stochastic load. Query-processing strategies may then be distributed over the processors of a network as probability distributions, in a manner which accommodates many queries over time. It is then shown that the model leads to the determination of optimal query-processing strategies as the solution of mathematical programming problems, and analytical results for several examples are presented. Furthermore, a divide-and-conquer approach is introduced for decomposing stochastic query optimization problems into distinct subproblems for processing queries sequentially and in parallel.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1992585497",
    "type": "article"
  },
  {
    "title": "On Roth, Korth, and Silberschatz's extended algebra and calculus for nested relational databases",
    "doi": "https://doi.org/10.1145/128903.128908",
    "publication_date": "1992-06-01",
    "publication_year": 1992,
    "authors": "Abdullah Uz Tansel; Lucy Garnett",
    "corresponding_authors": "",
    "abstract": "We discuss the issues encountered in the extended algebra and calculus languages for nested relations defined by Roth, Korth, and Silberschatz.[4]. Their equivalence proof between algebra and calculus fails because of the keying problems and the use of extended set operations. Extended set operations also have unintended side effects. Furthermore, their calculus seems to allow the generation of power sets, thus making it more powerful than their algebra.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1972553561",
    "type": "article"
  },
  {
    "title": "Extended ephemeral logging",
    "doi": "https://doi.org/10.1145/244810.244811",
    "publication_date": "1997-03-01",
    "publication_year": 1997,
    "authors": "John S. Keen; William J. Dally",
    "corresponding_authors": "",
    "abstract": "article Free AccessExtended ephemeral logging: log storage management for applications with long lived transactions Authors: John S. Keen Silicon Graphics, Mountain View, CA Silicon Graphics, Mountain View, CAView Profile , William J. Dally Massachusetts Institute of Technology, Cambridge, MA Massachusetts Institute of Technology, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 22Issue 1pp 1–42https://doi.org/10.1145/244810.244811Published:01 March 1997Publication History 5citation636DownloadsMetricsTotal Citations5Total Downloads636Last 12 Months13Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2082791906",
    "type": "article"
  },
  {
    "title": "Performance of recovery architectures in parallel associative database processors",
    "doi": "https://doi.org/10.1145/319989.319990",
    "publication_date": "1983-09-01",
    "publication_year": 1983,
    "authors": "Alfonso F. Cárdenas; Farid Alavian; Algirdas Avižienis",
    "corresponding_authors": "",
    "abstract": "The need for robust recovery facilities in modern database management systems is quite well known. Various authors have addressed recovery facilities and specific techniques, but none have delved into the problem of recovery in database machines. In this paper, the types of undesirable events that occur in a database environment are classified and the necessary recovery information, with subsequent actions to recover the correct state of the database, is summarized. A model of the “processor-per-track” class of parallel associative database processor is presented. Three different types of recovery mechanisms that may be considered for parallel associative database processors are identified. For each architecture, both the workload imposed by the recovery mechanisms on the execution of database operations (i.e., retrieve, modify, delete, and insert) and the workload involved in the recovery actions (i.e., rollback, restart, restore, and reconstruct) are analyzed. The performance of the three architectures is quantitatively compared. This comparison is made in terms of the number of extra revolutions of the database area required to process a transaction versus the number of records affected by a transaction. A variety of different design parameters of the database processor, of the database, and of a mix of transaction types (modify, insert, and delete) are considered. A large number of combinations is selected and the effects of the parameters on the extra processing time are identified.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1981207589",
    "type": "article"
  },
  {
    "title": "Shadowed management of free disk pages with a linked list",
    "doi": "https://doi.org/10.1145/319996.320002",
    "publication_date": "1983-12-01",
    "publication_year": 1983,
    "authors": "Matthew S. Hecht; John D. Gabbe",
    "corresponding_authors": "",
    "abstract": "We describe and prove correct a programming technique using a linked list of pages for managing the free disk pages of a file system where shadowing is the recovery technique. Our technique requires a window of only two pages of main memory for accessing and maintaining the free list, and avoids wholesale copying of free-list pages during a checkpoint or recover operation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2074767330",
    "type": "article"
  },
  {
    "title": "Scaling up output capacity and performance results from information systems prototypes",
    "doi": "https://doi.org/10.1145/88636.87943",
    "publication_date": "1990-09-01",
    "publication_year": 1990,
    "authors": "J. Christopher Westland",
    "corresponding_authors": "J. Christopher Westland",
    "abstract": "The advantage of information system prototyping arises from its predict problems and end-user satisfaction with a system early in the development process, before significant commitments of time and effort have been made. Predictions of problems and end-user satisfaction have risen in importance with the increasing complexity of business information systems and the exponential growth of database size. This research investigates the reporting of information to an end user, and the process of inferring from a prototype to a full-scale information system. This inference is called scaling up , and is an important part of the systems development planning process. The research investigates information systems reporting from a linguistic perspective, where a database is used as a central receptacle for information storage. It then investigates the manner in which reporting statistics from the prototype information system may be used to infer the behavior and performance of the full-scale system. An example is presented for the application of the algorithm, and the final section discusses the usefulness, application, and implications of the algorithm developed in this research.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2161447035",
    "type": "article"
  },
  {
    "title": "An extended owner-coupled set data model and predicate calculus for database management",
    "doi": "https://doi.org/10.1145/320289.320294",
    "publication_date": "1978-12-01",
    "publication_year": 1978,
    "authors": "James Bradley",
    "corresponding_authors": "James Bradley",
    "abstract": "A data model is presented, based on the extension of the concept of a DBTG owner-coupled set to permit static and dynamic sets and a new kind of set referred to as a virtual set. The notion of connection fields is introduced, and it is shown how connection fields may be used to construct derived information bearing set names, and hence permit the specification of (dynamic) sets which are not predeclared in a schema. Virtual sets are shown to reflect the functional dependencies which can exist within a file. A technique which permits the data model to be fully described diagrammatically by extended Bachman diagrams is described. A predicate calculus for manipulation of this data model is presented. Expressions written in this calculus are compared with corresponding expressions in a relational predicate calculus, DSL ALPHA. An argument for the relational completeness of the language is given.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2032077869",
    "type": "article"
  },
  {
    "title": "Maintaining stream statistics over multiscale sliding windows",
    "doi": "https://doi.org/10.1145/1189769.1189773",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Yishan Jiao",
    "corresponding_authors": "Yishan Jiao",
    "abstract": "In this article, we propose a new multiscale sliding window model which differentiates data items in different time periods of the data stream, based on a reasonable monotonicity of resolution assumption. Our model, as a well-motivated extension of the sliding window model, stands halfway between the traditional all-history and time-decaying models. We also present algorithms for estimating two significant data stream statistics--- F 0 and Jacard's similarity coefficient---with reasonable accuracies under the new model. Our algorithms use space logarithmic in the data stream size and linear in the number of windows; they support update time logarithmic in the number of windows and independent of the accuracy required. Our algorithms are easy to implement. Experimental results demonstrate the efficiencies of our algorithms. Our techniques apply to scenarios in which universe sampling is used.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2151072977",
    "type": "article"
  },
  {
    "title": "Bounded Query Rewriting Using Views",
    "doi": "https://doi.org/10.1145/3183673",
    "publication_date": "2018-03-23",
    "publication_year": 2018,
    "authors": "Yang Cao; Wenfei Fan; Floris Geerts; Ping Lü",
    "corresponding_authors": "",
    "abstract": "A query Q in a language L has a bounded rewriting using a set of L-definable views if there exists a query Q ′ in L such that given any dataset D , Q ( D ) can be computed by Q ′ that accesses only cached views and a small fraction D Q of D . We consider datasets D that satisfy a set of access constraints, which are a combination of simple cardinality constraints and associated indices, such that the size | D Q | of D Q and the time to identify D Q are independent of | D |, no matter how big D is. In this article, we study the problem for deciding whether a query has a bounded rewriting given a set V of views and a set A of access constraints. We establish the complexity of the problem for various query languages L , from Σ 3 p -complete for conjunctive queries (CQ) to undecidable for relational algebra (FO). We show that the intractability for CQ is rather robust even for acyclic CQ with fixed V and A , and characterize when the problem is in PTIME. To make practical use of bounded rewriting, we provide an effective syntax for FO queries that have a bounded rewriting. The syntax characterizes a key subclass of such queries without sacrificing the expressive power, and can be checked in PTIME. Finally, we investigate L 1 -to- L 2 bounded rewriting, when Q in L 1 is allowed to be rewritten into a query Q ′ in another language L 2 . We show that this relaxation does not simplify the analysis of bounded query rewriting using views.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2972481425",
    "type": "article"
  },
  {
    "title": "General Temporally Biased Sampling Schemes for Online Model Management",
    "doi": "https://doi.org/10.1145/3360903",
    "publication_date": "2019-12-06",
    "publication_year": 2019,
    "authors": "Brian Hentschel; Peter J. Haas; Yuanyuan Tian",
    "corresponding_authors": "",
    "abstract": "To maintain the accuracy of supervised learning models in the presence of evolving data streams, we provide temporally biased sampling schemes that weight recent data most heavily, with inclusion probabilities for a given data item decaying over time according to a specified “decay function.” We then periodically retrain the models on the current sample. This approach speeds up the training process relative to training on all of the data. Moreover, time-biasing lets the models adapt to recent changes in the data while—unlike in a sliding-window approach—still keeping some old data to ensure robustness in the face of temporary fluctuations and periodicities in the data values. In addition, the sampling-based approach allows existing analytic algorithms for static data to be applied to dynamic streaming data essentially without change. We provide and analyze both a simple sampling scheme (Targeted-Size Time-Biased Sampling (T-TBS)) that probabilistically maintains a target sample size and a novel reservoir-based scheme (Reservoir-Based Time-Biased Sampling (R-TBS)) that is the first to provide both control over the decay rate and a guaranteed upper bound on the sample size. If the decay function is exponential, then control over the decay rate is complete, and R-TBS maximizes both expected sample size and sample-size stability. For general decay functions, the actual item inclusion probabilities can be made arbitrarily close to the nominal probabilities, and we provide a scheme that allows a tradeoff between sample footprint and sample-size stability. R-TBS rests on the notion of a “fractional sample” and allows for data arrival rates that are unknown and time varying (unlike T-TBS). The R-TBS and T-TBS schemes are of independent interest, extending the known set of unequal-probability sampling schemes. We discuss distributed implementation strategies; experiments in Spark illuminate the performance and scalability of the algorithms, and show that our approach can increase machine learning robustness in the face of evolving data.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3005031200",
    "type": "article"
  },
  {
    "title": "Query evaluation in deductive databases with alternating fixpoint semantics",
    "doi": "https://doi.org/10.1145/211414.211416",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Weidong Chen",
    "corresponding_authors": "Weidong Chen",
    "abstract": "First-order formulas allow natural descriptions of queries and rules. Van Gelder's alternating fixpoint semantics extends the well-founded semantics of normal logic programs to general logic programs with arbitrary first-order formulas in rule bodies. However, an implementation of general logic programs through the standard translation into normal logic programs does not preserve the alternating fixpoint semantics. This paper presents a direct method for goal-oriented query evaluation of general logic programs. Every general logic program is first transformed into a normal form where the body of each rule is either an existential conjunction of literals or a universal disjunction of literals. Techniques of memoing and loop checking are incorporated so that termination and polynomial-time data complexity are guaranteed for deductive databases (or function-free programs). Results of the soundness and search space completeness are established.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1969477971",
    "type": "article"
  },
  {
    "title": "Towards a theory of cost management for digital libraries and electronic commerce",
    "doi": "https://doi.org/10.1145/296854.277641",
    "publication_date": "1998-12-01",
    "publication_year": 1998,
    "authors": "A. Prasad Sistla; Ouri Wolfson; Yelena Yesha; Robert H. Sloan",
    "corresponding_authors": "",
    "abstract": "One of the features that distinguishes digital libraries from traditional databases is new cost models for client access to intellectual property. Clients will pay for accessing data items in digital libraries, and we believe that optimizing these costs will be as important as optimizing performance in traditional databases. In this article we discuss cost models and protocols for accessing digital libraries, with the objective of determining the minimum cost protocol for each model. We expect that in the future information appliances will come equipped with a cost optimizer, in the same way that computers today come with a built-in operating system. This article makes the initial steps towards a thery and practice of intellectual property cost management.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2075963519",
    "type": "article"
  },
  {
    "title": "Removing permissions in the flexible authorization framework",
    "doi": "https://doi.org/10.1145/937598.937599",
    "publication_date": "2003-09-01",
    "publication_year": 2003,
    "authors": "Duminda Wijesekera; Sushil Jajodia; Francesco Parisi-Presicce; A.L. Hagström",
    "corresponding_authors": "",
    "abstract": "The Flexible Authorization Framework (FAF) defined by Jajodia et al. [2001] provides a policy-neutral framework for specifying access control policies that is expressive enough to specify many known access control policies. Although the original formulation of FAF indicated how rules could be added to or deleted from a FAF specification, it did not address the removal of access permissions from users. We present two options for removing permissions in FAF and provide details on the option which is representation independent.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1995626385",
    "type": "article"
  },
  {
    "title": "Validation algorithms for pointer values in DBTG databases",
    "doi": "https://doi.org/10.1145/320576.320589",
    "publication_date": "1977-12-01",
    "publication_year": 1977,
    "authors": "D. A. Thomas; B. Pagurek; R. J. A. Buhr",
    "corresponding_authors": "",
    "abstract": "This paper develops algorithms for verifying pointer values in DBTG (Data Base Task Group) type databases. To validate pointer implemented access paths and set structures, two algorithms are developed. The first procedure exploits the “typed pointer” concept employed in modern programming languages to diagnose abnormalities in directories and set instances. The second algorithm completes pointer validation by examining set instances to ensure that each DBTG set has a unique owner. Sequential processing is used by both algorithms, allowing a straightforward implementation which is efficient in both time and space. As presented, the algorithms are independent of implementation schema and physical structure.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2067405449",
    "type": "article"
  },
  {
    "title": "Data file management in shift-register memories",
    "doi": "https://doi.org/10.1145/320251.320256",
    "publication_date": "1978-06-01",
    "publication_year": 1978,
    "authors": "Werner Kluge",
    "corresponding_authors": "Werner Kluge",
    "abstract": "The paper proposes a shift-register memory, structured as a two-dimensional array of uniform shift-register loops which are linked by flow-steering switches, whose switch control scheme is tailored to perform with great efficiency data management operations on sequentially organized files. The memory operates in a linear input/output mode to perform record insertion, deletion, and relocation on an existing file, and in a sublinear mode for rapid internal file movement to expedite file positioning and record retrieval and update operations. The memory, implemented as a large capacity charge-coupled device or magnetic domain memory, permits efficient data management on very large databases at the level of secondary storage and lends itself to applications as a universal disk replacement, particularly in database computers.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2131298490",
    "type": "article"
  },
  {
    "title": "Extracting predicates from mining models for efficient query evaluation",
    "doi": "https://doi.org/10.1145/1016028.1016031",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Surajit Chaudhuri; Vivek Narasayya; Sunita Sarawagi",
    "corresponding_authors": "",
    "abstract": "Modern relational database systems are beginning to support ad hoc queries on mining models. In this article, we explore novel techniques for optimizing queries that contain predicates on the results of application of mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for a large class of popular discrete mining models: decision trees, naive Bayes, clustering and linear support vector machines. Our experiments on Microsoft SQL Server demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2082002584",
    "type": "article"
  },
  {
    "title": "An Empirical Study of Moment Estimators for Quantile Approximation",
    "doi": "https://doi.org/10.1145/3442337",
    "publication_date": "2021-03-18",
    "publication_year": 2021,
    "authors": "Rory Mitchell; Eibe Frank; Geoffrey Holmes",
    "corresponding_authors": "",
    "abstract": "We empirically evaluate lightweight moment estimators for the single-pass quantile approximation problem, including maximum entropy methods and orthogonal series with Fourier, Cosine, Legendre, Chebyshev and Hermite basis functions. We show how to apply stable summation formulas to offset numerical precision issues for higher-order moments, leading to reliable single-pass moment estimators up to order 15. Additionally, we provide an algorithm for GPU-accelerated quantile approximation based on parallel tree reduction. Experiments evaluate the accuracy and runtime of moment estimators against the state-of-the-art KLL quantile estimator on 14,072 real-world datasets drawn from the OpenML database. Our analysis highlights the effectiveness of variants of moment-based quantile approximation for highly space efficient summaries: their average performance using as few as five sample moments can approach the performance of a KLL sketch containing 500 elements. Experiments also illustrate the difficulty of applying the method reliably and showcases which moment-based approximations can be expected to fail or perform poorly.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3136306008",
    "type": "article"
  },
  {
    "title": "On Finding Rank Regret Representatives",
    "doi": "https://doi.org/10.1145/3531054",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Abolfazl Asudeh; Gautam Das; H. V. Jagadish; Shangqi Lu; Azade Nazi; Yufei Tao; Nan Zhang; Jianwen Zhao",
    "corresponding_authors": "",
    "abstract": "Selecting the best items in a dataset is a common task in data exploration. However, the concept of “best” lies in the eyes of the beholder: Different users may consider different attributes more important and, hence, arrive at different rankings. Nevertheless, one can remove “dominated” items and create a “representative” subset of the data, comprising the “best items” in it. A Pareto-optimal representative is guaranteed to contain the best item of each possible ranking, but it can be a large portion of data. A much smaller representative can be found if we relax the requirement of including the best item for each user and instead just limit the users’ “regret.” Existing work defines regret as the loss in score by limiting consideration to the representative instead of the full dataset, for any chosen ranking function. However, the score is often not a meaningful number, and users may not understand its absolute value. Sometimes small ranges in score can include large fractions of the dataset. In contrast, users do understand the notion of rank ordering. Therefore, we consider items’ positions in the ranked list in defining the regret and propose the rank-regret representative as the minimal subset of the data containing at least one of the top- k of any possible ranking function. This problem is polynomial time solvable in two-dimensional space but is NP-hard on three or more dimensions. We design a suite of algorithms to fulfill different purposes, such as whether relaxation is permitted on k , the result size, or both, whether a distribution is known, whether theoretical guarantees or practical efficiency is important, and so on. Experiments on real datasets demonstrate that we can efficiently find small subsets with small rank-regrets.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4292484539",
    "type": "article"
  },
  {
    "title": "Constant-time-maintainable BCNF database schemes",
    "doi": "https://doi.org/10.1145/115302.115301",
    "publication_date": "1991-12-01",
    "publication_year": 1991,
    "authors": "Héctor J. Hernández; Edward P. F. Chan",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Constant-time-maintainable BCNF database schemes Authors: Héctor J. Hernández New Mexico State Univ., Las Cruces New Mexico State Univ., Las CrucesView Profile , Edward P. F. Chan Univ. of Waterloo, Waterloo, Ont., Canada Univ. of Waterloo, Waterloo, Ont., CanadaView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 16Issue 4Dec. 1991 pp 571–599https://doi.org/10.1145/115302.115301Online:01 December 1991Publication History 4citation356DownloadsMetricsTotal Citations4Total Downloads356Last 12 Months13Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1989522453",
    "type": "article"
  },
  {
    "title": "A dynamic hash method with signature",
    "doi": "https://doi.org/10.1145/114325.103714",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "F. Cesarini; G. Soda",
    "corresponding_authors": "",
    "abstract": "We present a dynamic external hash method that allows retrieval of a record by only one access to mass storage while maintaining a high load factor. The hash function is based on generalized spiral storage. Both primary and overflow records are allocated to the same file, and file expansion depends on being able to allocate every overflow chain to one bucket. An in-core index, built by means of a signature function, discriminates between primary and overflow records and assures one access to storage in the case of either successful or unsuccessful searching. Simulation results confirm the good expected performance.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2025864627",
    "type": "article"
  },
  {
    "title": "A relation-based language interpreter for a content addressable file store",
    "doi": "https://doi.org/10.1145/319702.319705",
    "publication_date": "1982-06-01",
    "publication_year": 1982,
    "authors": "T. R. Addis",
    "corresponding_authors": "T. R. Addis",
    "abstract": "The combination of the Content Addressable File Store (CAFS®; CAFS is a registered trademark of International Computers Limited) and an extension of relational analysis is described. This combination allows a simple and compact implementation of a database query and update language (FIDL). The language has one of the important properties of a “natural” language interface by using a “world model” derived from the relational analysis. The interpreter (FLIN) takes full advantage of the CAFS by employing a unique database storage technique which results in a fast response to both queries and updates.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1993602087",
    "type": "article"
  },
  {
    "title": "Synthesis of Incremental Linear Algebra Programs",
    "doi": "https://doi.org/10.1145/3385398",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Amir Shaikhha; Mohammed Elseidy; Stephan Mihaila; Daniel Espino; Christoph Koch",
    "corresponding_authors": "",
    "abstract": "This article targets the Incremental View Maintenance (IVM) of sophisticated analytics (such as statistical models, machine learning programs, and graph algorithms) expressed as linear algebra programs. We present LAGO, a unified framework for linear algebra that automatically synthesizes efficient incremental trigger programs, thereby freeing the user from error-prone manual derivations, performance tuning, and low-level implementation details. The key technique underlying our framework is abstract interpretation, which is used to infer various properties of analytical programs. These properties give the reasoning power required for the automatic synthesis of efficient incremental triggers. We evaluate the effectiveness of our framework on a wide range of applications from regression models to graph computations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3041543526",
    "type": "article"
  },
  {
    "title": "Workload-Driven Antijoin Cardinality Estimation",
    "doi": "https://doi.org/10.1145/2818178",
    "publication_date": "2015-10-23",
    "publication_year": 2015,
    "authors": "Florin Rusu; Zixuan Zhuang; Mingxi Wu; Chris Jermaine",
    "corresponding_authors": "",
    "abstract": "Antijoin cardinality estimation is among a handful of problems that has eluded accurate efficient solutions amenable to implementation in relational query optimizers. Given the widespread use of antijoin and subset-based queries in analytical workloads and the extensive research targeted at join cardinality estimation—a seemingly related problem—the lack of adequate solutions for antijoin cardinality estimation is intriguing. In this article, we introduce a novel sampling-based estimator for antijoin cardinality that (unlike existent estimators) provides sufficient accuracy and efficiency to be implemented in a query optimizer. The proposed estimator incorporates three novel ideas. First, we use prior workload information when learning a mixture superpopulation model of the data offline. Second, we design a Bayesian statistics framework that updates the superpopulation model according to the live queries, thus allowing the estimator to adapt dynamically to the online workload. Third, we develop an efficient algorithm for sampling from a hypergeometric distribution in order to generate Monte Carlo trials , without explicitly instantiating either the population or the sample. When put together, these ideas form the basis of an efficient antijoin cardinality estimator satisfying the strict requirements of a query optimizer, as shown by the extensive experimental results over synthetically-generated as well as massive TPC-H data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1974421547",
    "type": "article"
  },
  {
    "title": "UniAD",
    "doi": "https://doi.org/10.1145/3009957",
    "publication_date": "2016-11-21",
    "publication_year": 2016,
    "authors": "Xiaogang Shi; Bin Cui; Gillian Dobbie; Beng Chin Ooi",
    "corresponding_authors": "",
    "abstract": "Instead of constructing complex declarative queries, many users prefer to write their programs using procedural code embedded with simple queries. Since many users are not expert programmers or the programs are written in a rush, these programs usually exhibit poor performance in practice and it is a challenge to automatically and efficiently optimize these programs. In this article, we present UniAD, which stands for Uni fied execution for Ad hoc Data processing, a system designed to simplify the programming of data processing tasks and provide efficient execution for user programs. We provide the background of program semantics and propose a novel intermediate representation, called Unified Intermediate Representation (UniIR), which utilizes a simple and expressive mechanism HOQ to describe the operations performed in programs. By combining both procedural and declarative logics with the proposed intermediate representation, we can perform various optimizations across the boundary between procedural and declarative code. We propose a transformation-based optimizer to automatically optimize programs and implement the UniAD system. The extensive experimental results on various benchmarks demonstrate that our techniques can significantly improve the performance of a wide range of data processing programs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2554552572",
    "type": "article"
  },
  {
    "title": "Database hosting in strongly-typed programming languages",
    "doi": "https://doi.org/10.1145/3148.3327",
    "publication_date": "1985-03-01",
    "publication_year": 1985,
    "authors": "Martin Bever; Peter C. Lockemann",
    "corresponding_authors": "",
    "abstract": "Database system support has become an essential part of many computer applications, which have extended beyond the more traditional commercial applications to, among others, engineering applications. Correspondingly, application programming with the need to access databases has progressively shifted to scientifically oriented languages. Modern developments in these languages are characterized by advanced mechanisms for the liberal declaration of data types, for type checking, and facilities for modularization of large programs. The present paper examines how a DBMS can be accessed from such a language in a way that conforms to its syntax and utilizes its type-checking facilities, without modifying the language specification itself, and hence its compilers. The basic idea is to rely on facilities for defining modules as separately compilable units, and to use these to declare user-defined abstract data types. The idea is demonstrated by an experiment in which a specific DBMS (ADABAS) is hosted in the programming language (LIS). The paper outlines a number of approaches and their problems, shows how to embed the DML into LIS, and how a more user-oriented DML can be provided in LIS.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2043275201",
    "type": "article"
  },
  {
    "title": "A transformational grammar-based query processor for access control in a planning system",
    "doi": "https://doi.org/10.1145/320576.320582",
    "publication_date": "1977-12-01",
    "publication_year": 1977,
    "authors": "ROBERT H. BONCZEK; James I. Cash; Andrew B. Whinston",
    "corresponding_authors": "",
    "abstract": "Providing computer facilities and data availability to larger numbers of users generates increased system vulnerability which is partially offset by software security systems. Much too often these systems are presented as ad hoc additions to the basic data management system. One very important constituent of software security systems is the access control mechanism which may be the last resource available to prohibit unauthorized data retrieval. This paper presents a specification for an access control mechanism. The mechanism is specified in a context for use with the GPLAN decision support system by a theoretical description consistent with the formal definition of GPLAN's query language. Incorporation of the mechanism into the language guarantees it will not be an ad hoc addition. Furthermore, it provides a facile introduction of data security dictates into the language processor.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1965549219",
    "type": "article"
  },
  {
    "title": "Comments on “an integrated efficient solution for computing frequent and top-k elements in data streams”",
    "doi": "https://doi.org/10.1145/1735886.1735894",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Hongyan Liu; Xiaoyu Wang; Yinghui Yang",
    "corresponding_authors": "",
    "abstract": "We investigate a well-known algorithm, Space-Saving [Metwally et al. 2006], which has been proven efficient and effective at mining frequent elements in data streams. We discovered an error in one of the theorems in Metwally et al. [2006]. Experiments are conducted to illustrate the error.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2094641628",
    "type": "article"
  },
  {
    "title": "Tail recursion elimination in deductive databases",
    "doi": "https://doi.org/10.1145/232616.232628",
    "publication_date": "1996-06-01",
    "publication_year": 1996,
    "authors": "Kenneth A. Ross",
    "corresponding_authors": "Kenneth A. Ross",
    "abstract": "We consider an optimization technique for deductive and relational databases. The optimization technique is an extension of the magic templates rewriting, and it can improve the performance of query evaluation by not materializing the extension of intermediate views. Standard relational techniques, such as unfolding embedded view definitions, do not apply to recursively defined views, and so alternative techniques are necessary. We demonstrate the correctness of our rewriting. We define a class of “nonrepeating” view definitions, and show that for certain queries our rewriting performs at least as well as magic templates on nonrepeating views, and often much better. A syntactically recognizable property, called “weak right-linearity”, is proposed. Weak right-linearity is a sufficient condition for nonrepetition and is more general than right-linearity. Our technique gives the same benefits as right-linear evaluation of right-linear views, while applying to a significantly more general class of views.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2039063067",
    "type": "article"
  },
  {
    "title": "Comments on SDD-1 concurrency control mechanisms",
    "doi": "https://doi.org/10.1145/319566.319585",
    "publication_date": "1981-06-01",
    "publication_year": 1981,
    "authors": "Gordon McLean",
    "corresponding_authors": "Gordon McLean",
    "abstract": "article Free Access Share on Comments on SDD-1 concurrency control mechanisms Author: Gordon McLean Prime Computer, Inc., Framingham, MA Prime Computer, Inc., Framingham, MAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 6Issue 2pp 347–350https://doi.org/10.1145/319566.319585Published:01 June 1981Publication History 4citation302DownloadsMetricsTotal Citations4Total Downloads302Last 12 Months9Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2067351850",
    "type": "article"
  },
  {
    "title": "A database management facility for automatic generation of database managers",
    "doi": "https://doi.org/10.1145/320434.320452",
    "publication_date": "1976-03-01",
    "publication_year": 1976,
    "authors": "David Stemple",
    "corresponding_authors": "David Stemple",
    "abstract": "A facility is described for the implementation of database management systems having high degrees of horizontal data independence, i.e. independence from chosen logical properties of a database as opposed to vertical independence from storage structures. The facility consists of a high level language for the specification of virtual database managers, a compiler from this language to a pseudomachine language, and an interpreter for the pseudomachine language. It is shown how this facility can be used to produce efficient database management systems with any degree of both horizontal and vertical data independence. Two key features of this tool are the compilation of tailored database managers from individual schemas and multiple levels of optional binding.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4252550013",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1132863",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Bitmap indices are efficient for answering queries on low-cardinality attributes. In this article, we present a new compression scheme called Word-Aligned Hybrid (WAH) code that makes compressed bitmap indices efficient even for high-cardinality ...",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4254961563",
    "type": "paratext"
  },
  {
    "title": "Learning From Query-Answers",
    "doi": "https://doi.org/10.1145/3277503",
    "publication_date": "2018-12-08",
    "publication_year": 2018,
    "authors": "Niccolò Meneghetti; Oliver Kennedy; Wolfgang Gatterbauer",
    "corresponding_authors": "",
    "abstract": "Tuple-independent and disjoint-independent probabilistic databases (TI- and DI-PDBs) represent uncertain data in a factorized form as a product of independent random variables that represent either tuples (TI-PDBs) or sets of tuples (DI-PDBs). When the user submits a query, the database derives the marginal probabilities of each output-tuple, exploiting the underlying assumptions of statistical independence. While query processing in TI- and DI-PDBs has been studied extensively, limited research has been dedicated to the problems of updating or deriving the parameters from observations of query results . Addressing this problem is the main focus of this article. We first introduce Beta Probabilistic Databases (B-PDBs), a generalization of TI-PDBs designed to support both (i) belief updating and (ii) parameter learning in a principled and scalable way. The key idea of B-PDBs is to treat each parameter as a latent, Beta-distributed random variable. We show how this simple expedient enables both belief updating and parameter learning in a principled way, without imposing any burden on regular query processing. Building on B-PDBs, we then introduce Dirichlet Probabilistic Databases (D-PDBs), a generalization of DI-PDBs with similar properties. We provide the following key contributions for both B- and D-PDBs: (i) We study the complexity of performing Bayesian belief updates and devise efficient algorithms for certain tractable classes of queries; (ii) we propose a soft-EM algorithm for computing maximum-likelihood estimates of the parameters; (iii) we present an algorithm for efficiently computing conditional probabilities, allowing us to efficiently implement B- and D-PDBs via a standard relational engine; and (iv) we support our conclusions with extensive experimental results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2905458849",
    "type": "article"
  },
  {
    "title": "Inferring Insertion Times and Optimizing Error Penalties in Time-decaying Bloom Filters",
    "doi": "https://doi.org/10.1145/3284552",
    "publication_date": "2019-03-15",
    "publication_year": 2019,
    "authors": "Jonathan L. Dautrich; Chinya V. Ravishankar",
    "corresponding_authors": "",
    "abstract": "Current Bloom Filters tend to ignore Bayesian priors as well as a great deal of useful information they hold, compromising the accuracy of their responses. Incorrect responses cause users to incur penalties that are both application- and item-specific, but current Bloom Filters are typically tuned only for static penalties. Such shortcomings are problematic for all Bloom Filter variants, but especially so for Time-decaying Bloom Filters, in which the memory of older items decays over time, causing both false positives and false negatives. We address these issues by introducing inferential filters, which integrate Bayesian priors and information latent in filters to make penalty-optimal, query-specific decisions. We also show how to properly infer insertion times in such filters. Our methods are general, but here we illustrate their application to inferential time-decaying filters to support novel query types and sliding window queries with dynamic error penalties. We present inferential versions of the Timing Bloom Filter and Generalized Bloom Filter. Our experiments on real and synthetic datasets show that our methods reduce penalties for incorrect responses to sliding-window queries in these filters by up to 70% when penalties are dynamic.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2922133373",
    "type": "article"
  },
  {
    "title": "ϵ <scp>KTELO</scp>",
    "doi": "https://doi.org/10.1145/3362032",
    "publication_date": "2020-02-08",
    "publication_year": 2020,
    "authors": "Dan Zhang; Ryan McKenna; Ios Kotsogiannis; George Bissias; Michael Hay; Ashwin Machanavajjhala; Gerome Miklau",
    "corresponding_authors": "",
    "abstract": "The adoption of differential privacy is growing, but the complexity of designing private, efficient, and accurate algorithms is still high. We propose a novel programming framework and system, ϵ KTELO for implementing both existing and new privacy algorithms. For the task of answering linear counting queries, we show that nearly all existing algorithms can be composed from operators, each conforming to one of a small number of operator classes. While past programming frameworks have helped to ensure the privacy of programs, the novelty of our framework is its significant support for authoring accurate and efficient (as well as private) programs. After describing the design and architecture of the ϵ KTELO system, we show that ϵ KTELO is expressive, allows for safer implementations through code reuse, and allows both privacy novices and experts to easily design algorithms. We provide a number of novel implementation techniques to support the generality and scalability of ϵ KTELO operators. These include methods to automatically compute lossless reductions of the data representation, implicit matrices that avoid materialized state but still support computations, and iterative inference implementations that generalize techniques from the privacy literature. We demonstrate the utility of ϵ KTELO by designing several new state-of-the-art algorithms, most of which result from simple re-combinations of operators defined in the framework. We study the accuracy and scalability of ϵ KTELO plans in a thorough empirical evaluation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3005418108",
    "type": "article"
  },
  {
    "title": "Mining Order-preserving Submatrices under Data Uncertainty: A Possible-world Approach and Efficient Approximation Methods",
    "doi": "https://doi.org/10.1145/3524915",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Ji Cheng; Da Yan; Wenwen Qu; Xiaotian Hao; Cheng Long; Wilfred Ng; Xiaoling Wang",
    "corresponding_authors": "",
    "abstract": "Given a data matrix \\( D \\) , a submatrix \\( S \\) of \\( D \\) is an order-preserving submatrix (OPSM) if there is a permutation of the columns of \\( S \\) , under which the entry values of each row in \\( S \\) are strictly increasing. OPSM mining is widely used in real-life applications such as identifying coexpressed genes and finding customers with similar preference. However, noise is ubiquitous in real data matrices due to variable experimental conditions and measurement errors, which makes conventional OPSM mining algorithms inapplicable. No previous work on OPSM has ever considered uncertain value intervals using the well-established possible world semantics. We establish two different definitions of significant OPSMs based on the possible world semantics : (1) expected support-based and (2) probabilistic frequentness-based. An optimized dynamic programming approach is proposed to compute the probability that a row supports a particular column permutation, with a closed-form formula derived to efficiently handle the special case of uniform value distribution and an accurate cubic spline approximation approach that works well with any uncertain value distributions. To efficiently check the probabilistic frequentness, several effective pruning rules are designed to efficiently prune insignificant OPSMs; two approximation techniques based on the Poisson and Gaussian distributions, respectively, are proposed for further speedup. These techniques are integrated into our two OPSM mining algorithms, based on prefix-projection and Apriori, respectively. We further parallelize our prefix-projection-based mining algorithm using PrefixFPM, a recently proposed framework for parallel frequent pattern mining, and we achieve a good speedup with the number of CPU cores. Extensive experiments on real microarray data demonstrate that the OPSMs found by our algorithms have a much higher quality than those found by existing approaches.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4221137048",
    "type": "article"
  },
  {
    "title": "Persistent Summaries",
    "doi": "https://doi.org/10.1145/3531053",
    "publication_date": "2022-05-23",
    "publication_year": 2022,
    "authors": "Tianjing Zeng; Zhewei Wei; Ge Luo; Ke Yi; Xiaoyong Du; Ji-Rong Wen",
    "corresponding_authors": "",
    "abstract": "A persistent data structure , also known as a multiversion data structure in the database literature, is a data structure that preserves all its previous versions as it is updated over time. Every update (inserting, deleting, or changing a data record) to the data structure creates a new version, while all the versions are kept in the data structure so that any previous version can still be queried. Persistent data structures aim at recording all versions accurately, which results in a space requirement that is at least linear to the number of updates. In many of today’s big data applications, in particular, for high-speed streaming data, the volume and velocity of the data are so high that we cannot afford to store everything. Therefore, streaming algorithms have received a lot of attention in the research community, which uses only sublinear space by sacrificing slightly on accuracy. All streaming algorithms work by maintaining a small data structure in memory, which is usually called a sketch , summary , or synopsis . The summary is updated upon the arrival of every element in the stream, thus it is ephemeral , meaning that it can only answer queries about the current status of the stream. In this article, we aim at designing persistent summaries, thereby giving streaming algorithms the ability to answer queries about the stream at any prior time.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4281390594",
    "type": "article"
  },
  {
    "title": "Deciding Robustness for Lower SQL Isolation Levels",
    "doi": "https://doi.org/10.1145/3561049",
    "publication_date": "2022-09-03",
    "publication_year": 2022,
    "authors": "Bas Ketsman; Christoph Koch; Frank Neven; Brecht Vandevoort",
    "corresponding_authors": "",
    "abstract": "While serializability always guarantees application correctness, lower isolation levels can be chosen to improve transaction throughput at the risk of introducing certain anomalies. A set of transactions is robust against a given isolation level if every possible interleaving of the transactions under the specified isolation level is serializable. Robustness therefore always guarantees application correctness with the performance benefit of the lower isolation level. While the robustness problem has received considerable attention in the literature, only sufficient conditions have been obtained. The most notable exception is the seminal work by Fekete where he obtained a characterization for deciding robustness against SNAPSHOT ISOLATION. In this article, we address the robustness problem for the lower SQL isolation levels READ UNCOMMITTED and READ COMMITTED, which are defined in terms of the forbidden dirty write and dirty read patterns. The first main contribution of this article is that we characterize robustness against both isolation levels in terms of the absence of counter-example schedules of a specific form (split and multi-split schedules) and by the absence of cycles in interference graphs that satisfy various properties. A critical difference with Fekete’s work, is that the properties of cycles obtained in this article have to take the relative ordering of operations within transactions into account as READ UNCOMMITTED and READ COMMITTED do not satisfy the atomic visibility requirement. A particular consequence is that the latter renders the robustness problem against READ COMMITTED coNP-complete. The second main contribution of this article is the coNP-hardness proof. For READ UNCOMMITTED, we obtain LOGSPACE-completeness.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4294408690",
    "type": "article"
  },
  {
    "title": "Proximity Queries on Terrain Surface",
    "doi": "https://doi.org/10.1145/3563773",
    "publication_date": "2022-09-16",
    "publication_year": 2022,
    "authors": "Victor Junqiu Wei; Raymond Chi-Wing Wong; Cheng Long; David M. Mount; Hanan Samet",
    "corresponding_authors": "",
    "abstract": "Due to the advance of the geo-spatial positioning and the computer graphics technology, digital terrain data has become increasingly popular nowadays. Query processing on terrain data has attracted considerable attention from both the academic and the industry communities. Proximity queries such as the shortest path/distance query, k nearest/farthest neighbor query, and top- k closest/farthest pairs query are fundamental and important queries in the context of the terrain surfaces, and they have a lot of applications in Geographical Information System, 3D object feature vector construction, and 3D object data mining. In this article, we first study the most fundamental type of query, namely, shortest distance and path query, which is to find the shortest distance and path between two points of interest on the surface of the terrain. As observed by existing studies, computing the exact shortest distance/path is very expensive. Some existing studies proposed ϵ -approximate distance and path oracles, where ϵ is a non-negative real-valued error parameter. However, the best-known algorithm has a large oracle construction time, a large oracle size, and a large query time. Motivated by this, we propose a novel ϵ -approximate distance and path oracle called the S pace E fficient distance and path oracle (SE), which has a small oracle construction time, a small oracle size, and a small distance and path query time, thanks to its compactness of storing concise information about pairwise distances between any two points-of-interest. Then, we propose several algorithms for the k nearest/farthest neighbor and top- k closest/farthest pairs queries with the assistance of our distance and path oracle SE . Our experimental results show that the oracle construction time, the oracle size, and the distance and path query time of SE are up to two, three, and five orders of magnitude faster than the best-known algorithm, respectively. Besides, our algorithms for other proximity queries including k nearest/farthest neighbor queries and top- k closest/farthest pairs queries significantly outperform the state-of-the-art algorithms by up to two orders of magnitude.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4296058488",
    "type": "article"
  },
  {
    "title": "Robust and Efficient Sorting with Offset-value Coding",
    "doi": "https://doi.org/10.1145/3570956",
    "publication_date": "2022-11-11",
    "publication_year": 2022,
    "authors": "Thanh Do; Goetz Graefe",
    "corresponding_authors": "",
    "abstract": "Sorting and searching are large parts of database query processing, e.g., in the forms of index creation, index maintenance, and index lookup, and comparing pairs of keys is a substantial part of the effort in sorting and searching. We have worked on simple, efficient implementations of decades-old, neglected, effective techniques for fast comparisons and fast sorting, in particular offset-value coding. In the process, we happened upon its mutually beneficial relationship with prefix truncation in run files as well as the duality of compression techniques in row- and column-format storage structures, namely prefix truncation and run-length encoding of leading key columns. We also found a beneficial relationship with consumers of sorted streams, e.g., merging parallel streams, in-stream aggregation, and merge join. We report on our implementation in the context of Google’s Napa and F1 Query systems as well as an experimental evaluation of performance and scalability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4308915727",
    "type": "article"
  },
  {
    "title": "Lightweight Query Authentication on Streams",
    "doi": "https://doi.org/10.1145/2656336",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Stavros Papadopoulos; Graham Cormode; Antonios Deligiannakis; Minos Garofalakis",
    "corresponding_authors": "",
    "abstract": "We consider a stream outsourcing setting, where a data owner delegates the management of a set of disjoint data streams to an untrusted server. The owner authenticates his streams via signatures. The server processes continuous queries on the union of the streams for clients trusted by the owner. Along with the results, the server sends proofs of result correctness derived from the owner's signatures, which are verifiable by the clients. We design novel constructions for a collection of fundamental problems over streams represented as linear algebraic queries. In particular, our basic schemes authenticate dynamic vector sums, matrix products, and dot products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group-by queries, joins, in-network aggregation, similarity matching, and event processing. We also present extensions to address the case of sliding window queries, and when multiple clients are interested in different subsets of the data. These methods take advantage of a novel nonce chaining technique that we introduce, which is used to reduce the verification cost without affecting any other costs. All our schemes are lightweight and offer strong cryptographic guarantees derived from formal definitions and proofs. We experimentally confirm the practicality of our schemes in the performance-sensitive streaming setting.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2020971160",
    "type": "article"
  },
  {
    "title": "Bounded Repairability for Regular Tree Languages",
    "doi": "https://doi.org/10.1145/2898995",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "Pierre Bourhis; Gabriele Puppis; Cristian Riveros; Sławek Staworko",
    "corresponding_authors": "",
    "abstract": "We study the problem of bounded repairability of a given restriction tree language R into a target tree language T . More precisely, we say that R is bounded repairable with respect to T if there exists a bound on the number of standard tree editing operations necessary to apply to any tree in R to obtain a tree in T . We consider a number of possible specifications for tree languages: bottom-up tree automata (on curry encoding of unranked trees) that capture the class of XML schemas and document type definitions (DTDs). We also consider a special case when the restriction language R is universal (i.e., contains all trees over a given alphabet). We give an effective characterization of bounded repairability between pairs of tree languages represented with automata. This characterization introduces two tools—synopsis trees and a coverage relation between them—allowing one to reason about tree languages that undergo a bounded number of editing operations. We then employ this characterization to provide upper bounds to the complexity of deciding bounded repairability and show that these bounds are tight. In particular, when the input tree languages are specified with arbitrary bottom-up automata, the problem is coNExp-complete. The problem remains coNExp-complete even if we use deterministic nonrecursive DTDs to specify the input languages. The complexity of the problem can be reduced if we assume that the alphabet, the set of node labels, is fixed: the problem becomes PS pace -complete for nonrecursive DTDs and coNP-complete for deterministic nonrecursive DTDs. Finally, when the restriction tree language R is universal, we show that the bounded repairability problem becomes E xp -complete if the target language is specified by an arbitrary bottom-up tree automaton and becomes tractable (P-complete, in fact) when a deterministic bottom-up automaton is used.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2466397727",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2893581",
    "publication_date": "2016-03-18",
    "publication_year": 2016,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2587381087",
    "type": "editorial"
  },
  {
    "title": "A Database Management System for the Federal Courts",
    "doi": "https://doi.org/10.1145/348.318587",
    "publication_date": "1984-03-23",
    "publication_year": 1984,
    "authors": "Jack R. Buchanan; Richard D. Fennell; Hanan Samet",
    "corresponding_authors": "",
    "abstract": "A judicial systems laboratory has been established and several large-scale information management systems projects have been undertaken within the Federal Judicial Center in Washington, D.C. The newness of the court application area, together with the experimental nature of the initial prototypes, required that the system building tools be as flexible and efficient as possible for effective software design and development. The size of the databases, the expected transaction volumes, and the long-term value of the court records required a data manipulation system capable of providing high performance and integrity. The resulting design criteria, the programming capabilities developed, and their use in system construction are described herein. This database programming facility has been especially designed as a technical management tool for the database administrator, while providing the applications programmer with a flexible database software interface for high productivity. Specifically, a network-type database management system using SAIL as the data manipulation host language is described. Generic data manipulation verb formats using SAIL's macro facilities and dynamic data structuring facilities allowing in-core database representations have been developed to achieve a level of flexibility not usually attained in conventional database systems.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2113186222",
    "type": "article"
  },
  {
    "title": "Foreword",
    "doi": "https://doi.org/10.1145/2338626.2338627",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Daniel Lemire; Owen Kaser; Eduardo Gutarra",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2951897045",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2508020",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Materialized views can bring important performance benefits when querying XML documents. In the presence of XML document changes, materialized views need to be updated to faithfully reflect the changed document. In this work, we present an algebraic ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4248202580",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2539032",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "While Complex Event Processing (CEP) constitutes a considerable portion of the so-called Big Data analytics, current CEP systems can only process data having a simple structure, and are otherwise limited in their ability to efficiently support complex ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4255156690",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2000824",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the increasing amount of data and the need to integrate data from multiple data sources, one of the challenging issues is to identify near-duplicate records efficiently. In this article, we focus on efficient algorithms to find a pair of records ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4256050984",
    "type": "paratext"
  },
  {
    "title": "Comments on “Process synchronizaiton in databases systems”",
    "doi": "https://doi.org/10.1145/320107.320126",
    "publication_date": "1979-12-01",
    "publication_year": 1979,
    "authors": "Philip A. Bernstein; Marco A. Casanova; Nathan Goodman",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2048552999",
    "type": "article"
  },
  {
    "title": "A facility for defining and manipulating generalized data structures",
    "doi": "https://doi.org/10.1145/320576.320591",
    "publication_date": "1977-12-01",
    "publication_year": 1977,
    "authors": "Billy G. Claybrook",
    "corresponding_authors": "Billy G. Claybrook",
    "abstract": "A data structure definition facility (DSDF) is described that provides definitions for several primitive data types, homogeneous and heterogeneous arrays, cells, stacks, queues, trees, and general lists. Each nonprimitive data structure consists of two separate entities—a head and a body. The head contains the entry point(s) to the body of the structure; by treating the head like a cell, the DSDF operations are capable of creating and manipulating very general data structures. A template structure is described that permits data structures to share templates. The primary objectives of the DSDF are: (1) to develop a definition facility that permits the programmer to explicitly define and manipulate generalized data structures in a consistent manner, (2) to detect mistakes and prevent the programmer from creating (either inadvertently or intentionally) undesirable (or illegal) data structures, (3) to provide a syntactic construction mechanism that separates the implementation of a data structure from its use in the program in which it is defined, and (4) to facilitate the development of reliable software.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2069009365",
    "type": "article"
  },
  {
    "title": "Designing a Query Language for RDF",
    "doi": null,
    "publication_date": "2017-01-01",
    "publication_year": 2017,
    "authors": "Marcelo Arenas; Martín Ugarte",
    "corresponding_authors": "",
    "abstract": "When querying an Resource Description Framework (RDF) graph, a prominent feature is the possibility of extending the answer to a query with optional information. However, the definition of this feature in SPARQL—the standard RDF query language—has raised some important issues. Most notably, the use of this feature increases the complexity of the evaluation problem, and its closed-world semantics is in conflict with the underlying open-world semantics of RDF. Many approaches for fixing such problems have been proposed, the most prominent being the introduction of the semantic notion of weakly monotone SPARQL query. Weakly monotone SPARQL queries have shaped the class of queries that conform to the open-world semantics of RDF. Unfortunately, finding an effective way of restricting SPARQL to the fragment of weakly monotone queries has proven to be an elusive problem. In practice, the most widely adopted fragment for writing SPARQL queries is based on the syntactic notion of well designedness. This notion has proven to be a good approach for writing SPARQL queries, but its expressive power has yet to be fully understood.\r\n\r\nThe starting point of this article is to understand the relation between well-designed queries and the semantic notion of weak monotonicity. It is known that every well-designed SPARQL query is weakly monotone; as our first contribution we prove that the converse does not hold, even if an extension of this notion based on the use of disjunction is considered. Given this negative result, we embark on the task of defining syntactic fragments that are weakly monotone and have higher expressive power than the fragment of well-designed queries. To this end, we move to a more general scenario where infinite RDF graphs are also allowed, so interpolation techniques studied for first-order logic can be applied. With the use of these techniques, we are able to define a new operator for SPARQL that gives rise to a query language with the desired properties (over finite and infinite RDF graphs). It should be noticed that every query in this fragment is weakly monotone if we restrict the semantics to finite RDF graphs. Moreover, we use this result to provide a simple characterization of the class of monotone CONSTRUCT queries, that is, the class of SPARQL queries that produce RDF graphs as output. Finally, we pinpoint the complexity of the evaluation problem for the query languages identified in the article.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2993679842",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1670243",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4244674047",
    "type": "paratext"
  },
  {
    "title": "Space optimization in deductive databases",
    "doi": "https://doi.org/10.1145/219035.219056",
    "publication_date": "1995-12-01",
    "publication_year": 1995,
    "authors": "Divesh Srivastava; S. Sudarshan; Raghu Ramakrishnan; Jeffrey F. Naughton",
    "corresponding_authors": "",
    "abstract": "In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce “fact lifetimes” (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method. We describe how techniques for each of the three components can be combined in practice to obtain a space optimization method for a program. Our results are also of importance in applications such as sequence querying, and in active databases where triggers are defined over multiple “events.”",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2095901123",
    "type": "article"
  },
  {
    "title": "Historic Moments Discovery in Sequence Data",
    "doi": "https://doi.org/10.1145/3276975",
    "publication_date": "2019-01-29",
    "publication_year": 2019,
    "authors": "Ran Bai; Wing-Kai Hon; Eric Lo; Zhian He; Kenny Q. Zhu",
    "corresponding_authors": "",
    "abstract": "Many emerging applications are based on finding interesting subsequences from sequence data. Finding “prominent streaks,” a set of the longest contiguous subsequences with values all above (or below) a certain threshold, from sequence data is one of that kind that receives much attention. Motivated from real applications, we observe that prominent streaks alone are not insightful enough but require the discovery of something we coined as “historic moments” as companions. In this article, we present an algorithm to efficiently compute historic moments from sequence data. The algorithm is incremental and space optimal , meaning that when facing new data arrival, it is able to efficiently refresh the results by keeping minimal information. Case studies show that historic moments can significantly improve the insights offered by prominent streaks alone. Furthermore, experiments show that our algorithm can outperform the baseline in both time and space.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2912820261",
    "type": "article"
  },
  {
    "title": "<scp>GraphZeppelin</scp> : How to Find Connected Components (Even When Graphs Are Dense, Dynamic, and Massive)",
    "doi": "https://doi.org/10.1145/3643846",
    "publication_date": "2024-02-20",
    "publication_year": 2024,
    "authors": "David Tench; Evan West; Victor Zhang; Michael A. Bender; Abiyaz Chowdhury; Daniel DeLayo; J. Ahmed Dellas; Martı́n Farach-Colton; Tyler Seip; Kenny Zhang",
    "corresponding_authors": "",
    "abstract": "Finding the connected components of a graph is a fundamental problem with uses throughout computer science and engineering. The task of computing connected components becomes more difficult when graphs are very large, or when they are dynamic, meaning the edge set changes over time subject to a stream of edge insertions and deletions. A natural approach to computing the connected components problem on a large, dynamic graph stream is to buy enough RAM to store the entire graph. However, the requirement that the graph fit in RAM is an inherent limitation of this approach and is prohibitive for very large graphs. Thus, there is an unmet need for systems that can process dense dynamic graphs, especially when those graphs are larger than available RAM. We present a new high-performance streaming graph-processing system for computing the connected components of a graph. This system, which we call GraphZeppelin , uses new linear sketching data structures ( CubeSketch ) to solve the streaming connected components problem and as a result requires space asymptotically smaller than the space required for a lossless representation of the graph. GraphZeppelin is optimized for massive dense graphs: GraphZeppelin can process millions of edge updates (both insertions and deletions) per second, even when the underlying graph is far too large to fit in available RAM. As a result GraphZeppelin vastly increases the scale of graphs that can be processed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391971879",
    "type": "article"
  },
  {
    "title": "Sharing Queries with Nonequivalent User-Defined Aggregate Functions",
    "doi": "https://doi.org/10.1145/3649133",
    "publication_date": "2024-02-24",
    "publication_year": 2024,
    "authors": "Chao Zhang; Farouk Toumani",
    "corresponding_authors": "",
    "abstract": "This article presents Sharing User-Defined Aggregate Function (SUDAF), a declarative framework that allows users to write User-defined Aggregate Functions (UDAFs) as mathematical expressions and use them in Structured Query Language statements. SUDAF rewrites partial aggregates of UDAFs using built-in aggregate functions and supports efficient dynamic caching and reusing of partial aggregates. Our experiments show that rewriting UDAFs using built-in functions can significantly speed up queries with UDAFs, and the proposed sharing approach can yield up to two orders of magnitude improvement in query execution time. The article studies also an extension of SUDAF to support sharing partial results between arbitrary queries with UDAFs. We show a connection with the problem of query rewriting using views and introduce a new class of rewritings, called SUDAF rewritings, which enables to use views that have aggregate functions different from the ones used in the input query. We investigate the underlying rewriting-checking and rewriting-existing problem. Our main technical result is a reduction of these problems to, respectively, rewriting-checking and rewriting-existing of the so-called aggregate candidates , a class of rewritings that has been deeply investigated in the literature.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392139269",
    "type": "article"
  },
  {
    "title": "Automated Category Tree Construction: Hardness Bounds and Algorithms",
    "doi": "https://doi.org/10.1145/3664283",
    "publication_date": "2024-05-09",
    "publication_year": 2024,
    "authors": "Shay Gershtein; Uri Avron; Ido Guy; Tova Milo; Slava Novgorodov",
    "corresponding_authors": "",
    "abstract": "Category trees, or taxonomies, are rooted trees where each node, called a category, corresponds to a set of related items. The construction of taxonomies has been studied in various domains, including e-commerce, document management, and question answering. Multiple algorithms for automating construction have been proposed, employing a variety of clustering approaches and crowdsourcing. However, no formal model to capture such categorization problems has been devised, and their complexity has not been studied. To address this, we propose in this work a combinatorial model that captures many practical settings and show that the aforementioned empirical approach has been warranted, as we prove strong inapproximability bounds for various problem variants and special cases when the goal is to produce a categorization of the maximum utility. In our model, the input is a set of n weighted item sets that the tree would ideally contain as categories. Each category, rather than perfectly match the corresponding input set, is allowed to exceed a given threshold for a given similarity function. The goal is to produce a tree that maximizes the total weight of the sets for which it contains a matching category. A key parameter is an upper bound on the number of categories an item may belong to, which produces the hardness of the problem, as initially each item may be contained in an arbitrary number of input sets. For this model, we prove inapproximability bounds, of order \\(\\tilde{\\Theta }(\\sqrt {n})\\) or \\(\\tilde{\\Theta }(n)\\) , for various problem variants and special cases, loosely justifying the aforementioned heuristic approach. Our work includes reductions based on parameterized randomized constructions that highlight how various problem parameters and properties of the input may affect the hardness. Moreover, for the special case where the category must be identical to the corresponding input set, we devise an algorithm whose approximation guarantee depends solely on a more granular parameter, allowing improved worst-case guarantees, as well as the application of practical exact solvers. We further provide efficient algorithms with much improved approximation guarantees for practical special cases where the cardinalities of the input sets or the number of input sets each items belongs to are not too large. Finally, we also generalize our results to DAG-based and non-hierarchical categorization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396773650",
    "type": "article"
  },
  {
    "title": "Rewriting the Infinite Chase for Guarded TGDs",
    "doi": "https://doi.org/10.1145/3696416",
    "publication_date": "2024-09-18",
    "publication_year": 2024,
    "authors": "Michael Benedikt; Maxime Buron; Stefano Germano; Kevin Kappelmann; Boris Motik",
    "corresponding_authors": "",
    "abstract": "Guarded tuple-generating dependencies (GTGDs) are a natural extension of description logics and referential constraints. It has long been known that queries over GTGDs can be answered by a variant of the chase —a quintessential technique for reasoning with dependencies. However, there has been little work on concrete algorithms and even less on implementation. To address this gap, we revisit Datalog rewriting approaches to query answering, where a set of GTGDs is transformed to a Datalog program that entails the same base facts on each base instance. We show that a rewriting consists of “shortcut” rules that circumvent certain chase steps, we present several algorithms that compute a rewriting by deriving such “shortcuts” efficiently, and we discuss important implementation issues. Finally, we show empirically that our techniques can process complex GTGDs derived from synthetic and real benchmarks and are thus suitable for practical use.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402607702",
    "type": "article"
  },
  {
    "title": "Marrying Top-k with Skyline Queries: Operators with Relaxed Preference Input and Controllable Output Size",
    "doi": "https://doi.org/10.1145/3705726",
    "publication_date": "2024-11-22",
    "publication_year": 2024,
    "authors": "Kyriakos Mouratidis; K. Li; Bo Tang",
    "corresponding_authors": "",
    "abstract": "The two most common paradigms to identify records of preference in a multi-objective setting rely either on dominance (e.g., the skyline operator) or on a utility function defined over the records’ attributes (typically, using a top- k query). Despite their proliferation, each of them has its own palpable drawbacks. Motivated by these drawbacks, we identify three hard requirements for practical decision support, namely, personalization, controllable output size, and flexibility in preference specification. With these requirements as a guide, we combine elements from both paradigms and propose two new operators, \\(\\mathsf {ORD} \\) and \\(\\mathsf {ORU} \\) . We present a suite of algorithms for their efficient processing, dedicating more technical effort to \\(\\mathsf {ORU} \\) , whose nature is inherently more challenging. Specifically, besides a sophisticated algorithm for \\(\\mathsf {ORD} \\) , we describe two exact methods for \\(\\mathsf {ORU} \\) , and one approximate. We perform a qualitative study to demonstrate how our operators work, and evaluate the performance of our algorithms against adaptations of previous work that mimic their output.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404613860",
    "type": "article"
  },
  {
    "title": "Tight Fine-Grained Bounds for Direct Access on Join Queries",
    "doi": "https://doi.org/10.1145/3707448",
    "publication_date": "2024-12-06",
    "publication_year": 2024,
    "authors": "Karl Bringmann; Nofar Carmeli; Stefan Mengel",
    "corresponding_authors": "",
    "abstract": "We consider the task of lexicographic direct access to query answers. That is, we want to simulate an array containing the answers of a join query sorted in a lexicographic order chosen by the user. A recent dichotomy showed for which queries and orders this task can be done in polylogarithmic access time after quasilinear preprocessing, but this dichotomy does not tell us how much time is required in the cases classified as hard. We determine the preprocessing time needed to achieve polylogarithmic access time for all join queries and all lexicographical orders. To this end, we propose a decomposition-based general algorithm for direct access on join queries. We then explore its optimality by proving lower bounds for the preprocessing time based on the hardness of a certain online Set-Disjointness problem, which shows that our algorithm’s bounds are tight for all lexicographic orders on join queries. Then, we prove the hardness of Set-Disjointness based on the Zero-Clique Conjecture, which is an established conjecture from fine-grained complexity theory. Interestingly, while proving our lower bound, we show that self-joins do not affect the complexity of direct access (up to logarithmic factors). Our algorithm can also be used to solve queries with projections and relaxed order requirements, though in these cases, its running time is not necessarily optimal. We also show that similar techniques to those used in our lower bounds can be used to prove that, for enumerating answers to Loomis-Whitney joins, it is not possible to significantly improve upon trivially computing all answers at preprocessing. This, in turn, gives further evidence (based on the Zero-Clique Conjecture) to the enumeration hardness of self-join-free cyclic joins with respect to linear preprocessing and constant delay.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405114273",
    "type": "article"
  },
  {
    "title": "The INCINERATE data model",
    "doi": "https://doi.org/10.1145/202106.202113",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "H. V. Jagadish",
    "corresponding_authors": "H. V. Jagadish",
    "abstract": "In this article, we present an extended relational algebra with universally or existentially quantified classes as attribute values. The proposed extension can greatly enhance the expressive power of relational systems, and significantly reduce the size of a database, at small additional computational cost. We also show how the proposed extensions can be built on top of a standard relational database system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2050385417",
    "type": "article"
  },
  {
    "title": "A note on estimating the cardinality of the projection of a database relation",
    "doi": "https://doi.org/10.1145/111197.111218",
    "publication_date": "1991-09-01",
    "publication_year": 1991,
    "authors": "Ravi Mukkamala; Sushil Jajodia",
    "corresponding_authors": "",
    "abstract": "The paper by Ahad et al. [1] derives an analytical expression to estimate the cardinality of the projection of a database relation. In this note, we propose to show that this expression is in error even when all the parameters are assumed to be constant. We derive the correct formula for this expression.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2075278905",
    "type": "article"
  },
  {
    "title": "Introduction to special ICDT section",
    "doi": "https://doi.org/10.1145/1166074.1166075",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Thomas Eiter; Leonid Libkin",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2047063031",
    "type": "article"
  },
  {
    "title": "Conjunctive Regular Path Queries with Capture Groups",
    "doi": "https://doi.org/10.1145/3514230",
    "publication_date": "2022-02-23",
    "publication_year": 2022,
    "authors": "Markus L. Schmid",
    "corresponding_authors": "Markus L. Schmid",
    "abstract": "In practice, regular expressions are usually extended by so-called capture groups or capture variables, which allow to capture a subexpression by a variable that can be referenced in the regular expression in order to describe repetitions of subwords. We investigate how this concept could be used for pattern-based graph querying; i.e., we investigate conjunctive regular path queries (CRPQs) that are extended by capture variables. If capture variables are added to CRPQs in a completely unrestricted way, then Boolean evaluation becomes PSPACE-hard in data complexity, even for single-edge graph patterns. On the other hand, if capture variables do not occur under a Kleene star, then the data complexity drops to NL-completeness. Combined complexity is in EXPSPACE but drops to PSPACE-completeness if the depth (i.e., the nesting depth of capture variables) is bounded, and it drops to NP-completeness if the size of the images of capture variables is bounded by a constant (regardless of the depth or of whether capture variables occur under a Kleene star). In the application of regular expressions as string searching tools, references to capture variables only describe exact repetitions of subwords (i.e., they implement the equality relation on strings). Following recent developments in graph database research, we also study CRPQs with capture variables that describe arbitrary regular relations. We show that if the expressions have depth 0, or if the size of the images of capture variables is bounded by a constant, then we can allow arbitrary regular relations while staying in the same complexity bounds. We also investigate the problems of checking whether a given tuple is in the solution set and computing the whole solution set. On the conceptual side, we add capture variables to CRPQs in such a way that they can be defined in an expression on one arc of the graph pattern but also referenced in expressions on other arcs. Hence, they add to CRPQs the possibility to define inter-dependencies between different paths, which is a relevant feature of pattern-based graph querying.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4213413132",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1189769",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The relational data model has simple and clear foundations on which significant theoretical and systems research has flourished. By contrast, most research on data mining has focused on algorithmic issues. A major open question is: what's an appropriate ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4234064057",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2770931",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1516869194",
    "type": "editorial"
  },
  {
    "title": "Foreword to Invited Articles Issue",
    "doi": "https://doi.org/10.1145/2697050",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2146729698",
    "type": "article"
  },
  {
    "title": "Cost-Effective Conceptual Design for Information Extraction",
    "doi": "https://doi.org/10.1145/2716321",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Arash Termehchy; Ali Vakilian; Yodsawalai Chodpathumwan; Marianne Winslett",
    "corresponding_authors": "",
    "abstract": "It is well established that extracting and annotating occurrences of entities in a collection of unstructured text documents with their concepts improves the effectiveness of answering queries over the collection. However, it is very resource intensive to create and maintain large annotated collections. Since the available resources of an enterprise are limited and/or its users may have urgent information needs, it may have to select only a subset of relevant concepts for extraction and annotation. We call this subset a conceptual design for the annotated collection. In this article, we introduce and formally define the problem of cost-effective conceptual design where, given a collection, a set of relevant concepts, and a fixed budget, one likes to find a conceptual design that most improves the effectiveness of answering queries over the collection. We provide efficient algorithms for special cases of the problem and prove it is generally NP-hard in the number of relevant concepts. We propose three efficient approximations to solve the problem: a greedy algorithm, an approximate popularity maximization (APM for short), and approximate annotation-benefit maximization (AAM for short). We show that, if there are no constraints regrading the overlap of concepts, APM is a fully polynomial time approximation scheme. We also prove that if the relevant concepts are mutually exclusive, the greedy algorithm delivers a constant approximation ratio if the concepts are equally costly, APM has a constant approximation ratio, and AAM is a fully polynomial-time approximation scheme. Our empirical results using a Wikipedia collection and a search engine query log validate the proposed formalization of the problem and show that APM and AAM efficiently compute conceptual designs. They also indicate that, in general, APM delivers the optimal conceptual designs if the relevant concepts are not mutually exclusive. Also, if the relevant concepts are mutually exclusive, the conceptual designs delivered by AAM improve the effectiveness of answering queries over the collection more than the solutions provided by APM.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2264424170",
    "type": "article"
  },
  {
    "title": "Deciding Determinism with Fairness for Simple Transducer Networks",
    "doi": "https://doi.org/10.1145/2757215",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Tom J. Ameloot",
    "corresponding_authors": "Tom J. Ameloot",
    "abstract": "A distributed database system often operates in an asynchronous communication model where messages can be arbitrarily delayed. This communication model causes nondeterministic effects like unpredictable arrival orders of messages. Nonetheless, in general we want the distributed system to be deterministic; the system should produce the same output despite the nondeterministic effects on messages. Previously, two interpretations of determinism have been proposed. The first says that all infinite fair computation traces produce the same output. The second interpretation is a confluence notion, saying that all finite computation traces can still be extended to produce the same output. A decidability result for the confluence notion was previously obtained for so-called simple transducer networks, a model from the field of declarative networking. In the current article, we also present a decidability result for simple transducer networks, but this time for the first interpretation of determinism, with infinite fair computation traces. We also compare the expressivity of simple transducer networks under both interpretations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2282105004",
    "type": "article"
  },
  {
    "title": "Monadic Datalog and Regular Tree Pattern Queries",
    "doi": "https://doi.org/10.1145/2925986",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "Filip Mazowiecki; Filip Murlak; Adam Witkowski",
    "corresponding_authors": "",
    "abstract": "Containment of monadic datalog programs over trees is decidable. The situation is more complex when tree nodes carry labels from an infinite alphabet that can be tested for equality. It then matters whether the descendant relation is allowed or not: the descendant relation can be eliminated easily from monadic programs only when label equalities are not used. With descendant, even containment of linear monadic programs in unions of conjunctive queries is undecidable, and positive results are known only for bounded-depth trees. We show that without descendant, containment of connected monadic programs is decidable over ranked trees, but over unranked trees it is so only for linear programs. With descendant, it becomes decidable over unranked trees under restriction to downward programs: each rule only moves down from the node in the head. This restriction is motivated by regular tree pattern queries, a recent formalism in the area of ActiveXML, which we show to be equivalent to linear downward programs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2471850623",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2936309",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many web applications store persistent data in databases. During execution, such applications spend a significant amount of time communicating with the database for retrieval and storing of persistent data over the network. These network round-trips ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231023141",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2691190",
    "publication_date": "2014-12-30",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article studies I/O-efficient algorithms for the triangle listing problem and the triangle counting problem, whose solutions are basic operators in dealing with many other graph problems. In the former problem, given an undirected graph G, the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233285687",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2627748",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Space-filling curves have been used in the design of data structures for multidimensional data for many decades. A fundamental quality metric of a space-filling curve is its “clustering number” with respect to a class of queries, which is the average ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234532696",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2751312",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A sliding window top-k (top-k/w) query monitors incoming data stream objects within a sliding window of size w to identify the k highest-ranked objects with respect to a given scoring function over time. Processing of such queries is challenging because,...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236430301",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2662448",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237167610",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2966276",
    "publication_date": "2016-08-08",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Databases can provide scalability by partitioning data across several servers. However, multipartition, multioperation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237831706",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2838914",
    "publication_date": "2015-10-23",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A string-similarity measure quantifies the similarity between two text strings for approximate string matching or comparison. For example, the strings “Sam” and “Samuel” can be considered to be similar. Most existing work that computes the similarity of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237965119",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3014437",
    "publication_date": "2016-12-23",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Hadoop Distributed File System (HDFS) has become an important data repository in the enterprise as the center for all business analytics, from SQL queries and machine learning to reporting. At the same time, enterprise data warehouses (EDWs) ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240921903",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2897141",
    "publication_date": "2016-04-07",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The goal of the article is to bridge the difference between theoretical and practical approaches to answering queries over databases with nulls. Theoretical research has long ago identified the notion of correctness of query answering over incomplete ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248668717",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2799368",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Frequent sequence mining is one of the fundamental building blocks in data mining. While the problem has been extensively studied, few of the available techniques are sufficiently scalable to handle datasets with billions of sequences; such large-scale ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250956189",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2866579",
    "publication_date": "2016-02-03",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The CALM-conjecture, first stated by Hellerstein [2010] and proved in its revised form by Ameloot et al. [2013] within the framework of relational transducer networks, asserts that a query has a coordination-free execution strategy if and only if the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252889795",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2576988",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Egor V. Kostylev; Juan L. Reutter; András Z. Salamon",
    "corresponding_authors": "",
    "abstract": "We study the problem of query containment of conjunctive queries over annotated databases. Annotations are typically attached to tuples and represent metadata, such as probability, multiplicity, comments, or provenance. It is usually assumed that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253025652",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2676651",
    "publication_date": "2014-10-07",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Conventional spatial indexes, represented by the R-tree, employ multidimensional tree structures that are complicated and require enormous efforts to implement in a full-fledged database management system (DBMS). An alternative approach for supporting ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255237495",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/202106.569784",
    "publication_date": "1995-03-01",
    "publication_year": 1995,
    "authors": "Won Kim",
    "corresponding_authors": "Won Kim",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2025818825",
    "type": "editorial"
  },
  {
    "title": "Editorial directons",
    "doi": "https://doi.org/10.1145/211414.569785",
    "publication_date": "1995-09-01",
    "publication_year": 1995,
    "authors": "Won Kim",
    "corresponding_authors": "Won Kim",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2293616785",
    "type": "article"
  },
  {
    "title": "Balancing Expressiveness and Inexpressiveness in View Design",
    "doi": "https://doi.org/10.1145/3488370",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Michael Benedikt; Pierre Bourhis; Louis Jachiet; Efthymia Tsamoura",
    "corresponding_authors": "",
    "abstract": "We study the design of data publishing mechanisms that allow a collection of autonomous distributed data sources to collaborate to support queries. A common mechanism for data publishing is via views : functions that expose derived data to users, usually specified as declarative queries. Our autonomy assumption is that the views must be on individual sources, but with the intention of supporting integrated queries. In deciding what data to expose to users, two considerations must be balanced. The views must be sufficiently expressive to support queries that users want to ask—the utility of the publishing mechanism. But there may also be some expressiveness restrictions. Here, we consider two restrictions, a minimal information requirement, saying that the views should reveal as little as possible while supporting the utility query, and a non-disclosure requirement, formalizing the need to prevent external users from computing information that data owners do not want revealed. We investigate the problem of designing views that satisfy both expressiveness and inexpressiveness requirements, for views in a restricted information systems - query languages (conjunctive queries), and for arbitrary views.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3203685703",
    "type": "article"
  },
  {
    "title": "Foreword to invited papers issue",
    "doi": "https://doi.org/10.1145/2539032.2539033",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Z. Meral Özsoyoğlu",
    "corresponding_authors": "Z. Meral Özsoyoğlu",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1998906113",
    "type": "article"
  },
  {
    "title": "Revisiting “forward node-selecting queries over trees”",
    "doi": "https://doi.org/10.1145/2487259.2487265",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "James Cheney",
    "corresponding_authors": "James Cheney",
    "abstract": "In “Forward Node-Selecting Queries over Trees,” Olteanu [2007] gives three rewriting systems for eliminating reverse XPath axis steps from node-selecting queries over trees, together with arguments for their correctness and termination for a large class of input graphs, including cyclic ones. These proofs are valid for tree or acyclic formulas, but two of the rewrite systems ( TRS 2 and TRS 3 ) do not terminate on cyclic graphs; that is, there are infinite rewrite sequences that never yield a normal form. We investigate the reasons why the termination arguments do not work for general cyclic formulas, and develop alternative algorithms that can be used instead. We prove that TRS 2 is weakly normalizing, while TRS 3 is not weakly normalizing, but it can be extended to a weakly normalizing system TRS 3 ○ . The algorithms and proof techniques illustrate unforeseen subtleties in the handling of cyclic queries.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2005045368",
    "type": "article"
  },
  {
    "title": "Foreword to TODS invited papers issue 2011",
    "doi": "https://doi.org/10.1145/2043652.2043653",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Z. Meral Özsoyoğlu",
    "corresponding_authors": "Z. Meral Özsoyoğlu",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2046828465",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2445583",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the increasing use of Web 2.0 to create, disseminate, and consume large volumes of data, more and more information is published and becomes available for potential data consumers, that is, applications/services, individual users and communities, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232480850",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2043652",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Maximal clique enumeration is a fundamental problem in graph theory and has important applications in many areas such as social network analysis and bioinformatics. The problem is extensively studied; however, the best existing algorithms require memory ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237771732",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1966385",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Most data stream management systems are based on extensions of the relational data model and query languages, but rigorous analyses of the problems and limitations of this approach, and how to overcome them, are still wanting. In this article, we ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238816545",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1929934",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252240748",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2487259",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "BE-Tree is a novel dynamic data structure designed to efficiently index Boolean expressions over a high-dimensional discrete space. BE Tree-copes with both high-dimensionality and expressiveness of Boolean expressions by introducing an effective two-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253419591",
    "type": "paratext"
  },
  {
    "title": "Foreword to TODS invited papers issue",
    "doi": "https://doi.org/10.1145/1862919.1862920",
    "publication_date": "2010-10-12",
    "publication_year": 2010,
    "authors": "Z. Meral Özsoyoğlu",
    "corresponding_authors": "Z. Meral Özsoyoğlu",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1982397485",
    "type": "article"
  },
  {
    "title": "Foreword to TODS SIGMOD/PODS 2008 special issue",
    "doi": "https://doi.org/10.1145/1620585.1620586",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Dennis Sasha; Maurizio Lenzerini; Z. Meral Özsoyoǧlu",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2059886180",
    "type": "article"
  },
  {
    "title": "Comparing Workflows: A Matter of Views",
    "doi": null,
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Pierre Bourhis",
    "corresponding_authors": "Pierre Bourhis",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2340115536",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1567274",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article considers the problem of scalably processing a large number of continuous queries. Our approach, consisting of novel data structures and algorithms and a flexible processing framework, advances the state-of-the-art in several ways. First, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234781152",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2389241",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In deletion propagation, tuples from the database are deleted in order to reflect the deletion of a tuple from the view. Such an operation may result in the (often necessary) deletion of additional tuples from the view, besides the intentionally deleted ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238425102",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2188349",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Besides topological relationships and approximate relationships, cardinal directions like north and southwest have turned out to be an important class of qualitative spatial relationships. They are of interdisciplinary interest in fields like cognitive ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239394305",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1862919",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Automatic recycling of intermediate results to improve both query response time and throughput is a grand challenge for state-of-the-art databases. Tuples are loaded and streamed through a tuple-at-a-time processing pipeline, avoiding materialization of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245045764",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1538909",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246810462",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1620585",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many popular database management systems implement a multiversion concurrency control algorithm called snapshot isolation rather than providing full serializability based on locking. There are well-known anomalies permitted by snapshot isolation that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249819973",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1806907",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250899737",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2109196",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "B-trees have been ubiquitous in database management systems for several decades, and they serve in many other storage systems as well. Their basic structure and their basic operations are well understood including search, insertion, and deletion. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253830519",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2338626",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Foster B-trees are a new variant of B-trees that combines advantages of prior B-tree variants optimized for many-core processors and modern memory hierarchies with flash storage and nonvolatile memory. Specific goals include: (i) minimal concurrency ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254048325",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1508857",
    "publication_date": "2009-04-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Query monitoring refers to the problem of observing and predicting various parameters related to the execution of a query in a database system. In addition to being a useful tool for database users and administrators, it can also serve as an information ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254048496",
    "type": "paratext"
  },
  {
    "title": "Foreword",
    "doi": "https://doi.org/10.1145/2389241.2389242",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Z. Meral Özsoyoǧlu",
    "corresponding_authors": "Z. Meral Özsoyoǧlu",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254489581",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1735886",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Numerous generalization techniques have been proposed for privacy-preserving data publishing. Most existing techniques, however, implicitly assume that the adversary knows little about the anonymization algorithm adopted by the data publisher. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255556548",
    "type": "paratext"
  },
  {
    "title": "Introduction to ACM SIGMOD 2007 special section",
    "doi": "https://doi.org/10.1145/1412331.1412333",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Beng Chin Ooi",
    "corresponding_authors": "Beng Chin Ooi",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1985212977",
    "type": "article"
  },
  {
    "title": "Introduction to ACM SIGMOD 2006 conference papers",
    "doi": "https://doi.org/10.1145/1292609.1292610",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Surajit Chaudhuri",
    "corresponding_authors": "Surajit Chaudhuri",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1989975716",
    "type": "article"
  },
  {
    "title": "Introduction to ICDT 2007 special section",
    "doi": "https://doi.org/10.1145/1412331.1412339",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Thomas Schwentick; Dan Suciu",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2041706874",
    "type": "article"
  },
  {
    "title": "Introduction to the PODS 2006 special section",
    "doi": "https://doi.org/10.1145/1292609.1292614",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Jan Van den Bussche",
    "corresponding_authors": "Jan Van den Bussche",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2044321852",
    "type": "article"
  },
  {
    "title": "Introduction to the PODS 2007 special section",
    "doi": "https://doi.org/10.1145/1412331.1412336",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Leonid Libkin",
    "corresponding_authors": "Leonid Libkin",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2057445319",
    "type": "article"
  },
  {
    "title": "Introduction to the EDBT 2006 special section",
    "doi": "https://doi.org/10.1145/1292609.1292617",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "Yannis Ioannidis",
    "corresponding_authors": "Yannis Ioannidis",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2083939267",
    "type": "article"
  },
  {
    "title": "Foreword to TODS SIGMOD/PODS/ICDT 2007 special issue",
    "doi": "https://doi.org/10.1145/1412331.1412332",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Z. Meral Özsoyoğlu",
    "corresponding_authors": "Z. Meral Özsoyoğlu",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2164180071",
    "type": "article"
  },
  {
    "title": "Response to “Differential Dependencies Revisited”",
    "doi": "https://doi.org/10.1145/2983602",
    "publication_date": "2017-01-14",
    "publication_year": 2017,
    "authors": "Shaoxu Song; Lei Chen",
    "corresponding_authors": "",
    "abstract": "A recent article [Vincent et al. 2015] concerns the correctness of several results in reasoning about differential dependencies ( dds ), originally reported in Song and Chen [2011]. The major concern by Vincent et al. [2015] roots from assuming a type of infeasible differential functions in the given dds for consistency and implication analysis, which are not allowed in Song and Chen [2011]. A differential function is said to be infeasible if there is no tuple pair with values that can satisfy the specified distance constraints. For example, [price(&lt;2, &gt; 4)] requires the difference of two price values to be &lt; 2 and &gt; 4 at the same time, which is clearly impossible. Although dds involving infeasible differential functions may be syntactically interesting, they are semantically meaningless and would neither be specified by domain experts nor discovered from data. For these reasons, infeasible differential functions are not considered [Song and Chen 2011] and the results in Song and Chen [2011] are correct, in contrast to what is claimed in Vincent et al. [2015].",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2575050392",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3041040",
    "publication_date": "2017-03-02",
    "publication_year": 2017,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230324322",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1242524",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230744634",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1386118",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Ranking and aggregation queries are widely used in data exploration, data analysis, and decision-making scenarios. While most of the currently proposed ranking and aggregation techniques focus on deterministic data, several emerging applications involve ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235600993",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1272743",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Large amounts of (often valuable) information are stored in web-accessible text databases. “Metasearchers” provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240891607",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3129336",
    "publication_date": "2017-08-24",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "DBSCAN is a method proposed in 1996 for clustering multi-dimensional points, and has received extensive applications. Its computational hardness is still unsolved to this date. The original KDD‚96 paper claimed an algorithm of O(n log n) ”average ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245450512",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3015779",
    "publication_date": "2017-03-02",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We prove exponential lower bounds on the running time of the state-of-the-art exact model counting algorithms—algorithms for exactly computing the number of satisfying assignments, or the satisfying probability, of Boolean formulas. These algorithms can ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245548716",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1331904",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Our system applies authority-based ranking to keyword search in databases modeled as labeled graphs. Three ranking factors are used: the relevance to the query, the specificity and the importance of the result. All factors are handled using authority-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248059265",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1366102",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a class of integrity constraints for relational databases, referred to as conditional functional dependencies (CFDs), and study their applications in data cleaning. In contrast to traditional functional dependencies (FDs) that were developed ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248772325",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3086510",
    "publication_date": "2017-06-01",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250017579",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1412331",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255516921",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1292609",
    "publication_date": "2007-11-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255841581",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1206049",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This editorial analyzes from a variety of perspectives the controversial issue of single-blind versus double-blind reviewing. In single-blind reviewing, the reviewer is unknown to the author, but the identity of the author is known to the reviewer. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255870251",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3155316",
    "publication_date": "2017-11-13",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "There are two types of high-performance graph processing engines: low- and high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide optimized data structures and computation models but require users to write low-level imperative code, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256129774",
    "type": "paratext"
  },
  {
    "title": "Proportionality on Spatial Data with Context",
    "doi": "https://doi.org/10.1145/3588434",
    "publication_date": "2023-03-17",
    "publication_year": 2023,
    "authors": "Georgios J. Fakas; Georgios Kalamatianos",
    "corresponding_authors": "",
    "abstract": "More often than not, spatial objects are associated with some context, in the form of text, descriptive tags (e.g., points of interest, flickr photos), or linked entities in semantic graphs (e.g., Yago2, DBpedia). Hence, location-based retrieval should be extended to consider not only the locations but also the context of the objects, especially when the retrieved objects are too many and the query result is overwhelming. In this article, we study the problem of selecting a subset of the query result, which is the most representative. We argue that objects with similar context and nearby locations should proportionally be represented in the selection. Proportionality dictates the pairwise comparison of all retrieved objects and hence bears a high cost. We propose novel algorithms which greatly reduce the cost of proportional object selection in practice. In addition, we propose pre-processing, pruning, and approximate computation techniques that their combination reduces the computational cost of the algorithms even further. We theoretically analyze the approximation quality of our approaches. Extensive empirical studies on real datasets show that our algorithms are effective and efficient. A user evaluation verifies that proportional selection is more preferable than random selection and selection based on object diversification.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4327727786",
    "type": "article"
  },
  {
    "title": "Efficient Bi-objective SQL Optimization for Enclaved Cloud Databases with Differentially Private Padding",
    "doi": "https://doi.org/10.1145/3597021",
    "publication_date": "2023-05-11",
    "publication_year": 2023,
    "authors": "Yaxing Chen; Qinghua Zheng; Zheng Yan",
    "corresponding_authors": "",
    "abstract": "Hardware-enabled enclaves have been applied to efficiently enforce data security and privacy protection in cloud database services. Such enclaved systems, however, are reported to suffer from I/O-size (also referred to as communication-volume)-based side-channel attacks. Albeit differentially private padding has been exploited to defend against these attacks as a principle method, it introduces a challenging bi-objective parametric query optimization (BPQO) problem and current solutions are still not satisfactory. Concretely, the goal in BPQO is to find a Pareto-optimal plan that makes a tradeoff between query performance and privacy loss; existing solutions are subjected to poor computational efficiency and high cloud resource waste. In this article, we propose a two-phase optimization algorithm called TPOA to solve the BPQO problem. TPOA incorporates two novel ideas: divide-and-conquer to separately handle parameters according to their types in optimization for dimensionality reduction; on-demand-optimization to progressively build a set of necessary Pareto-optimal plans instead of seeking a complete set for saving resources. Besides, we introduce an acceleration mechanism in TPOA to improve its efficiency, which prunes the non-optimal candidate plans in advance. We theoretically prove the correctness of TPOA, numerically analyze its complexity, and formally give an end-to-end privacy analysis. Through a comprehensive evaluation on its efficiency by running baseline algorithms over synthetic and test-bed benchmarks, we can conclude that TPOA outperforms all benchmarked methods with an overall efficiency improvement of roughly two orders of magnitude; moreover, the acceleration mechanism speeds up TPOA by 10-200×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4376149845",
    "type": "article"
  },
  {
    "title": "Model Counting Meets <i>F</i> <sub>0</sub> Estimation",
    "doi": "https://doi.org/10.1145/3603496",
    "publication_date": "2023-06-20",
    "publication_year": 2023,
    "authors": "A. Pavan; N. V. Vinodchandran; Arnab Bhattacharyya; Kuldeep S. Meel",
    "corresponding_authors": "",
    "abstract": "Constraint satisfaction problems (CSPs) and data stream models are two powerful abstractions to capture a wide variety of problems arising in different domains of computer science. Developments in the two communities have mostly occurred independently and with little interaction between them. In this work, we seek to investigate whether bridging the seeming communication gap between the two communities may pave the way to richer fundamental insights. To this end, we focus on two foundational problems: model counting for CSP’s and computation of zeroth frequency moments ( F 0 ) for data streams. Our investigations lead us to observe a striking similarity in the core techniques employed in the algorithmic frameworks that have evolved separately for model counting and F 0 computation. We design a recipe for translating algorithms developed for F 0 estimation to model counting, resulting in new algorithms for model counting. We also provide a recipe for transforming sampling algorithm over streams to constraint sampling algorithms. We then observe that algorithms in the context of distributed streaming can be transformed into distributed algorithms for model counting. We next turn our attention to viewing streaming from the lens of counting and show that framing F 0 estimation as a special case of #DNF counting allows us to obtain a general recipe for a rich class of streaming problems, which had been subjected to case-specific analysis in prior works. In particular, our view yields an algorithm for multidimensional range efficient F 0 estimation with a simpler analysis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4381379333",
    "type": "article"
  },
  {
    "title": "Performance analysis of file organizations that use multibucket data leaves with partial expansions",
    "doi": "https://doi.org/10.1145/151284.151289",
    "publication_date": "1993-03-01",
    "publication_year": 1993,
    "authors": "Gabriel Matsliach",
    "corresponding_authors": "Gabriel Matsliach",
    "abstract": "We present an exact performance analysis, under random insertions, of file organizations that use multibucket data leaves and perform partial expansions before splitting. We evaluate the expected disk space utilization of the file and show how the expected search and insert costs can be estimated. The analytical results are confirmed by simulations. The analysis can be used to investigate both the dynamic and the asymptotic behaviors.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2078152513",
    "type": "article"
  },
  {
    "title": "Foreword to special section on SIGMOD/PODS 2005",
    "doi": "https://doi.org/10.1145/1189769.1189776",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Foto Afrati; Jennifer Widom",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2003957003",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3339885",
    "publication_date": "2019-06-19",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Lightweight integer compression algorithms are frequently applied in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the vectorization of basic techniques such as delta coding and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229679880",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3309575",
    "publication_date": "2019-01-29",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Today’s streaming applications demand increasingly high event throughput rates and are often subject to strict latency constraints. To allow for more complex workloads, such as window-based aggregations, streaming systems need to support stateful event ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231463835",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3366712",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Reactive security monitoring, self-driving cars, the Internet of Things (IoT), and many other novel applications require systems for both writing events arriving at very high and fluctuating rates to persistent storage as well as supporting analytical ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234059662",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1138394",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Similarity searches in multidimensional Non-ordered Discrete Data Spaces (NDDS) are becoming increasingly important for application areas such as bioinformatics, biometrics, data mining and E-commerce. Efficient similarity searches require robust ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237163507",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1114244",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238754386",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3313802",
    "publication_date": "2019-04-08",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241394798",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3243648",
    "publication_date": "2018-09-05",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We investigate the query evaluation problem for fixed queries over fully dynamic databases, where tuples can be inserted or deleted. The task is to design a dynamic algorithm that immediately reports the new result of a fixed query after every database ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244349320",
    "type": "paratext"
  },
  {
    "title": "Foreword",
    "doi": "https://doi.org/10.1145/1114244.1114245",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Dan Suciu; Gerhard Weikum",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244674768",
    "type": "article"
  },
  {
    "title": "Foreword",
    "doi": "https://doi.org/10.1145/974750.974751",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Phokion Kolaitis; Michael J. Franklin",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249122963",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3194314",
    "publication_date": "2018-04-11",
    "publication_year": 2018,
    "authors": "Walid G. Aref; Graham Cormode; Gautam Das; Sabrina De Capitani; Di Vimercati; Dirk Van; Bernhard Seeger",
    "corresponding_authors": "",
    "abstract": "We consider a data owner that outsources its dataset to an untrusted server. The owner wishes to enable the server to answer range queries on a single attribute, without compromising the privacy of the data and the queries. There are several schemes on “...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250659254",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3183376",
    "publication_date": "2018-03-06",
    "publication_year": 2018,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250877836",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3298792",
    "publication_date": "2018-12-16",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As data volumes continue to rise, manual inspection is becoming increasingly untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251798723",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3284689",
    "publication_date": "2018-11-26",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In the design of analytical procedures and machine learning solutions, a critical and time-consuming task is that of feature engineering, for which various recipes and tooling approaches have been developed. In this article, we embark on the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252857760",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1166074",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256109084",
    "type": "paratext"
  },
  {
    "title": "A Game-theoretic Approach to Data Interaction",
    "doi": "https://doi.org/10.1145/3351450",
    "publication_date": "2020-02-08",
    "publication_year": 2020,
    "authors": "Ben McCamish; Vahid Ghadakchi; Arash Termehchy; Behrouz Touri; Eduardo Cotilla‐Sanchez; Liang Huang; Soravit Changpinyo",
    "corresponding_authors": "",
    "abstract": "As most users do not precisely know the structure and/or the content of databases, their queries do not exactly reflect their information needs. The database management system (DBMS) may interact with users and use their feedback on the returned results to learn the information needs behind their queries. Current query interfaces assume that users do not learn and modify the way they express their information needs in the form of queries during their interaction with the DBMS. Using a real-world interaction workload, we show that users learn and modify how to express their information needs during their interactions with the DBMS and their learning is accurately modeled by a well-known reinforcement learning mechanism. As current data interaction systems assume that users do not modify their strategies, they cannot discover the information needs behind users’ queries effectively. We model the interaction between the user and the DBMS as a game with identical interest between two rational agents whose goal is to establish a common language for representing information needs in the form of queries. We propose a reinforcement learning method that learns and answers the information needs behind queries and adapts to the changes in users’ strategies and proves that it improves the effectiveness of answering queries, stochastically speaking. We propose two efficient implementations of this method over large relational databases. Our extensive empirical studies over real-world query workloads indicate that our algorithms are efficient and effective.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3004679574",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3417730",
    "publication_date": "2020-09-11",
    "publication_year": 2020,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239015827",
    "type": "editorial"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3381020",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Christian S. Jensen",
    "corresponding_authors": "Christian S. Jensen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241198443",
    "type": "editorial"
  },
  {
    "title": "ACM Transactions on Database Systems: Editorial",
    "doi": null,
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Richard T. Snodgrass",
    "corresponding_authors": "Richard T. Snodgrass",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2617158739",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/502030.505049",
    "publication_date": "2001-09-01",
    "publication_year": 2001,
    "authors": "Richard T. Snodgrass",
    "corresponding_authors": "Richard T. Snodgrass",
    "abstract": "editorial Free Access Share on Editorial Author: Richard Snodgrass Department of Computer Science, University of Arizona. Department of Computer Science, University of Arizona.View Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 26Issue 3September 2001 pp 261–263https://doi.org/10.1145/502030.505049Published:01 September 2001Publication History 0citation350DownloadsMetricsTotal Citations0Total Downloads350Last 12 Months9Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249562597",
    "type": "editorial"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/503099.505055",
    "publication_date": "2001-12-01",
    "publication_year": 2001,
    "authors": "Richard T. Snodgrass",
    "corresponding_authors": "Richard T. Snodgrass",
    "abstract": "editorial Free Access Share on Editorial Author: Richard Snodgrass Department of Computer Science, University of Arizona. Department of Computer Science, University of Arizona.View Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 26Issue 4pp 385–387https://doi.org/10.1145/503099.505055Published:01 December 2001Publication History 0citation476DownloadsMetricsTotal Citations0Total Downloads476Last 12 Months10Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252008787",
    "type": "editorial"
  },
  {
    "title": "Corrigenda: Linear Queries in Statistical Databases.",
    "doi": null,
    "publication_date": "1980-01-01",
    "publication_year": 1980,
    "authors": "Dorothy E. Denning",
    "corresponding_authors": "Dorothy E. Denning",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W108393895",
    "type": "article"
  },
  {
    "title": "Timely Reporting of Heavy Hitters Using External Memory",
    "doi": "https://doi.org/10.1145/3472392",
    "publication_date": "2021-11-15",
    "publication_year": 2021,
    "authors": "Shikha Singh; Prashant Pandey; Michael A. Bender; Jonathan W. Berry; Martı́n Farach-Colton; Rob Johnson; Thomas M Kroeger; Cynthia A. Phillips",
    "corresponding_authors": "",
    "abstract": "Given an input stream S of size N , a ɸ-heavy hitter is an item that occurs at least ɸN times in S . The problem of finding heavy-hitters is extensively studied in the database literature. We study a real-time heavy-hitters variant in which an element must be reported shortly after we see its T = ɸ N-th occurrence (and hence it becomes a heavy hitter). We call this the Timely Event Detection ( TED ) Problem. The TED problem models the needs of many real-world monitoring systems, which demand accurate (i.e., no false negatives) and timely reporting of all events from large, high-speed streams with a low reporting threshold (high sensitivity). Like the classic heavy-hitters problem, solving the TED problem without false-positives requires large space (Ω (N) words). Thus in-RAM heavy-hitters algorithms typically sacrifice accuracy (i.e., allow false positives), sensitivity, or timeliness (i.e., use multiple passes). We show how to adapt heavy-hitters algorithms to external memory to solve the TED problem on large high-speed streams while guaranteeing accuracy, sensitivity, and timeliness. Our data structures are limited only by I/O-bandwidth (not latency) and support a tunable tradeoff between reporting delay and I/O overhead. With a small bounded reporting delay, our algorithms incur only a logarithmic I/O overhead. We implement and validate our data structures empirically using the Firehose streaming benchmark. Multi-threaded versions of our structures can scale to process 11M observations per second before becoming CPU bound. In comparison, a naive adaptation of the standard heavy-hitters algorithm to external memory would be limited by the storage device’s random I/O throughput, i.e., ≈100K observations per second.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3211944403",
    "type": "article"
  },
  {
    "title": "Michael E. Senko: 1931-1978.",
    "doi": null,
    "publication_date": "1979-01-01",
    "publication_year": 1979,
    "authors": "David K. Hsiao",
    "corresponding_authors": "David K. Hsiao",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W183422534",
    "type": "article"
  },
  {
    "title": "On a partitioning problem",
    "doi": "https://doi.org/10.1145/320263.320287",
    "publication_date": "1978-09-01",
    "publication_year": 1978,
    "authors": "C. Yu; Man‐Keung Siu; Kei Fong Lam",
    "corresponding_authors": "",
    "abstract": "This paper investigates the problem of locating a set of “boundary points” of a large number of records. Conceptually, the boundary points partition the records into subsets of roughly the same number of elements, such that the key values of the records in one subset are all smaller or all larger than those of the records in another subset. We guess the locations of the boundary points by linear interpolation and check their accuracy by reading the key values of the records on one pass. This process is repeated until all boundary points are determined. Clearly, this problem can also be solved by performing an external tape sort. Both analytical and empirical results indicate that the number of passes required is small in comparison with that in an external tape sort. This kind of record partitioning may be of interest in setting up a statistical database system.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2084460975",
    "type": "article"
  },
  {
    "title": "Addendum to \"Automatic Generation of Production Rules for Integrity Maintenance\".",
    "doi": null,
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Stefano Ceri; Piero Fraternali; Stefano Paraboschi; Letizia Tanca",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W115300927",
    "type": "article"
  },
  {
    "title": "Corrigendum: The Theory of Joins in Relational Databases.",
    "doi": null,
    "publication_date": "1983-01-01",
    "publication_year": 1983,
    "authors": "Jeffrey D. Ullman",
    "corresponding_authors": "Jeffrey D. Ullman",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W150735332",
    "type": "erratum"
  },
  {
    "title": "Addendum to \"Analysis of Some New Variants of Coalesced Hashing\".",
    "doi": null,
    "publication_date": "1985-01-01",
    "publication_year": 1985,
    "authors": "Wen‐Chin Chen; Jeffrey Scott Vitter",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W16311766",
    "type": "article"
  },
  {
    "title": "On \"Human Factors Comparison of a Procedural and a Nonprocedural Query Language\".",
    "doi": null,
    "publication_date": "1982-01-01",
    "publication_year": 1982,
    "authors": "Donald D. Chamberlin",
    "corresponding_authors": "Donald D. Chamberlin",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W184426432",
    "type": "article"
  },
  {
    "title": "Report on the international workshop on high-performance transaction systems",
    "doi": "https://doi.org/10.1145/7239.17346",
    "publication_date": "1986-12-01",
    "publication_year": 1986,
    "authors": "Dieter Gawlick",
    "corresponding_authors": "Dieter Gawlick",
    "abstract": "article Free Access Share on Report on the international workshop on high-performance transaction systems Author: Dieter Gawlick Amdahl Corporation, Sunnyvale, CA Amdahl Corporation, Sunnyvale, CAView Profile Authors Info & Claims ACM Transactions on Database SystemsVolume 11Issue 4Dec. 1986 pp 375–377https://doi.org/10.1145/7239.17346Online:01 December 1986Publication History 0citation246DownloadsMetricsTotal Citations0Total Downloads246Last 12 Months4Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1963920394",
    "type": "article"
  },
  {
    "title": "Charter and Scope.",
    "doi": null,
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Won Bae Kim",
    "corresponding_authors": "Won Bae Kim",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W60722347",
    "type": "article"
  }
]