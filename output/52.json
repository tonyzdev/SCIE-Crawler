[
  {
    "title": "Write off-loading",
    "doi": "https://doi.org/10.1145/1416944.1416949",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Dushyanth Narayanan; Austin Donnelly; Antony Rowstron",
    "corresponding_authors": "",
    "abstract": "In enterprise data centers power usage is a problem impacting server density and the total cost of ownership. Storage uses a significant fraction of the power budget and there are no widely deployed power-saving solutions for enterprise storage systems. The traditional view is that enterprise workloads make spinning disks down ineffective because idle periods are too short. We analyzed block-level traces from 36 volumes in an enterprise data center for one week and concluded that significant idle periods exist, and that they can be further increased by modifying the read/write patterns using write off-loading . Write off-loading allows write requests on spun-down disks to be temporarily redirected to persistent storage elsewhere in the data center. The key challenge is doing this transparently and efficiently at the block level, without sacrificing consistency or failure resilience. We describe our write off-loading design and implementation that achieves these goals. We evaluate it by replaying portions of our traces on a rack-based testbed. Results show that just spinning disks down when idle saves 28--36% of energy, and write off-loading further increases the savings to 45--60%.",
    "cited_by_count": 725,
    "openalex_id": "https://openalex.org/W2201220957",
    "type": "article"
  },
  {
    "title": "A study of practical deduplication",
    "doi": "https://doi.org/10.1145/2078861.2078864",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Dutch T. Meyer; William J. Bolosky",
    "corresponding_authors": "",
    "abstract": "We collected file system content data from 857 desktop computers at Microsoft over a span of 4 weeks. We analyzed the data to determine the relative efficacy of data deduplication, particularly considering whole-file versus block-level elimination of redundancy. We found that whole-file deduplication achieves about three quarters of the space savings of the most aggressive block-level deduplication for storage of live file systems, and 87% of the savings for backup images. We also studied file fragmentation, finding that it is not prevalent, and updated prior file system metadata studies, finding that the distribution of file sizes continues to skew toward very large unstructured files.",
    "cited_by_count": 440,
    "openalex_id": "https://openalex.org/W2110322986",
    "type": "article"
  },
  {
    "title": "BTRFS",
    "doi": "https://doi.org/10.1145/2501620.2501623",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Ohad Rodeh; Josef Bacik; Chris Mason",
    "corresponding_authors": "",
    "abstract": "BTRFS is a Linux filesystem that has been adopted as the default filesystem in some popular versions of Linux. It is based on copy-on-write, allowing for efficient snapshots and clones. It uses B-trees as its main on-disk data structure. The design goal is to work well for many use cases and workloads. To this end, much effort has been directed to maintaining even performance as the filesystem ages, rather than trying to support a particular narrow benchmark use-case. Linux filesystems are installed on smartphones as well as enterprise servers. This entails challenges on many different fronts. --- Scalability . The filesystem must scale in many dimensions: disk space, memory, and CPUs. --- Data integrity . Losing data is not an option, and much effort is expended to safeguard the content. This includes checksums, metadata duplication, and RAID support built into the filesystem. --- Disk diversity . The system should work well with SSDs and hard disks. It is also expected to be able to use an array of different sized disks, which poses challenges to the RAID and striping mechanisms. This article describes the core ideas, data structures, and algorithms of this filesystem. It sheds light on the challenges posed by defragmentation in the presence of snapshots, and the tradeoffs required to maintain even performance in the face of a wide spectrum of workloads.",
    "cited_by_count": 350,
    "openalex_id": "https://openalex.org/W1964802076",
    "type": "article"
  },
  {
    "title": "A five-year study of file-system metadata",
    "doi": "https://doi.org/10.1145/1288783.1288788",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Nitin Agrawal; William J. Bolosky; John R. Douceur; Jacob R. Lorch",
    "corresponding_authors": "",
    "abstract": "For five years, we collected annual snapshots of file-system metadata from over 60,000 Windows PC file systems in a large corporation. In this article, we use these snapshots to study temporal changes in file size, file age, file-type frequency, directory size, namespace structure, file-system population, storage capacity and consumption, and degree of file modification. We present a generative model that explains the namespace structure and the distribution of directory sizes. We find significant temporal trends relating to the popularity of certain file types, the origin of file content, the way the namespace is used, and the degree of variation among file systems, as well as more pedestrian changes in size and capacities. We give examples of consequent lessons for designers of file systems and related software.",
    "cited_by_count": 328,
    "openalex_id": "https://openalex.org/W2160127232",
    "type": "article"
  },
  {
    "title": "DepSky",
    "doi": "https://doi.org/10.1145/2535929",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Alysson Bessani; Miguel Correia; Bruno Quaresma; Fernando André; Paulo Sousa",
    "corresponding_authors": "",
    "abstract": "The increasing popularity of cloud storage services has lead companies that handle critical data to think about using these services for their storage needs. Medical record databases, large biomedical datasets, historical information about power systems and financial data are some examples of critical data that could be moved to the cloud. However, the reliability and security of data stored in the cloud still remain major concerns. In this work we present DepSky, a system that improves the availability, integrity, and confidentiality of information stored in the cloud through the encryption, encoding, and replication of the data on diverse clouds that form a cloud-of-clouds. We deployed our system using four commercial clouds and used PlanetLab to run clients accessing the service from different countries. We observed that our protocols improved the perceived availability, and in most cases, the access latency, when compared with cloud providers individually. Moreover, the monetary costs of using DepSky in this scenario is at most twice the cost of using a single cloud, which is optimal and seems to be a reasonable cost, given the benefits.",
    "cited_by_count": 317,
    "openalex_id": "https://openalex.org/W2162460714",
    "type": "article"
  },
  {
    "title": "WiscKey",
    "doi": "https://doi.org/10.1145/3033273",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "L. Lu; Thanumalayan Sankaranarayana Pillai; Hariharan Gopalakrishnan; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We present WiscKey, a persistent LSM-tree-based key-value store with a performance-oriented data layout that separates keys from values to minimize I/O amplification. The design of WiscKey is highly SSD optimized, leveraging both the sequential and random performance characteristics of the device. We demonstrate the advantages of WiscKey with both microbenchmarks and YCSB workloads. Microbenchmark results show that WiscKey is 2.5 × to 111 × faster than LevelDB for loading a database (with significantly better tail latencies) and 1.6 × to 14 × faster for random lookups. WiscKey is faster than both LevelDB and RocksDB in all six YCSB workloads.",
    "cited_by_count": 239,
    "openalex_id": "https://openalex.org/W2594680891",
    "type": "article"
  },
  {
    "title": "RocksDB: Evolution of Development Priorities in a Key-value Store Serving Large-scale Applications",
    "doi": "https://doi.org/10.1145/3483840",
    "publication_date": "2021-10-15",
    "publication_year": 2021,
    "authors": "Siying Dong; Andrew Kryczka; Yanqin Jin; Michael Stumm",
    "corresponding_authors": "",
    "abstract": "This article is an eight-year retrospective on development priorities for RocksDB, a key-value store developed at Facebook that targets large-scale distributed systems and that is optimized for Solid State Drives (SSDs). We describe how the priorities evolved over time as a result of hardware trends and extensive experiences running RocksDB at scale in production at a number of organizations: from optimizing write amplification, to space amplification, to CPU utilization. We describe lessons from running large-scale applications, including that resource allocation needs to be managed across different RocksDB instances, that data formats need to remain backward- and forward-compatible to allow incremental software rollouts, and that appropriate support for database replication and backups are needed. Lessons from failure handling taught us that data corruption errors needed to be detected earlier and that data integrity protection mechanisms are needed at every layer of the system. We describe improvements to the key-value interface. We describe a number of efforts that in retrospect proved to be misguided. Finally, we describe a number of open problems that could benefit from future research.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W3207452942",
    "type": "article"
  },
  {
    "title": "The Design of Fast and Lightweight Resemblance Detection for Efficient Post-Deduplication Delta Compression",
    "doi": "https://doi.org/10.1145/3584663",
    "publication_date": "2023-02-16",
    "publication_year": 2023,
    "authors": "Wen Xia; Lifeng Pu; Xiangyu Zou; Philip Shilane; Shiyi Li; Haijun Zhang; Xuan Wang",
    "corresponding_authors": "",
    "abstract": "Post-deduplication delta compression is a data reduction technique that calculates and stores the differences of very similar but non-duplicate chunks in storage systems, which is able to achieve a very high compression ratio. However, the low throughput of widely used resemblance detection approaches (e.g., N-Transform) usually becomes the bottleneck of delta compression systems due to introducing high computational overhead. Generally, this overhead mainly consists of two parts: ① calculating the rolling hash byte by byte across data chunks and ② applying multiple transforms on all of the calculated rolling hash values. In this article, we propose Odess, a fast and lightweight resemblance detection approach, that greatly reduces the computational overhead for resemblance detection while achieving high detection accuracy and a high compression ratio. Odess first utilizes a novel Subwindow-based Parallel Rolling (SWPR) hash method using Single Instruction Multiple Data [ 1 ] (SIMD) to accelerate calculation of rolling hashes (corresponding to the first part of the overhead). Odess then uses a novel Content-Defined Sampling method to generate a much smaller proxy hash set from the whole rolling hash set and quickly applies transforms on this small hash set for resemblance detection (corresponding to the second part of the overhead). Evaluation results show that during the stage of resemblance detection, the Odess approach is ∼31.4× and ∼7.9× faster than the state-of-the-art N-Transform and Finesse (a recent variant of N-Transform [ 39 ]), respectively. When considering an end-to-end data reduction storage system, the Odess-based system’s throughput is about 3.20× and 1.41× higher than the N-Transform- and Finesse-based systems’ throughput, respectively, while maintaining the high compression ratio of N-Transform and achieving ∼1.22× higher compression ratio over Finesse.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W4321089583",
    "type": "article"
  },
  {
    "title": "Authentication and integrity in outsourced databases",
    "doi": "https://doi.org/10.1145/1149976.1149977",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Einar Mykletun; Maithili Narasimha; Gene Tsudik",
    "corresponding_authors": "",
    "abstract": "In the Outsourced Database (ODB) model, entities outsource their data management needs to a third-party service provider. Such a service provider offers mechanisms for its clients to create, store, update, and access (query) their databases. This work provides mechanisms to ensure data integrity and authenticity for outsourced databases. Specifically, this article provides mechanisms that assure the querier that the query results have not been tampered with and are authentic (with respect to the actual data owner). It investigates both the security and efficiency aspects of the problem and constructs several secure and practical schemes that facilitate the integrity and authenticity of query replies while incurring low computational and communication costs.",
    "cited_by_count": 299,
    "openalex_id": "https://openalex.org/W2075280979",
    "type": "article"
  },
  {
    "title": "An analysis of data corruption in the storage stack",
    "doi": "https://doi.org/10.1145/1416944.1416947",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Lakshmi N. Bairavasundaram; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau; Garth R. Goodson; Bianca Schroeder",
    "corresponding_authors": "",
    "abstract": "An important threat to reliable storage of data is silent data corruption. In order to develop suitable protection mechanisms against data corruption, it is essential to understand its characteristics. In this article, we present the first large-scale study of data corruption. We analyze corruption instances recorded in production storage systems containing a total of 1.53 million disk drives, over a period of 41 months. We study three classes of corruption: checksum mismatches, identity discrepancies, and parity inconsistencies. We focus on checksum mismatches since they occur the most. We find more than 400,000 instances of checksum mismatches over the 41-month period. We find many interesting trends among these instances, including: (i) nearline disks (and their adapters) develop checksum mismatches an order of magnitude more often than enterprise-class disk drives, (ii) checksum mismatches within the same disk are not independent events and they show high spatial and temporal locality, and (iii) checksum mismatches across different disks in the same storage system are not independent. We use our observations to derive lessons for corruption-proof system design.",
    "cited_by_count": 278,
    "openalex_id": "https://openalex.org/W2103315535",
    "type": "article"
  },
  {
    "title": "I/O Deduplication",
    "doi": "https://doi.org/10.1145/1837915.1837921",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Ricardo Koller; Raju Rangaswami",
    "corresponding_authors": "",
    "abstract": "Duplication of data in storage systems is becoming increasingly common. We introduce I/O Deduplication, a storage optimization that utilizes content similarity for improving I/O performance by eliminating I/O operations and reducing the mechanical delays during I/O operations. I/O Deduplication consists of three main techniques: content-based caching, dynamic replica retrieval , and selective duplication . Each of these techniques is motivated by our observations with I/O workload traces obtained from actively-used production storage systems, all of which revealed surprisingly high levels of content similarity for both stored and accessed data. Evaluation of a prototype implementation using these workloads showed an overall improvement in disk I/O performance of 28 to 47% across these workloads. Further breakdown also showed that each of the three techniques contributed significantly to the overall performance improvement.",
    "cited_by_count": 222,
    "openalex_id": "https://openalex.org/W2147407897",
    "type": "article"
  },
  {
    "title": "A nine year study of file system and storage benchmarking",
    "doi": "https://doi.org/10.1145/1367829.1367831",
    "publication_date": "2008-05-11",
    "publication_year": 2008,
    "authors": "Avishay Traeger; Erez Zadok; Nikolai Joukov; Charles P. Wright",
    "corresponding_authors": "",
    "abstract": "Benchmarking is critical when evaluating performance, but is especially difficult for file and storage systems. Complex interactions between I/O devices, caches, kernel daemons, and other OS components result in behavior that is rather difficult to analyze. Moreover, systems have different features and optimizations, so no single benchmark is always suitable. The large variety of workloads that these systems experience in the real world also adds to this difficulty. In this article we survey 415 file system and storage benchmarks from 106 recent papers. We found that most popular benchmarks are flawed and many research papers do not provide a clear indication of true performance. We provide guidelines that we hope will improve future performance evaluations. To show how some widely used benchmarks can conceal or overemphasize overheads, we conducted a set of experiments. As a specific example, slowing down read operations on ext2 by a factor of 32 resulted in only a 2--5% wall-clock slowdown in a popular compile benchmark. Finally, we discuss future work to improve file system and storage benchmarking.",
    "cited_by_count": 205,
    "openalex_id": "https://openalex.org/W2159081928",
    "type": "article"
  },
  {
    "title": "PARAID",
    "doi": "https://doi.org/10.1145/1288783.1289721",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Charles Weddle; Mathew Oldham; Jin-yuan Qian; An-I Andy Wang; Peter Reiher; Geoff Kuenning",
    "corresponding_authors": "",
    "abstract": "Reducing power consumption for server-class computers is important, since increased energy usage causes more heat dissipation, greater cooling requirements, reduced computational density, and higher operating costs. For a typical data center, storage accounts for 27% of energy consumption. Conventional server-class RAIDs cannot easily reduce power because loads are balanced to use all disks, even for light loads. We have built the power-aware RAID (PARAID), which reduces energy use of commodity server-class disks without specialized hardware. PARAID uses a skewed striping pattern to adapt to the system load by varying the number of powered disks. By spinning disks down during light loads, PARAID can reduce power consumption, while still meeting performance demands, by matching the number of powered disks to the system load. Reliability is achieved by limiting disk power cycles and using different RAID encoding schemes. Based on our five-disk prototype, PARAID uses up to 34% less power than conventional RAIDs while achieving similar performance and reliability.",
    "cited_by_count": 198,
    "openalex_id": "https://openalex.org/W2167118853",
    "type": "article"
  },
  {
    "title": "Efficient identification of hot data for flash memory storage systems",
    "doi": "https://doi.org/10.1145/1138041.1138043",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Jen-Wei Hsieh; Tei‐Wei Kuo; Li-Pin Chang",
    "corresponding_authors": "",
    "abstract": "Hot data identification for flash memory storage systems not only imposes great impacts on flash memory garbage collection but also strongly affects the performance of flash memory access and its lifetime (due to wear-levelling). This research proposes a highly efficient method for on-line hot data identification with limited space requirements. Different from past work, multiple independent hash functions are adopted to reduce the chance of false identification of hot data and to provide predictable and excellent performance for hot data identification. This research not only offers an efficient implementation for the proposed framework, but also presents an analytic study on the chance of false hot data identification. A series of experiments was conducted to verify the performance of the proposed method, and very encouraging results are presented.",
    "cited_by_count": 192,
    "openalex_id": "https://openalex.org/W1963830244",
    "type": "article"
  },
  {
    "title": "A new approach to secure logging",
    "doi": "https://doi.org/10.1145/1502777.1502779",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Di Ma; Gene Tsudik",
    "corresponding_authors": "",
    "abstract": "The need for secure logging is well-understood by the security professionals, including both researchers and practitioners. The ability to efficiently verify all (or some) log entries is important to any application employing secure logging techniques. In this article, we begin by examining the state of the art in secure logging and identify some problems inherent to systems based on trusted third-party servers. We then propose a different approach to secure logging based upon recently developed Forward-Secure Sequential Aggregate (FssAgg) authentication techniques. Our approach offers both space-efficiency and provable security. We illustrate two concrete schemes—one private-verifiable and one public-verifiable—that offer practical secure logging without any reliance on online trusted third parties or secure hardware. We also investigate the concept of immutability in the context of forward-secure sequential aggregate authentication to provide finer grained verification. Finally we evaluate proposed schemes and report on our experience with implementing them within a secure logging system.",
    "cited_by_count": 189,
    "openalex_id": "https://openalex.org/W1983949504",
    "type": "article"
  },
  {
    "title": "DFS",
    "doi": "https://doi.org/10.1145/1837915.1837922",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "William Josephson; Lars Ailo Bongo; Kai Li; David Flynn",
    "corresponding_authors": "",
    "abstract": "We present the design, implementation, and evaluation of Direct File System (DFS) for virtualized flash storage. Instead of using traditional layers of abstraction, our layers of abstraction are designed for directly accessing flash memory devices. DFS has two main novel features. First, it lays out its files directly in a very large virtual storage address space provided by FusionIO's virtual flash storage layer. Second, it leverages the virtual flash storage layer to perform block allocations and atomic updates. As a result, DFS performs better and is much simpler than a traditional Unix file system with similar functionalities. Our microbenchmark results show that DFS can deliver 94,000 I/O operations per second (IOPS) for direct reads and 71,000 IOPS for direct writes with the virtualized flash storage layer on FusionIO's ioDrive. For direct access performance, DFS is consistently better than ext3 on the same platform, sometimes by 20%. For buffered access performance, DFS is also consistently better than ext3, and sometimes by over 149%. Our application benchmarks show that DFS outperforms ext3 by 7% to 250% while requiring less CPU power.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W2111262474",
    "type": "article"
  },
  {
    "title": "Understanding and Improving Computational Science Storage Access through Continuous Characterization",
    "doi": "https://doi.org/10.1145/2027066.2027068",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Philip Carns; Kevin Harms; William Allcock; Charles Bacon; Samuel Lang; Robert Latham; Robert Ross",
    "corresponding_authors": "",
    "abstract": "Computational science applications are driving a demand for increasingly powerful storage systems. While many techniques are available for capturing the I/O behavior of individual application trial runs and specific components of the storage system, continuous characterization of a production system remains a daunting challenge for systems with hundreds of thousands of compute cores and multiple petabytes of storage. As a result, these storage systems are often designed without a clear understanding of the diverse computational science workloads they will support. In this study, we outline a methodology for scalable, continuous, systemwide I/O characterization that combines storage device instrumentation, static file system analysis, and a new mechanism for capturing detailed application-level behavior. This methodology allows us to identify both system-wide trends and application-specific I/O strategies. We demonstrate the effectiveness of our methodology by performing a multilevel, two-month study of Intrepid, a 557-teraflop IBM Blue Gene/P system. During that time, we captured application-level I/O characterizations from 6,481 unique jobs spanning 38 science and engineering projects. We used the results of our study to tune example applications, highlight trends that impact the design of future storage systems, and identify opportunities for improvement in I/O characterization methodology.",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W2021171338",
    "type": "article"
  },
  {
    "title": "Cumulus",
    "doi": "https://doi.org/10.1145/1629080.1629084",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Michael Vrable; Stefan Savage; Geoffrey M. Voelker",
    "corresponding_authors": "",
    "abstract": "Cumulus is a system for efficiently implementing filesystem backups over the Internet, specifically designed under a thin cloud assumption—that the remote datacenter storing the backups does not provide any special backup services, but only a least-common-denominator storage interface. Cumulus aggregates data from small files for storage and uses LFS-inspired segment cleaning to maintain storage efficiency. While Cumulus can use virtually any storage service, we show its efficiency is comparable to integrated approaches.",
    "cited_by_count": 163,
    "openalex_id": "https://openalex.org/W2112671267",
    "type": "article"
  },
  {
    "title": "Pyramid Codes",
    "doi": "https://doi.org/10.1145/2435204.2435207",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Cheng Huang; Minghua Chen; Jin Li",
    "corresponding_authors": "",
    "abstract": "We design flexible schemes to explore the tradeoffs between storage space and access efficiency in reliable data storage systems. Aiming at this goal, two new classes of erasure-resilient codes are introduced -- Basic Pyramid Codes (BPC) and Generalized Pyramid Codes (GPC). Both schemes require slightly more storage space than conventional schemes, but significantly improve the critical performance of read during failures and unavailability. As a by-product, we establish a necessary matching condition to characterize the limit of failure recovery, that is, unless the matching condition is satisfied, a failure case is impossible to recover. In addition, we define a maximally recoverable (MR) property. For all ERC schemes holding the MR property, the matching condition becomes sufficient, that is, all failure cases satisfying the matching condition are indeed recoverable. We show that GPC is the first class of non-MDS schemes holding the MR property.",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W1996042140",
    "type": "article"
  },
  {
    "title": "Are disks the dominant contributor for storage failures?",
    "doi": "https://doi.org/10.1145/1416944.1416946",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Weihang Jiang; Chongfeng Hu; Yuanyuan Zhou; Arkady Kanevsky",
    "corresponding_authors": "",
    "abstract": "Building reliable storage systems becomes increasingly challenging as the complexity of modern storage systems continues to grow. Understanding storage failure characteristics is crucially important for designing and building a reliable storage system. While several recent studies have been conducted on understanding storage failures, almost all of them focus on the failure characteristics of one component—disks—and do not study other storage component failures. This article analyzes the failure characteristics of storage subsystems. More specifically, we analyzed the storage logs collected from about 39,000 storage systems commercially deployed at various customer sites. The dataset covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage-shelf enclosures. Our study reveals many interesting findings, providing useful guidelines for designing reliable storage systems. Some of our major findings include: (1) In addition to disk failures that contribute to 20--55% of storage subsystem failures, other components such as physical interconnects and protocol stacks also account for a significant percentage of storage subsystem failures. (2) Each individual storage subsystem failure type, and storage subsystem failure as a whole, exhibits strong self-correlations. In addition, these failures exhibit “bursty” patterns. (3) Storage subsystems configured with redundant interconnects experience 30--40% lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for storage subsystems than within a single shelf.",
    "cited_by_count": 159,
    "openalex_id": "https://openalex.org/W2125646999",
    "type": "article"
  },
  {
    "title": "Strong accountability for network storage",
    "doi": "https://doi.org/10.1145/1288783.1288786",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Aydan Yumerefendi; Jeffrey S. Chase",
    "corresponding_authors": "",
    "abstract": "This article presents the design, implementation, and evaluation of CATS, a network storage service with strong accountability properties. CATS offers a simple web services interface that allows clients to read and write opaque objects of variable size. This interface is similar to the one offered by existing commercial Internet storage services. CATS extends the functionality of commercial Internet storage services by offering support for strong accountability. A CATS server annotates read and write responses with evidence of correct execution, and offers audit and challenge interfaces that enable clients to verify that the server is faithful. A faulty server cannot conceal its misbehavior, and evidence of misbehavior is independently verifiable by any participant. CATS clients are also accountable for their actions on the service. A client cannot deny its actions, and the server can prove the impact of those actions on the state views it presented to other clients. Experiments with a CATS prototype evaluate the cost of accountability under a range of conditions and expose the primary factors influencing the level of assurance and the performance of a strongly accountable storage server. The results show that strong accountability is practical for network storage systems in settings with strong identity and modest degrees of write-sharing. We discuss how the accountability concepts and techniques used in CATS generalize to other classes of network services.",
    "cited_by_count": 154,
    "openalex_id": "https://openalex.org/W2197404543",
    "type": "article"
  },
  {
    "title": "Revisiting storage for smartphones",
    "doi": "https://doi.org/10.1145/2385603.2385607",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Hyojun Kim; Nitin Agrawal; Cristian Ungureanu",
    "corresponding_authors": "",
    "abstract": "Conventional wisdom holds that storage is not a big contributor to application performance on mobile devices. Flash storage (the type most commonly used today) draws little power, and its performance is thought to exceed that of the network subsystem. In this article, we present evidence that storage performance does indeed affect the performance of several common applications such as Web browsing, maps, application install, email, and Facebook. For several Android smartphones, we find that just by varying the underlying flash storage, performance over WiFi can typically vary between 100% and 300% across applications; in one extreme scenario, the variation jumped to over 2000%. With a faster network (set up over USB), the performance variation rose even further. We identify the reasons for the strong correlation between storage and application performance to be a combination of poor flash device performance, random I/O from application databases, and heavy-handed use of synchronous writes. Based on our findings, we implement and evaluate a set of pilot solutions to address the storage performance deficiencies in smartphones.",
    "cited_by_count": 148,
    "openalex_id": "https://openalex.org/W2132929952",
    "type": "article"
  },
  {
    "title": "Tiny-Tail Flash",
    "doi": "https://doi.org/10.1145/3121133",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Shiqin Yan; Huaicheng Li; Mingzhe Hao; Michael Hao Tong; Sundararaman Swaminathan; Andrew A. Chien; Haryadi S. Gunawi",
    "corresponding_authors": "",
    "abstract": "Flash storage has become the mainstream destination for storage users. However, SSDs do not always deliver the performance that users expect. The core culprit of flash performance instability is the well-known garbage collection (GC) process, which causes long delays as the SSD cannot serve (blocks) incoming I/Os, which then induces the long tail latency problem. We present tt F lash as a solution to this problem. tt F lash is a “tiny-tail” flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GC-blocked I/Os with four novel strategies: plane-blocking GC, rotating GC, GC-tolerant read, and GC-tolerant flush. These four strategies leverage the timely combination of modern SSD internal technologies such as powerful controllers, parity-based redundancies, and capacitor-backed RAM. Our strategies are dependent on the use of intra-plane copyback operations. Through an extensive evaluation, we show that tt F lash comes significantly close to a “no-GC” scenario. Specifically, between the 99 and 99.99th percentiles, tt F lash is only 1.0 to 2.6× slower than the no-GC case, while a base approach suffers from 5–138× GC-induced slowdowns.",
    "cited_by_count": 144,
    "openalex_id": "https://openalex.org/W2765844783",
    "type": "article"
  },
  {
    "title": "Understanding latent sector errors and how to protect against them",
    "doi": "https://doi.org/10.1145/1837915.1837917",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Bianca Schroeder; Sotirios Damouras; Phillipa Gill",
    "corresponding_authors": "",
    "abstract": "Latent sector errors (LSEs) refer to the situation where particular sectors on a drive become inaccessible. LSEs are a critical factor in data reliability, since a single LSE can lead to data loss when encountered during RAID reconstruction after a disk failure or in systems without redundancy. LSEs happen at a significant rate in the field [Bairavasundaram et al. 2007], and are expected to grow more frequent with new drive technologies and increasing drive capacities. While two approaches, data scrubbing and intra-disk redundancy, have been proposed to reduce data loss due to LSEs, none of these approaches has been evaluated on real field data. This article makes two contributions. We provide an extended statistical analysis of latent sector errors in the field, specifically from the view point of how to protect against LSEs. In addition to providing interesting insights into LSEs, we hope the results (including parameters for models we fit to the data) will help researchers and practitioners without access to data in driving their simulations or analysis of LSEs. Our second contribution is an evaluation of five different scrubbing policies and five different intra-disk redundancy schemes and their potential in protecting against LSEs. Our study includes schemes and policies that have been suggested before, but have never been evaluated on field data, as well as new policies that we propose based on our analysis of LSEs in the field.",
    "cited_by_count": 133,
    "openalex_id": "https://openalex.org/W2138872609",
    "type": "article"
  },
  {
    "title": "TinyLFU",
    "doi": "https://doi.org/10.1145/3149371",
    "publication_date": "2017-11-17",
    "publication_year": 2017,
    "authors": "Gil Einziger; Roy Friedman; Ben Manes",
    "corresponding_authors": "",
    "abstract": "This article proposes to use a frequency-based cache admission policy in order to boost the effectiveness of caches subject to skewed access distributions. Given a newly accessed item and an eviction candidate from the cache, our scheme decides, based on the recent access history, whether it is worth admitting the new item into the cache at the expense of the eviction candidate. This concept is enabled through a novel approximate LFU structure called TinyLFU , which maintains an approximate representation of the access frequency of a large sample of recently accessed items. TinyLFU is very compact and lightweight as it builds upon Bloom filter theory. We study the properties of TinyLFU through simulations of both synthetic workloads and multiple real traces from several sources. These simulations demonstrate the performance boost obtained by enhancing various replacement policies with the TinyLFU admission policy. Also, a new combined replacement and eviction policy scheme nicknamed W-TinyLFU is presented. W-TinyLFU is demonstrated to obtain equal or better hit ratios than other state-of-the-art replacement policies on these traces. It is the only scheme to obtain such good results on all traces.",
    "cited_by_count": 115,
    "openalex_id": "https://openalex.org/W2963979055",
    "type": "article"
  },
  {
    "title": "Fail-Slow at Scale",
    "doi": "https://doi.org/10.1145/3242086",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Haryadi S. Gunawi; Riza O. Suminto; Russell Sears; Casey Golliher; Sundararaman Swaminathan; Xing Lin; Tim Emami; Weiguang Sheng; Nematollah Bidokhti; Caitie McCaffrey; Deepthi Srinivasan; Biswaranjan Panda; Andrew Baptist; Gary Grider; Parks Fields; Kevin Harms; Robert Ross; Andree Jacobson; Robert Ricci; Kirk Webb; Peter Alvaro; Hakizumwami Birali Runesha; Mingzhe Hao; Huaicheng Li",
    "corresponding_authors": "",
    "abstract": "Fail-slow hardware is an under-studied failure mode. We present a study of 114 reports of fail-slow hardware incidents, collected from large-scale cluster deployments in 14 institutions. We show that all hardware types such as disk, SSD, CPU, memory, and network components can exhibit performance faults. We made several important observations such as faults convert from one form to another, the cascading root causes and impacts can be long, and fail-slow faults can have varying symptoms. From this study, we make suggestions to vendors, operators, and systems designers.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2895690683",
    "type": "article"
  },
  {
    "title": "From Hyper-dimensional Structures to Linear Structures: Maintaining Deduplicated Data’s Locality",
    "doi": "https://doi.org/10.1145/3507921",
    "publication_date": "2022-06-02",
    "publication_year": 2022,
    "authors": "Xiangyu Zou; Jingsong Yuan; Philip Shilane; Wen Xia; Haijun Zhang; Xuan Wang",
    "corresponding_authors": "",
    "abstract": "Data deduplication is widely used to reduce the size of backup workloads, but it has the known disadvantage of causing poor data locality, also referred to as the fragmentation problem. This results from the gap between the hyper-dimensional structure of deduplicated data and the sequential nature of many storage devices, and this leads to poor restore and garbage collection (GC) performance. Current research has considered writing duplicates to maintain locality (e.g., rewriting) or caching data in memory or SSD, but fragmentation continues to lower restore and GC performance. Investigating the locality issue, we design a method to flatten the hyper-dimensional structured deduplicated data to a one-dimensional format, which is based on classification of each chunk’s lifecycle, and this creates our proposed data layout. Furthermore, we present a novel management-friendly deduplication framework, called MFDedup, that applies our data layout and maintains locality as much as possible. Specifically, we use two key techniques in MFDedup: Neighbor-duplicate-focus indexing (NDF) and Across-version-aware Reorganization scheme (AVAR). NDF performs duplicate detection against a previous backup, then AVAR rearranges chunks with an offline and iterative algorithm into a compact, sequential layout, which nearly eliminates random I/O during file restores after deduplication. Evaluation results with five backup datasets demonstrate that, compared with state-of-the-art techniques, MFDedup achieves deduplication ratios that are 1.12× to 2.19× higher and restore throughputs that are 1.92× to 10.02× faster due to the improved data layout. While the rearranging stage introduces overheads, it is more than offset by a nearly-zero overhead GC process. Moreover, the NDF index only requires indices for two backup versions, while the traditional index grows with the number of versions retained.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W4281673957",
    "type": "article"
  },
  {
    "title": "SSD-based Workload Characteristics and Their Performance Implications",
    "doi": "https://doi.org/10.1145/3423137",
    "publication_date": "2021-01-08",
    "publication_year": 2021,
    "authors": "Gala Yadgar; Moshe Gabel; Shehbaz Jaffer; Bianca Schroeder",
    "corresponding_authors": "",
    "abstract": "Storage systems are designed and optimized relying on wisdom derived from analysis studies of file-system and block-level workloads. However, while SSDs are becoming a dominant building block in many storage systems, their design continues to build on knowledge derived from analysis targeted at hard disk optimization. Though still valuable, it does not cover important aspects relevant for SSD performance. In a sense, we are “searching under the streetlight,” possibly missing important opportunities for optimizing storage system design. We present the first I/O workload analysis designed with SSDs in mind. We characterize traces from four repositories and examine their “temperature” ranges, sensitivity to page size, and “logical locality.” We then take the first step towards correlating these characteristics with three standard performance metrics: write amplification, read amplification, and flash read costs. Our results show that SSD-specific characteristics strongly affect performance, often in surprising ways.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W3119951918",
    "type": "article"
  },
  {
    "title": "An Overview of the Singularity Project",
    "doi": null,
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Galen Hunt; James R. Larus; Martı́n Abadi; Mark Aiken; Paul Barham; Manuel Fähndrich; Chris Hawblitzel; Orion Hodson; Steven Levi; Nicholas A. Murphy; Bjarne Steensgaard; David Tarditi; Ted Wobber; Brian Zill",
    "corresponding_authors": "",
    "abstract": "Abstract. Singularity is a research project in Microsoft Research that started with the question: what would a software platform look like if it was designed from scratch with the primary goal of dependability? Singularity is working to answer this question by building on advances in programming languages and tools to develop a new system architecture and operating system (named Singularity), with the aim of producing a more robust and dependable software platform. Singularity demonstrates the practicality of new technologies and architectural decisions, which should lead to the construction of more robust and dependable systems.",
    "cited_by_count": 147,
    "openalex_id": "https://openalex.org/W1864473657",
    "type": "article"
  },
  {
    "title": "Improving duplicate elimination in storage systems",
    "doi": "https://doi.org/10.1145/1210596.1210599",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Deepak R. Bobbarjung; Suresh Jagannathan; Cezary Dubnicki",
    "corresponding_authors": "",
    "abstract": "Minimizing the amount of data that must be stored and managed is a key goal for any storage architecture that purports to be scalable. One way to achieve this goal is to avoid maintaining duplicate copies of the same data. Eliminating redundant data at the source by not writing data which has already been stored not only reduces storage overheads, but can also improve bandwidth utilization. For these reasons, in the face of today's exponentially growing data volumes, redundant data elimination techniques have assumed critical significance in the design of modern storage systems.Intelligent object partitioning techniques identify data that is new when objects are updated, and transfer only these chunks to a storage server. In this article, we propose a new object partitioning technique, called fingerdiff , that improves upon existing schemes in several important respects. Most notably, fingerdiff dynamically chooses a partitioning strategy for a data object based on its similarities with previously stored objects in order to improve storage and bandwidth utilization. We present a detailed evaluation of fingerdiff , and other existing object partitioning schemes, using a set of real-world workloads. We show that for these workloads, the duplicate elimination strategies employed by fingerdiff improve storage utilization on average by 25%, and bandwidth utilization on average by 40% over comparable techniques.",
    "cited_by_count": 147,
    "openalex_id": "https://openalex.org/W2004286258",
    "type": "article"
  },
  {
    "title": "Ext3cow: a time-shifting file system for regulatory compliance",
    "doi": "https://doi.org/10.1145/1063786.1063789",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Zachary Peterson; Randal Burns",
    "corresponding_authors": "",
    "abstract": "The ext3cow file system, built on the popular ext3 file system, provides an open-source file versioning and snapshot platform for compliance with the versioning and audtitability requirements of recent electronic record retention legislation. Ext3cow provides a time-shifting interface that permits a real-time and continuous view of data in the past. Time-shifting does not pollute the file system namespace nor require snapshots to be mounted as a separate file system. Further, ext3cow is implemented entirely in the file system space and, therefore, does not modify kernel interfaces or change the operation of other file systems. Ext3cow takes advantage of the fine-grained control of on-disk and in-memory data available only to a file system, resulting in minimal degradation of performance and functionality. Experimental results confirm this hypothesis; ext3cow performs comparably to ext3 on many benchmarks and on trace-driven experiments.",
    "cited_by_count": 146,
    "openalex_id": "https://openalex.org/W2161212857",
    "type": "article"
  },
  {
    "title": "Preventing history forgery with secure provenance",
    "doi": "https://doi.org/10.1145/1629080.1629082",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Ragib Hasan; Radu Sion; Marianne Winslett",
    "corresponding_authors": "",
    "abstract": "As increasing amounts of valuable information are produced and persist digitally, the ability to determine the origin of data becomes important. In science, medicine, commerce, and government, data provenance tracking is essential for rights protection, regulatory compliance, management of intelligence and medical data, and authentication of information as it flows through workplace tasks. While significant research has been conducted in this area, the associated security and privacy issues have not been explored, leaving provenance information vulnerable to illicit alteration as it passes through untrusted environments. In this article, we show how to provide strong integrity and confidentiality assurances for data provenance information at the kernel, file system, or application layer. We describe Sprov, our provenance-aware system prototype that implements provenance tracking of data writes at the application layer, which makes Sprov extremely easy to deploy. We present empirical results that show that, for real-life workloads, the runtime overhead of Sprov for recording provenance with confidentiality and integrity guarantees ranges from 1% to 13%, when all file modifications are recorded, and from 12% to 16%, when all file read and modifications are tracked.",
    "cited_by_count": 130,
    "openalex_id": "https://openalex.org/W2142753309",
    "type": "article"
  },
  {
    "title": "Understanding disk failure rates",
    "doi": "https://doi.org/10.1145/1288783.1288785",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Bianca Schroeder; Garth A. Gibson",
    "corresponding_authors": "",
    "abstract": "Component failure in large-scale IT installations is becoming an ever-larger problem as the number of components in a single cluster approaches a million. This article is an extension of our previous study on disk failures [Schroeder and Gibson 2007] and presents and analyzes field-gathered disk replacement data from a number of large production systems, including high-performance computing sites and internet services sites. More than 110,000 disks are covered by this data, some for an entire lifetime of five years. The data includes drives with SCSI and FC, as well as SATA interfaces. The mean time-to-failure (MTTF) of those drives, as specified in their datasheets, ranges from 1,000,000 to 1,500,000 hours, suggesting a nominal annual failure rate of at most 0.88%. We find that in the field, annual disk replacement rates typically exceed 1%, with 2--4% common and up to 13% observed on some systems. This suggests that field replacement is a fairly different process than one might predict based on datasheet MTTF. We also find evidence, based on records of disk replacements in the field, that failure rate is not constant with age, and that rather than a significant infant mortality effect, we see a significant early onset of wear-out degradation. In other words, the replacement rates in our data grew constantly with age, an effect often assumed not to set in until after a nominal lifetime of 5 years. Interestingly, we observe little difference in replacement rates between SCSI, FC, and SATA drives, potentially an indication that disk-independent factors such as operating conditions affect replacement rates more than component-specific ones. On the other hand, we see only one instance of a customer rejecting an entire population of disks as a bad batch, in this case because of media error rates, and this instance involved SATA disks. Time between replacement, a proxy for time between failure, is not well modeled by an exponential distribution and exhibits significant levels of correlation, including autocorrelation and long-range dependence.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W2023917386",
    "type": "article"
  },
  {
    "title": "Differential RAID",
    "doi": "https://doi.org/10.1145/1807060.1807061",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Mahesh Balakrishnan; Asim Kadav; Vijayan Prabhakaran; Dahlia Malkhi",
    "corresponding_authors": "",
    "abstract": "SSDs exhibit very different failure characteristics compared to hard drives. In particular, the bit error rate (BER) of an SSD climbs as it receives more writes. As a result, RAID arrays composed from SSDs are subject to correlated failures. By balancing writes evenly across the array, RAID schemes can wear out devices at similar times. When a device in the array fails towards the end of its lifetime, the high BER of the remaining devices can result in data loss. We propose Diff-RAID, a parity-based redundancy solution that creates an age differential in an array of SSDs. Diff-RAID distributes parity blocks unevenly across the array, leveraging their higher update rate to age devices at different rates. To maintain this age differential when old devices are replaced by new ones, Diff-RAID reshuffles the parity distribution on each drive replacement. We evaluate Diff-RAID's reliability by using real BER data from 12 flash chips on a simulator and show that it is more reliable than RAID-5, in some cases by multiple orders of magnitude. We also evaluate Diff-RAID's performance using a software implementation on a 5-device array of 80 GB Intel X25-M SSDs and show that it offers a trade-off between throughput and reliability.",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W2056928612",
    "type": "article"
  },
  {
    "title": "A new intra-disk redundancy scheme for high-reliability RAID storage systems in the presence of unrecoverable errors",
    "doi": "https://doi.org/10.1145/1353452.1353453",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Ajay Dholakia; Evangelos Eleftheriou; Xiaoyu Hu; Ilias Iliadis; Jai Menon; K. Koteswara Rao",
    "corresponding_authors": "",
    "abstract": "Today's data storage systems are increasingly adopting low-cost disk drives that have higher capacity but lower reliability, leading to more frequent rebuilds and to a higher risk of unrecoverable media errors. We propose an efficient intradisk redundancy scheme to enhance the reliability of RAID systems. This scheme introduces an additional level of redundancy inside each disk, on top of the RAID redundancy across multiple disks. The RAID parity provides protection against disk failures, whereas the proposed scheme aims to protect against media-related unrecoverable errors. In particular, we consider an intradisk redundancy architecture that is based on an interleaved parity-check coding scheme, which incurs only negligible I/O performance degradation. A comparison between this coding scheme and schemes based on traditional Reed--Solomon codes and single-parity-check codes is conducted by analytical means. A new model is developed to capture the effect of correlated unrecoverable sector errors. The probability of an unrecoverable failure associated with these schemes is derived for the new correlated model, as well as for the simpler independent error model. We also derive closed-form expressions for the mean time to data loss of RAID-5 and RAID-6 systems in the presence of unrecoverable errors and disk failures. We then combine these results to characterize the reliability of RAID systems that incorporate the intradisk redundancy scheme. Our results show that in the practical case of correlated errors, the interleaved parity-check scheme provides the same reliability as the optimum, albeit more complex, Reed--Solomon coding scheme. Finally, the I/O and throughput performances are evaluated by means of analysis and event-driven simulation.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W2074707294",
    "type": "article"
  },
  {
    "title": "B-trees, shadowing, and clones",
    "doi": "https://doi.org/10.1145/1326542.1326544",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Ohad Rodeh",
    "corresponding_authors": "Ohad Rodeh",
    "abstract": "B-trees are used by many file systems to represent files and directories. They provide guaranteed logarithmic time key-search, insert, and remove. File systems like WAFL and ZFS use shadowing, or copy-on-write, to implement snapshots, crash recovery, write-batching, and RAID. Serious difficulties arise when trying to use b-trees and shadowing in a single system. This article is about a set of b-tree algorithms that respects shadowing, achieves good concurrency, and implements cloning (writeable snapshots). Our cloning algorithm is efficient and allows the creation of a large number of clones. We believe that using our b-trees would allow shadowing file systems to better scale their on-disk data structures.",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W2149649512",
    "type": "article"
  },
  {
    "title": "HPDA",
    "doi": "https://doi.org/10.1145/2093139.2093143",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Bo Mao; Hong Jiang; Suzhen Wu; Lei Tian; Dan Feng; Jianxi Chen; Lingfang Zeng",
    "corresponding_authors": "",
    "abstract": "Flash-based Solid State Drive (SSD) has been productively shipped and deployed in large scale storage systems. However, a single flash-based SSD cannot satisfy the capacity, performance and reliability requirements of the modern storage systems that support increasingly demanding data-intensive computing applications. Applying RAID schemes to SSDs to meet these requirements, while a logical and viable solution, faces many challenges. In this article, we propose a Hybrid Parity-based Disk Array architecture (short for HPDA), which combines a group of SSDs and two hard disk drives (HDDs) to improve the performance and reliability of SSD-based storage systems. In HPDA, the SSDs (data disks) and part of one HDD (parity disk) compose a RAID4 disk array. Meanwhile, a second HDD and the free space of the parity disk are mirrored to form a RAID1-style write buffer that temporarily absorbs the small write requests and acts as a surrogate set during recovery when a disk fails. The write data is reclaimed to the data disks during the lightly loaded or idle periods of the system. Reliability analysis shows that the reliability of HPDA, in terms of MTTDL (Mean Time To Data Loss), is better than that of either pure HDD-based or SSD-based disk array. Our prototype implementation of HPDA and the performance evaluations show that HPDA significantly outperforms either HDD-based or SSD-based disk array.",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W2120860473",
    "type": "article"
  },
  {
    "title": "WAN-optimized replication of backup datasets using stream-informed delta compression",
    "doi": "https://doi.org/10.1145/2385603.2385606",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Philip Shilane; Mark Huang; Grant Wallace; Windsor Hsu",
    "corresponding_authors": "",
    "abstract": "Replicating data off site is critical for disaster recovery reasons, but the current approach of transferring tapes is cumbersome and error prone. Replicating across a wide area network (WAN) is a promising alternative, but fast network connections are expensive or impractical in many remote locations, so improved compression is needed to make WAN replication truly practical. We present a new technique for replicating backup datasets across a WAN that not only eliminates duplicate regions of files (deduplication) but also compresses similar regions of files with delta compression, which is available as a feature of EMC Data Domain systems. Our main contribution is an architecture that adds stream-informed delta compression to already existing deduplication systems and eliminates the need for new, persistent indexes. Unlike techniques based on knowing a file's version or that use a memory cache, our approach achieves delta compression across all data replicated to a server at any time in the past. From a detailed analysis of datasets and statistics from hundreds of customers using our product, we achieve an additional 2X compression from delta compression beyond deduplication and local compression, which enables customers to replicate data that would otherwise fail to complete within their backup window.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2073370301",
    "type": "article"
  },
  {
    "title": "Analytic Models of SSD Write Performance",
    "doi": "https://doi.org/10.1145/2577384",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Peter Desnoyers",
    "corresponding_authors": "Peter Desnoyers",
    "abstract": "Solid-state drives (SSDs) update data by writing a new copy, rather than overwriting old data, causing prior copies of the same data to be invalidated . These writes are performed in units of pages , while space is reclaimed in units of multipage erase blocks , necessitating copying of any remaining valid pages in the block before reclamation. The efficiency of this cleaning process greatly affects performance under random workloads; in particular, in SSDs, the write bottleneck is typically internal media throughput, and write amplification due to additional internal copying directly reduces application throughput. We present the first nearly-exact closed-form solution for write amplification under greedy cleaning for uniformly-distributed random traffic, validate its accuracy via simulation, and show that its inaccuracies are negligible for reasonable block sizes and overprovisioning ratios. In addition, we also present the first models which predict performance degradation for both LRW (least-recently-written) cleaning and greedy cleaning under simple nonuniform traffic conditions; simulation results show the first model to be exact and the second to be accurate within 2%. We extend the LRW model to arbitrary combinations of random traffic and demonstrate its use in predicting cleaning performance for real-world workloads. Using these analytic models, we examine the strategy of separating “hot” and “cold” data, showing that for our traffic model, such separation eliminates any loss in performance due to nonuniform traffic. We then show how a system which segregates hot and cold data into different block pools may shift free space between these pools in order to achieve improved performance, and how numeric methods may be used with our model to find the optimum operating point, which approaches a write amplification of 1.0 for increasingly skewed traffic. We examine online methods for achieving this optimal operating point and show a control strategy based on our model which achieves high performance for a number of real-world block traces.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2029118431",
    "type": "article"
  },
  {
    "title": "A Study of Linux File System Evolution",
    "doi": "https://doi.org/10.1145/2560012",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "L. Lu; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau; Shan Lu",
    "corresponding_authors": "",
    "abstract": "We conduct a comprehensive study of file-system code evolution. By analyzing eight years of Linux file-system changes across 5079 patches, we derive numerous new (and sometimes surprising) insights into the file-system development process; our results should be useful for both the development of file systems themselves as well as the improvement of bug-finding tools.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2138555106",
    "type": "article"
  },
  {
    "title": "Internal Parallelism of Flash Memory-Based Solid-State Drives",
    "doi": "https://doi.org/10.1145/2818376",
    "publication_date": "2016-05-12",
    "publication_year": 2016,
    "authors": "Feng Chen; Binbing Hou; Rubao Lee",
    "corresponding_authors": "",
    "abstract": "A unique merit of a solid-state drive (SSD) is its internal parallelism . In this article, we present a set of comprehensive studies on understanding and exploiting internal parallelism of SSDs. Through extensive experiments and thorough analysis, we show that exploiting internal parallelism of SSDs can not only substantially improve input/output (I/O) performance but also may lead to some surprising side effects and dynamics. For example, we find that with parallel I/Os, SSD performance is no longer highly sensitive to access patterns (random or sequential), but rather to other factors, such as data access interferences and physical data layout. Many of our prior understandings about SSDs also need to be reconsidered. For example, we find that with parallel I/Os, write performance could outperform reads and is largely independent of access patterns, which is opposite to our long-existing common understanding about slow random writes on SSDs. We have also observed a strong interference between concurrent reads and writes as well as the impact of physical data layout to parallel I/O performance. Based on these findings, we present a set of case studies in database management systems, a typical data-intensive application. Our case studies show that exploiting internal parallelism is not only the key to enhancing application performance, and more importantly, it also fundamentally changes the equation for optimizing applications. This calls for a careful reconsideration of various aspects in application and system designs. Furthermore, we give a set of experimental studies on new-generation SSDs and the interaction between internal and external parallelism in an SSD-based Redundant Array of Independent Disks (RAID) storage. With these critical findings, we finally make a set of recommendations to system architects and application designers for effectively exploiting internal parallelism.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2352208637",
    "type": "article"
  },
  {
    "title": "Evaluating Phase Change Memory for Enterprise Storage Systems",
    "doi": "https://doi.org/10.1145/2668128",
    "publication_date": "2014-10-31",
    "publication_year": 2014,
    "authors": "Hyojun Kim; S. Seshadri; Clement L. Dickey; Lawrence Chiu",
    "corresponding_authors": "",
    "abstract": "Storage systems based on Phase Change Memory (PCM) devices are beginning to generate considerable attention in both industry and academic communities. But whether the technology in its current state will be a commercially and technically viable alternative to entrenched technologies such as flash-based SSDs remains undecided. To address this, it is important to consider PCM SSD devices not just from a device standpoint, but also from a holistic perspective. This article presents the results of our performance study of a recent all-PCM SSD prototype. The average latency for a 4KiB random read is 6.7μs, which is about 16× faster than a comparable eMLC flash SSD. The distribution of I/O response times is also much narrower than flash SSD for both reads and writes. Based on the performance measurements and real-world workload traces, we explore two typical storage use cases: tiering and caching. We report that the IOPS/$ of a tiered storage system can be improved by 12--66% and the aggregate elapsed time of a server-side caching solution can be improved by up to 35% by adding PCM. Our results show that (even at current price points) PCM storage devices show promising performance as a new component in enterprise storage systems.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W3161903410",
    "type": "article"
  },
  {
    "title": "Efficient and Available In-Memory KV-Store with Hybrid Erasure Coding and Replication",
    "doi": "https://doi.org/10.1145/3129900",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Haibo Chen; Heng Zhang; Mingkai Dong; Zhaoguo Wang; Yubin Xia; Haibing Guan; Binyu Zang",
    "corresponding_authors": "",
    "abstract": "In-memory key/value store (KV-store) is a key building block for many systems like databases and large websites. Two key requirements for such systems are efficiency and availability, which demand a KV-store to continuously handle millions of requests per second. A common approach to availability is using replication, such as primary-backup (PBR), which, however, requires M +1 times memory to tolerate M failures. This renders scarce memory unable to handle useful user jobs. This article makes the first case of building highly available in-memory KV-store by integrating erasure coding to achieve memory efficiency, while not notably degrading performance. A main challenge is that an in-memory KV-store has much scattered metadata. A single KV put may cause excessive coding operations and parity updates due to excessive small updates to metadata. Our approach, namely Cocytus, addresses this challenge by using a hybrid scheme that leverages PBR for small-sized and scattered data (e.g., metadata and key), while only applying erasure coding to relatively large data (e.g., value). To mitigate well-known issues like lengthy recovery of erasure coding, Cocytus uses an online recovery scheme by leveraging the replicated metadata information to continuously serve KV requests. To further demonstrate the usefulness of Cocytus, we have built a transaction layer by using Cocytus as a fast and reliable storage layer to store database records and transaction logs. We have integrated the design of Cocytus to Memcached and extend it to support in-memory transactions. Evaluation using YCSB with different KV configurations shows that Cocytus incurs low overhead for latency and throughput, can tolerate node failures with fast online recovery, while saving 33% to 46% memory compared to PBR when tolerating two failures. A further evaluation using the SmallBank OLTP benchmark shows that in-memory transactions can run atop Cocytus with high throughput, low latency, and low abort rate and recover fast from consecutive failures.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2755334139",
    "type": "article"
  },
  {
    "title": "Sector-Disk (SD) Erasure Codes for Mixed Failure Modes in RAID Systems",
    "doi": "https://doi.org/10.1145/2560013",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "James S. Plank; Mario Blaum",
    "corresponding_authors": "",
    "abstract": "Traditionally, when storage systems employ erasure codes, they are designed to tolerate the failures of entire disks. However, the most common types of failures are latent sector failures, which only affect individual disk sectors, and block failures which arise through wear on SSD’s. This article introduces SD codes, which are designed to tolerate combinations of disk and sector failures. As such, they consume far less storage resources than traditional erasure codes. We specify the codes with enough detail for the storage practitioner to employ them, discuss their practical properties, and detail an open-source implementation.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2076294573",
    "type": "article"
  },
  {
    "title": "Optimal Repair Layering for Erasure-Coded Data Centers",
    "doi": "https://doi.org/10.1145/3149349",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Yuchong Hu; Xiaolu Li; Mi Zhang; Patrick P. C. Lee; Xiaoyang Zhang; Pan Zhou; Dan Feng",
    "corresponding_authors": "",
    "abstract": "Repair performance in hierarchical data centers is often bottlenecked by cross-rack network transfer. Recent theoretical results show that the cross-rack repair traffic can be minimized through repair layering, whose idea is to partition a repair operation into inner-rack and cross-rack layers. However, how repair layering should be implemented and deployed in practice remains an open issue. In this article, we address this issue by proposing a practical repair layering framework called DoubleR . We design two families of practical double regenerating codes (DRC), which not only minimize the cross-rack repair traffic but also have several practical properties that improve state-of-the-art regenerating codes. We implement and deploy DoubleR atop the Hadoop Distributed File System (HDFS) and show that DoubleR maintains the theoretical guarantees of DRC and improves the repair performance of regenerating codes in both node recovery and degraded read operations.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2608849736",
    "type": "article"
  },
  {
    "title": "Skylight—A Window on Shingled Disk Operation",
    "doi": "https://doi.org/10.1145/2821511",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "Abutalib Aghayev; Mansour Shafaei; Peter Desnoyers",
    "corresponding_authors": "",
    "abstract": "We introduce Skylight, a novel methodology that combines software and hardware techniques to reverse engineer key properties of drive-managed Shingled Magnetic Recording (SMR) drives. The software part of Skylight measures the latency of controlled I/O operations to infer important properties of drive-managed SMR, including type, structure, and size of the persistent cache; type of cleaning algorithm; type of block mapping; and size of bands. The hardware part of Skylight tracks drive head movements during these tests, using a high-speed camera through an observation window drilled through the cover of the drive. These observations not only confirm inferences from measurements, but resolve ambiguities that arise from the use of latency measurements alone. We show the generality and efficacy of our techniques by running them on top of three emulated and two real SMR drives, discovering valuable performance-relevant details of the behavior of the real SMR drives.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W3160805206",
    "type": "article"
  },
  {
    "title": "Improving Flash-Based Disk Cache with Lazy Adaptive Replacement",
    "doi": "https://doi.org/10.1145/2737832",
    "publication_date": "2016-02-26",
    "publication_year": 2016,
    "authors": "Sai Huang; Qingsong Wei; Dan Feng; Jianxi Chen; Cheng Chen",
    "corresponding_authors": "",
    "abstract": "For years, the increasing popularity of flash memory has been changing storage systems. Flash-based solid-state drives (SSDs) are widely used as a new cache tier on top of hard disk drives (HDDs) to speed up data-intensive applications. However, the endurance problem of flash memory remains a concern and is getting worse with the adoption of MLC and TLC flash. In this article, we propose a novel cache management algorithm for flash-based disk cache named Lazy Adaptive Replacement Cache (LARC). LARC adopts the idea of selective caching to filter out seldom accessed blocks and prevent them from entering cache. This avoids cache pollution and preserves popular blocks in cache for a longer period of time, leading to a higher hit rate. Meanwhile, by avoiding unnecessary cache replacements, LARC reduces the volume of data written to the SSD and yields an SSD-friendly access pattern. In this way, LARC improves the performance and endurance of the SSD at the same time. LARC is self-tuning and incurs little overhead. It has been extensively evaluated by both trace-driven simulations and synthetic benchmarks on a prototype implementation. Our experiments show that LARC outperforms state-of-art algorithms for different kinds of workloads and extends SSD lifetime by up to 15.7 times.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2295387928",
    "type": "article"
  },
  {
    "title": "RAIDShield",
    "doi": "https://doi.org/10.1145/2820615",
    "publication_date": "2015-11-20",
    "publication_year": 2015,
    "authors": "Ao Ma; Rachel Traylor; Fred Douglis; Mark Chamness; Guanlin Lu; Darren Sawyer; Surendar Chandra; Windsor Hsu",
    "corresponding_authors": "",
    "abstract": "Modern storage systems orchestrate a group of disks to achieve their performance and reliability goals. Even though such systems are designed to withstand the failure of individual disks, failure of multiple disks poses a unique set of challenges. We empirically investigate disk failure data from a large number of production systems, specifically focusing on the impact of disk failures on RAID storage systems. Our data covers about one million SATA disks from six disk models for periods up to 5 years. We show how observed disk failures weaken the protection provided by RAID. The count of reallocated sectors correlates strongly with impending failures. With these findings we designed RAIDS hield , which consists of two components. First, we have built and evaluated an active defense mechanism that monitors the health of each disk and replaces those that are predicted to fail imminently. This proactive protection has been incorporated into our product and is observed to eliminate 88% of triple disk errors, which are 80% of all RAID failures. Second, we have designed and simulated a method of using the joint failure probability to quantify and predict how likely a RAID group is to face multiple simultaneous disk failures, which can identify disks that collectively represent a risk of failure even when no individual disk is flagged in isolation. We find in simulation that RAID-level analysis can effectively identify most vulnerable RAID-6 systems, improving the coverage to 98% of triple errors. We conclude with discussions of operational considerations in deploying RAIDS hield more broadly and new directions in the analysis of disk errors. One interesting approach is to combine multiple metrics, allowing the values of different indicators to be used for predictions. Using newer field data that reports an additional metric, medium errors , we find that the relative efficacy of reallocated sectors and medium errors varies across disk models, offering an additional way to predict failures.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W4238631751",
    "type": "article"
  },
  {
    "title": "A Large-scale Analysis of Hundreds of In-memory Key-value Cache Clusters at Twitter",
    "doi": "https://doi.org/10.1145/3468521",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Juncheng Yang; Yao Yue; K. V. Rashmi",
    "corresponding_authors": "",
    "abstract": "Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W3194881980",
    "type": "article"
  },
  {
    "title": "A Survey on Flash-Memory Storage Systems: A Host-Side Perspective",
    "doi": "https://doi.org/10.1145/3723167",
    "publication_date": "2025-03-17",
    "publication_year": 2025,
    "authors": "Jalil Boukhobza; Pierre Olivier; Wen Sheng Lim; Liang-Chi Chen; Yun-Shan Hsieh; Shin-Ting Wu; Chien-Chung Ho; Po‐Chun Huang; Yuan-Hao Chang",
    "corresponding_authors": "",
    "abstract": "NAND flash memory has become the dominant storage media choice in a vast majority of application scenarios. Compared to mechanical hard disks, flash offers better access performance, energy efficiency, and shock resistance. However, the unique hardware peculiarities of this technology require dedicated facilities to manage the flash space and data. The implementation of flash management facilities has alternatively been realized either at the device or host computer level. Managing flash on the device side eases integration/compatibility and increases performance in certain scenarios. However, the limited computing resources inherent to devices and the lack of higher-level file system/application information make these solutions suboptimal in many situations. Managing flash on the host allows leveraging its abundant resources, and host-side knowledge such as data access patterns can be exploited to optimize flash management, at the cost of increased host-side complexity. The pros and cons of each approach also led to the appearance of hybrid, cross-layer solutions, enabling the collaboration of different layers of the storage stack. Recently, the pressure on modern storage systems requires that an increasing amount of flash management responsibilities is offloaded to the host, and the development of application-specific cross layer solutions: in that context, it is crucial to review these developments. In this paper, we make a comprehensive survey of the host-side management technologies of flash memory, application-/system-level flash-friendly designs, and emergent applications based on flash memory.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4408530689",
    "type": "article"
  },
  {
    "title": "Multiresolution storage and search in sensor networks",
    "doi": "https://doi.org/10.1145/1084779.1084780",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Deepak Ganesan; Ben Greenstein; Deborah Estrin; John Heidemann; Ramesh Govindan",
    "corresponding_authors": "",
    "abstract": "Wireless sensor networks enable dense sensing of the environment, offering unprecedented opportunities for observing the physical world. This article addresses two key challenges in wireless sensor networks: in-network storage and distributed search. The need for these techniques arises from the inability to provide persistent, centralized storage and querying in many sensor networks. Centralized storage requires multihop transmission of sensor data to Internet gateways which can quickly drain battery-operated nodes.Constructing a storage and search system that satisfies the requirements of data-rich scientific applications is a daunting task for many reasons: (a) the data requirements may be large compared to available storage and communication capacity of resource-constrained nodes, (b) user requirements are diverse and range from identification and collection of interesting event signatures to obtaining a deeper understanding of long-term trends and anomalies in the sensor events, and (c) many applications are in new domains where a priori information may not be available to reduce these requirements.This article describes a lossy, gracefully degrading storage model . We believe that such a model is necessary and sufficient for many scientific applications since it supports both progressive data collection for interesting events as well as long-term in-network storage for in-network querying and processing. Our system demonstrates the use of in-network wavelet-based summarization and progressive aging of summaries in support of long-term querying in storage and communication-constrained networks. We evaluate the performance of our linux implementation and show that it achieves: (a) low communication overhead for multiresolution summarization, (b) highly efficient drill-down search over such summaries, and (c) efficient use of network storage capacity through load-balancing and progressive aging of summaries.",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2050544369",
    "type": "article"
  },
  {
    "title": "Efficient management for large-scale flash-memory storage systems with resource conservation",
    "doi": "https://doi.org/10.1145/1111609.1111610",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Li-Pin Chang; Tei‐Wei Kuo",
    "corresponding_authors": "",
    "abstract": "Many existing approaches on flash-memory management are based on RAM-resident tables in which one single granularity size is used for both address translation and space management. As high-capacity flash memory is becoming more affordable than ever, the dilemma of how to manage the RAM space or how to improve the access performance is emerging for many vendors. In this article, we propose a tree-based management scheme which adopts multiple granularities in flash-memory management. Our objective is to not only reduce the run-time RAM footprint but also manage the write workload, due to housekeeping. The proposed method was evaluated under realistic workloads, where significant advantages over existing approaches were observed, in terms of the RAM space, access performance, and flash-memory lifetime.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W1990661559",
    "type": "article"
  },
  {
    "title": "Triage",
    "doi": "https://doi.org/10.1145/1111609.1111612",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Magnus Karlsson; Christos Karamanolis; Xiaoyun Zhu",
    "corresponding_authors": "",
    "abstract": "Ensuring performance isolation and differentiation among workloads that share a storage infrastructure is a basic requirement in consolidated data centers. Existing management tools rely on resource provisioning to meet performance goals; they require detailed knowledge of the system characteristics and the workloads. Provisioning is inherently slow to react to system and workload dynamics and, in the general case, it is not practical to provision for the worst case.We propose a software-only solution that ensures predictable performance for storage access. It is applicable to a wide range of storage systems and makes no assumptions about workload characteristics. We use an online feedback loop with an adaptive controller that throttles storage access requests to ensure that the available system throughput is shared among workloads according to their performance goals and their relative importance. The controller considers the system as a “black box” and adapts automatically to system and workload changes. The controller is distributed to ensure high availability under overload conditions, and it can be used for both block and file access protocols. The evaluation of Triage , our experimental prototype, demonstrates workload isolation and differentiation in an overloaded cluster file-system where workloads and system components are changing.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2036745989",
    "type": "article"
  },
  {
    "title": "Storage performance virtualization via throughput and latency control",
    "doi": "https://doi.org/10.1145/1168910.1168913",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Jianyong Zhang; Anand Sivasubramaniam; Qian Wang; Alma Riska; Erik Riedel",
    "corresponding_authors": "",
    "abstract": "I/O consolidation is a growing trend in production environments due to increasing complexity in tuning and managing storage systems. A consequence of this trend is the need to serve multiple users and/or workloads simultaneously. It is imperative to ensure that these users are insulated from each other by virtualization in order to meet any service-level objective (SLO). Previous proposals for performance virtualization suffer from one or more of the following drawbacks: (1) They rely on a fairly detailed performance model of the underlying storage system; (2) couple rate and latency allocation in a single scheduler, making them less flexible; or (3) may not always exploit the full bandwidth offered by the storage system.This article presents a two-level scheduling framework that can be built on top of an existing storage utility. This framework uses a low-level feedback-driven request scheduler, called AVATAR, that is intended to meet the latency bounds determined by the SLO. The load imposed on AVATAR is regulated by a high-level rate controller, called SARC, to insulate the users from each other. In addition, SARC is work-conserving and tries to fairly distribute any spare bandwidth in the storage system to the different users. This framework naturally decouples rate and latency allocation. Using extensive I/O traces and a detailed storage simulator, we demonstrate that this two-level framework can simultaneously meet the latency and throughput requirements imposed by an SLO, without requiring extensive knowledge of the underlying storage system.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W1991376651",
    "type": "article"
  },
  {
    "title": "Improving storage system availability with D-GRAID",
    "doi": "https://doi.org/10.1145/1063786.1063787",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Muthian Sivathanu; Vijayan Prabhakaran; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We present the design, implementation, and evaluation of D-GRAID, a gracefully degrading and quickly recovering RAID storage array. D-GRAID ensures that most files within the file system remain available even when an unexpectedly high number of faults occur. D-GRAID achieves high availability through aggressive replication of semantically critical data, and fault-isolated placement of logically related data. D-GRAID also recovers from failures quickly, restoring only live file system data to a hot spare. Both graceful degradation and live-block recovery are implemented in a prototype SCSI-based storage system underneath unmodified file systems, demonstrating that powerful “file-system like” functionality can be implemented within a “semantically smart” disk system behind a narrow block-based interface.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2106865329",
    "type": "article"
  },
  {
    "title": "Generating realistic <i>impressions</i> for file-system benchmarking",
    "doi": "https://doi.org/10.1145/1629080.1629086",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Nitin Agrawal; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "The performance of file systems and related software depends on characteristics of the underlying file-system image (i.e., file-system metadata and file contents). Unfortunately, rather than benchmarking with realistic file-system images, most system designers and evaluators rely on ad hoc assumptions and (often inaccurate) rules of thumb. Furthermore, the lack of standardization and reproducibility makes file-system benchmarking ineffective. To remedy these problems, we develop Impressions, a framework to generate statistically accurate file-system images with realistic metadata and content. Impressions is flexible, supporting user-specified constraints on various file-system parameters using a number of statistical techniques to generate consistent images. In this article, we present the design, implementation, and evaluation of Impressions and demonstrate its utility using desktop search as a case study. We believe Impressions will prove to be useful to system developers and users alike.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2143730413",
    "type": "article"
  },
  {
    "title": "Intel® Turbo Memory",
    "doi": "https://doi.org/10.1145/1367829.1367830",
    "publication_date": "2008-05-11",
    "publication_year": 2008,
    "authors": "Jeanna Matthews; Sanjeev Trika; D. Hensgen; Rick Coulson; Knut Grimsrud",
    "corresponding_authors": "",
    "abstract": "Hard-disk drives are a significant bottleneck to system performance and are also responsible for a significant fraction of total system power consumption. Intel Turbo Memory addresses these problems by adding a new layer to the storage hierarchy: a platform-based and nonvolatile, disk cache. In this article, we describe the hardware and software elements of the Intel Turbo Memory architecture. We show how it supports the new ReadyBoost and ReadyDrive features in Microsoft Vista and describe its key caching algorithms. We present performance, power savings, and wear-leveling results achieved by Intel Turbo Memory.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W1995467417",
    "type": "article"
  },
  {
    "title": "POTSHARDS—a secure, recoverable, long-term archival storage system",
    "doi": "https://doi.org/10.1145/1534912.1534914",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Mark W. Storer; Kevin M. Greenan; Ethan L. Miller; Kaladhar Voruganti",
    "corresponding_authors": "",
    "abstract": "Users are storing ever-increasing amounts of information digitally, driven by many factors including government regulations and the public's desire to digitally record their personal histories. Unfortunately, many of the security mechanisms that modern systems rely upon, such as encryption, are poorly suited for storing data for indefinitely long periods of time; it is very difficult to manage keys and update cryptosystems to provide secrecy through encryption over periods of decades. Worse, an adversary who can compromise an archive need only wait for cryptanalysis techniques to catch up to the encryption algorithm used at the time of the compromise in order to obtain “secure” data. To address these concerns, we have developed POTSHARDS, an archival storage system that provides long-term security for data with very long lifetimes without using encryption. Secrecy is achieved by using unconditionally secure secret splitting and spreading the resulting shares across separately managed archives. Providing availability and data recovery in such a system can be difficult; thus, we use a new technique, approximate pointers, in conjunction with secure distributed RAID techniques to provide availability and reliability across independent archives. To validate our design, we developed a prototype POTSHARDS implementation. In addition to providing us with an experimental testbed, this prototype helped us to understand the design issues that must be addressed in order to maximize security.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W1985405465",
    "type": "article"
  },
  {
    "title": "Read-Performance Optimization for Deduplication-Based Storage Systems in the Cloud",
    "doi": "https://doi.org/10.1145/2512348",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Bo Mao; Hong Jiang; Suzhen Wu; Yinjin Fu; Lei Tian",
    "corresponding_authors": "",
    "abstract": "Data deduplication has been demonstrated to be an effective technique in reducing the total data transferred over the network and the storage space in cloud backup, archiving, and primary storage systems, such as VM (virtual machine) platforms. However, the performance of restore operations from a deduplicated backup can be significantly lower than that without deduplication. The main reason lies in the fact that a file or block is split into multiple small data chunks that are often located in different disks after deduplication, which can cause a subsequent read operation to invoke many disk IOs involving multiple disks and thus degrade the read performance significantly. While this problem has been by and large ignored in the literature thus far, we argue that the time is ripe for us to pay significant attention to it in light of the emerging cloud storage applications and the increasing popularity of the VM platform in the cloud. This is because, in a cloud storage or VM environment, a simple read request on the client side may translate into a restore operation if the data to be read or a VM suspended by the user was previously deduplicated when written to the cloud or the VM storage server, a likely scenario considering the network bandwidth and storage capacity concerns in such an environment. To address this problem, in this article, we propose SAR, an SSD (solid-state drive)-Assisted Read scheme, that effectively exploits the high random-read performance properties of SSDs and the unique data-sharing characteristic of deduplication-based storage systems by storing in SSDs the unique data chunks with high reference count, small size, and nonsequential characteristics. In this way, many read requests to HDDs are replaced by read requests to SSDs, thus significantly improving the read performance of the deduplication-based storage systems in the cloud. The extensive trace-driven and VM restore evaluations on the prototype implementation of SAR show that SAR outperforms the traditional deduplication-based and flash-based cache schemes significantly, in terms of the average response times.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W1979162731",
    "type": "article"
  },
  {
    "title": "ImmortalGraph",
    "doi": "https://doi.org/10.1145/2700302",
    "publication_date": "2015-07-24",
    "publication_year": 2015,
    "authors": "Youshan Miao; Wentao Han; Kaiwei Li; Ming Wu; Fan Yang; Lidong Zhou; Vijayan Prabhakaran; Enhong Chen; Wenguang Chen",
    "corresponding_authors": "",
    "abstract": "Temporal graphs that capture graph changes over time are attracting increasing interest from research communities, for functions such as understanding temporal characteristics of social interactions on a time-evolving social graph. ImmortalGraph is a storage and execution engine designed and optimized specifically for temporal graphs. Locality is at the center of ImmortalGraph’s design: temporal graphs are carefully laid out in both persistent storage and memory, taking into account data locality in both time and graph-structure dimensions. ImmortalGraph introduces the notion of locality-aware batch scheduling in computation, so that common “bulk” operations on temporal graphs are scheduled to maximize the benefit of in-memory data locality. The design of ImmortalGraph explores an interesting interplay among locality, parallelism, and incremental computation in supporting common mining tasks on temporal graphs. The result is a high-performance temporal-graph system that is up to 5 times more efficient than existing database solutions for graph queries. The locality optimizations in ImmortalGraph offer up to an order of magnitude speedup for temporal iterative graph mining compared to a straightforward application of existing graph engines on a series of snapshots.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W973024310",
    "type": "article"
  },
  {
    "title": "A Hybrid Approach to Failed Disk Recovery Using RAID-6 Codes",
    "doi": "https://doi.org/10.1145/2027066.2027071",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Liping Xiang; Yinlong Xu; John C. S. Lui; Qian Chang; Yubiao Pan; Runhui Li",
    "corresponding_authors": "",
    "abstract": "The current parallel storage systems use thousands of inexpensive disks to meet the storage requirement of applications. Data redundancy and/or coding are used to enhance data availability, for instance, Row-diagonal parity (RDP) and EVENODD codes, which are widely used in RAID-6 storage systems, provide data availability with up to two disk failures . To reduce the probability of data unavailability, whenever a single disk fails, disk recovery will be carried out. We find that the conventional recovery schemes of RDP and EVENODD codes for a single failed disk only use one parity disk. However, there are two parity disks in the system, and both can be used for single disk failure recovery. In this article, we propose a hybrid recovery approach that uses both parities for single disk failure recovery, and we design efficient recovery schemes for RDP code (RDOR-RDP) and EVENODD code (RDOR-EVENODD). Our recovery scheme has the following attractive properties: (1) “ read optimality ” in the sense that our scheme issues the smallest number of disk reads to recover a single failed disk and it reduces approximately 1/4 of disk reads compared with conventional schemes; (2) “ load balancing property ” in that all surviving disks will be subjected to the same (or almost the same) amount of additional workload in rebuilding the failed disk. We carry out performance evaluation to quantify the merits of RDOR-RDP and RDOR-EVENODD on some widely used disks with DiskSim. The offline experimental results show that RDOR-RDP and RDOR-EVENODD outperform the conventional recovery schemes of RDP and EVENODD codes in terms of total recovery time and recovery workload on individual surviving disk. However, the improvements are less than the theoretical value (approximately 25%), as RDOR-RDP and RDOR-EVENODD change the disk access pattern from purely sequential to a more random one compared with their conventional schemes.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2008200179",
    "type": "article"
  },
  {
    "title": "STAIR Codes",
    "doi": "https://doi.org/10.1145/2658991",
    "publication_date": "2014-10-31",
    "publication_year": 2014,
    "authors": "Mingqiang Li; Patrick P. C. Lee",
    "corresponding_authors": "",
    "abstract": "Practical storage systems often adopt erasure codes to tolerate device failures and sector failures, both of which are prevalent in the field. However, traditional erasure codes employ device-level redundancy to protect against sector failures, and hence incur significant space overhead. Recent sector-disk (SD) codes are available only for limited configurations. By making a relaxed but practical assumption, we construct a general family of erasure codes called STAIR codes , which efficiently and provably tolerate both device and sector failures without any restriction on the size of a storage array and the numbers of tolerable device failures and sector failures. We propose the upstairs encoding and downstairs encoding methods, which provide complementary performance advantages for different configurations. We conduct extensive experiments on STAIR codes in terms of space saving, encoding/decoding speed, and update cost. We demonstrate that STAIR codes not only improve space efficiency over traditional erasure codes, but also provide better computational efficiency than SD codes based on our special code construction. Finally, we present analytical models that characterize the reliability of STAIR codes, and show that the support of a wider range of configurations by STAIR codes is critical for tolerating sector failure bursts discovered in the field.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2072134075",
    "type": "article"
  },
  {
    "title": "Cosmos+ OpenSSD",
    "doi": "https://doi.org/10.1145/3385073",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Jaewook Kwak; Sangjin Lee; Kibin Park; Jinwoo Jeong; Yong Ho Song",
    "corresponding_authors": "",
    "abstract": "As semiconductor technology has advanced, many storage systems have begun to use non-volatile memories as storage media. The organization and architecture of storage controllers have become more complex to meet various design requirements in terms of performance, response time, quality of service (QoS), and so on. In addition, due to the evolution of memory technology and the emergence of new applications, storage controllers employ new firmware algorithms and hardware modules. When designing storage controllers, engineers often evaluate the performance impact of using new software and hardware components using software simulators. However, this technique often yields limited evaluation accuracy because of the difficulty of modeling complex operations of components and the interactions among them. In this article, we present a reconfigurable flash storage controller design that serves as a rapid prototype. This design can be synthesized into a field-programmable gate array device and used in a realistic performance evaluation environment. We show the usefulness of our design by demonstrating the performance impact of design parameters.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W3041587974",
    "type": "article"
  },
  {
    "title": "Write Skew and Zipf Distribution",
    "doi": "https://doi.org/10.1145/2908557",
    "publication_date": "2016-06-08",
    "publication_year": 2016,
    "authors": "Yue Yang; Jianwen Zhu",
    "corresponding_authors": "",
    "abstract": "Understanding workload characteristics is essential to storage systems design and performance optimization. With the emergence of flash memory as a new viable storage medium, the new design concern of flash endurance arises, necessitating a revisit of workload characteristics, in particular, of the write behavior. Inspired by Web caching studies where a Zipf-like access pattern is commonly found, we hypothesize that write count distribution at the block level may also follow Zipf’s Law. To validate this hypothesis, we study 48 block I/O traces collected from a wide variety of real and benchmark applications. Through extensive analysis, we demonstrate that the Zipf-like pattern indeed widely exists in write traffic provided its disguises are removed by statistical processing. This finding implies that write skew in a large class of applications could be analytically expressed and, thus, facilitates design tradeoff explorations adaptive to workload characteristics.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2418598077",
    "type": "article"
  },
  {
    "title": "ZNSwap: un-Block your Swap",
    "doi": "https://doi.org/10.1145/3582434",
    "publication_date": "2023-02-01",
    "publication_year": 2023,
    "authors": "Shai Bergman; Niklas Cassel; Matias Bjørling; Mark Silberstein",
    "corresponding_authors": "",
    "abstract": "We introduce ZNSwap , a novel swap subsystem optimized for the recent Zoned Namespace (ZNS) SSDs. ZNSwap leverages ZNS’s explicit control over data management on the drive and introduces a space-efficient host-side Garbage Collector (GC) for swap storage co-designed with the OS swap logic. ZNSwap enables cross-layer optimizations, such as direct access to the in-kernel swap usage statistics by the GC to enable fine-grain swap storage management, and correct accounting of the GC bandwidth usage in the OS resource isolation mechanisms to improve performance isolation in multi-tenant environments. We evaluate ZNSwap using standard Linux swap benchmarks and two production key-value stores. ZNSwap shows significant performance benefits over the Linux swap on traditional SSDs, such as stable throughput for different memory access patterns, and 10× lower 99th percentile latency and 5× higher throughput for memcached key-value store under realistic usage scenarios.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4319796504",
    "type": "article"
  },
  {
    "title": "Practical Design Considerations for Wide Locally Recoverable Codes (LRCs)",
    "doi": "https://doi.org/10.1145/3626198",
    "publication_date": "2023-10-05",
    "publication_year": 2023,
    "authors": "Saurabh Kadekodi; Shashwat Silas; David Clausen; Arif Merchant",
    "corresponding_authors": "",
    "abstract": "Most of the data in large-scale storage clusters is erasure coded. At exascale, optimizing erasure codes for low storage overhead, efficient reconstruction, and easy deployment is of critical importance. Locally recoverable codes (LRCs) have deservedly gained central importance in this field, because they can balance many of these requirements. In our work, we study wide LRCs; LRCs with large number of blocks per stripe and low storage overhead. These codes are a natural next step for practitioners to unlock higher storage savings, but they come with their own challenges. Of particular interest is their reliability , since wider stripes are prone to more simultaneous failures. We conduct a practically minded analysis of several popular and novel LRCs. We find that wide LRC reliability is a subtle phenomenon that is sensitive to several design choices, some of which are overlooked by theoreticians, and others by practitioners. Based on these insights, we construct novel LRCs called Uniform Cauchy LRCs , which show excellent performance in simulations and a 33% improvement in reliability on unavailability events observed by a wide LRC deployed in a Google storage cluster. We also show that these codes are easy to deploy in a manner that improves their robustness to common maintenance events. Along the way, we also give a remarkably simple and novel construction of distance-optimal LRCs (other constructions are also known), which may be of interest to theory-minded readers.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4387377969",
    "type": "article"
  },
  {
    "title": "Higher reliability redundant disk arrays",
    "doi": "https://doi.org/10.1145/1629075.1629076",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Alexander Thomasian; Mario Blaum",
    "corresponding_authors": "",
    "abstract": "Parity is a popular form of data protection in redundant arrays of inexpensive/independent disks (RAID) . RAID5 dedicates one out of N disks to parity to mask single disk failures, that is, the contents of a block on a failed disk can be reconstructed by exclusive-ORing the corresponding blocks on surviving disks. RAID5 can mask a single disk failure, and it is vulnerable to data loss if a second disk failure occurs. The RAID5 rebuild process systematically reconstructs the contents of a failed disk on a spare disk, returning the system to its original state, but the rebuild process may be unsuccessful due to unreadable sectors. This has led to two disk failure tolerant arrays (2DFTs) , such as RAID6 based on Reed-Solomon (RS) codes. EVENODD, RDP (Row-Diagonal-Parity), the X-code, and RM2 (Row-Matrix) are 2DFTs with parity coding. RM2 incurs a higher level of redundancy than two disks, while the X-code is limited to a prime number of disks. RDP is optimal with respect to the number of XOR operations at the encoding, but not for short write operations. For small symbol sizes EVENODD and RDP have the same disk access pattern as RAID6, while RM2 and the X-code incur a high recovery cost with two failed disks. We describe variations to RAID5 and RAID6 organizations, including clustered RAID, different methods to update parities, rebuild processing, disk scrubbing to eliminate sector errors, and the intra-disk redundancy (IDR) method to deal with sector errors. We summarize the results of recent studies of failures in hard disk drives. We describe Markov chain reliability models to estimate RAID mean time to data loss (MTTDL) taking into account sector errors and the effect of disk scrubbing. Numerical results show that RAID5 plus IDR attains the same MTTDL level as RAID6, while incurring a lower performance penalty. We conclude with a survey of analytic and simulation studies of RAID performance and tools and benchmarks for RAID performance evaluation.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2069043460",
    "type": "article"
  },
  {
    "title": "GRID codes",
    "doi": "https://doi.org/10.1145/1480439.1480444",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Mingqiang Li; Jiwu Shu; Weimin Zheng",
    "corresponding_authors": "",
    "abstract": "As storage systems grow in size and complexity, they are increasingly confronted with concurrent disk failures together with multiple unrecoverable sector errors. To ensure high data reliability and availability, erasure codes with high fault tolerance are required. In this article, we present a new family of erasure codes with high fault tolerance, named GRID codes. They are called such because they are a family of strip-based codes whose strips are arranged into multi-dimensional grids. In the construction of GRID codes, we first introduce a concept of matched codes and then discuss how to use matched codes to construct GRID codes. In addition, we propose an iterative reconstruction algorithm for GRID codes. We also discuss some important features of GRID codes. Finally, we compare GRID codes with several categories of existing codes. Our comparisons show that for large-scale storage systems, our GRID codes have attractive advantages over many existing erasure codes: (a) They are completely XOR-based and have very regular structures, ensuring easy implementation; (b) they can provide up to 15 and even higher fault tolerance; and (c) their storage efficiency can reach up to 80% and even higher. All the advantages make GRID codes more suitable for large-scale storage systems.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2030250129",
    "type": "article"
  },
  {
    "title": "JFTL",
    "doi": "https://doi.org/10.1145/1480439.1480443",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Hyun Jin Choi; Seung‐Ho Lim; Kyu Ho Park",
    "corresponding_authors": "",
    "abstract": "In flash memory-based storage, a Flash Translation Layer (FTL) manages the mapping between the logical addresses of a file system and the physical addresses of the flash memory. When a journaling file system is set up on the FTL, the consistency of the file system is guaranteed by duplications of the same file system changes in both the journal region of the file system and the home locations of the changes. However, these duplications inevitably degrade the performance of the file system. In this article we present an efficient FTL, called JFTL , based on a journal remapping technique. The FTL uses an address mapping method to write all the data to a new region in a process known as an out-of-place update. Because of this process, the existing data in flash memory is not overwritten by such an update. By using this characteristic of the FTL, the JFTL remaps addresses of the logged file system changes to addresses of the home locations of the changes, instead of writing the changes once more to flash memory. Thus, the JFTL efficiently eliminates redundant data in the flash memory as well as preserving the consistency of the journaling file system. Our experiments confirm that, when associated with a writeback or ordered mode of a conventional EXT3 file system, the JFTL enhances the performance of EXT3 by up to 20%. Furthermore, when the JFTL operates with a journaled mode of EXT3, there is almost a twofold performance gain in many cases. Moreover, the recovery performance of the JFTL is much better than that of the FTL.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2052655979",
    "type": "article"
  },
  {
    "title": "Datacenter Scale Evaluation of the Impact of Temperature on Hard Disk Drive Failures",
    "doi": "https://doi.org/10.1145/2491472.2491475",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Sriram Sankar; Mark Shaw; Kushagra Vaid; Sudhanva Gurumurthi",
    "corresponding_authors": "",
    "abstract": "With the advent of cloud computing and online services, large enterprises rely heavily on their datacenters to serve end users. A large datacenter facility incurs increased maintenance costs in addition to service unavailability when there are increased failures. Among different server components, hard disk drives are known to contribute significantly to server failures; however, there is very little understanding of the major determinants of disk failures in datacenters. In this work, we focus on the interrelationship between temperature, workload, and hard disk drive failures in a large scale datacenter. We present a dense storage case study from a population housing thousands of servers and tens of thousands of disk drives, hosting a large-scale online service at Microsoft. We specifically establish correlation between temperatures and failures observed at different location granularities: (a) inside drive locations in a server chassis, (b) across server locations in a rack, and (c) across multiple racks in a datacenter. We show that temperature exhibits a stronger correlation to failures than the correlation of disk utilization with drive failures. We establish that variations in temperature are not significant in datacenters and have little impact on failures. We also explore workload impacts on temperature and disk failures and show that the impact of workload is not significant. We then experimentally evaluate knobs that control disk drive temperature, including workload and chassis design knobs. We corroborate our findings from the real data study and show that workload knobs show minimal impact on temperature. Chassis knobs like disk placement and fan speeds have a larger impact on temperature. Finally, we also show the proposed cost benefit of temperature optimizations that increase hard disk drive reliability.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W1993578092",
    "type": "article"
  },
  {
    "title": "Analysis of Workload Behavior in Scientific and Historical Long-Term Data Repositories",
    "doi": "https://doi.org/10.1145/2180905.2180907",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Ian F. Adams; Mark W. Storer; Ethan L. Miller",
    "corresponding_authors": "",
    "abstract": "The scope of archival systems is expanding beyond cheap tertiary storage: scientific and medical data is increasingly digital, and the public has a growing desire to digitally record their personal histories. Driven by the increase in cost efficiency of hard drives, and the rise of the Internet, content archives have become a means of providing the public with fast, cheap access to long-term data. Unfortunately, designers of purpose-built archival systems are either forced to rely on workload behavior obtained from a narrow, anachronistic view of archives as simply cheap tertiary storage, or extrapolate from marginally related enterprise workload data and traditional library access patterns. To close this knowledge gap and provide relevant input for the design of effective long-term data storage systems, we studied the workload behavior of several systems within this expanded archival storage space. Our study examined several scientific and historical archives, covering a mixture of purposes, media types, and access models---that is, public versus private. Our findings show that, for more traditional private scientific archival storage, files have become larger, but update rates have remained largely unchanged. However, in the public content archives we observed, we saw behavior that diverges from the traditional “write-once, read-maybe” behavior of tertiary storage. Our study shows that the majority of such data is modified---sometimes unnecessarily---relatively frequently, and that indexing services such as Google and internal data management processes may routinely access large portions of an archive, accounting for most of the accesses. Based on these observations, we identify areas for improving the efficiency and performance of archival storage systems.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2042835539",
    "type": "article"
  },
  {
    "title": "Efficient Hybrid Inline and Out-of-Line Deduplication for Backup Storage",
    "doi": "https://doi.org/10.1145/2641572",
    "publication_date": "2014-12-29",
    "publication_year": 2014,
    "authors": "Yan-Kit Li; Min Xu; Chun-Ho Ng; Patrick P. C. Lee",
    "corresponding_authors": "",
    "abstract": "Backup storage systems often remove redundancy across backups via inline deduplication, which works by referring duplicate chunks of the latest backup to those of existing backups. However, inline deduplication degrades restore performance of the latest backup due to fragmentation, and complicates deletion of expired backups due to the sharing of data chunks. While out-of-line deduplication addresses the problems by forward-pointing existing duplicate chunks to those of the latest backup, it introduces additional I/Os of writing and removing duplicate chunks. We design and implement RevDedup , an efficient hybrid inline and out-of-line deduplication system for backup storage. It applies coarse-grained inline deduplication to remove duplicates of the latest backup, and then fine-grained out-of-line reverse deduplication to remove duplicates from older backups. Our reverse deduplication design limits the I/O overhead and prepares for efficient deletion of expired backups. Through extensive testbed experiments using synthetic and real-world datasets, we show that RevDedup can bring high performance to the backup, restore, and deletion operations, while maintaining high storage efficiency comparable to conventional inline deduplication.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2012359782",
    "type": "article"
  },
  {
    "title": "HEAPO",
    "doi": "https://doi.org/10.1145/2629619",
    "publication_date": "2014-12-29",
    "publication_year": 2014,
    "authors": "Tae-Ho Hwang; Jaemin Jung; Youjip Won",
    "corresponding_authors": "",
    "abstract": "In this work, we developed a Heap-Based Persistent Object Store (HEAPO) to manage persistent objects in byte-addressable Nonvolatile RAM (NVRAM). HEAPO defines its own persistent heap layout, the persistent object format, name space organization, object sharing and protection mechanism, and undo-only log-based crash recovery, all of which are effectively tailored for NVRAM. We put our effort into developing a lightweight and flexible layer to exploit the DRAM-like access latency of NVRAM. To address this objective, we developed (i) a native management layer for NVRAM to eliminate redundancy between in-core and on-disk copies of the metadata, (ii) an expandable object format, (iii) a burst trie-based global name space with local name space caching, (iv) static address binding, and (v) minimal logging for undo-only crash recovery. We implemented HEAPO at commodity OS (Linux 2.6.32) and measured the performance. By eliminating metadata redundancy, HEAPO improved the speed of creating, attaching, and expanding an object by 1.3×, 4.5×, and 3.8×, respectively, compared to memory-mapped file-based persistent object store. Burst trie-based name space organization of HEAPO yielded 7.6× better lookup performance compared to hashed B-tree-based name space of EXT4. We modified memcachedb to use HEAPO in maintaining its search structure. For hash table update, HEAPO-based memcachedb yielded 3.4× performance improvement against original memcachedb implementation which uses mmap() over ramdisk approach to maintain the key-value store in memory.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2023972400",
    "type": "article"
  },
  {
    "title": "BetrFS",
    "doi": "https://doi.org/10.1145/2798729",
    "publication_date": "2015-11-04",
    "publication_year": 2015,
    "authors": "William Jannen; Jun Yuan; Yang Zhan; Amogh Akshintala; John Esmet; Yizheng Jiao; Ankur Mittal; Prashant Pandey; Phaneendra Reddy; Leif Walsh; Michael A. Bender; Martı́n Farach-Colton; Rob Johnson; Bradley C. Kuszmaul; Donald E. Porter",
    "corresponding_authors": "",
    "abstract": "The B ε -tree File System , or B e trFS (pronounced “better eff ess”), is the first in-kernel file system to use a write-optimized data structure (WODS). WODS are promising building blocks for storage systems because they support both microwrites and large scans efficiently. Previous WODS-based file systems have shown promise but have been hampered in several ways, which B e trFS mitigates or eliminates altogether. For example, previous WODS-based file systems were implemented in user space using FUSE, which superimposes many reads on a write-intensive workload, reducing the effectiveness of the WODS. This article also contributes several techniques for exploiting write-optimization within existing kernel infrastructure. B e trFS dramatically improves performance of certain types of large scans, such as recursive directory traversals, as well as performance of arbitrary microdata operations, such as file creates, metadata updates, and small writes to files. B e trFS can make small, random updates within a large file 2 orders of magnitude faster than other local file systems. B e trFS is an ongoing prototype effort and requires additional data-structure tuning to match current general-purpose file systems on some operations, including deletes, directory renames, and large sequential writes. Nonetheless, many applications realize significant performance improvements on B e trFS. For instance, an in-place rsync of the Linux kernel source sees roughly 1.6--22 × speedup over commodity file systems.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2257052164",
    "type": "article"
  },
  {
    "title": "Characterizing 3D Floating Gate NAND Flash",
    "doi": "https://doi.org/10.1145/3162616",
    "publication_date": "2018-04-12",
    "publication_year": 2018,
    "authors": "Qin Xiong; Fei Wu; Zhonghai Lu; Yue Zhu; You Zhou; Yibing Chu; Changsheng Xie; Ping Huang",
    "corresponding_authors": "",
    "abstract": "As both NAND flash memory manufacturers and users are turning their attentions from planar architecture towards three-dimensional (3D) architecture, it becomes critical and urgent to understand the characteristics of 3D NAND flash memory. These characteristics, especially those different from planar NAND flash, can significantly affect design choices of flash management techniques. In this article, we present a characterization study on the state-of-the-art 3D floating gate (FG) NAND flash memory through comprehensive experiments on an FPGA-based 3D NAND flash evaluation platform. We make distinct observations on its performance and reliability, such as operation latencies and various error patterns, followed by careful analyses from physical and circuit-level perspectives. Although 3D FG NAND flash provides much higher storage densities than planar NAND flash, it faces new performance challenges of garbage collection overhead and program performance variations and more complicated reliability issues due to, e.g., distinct location dependence and value dependence of errors. We also summarize the differences between 3D FG NAND flash and planar NAND flash and discuss implications on the designs of NAND flash management techniques brought by the architecture innovation. We believe that our work will facilitate developing novel 3D FG NAND flash-oriented designs to achieve better performance and reliability.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2797923169",
    "type": "article"
  },
  {
    "title": "G <scp>raph</scp> O <scp>ne</scp>",
    "doi": "https://doi.org/10.1145/3364180",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Pradeep Kumar; H. Howie Huang",
    "corresponding_authors": "",
    "abstract": "There is a growing need to perform a diverse set of real-time analytics (batch and stream analytics) on evolving graphs to deliver the values of big data to users. The key requirement from such applications is to have a data store to support their diverse data access efficiently, while concurrently ingesting fine-grained updates at a high velocity. Unfortunately, current graph systems, either graph databases or analytics engines, are not designed to achieve high performance for both operations; rather, they excel in one area that keeps a private data store in a specialized way to favor their operations only. To address this challenge, we have designed and developed G raph O ne , a graph data store that abstracts the graph data store away from the specialized systems to solve the fundamental research problems associated with the data store design. It combines two complementary graph storage formats (edge list and adjacency list) and uses dual versioning to decouple graph computations from updates. Importantly, it presents a new data abstraction, GraphView , to enable data access at two different granularities of data ingestions (called data visibility ) for concurrent execution of diverse classes of real-time graph analytics with only a small data duplication. Experimental results show that G raph O ne is able to deliver 11.40× and 5.36× average speedup in ingestion rate against LLAMA and Stinger, the two state-of-the-art dynamic graph systems, respectively. Further, they achieve an average speedup of 8.75× and 4.14× against LLAMA and 12.80× and 3.18× against Stinger for BFS and PageRank analytics (batch version), respectively. G raph O ne also gains over 2,000× speedup against Kickstarter, a state-of-the-art stream analytics engine in ingesting the streaming edges and performing streaming BFS when treating first half as a base snapshot and rest as streaming edge in a synthetic graph. G raph O ne also achieves an ingestion rate of two to three orders of magnitude higher than graph databases. Finally, we demonstrate that it is possible to run concurrent stream analytics from the same data store.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3013987090",
    "type": "article"
  },
  {
    "title": "Cost-effective, Energy-efficient, and Scalable Storage Computing for Large-scale AI Applications",
    "doi": "https://doi.org/10.1145/3415580",
    "publication_date": "2020-10-12",
    "publication_year": 2020,
    "authors": "Jaeyoung Do; Victor C. Ferreira; Hossein Bobarshad; Mahdi Torabzadehkashi; Siavash Rezaei; Ali HeydariGorji; Diego Souza; Brunno F. Goldstein; Leandro Santiago; Min Soo Kim; Priscila M. V. Lima; Felipe M. G. França; V.C. Alves",
    "corresponding_authors": "",
    "abstract": "The growing volume of data produced continuously in the Cloud and at the Edge poses significant challenges for large-scale AI applications to extract and learn useful information from the data in a timely and efficient way. The goal of this article is to explore the use of computational storage to address such challenges by distributed near-data processing. We describe Newport, a high-performance and energy-efficient computational storage developed for realizing the full potential of in-storage processing. To the best of our knowledge, Newport is the first commodity SSD that can be configured to run a server-like operating system, greatly minimizing the effort for creating and maintaining applications running inside the storage. We analyze the benefits of using Newport by running complex AI applications such as image similarity search and object tracking on a large visual dataset. The results demonstrate that data-intensive AI workloads can be efficiently parallelized and offloaded, even to a small set of Newport drives with significant performance gains and energy savings. In addition, we introduce a comprehensive taxonomy of existing computational storage solutions together with a realistic cost analysis for high-volume production, giving a good big picture of the economic feasibility of the computational storage technology.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3095502094",
    "type": "article"
  },
  {
    "title": "An End-to-End High-Performance Deduplication Scheme for Docker Registries and Docker Container Storage Systems",
    "doi": "https://doi.org/10.1145/3643819",
    "publication_date": "2024-01-30",
    "publication_year": 2024,
    "authors": "Nannan Zhao; Muhui Lin; Hadeel Albahar; Arnab K. Paul; Zhijie Huan; Subil Abraham; Keren Chen; Vasily Tarasov; Dimitrios Skourtis; Ali Anwar; Ali R. Butt",
    "corresponding_authors": "",
    "abstract": "The wide adoption of Docker containers for supporting agile and elastic enterprise applications has led to a broad proliferation of container images. The associated storage performance and capacity requirements place a high pressure on the infrastructure of container registries that store and distribute images and container storage systems on the Docker client side that manage image layers and store ephemeral data generated at container runtime. The storage demand is worsened by the large amount of duplicate data in images. Moreover, container storage systems that use Copy-on-Write (CoW) file systems as storage drivers exacerbate the redundancy. Exploiting the high file redundancy in real-world images is a promising approach to drastically reduce the growing storage requirements of container registries and improve the space efficiency of container storage systems. However, existing deduplication techniques significantly degrade the performance of both registries and container storage systems because of data reconstruction overhead as well as the deduplication cost. We propose DupHunter, an end-to-end deduplication scheme that deduplicates layers for both Docker registries and container storage systems while maintaining a high image distribution speed and container I/O performance. DupHunter is divided into three tiers: registry tier, middle tier, and client tier. Specifically, we first build a high-performance deduplication engine at the registry tier that not only natively deduplicates layers for space savings but also reduces layer restore overhead. Then, we use deduplication offloading at the middle tier to eliminate the redundant files from the client tier and avoid bringing deduplication overhead to the clients. To further reduce the data duplicates caused by CoWs and improve the container I/O performance, we utilize a container-aware storage system at the client tier that reserves space for each container and arranges the placement of files and their modifications on the disk to preserve locality. Under real workloads, DupHunter reduces storage space by up to 6.9× and reduces the GET layer latency up to 2.8× compared to the state-of-the-art. Moreover, DupHunter can improve the container I/O performance by up to 93% for reads and 64% for writes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4391360785",
    "type": "article"
  },
  {
    "title": "Magnetic Tape Storage Technology",
    "doi": "https://doi.org/10.1145/3708997",
    "publication_date": "2024-12-20",
    "publication_year": 2024,
    "authors": "Mark A. Lantz; Simeon Furrer; Martin Petermann; H. Rothuizen; Stella Brach; Luzius Kronig; Ilias Iliadis; Beat Weiss; Ed Childers; David Pease",
    "corresponding_authors": "",
    "abstract": "Magnetic tape provides a cost-effective way to retain the exponentially increasing volumes of data being created in recent years. The low cost per terabyte combined with tape’s low energy consumption make it an appealing option for storing infrequently accessed data and has resulted in a resurgence in use of the technology. Magnetic tape as a digital data storage technology was first commercialized in the early 1950’s and has evolved continuously since then. Despite its long history, tape has significant potential for continued capacity and data rate scaling. This paper strives to provide an overview of linear magnetic tape technology, usage, history, and future outlook. After a short introduction, the paper delves into the details of how modern tape drives and media operate, including the basic mechanism and physics of magnetic recording, current tape media technology, state-of-the art-tape head technology, tape layout and encoding, data retrieval, timing-based servo and mechatronics of a tape drive, and the capabilities of current drives. This is followed by a discussion of tape libraries, an overview of tape library performance modeling research, operating system-level and application-level tape support, tape use cases, and the future scaling potential and outlook of tape. The paper concludes with a history of tape hardware, media, usage and software.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4405647364",
    "type": "article"
  },
  {
    "title": "The <i>Conquest</i> file system",
    "doi": "https://doi.org/10.1145/1168910.1168914",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "An-I Andy Wang; Geoff Kuenning; Peter Reiher; Gerald J. Popek",
    "corresponding_authors": "",
    "abstract": "Modern file systems assume the use of disk, a system-wide performance bottleneck for over a decade. Current disk caching and RAM file systems either impose high overhead to access memory content or fail to provide mechanisms to achieve data persistence across reboots.The Conquest file system is based on the observation that memory is becoming inexpensive, which enables all file system services to be delivered from memory, except for providing large storage capacity. Unlike caching, Conquest uses memory with battery backup as persistent storage, and provides specialized and separate data paths to memory and disk. Therefore, the memory data path contains no disk-related complexity. The disk data path consists of optimizations only for the specialized disk usage pattern.Compared to a memory-based file system, Conquest incurs little performance overhead. Compared to several disk-based file systems, Conquest achieves 1.3x to 19x faster memory performance, and 1.4x to 2.0x faster performance when exercising both memory and disk. Conquest realizes most of the benefits of persistent RAM at a fraction of the cost of a RAM-only solution. It also demonstrates that disk-related optimizations impose high overheads for accessing memory content in a memory-rich environment.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W1967212268",
    "type": "article"
  },
  {
    "title": "Versatility and Unix semantics in namespace unification",
    "doi": "https://doi.org/10.1145/1138041.1138045",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Charles P. Wright; Jay Dave; Puja Gupta; Harikesavan Krishnan; David Quigley; Erez Zadok; Mohammad Nayyer Zubair",
    "corresponding_authors": "",
    "abstract": "Administrators often prefer to keep related sets of files in different locations or media, as it is easier to maintain them separately. Users, however, prefer to see all files in one location for convenience. One solution that accommodates both needs is virtual namespace unification---providing a merged view of several directories without physically merging them. For example, namespace unification can merge the contents of several CD-ROM images without unpacking them, merge binary directories from different packages, merge views from several file servers, and more. Namespace unification can also enable snapshotting by marking some data sources read-only and then utilizing copy-on-write for the read-only sources. For example, an OS image may be contained on a read-only CD-ROM image---and the user's configuration, data, and programs could be stored in a separate read-write directory. With copy-on-write unification, the user need not be concerned about the two disparate file systems.It is difficult to maintain Unix semantics while offering a versatile namespace unification system. Past efforts to provide such unification often compromised on the set of features provided or Unix compatibility---resulting in an incomplete solution that users could not use.We designed and implemented a versatile namespace unification system called Unionfs . Unionfs maintains Unix semantics while offering advanced namespace unification features: dynamic insertion and removal of namespaces at any point in the merged view, mixing read-only and read-write components, efficient in-kernel duplicate elimination, NFS interoperability, and more. Since releasing our Linux implementation, it has been used by thousands of users and over a dozen Linux distributions, which helped us discover and solve many practical problems.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2117923448",
    "type": "article"
  },
  {
    "title": "Extending ACID semantics to the file system",
    "doi": "https://doi.org/10.1145/1242520.1242521",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Charles P. Wright; Richard P. Spillane; Gopalan Sivathanu; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "An organization's data is often its most valuable asset, but today's file systems provide few facilities to ensure its safety. Databases, on the other hand, have long provided transactions. Transactions are useful because they provide atomicity, consistency, isolation, and durability (ACID). Many applications could make use of these semantics, but databases have a wide variety of nonstandard interfaces. For example, applications like mail servers currently perform elaborate error handling to ensure atomicity and consistency, because it is easier than using a DBMS. A transaction-oriented programming model eliminates complex error-handling code because failed operations can simply be aborted without side effects. We have designed a file system that exports ACID transactions to user-level applications, while preserving the ubiquitous and convenient POSIX interface. In our prototype ACID file system, called Amino, updated applications can protect arbitrary sequences of system calls within a transaction. Unmodified applications operate without any changes, but each system call is transaction protected. We also built a recoverable memory library with support for nested transactions to allow applications to keep their in-memory data structures consistent with the file system. Our performance evaluation shows that ACID semantics can be added to applications with acceptable overheads. When Amino adds atomicity, consistency, and isolation functionality to an application, it performs close to Ext3. Amino achieves durability up to 46% faster than Ext3, thanks to improved locality.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2141423292",
    "type": "article"
  },
  {
    "title": "A new approach to dynamic self-tuning of database buffers",
    "doi": "https://doi.org/10.1145/1353452.1353455",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Dinh Nguyen Tran; Phung Chinh Huynh; Y. C. Tay; Anthony K. H. Tung",
    "corresponding_authors": "",
    "abstract": "Current businesses rely heavily on efficient access to their databases. Manual tuning of these database systems by performance experts is increasingly infeasible: For small companies, hiring an expert may be too expensive; for large enterprises, even an expert may not fully understand the interaction between a large system and its multiple changing workloads. This trend has led major vendors to offer tools that automatically and dynamically tune a database system. Many database tuning knobs concern the buffer pool for caching data and disk pages. Specifically, these knobs control the buffer allocation and thus the cache miss probability, which has direct impact on performance. Previous methods for automatic buffer tuning are based on simulation, black-box control, gradient descent, and empirical equations. This article presents a new approach, using calculations with an analytically-derived equation that relates miss probability to buffer allocation; this equation fits four buffer replacement policies, as well as twelve datasets from mainframes running commercial databases in large corporations. The equation identifies a buffer-size limit that is useful for buffer tuning and powering down idle buffers. It can also replace simulation in predicting I/O costs. Experiments with PostgreSQL illustrate how the equation can help optimize online buffer partitioning, ensure fairness in buffer reclamation, and dynamically retune the allocation when workloads change. It is also used, in conjunction with DB2's interface for retrieving miss data, for tuning DB2 buffer allocation to achieve targets for differentiated service.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2005228395",
    "type": "article"
  },
  {
    "title": "FRASH",
    "doi": "https://doi.org/10.1145/1714454.1714457",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Jaemin Jung; Youjip Won; Eunki Kim; Hyungjong Shin; Byeong-Gil Jeon",
    "corresponding_authors": "",
    "abstract": "In this work, we develop a novel hybrid file system, FRASH, for storage-class memory and NAND Flash. Despite the promising physical characteristics of storage-class memory, its scale is an order of magnitude smaller than the current storage device scale. This fact makes it less than desirable for use as an independent storage device. We carefully analyze in-memory and on-disk file system objects in a log-structured file system, and exploit memory and storage aspects of the storage-class memory to overcome the drawbacks of the current log-structured file system. FRASH provides a hybrid view storage-class memory. It harbors an in-memory data structure as well as a on-disk structure. It provides nonvolatility to key data structures which have been maintained in-memory in a legacy log-structured file system. This approach greatly improves the mount latency and effectively resolves the robustness issue. By maintaining on-disk structure in storage-class memory, FRASH provides byte-addressability to the file system object and metadata for page, and subsequently greatly improves the I/O performance compared to the legacy log-structured approach. While storage-class memory offers byte granularity, it is still far slower than its DRAM counter part. We develop a copy-on-mount technique to overcome the access latency difference between main memory and storage-class memory. Our file system was able to reduce the mount time by 92% and file system I/O performance was increased by 16%.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2024219610",
    "type": "article"
  },
  {
    "title": "An adaptive write buffer management scheme for flash-based SSDs",
    "doi": "https://doi.org/10.1145/2093139.2093140",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Guanying Wu; Xubin He; B. Eckart",
    "corresponding_authors": "",
    "abstract": "Solid State Drives (SSD's) have shown promise to be a candidate to replace traditional hard disk drives. The benefits of SSD's over HDD's include better durability, higher performance, and lower power consumption, but due to certain physical characteristics of NAND flash, which comprise SSD's, there are some challenging areas of improvement and further research. We focus on the layout and management of the small amount of RAM that serves as a cache between the SSD and the system that uses it. Of the techniques that have previously been proposed to manage this cache, we identify several sources of inefficient cache space management due to the way pages are clustered in blocks and the limited replacement policy. We find that in many traces hot pages reside in otherwise cold blocks, and that the spatial locality of most clusters can be fully exploited in a limited time period, so we develop a hybrid page/block architecture along with an advanced replacement policy, called BPAC, or Block-Page Adaptive Cache, to exploit both temporal and spatial locality. Our technique involves adaptively partitioning the SSD on-disk cache to separately hold pages with high temporal locality in a page list and clusters of pages with low temporal but high spatial locality in a block list. In addition, we have developed a novel mechanism for flash-based SSD's to characterize the spatial locality of the disk I/O workload and an approach to dynamically identify the set of low spatial locality clusters. We run trace-driven simulations to verify our design and find that it outperforms other popular flash-aware cache schemes under different workloads. For instance, compared to a popular flash aware cache algorithm BPLRU, BPAC reduces the number of cache evictions by up to 79.6% and 34% on average.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2014390700",
    "type": "article"
  },
  {
    "title": "Evaluation of a Hybrid Approach for Efficient Provenance Storage",
    "doi": "https://doi.org/10.1145/2501986",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Yulai Xie; Kiran‐Kumar Muniswamy‐Reddy; Dan Feng; Yan Li; Darrell D. E. Long",
    "corresponding_authors": "",
    "abstract": "Provenance is the metadata that describes the history of objects. Provenance provides new functionality in a variety of areas, including experimental documentation, debugging, search, and security. As a result, a number of groups have built systems to capture provenance. Most of these systems focus on provenance collection, a few systems focus on building applications that use the provenance, but all of these systems ignore an important aspect: efficient long-term storage of provenance. In this article, we first analyze the provenance collected from multiple workloads and characterize the properties of provenance with respect to long-term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of Web graph compression (adapted for provenance) and dictionary encoding, provides the best trade-off in terms of compression ratio, compression time, and query performance when compared to other compression schemes.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2081276694",
    "type": "article"
  },
  {
    "title": "Beyond MTTDL",
    "doi": "https://doi.org/10.1145/2577386",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "J.G. Elerath; Jiří Schindler",
    "corresponding_authors": "",
    "abstract": "We introduce a new closed-form equation for estimating the number of data-loss events for a redundant array of inexpensive disks in a RAID-6 configuration. The equation expresses operational failures, their restorations, latent (sector) defects, and disk media scrubbing by time-based distributions that can represent non-homogeneous Poisson processes. It uses two-parameter Weibull distributions that allows the distributions to take on many different shapes, modeling increasing, decreasing, or constant occurrence rates. This article focuses on the statistical basis of the equation. It also presents time-based distributions of the four processes based on an extensive analysis of field data collected over several years from 10,000s of commercially available systems with 100,000s of disk drives. Our results for RAID-6 groups of size 16 indicate that the closed-form expression yields much more accurate results compared to the MTTDL reliability equation and matching computationally-intensive Monte Carlo simulations.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2055670512",
    "type": "article"
  },
  {
    "title": "Building Efficient Key-Value Stores via a Lightweight Compaction Tree",
    "doi": "https://doi.org/10.1145/3139922",
    "publication_date": "2017-11-24",
    "publication_year": 2017,
    "authors": "Ting Yao; Jiguang Wan; Ping Huang; Xubin He; Fei Wu; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "Log-Structure Merge tree (LSM-tree) has been one of the mainstream indexes in key-value systems supporting a variety of write-intensive Internet applications in today’s data centers. However, the performance of LSM-tree is seriously hampered by constantly occurring compaction procedures, which incur significant write amplification and degrade the write throughput. To alleviate the performance degradation caused by compactions, we introduce a lightweight compaction tree (LWC-tree), a variant of LSM-tree index optimized for minimizing the write amplification and maximizing the system throughput. The lightweight compaction drastically decreases write amplification by appending data in a table and only merging the metadata that have much smaller size. Using our proposed LWC-tree, we have implemented three key-value LWC-stores on different storage mediums including Shingled Magnetic Recording (SMR) drives, Solid State Drives (SSD), and conventional Hard Disk Drives (HDDs). The LWC-store is particularly optimized for SMR drives, as it eliminates the multiplicative I/O amplification from both LSM-trees and SMR drives. Due to the lightweight compaction procedure, LWC-store reduces the write amplification by a factor of up to 5× compared to the popular LevelDB key-value store. Moreover, the random write throughput of the LWC-tree on SMR drives is significantly improved by up to 467% even compared with LevelDB on conventional HDDs. Furthermore, LWC-tree has wide applicability and delivers impressive performance improvement in various conditions, including different storage mediums (i.e., SMR, HDD, SSD) and various value sizes and access patterns (i.e., uniform and Zipfian).",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2770617898",
    "type": "article"
  },
  {
    "title": "Fast Erasure Coding for Data Storage",
    "doi": "https://doi.org/10.1145/3375554",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Tianli Zhou; Chao Tian",
    "corresponding_authors": "",
    "abstract": "Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design and computation schedule, common XOR (exclusive-OR) operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually, and, in this work, we seek to use them jointly. To accomplish this task, these techniques need to be thoroughly evaluated individually and their relation better understood. Building on extensive testing, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted computation cost function, and also a straightforward coding procedure—follow a computation schedule produced from the optimized bitmatrix to apply XOR-level vectorization. This procedure provides better performances than most existing techniques (e.g., those used in ISA-L and Jerasure libraries), and sometimes can even compete against well-known but less general codes such as EVENODD, RDP, and STAR codes. One particularly important observation is that vectorizing the XOR operations is a better choice than directly vectorizing finite field operations, not only because of the flexibility in choosing finite field size and the better encoding throughput, but also its minimal migration efforts onto newer CPUs.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3032917738",
    "type": "article"
  },
  {
    "title": "Characterization Summary of Performance, Reliability, and Threshold Voltage Distribution of 3D Charge-Trap NAND Flash Memory",
    "doi": "https://doi.org/10.1145/3491230",
    "publication_date": "2022-03-10",
    "publication_year": 2022,
    "authors": "Weihua Liu; Fei Wu; Xiang Chen; Meng Zhang; Yu Wang; Xiangfeng Lu; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "Solid-state drive (SSD) gradually dominates in the high-performance storage scenarios. Three-dimension (3D) NAND flash memory owning high-storage capacity is becoming a mainstream storage component of SSD. However, the interferences of the new 3D charge-trap (CT) NAND flash are getting unprecedentedly complicated, yielding to many problems regarding reliability and performance. Alleviating these problems needs to understand the characteristics of 3D CT NAND flash memory deeply. To facilitate such understanding, in this article, we delve into characterizing the performance, reliability, and threshold voltage ( V th ) distribution of 3D CT NAND flash memory. We make a summary of these characteristics with multiple interferences and variations and give several new insights and a characterization methodology. Especially, we characterize the skewed ( V th ) distribution, ( V th ) shift laws, and the exclusive layer variation in 3D NAND flash memory. The characterization is the backbone of designing more reliable and efficient flash-based storage solutions.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4220661181",
    "type": "article"
  },
  {
    "title": "TM-Training: An Energy-Efficient Tiered Memory System for Deep Learning Training in NPUs",
    "doi": "https://doi.org/10.1145/3721484",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Jaeyong Park; Sangun Choi; Jongmin Kim; Gunjae Koo; Myung Kuk Yoon; Yunho Oh",
    "corresponding_authors": "",
    "abstract": "DRAM accounts for a large fraction of the total cost of ownership of memory systems in deep learning acceleration systems. To achieve sustainable scalability, tiered memory systems with denser technologies become critical. Prior work has proposed various tiered memory systems, but no matter how a system is designed, data movements between tiers consume substantial energy. In particular, as model sizes and memory capacity demands grow, the data movements between memory tiers become more frequent, posing a challenge that tiered memory systems may reduce deployment costs but suffer from low energy efficiency. If a memory system proactively places data into a tier and timely fetches it from the tier, the excessive data movement between tiers can be mitigated. We find that a system can statically anticipate such behaviors for all pages and localities. With this insight, we propose a new DNN acceleration system called TM-Training using flash memory. TM-Training capitalizes on the repetitive nature of the same computational patterns during execution, enabling the static establishment of optimal data placement for subsequent operations. Moreover, TM-Training employs a new data-splitting scheme to enable precise memory management. Our evaluation demonstrates that TM-Training reduces inter-tier data traffic by 64% and achieves a 55% higher throughput per watt in training than prior work.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408218547",
    "type": "article"
  },
  {
    "title": "FairyWREN: A Sustainable Cache for Emerging Write-Read-Erase Flash Interfaces",
    "doi": "https://doi.org/10.1145/3718390",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Sara McAllister; Y. Wang; Benjamin Wagner vom Berg; Daniel S. Berger; Nathan Beckmann; George Amvrosiadis; Gregory R. Ganger",
    "corresponding_authors": "",
    "abstract": "Datacenters need to reduce embodied carbon emissions, particularly for flash, which accounts for 40% of embodied carbon in servers. However, decreasing flash’s embodied emissions is challenging due to flash’s limited write endurance, which more than halves with each generation of denser flash. Reducing embodied emissions requires extending flash lifetime, stressing its limited write endurance even further. The legacy Logical Block-Addressable Device (LBAD) interface exacerbates the problem by forcing devices to perform garbage collection, leading to even more writes. Flash-based caches in particular write frequently, limiting the lifetimes and densities of the devices they use. These flash caches illustrate the need to break away from LBAD and switch to the new Write-Read-Erase iNterfaces (WREN) now coming to market. WREN affords applications control over data placement and garbage collection. We present Fairy Wren , a flash cache designed for WREN. Fairy Wren reduces writes by co-designing caching policies and flash garbage collection. Fairy Wren provides a 12.5 × write reduction over state-of-the-art LBAD caches. This decrease in writes allows flash devices to last longer, decreasing flash cost by 35% and flash carbon emissions by 33%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408218722",
    "type": "article"
  },
  {
    "title": "Maintaining Inter-Layer Equilibrium in Hierarchical-Storage-based KV Store",
    "doi": "https://doi.org/10.1145/3722224",
    "publication_date": "2025-03-08",
    "publication_year": 2025,
    "authors": "Kyoungho Koo; Junhan Lee",
    "corresponding_authors": "",
    "abstract": "Modern storage technologies aim to enhance performance and lower costs. With advances in storage devices, numerous studies propose key-value store designs for heterogeneous storage systems. Many rely on the Log-Structured Merge-Tree(LSM), which optimizes write-heavy workloads and flash storage. However, LSM-tree-based key-value stores on heterogeneous storage systems suffer from severe performance degradation and a surge in query latency. A key issue is write stall constraints , which arise from considering only single-tier storage, limiting key-value stores from leveraging multi-tier architectures. Another problem with key-value stores for heterogeneous storage systems is the inter-storage imbalance . Performance discrepancies between storage tiers fluctuate due to each storage device’s differing storage media technologies and garbage collection mechanisms. Consequently, existing hierarchical storage-based key-value stores (HSKVS) do not fully utilize the resources of storage devices across tiers and do not account for the performance imbalance between storage tiers. This paper presents an I/O scheduler designed to maintain data balance across storage tiers of multi-tiered storage engines. Our approach leverages two key techniques: dynamic data layout and flush I/O throttling . Dynamic data layout optimizes data placement by adapting to performance metrics and workload demands. This approach ensures data is stored in the most appropriate tier, improving access times and reducing latency. Meanwhile, flush I/O throttling aims to efficiently manage the I/O workload by controlling data movement between memory and storage. It balances data flow, preventing bottlenecks, especially during peak usage. To evaluate the effectiveness of our proposed I/O scheduler, we integrated it into a conventional HSKVS system. This integration achieved up to a 14.7 × performance improvement on YCSB workload D compared to conventional HSKVS systems. Furthermore, our proposed scheduler reduced the tail latency for get queries by about 9.3 × , from 13 ms to 1.4 ms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408256708",
    "type": "article"
  },
  {
    "title": "Exploiting Multiple Similarity Spaces for Deduplication of Encrypted Container Images",
    "doi": "https://doi.org/10.1145/3725220",
    "publication_date": "2025-03-19",
    "publication_year": 2025,
    "authors": "Tong Sun; Bowen Jiang; Borui Li; Jiamei Lv; Yi Gao; Wei Dong",
    "corresponding_authors": "",
    "abstract": "The growing popularity of encrypted container images in registries poses unique challenges for storage management due to the necessity for deduplication amidst rising image volumes. Traditional deduplication struggles with encrypted content, which inherently disguises duplicate data as distinct due to its randomized nature. Current advanced methods tackle this issue by decompressing images and applying message-locked encryption (MLE). However, these techniques face considerable challenges. Minor content changes can impair deduplication effectiveness, and decompressing layers increases storage requirements. Furthermore, this process negatively impacts both the speed at which users access the images and the overall system throughput. We propose SimEnc, a high-performance and secure deduplication system for encrypted container images by exploiting multiple similarity spaces. SimEnc pioneers the integration of semantic hashing with MLE to effectively parse semantic relationships across layers, thereby increasing deduplication efficacy. This system incorporates a rapid selection mechanism for similarity spaces, offering enhanced flexibility over previous models that relied on full decompression. By adopting Huffman decoding to navigate new similarity spaces, SimEnc not only improves deduplication ratios but also enhances overall performance. Our experimental results demonstrate that SimEnc substantially reduces storage needs by up to 261.7% compared to encrypted serverless platforms and by 54.2% against plaintext registries, while also delivering superior pull latency metrics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408622955",
    "type": "article"
  },
  {
    "title": "A SmartSSD-based Near Data Processing Architecture for Scalable Billion-point Approximate Nearest Neighbor Search",
    "doi": "https://doi.org/10.1145/3736589",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Haikun Liu; Bin Tian; Zhuohui Duan; Xiaofei Liao; Yu Zhang",
    "corresponding_authors": "",
    "abstract": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces has become increasingly crucial in database and machine learning applications. Most previous ANNS algorithms require TB-scale memory to store indices of billion-scale datasets, making their deployment extremely expensive for high-performance ANNS services. The emerging SmartSSD technology offers an opportunity to achieve scalable ANNS via near data processing (NDP). However, there remain several challenges to directly adopt existing ANNS algorithms on multiple SmartSSDs. In this paper, we present SmartANNS, a SmartSSD-empowered billion-scale ANNS solution based on a hierarchical indexing methodology. We propose several novel designs to achieve near-linear scaling with multiple SmartSSDs. First, we propose a “host CPUs + SmartSSDs” cooperative architecture incorporated with hierarchical indices to significantly reduce data accesses and computations on SmartSSDs. Second, we propose dynamic task scheduling based on optimized data layout to achieve both load balancing and data reusing for multiple SmartSSDs. Third, we further propose a learning-based shard pruning algorithm to eliminate unnecessary computations on SmartSSDs. We implement SmartANNS using Samsung’s commercial SmartSSDs. Experimental results show that SmartANNS can improve query per second (QPS) by up to 10.7 × compared with the state-of-the-art SmartSSD-based ANNS solution–CSDANNS. Moreover, SmartANNS can achieve near-linear performance scalability for large-scale datasets using multiple SmartSSDs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410519155",
    "type": "article"
  },
  {
    "title": "Performance directed energy management for main memory and disks",
    "doi": "https://doi.org/10.1145/1084779.1084782",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Xiaodong Li; Zhenmin Li; Yuanyuan Zhou; Sarita V. Adve",
    "corresponding_authors": "",
    "abstract": "Much research has been conducted on energy management for memory and disks. Most studies use control algorithms that dynamically transition devices to low power modes after they are idle for a certain threshold period of time. The control algorithms used in the past have two major limitations. First, they require painstaking, application-dependent manual tuning of their thresholds to achieve energy savings without significantly degrading performance. Second, they do not provide performance guarantees.This article addresses these two limitations for both memory and disks, making memory/disk energy-saving schemes practical enough to use in real systems. Specifically, we make four main contributions. (1) We propose a technique that provides a performance guarantee for control algorithms. We show that our method works well for all tested cases, even with previously proposed algorithms that are not performance-aware. (2) We propose a new control algorithm, Performance-Directed Dynamic (PD), that dynamically adjusts its thresholds periodically, based on available slack and recent workload characteristics. For memory, PD consumes the least energy when compared to previous hand-tuned algorithms combined with a performance guarantee. However, for disks, PD is too complex and its self-tuning is unable to beat previous hand-tuned algorithms. (3) To improve on PD, we propose a simpler, optimization-based, threshold-free control algorithm, Performance-Directed Static (PS). PS periodically assigns a static configuration by solving an optimization problem that incorporates information about the available slack and recent traffic variability to different chips/disks. We find that PS is the best or close to the best across all performance-guaranteed disk algorithms, including hand-tuned versions. (4) We also explore a hybrid scheme that combines PS and PD algorithms to further improve energy savings.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W1972419704",
    "type": "article"
  },
  {
    "title": "Mining block correlations to improve storage performance",
    "doi": "https://doi.org/10.1145/1063786.1063790",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Zhenmin Li; Zhifeng Chen; Yuanyuan Zhou",
    "corresponding_authors": "",
    "abstract": "Block correlations are common semantic patterns in storage systems. They can be exploited for improving the effectiveness of storage caching, prefetching, data layout, and disk scheduling. Unfortunately, information about block correlations is unavailable at the storage system level. Previous approaches for discovering file correlations in file systems do not scale well enough for discovering block correlations in storage systems.In this article, we propose two algorithms, C-Miner and C-Miner *, that use a data mining technique called frequent sequence mining to discover block correlations in storage systems. Both algorithms run reasonably fast with feasible space requirement, indicating that they are practical for dynamically inferring correlations in a storage system. C-Miner is a direct application of a frequent-sequence mining algorithm with a few modifications; compared with C-Miner , C-Miner * is redesigned for mining block correlations by making concessions for the specific problem of long sequences in storage system traces. Therefore, C-Miner * can discover 7--109% more correlation rules within 2--15 times shorter time than C-Miner . Moreover, we have also evaluated the benefits of block correlation-directed prefetching and data layout through experiments. Our results using real system workloads show that correlation-directed prefetching and data layout can reduce average I/O response time by 12--30% compared to the base case, and 7--25% compared to the commonly used sequential prefetching scheme for most workloads.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2070005502",
    "type": "article"
  },
  {
    "title": "On incremental file system development",
    "doi": "https://doi.org/10.1145/1149976.1149979",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Erez Zadok; Rakesh Iyer; Nikolai Joukov; Gopalan Sivathanu; Charles P. Wright",
    "corresponding_authors": "",
    "abstract": "Developing file systems from scratch is difficult and error prone. Using layered, or stackable, file systems is a powerful technique to incrementally extend the functionality of existing file systems on commodity OSes at runtime. In this article, we analyze the evolution of layering from historical models to what is found in four different present day commodity OSes: Solaris, FreeBSD, Linux, and Microsoft Windows. We classify layered file systems into five types based on their functionality and identify the requirements that each class imposes on the OS. We then present five major design issues that we encountered during our experience of developing over twenty layered file systems on four OSes. We discuss how we have addressed each of these issues on current OSes, and present insights into useful OS and VFS features that would provide future developers more versatile solutions for incremental file system development.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2162314411",
    "type": "article"
  },
  {
    "title": "SLAS",
    "doi": "https://doi.org/10.1145/1227835.1227838",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Guangyan Zhang; Jiwu Shu; Wei Xue; Weimin Zheng",
    "corresponding_authors": "",
    "abstract": "Round-robin striping, due to its uniform distribution and low-complexity computation, is widely used by applications which demand high bandwidth and massive storage. Because many systems are nonstoppable when their storage capacity and I/O bandwidth need increasing, an efficient and online mechanism to add more disks to striped volumes is very important. In this article, it is presented and proved that during data redistribution caused by scaling a round-robin striped volume, there is always a reordering window where data consistency can be maintained while changing the order of data movements. Furthermore, by exploiting the reordering window characteristic, SLAS is proposed to scale round-robin striped volumes, which reduces the cost of data redistribution effectively. First, SLAS applies a new mapping management solution based on a sliding window to support data redistribution without loss of scalability; second, it uses lazy updates of mapping metadata to decrease the number of metadata writes required by data redistribution; third, it changes the order of data chunk movements to aggregate reads/writes of data chunks. Our results from detailed simulations using real-system workloads show that, compared with the traditional approach, SLAS can reduce redistribution duration by up to 40.79% with similar maximum response time of foreground I/Os. Finally, our discussion indicates that the SLAS approach works for both disk addition and disk removal to/from striped volumes.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2055869642",
    "type": "article"
  },
  {
    "title": "Niobe",
    "doi": "https://doi.org/10.1145/1326542.1326543",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "John MacCormick; Chandramohan A. Thekkath; Markus Jäger; Kristof Roomp; Lidong Zhou; Ryan S. Peterson",
    "corresponding_authors": "",
    "abstract": "The task of consistently and reliably replicating data is fundamental in distributed systems, and numerous existing protocols are able to achieve such replication efficiently. When called on to build a large-scale enterprise storage system with built-in replication, we were therefore surprised to discover that no existing protocols met our requirements. As a result, we designed and deployed a new replication protocol called Niobe . Niobe is in the primary-backup family of protocols, and shares many similarities with other protocols in this family. But we believe Niobe is significantly more practical for large-scale enterprise storage than previously published protocols. In particular, Niobe is simple, flexible, has rigorously proven yet simply stated consistency guarantees, and exhibits excellent performance. Niobe has been deployed as the backend for a commercial Internet service; its consistency properties have been proved formally from first principles, and further verified using the TLA + specification language. We describe the protocol itself, the system built to deploy it, and some of our experiences in doing so.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1978115992",
    "type": "article"
  },
  {
    "title": "Efficient management of idleness in storage systems",
    "doi": "https://doi.org/10.1145/1534912.1534913",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Ningfang Mi; Alma Riska; Qi Zhang; Evgenia Smirni; Erik Riedel",
    "corresponding_authors": "",
    "abstract": "Various activities that intend to enhance performance, reliability, and availability of storage systems are scheduled with low priority and served during idle times. Under such conditions, idleness becomes a valuable “resource” that needs to be efficiently managed. A common approach in system design is to be nonwork conserving by “idle waiting”, that is, delay the scheduling of background jobs to avoid slowing down upcoming foreground tasks. In this article, we complement “idle waiting” with the “estimation” of background work to be served in every idle interval to effectively manage the trade-off between the performance of foreground and background tasks. As a result, the storage system is better utilized without compromising foreground performance. Our analysis shows that if idle times have low variability, then idle waiting is not necessary. Only if idle times are highly variable does idle waiting become necessary to minimize the impact of background activity on foreground performance. We further show that if there is burstiness in idle intervals, then it is possible to predict accurately the length of incoming idle intervals and use this information to serve more background jobs without affecting foreground performance.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2019993241",
    "type": "article"
  },
  {
    "title": "A Prefetching Scheme Exploiting both Data Layout and Access History on Disk",
    "doi": "https://doi.org/10.1145/2508010",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Song Jiang; Xiaoning Ding; Yuehai Xu; Kei Davis",
    "corresponding_authors": "",
    "abstract": "Prefetching is an important technique for improving effective hard disk performance. A prefetcher seeks to accurately predict which data will be requested and load it ahead of the arrival of the corresponding requests. Current disk prefetch policies in major operating systems track access patterns at the level of file abstraction. While this is useful for exploiting application-level access patterns, for two reasons file-level prefetching cannot realize the full performance improvements achievable by prefetching. First, certain prefetch opportunities can only be detected by knowing the data layout on disk, such as the contiguous layout of file metadata or data from multiple files. Second, nonsequential access of disk data (requiring disk head movement) is much slower than sequential access, and the performance penalty for mis-prefetching a randomly located block, relative to that of a sequential block, is correspondingly greater. To overcome the inherent limitations of prefetching at logical file level, we propose to perform prefetching directly at the level of disk layout, and in a portable way. Our technique, called DiskSeen , is intended to be supplementary to, and to work synergistically with, any present file-level prefetch policies. DiskSeen tracks the locations and access times of disk blocks and, based on analysis of their temporal and spatial relationships, seeks to improve the sequentiality of disk accesses and overall prefetching performance. It also implements a mechanism to minimize mis-prefetching, on a per-application basis, to mitigate the corresponding performance penalty. Our implementation of the DiskSeen scheme in the Linux 2.6 kernel shows that it can significantly improve the effectiveness of prefetching, reducing execution times by 20%--60% for microbenchmarks and real applications such as grep , CVS , and TPC-H . Even for workloads specifically designed to expose its weaknesses, DiskSeen incurs only minor performance loss.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1991932143",
    "type": "article"
  },
  {
    "title": "Disk Scrubbing Versus Intradisk Redundancy for RAID Storage Systems",
    "doi": "https://doi.org/10.1145/1970348.1970350",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Ilias Iliadis; Robert Haas; Xiaoyu Hu; Evangelos Eleftheriou",
    "corresponding_authors": "",
    "abstract": "Two schemes proposed to cope with unrecoverable or latent media errors and enhance the reliability of RAID systems are examined. The first scheme is the established, widely used, disk scrubbing scheme, which operates by periodically accessing disk drives to detect media-related unrecoverable errors. These errors are subsequently corrected by rebuilding the sectors affected. The second scheme is the recently proposed intradisk redundancy scheme, which uses a further level of redundancy inside each disk, in addition to the RAID redundancy across multiple disks. A new model is developed to evaluate the extent to which disk scrubbing reduces the unrecoverable sector errors. The probability of encountering unrecoverable sector errors is derived analytically under very general conditions regarding the characteristics of the read/write process of uniformly distributed random workloads and for a broad spectrum of disk scrubbing schemes, which includes the deterministic and random scrubbing schemes. We show that the deterministic scrubbing scheme is the most efficient one. We also derive closed-form expressions for the percentage of unrecoverable sector errors that the scrubbing scheme detects and corrects, the throughput performance, and the minimum scrubbing period achievable under operation with random, uniformly distributed I/O requests. Our results demonstrate that the reliability improvement due to disk scrubbing depends on the scrubbing frequency and the load of the system, and, for heavy-write workloads, may not reach the reliability level achieved by a simple interleaved parity-check (IPC)-based intradisk redundancy scheme, which is insensitive to the load. In fact, for small unrecoverable sector error probabilities, the IPC-based intradisk redundancy scheme achieves essentially the same reliability as that of a system operating without unrecoverable sector errors. For heavy loads, the reliability achieved by the scrubbing scheme can be orders of magnitude less than that of the intradisk redundancy scheme. Finally, the I/O and throughput performances are evaluated by means of analysis and event-driven simulation.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2061424809",
    "type": "article"
  },
  {
    "title": "Reducing Write Amplification of Flash Storage through Cooperative Data Management with NVM",
    "doi": "https://doi.org/10.1145/3060146",
    "publication_date": "2017-05-29",
    "publication_year": 2017,
    "authors": "Eunji Lee; Julie Kim; Hyokyung Bahn; Sunjin Lee; Sam H. Noh",
    "corresponding_authors": "",
    "abstract": "Write amplification is a critical factor that limits the stable performance of flash-based storage systems. To reduce write amplification, this article presents a new technique that cooperatively manages data in flash storage and nonvolatile memory (NVM). Our scheme basically considers NVM as the cache of flash storage, but allows the original data in flash storage to be invalidated if there is a cached copy in NVM, which can temporarily serve as the original data. This scheme eliminates the copy-out operation for a substantial number of cached data, thereby enhancing garbage collection efficiency. Simulated results show that the proposed scheme reduces the copy-out overhead of garbage collection by 51.4% and decreases the standard deviation of response time by 35.4% on average. Measurement results obtained by implementing the proposed scheme in BlueDBM, 1 an open-source flash development platform developed by MIT, show that the proposed scheme reduces the execution time and increases IOPS by 2--21% and 3--18%, respectively, for the workloads that we considered. This article is an extended version of Lee et al. [2016], which was presented at the 32nd International Conference on Massive Data Storage Systems and Technology in 2016.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2619488106",
    "type": "article"
  },
  {
    "title": "Application Crash Consistency and Performance with CCFS",
    "doi": "https://doi.org/10.1145/3119897",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Thanumalayan Sankaranarayana Pillai; Ramnatthan Alagappan; L. Lu; Vijay Chidambaram; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "Recent research has shown that applications often incorrectly implement crash consistency. We present the Crash-Consistent File System (ccfs), a file system that improves the correctness of application-level crash consistency protocols while maintaining high performance. A key idea in ccfs is the abstraction of a stream . Within a stream, updates are committed in program order, improving correctness; across streams, there are no ordering restrictions, enabling scheduling flexibility and high performance. We empirically demonstrate that applications running atop ccfs achieve high levels of crash consistency. Further, we show that ccfs performance under standard file-system benchmarks is excellent, in the worst case on par with the highest performing modes of Linux ext4, and in some cases notably better. Overall, we demonstrate that both application correctness and high performance can be realized in a modern file system.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2759102737",
    "type": "article"
  },
  {
    "title": "HiNFS",
    "doi": "https://doi.org/10.1145/3204454",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Youmin Chen; Jiwu Shu; Jiaxin Ou; Youyou Lu",
    "corresponding_authors": "",
    "abstract": "Persistent memory provides data persistence at main memory with emerging non-volatile main memories (NVMMs). Recent persistent memory file systems aggressively use direct access , which directly copy data between user buffer and the storage layer, to avoid the double-copy overheads through the OS page cache. However, we observe they all suffer from slow writes due to NVMMs’ asymmetric read-write performance and much slower performance than DRAM. In this article, we propose HiNFS, a high-performance file system for non-volatile main memory, to combine both buffering and direct access for fine-grained file system operations. HiNFS uses an NVMM-aware Write Buffer to buffer the lazy-persistent file writes in DRAM, while performing direct access to NVMM for eager-persistent file writes. It directly reads file data from both DRAM and NVMM, by ensuring read consistency with a combination of the DRAM Block Index and Cacheline Bitmap to track the latest data between DRAM and NVMM. HiNFS also employs a Buffer Benefit Model to identify the eager-persistent file writes before issuing I/Os. Evaluations show that HiNFS significantly improves throughput by up to 184% and reduces execution time by up to 64%comparing with state-of-the-art persistent memory file systems PMFS and EXT4-DAX.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2796347822",
    "type": "article"
  },
  {
    "title": "SCMFS",
    "doi": "https://doi.org/10.1145/2501620.2501621",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Xiaojian Wu; Sheng Qiu; A. L. Narasimha Reddy",
    "corresponding_authors": "",
    "abstract": "Modern computer systems have been built around the assumption that persistent storage is accessed via a slow, block-based interface. However, emerging nonvolatile memory technologies (sometimes referred to as storage class memory (SCM)), are poised to revolutionize storage systems. The SCM devices can be attached directly to the memory bus and offer fast, fine-grained access to persistent storage. In this article, we propose a new file system---SCMFS, which is specially designed for Storage Class Memory. SCMFS is implemented on the virtual address space and utilizes the existing memory management module of the operating system to help mange the file system space. As a result, we largely simplified the file system operations of SCMFS, which allowed us a better exploration of performance gain from SCM. We have implemented a prototype in Linux and evaluated its performance through multiple benchmarks. The experimental results show that SCMFS outperforms other memory resident file systems, tmpfs, ramfs and ext2 on ramdisk, and achieves about 70% of memory bandwidth for file read/write operations.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2114482828",
    "type": "article"
  },
  {
    "title": "Fast Miss Ratio Curve Modeling for Storage Cache",
    "doi": "https://doi.org/10.1145/3185751",
    "publication_date": "2018-04-12",
    "publication_year": 2018,
    "authors": "Xiameng Hu; Xiaolin Wang; Lan Hua Zhou; Yingwei Luo; Zhenlin Wang; Chen Ding; Chencheng Ye",
    "corresponding_authors": "",
    "abstract": "The reuse distance (least recently used (LRU) stack distance) is an essential metric for performance prediction and optimization of storage cache. Over the past four decades, there have been steady improvements in the algorithmic efficiency of reuse distance measurement. This progress is accelerating in recent years, both in theory and practical implementation. In this article, we present a kinetic model of LRU cache memory, based on the average eviction time (AET) of the cached data. The AET model enables fast measurement and use of low-cost sampling. It can produce the miss ratio curve in linear time with extremely low space costs. On storage trace benchmarks, AET reduces the time and space costs compared to former techniques. Furthermore, AET is a composable model that can characterize shared cache behavior through sampling and modeling individual programs or traces.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2796678365",
    "type": "article"
  },
  {
    "title": "DIDACache",
    "doi": "https://doi.org/10.1145/3203410",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Zhaoyan Shen; Feng Chen; Yichen Jia; Zili Shao",
    "corresponding_authors": "",
    "abstract": "Key-value caching is crucial to today’s low-latency Internet services. Conventional key-value cache systems, such as Memcached, heavily rely on expensive DRAM memory. To lower Total Cost of Ownership, the industry recently is moving toward more cost-efficient flash-based solutions, such as Facebook’s McDipper [14] and Twitter’s Fatcache [56]. These cache systems typically take commercial SSDs and adopt a Memcached-like scheme to store and manage key-value cache data in flash. Such a practice, though simple, is inefficient due to the huge semantic gap between the key-value cache manager and the underlying flash devices. In this article, we advocate to reconsider the cache system design and directly open device-level details of the underlying flash storage for key-value caching. We propose an enhanced flash-aware key-value cache manager, which consists of a novel unified address mapping module, an integrated garbage collection policy, a dynamic over-provisioning space management, and a customized wear-leveling policy, to directly drive the flash management. A thin intermediate library layer provides a slab-based abstraction of low-level flash memory space and an API interface for directly and easily operating flash devices. A special flash memory SSD hardware that exposes flash physical details is adopted to store key-value items. This co-design approach bridges the semantic gap and well connects the two layers together, which allows us to leverage both the domain knowledge of key-value caches and the unique device properties. In this way, we can maximize the efficiency of key-value caching on flash devices while minimizing its weakness. We implemented a prototype, called DIDACache, based on the Open-Channel SSD platform. Our experiments on real hardware show that we can significantly increase the throughput by 35.5%, reduce the latency by 23.6%, and remove unnecessary erase operations by 28%.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2899042051",
    "type": "article"
  },
  {
    "title": "LDM",
    "doi": "https://doi.org/10.1145/2892639",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Suzhen Wu; Bo Mao; Xiaolan Chen; Hong Jiang",
    "corresponding_authors": "",
    "abstract": "With the explosive growth in data volume, the I/O bottleneck has become an increasingly daunting challenge for big data analytics. Economic forces, driven by the desire to introduce flash-based Solid-State Drives (SSDs) into the high-end storage market, have resulted in hybrid storage systems in the cloud. However, a single flash-based SSD cannot satisfy the performance, reliability, and capacity requirements of enterprise or HPC storage systems in the cloud. While an array of SSDs organized in a RAID structure, such as RAID5, provides the potential for high storage capacity and bandwidth, reliability and performance problems will likely result from the parity update operations. In this article, we propose a Log Disk Mirroring scheme (LDM) to improve the performance and reliability of SSD-based disk arrays. LDM is a hybrid disk array architecture that consists of several SSDs and two hard disk drives (HDDs). In an LDM array, the two HDDs are mirrored as a write buffer that temporally absorbs the small write requests. The small and random write data are written on the mirroring buffer by using the logging technique that sequentially appends new data. The small write data are merged and destaged to the SSD-based disk array during the system idle periods. Our prototype implementation of the LDM array and the performance evaluations show that the LDM array significantly outperforms the pure SSD-based disk arrays by a factor of 20.4 on average, and outperforms HPDA by a factor of 5.0 on average. The reliability analysis shows that the MTTDL of the LDM array is 2.7 times and 1.7 times better than that of pure SSD-based disk arrays and HPDA disk arrays.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2402605093",
    "type": "article"
  },
  {
    "title": "Spin MOSFETs as a basis for spintronics",
    "doi": "https://doi.org/10.1145/1149976.1149980",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Satoshi Sugahara; Masaaki Tanaka",
    "corresponding_authors": "",
    "abstract": "This article reviews a recently proposed new class of spin transistors referred to as spin metal-oxide-semiconductor field-effect transistors (spin MOSFETs), and their integrated circuit applications. The fundamental device structures, operating principle, and theoretically predicted device performance are presented. Spin MOSFETs potentially exhibit significant magnetotransport effects, such as large magneto-current, and also satisfy important requirements for integrated circuit applications such as high transconductance, low power-delay product, and low off-current. Since spin MOSFETs can perform signal processing and logic operations and can store digital data using both charge transport and spin degrees of freedom, they are expected to be building blocks for memory cells and logic gates in spin-electronic integrated circuits. Novel spin-electronic integrated circuit architectures for nonvolatile memory and reconfigurable logic employing spin MOSFETs are also presented.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1992987377",
    "type": "article"
  },
  {
    "title": "Efficient indexing data structures for flash-based sensor devices",
    "doi": "https://doi.org/10.1145/1210596.1210601",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Song Lin; Demetrios Zeinalipour-Yazti; Vana Kalogeraki; Dimitrios Gunopulos; Walid Najjar",
    "corresponding_authors": "",
    "abstract": "Flash memory is the most prevalent storage medium found on modern wireless sensor devices (WSDs) . In this article we present two external memory index structures for the efficient retrieval of records stored on the local flash memory of a WSD. Our index structures, MicroHash and MicroGF (micro grid files) , exploit the asymmetric read/write and wear characteristics of flash memory in order to offer high-performance indexing and searching capabilities in the presence of a low-energy budget, which is typical for the devices under discussion. Both structures organize data and index pages on the flash media using a sorted by timestamp file organization. A key idea behind these index structures is that expensive random access deletions are completely eliminated. MicroHash enables equality searches by value in constant time and equality searches by timestamp in logarithmic time at a small cost of storing index pages on the flash media. Similarly, MicroGF enables spatial equality and proximity searches in constant time. We have implemented these index structures in nesC, the programming language of the TinyOS operating system. Our trace-driven experimentation with several real datasets reveals that our index structures offer excellent search performance at a small cost of constructing and maintaining the index.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2071364162",
    "type": "article"
  },
  {
    "title": "Predictive data grouping",
    "doi": "https://doi.org/10.1145/1353452.1353454",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "David Essary; Ahmed Amer",
    "corresponding_authors": "",
    "abstract": "We demonstrate that predictive grouping is an effective mechanism for reducing disk arm movement, thereby simultaneously reducing energy consumption and data access latency. We further demonstrate that predictive grouping has untapped dramatic potential to further improve access performance and limit energy consumption. Data retrieval latencies are considered a major bottleneck, and with growing volumes of data and increased storage needs it is only growing in significance. Data storage infrastructure is therefore a growing consumer of energy at data-center scales, while the individual disk is already a significant concern for mobile computing (accounting for almost a third of a mobile system's energy demands). While improving responsiveness of storage subsystems and hence reducing latencies in data retrieval is often considered contradictory with efforts to reduce disk energy consumption, we demonstrate that predictive data grouping has the potential to simultaneously work towards both these goals. Predictive data grouping has advantages in its applicability compared to both prior approaches to reducing latencies and to reducing energy usage. For latencies, grouping can be performed opportunistically, thereby avoiding the serious performance penalties that can be incurred with prior applications of access prediction (such as predictive prefetching of data). For energy, we show how predictive grouping can even save energy use for an individual disk that is never idle. Predictive data grouping with effective replication results in a reduction of the overall mechanical movement required to retrieve data. We have built upon our detailed measurements of disk power consumption, and have estimated both the energy expended by a hard disk for its mechanical components, and that needed to move the disk arm. We have further compared, via simulation, three models of predictive grouping of on-disk data, including an optimal arrangement of data that is guaranteed to minimize disk arm movement. These experiments have allowed us to measure the limits of performance improvement achievable with optimal data grouping and replication strategies on a single device, and have further allowed us to demonstrate the potential of such schemes to reduce energy consumption of mechanical components by up to 70%.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W1963882174",
    "type": "article"
  },
  {
    "title": "P/PA-SPTF",
    "doi": "https://doi.org/10.1145/1502777.1502778",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Hyokyung Bahn; Soyoon Lee; Sam H. Noh",
    "corresponding_authors": "",
    "abstract": "MEMS-based storage is foreseen as a promising storage media that provides high-bandwidth, low-power consumption, high-density, and low cost. Due to these versatile features, MEMS storage is anticipated to be used for a wide range of applications from storage for small handheld devices to high capacity mass storage servers. However, MEMS storage has vastly different physical characteristics compared to a traditional disk. First, MEMS storage has thousands of heads that can be activated simultaneously. Second, the media of MEMS storage is a square structure which is different from the platter structure of disks. This article presents a new request scheduling algorithm for MEMS storage called P-SPTF that makes use of the aforementioned characteristics. P-SPTF considers the parallelism of MEMS storage as well as the seek time of requests on the two dimensional square structure. We then present another algorithm called PA-SPTF that considers the aging factor so that starvation resistance is improved. Simulation studies show that PA-SPTF improves the performance of MEMS storage by up to 39.2% in terms of the average response time and 62.4% in terms of starvation resistance compared to the widely acknowledged SPTF algorithm. We also show that there exists a spectrum of scheduling algorithms that subsumes both the P-SPTF and PA-SPTF algorithms.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1975709371",
    "type": "article"
  },
  {
    "title": "Kinesis",
    "doi": "https://doi.org/10.1145/1480439.1480440",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "John MacCormick; Nicholas A. Murphy; Venugopalan Ramasubramanian; Udi Wieder; Junfeng Yang; Lidong Zhou",
    "corresponding_authors": "",
    "abstract": "Kinesis is a novel data placement model for distributed storage systems. It exemplifies three design principles: structure (division of servers into a few failure-isolated segments), freedom of choice (freedom to allocate the best servers to store and retrieve data based on current resource availability), and scattered distribution (independent, pseudo-random spread of replicas in the system). These design principles enable storage systems to achieve balanced utilization of storage and network resources in the presence of incremental system expansions, failures of single and shared components, and skewed distributions of data size and popularity. In turn, this ability leads to significantly reduced resource provisioning costs, good user-perceived response times, and fast, parallelized recovery from independent and correlated failures. This article validates Kinesis through theoretical analysis, simulations, and experiments on a prototype implementation. Evaluations driven by real-world traces show that Kinesis can significantly outperform the widely used Chain replica-placement strategy in terms of resource requirements, end-to-end delay, and failure recovery.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2030433619",
    "type": "article"
  },
  {
    "title": "Membrane",
    "doi": "https://doi.org/10.1145/1837915.1837919",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Sundararaman Swaminathan; Sriram Subramanian; Abhishek Rajimwale; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau; Michael M. Swift",
    "corresponding_authors": "",
    "abstract": "We introduce Membrane, a set of changes to the operating system to support restartable file systems. Membrane allows an operating system to tolerate a broad class of file system failures, and does so while remaining transparent to running applications; upon failure, the file system restarts, its state is restored, and pending application requests are serviced as if no failure had occurred. Membrane provides transparent recovery through a lightweight logging and checkpoint infrastructure, and includes novel techniques to improve performance and correctness of its fault-anticipation and recovery machinery. We tested Membrane with ext2, ext3, and VFAT. Through experimentation, we show that Membrane induces little performance overhead and can tolerate a wide range of file system crashes. More critically, Membrane does so with little or no change to existing file systems, thus improving robustness to crashes without mandating intrusive changes to existing file-system code.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2019457848",
    "type": "article"
  },
  {
    "title": "Efficient software implementations of large finite fields <i>GF</i> (2 <sup> <i>n</i> </sup> ) for secure storage applications",
    "doi": "https://doi.org/10.1145/2093139.2093141",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Jianqiang Luo; Kevin D. Bowers; Alina Oprea; Lihao Xu",
    "corresponding_authors": "",
    "abstract": "Finite fields are widely used in constructing error-correcting codes and cryptographic algorithms. In practice, error-correcting codes use small finite fields to achieve high-throughput encoding and decoding. Conversely, cryptographic systems employ considerably larger finite fields to achieve high levels of security. We focus on developing efficient software implementations of arithmetic operations in reasonably large finite fields as needed by secure storage applications. In this article, we study several arithmetic operation implementations for finite fields ranging from GF (2 32 ) to GF (2 128 ). We implement multiplication and division in these finite fields by making use of precomputed tables in smaller fields, and several techniques of extending smaller field arithmetic into larger field operations. We show that by exploiting known techniques, as well as new optimizations, we are able to efficiently support operations over finite fields of interest. We perform a detailed evaluation of several techniques, and show that we achieve very practical performance for both multiplication and division. Finally, we show how these techniques find applications in the implementation of HAIL, a highly available distributed cloud storage layer. Using the newly implemented arithmetic operations in GF (2 64 ), HAIL improves its performance by a factor of two, while simultaneously providing a higher level of security.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1998176833",
    "type": "article"
  },
  {
    "title": "Ursa",
    "doi": "https://doi.org/10.1145/2435204.2435205",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Gae-won You; Seung-won Hwang; Navendu Jain",
    "corresponding_authors": "",
    "abstract": "Enterprise and cloud data centers are comprised of tens of thousands of servers providing petabytes of storage to a large number of users and applications. At such a scale, these storage systems face two key challenges: (1) hot-spots due to the dynamic popularity of stored objects; and (2) high operational costs due to power and cooling. Existing storage solutions, however, are unsuitable to address these challenges because of the large number of servers and data objects. This article describes the design, implementation, and evaluation of Ursa, a system that scales to a large number of storage nodes and objects, and aims to minimize latency and bandwidth costs during system reconfiguration. Toward this goal, Ursa formulates an optimization problem that selects a subset of objects from hot-spot servers and performs topology-aware migration to minimize reconfiguration costs. As exact optimization is computationally expensive, we devise scalable approximation techniques for node selection and efficient divide-and-conquer computation. We also show that the same dynamic reconfiguration techniques can be leveraged to reduce power costs by dynamically migrating data off under-utilized nodes, and powering up servers neighboring existing hot-spots to reduce reconfiguration costs. Our evaluation shows that Ursa achieves cost-effective load management, is time-responsive in computing placement decisions (e.g., about two minutes for 10K nodes and 10M objects), and provides power savings of 15%--37%.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2080242564",
    "type": "article"
  },
  {
    "title": "A driver-layer caching policy for removable storage devices",
    "doi": "https://doi.org/10.1145/1970343.1970344",
    "publication_date": "2011-06-01",
    "publication_year": 2011,
    "authors": "Yuan-Hao Chang; Ping-Yi Hsu; Yung-Feng Lu; Tei‐Wei Kuo",
    "corresponding_authors": "",
    "abstract": "The growing popularity of flash memory is expected to draw attention to the limitations of file-system performance over flash memory. This work was motivated by the modular designs of operating system components such as bus and device drivers. A filter-driver-layered caching design is proposed to resolve the performance gap among file systems and to improve their performance with the considerations of flash memory characteristics. An efficient hybrid tree structure is presented to organize and manipulate the intervals of cached writes. Algorithms are proposed in the merging, padding, and removing of the data of writes. The effectiveness of the proposed approach is demonstrated with some analysis study of FAT-formatted and NTFS-formatted USB flash disks. The proposed cohesive caching policy was implemented as a filter driver in Windows XP/Vista for performance evaluation. In the experiments, a ten-fold or larger performance improvement was usually achieved when the cache size was only 64KB. Other substantial improvements were also observed in the experiments. For example, the proposed design enabled FAT-formatted and NTFS-formatted flash-memory devices to copy Linux image files 93% and 14% faster than conventional flash drives, respectively.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2097277747",
    "type": "article"
  },
  {
    "title": "Caching Strategies for High-Performance Storage Media",
    "doi": "https://doi.org/10.1145/2633691",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "Eunji Lee; Hyokyung Bahn",
    "corresponding_authors": "",
    "abstract": "Due to the large access latency of hard disks during data retrieval in computer systems, buffer caching mechanisms have been studied extensively in database and operating systems. By storing requested data into the buffer cache, subsequent requests can be directly serviced without accessing slow disk storage. Meanwhile, high-speed storage media like PCM (phase-change memory) have emerged recently, and one may wonder if the traditional buffer cache will be still effective for these high-speed storage media. This article answers the question by showing that the buffer cache is still effective in such environments due to the software overhead and the bimodal data access characteristics. Based on this observation, we present a new buffer cache management scheme appropriately designed for the system where the speed gap between cache and storage is narrow. To this end, we analyze the condition that caching will be effective and find the characteristics of access patterns that can be exploited in managing buffer cache for high performance storage like PCM.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2030012828",
    "type": "article"
  },
  {
    "title": "A Novel ReRAM-Based Processing-in-Memory Architecture for Graph Traversal",
    "doi": "https://doi.org/10.1145/3177916",
    "publication_date": "2018-02-26",
    "publication_year": 2018,
    "authors": "Lei Han; Zhaoyan Shen; Duo Liu; Zili Shao; H. Howie Huang; Tao Li",
    "corresponding_authors": "",
    "abstract": "Graph algorithms such as graph traversal have been gaining ever-increasing importance in the era of big data. However, graph processing on traditional architectures issues many random and irregular memory accesses, leading to a huge number of data movements and the consumption of very large amounts of energy. To minimize the waste of memory bandwidth, we investigate utilizing processing-in-memory (PIM), combined with non-volatile metal-oxide resistive random access memory (ReRAM), to improve both computation and I/O performance. We propose a new ReRAM-based processing-in-memory architecture called RPBFS, in which graph data can be persistently stored and processed in place. We study the problem of graph traversal, and we design an efficient graph traversal algorithm in RPBFS. Benefiting from low data movement overhead and high bank-level parallel computation, RPBFS shows a significant performance improvement compared with both the CPU-based and the GPU-based BFS implementations. On a suite of real-world graphs, our architecture yields a speedup in graph traversal performance of up to 33.8×, and achieves a reduction in energy over conventional systems of up to 142.8×.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2792682036",
    "type": "article"
  },
  {
    "title": "Efficient Memory-Mapped I/O on Fast Storage Device",
    "doi": "https://doi.org/10.1145/2846100",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Nae Young Song; Yongseok Son; Hyuck Han; Heon Y. Yeom",
    "corresponding_authors": "",
    "abstract": "In modern operating systems, memory-mapped I/O ( mmio ) is an important access method that maps a file or file-like resource to a region of memory. The mapping allows applications to access data from files through memory semantics (i.e., load/store) and it provides ease of programming. The number of applications that use mmio are increasing because memory semantics can provide better performance than file semantics (i.e., read/write). As more data are located in the main memory, the performance of applications can be enhanced owing to the effect of a large cache. When mmio is used, hot data tend to reside in the main memory and cold data are located in storage devices such as HDD and SSD; data placement in the memory hierarchy depends on the virtual memory subsystem of the operating system. Generally, the performance of storage devices has a direct impact on the performance of mmio . It is widely expected that better storage devices will lead to better performance. However, the expectation is limited when fast storage devices are used since the virtual memory subsystem does not reflect the performance feature of those devices. In this article, we examine the Linux virtual memory subsystem and mmio path to determine the influence of fast storage on the existing Linux kernel. Throughout our investigation, we find that the overhead of the Linux virtual memory subsystem, negligible on the HDD, prevents applications from using the full performance of fast storage devices. To reduce the overheads and fully exploit the fast storage devices, we present several optimization techniques. We modify the Linux kernel to implement our optimization techniques and evaluate our prototyped system with low-latency storage devices. Experimental results show that our optimized mmio has up to 7x better performance than the original mmio . We also compare our system to a system that has enough memory to keep all data in the main memory. The system with insufficient memory and our mmio achieves 92% performance of the resource-rich system. This result implies that our virtual memory subsystem for mmap can effectively extend the main memory with fast storage devices.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2402205160",
    "type": "article"
  },
  {
    "title": "FlexDPDP",
    "doi": "https://doi.org/10.1145/2943783",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Ertem Esiner; Adilet Kachkeev; Samuel Braunfeld; Alptekın Küpçü; Öznur Özkasap",
    "corresponding_authors": "",
    "abstract": "With increasing popularity of cloud storage, efficiently proving the integrity of data stored on an untrusted server has become significant. Authenticated skip lists and rank-based authenticated skip lists (RBASL) have been used to provide support for provable data update operations in cloud storage. However, in a dynamic file scenario, an RBASL based on block indices falls short when updates are not proportional to a fixed block size; such an update to the file, even if small, may result in O ( n ) updates on the data structure for a file with n blocks. To overcome this problem, we introduce FlexList, a flexible length-based authenticated skip list. FlexList translates variable-size updates to O (⌈ u/B ⌉) insertions, removals, or modifications, where u is the size of the update and B is the (average) block size. We further present various optimizations on the four types of skip lists (regular, authenticated, rank-based authenticated, and FlexList). We build such a structure in O ( n ) time and parallelize this operation for the first time. We compute one single proof to answer multiple (non)membership queries and obtain efficiency gains of 35%, 35%, and 40% in terms of proof time, energy, and size, respectively. We propose a method of handling multiple updates at once, achieving efficiency gains of up to 60% at the server side and 90% at the client side. We also deployed our implementation of FlexDPDP (dynamic provable data possession (DPDP) with FlexList instead of RBASL) on PlanetLab, demonstrating that FlexDPDP performs comparable to the most efficient static storage scheme (provable data possession (PDP)) while providing dynamic data support.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2517677527",
    "type": "article"
  },
  {
    "title": "Performance Characterization of NVMe-over-Fabrics Storage Disaggregation",
    "doi": "https://doi.org/10.1145/3239563",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Zvika Guz; Harry Li; Anahita Shayesteh; Vijay Balakrishnan",
    "corresponding_authors": "",
    "abstract": "Storage disaggregation separates compute and storage to different nodes to allow for independent resource scaling and, thus, better hardware resource utilization. While disaggregation of hard-drives storage is a common practice, NVMe-SSD (i.e., PCIe-based SSD) disaggregation is considered more challenging. This is because SSDs are significantly faster than hard drives, so the latency overheads (due to both network and CPU processing) as well as the extra compute cycles needed for the offloading stack become much more pronounced. In this work, we characterize the overheads of NVMe-SSD disaggregation. We show that NVMe-over-Fabrics (NVMe-oF)—a recently released remote storage protocol specification—reduces the overheads of remote access to a bare minimum, thus greatly increasing the cost-efficiency of Flash disaggregation. Specifically, while recent work showed that SSD storage disaggregation via iSCSI degrades application-level throughput by 20%, we report on negligible performance degradation with NVMe-oF—both when using stress-tests as well as with a more-realistic KV-store workload.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2902903728",
    "type": "article"
  },
  {
    "title": "clfB-tree",
    "doi": "https://doi.org/10.1145/3129263",
    "publication_date": "2018-02-26",
    "publication_year": 2018,
    "authors": "Wook-Hee Kim; Jihye Seo; Jinwoong Kim; Beomseok Nam",
    "corresponding_authors": "",
    "abstract": "Emerging byte-addressable non-volatile memory (NVRAM) is expected to replace block device storages as an alternative low-latency persistent storage device. If NVRAM is used as a persistent storage device, a cache line instead of a disk page will be the unit of data transfer, consistency, and durability. In this work, we design and develop clfB-tree —a B-tree structure whose tree node fits in a single cache line. We employ existing write combining store buffer and restricted transactional memory to provide a failure-atomic cache line write operation. Using the failure-atomic cache line write operations, we atomically update a clfB-tree node via a single cache line flush instruction without major changes in hardware. However, there exist many processors that do not provide SW interface for transactional memory. For those processors, our proposed clfB-tree achieves atomicity and consistency via in-place update, which requires maximum four cache line flushes. We evaluate the performance of clfB-tree on an NVRAM emulation board with ARM Cortex A-9 processor and a workstation that has Intel Xeon E7-4809 v3 processor. Our experimental results show clfB-tree outperforms wB-tree and CDDS B-tree by a large margin in terms of both insertion and search performance.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2790811920",
    "type": "article"
  },
  {
    "title": "Shuffle Index",
    "doi": "https://doi.org/10.1145/2747878",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "Sabrina De Capitani di Vimercati; Sara Foresti; Stefano Paraboschi; Gerardo Pelosi; Pierangela Samarati",
    "corresponding_authors": "",
    "abstract": "Data outsourcing and cloud computing have been emerging at an ever-growing rate as successful approaches for allowing users and companies to rely on external services for storing and managing data. As data and access to them are not under the control of the data owner, there is a clear need to provide proper confidentiality protection. Such requirements concern the confidentiality not only of the stored data (content) but also of the specific accesses (or patterns of them) that users make on such data. In this article, we address these issues and propose an approach for guaranteeing content, access, and pattern confidentiality in a data outsourcing scenario. The proposed solution is based on the definition of a shuffle index structure, which adapts traditional B +-trees and, by applying a combination of techniques (covers, caches, and shuffling), ensures confidentiality of the data and of queries over them, protecting each single access as well as sequences thereof. The proposed solution also supports update operations over the data, while making reads and writes not recognizable as such by the server. We show that the shuffle index exhibits a limited performance cost, thus resulting effectively usable in practice.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1997575000",
    "type": "article"
  },
  {
    "title": "Blurred Persistence",
    "doi": "https://doi.org/10.1145/2851504",
    "publication_date": "2016-01-07",
    "publication_year": 2016,
    "authors": "Youyou Lu; Jiwu Shu; Long Sun",
    "corresponding_authors": "",
    "abstract": "Persistent memory provides data durability in main memory and enables memory-level storage systems. To ensure consistency of such storage systems, memory writes need to be transactional and are carefully moved across the boundary between the volatile CPU cache and the persistent main memory. Unfortunately, cache management in the CPU cache is hardware-controlled. Legacy transaction mechanisms, which are designed for disk-based storage systems, are inefficient in ordered data persistence of transactions in persistent memory. In this article, we propose the Blurred Persistence mechanism to reduce the transaction overhead of persistent memory by blurring the volatility-persistence boundary. Blurred Persistence consists of two techniques. First, Execution in Log executes a transaction in the log to eliminate duplicated data copies for execution. It allows persistence of the volatile uncommitted data, which are detectable with reorganized log structure. Second, Volatile Checkpoint with Bulk Persistence allows the committed data to aggressively stay volatile by leveraging the data durability in the log, as long as the commit order across threads is kept. By doing so, it reduces the frequency of forced persistence and improves cache efficiency. Evaluations show that our mechanism improves system performance by 56.3% to 143.7% for a variety of workloads.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2225730524",
    "type": "article"
  },
  {
    "title": "Efficient Dynamic Provable Possession of Remote Data via Update Trees",
    "doi": "https://doi.org/10.1145/2747877",
    "publication_date": "2016-02-19",
    "publication_year": 2016,
    "authors": "Yihua Zhang; Marina Blanton",
    "corresponding_authors": "",
    "abstract": "The emergence and wide availability of remote storage service providers prompted work in the security community that allows clients to verify integrity and availability of the data that they outsourced to a not fully trusted remote storage server at a relatively low cost. Most recent solutions to this problem allow clients to read and update (i.e., insert, modify, or delete) stored data blocks while trying to lower the overhead associated with verifying the integrity of the stored data. In this work, we develop a novel scheme, performance of which favorably compares with the existing solutions. Our solution additionally enjoys a number of new features, such as a natural support for operations on ranges of blocks, revision control, and support for multiple user access to shared content. The performance guarantees that we achieve stem from a novel data structure called a balanced update tree and removing the need for interaction during update operations in addition to communicating the updates themselves.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2320538221",
    "type": "article"
  },
  {
    "title": "Inspection and Characterization of App File Usage in Mobile Devices",
    "doi": "https://doi.org/10.1145/3404119",
    "publication_date": "2020-09-24",
    "publication_year": 2020,
    "authors": "Cheng Ji; Riwei Pan; Li-Pin Chang; Liang Shi; Zongwei Zhu; Yu Liang; Tei‐Wei Kuo; Chun Jason Xue",
    "corresponding_authors": "",
    "abstract": "While the computing power of mobile devices has been quickly evolving in recent years, the growth of mobile storage capacity is, however, relatively slower. A common problem shared by budget-phone users is that they frequently run out of storage space. This article conducts a deep inspection of file usage of mobile applications and their potential implications on user experience. Our major findings are as follows: First, mobile applications could rapidly consume storage space by creating temporary cache files, but these cache files quickly become obsolete after being re-used for a short period of time. Second, file access patterns of large files, especially executable files, appear highly sparse and random, and therefore large portions of file space are never visited. Third, file prefetching brings an excessive amount of file data into page cache but only a few prefetched data are actually used. The unnecessary memory pressure causes premature memory reclamation and prolongs application launching time. Through the feasibility study of two preliminary optimizations, we demonstrated a high potential to eliminate unnecessary storage and memory space consumption with a minimal impact on user experience.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3088780467",
    "type": "article"
  },
  {
    "title": "Information Leakage in Encrypted Deduplication via Frequency Analysis",
    "doi": "https://doi.org/10.1145/3365840",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Jingwei Li; Patrick P. C. Lee; Chufeng Tan; Chuan Qin; Xiaosong Zhang",
    "corresponding_authors": "",
    "abstract": "Encrypted deduplication combines encryption and deduplication to simultaneously achieve both data security and storage efficiency. State-of-the-art encrypted deduplication systems mainly build on deterministic encryption to preserve deduplication effectiveness. However, such deterministic encryption reveals the underlying frequency distribution of the original plaintext chunks. This allows an adversary to launch frequency analysis against the ciphertext chunks and infer the content of the original plaintext chunks. In this article, we study how frequency analysis affects information leakage in encrypted deduplication, from both attack and defense perspectives. Specifically, we target backup workloads and propose a new inference attack that exploits chunk locality to increase the coverage of inferred chunks. We further combine the new inference attack with the knowledge of chunk sizes and show its attack effectiveness against variable-size chunks. We conduct trace-driven evaluation on both real-world and synthetic datasets and show that our proposed attacks infer a significant fraction of plaintext chunks under backup workloads. To defend against frequency analysis, we present two defense approaches, namely MinHash encryption and scrambling. Our trace-driven evaluation shows that our combined MinHash encryption and scrambling scheme effectively mitigates the severity of the inference attacks, while maintaining high storage efficiency and incurring limited metadata access overhead.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3013707356",
    "type": "article"
  },
  {
    "title": "NVLSM: A Persistent Memory Key-Value Store Using Log-Structured Merge Tree with Accumulative Compaction",
    "doi": "https://doi.org/10.1145/3453300",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Baoquan Zhang; David H. C. Du",
    "corresponding_authors": "",
    "abstract": "Computer systems utilizing byte-addressable Non-Volatile Memory ( NVM ) as memory/storage can provide low-latency data persistence. The widely used key-value stores using Log-Structured Merge Tree ( LSM-Tree ) are still beneficial for NVM systems in aspects of the space and write efficiency. However, the significant write amplification introduced by the leveled compaction of LSM-Tree degrades the write performance of the key-value store and shortens the lifetime of the NVM devices. The existing studies propose new compaction methods to reduce write amplification. Unfortunately, they result in a relatively large read amplification. In this article, we propose NVLSM, a key-value store for NVM systems using LSM-Tree with new accumulative compaction. By fully utilizing the byte-addressability of NVM, accumulative compaction uses pointers to accumulate data into multiple floors in a logically sorted run to reduce the number of compactions required. We have also proposed a cascading searching scheme for reads among the multiple floors to reduce read amplification. Therefore, NVLSM reduces write amplification with small increases in read amplification. We compare NVLSM with key-value stores using LSM-Tree with two other compaction methods: leveled compaction and fragmented compaction. Our evaluations show that NVLSM reduces write amplification by up to 67% compared with LSM-Tree using leveled compaction without significantly increasing the read amplification. In write-intensive workloads, NVLSM reduces the average latency by 15.73%–41.2% compared to other key-value stores.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3195948004",
    "type": "article"
  },
  {
    "title": "Pattern-Based Prefetching with Adaptive Cache Management Inside of Solid-State Drives",
    "doi": "https://doi.org/10.1145/3474393",
    "publication_date": "2022-01-29",
    "publication_year": 2022,
    "authors": "Jun Li; Xiaofei Xu; Zhigang Cai; Jianwei Liao; Kenli Li; Balazs Gerofi; Yutaka Ishikawa",
    "corresponding_authors": "",
    "abstract": "This article proposes a pattern-based prefetching scheme with the support of adaptive cache management, at the flash translation layer of solid-state drives ( SSDs ). It works inside of SSDs and has features of OS dependence and uses transparency. Specifically, it first mines frequent block access patterns that reflect the correlation among the occurred I/O requests. Then, it compares the requests in the current time window with the identified patterns to direct prefetching data into the cache of SSDs. More importantly, to maximize the cache use efficiency, we build a mathematical model to adaptively determine the cache partition on the basis of I/O workload characteristics, for separately buffering the prefetched data and the written data. Experimental results show that our proposal can yield improvements on average read latency by 1.8 %– 36.5 % without noticeably increasing the write latency, in contrast to conventional SSD-inside prefetching schemes.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4210292057",
    "type": "article"
  },
  {
    "title": "Survey of Distributed File System Design Choices",
    "doi": "https://doi.org/10.1145/3465405",
    "publication_date": "2022-02-28",
    "publication_year": 2022,
    "authors": "Peter Macko; Jason Hennessey",
    "corresponding_authors": "",
    "abstract": "Decades of research on distributed file systems and storage systems exists. New researchers and engineers have a lot of literature to study, but only a comparatively small number of high-level design choices are available when creating a distributed file system. And within each aspect of the system, typically several common approaches are used. So, rather than surveying distributed file systems, this article presents a survey of important design decisions and, within those decisions, the most commonly used options. It also presents a qualitative exploration of their tradeoffs. We include several relatively recent designs and their variations that illustrate other tradeoff choices in the design space, despite being underexplored. In doing so, we provide a primer on distributed file systems, and we also show areas that are overexplored and underexplored, in the hopes of inspiring new research.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4214919759",
    "type": "article"
  },
  {
    "title": "A Survey of the Past, Present, and Future of Erasure Coding for Storage Systems",
    "doi": "https://doi.org/10.1145/3708994",
    "publication_date": "2024-12-31",
    "publication_year": 2024,
    "authors": "Zhirong Shen; Yuhui Cai; Keyun Cheng; Patrick P. C. Lee; Xiaolu Li; Yuchong Hu; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Erasure coding is a known redundancy technique that has been popularly deployed in modern storage systems to protect against failures. By introducing a small portion of coded redundancy into data storage, erasure coding is shown to provide higher reliability guarantees than replication under the same storage overhead. Despite its storage efficiency, erasure coding incurs high performance overhead in repair and updates, and its reliability also depends on the amount of redundancy. How to resolve the tensions among storage efficiency, performance, and reliability has been the major research direction in the literature for decades. In this paper, we present an in-depth survey of the past, present, and future of erasure coding in storage systems. We conduct our survey from a systems perspective, with an emphasis on how erasure coding is deployed in practical storage systems. Specifically, we first review the use of erasure coding in storage systems from both academia and industry, and state the challenges of deploying erasure coding in practice. We then review the topics of erasure coding in three aspects: (i) new erasure code constructions, (ii) algorithmic techniques for efficient erasure coding operations, and (iii) erasure coding for emerging architectures. Finally, we provide future research directions for erasure coding.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4405950648",
    "type": "article"
  },
  {
    "title": "Reliability and security of RAID storage systems and D2D archives using SATA disk drives",
    "doi": "https://doi.org/10.1145/1044956.1044961",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Gordon F. Hughes; Joseph F. Murray",
    "corresponding_authors": "",
    "abstract": "Information storage reliability and security is addressed by using personal computer disk drives in enterprise-class nearline and archival storage systems. The low cost of these serial ATA (SATA) PC drives is a tradeoff against drive reliability design and demonstration test levels, which are higher in the more expensive SCSI and Fibre Channel drives. This article discusses the tradeoff between SATA which has the advantage that fewer higher capacity drives are needed for a given system storage capacity, which further reduces cost and allows higher drive failure rates, and the use of additional storage system redundancy and drive failure prediction to maintain system data integrity using less reliable drives. RAID stripe failure probability is calculated using typical ATA and SCSI drive failure rates, for single and double parity data reconstruction failure, and failure due to drive unrecoverable block errors. Reliability improvement from drive failure prediction is also calculated, and can be significant. Today's SATA drive specifications for unrecoverable block errors appear to allow stripe reconstruction failure, and additional in-drive parity blocks are suggested as a solution. The possibility of using low cost disks data for backup and archiving is discussed, replacing higher cost magnetic tape. This requires significantly better RAID stripe failure probability, and suitable drive technology alternatives are discussed. The failure rate of nonoperating drives is estimated using failure analysis results from ≈4000 drives. Nonoperating RAID stripe failure rates are thereby estimated. User data security needs to be assured in addition to reliability, and to extend past the point where physical control of drives is lost, such as when drives are removed from systems for data vaulting, repair, sale, or discard. Today, over a third of resold drives contain unerased user data. Security is proposed via the existing SATA drive secure-erase command, or via the existing SATA drive password commands, or by data encryption. Finally, backup and archival disc storage is compared to magnetic tape, a technology with a proven reliability record over the full half-century of digital data storage. In contrast, tape archives are not vulnerable to tape transport failure modes. Only failure modes in the archived tapes and reels will make data unrecoverable.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2086280354",
    "type": "article"
  },
  {
    "title": "The Design of efficient initialization and crash recovery for log-based file systems over flash memory",
    "doi": "https://doi.org/10.1145/1210596.1210600",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Chin-Hsien Wu; Tei‐Wei Kuo; Li-Pin Chang",
    "corresponding_authors": "",
    "abstract": "While flash memory has been widely adopted for storage systems for various embedded systems, issues of performance and reliability have started receiving growing attention in recent years. How to provide efficient roll back and quick mounting for flash-memory file systems has become an important research topic in recent years, in addition to the work on effective garbage collection and superb runtime performance. Such an observation motivates our work on the investigation of efficient initialization and crash recovery of flash-memory file systems based on log structures. A methodology is proposed for the acceleration of mounting and crash recovery for log-based file systems. A system prototype based on a well-known flash-memory file system, YAFFS, was implemented with performance evaluation. Experimental results show that the proposed methodology can reduce mounting time significantly, regardless of whether the file system is properly unmounted.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2056141528",
    "type": "article"
  },
  {
    "title": "A file assignment strategy independent of workload characteristic assumptions",
    "doi": "https://doi.org/10.1145/1629075.1629079",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Tao Xie; Yao Sun",
    "corresponding_authors": "",
    "abstract": "The problem of statically assigning nonpartitioned files in a parallel I/O system has been extensively investigated. A basic workload characteristic assumption of most existing solutions to the problem is that there exists a strong inverse correlation between file access frequency and file size. In other words, the most popular files are typically small in size, while the large files are relatively unpopular. Recent studies on the characteristics of Web proxy traces suggested, however, the correlation, if any, is so weak that it can be ignored. Hence, the following two questions arise naturally. First, can existing algorithms still perform well when the workload assumption does not hold? Second, if not, can one develop a new file assignment strategy that is immune to the workload assumption? To answer these questions, we first evaluate the performance of three well-known file assignment algorithms with and without the workload assumption, respectively. Next, we develop a novel static nonpartitioned file assignment strategy for parallel I/O systems, called static round-robin (SOR), which is immune to the workload assumption. Comprehensive experimental results show that SOR consistently improves the performance in terms of mean response time over the existing schemes.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2087542788",
    "type": "article"
  },
  {
    "title": "NCQ vs. I/O scheduler",
    "doi": "https://doi.org/10.1145/1714454.1714456",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Young Jin Yu; Dong In Shin; Hyeonsang Eom; Heon Y. Yeom",
    "corresponding_authors": "",
    "abstract": "Native Command Queueing (NCQ) is an optimization technology to maximize throughput by reordering requests inside a disk drive. It has been so successful that NCQ has become the standard in SATA 2 protocol specification, and the great majority of disk vendors have adopted it for their recent disks. However, there is a possibility that the technology may lead to an information gap between the OS and a disk drive. A NCQ-enabled disk tries to optimize throughput without realizing the intention of an OS, whereas the OS does its best under the assumption that the disk will do as it is told without specific knowledge regarding the details of the disk mechanism. Let us call this expectation discord , which may cause serious problems such as request starvations or performance anomaly. In this article, we (1) confirm that expectation discord actually occurs in real systems; (2) propose software-level approaches to solve them; and (3) evaluate our mechanism. Experimental results show that our solution is simple, cheap (no special hardware required), portable, and effective.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2152529048",
    "type": "article"
  },
  {
    "title": "Causality-based versioning",
    "doi": "https://doi.org/10.1145/1629080.1629083",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Kiran‐Kumar Muniswamy‐Reddy; David A. Holland",
    "corresponding_authors": "",
    "abstract": "Versioning file systems provide the ability to recover from a variety of failures, including file corruption, virus and worm infestations, and user mistakes. However, using versions to recover from data-corrupting events requires a human to determine precisely which files and versions to restore. We can create more meaningful versions and enhance the value of those versions by capturing the causal connections among files, facilitating selection and recovery of precisely the right versions after data corrupting events. We determine when to create new versions of files automatically using the causal relationships among files. The literature on versioning file systems usually examines two extremes of possible version-creation algorithms: open-to-close versioning and versioning on every write. We evaluate causal versions of these two algorithms and introduce two additional causality-based algorithms: Cycle-Avoidance and Graph-Finesse. We show that capturing and maintaining causal relationships imposes less than 7% overhead on a versioning system, providing benefit at low cost. We then show that Cycle-Avoidance provides more meaningful versions of files created during concurrent program execution, with overhead comparable to open/close versioning. Graph-Finesse provides even greater control, frequently at comparable overhead, but sometimes at unacceptable overhead. Versioning on every write is an interesting extreme case, but is far too costly to be useful in practice.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2020877492",
    "type": "article"
  },
  {
    "title": "Exploiting Redundancies and Deferred Writes to Conserve Energy in Erasure-Coded Storage Clusters",
    "doi": "https://doi.org/10.1145/2491472.2491473",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Jianzhong Huang; Fenghao Zhang; Xiao Qin; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "We present a power-efficient scheme for erasure-coded storage clusters---ECS 2 ---which aims to offer high energy efficiency with marginal reliability degradation. ECS 2 utilizes data redundancies and deferred writes to conserve energy. In ECS 2 parity blocks are buffered exclusively in active data nodes whereas parity nodes are placed into low-power mode. ( k + r , k ) RS-coded ECS 2 can achieve ⌈( r + 1)/2⌉-fault tolerance for k active data nodes and r -fault tolerance for all k + r nodes. ECS 2 employs the following three optimizing approaches to improve the energy efficiency of storage clusters. (1) An adaptive threshold policy takes system configurations and I/O workloads into account to maximize standby time periods; (2) a selective activation policy minimizes the number of power-transitions in storage nodes; and (3) a region-based buffer policy speeds up the synchronization process by migrating parity blocks in a batch method. After implementing an ECS 2 -based prototype in a Linux cluster, we evaluated its energy efficiency and performance using four different types of I/O workloads. The experimental results indicate that compared to energy-oblivious erasure-coded storage, ECS 2 can save the energy used by storage clusters up to 29.8% and 28.0% in read-intensive and write-dominated workloads when k = 6 and r = 3, respectively. The results also show that ECS 2 accomplishes high power efficiency in both normal and failed cases without noticeably affecting the I/O performance of storage clusters.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2018478911",
    "type": "article"
  },
  {
    "title": "Redundancy Does Not Imply Fault Tolerance",
    "doi": "https://doi.org/10.1145/3125497",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Aishwarya Ganesan; Ramnatthan Alagappan; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We analyze how modern distributed storage systems behave in the presence of file-system faults such as data corruption and read and write errors. We characterize eight popular distributed storage systems and uncover numerous problems related to file-system fault tolerance. We find that modern distributed systems do not consistently use redundancy to recover from file-system faults: a single file-system fault can cause catastrophic outcomes such as data loss, corruption, and unavailability. We also find that the above outcomes arise due to fundamental problems in file-system fault handling that are common across many systems. Our results have implications for the design of next-generation fault-tolerant distributed and cloud storage systems.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2759957043",
    "type": "article"
  },
  {
    "title": "The Design and Implementation of a Rekeying-Aware Encrypted Deduplication Storage System",
    "doi": "https://doi.org/10.1145/3032966",
    "publication_date": "2017-02-25",
    "publication_year": 2017,
    "authors": "Chuan Qin; Jingwei Li; Patrick P. C. Lee",
    "corresponding_authors": "",
    "abstract": "Rekeying refers to an operation of replacing an existing key with a new key for encryption. It renews security protection to protect against key compromise and enable dynamic access control in cryptographic storage. However, it is non-trivial to realize efficient rekeying in encrypted deduplication storage systems, which use deterministic content-derived encryption keys to allow deduplication on ciphertexts. We design and implement a rekeying-aware encrypted deduplication (REED) storage system. REED builds on a deterministic version of all-or-nothing transform, such that it enables secure and lightweight rekeying, while preserving the deduplication capability. We propose two REED encryption schemes that trade between performance and security and extend REED for dynamic access control. We implement a REED prototype with various performance optimization techniques and demonstrate how we can exploit similarity to mitigate key generation overhead. Our trace-driven testbed evaluation shows that our REED prototype maintains high performance and storage efficiency.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3121854121",
    "type": "article"
  },
  {
    "title": "Accelerating File System Metadata Access with Byte-Addressable Nonvolatile Memory",
    "doi": "https://doi.org/10.1145/2766453",
    "publication_date": "2015-07-24",
    "publication_year": 2015,
    "authors": "Qingsong Wei; Jianxi Chen; Cheng Chen",
    "corresponding_authors": "",
    "abstract": "File system performance is dominated by small and frequent metadata access. Metadata is stored as blocks on the hard disk drive. Partial metadata update results in whole-block read or write, which significantly amplifies disk I/O. Furthermore, a huge performance gap between the CPU and disk aggravates this problem. In this article, a file system metadata accelerator (referred to as FSMAC) is proposed to optimize metadata access by efficiently exploiting the persistency and byte-addressability of Nonvolatile Memory (NVM). The FSMAC decouples data and metadata access path, putting data on disk and metadata in byte-addressable NVM at runtime. Thus, data is accessed in a block from I/O the bus and metadata is accessed in a byte-addressable manner from the memory bus. Metadata access is significantly accelerated and metadata I/O is eliminated because metadata in NVM is no longer flushed back to the disk periodically. A lightweight consistency mechanism combining fine-grained versioning and transaction is introduced in the FSMAC. The FSMAC is implemented on a real NVDIMM platform and intensively evaluated under different workloads. Evaluation results show that the FSMAC accelerates the file system up to 49.2 times for synchronized I/O and 7.22 times for asynchronized I/O. Moreover, it can achieve significant performance speedup in network storage and database environment, especially for metadata-intensive or write-dominated workloads.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2180142806",
    "type": "article"
  },
  {
    "title": "Towards Robust File System Checkers",
    "doi": "https://doi.org/10.1145/3281031",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Om Rameshwar Gatla; Mai Zheng; Muhammad Hameed; Viacheslav Dubeyko; Adam Manzanares; Filip Blagojević; Cyril Guyot; Robert Mateescu",
    "corresponding_authors": "",
    "abstract": "File systems may become corrupted for many reasons despite various protection techniques. Therefore, most file systems come with a checker to recover the file system to a consistent state. However, existing checkers are commonly assumed to be able to complete the repair without interruption, which may not be true in practice. In this work, we demonstrate via fault injection experiments that checkers of widely used file systems (EXT4, XFS, BtrFS, and F2FS) may leave the file system in an uncorrectable state if the repair procedure is interrupted unexpectedly. To address the problem, we first fix the ordering issue in the undo logging of e2fsck and then build a general logging library (i.e., rfsck-lib) for strengthening checkers. To demonstrate the practicality, we integrate rfsck-lib with existing checkers and create two new checkers: rfsck-ext, a robust checker for Ext-family file systems, and rfsck-xfs, a robust checker for XFS file systems, both of which require only tens of lines of modification to the original versions. Both rfsck-ext and rfsck-xfs are resilient to faults in our experiments. Also, both checkers incur reasonable performance overhead (i.e., up to 12%) compared to the original unreliable versions. Moreover, rfsck-ext outperforms the patched e2fsck by up to nine times while achieving the same level of robustness.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2903092728",
    "type": "article"
  },
  {
    "title": "REGISTOR",
    "doi": "https://doi.org/10.1145/3310149",
    "publication_date": "2019-02-28",
    "publication_year": 2019,
    "authors": "Shuyi Pei; Jing Yang; Qing Yang",
    "corresponding_authors": "",
    "abstract": "This article presents REGISTOR, a platform for r egular e xpression g rabbing i nside stor age. The main idea of Registor is accelerating regular expression (regex) search inside storage where large data set is stored, eliminating the I/O bottleneck problem. A special hardware engine for regex search is designed and augmented inside a flash SSD that processes data on-the-fly during data transmission from NAND flash to host. To make the speed of regex search match the internal bus speed of a modern SSD, a deep pipeline structure is designed in Registor hardware consisting of a file semantics extractor, matching candidates finder, regex matching units (REMUs), and results organizer. Furthermore, each stage of the pipeline makes the use of maximal parallelism possible. To make Registor readily usable by high-level applications, we have developed a set of APIs and libraries in Linux allowing Registor to process files in the SSD by recombining separate data blocks into files efficiently. A working prototype of Registor has been built in our newly designed NVMe-SSD. Extensive experiments and analyses have been carried out to show that Registor achieves high throughput, reduces the I/O bandwidth requirement by up to 97%, and reduces CPU utilization by as much as 82% for regex search in large datasets.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2923965289",
    "type": "article"
  },
  {
    "title": "Liquid Cloud Storage",
    "doi": "https://doi.org/10.1145/3281276",
    "publication_date": "2019-02-18",
    "publication_year": 2019,
    "authors": "Michael Luby; Roberto Padovani; Thomas J. Richardson; Lorenz Minder; Pooja Aggarwal",
    "corresponding_authors": "",
    "abstract": "A liquid system provides durable object storage based on spreading redundantly generated data across a network of hundreds to thousands of potentially unreliable storage nodes. A liquid system uses a combination of a large code , lazy repair , and flow storage organization . We show that a liquid system can be operated to enable flexible and essentially optimal combinations of storage durability, storage overhead, repair bandwidth usage, and access performance.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2963151010",
    "type": "article"
  },
  {
    "title": "<scp>XStore</scp> : Fast RDMA-Based Ordered Key-Value Store Using Remote Learned Cache",
    "doi": "https://doi.org/10.1145/3468520",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Xingda Wei; Rong Chen; Haibo Chen; Binyu Zang",
    "corresponding_authors": "",
    "abstract": "RDMA ( Remote Direct Memory Access ) has gained considerable interests in network-attached in-memory key-value stores. However, traversing the remote tree-based index in ordered key-value stores with RDMA becomes a critical obstacle, causing an order-of-magnitude slowdown and limited scalability due to multiple round trips. Using index cache with conventional wisdom—caching partial data and traversing them locally—usually leads to limited effect because of unavoidable capacity misses, massive random accesses, and costly cache invalidations. We argue that the machine learning (ML) model is a perfect cache structure for the tree-based index, termed learned cache . Based on it, we design and implement XStore , an RDMA-based ordered key-value store with a new hybrid architecture that retains a tree-based index at the server to perform dynamic workloads (e.g., inserts) and leverages a learned cache at the client to perform static workloads (e.g., gets and scans). The key idea is to decouple ML model retraining from index updating by maintaining a layer of indirection from logical to actual positions of key-value pairs. It allows a stale learned cache to continue predicting a correct position for a lookup key. XStore ensures correctness using a validation mechanism with a fallback path and further uses speculative execution to minimize the cost of cache misses. Evaluations with YCSB benchmarks and production workloads show that a single XStore server can achieve over 80 million read-only requests per second. This number outperforms state-of-the-art RDMA-based ordered key-value stores (namely, DrTM-Tree, Cell, and eRPC+Masstree) by up to 5.9× (from 3.7×). For workloads with inserts, XStore still provides up to 3.5× (from 2.7×) throughput speedup, achieving 53M reqs/s. The learned cache can also reduce client-side memory usage and further provides an efficient memory-performance tradeoff, e.g., saving 99% memory at the cost of 20% peak throughput.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3195621029",
    "type": "article"
  },
  {
    "title": "A Study of Failure Recovery and Logging of High-Performance Parallel File Systems",
    "doi": "https://doi.org/10.1145/3483447",
    "publication_date": "2022-03-29",
    "publication_year": 2022,
    "authors": "Runzhou Han; Om Rameshwar Gatla; Mai Zheng; Jinrui Cao; Di Zhang; Dong Dai; Yong Chen; Jonathan E. Cook",
    "corresponding_authors": "",
    "abstract": "Large-scale parallel file systems (PFSs) play an essential role in high-performance computing (HPC). However, despite their importance, their reliability is much less studied or understood compared with that of local storage systems or cloud storage systems. Recent failure incidents at real HPC centers have exposed the latent defects in PFS clusters as well as the urgent need for a systematic analysis. To address the challenge, we perform a study of the failure recovery and logging mechanisms of PFSs in this article. First, to trigger the failure recovery and logging operations of the target PFS, we introduce a black-box fault injection tool called PFault , which is transparent to PFSs and easy to deploy in practice. PFault emulates the failure state of individual storage nodes in the PFS based on a set of pre-defined fault models and enables examining the PFS behavior under fault systematically. Next, we apply PFault to study two widely used PFSs: Lustre and BeeGFS. Our analysis reveals the unique failure recovery and logging patterns of the target PFSs and identifies multiple cases where the PFSs are imperfect in terms of failure handling. For example, Lustre includes a recovery component called LFSCK to detect and fix PFS-level inconsistencies, but we find that LFSCK itself may hang or trigger kernel panics when scanning a corrupted Lustre. Even after the recovery attempt of LFSCK, the subsequent workloads applied to Lustre may still behave abnormally (e.g., hang or report I/O errors). Similar issues have also been observed in BeeGFS and its recovery component BeeGFS-FSCK. We analyze the root causes of the abnormal symptoms observed in depth, which has led to a new patch set to be merged into the coming Lustre release. In addition, we characterize the extensive logs generated in the experiments in detail and identify the unique patterns and limitations of PFSs in terms of failure logging. We hope this study and the resulting tool and dataset can facilitate follow-up research in the communities and help improve PFSs for reliable high-performance computing.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4220730878",
    "type": "article"
  },
  {
    "title": "RACE: One-sided RDMA-conscious Extendible Hashing",
    "doi": "https://doi.org/10.1145/3511895",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Pengfei Zuo; Qihui Zhou; Jiazhao Sun; Yang Liu; Shuangwu Zhang; Yu Hua; James Cheng; Rongfeng He; Huabing Yan",
    "corresponding_authors": "",
    "abstract": "Memory disaggregation is a promising technique in datacenters with the benefit of improving resource utilization, failure isolation, and elasticity. Hashing indexes have been widely used to provide fast lookup services in distributed memory systems. However, traditional hashing indexes become inefficient for disaggregated memory, since the computing power in the memory pool is too weak to execute complex index requests. To provide efficient indexing services in disaggregated memory scenarios, this article proposes RACE hashing, a one-sided RDMA-Conscious Extendible hashing index with lock-free remote concurrency control and efficient remote resizing. RACE hashing enables all index operations to be efficiently executed by using only one-sided RDMA verbs without involving any compute resource in the memory pool. To support remote concurrent access with high performance, RACE hashing leverages a lock-free remote concurrency control scheme to enable different clients to concurrently operate the same hashing index in the memory pool in a lock-free manner. To resize the hash table with low overheads, RACE hashing leverages an extendible remote resizing scheme to reduce extra RDMA accesses caused by extendible resizing and allow concurrent request execution during resizing. Extensive experimental results demonstrate that RACE hashing outperforms state-of-the-art distributed in-memory hashing indexes by 1.4–13.7× in YCSB hybrid workloads.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4293069690",
    "type": "article"
  },
  {
    "title": "End-to-end I/O Monitoring on Leading Supercomputers",
    "doi": "https://doi.org/10.1145/3568425",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Bin Yang; Wei Xue; Tianyu Zhang; Shichao Liu; Xiaosong Ma; Xiyang Wang; Weiguo Liu",
    "corresponding_authors": "",
    "abstract": "This paper offers a solution to overcome the complexities of production system I/O performance monitoring. We present Beacon, an end-to-end I/O resource monitoring and diagnosis system for the 40960-node Sunway TaihuLight supercomputer, currently the fourth-ranked supercomputer in the world. Beacon simultaneously collects and correlates I/O tracing/profiling data from all the compute nodes, forwarding nodes, storage nodes, and metadata servers. With mechanisms such as aggressive online and offline trace compression and distributed caching/storage, it delivers scalable, low-overhead, and sustainable I/O diagnosis under production use. With Beacon’s deployment on TaihuLight for more than three years, we demonstrate Beacon’s effectiveness with real-world use cases for I/O performance issue identification and diagnosis. It has already successfully helped center administrators identify obscure design or configuration flaws, system anomaly occurrences, I/O performance interference, and resource under- or over-provisioning problems. Several of the exposed problems have already been fixed, with others being currently addressed. Encouraged by Beacon’s success in I/O monitoring, we extend it to monitor interconnection networks, which is another contention point on supercomputers. In addition, we demonstrate Beacon’s generality by extending it to other supercomputers. Both Beacon codes and part of collected monitoring data are released. 1",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4309561483",
    "type": "article"
  },
  {
    "title": "Polling Sanitization to Balance I/O Latency and Data Security of High-density SSDs",
    "doi": "https://doi.org/10.1145/3639826",
    "publication_date": "2024-01-06",
    "publication_year": 2024,
    "authors": "Jiaojiao Wu; Zhigang Cai; Fan Yang; Jun Li; François Trahay; Zheng Yang; Chao Wang; Jianwei Liao",
    "corresponding_authors": "",
    "abstract": "Sanitization is an effective approach for ensuring data security through scrubbing invalid but sensitive data pages, with the cost of impacts on storage performance due to moving out valid pages from the sanitization-required wordline, which is a logical read/write unit and consists of multiple pages in high-density SSDs. To minimize the impacts on I/O latency and data security, this article proposes a polling-based scheduling approach for data sanitization in high-density SSDs. Our method polls a specific SSD channel for completing data sanitization at the block granularity, meanwhile other channels can still service I/O requests. Furthermore, our method assigns a low priority to the blocks that are more likely to have future adjacent page invalidations inside sanitization-required wordlines, while selecting the sanitization block, to minimize the negative impacts of moving valid pages. Through a series of emulation experiments on several disk traces of real-world applications, we show that our proposal can decrease the negative effects of data sanitization in terms of the risk-performance index, which is a united time metric of I/O responsiveness and the unsafe time interval, by 16.34% , on average, compared to related sanitization methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390640584",
    "type": "article"
  },
  {
    "title": "Holographic Storage for the Cloud: advances and challenges",
    "doi": "https://doi.org/10.1145/3708993",
    "publication_date": "2024-12-20",
    "publication_year": 2024,
    "authors": "Nathanaël Cheriere; Jiaqi Chu; Grace Brennan; Pashmina Cameron; Pedro F. da Costa; Jannes Gladrow; Guilherme Ilunga; Douglas J. Kelly; Sarah Lewis; Joowon Lim; Giorgio Maltese; Tony Mason; Greg O’Shea; Soujanya Ponnapalli; Michael Rudow; Alan Sanders; Theano Stavrinos; Xingbo Wu; Mengyang Yang; Dushyanth Narayanan; Benn C. Thomsen; Antony Rowstron",
    "corresponding_authors": "",
    "abstract": "Holographic Storage is an old idea that has always promised high density and fast random access, but has never been commercially competitive with Hard Disk Drives (HDDs) and Solid State Devices (SSDs). In Project HSD at Microsoft Research we asked the question: “Does holographic storage finally make sense for cloud storage?” This paper describes our journey towards answering this question. We achieved 1.8x higher density than the previous state of the art, using commodity components available today and leveraging machine learning to compensate for the noise and distortions introduced by commodity components. This uncovered two new challenges which are the focus of this paper: achieving high end-to-end energy efficiency without sacrificing capacity, and spatial multiplexing without mechanical movement. Improving end-to-end energy efficiency requires joint optimization across low-level media parameters and higher-level system parameters that govern background maintenance operations such as read refresh and garbage collection. We developed new physics models of the media; analytic and simulation models of the media access and background media maintenance; and workload-driven optimization to find optimal parameter combinations. These techniques resulted in a 14x improvement over the previous approach for typical workloads without sacrificing capacity. We also designed the first scalable and mechanical movement free spatial multiplexing system for holographic storage. Despite these advances, we conclude that currently holographic storage is still far from the combination of density, capacity scaling, and energy efficiency needed to compete with the incumbent technologies. We need fundamental advances in the physical media that improve energy efficiency by another 1–2 orders of magnitude without reducing data density. Further advances in optics are also required to achieve spatial multiplexing that is simultaneously scalable, low-loss, and high-density.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4405647145",
    "type": "article"
  },
  {
    "title": "Dynamic load balancing for I/O-intensive applications on clusters",
    "doi": "https://doi.org/10.1145/1629075.1629078",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Xiao Qin; Hong Jiang; Adam Manzanares; Xiaojun Ruan; Shu Yin",
    "corresponding_authors": "",
    "abstract": "Load balancing for clusters has been investigated extensively, mainly focusing on the effective usage of global CPU and memory resources. However, previous CPU- or memory-centric load balancing schemes suffer significant performance drop under I/O-intensive workloads due to the imbalance of I/O load. To solve this problem, we propose two simple yet effective I/O-aware load-balancing schemes for two types of clusters: (1) homogeneous clusters where nodes are identical and (2) heterogeneous clusters, which are comprised of a variety of nodes with different performance characteristics in computing power, memory capacity, and disk speed. In addition to assigning I/O-intensive sequential and parallel jobs to nodes with light I/O loads, the proposed schemes judiciously take into account both CPU and memory load sharing in the system. Therefore, our schemes are able to maintain high performance for a wide spectrum of workloads. We develop analytic models to study mean slowdowns, task arrival, and transfer processes in system levels. Using a set of real I/O-intensive parallel applications and synthetic parallel jobs with various I/O characteristics, we show that our proposed schemes consistently improve the performance over existing non-I/O-aware load-balancing schemes, including CPU- and Memory-aware schemes and a PBS-like batch scheduler for parallel and sequential jobs, for a diverse set of workload conditions. Importantly, this performance improvement becomes much more pronounced when the applications are I/O-intensive. For example, the proposed approaches deliver 23.6--88.0 % performance improvements for I/O-intensive applications such as LU decomposition, Sparse Cholesky, Titan, Parallel text searching, and Data Mining. When I/O load is low or well balanced, the proposed schemes are capable of maintaining the same level of performance as the existing non-I/O-aware schemes.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2132316133",
    "type": "article"
  },
  {
    "title": "quFiles",
    "doi": "https://doi.org/10.1145/1837915.1837920",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Kaushik Veeraraghavan; Jason Flinn; Edmund B. Nightingale; Brian Noble",
    "corresponding_authors": "",
    "abstract": "A quFile is a unifying abstraction that simplifies data management by encapsulating different physical representations of the same logical data. Similar to a quBit (quantum bit), the particular representation of the logical data displayed by a quFile is not determined until the moment it is needed. The representation returned by a quFile is specified by a data-specific policy that can take context into account such as the application requesting the data, the device on which data is accessed, screen size, and battery status. We demonstrate the generality of the quFile abstraction by using it to implement six case studies: resource management, copy-on-write versioning, data redaction, resource-aware directories, application-aware adaptation, and platform-specific encoding. Most quFile policies were expressed using less than one hundred lines of code. Our experimental results show that, with caching and other performance optimizations, quFiles add less than 1% overhead to application-level file system.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2084736653",
    "type": "article"
  },
  {
    "title": "PRESIDIO",
    "doi": "https://doi.org/10.1145/1970348.1970351",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Lawrence L. You; K.T. Pollack; Darrell D. E. Long; K. Gopinath",
    "corresponding_authors": "",
    "abstract": "The ever-increasing volume of archival data that needs to be reliably retained for long periods of time and the decreasing costs of disk storage, memory, and processing have motivated the design of low-cost, high-efficiency disk-based storage systems. However, managed disk storage is still expensive. To further lower the cost, redundancy can be eliminated with the use of interfile and intrafile data compression. However, it is not clear what the optimal strategy for compressing data is, given the diverse collections of data. To create a scalable archival storage system that efficiently stores diverse data, we present PRESIDIO, a framework that selects from different space-reduction efficent storage methods (ESMs) to detect similarity and reduce or eliminate redundancy when storing objects. In addition, the framework uses a virtualized content addressable store (VCAS) that hides from the user the complexity of knowing which space-efficient techniques are used, including chunk-based deduplication or delta compression. Storing and retrieving objects are polymorphic operations independent of their content-based address. A new technique, harmonic super-fingerprinting, is also used for obtaining successively more accurate (but also more costly) measures of similarity to identify the existing objects in a very large data set that are most similar to an incoming new object. The PRESIDIO design, when reported earlier, had comprehensively introduced for the first time the notion of deduplication, which is now being offered as a service in storage systems by major vendors. As an aid to the design of such systems, we evaluate and present various parameters that affect the efficiency of a storage system using empirical data.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2137092019",
    "type": "article"
  },
  {
    "title": "A Prefetching Scheme Exploiting both Data Layout and Access History on Disk",
    "doi": "https://doi.org/10.1145/2501620.2508010",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Song Jiang; Xiaoning Ding; Yuehai Xu; Kei Davis",
    "corresponding_authors": "",
    "abstract": "Prefetching is an important technique for improving effective hard disk performance. A prefetcher seeks to accurately predict which data will be requested and load it ahead of the arrival of the corresponding requests. Current disk prefetch policies in major operating systems track access patterns at the level of file abstraction. While this is useful for exploiting application-level access patterns, for two reasons file-level prefetching cannot realize the full performance improvements achievable by prefetching. First, certain prefetch opportunities can only be detected by knowing the data layout on disk, such as the contiguous layout of file metadata or data from multiple files. Second, nonsequential access of disk data (requiring disk head movement) is much slower than sequential access, and the performance penalty for mis-prefetching a randomly located block, relative to that of a sequential block, is correspondingly greater.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4243796148",
    "type": "article"
  },
  {
    "title": "PRE-BUD",
    "doi": "https://doi.org/10.1145/1970343.1970346",
    "publication_date": "2011-06-01",
    "publication_year": 2011,
    "authors": "Adam Manzanares; Xiao Qin; Xiaojun Ruan; Shu Yin",
    "corresponding_authors": "",
    "abstract": "A critical problem with parallel I/O systems is the fact that disks consume a significant amount of energy. To design economically attractive and environmentally friendly parallel I/O systems, we propose an energy-aware prefetching strategy (PRE-BUD) for parallel I/O systems with disk buffers. We introduce a new architecture that provides significant energy savings for parallel I/O systems using buffer disks while maintaining high performance. There are two buffer disk configurations: (1) adding an extra buffer disk to accommodate prefetched data, and (2) utilizing an existing disk as the buffer disk. PRE-BUD is not only able to reduce the number of power-state transitions, but also to increase the length and number of standby periods. As such, PRE-BUD conserves energy by keeping data disks in the standby state for increased periods of time. Compared with the first prefetching configuration, the second configuration lowers the capacity of the parallel disk system. However, the second configuration is more cost-effective and energy-efficient than the first one. Finally, we quantitatively compare PRE-BUD with both disk configurations against three existing strategies. Empirical results show that PRE-BUD is able to reduce energy dissipation in parallel disk systems by up to 50 percent when compared against a non-energy aware approach. Similarly, our strategy is capable of conserving up to 30 percent energy when compared to the dynamic power management technique.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2163454967",
    "type": "article"
  },
  {
    "title": "Ffsck",
    "doi": "https://doi.org/10.1145/2560011",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Ao Ma; Chris Dragga; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau; Marshall Kirk McKusick",
    "corresponding_authors": "",
    "abstract": "Failures, errors, and bugs can corrupt file systems and cause data loss, despite the presence of journals and similar preventive techniques. While consistency checkers such as fsck can detect corruption and repair a damaged image, they are generally created as an afterthought, to be run only at rare intervals. Thus, checkers operate slowly, causing significant downtime for large scale storage systems. We address this dilemma by treating the checker as a key component of the overall file system, rather than a peripheral add-on. To this end, we present a modified ext3 file system, rext 3, to directly support the fast file-system checker, ffsck . Rext3 colocates and self-identifies its metadata blocks, removing the need for costly seeks and tree traversals during checking. These modifications allow ffsck to scan and repair the file system at rates approaching the full sequential bandwidth of the underlying device. In addition, we demonstrate that rext3 generally performs competitively with ext3 and exceeds it in handling random reads and large writes. Finally, we apply our principles to FreeBSD’s FFS file system and its checker, doing so in a lightweight fashion that preserves the file-system layout while still providing some of the performance gains from ffsck.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2098360054",
    "type": "article"
  },
  {
    "title": "Recon",
    "doi": "https://doi.org/10.1145/2385603.2385608",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Daniel Fryer; Kuei Sun; Rahat Mahmood; Tinghao Cheng; Shaun Benjamin; Ashvin Goel; Angela Demke Brown",
    "corresponding_authors": "",
    "abstract": "File system bugs that corrupt metadata on disk are insidious. Existing reliability methods, such as checksums, redundancy, or transactional updates, merely ensure that the corruption is reliably preserved. Typical workarounds, based on using backups or repairing the file system, are painfully slow. Worse, the recovery may result in further corruption. We present Recon, a system that protects file system metadata from buggy file system operations. Our approach leverages file systems that provide crash consistency using transactional updates. We define declarative statements called consistency invariants for a file system. These invariants must be satisfied by each transaction being committed to disk to preserve file system integrity. Recon checks these invariants at commit, thereby minimizing the damage caused by buggy file systems. The major challenges to this approach are specifying invariants and interpreting file system behavior correctly without relying on the file system code. Recon provides a framework for file-system specific metadata interpretation and invariant checking. We show the feasibility of interpreting metadata and writing consistency invariants for the Linux ext3 file system using this framework. Recon can detect random as well as targeted file-system corruption at runtime as effectively as the offline e2fsck file-system checker, with low overhead.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2147041723",
    "type": "article"
  },
  {
    "title": "Efficient Deduplication in a Distributed Primary Storage Infrastructure",
    "doi": "https://doi.org/10.1145/2876509",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "João Paulo; José Pereira",
    "corresponding_authors": "",
    "abstract": "A large amount of duplicate data typically exists across volumes of virtual machines in cloud computing infrastructures. Deduplication allows reclaiming these duplicates while improving the cost-effectiveness of large-scale multitenant infrastructures. However, traditional archival and backup deduplication systems impose prohibitive storage overhead for virtual machines hosting latency-sensitive applications. Primary deduplication systems reduce such penalty but rely on special cluster filesystems, centralized components, or restrictive workload assumptions. Also, some of these systems reduce storage overhead by confining deduplication to off-peak periods that may be scarce in a cloud environment. We present DEDIS, a dependable and fully decentralized system that performs cluster-wide off-line deduplication of virtual machines’ primary volumes. DEDIS works on top of any unsophisticated storage backend, centralized or distributed, as long as it exports a basic shared block device interface. Also, DEDIS does not rely on data locality assumptions and incorporates novel optimizations for reducing deduplication overhead and increasing its reliability. The evaluation of an open-source prototype shows that minimal I/O overhead is achievable even when deduplication and intensive storage I/O are executed simultaneously. Also, our design scales out and allows collocating DEDIS components and virtual machines in the same servers, thus, sparing the need of additional hardware.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2401572102",
    "type": "article"
  },
  {
    "title": "A Unified Buffer Cache Architecture that Subsumes Journaling Functionality via Nonvolatile Memory",
    "doi": "https://doi.org/10.1145/2560010",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Eunji Lee; Hyokyung Bahn; Sam H. Noh",
    "corresponding_authors": "",
    "abstract": "Journaling techniques are widely used in modern file systems as they provide high reliability and fast recovery from system failures. However, it reduces the performance benefit of buffer caching as journaling accounts for a bulk of the storage writes in real system environments. To relieve this problem, we present a novel buffer cache architecture that subsumes the functionality of caching and journaling by making use of nonvolatile memory such as PCM or STT-MRAM. Specifically, our buffer cache supports what we call the in-place commit scheme. This scheme avoids logging, but still provides the same journaling effect by simply altering the state of the cached block to frozen. As a frozen block still provides the functionality of a cache block, we show that in-place commit does not degrade cache performance. We implement our scheme on Linux 2.6.38 and measure the throughput and execution time of the scheme with various file I/O benchmarks. The results show that our scheme improves the throughput and execution time by 89% and 34% on average, respectively, compared to the existing Linux buffer cache with ext4 without any loss of reliability.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2092851472",
    "type": "article"
  },
  {
    "title": "Performance and Resource Utilization of FUSE User-Space File Systems",
    "doi": "https://doi.org/10.1145/3310148",
    "publication_date": "2019-05-08",
    "publication_year": 2019,
    "authors": "Bharath Kumar Reddy Vangoor; Prafful Agarwal; Manu Mathew; Arun Ramachandran; Swaminathan Sivaraman; Vasily Tarasov; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "Traditionally, file systems were implemented as part of operating systems kernels, which provide a limited set of tools and facilities to a programmer. As the complexity of file systems grew, many new file systems began being developed in user space. Low performance is considered the main disadvantage of user-space file systems but the extent of this problem has never been explored systematically. As a result, the topic of user-space file systems remains rather controversial: while some consider user-space file systems a “toy” not to be used in production, others develop full-fledged production file systems in user space. In this article, we analyze the design and implementation of a well-known user-space file system framework, FUSE, for Linux. We characterize its performance and resource utilization for a wide range of workloads. We present FUSE performance and also resource utilization with various mount and configuration options, using 45 different workloads that were generated using Filebench on two different hardware configurations. We instrumented FUSE to extract useful statistics and traces, which helped us analyze its performance bottlenecks and present our analysis results. Our experiments indicate that depending on the workload and hardware used, performance degradation (throughput) caused by FUSE can be completely imperceptible or as high as −83%, even when optimized; and latencies of FUSE file system operations can be increased from none to 4× when compared to Ext4. On the resource utilization side, FUSE can increase relative CPU utilization by up to 31% and underutilize disk bandwidth by as much as −80% compared to Ext4, though for many data-intensive workloads the impact was statistically indistinguishable. Our conclusion is that user-space file systems can indeed be used in production (non-“toy”) settings, but their applicability depends on the expected workloads.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2997431986",
    "type": "article"
  },
  {
    "title": "On Fault Tolerance, Locality, and Optimality in Locally Repairable Codes",
    "doi": "https://doi.org/10.1145/3381832",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Oleg Kolosov; Gala Yadgar; Matan Liram; Itzhak Tamo; Alexander Barg",
    "corresponding_authors": "",
    "abstract": "Erasure codes in large-scale storage systems allow recovery of data from a failed node. A recently developed class of codes, locally repairable codes (LRCs), offers tradeoffs between storage overhead and repair cost. LRCs facilitate efficient recovery scenarios by adding parity blocks to the system. However, these additional blocks may eventually increase the number of blocks that must be reconstructed. Existing LRCs differ in their use of the parity blocks, in their locality semantics, and in their parameter space. Thus, existing theoretical models cannot directly compare different LRCs to determine which code offers the best recovery performance, and at what cost. We perform the first systematic comparison of existing LRC approaches. We analyze Xorbas, Azure’s LRCs, and Optimal-LRCs in light of two new metrics: average degraded read cost and normalized repair cost. We show the tradeoff between these costs and the code’s fault tolerance, and that different approaches offer different choices in this tradeoff. Our experimental evaluation on a Ceph cluster further demonstrates the different effects of realistic system bottlenecks on the benefit from each LRC approach. Despite these differences, the normalized repair cost metric can reliably identify the LRC approach that would achieve the lowest repair cost in each setup.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3046192195",
    "type": "article"
  },
  {
    "title": "Repair Pipelining for Erasure-coded Storage: Algorithms and Evaluation",
    "doi": "https://doi.org/10.1145/3436890",
    "publication_date": "2021-05-28",
    "publication_year": 2021,
    "authors": "Xiaolu Li; Zuoru Yang; Jinhong Li; Runhui Li; Patrick P. C. Lee; Qun Huang; Yuchong Hu",
    "corresponding_authors": "",
    "abstract": "We propose repair pipelining , a technique that speeds up the repair performance in general erasure-coded storage. By carefully scheduling the repair of failed data in small-size units across storage nodes in a pipelined manner, repair pipelining reduces the single-block repair time to approximately the same as the normal read time for a single block in homogeneous environments. We further design different extensions of repair pipelining algorithms for heterogeneous environments and multi-block repair operations. We implement a repair pipelining prototype, called ECPipe , and integrate it as a middleware system into two versions of Hadoop Distributed File System (HDFS) (namely, HDFS-RAID and HDFS-3) as well as Quantcast File System. Experiments on a local testbed and Amazon EC2 show that repair pipelining significantly improves the performance of degraded reads and full-node recovery over existing repair techniques.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3171672558",
    "type": "article"
  },
  {
    "title": "Leveraging NVMe SSDs for Building a Fast, Cost-effective, LSM-tree-based KV Store",
    "doi": "https://doi.org/10.1145/3480963",
    "publication_date": "2021-10-15",
    "publication_year": 2021,
    "authors": "Cheng Li; Hao Chen; Chaoyi Ruan; Xiaosong Ma; Yinlong Xu",
    "corresponding_authors": "",
    "abstract": "Key-value (KV) stores support many crucial applications and services. They perform fast in-memory processing but are still often limited by I/O performance. The recent emergence of high-speed commodity non-volatile memory express solid-state drives (NVMe SSDs) has propelled new KV system designs that take advantage of their ultra-low latency and high bandwidth. Meanwhile, to switch to entirely new data layouts and scale up entire databases to high-end SSDs requires considerable investment. As a compromise, we propose SpanDB, an LSM-tree-based KV store that adapts the popular RocksDB system to utilize selective deployment of high-speed SSDs . SpanDB allows users to host the bulk of their data on cheaper and larger SSDs (and even hard disc drives with certain workloads), while relocating write-ahead logs (WAL) and the top levels of the LSM-tree to a much smaller and faster NVMe SSD. To better utilize this fast disk, SpanDB provides high-speed, parallel WAL writes via SPDK, and enables asynchronous request processing to mitigate inter-thread synchronization overhead and work efficiently with polling-based I/O. To ease the live data migration between fast and slow disks, we introduce TopFS, a stripped-down file system providing familiar file interface wrappers on top of SPDK I/O. Our evaluation shows that SpanDB simultaneously improves RocksDB's throughput by up to 8.8 \\times and reduces its latency by 9.5–58.3%. Compared with KVell, a system designed for high-end SSDs, SpanDB achieves 96–140% of its throughput, with a 2.3–21.6 \\times lower latency, at a cheaper storage configuration.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3205552847",
    "type": "article"
  },
  {
    "title": "<scp> Octopus <sup>+</sup> </scp> : An RDMA-Enabled Distributed Persistent Memory File System",
    "doi": "https://doi.org/10.1145/3448418",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Bohong Zhu; Youmin Chen; Qing Wang; Youyou Lu; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Non-volatile memory and remote direct memory access (RDMA) provide extremely high performance in storage and network hardware. However, existing distributed file systems strictly isolate file system and network layers, and the heavy layered software designs leave high-speed hardware under-exploited. In this article, we propose an RDMA-enabled distributed persistent memory file system, Octopus + , to redesign file system internal mechanisms by closely coupling non-volatile memory and RDMA features. For data operations, Octopus + directly accesses a shared persistent memory pool to reduce memory copying overhead, and actively fetches and pushes data all in clients to rebalance the load between the server and network. For metadata operations, Octopus + introduces self-identified remote procedure calls for immediate notification between file systems and networking, and an efficient distributed transaction mechanism for consistency. Octopus + is enabled with replication feature to provide better availability. Evaluations on Intel Optane DC Persistent Memory Modules show that Octopus + achieves nearly the raw bandwidth for large I/Os and orders of magnitude better performance than existing distributed file systems.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3194336529",
    "type": "article"
  },
  {
    "title": "IS-HBase: An In-Storage Computing Optimized HBase with I/O Offloading and Self-Adaptive Caching in Compute-Storage Disaggregated Infrastructure",
    "doi": "https://doi.org/10.1145/3488368",
    "publication_date": "2022-03-29",
    "publication_year": 2022,
    "authors": "Zhichao Cao; Huibing Dong; Yixun Wei; Shiyong Liu; David Hung-Chang Du",
    "corresponding_authors": "",
    "abstract": "Active storage devices and in-storage computing are proposed and developed in recent years to effectively reduce the amount of required data traffic and to improve the overall application performance. They are especially preferred in the compute-storage disaggregated infrastructure. In both techniques, a simple computing module is added to storage devices/servers such that some stored data can be processed in the storage devices/servers before being transmitted to application servers. This can reduce the required network bandwidth and offload certain computing requirements from application servers to storage devices/servers. However, several challenges exist when designing an in-storage computing- based architecture for applications. These include what computing functions need to be offloaded, how to design the protocol between in-storage modules and application servers, and how to deal with the caching issue in application servers. HBase is an important and widely used distributed Key-Value Store. It stores and indexes key-value pairs in large files in a storage system like HDFS. However, its performance especially read performance, is impacted by the heavy traffics between HBase RegionServers and storage servers in the compute-storage disaggregated infrastructure when the available network bandwidth is limited. We propose an I n- S torage-based HBase architecture, called IS-HBase , to improve the overall performance and to address the aforementioned challenges. First, IS-HBase executes a data pre-processing module ( I n- S torage S can N er, called ISSN ) for some read queries and returns the requested key-value pairs to RegionServers instead of returning data blocks in HFile. IS-HBase carries out compactions in storage servers to reduce the large amount of data being transmitted through the network and thus the compaction execution time is effectively reduced. Second, a set of new protocols is proposed to address the communication and coordination between HBase RegionServers at computing nodes and ISSNs at storage nodes. Third, a new self-adaptive caching scheme is proposed to better serve the read queries with fewer I/O operations and less network traffic. According to our experiments, the IS-HBase can reduce up to 97% network traffic for read queries and the throughput (queries per second) is significantly less affected by the fluctuation of available network bandwidth. The execution time of compaction in IS-HBase is only about 6.31% – 41.84% of the execution time of legacy HBase. In general, IS-HBase demonstrates the potential of adopting in-storage computing for other data-intensive distributed applications to significantly improve performance in compute-storage disaggregated infrastructure.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4220803117",
    "type": "article"
  },
  {
    "title": "gLSM: Using GPGPU to Accelerate Compactions in LSM-tree-based Key-value Stores",
    "doi": "https://doi.org/10.1145/3633782",
    "publication_date": "2023-11-24",
    "publication_year": 2023,
    "authors": "Hui Sun; Jinfeng Xu; Xiangxiang Jiang; Guanzhong Chen; Yinliang Yue; Xiao Qin",
    "corresponding_authors": "",
    "abstract": "Log-structured-merge tree or LSM-tree is a technological underpinning in key-value (KV) stores to support a wide range of performance-critical applications. By conducting data re-organization in the background by virtue of compaction operations, the KV stores have the potential to swiftly service write requests with sequential batched disk writes and read requests for KV items constantly sorted by the compaction. Compaction demands high I/O bandwidth and CPU speed to facilitate quality service to user read/write requests. With the emergence of high-speed SSDs, CPUs are increasingly becoming a performance bottleneck. To mitigate the bottleneck limiting the KV-store’s performance and that of the applications supported by the store, we propose a system - gLSM - to leverage GPGPU to remarkably accelerate the compaction operations. gLSM fully utilizes the parallelism and computational capability inside GPGPUs to improve the compaction performance. We design a driver framework to parallelize compaction operations handled between a pair of CPU and GPGPU. We employ data independence and GPGPU-orient radix-sorting algorithm to concurrently conduct compaction. A key-value separation method is devised to slash the transfer of data volume from CPU-side memory to the GPGPU counterpart. The results reveal that gLSM improves the throughput and compaction bandwidth by up to a factor of 2.9 and 26.0, respectively, compared with the four state-of-the-art KV stores. gLSM also reduces the write latency by 73.3%. gLSM exhibits a performance improvement by up to 45% compared against its variant where there are no KV separation and collaboration sort modules.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4388977666",
    "type": "article"
  },
  {
    "title": "Efficient disk replacement and data migration algorithms for large disk subsystems",
    "doi": "https://doi.org/10.1145/1084779.1084781",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Beomjoo Seo; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Random data placement, which is efficient and scalable for large-scale storage systems, has recently emerged as an alternative to traditional data striping. In this report, we study the disk replacement problem (DRP) to find a sequence of disk additions and removals for a storage system, while migrating the data and respecting the following constraints: (1) the data is initially balanced across the existing distributed disk configuration, (2) the data must again be balanced across the new configuration, and (3) the data migration cost must be minimized. In practice, migrating data from old disks to new devices is complicated by the fact that the total number of disks connected to the storage system is often limited by a fixed number of available slots and not all the old and new disks can be connected at the same time. This article presents solutions for both cases where the number of disk slots is either unconstrained or constrained.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2032249532",
    "type": "article"
  },
  {
    "title": "A buffer cache management scheme exploiting both temporal and spatial localities",
    "doi": "https://doi.org/10.1145/1242520.1242522",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Xiaoning Ding; Song Jiang; Feng Chen",
    "corresponding_authors": "",
    "abstract": "On-disk sequentiality of requested blocks, or their spatial locality, is critical to real disk performance where the throughput of access to sequentially-placed disk blocks can be an order of magnitude higher than that of access to randomly-placed blocks. Unfortunately, spatial locality of cached blocks is largely ignored, and only temporal locality is considered in current system buffer cache managements. Thus, disk performance for workloads without dominant sequential accesses can be seriously degraded. To address this problem, we propose a scheme called DULO ( DU al LO cality) which exploits both temporal and spatial localities in the buffer cache management. Leveraging the filtering effect of the buffer cache, DULO can influence the I/O request stream by making the requests passed to the disk more sequential, thus significantly increasing the effectiveness of I/O scheduling and prefetching for disk performance improvements. We have implemented a prototype of DULO in Linux 2.6.11. The implementation shows that DULO can significantly increases disk I/O throughput for real-world applications such as a Web server, TPC benchmark, file system benchmark, and scientific programs. It reduces their execution times by as much as 53%.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1997285359",
    "type": "article"
  },
  {
    "title": "CA-NFS",
    "doi": "https://doi.org/10.1145/1629080.1629085",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Alexandros Batsakis; Randal Burns; Arkady Kanevsky; James Lentini; Thomas Talpey",
    "corresponding_authors": "",
    "abstract": "We develop a holistic framework for adaptively scheduling asynchronous requests in distributed file systems. The system is holistic in that it manages all resources, including network bandwidth, server I/O, server CPU, and client and server memory utilization. It accelerates, defers, or cancels asynchronous requests in order to improve application-perceived performance directly. We employ congestion pricing via online auctions to coordinate the use of system resources by the file system clients so that they can detect shortages and adapt their resource usage. We implement our modifications in the Congestion-Aware Network File System (CA-NFS), an extension to the ubiquitous network file system (NFS). Our experimental result shows that CA-NFS results in a 20% improvement in execution times when compared with NFS for a variety of workloads.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2007513744",
    "type": "article"
  },
  {
    "title": "Extract and infer quickly",
    "doi": "https://doi.org/10.1145/1807060.1807063",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Jongmin Gim; Youjip Won",
    "corresponding_authors": "",
    "abstract": "The modern hard disk drive is a complex and complicated device. It consists of 2--4 heads, thousands of sectors per track, several hundred thousands of tracks, and tens of zones. The beginnings of adjacent tracks are placed with a certain angular offset. Sectors are placed on the tracks and accessed in some order. Angular offset and sector placement order vary widely subject to vendors and models. The success of an efficient file and storage subsystem design relies on the proper understanding of the underlying storage device characteristics. The characterization of hard disk drives has been a subject of intense research for more than a decade. The scale and complexity of state-of-the-art hard disk drive technology calls for a new way of extracting and analyzing the characteristics of the hard disk drive. In this work, we develop a novel disk characterization suite, DIG (Disk Geometry Analyzer), which allows us to rapidly extract and characterize the key performance metrics of the modern hard disk drive. Development of this tool is accompanied by thorough examination of four off-the-shelf hard disk drives. DIG consists of three key ingredients: O (1) a track boundary detection algorithm; O (log n ) a zone boundary detection algorithm; and hybrid sampling based seek time profiling. We particularly focus on addressing the scalability aspect of disk characterization. With DIG, we are able to extract key metrics of hard disk drives, for example, track sizes, zone information, sector geometry and so on, within 3--20 minutes. DIG allows us to determine the sector layout mechanism of the underlying hard disk drive, for example, hybrid serpentine, cylinder serpentine, and surface serpentine, and to a build complete sector map from LBN to the three dimensional space of (Cylinder, Head, Sector). Examining the hard disk drives with DIG, we made a number of important observations. In modern hard disk drives, head switch overhead is far greater than track switch overhead. It seems that hard disk drive vendors put greater emphasis on reducing the number of head switches for data access. Most disk vendors use surface serpentine, cylinder serpentine, or hybrid serpentine schemes in laying sectors on the platters. The legacy seek time model, which takes the form of a + b √ d leaves much to be desired for use in modern hard disk drives especially for short seeks (less than 5000 tracks). We compare the performance of the DIG against the existing state-of-the-art disk profiling algorithm. Compared to the existing state-of-the-art disk characterization algorithm, the DIG algorithm significantly decreases the time to extract comprehensive sector geometry information from 1920 minutes to 7 minutes and 1927 minutes to 180 minutes in best and worst case scenarios, respectively.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2074322015",
    "type": "article"
  },
  {
    "title": "Low-Complexity Implementation of RAID Based on Reed-Solomon Codes",
    "doi": "https://doi.org/10.1145/2700308",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Peter Trifonov",
    "corresponding_authors": "Peter Trifonov",
    "abstract": "Fast algorithms are proposed for encoding and reconstructing data in RAID based on Reed-Solomon codes. The proposed approach is based on the cyclotomic fast Fourier transform algorithm and enables one to significantly reduce the number of expensive Galois field multiplications required. The complexity of the obtained algorithms is much lower than those for existing MDS array codes. Software implementation of the proposed algorithms is discussed. The performance results show that the new algorithms provide substantially better performance compared with the standard algorithm.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2165339489",
    "type": "article"
  },
  {
    "title": "Understanding I/O Performance Behaviors of Cloud Storage from a Client’s Perspective",
    "doi": "https://doi.org/10.1145/3078838",
    "publication_date": "2017-05-22",
    "publication_year": 2017,
    "authors": "Binbing Hou; Feng Chen; Zhonghong Ou; Ren Wang; Michael P. Mesnier",
    "corresponding_authors": "",
    "abstract": "Cloud storage has gained increasing popularity in the past few years. In cloud storage, data is stored in the service provider’s data centers, and users access data via the network. For such a new storage model, our prior wisdom about conventional storage may not remain valid nor applicable to the emerging cloud storage. In this article, we present a comprehensive study to gain insight into the unique characteristics of cloud storage and optimize user experiences with cloud storage from a client’s perspective. Unlike prior measurement work that mostly aims to characterize cloud storage providers or specific client applications, we focus on analyzing the effects of various client-side factors on the user-experienced performance. Through extensive experiments and quantitative analysis, we have obtained several important findings. For example, we find that (1) a proper combination of parallelism and request size can achieve optimized bandwidths, (2) a client’s capabilities and geographical location play an important role in determining the end-to-end user-perceivable performance, and (3) the interference among mixed cloud storage requests may cause performance degradation. Based on our findings, we showcase a sampling- and inference-based method to determine a proper combination for different optimization goals. We further present a set of case studies on client-side chunking and parallelization for typical cloud-based applications. Our studies show that specific attention should be paid to fully exploiting the capabilities of clients and the great potential of cloud storage services.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2617094688",
    "type": "article"
  },
  {
    "title": "GDS-LC",
    "doi": "https://doi.org/10.1145/3149374",
    "publication_date": "2017-11-24",
    "publication_year": 2017,
    "authors": "Binbing Hou; Feng Chen",
    "corresponding_authors": "",
    "abstract": "Successfully integrating cloud storage as a primary storage layer in the I/O stack is highly challenging. This is essentially due to two inherent critical issues: the high and variant cloud I/O latency and the per-I/O pricing model of cloud storage. To minimize the associated latency and monetary cost with cloud I/Os, caching is a crucial technology, as it directly influences how frequently the client has to communicate with the cloud. Unfortunately, current cloud caching schemes are mostly designed to optimize miss reduction as the sole objective and only focus on improving system performance while ignoring the fact that various cache misses could have completely distinct effects in terms of latency and monetary cost. In this article, we present a cost-aware caching scheme, called GDS-LC , which is highly optimized for cloud storage caching. Different from traditional caching schemes that merely focus on improving cache hit ratios and the classic cost-aware schemes that can only achieve a single optimization target, GDS-LC offers a comprehensive cache design by considering not only the access locality but also the object size, associated latency, and price, aiming at enhancing the user experience with cloud storage from two aspects: access latency and monetary cost. To achieve this, GDS-LC virtually partitions the cache space into two regions: a high-priority latency-aware region and a low-priority price-aware region. Each region is managed by a cost-aware caching scheme, which is based on GreedyDual-Size (GDS) and designed for a cloud storage scenario by adopting clean-dirty differentiation and latency normalization. The GDS-LC framework is highly flexible, and we present a further enhanced algorithm, called GDS-LCF , by incorporating access frequency in caching decisions. We have built a prototype to emulate a typical cloud client cache and evaluate GDS-LC and GDS-LCF with Amazon Simple Storage Services (S3) in three different scenarios: local cloud, Internet cloud, and heterogeneous cloud. Our experimental results show that our caching schemes can effectively achieve both optimization goals: low access latency and low monetary cost. It is our hope that this work can inspire the community to reconsider the cache design in the cloud environment, especially for the purpose of integrating cloud storage into the current storage stack as a primary layer.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2769030905",
    "type": "article"
  },
  {
    "title": "Management of Next-Generation NAND Flash to Achieve Enterprise-Level Endurance and Latency Targets",
    "doi": "https://doi.org/10.1145/3241060",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Roman Pletka; Ioannis Koltsidas; Nikolas Ioannou; Saša Tomić; Nikolaos Papandreou; Thomas Parnell; Haralampos Pozidis; Aaron Fry; Tim Fisher",
    "corresponding_authors": "",
    "abstract": "Despite its widespread use in consumer devices and enterprise storage systems, NAND flash faces a growing number of challenges. While technology advances have helped to increase the storage density and reduce costs, they have also led to reduced endurance and larger block variations, which cannot be compensated solely by stronger ECC or read-retry schemes but have to be addressed holistically. Our goal is to enable low-cost NAND flash in enterprise storage for cost efficiency. We present novel flash-management approaches that reduce write amplification, achieve better wear leveling, and enhance endurance without sacrificing performance. We introduce block calibration, a technique to determine optimal read-threshold voltage levels that minimize error rates, and novel garbage-collection as well as data-placement schemes that alleviate the effects of block health variability and show how these techniques complement one another and thereby achieve enterprise storage requirements. By combining the proposed schemes, we improve endurance by up to 15× compared to the baseline endurance of NAND flash without using a stronger ECC scheme. The flash-management algorithms presented herein were designed and implemented in simulators, hardware test platforms, and eventually in the flash controllers of production enterprise all-flash arrays. Their effectiveness has been validated across thousands of customer deployments since 2015.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2903349259",
    "type": "article"
  },
  {
    "title": "On the Trade-Offs among Performance, Energy, and Endurance in a Versatile Hybrid Drive",
    "doi": "https://doi.org/10.1145/2700312",
    "publication_date": "2015-07-24",
    "publication_year": 2015,
    "authors": "Zhichao Li; Ming Chen; Amanpreet Mukker; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "There are trade-offs among performance, energy, and device endurance for storage systems. Designs optimized for one dimension or workload often suffer in another. Therefore, it is important to study the trade-offs to enable adaptation to workloads and dimensions. As Flash SSD has emerged, hybrid drives have been studied more closely. However, hybrids are mainly designed for high throughput, efficient energy consumption, or improving endurance—leaving quantitative study on the trade-offs unexplored. Past endurance studies also lack a concrete model to help study the trade-offs. Last, previous designs are often based on inflexible policies that cannot adapt easily to changing conditions. We designed and developed GreenDM , a versatile hybrid drive that combines Flash-based SSDs with traditional HDDs. The SSD can be used as cache or as primary storage for hot data. We present our endurance model together with GreenDM to study these trade-offs. GreenDM presents a block interface and requires no modifications to existing software. GreenDM offers tunable parameters to enable the system to adapt to many workloads. We have designed, developed, and carefully evaluated GreenDM with a variety of workloads using commodity SSD and HDD drives. We demonstrate the importance of versatility to enable adaptation to various workloads and dimensions.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2251023975",
    "type": "article"
  },
  {
    "title": "PBS",
    "doi": "https://doi.org/10.1145/3365839",
    "publication_date": "2020-02-28",
    "publication_year": 2020,
    "authors": "Yiming Zhang; Huiba Li; Shengyun Liu; Jiawei Xu; Guangtao Xue",
    "corresponding_authors": "",
    "abstract": "Block storage provides virtual disks that can be mounted by virtual machines (VMs). Although erasure coding (EC) has been widely used in many cloud storage systems for its high efficiency and durability, current EC schemes cannot provide high-performance block storage for the cloud. This is because they introduce significant overhead to small write operations (which perform partial write to an entire EC group), whereas cloud-oblivious applications running on VMs are often small-write-intensive. We identify the root cause for the poor performance of partial writes in state-of-the-art EC schemes: for each partial write, they have to perform a time-consuming write-after-read operation that reads the current value of the data and then computes and writes the parity delta, which will be used to “patch” the parity in journal replay. In this article, we present a speculative partial write scheme (called P ARI X) that supports fast small writes in erasure-coded storage systems. We transform the original formula of parity calculation to use the data deltas (between the current/original data values), instead of the parity deltas, to calculate the parities in journal replay. For each partial write, this allows P ARI X to speculatively log only the new value of the data without reading its original value. For a series of n partial writes to the same data, P ARI X performs pure write (instead of write-after-read) for the last n -1 ones while only introducing a small penalty of an extra network round-trip time to the first one. Based on P ARI X, we design and implement P ARI X Block Storage (PBS), an efficient block storage system that provides high-performance virtual disk service for VMs running cloud-oblivious applications. PBS not only supports fast partial writes but also realizes efficient full writes, background journal replay, and fast failure recovery with strong consistency guarantees. Both microbenchmarks and trace-driven evaluation show that PBS provides efficient block storage and outperforms state-of-the-art EC-based systems by orders of magnitude.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3009254104",
    "type": "article"
  },
  {
    "title": "Design Tradeoffs of SSDs",
    "doi": "https://doi.org/10.1145/2644818",
    "publication_date": "2015-03-24",
    "publication_year": 2015,
    "authors": "Seokhei Cho; Chang-Hyun Park; Youjip Won; Sooyong Kang; Jaehyuk Cha; Sungroh Yoon; Jongmoo Choi",
    "corresponding_authors": "",
    "abstract": "In this work, we studied the energy consumption characteristics of various SSD design parameters. We developed an accurate energy consumption model for SSDs that computes aggregate, as well as component-specific, energy consumption of SSDs in sub-msec time scale. In our study, we used five different FTLs (page mapping, DFTL, block mapping, and two different hybrid mappings) and four different channel configurations (two, four, eight, and 16 channels) under seven different workloads (from large-scale enterprise systems to small-scale desktop applications) in a combinatorial manner. For each combination of the aforementioned parameters, we examined the energy consumption for individual hardware components of an SSD (microcontroller, DRAM, NAND flash, and host interface). The following are some of our findings. First, DFTL is the most energy-efficient address-mapping scheme among the five FTLs we tested due to its good write amplification and small DRAM footprint. Second, a significant fraction of energy is being consumed by idle flash chips waiting for the completion of NAND operations in the other channels. FTL should be designed to fully exploit the internal parallelism so that energy consumption by idle chips is minimized. Third, as a means to increase the internal parallelism, increasing way parallelism (the number of flash chips in a channel) is more effective than increasing channel parallelism in terms of peak energy consumption, performance, and hardware complexity. Fourth, in designing high-performance and energy-efficient SSDs, channel switching delay, way switching delay, and page write latency need to be incorporated in an integrated manner to determine the optimal configuration of internal parallelism.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1981672155",
    "type": "article"
  },
  {
    "title": "Rebuttal to “Beyond MTTDL: A Closed-Form RAID-6 Reliability Equation”",
    "doi": "https://doi.org/10.1145/2700311",
    "publication_date": "2015-03-20",
    "publication_year": 2015,
    "authors": "Ilias Iliadis; Vinodh Venkatesan",
    "corresponding_authors": "",
    "abstract": "A recent article on the reliability of RAID-6 storage systems overlooks certain relevant prior work published in the past 20 years and concludes that the widely used mean time to data loss (MTTDL) metric does not provide accurate results. In this note, we refute this position by invoking uncited relevant prior work and demonstrating that the MTTDL remains a useful metric.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2069167364",
    "type": "article"
  },
  {
    "title": "NANDFlashSim",
    "doi": "https://doi.org/10.1145/2700310",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Myoungsoo Jung; Wonil Choi; Shuwen Gao; Ellis Wilson; David Donofrio; John Shalf; Mahmut Kandemir",
    "corresponding_authors": "",
    "abstract": "As the popularity of NAND flash expands in arenas from embedded systems to high-performance computing, a high-fidelity understanding of its specific properties becomes increasingly important. Further, with the increasing trend toward multiple-die, multiple-plane architectures and high-speed interfaces, flash memory systems are expected to continue to scale and cheapen, resulting in their broader proliferation. However, when designing NAND-based devices, making decisions about the optimal system configuration is nontrivial, because flash is sensitive to a number of parameters and suffers from inherent latency variations, and no available tools suffice for studying these nuances. The parameters include the architectures, such as multidie and multiplane, diverse node technologies, bit densities, and cell reliabilities. Therefore, we introduce NANDFlashSim, a high-fidelity, latency-variation-aware, and highly configurable NAND-flash simulator, which implements a detailed timing model for 16 state-of-the-art NAND operations. Using NANDFlashSim, we notably discover the following. First, regardless of the operation, reads fail to leverage internal parallelism. Second, MLC provides lower I/O bus contention than SLC, but contention becomes a serious problem as the number of dies increases. Third, many-die architectures outperform many-plane architectures for disk-friendly workloads. Finally, employing a high-performance I/O bus or an increased page size does not enhance energy savings. Our simulator is available at http://nfs.camelab.org.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2322680505",
    "type": "article"
  },
  {
    "title": "SWANS",
    "doi": "https://doi.org/10.1145/2756555",
    "publication_date": "2016-04-29",
    "publication_year": 2016,
    "authors": "Wei Wang; Tao Xie; Abhinav Sharma",
    "corresponding_authors": "",
    "abstract": "NAND flash memory–based solid state disks (SSDs) have been widely used in enterprise servers. However, flash memory has limited write endurance, as a block becomes unreliable after a finite number of program/erase cycles. Existing wear-leveling techniques are essentially intradisk data distribution schemes, as they can only even wear out across the flash medium within a single SSD. When multiple SSDs are organized in an array manner in server applications, an interdisk wear-leveling technique, which can ensure a uniform wear-out distribution across SSDs, is much needed. In this article, we propose a novel SSD-array level wear-leveling strategy called SWANS ( &lt;u&gt;S&lt;/u&gt; moothing &lt;u&gt;W&lt;/u&gt; ear &lt;u&gt;A&lt;/u&gt; cross &lt;u&gt;N&lt;/u&gt; &lt;u&gt;S&lt;/u&gt; SDs) for an SSD array structured in a RAID-0 format, which is frequently used in server applications. SWANS dynamically monitors and balances write distributions across SSDs in an intelligent way. Further, to evaluate its effectiveness, we build an SSD array simulator on top of a validated single SSD simulator. Next, SWANS is implemented in its array controller. Comprehensive experiments with real-world traces show that SWANS decreases the standard deviation of writes across SSDs on average by 16.7x. The gap in the total bytes written between the most written SSD and the least written SSD in an 8-SSD array shrinks at least 1.3x.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2347124870",
    "type": "article"
  },
  {
    "title": "TH-DPMS",
    "doi": "https://doi.org/10.1145/3412852",
    "publication_date": "2020-10-01",
    "publication_year": 2020,
    "authors": "Jiwu Shu; Youmin Chen; Qing Wang; Bohong Zhu; Junru Li; Youyou Lu",
    "corresponding_authors": "",
    "abstract": "The rapidly increasing data in recent years requires the datacenter infrastructure to store and process data with extremely high throughput and low latency. Fortunately, persistent memory (PM) and RDMA technologies bring new opportunities towards this goal. Both of them are capable of delivering more than 10 GB/s of bandwidth and sub-microsecond latency. However, our past experiences and recent studies show that it is non-trivial to build an efficient and distributed storage system with such new hardware. In this article, we design and implement TH-DPMS (&lt;underline&gt;T&lt;/underline&gt;sing&lt;underline&gt;H&lt;/underline&gt;ua &lt;underline&gt;D&lt;/underline&gt;istributed &lt;underline&gt;P&lt;/underline&gt;ersistent &lt;underline&gt;M&lt;/underline&gt;emory &lt;underline&gt;S&lt;/underline&gt;ystem) based on persistent memory and RDMA, which unifies the memory, file system, and key-value interface in a single system. TH-DPMS is designed based on a unified distributed persistent memory abstract, pDSM. pDSM acts as a generic layer to connect the PMs of different storage nodes via high-speed RDMA network and organizes them into a global shared address space. It provides the fundamental functionalities, including global address management, space management, fault tolerance, and crash consistency guarantees. Applications are enabled to access pDSM with a group of flexible and easy-to-use APIs by using either raw read/write interfaces or the transactional ones with ACID guarantees. Based on pDSM, we implement a distributed file system and a key-value store named pDFS and pDKVS, respectively. Together, they uphold TH-DPMS with high-performance, low-latency, and fault-tolerant data storage. We evaluate TH-DPMS with both micro-benchmarks and real-world memory-intensive workloads. Experimental results show that TH-DPMS is capable of delivering an aggregated bandwidth of 120 GB/s with 6 nodes. When processing memory-intensive workloads such as YCSB and Graph500, TH-DPMS improves the performance by one order of magnitude compared to existing systems and keeps consistent high efficiency when the workload size grows to multiple terabytes.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3089808481",
    "type": "article"
  },
  {
    "title": "Multi-objective Optimization of Data Placement in a Storage-as-a-Service Federated Cloud",
    "doi": "https://doi.org/10.1145/3452741",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Amina Chikhaoui; Laurent Lemarchand; Kamel Boukhalfa; Jalil Boukhobza",
    "corresponding_authors": "",
    "abstract": "Cloud federation enables service providers to collaborate to provide better services to customers. For cloud storage services, optimizing customer object placement for a member of a federation is a real challenge. Storage, migration, and latency costs need to be considered. These costs are contradictory in some cases. In this article, we modeled object placement as a multi-objective optimization problem. The proposed model takes into account parameters related to the local infrastructure, the federated environment, customer workloads, and their SLAs. For resolving this problem, we propose CDP-NSGAII IR , a Constraint Data Placement matheuristic based on NSGAII with Injection and Repair functions. The injection function aims to enhance the solutions’ quality. It consists to calculate some solutions using an exact method then inject them into the initial population of NSGAII. The repair function ensures that the solutions obey the problem constraints and so prevents from exploring large sets of unfeasible solutions. It reduces drastically the execution time of NSGAII. Experimental results show that the injection function improves the HV of NSGAII and the exact method by up to 94% and 60%, respectively, while the repair function reduces the execution time by an average of 68%.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3195264173",
    "type": "article"
  },
  {
    "title": "Exploration and Exploitation for Buffer-Controlled HDD-Writes for SSD-HDD Hybrid Storage Server",
    "doi": "https://doi.org/10.1145/3465410",
    "publication_date": "2022-01-29",
    "publication_year": 2022,
    "authors": "Shucheng Wang; Ziyi Lu; Qiang Cao; Hong Jiang; Jie Yao; Yuanyuan Dong; Puyuan Yang; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "Hybrid storage servers combining solid-state drives (SSDs) and hard-drive disks (HDDs) provide cost-effectiveness and μs-level responsiveness for applications. However, observations from cloud storage system Pangu manifest that HDDs are often underutilized while SSDs are overused, especially under intensive writes. It leads to fast wear-out and high tail latency to SSDs. On the other hand, our experimental study reveals that a series of sequential and continuous writes to HDDs exhibit a periodic, staircase-shaped pattern of write latency, i.e., low (e.g., 35 μs), middle (e.g., 55 μs), and high latency (e.g., 12 ms), resulting from buffered writes within HDD’s controller. It inspires us to explore and exploit the potential μs-level IO delay of HDDs to absorb excessive SSD writes without performance degradation. We first build an HDD writing model for describing the staircase behavior and design a profiling process to initialize and dynamically recalibrate the model parameters. Then, we propose a Buffer-Controlled Write approach (BCW) to proactively control buffered writes so that low- and mid-latency periods are scheduled with application data and high-latency periods are filled with padded data. Leveraging BCW, we design a mixed IO scheduler (MIOS) to adaptively steer incoming data to SSDs and HDDs. A multi-HDD scheduling is further designed to minimize HDD-write latency. We perform extensive evaluations under production workloads and benchmarks. The results show that MIOS removes up to 93% amount of data written to SSDs, reduces average and 99 th -percentile latencies of the hybrid server by 65% and 85%, respectively.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4210267052",
    "type": "article"
  },
  {
    "title": "RAIL: Predictable, Low Tail Latency for NVMe Flash",
    "doi": "https://doi.org/10.1145/3465406",
    "publication_date": "2022-01-29",
    "publication_year": 2022,
    "authors": "Heiner Litz; G Javier; Ana Klimovic; Christos Kozyrakis",
    "corresponding_authors": "",
    "abstract": "Flash-based storage is replacing disk for an increasing number of data center applications, providing orders of magnitude higher throughput and lower average latency. However, applications also require predictable storage latency. Existing Flash devices fail to provide low tail read latency in the presence of write operations. We propose two novel techniques to address SSD read tail latency, including Redundant Array of Independent LUNs (RAIL) which avoids serialization of reads behind user writes as well as latency-aware hot-cold separation (HC) which improves write throughput while maintaining low tail latency. RAIL leverages the internal parallelism of modern Flash devices and allocates data and parity pages to avoid reads getting stuck behind writes. We implement RAIL in the Linux Kernel as part of the LightNVM Flash translation layer and show that it can reduce read tail latency by 7× at the 99.99th percentile, while reducing relative bandwidth by only 33%.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4210548206",
    "type": "article"
  },
  {
    "title": "An In-depth Comparative Analysis of Cloud Block Storage Workloads: Findings and Implications",
    "doi": "https://doi.org/10.1145/3572779",
    "publication_date": "2022-11-22",
    "publication_year": 2022,
    "authors": "Jinhong Li; Qiuping Wang; Patrick P. C. Lee; Chao Shi",
    "corresponding_authors": "",
    "abstract": "Cloud block storage systems support diverse types of applications in modern cloud services. Characterizing their input/output (I/O) activities is critical for guiding better system designs and optimizations. In this article, we present an in-depth comparative analysis of production cloud block storage workloads through the block-level I/O traces of billions of I/O requests collected from two production systems, Alibaba Cloud and Tencent Cloud Block Storage. We study their characteristics of load intensities, spatial patterns, and temporal patterns. We also compare the cloud block storage workloads with the notable public block-level I/O workloads from the enterprise data centers at Microsoft Research Cambridge, and we identify the commonalities and differences of the three sources of traces. To this end, we provide 6 findings through the high-level analysis and 16 findings through the detailed analysis on load intensity, spatial patterns, and temporal patterns. We discuss the implications of our findings on load balancing, cache efficiency, and storage cluster management in cloud block storage systems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4309698607",
    "type": "article"
  },
  {
    "title": "TPFS: A High-Performance Tiered File System for Persistent Memories and Disks",
    "doi": "https://doi.org/10.1145/3580280",
    "publication_date": "2023-01-14",
    "publication_year": 2023,
    "authors": "Shengan Zheng; Morteza Hoseinzadeh; Steven Swanson; Linpeng Huang",
    "corresponding_authors": "",
    "abstract": "Emerging fast, byte-addressable persistent memory (PM) promises substantial storage performance gains compared with traditional disks. We present TPFS, a tiered file system that combines PM and slow disks to create a storage system with near-PM performance and large capacity. TPFS steers incoming file input/output (I/O) to PM, dynamic random access memory (DRAM), or disk depending on the synchronicity, write size, and read frequency. TPFS profiles the application’s access stream online to predict the behavior of file access. In the background, TPFS estimates the “temperature” of file data and migrates the write-cold and read-hot file data from PM to disks. To fully utilize disk bandwidth, TPFS coalesces data blocks into large, sequential writes. Experimental results show that with a small amount of PM and a large solid-state drive (SSD), TPFS achieves up to 7.3× and 7.9× throughput improvement compared with EXT4 and XFS running on an SSD alone, respectively. As the amount of PM grows, TPFS’s performance improves until it matches the performance of a PM-only file system.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4315977519",
    "type": "article"
  },
  {
    "title": "Constructing collaborative desktop storage caches for large scientific datasets",
    "doi": "https://doi.org/10.1145/1168910.1168911",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Sudharshan S. Vazhkudai; Xiaosong Ma; Vincent W. Freeh; Jonathan W. Strickland; Nandan Tammineedi; Tyler A. Simon; Stephen L. Scott",
    "corresponding_authors": "",
    "abstract": "High-end computing is suffering a data deluge from experiments, simulations, and apparatus that creates overwhelming application dataset sizes. This has led to the proliferation of high-end mass storage systems, storage area clusters, and data centers. These storage facilities offer a large range of choices in terms of capacity and access rate, as well as strong data availability and consistency support. However, for most end-users, the “last mile” in their analysis pipeline often requires data processing and visualization at local computers, typically local desktop workstations. End-user workstations---despite having more processing power than ever before---are ill-equipped to cope with such data demands due to insufficient secondary storage space and I/O rates. Meanwhile, a large portion of desktop storage is unused.We propose the FreeLoader framework, which aggregates unused desktop storage space and I/O bandwidth into a shared cache/scratch space, for hosting large, immutable datasets and exploiting data access locality. This article presents the FreeLoader architecture, component design, and performance results based on our proof-of-concept prototype. Its architecture comprises contributing benefactor nodes, steered by a management layer, providing services such as data integrity, high performance, load balancing, and impact control. Our experiments show that FreeLoader is an appealing low-cost solution to storing massive datasets by delivering higher data access rates than traditional storage facilities, namely, local or remote shared file systems, storage systems, and Internet data repositories. In particular, we present novel data striping techniques that allow FreeLoader to efficiently aggregate a workstation's network communication bandwidth and local I/O bandwidth. In addition, the performance impact on the native workload of donor machines is small and can be effectively controlled. Further, we show that security features such as data encryptions and integrity checks can be easily added as filters for interested clients. Finally, we demonstrate how legacy applications can use the FreeLoader API to store and retrieve datasets.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2033422105",
    "type": "article"
  },
  {
    "title": "Optimal multistream sequential prefetching in a shared cache",
    "doi": "https://doi.org/10.1145/1288783.1288789",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Binny S. Gill; Luis Angel D. Bathen",
    "corresponding_authors": "",
    "abstract": "Prefetching is a widely used technique in modern data storage systems. We study the most widely used class of prefetching algorithms known as sequential prefetching . There are two problems that plague the state-of-the-art sequential prefetching algorithms: (i) cache pollution , which occurs when prefetched data replaces more useful prefetched or demand-paged data, and (ii) prefetch wastage , which happens when prefetched data is evicted from the cache before it can be used. A sequential prefetching algorithm can have a fixed or adaptive degree of prefetch and can be either synchronous (when it can prefetch only on a miss) or asynchronous (when it can also prefetch on a hit). To capture these distinctions we define four classes of prefetching algorithms: fixed synchronous (FS), fixed asynchronous (FA), adaptive synchronous (AS), and adaptive asynchronous (AsynchA). We find that the relatively unexplored class of AsynchA algorithms is in fact the most promising for sequential prefetching. We provide a first formal analysis of the criteria necessary for optimal throughput when using an AsynchA algorithm in a cache shared by multiple steady sequential streams. We then provide a simple implementation called AMP (adaptive multistream prefetching) which adapts accordingly, leading to near-optimal performance for any kind of sequential workload and cache size. Our experimental setup consisted of an IBM xSeries 345 dual processor server running Linux using five SCSI disks. We observe that AMP convincingly outperforms all the contending members of the FA, FS, and AS classes for any number of streams and over all cache sizes. As anecdotal evidence, in an experiment with 100 concurrent sequential streams and varying cache sizes, AMP surpasses the FA, FS, and AS algorithms by 29--172%, 12--24%, and 21--210%, respectively, while outperforming OBL by a factor of 8. Even for complex workloads like SPC1-Read, AMP is consistently the best-performing algorithm. For the SPC2 video-on-demand workload, AMP can sustain at least 25% more streams than the next best algorithm. Furthermore, for a workload consisting of short sequences, where optimality is more elusive, AMP is able to outperform all the other contenders in overall performance. Finally, we implemented AMP in the state-of-the-art enterprise storage system, the IBM system storage DS8000 series. We demonstrated that AMP dramatically improves performance for common sequential and batch processing workloads and delivers up to a twofold increase in the sequential read capacity.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1970541850",
    "type": "article"
  },
  {
    "title": "Dynamic data reallocation in disk arrays",
    "doi": "https://doi.org/10.1145/1227835.1227837",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Ron Arnan; Eitan Bachmat; Tao Kai Lam; Ruben Michel",
    "corresponding_authors": "",
    "abstract": "We present an application which optimizes the configuration of data in a disk array. The application relies on very little input data which is readily available from many storage devices. We validate the application in a controlled experiment and in a production environment. In both cases, the reconfiguration process led to a 20--30% improvement in response time.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2053015279",
    "type": "article"
  },
  {
    "title": "Umbrella file system",
    "doi": "https://doi.org/10.1145/1502777.1502780",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "John A. Garrison; A. L. Narasimha Reddy",
    "corresponding_authors": "",
    "abstract": "With the advent of and recent developments in Flash storage, device characteristic diversity is becoming both more prevalent and more distinct. In this article, we describe the Umbrella File System (UmbrellaFS), a stackable file system designed to provide flexibility in matching diversity of file access characteristics to diversity of device characteristics through a user or system administrator specified policy. We present the design and results from a prototype implementation of UmbrellaFS on both Linux 2.4 and 2.6. The results show that UmbrellaFS has little overhead for most file system operations while providing an ability better to utilize the differences in Flash and traditional hard drives. With appropriate use of rules, we have shown improvements of up to 44% in certain situations.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2011783580",
    "type": "article"
  },
  {
    "title": "Rethinking FTP",
    "doi": "https://doi.org/10.1145/1480439.1480442",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Stergios V. Anastasiadis; Rajiv Wickremesinghe; Jeffrey S. Chase",
    "corresponding_authors": "",
    "abstract": "Whole-file transfer is a basic primitive for Internet content dissemination. Content servers are increasingly limited by disk arm movement, given the rapid growth in disk density, disk transfer rates, server network bandwidth, and content size. Individual file transfers are sequential, but the block access sequence on a content server is effectively random when many slow clients access large files concurrently. Although larger blocks can help improve disk throughput, buffering requirements increase linearly with block size. This article explores a novel block reordering technique that can reduce server disk traffic significantly when large content files are shared. The idea is to transfer blocks to each client in any order that is convenient for the server. The server sends blocks to each client opportunistically in order to maximize the advantage from the disk reads it issues to serve other clients accessing the same file. We first illustrate the motivation and potential impact of aggressive block reordering using simple analytical models. Then we describe a file transfer system using a simple block reordering algorithm, called Circus. Experimental results with the Circus prototype show that it can improve server throughput by a factor of two or more in workloads with strong file access locality.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1984690739",
    "type": "article"
  },
  {
    "title": "Hybris",
    "doi": "https://doi.org/10.1145/3119896",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Paolo Viotti; Dan Dobre; Marko Vukolić",
    "corresponding_authors": "",
    "abstract": "Besides well-known benefits, commodity cloud storage also raises concerns that include security, reliability, and consistency. We present Hybris key-value store, the first robust hybrid cloud storage system, aiming at addressing these concerns leveraging both private and public cloud resources. Hybris robustly replicates metadata on trusted private premises (private cloud), separately from data, which are dispersed (using replication or erasure coding) across multiple untrusted public clouds. Hybris maintains metadata stored on private premises at the order of few dozens of bytes per key, avoiding the scalability bottleneck at the private cloud. In turn, the hybrid design allows Hybris to efficiently and robustly tolerate cloud outages but also potential malice in clouds without overhead. Namely, to tolerate up to f malicious clouds, in the common case of the Hybris variant with data replication, writes replicate data across f +1 clouds, whereas reads involve a single cloud. In the worst case, only up to f additional clouds are used. This is considerably better than earlier multi-cloud storage systems that required costly 3 f +1 clouds to mask f potentially malicious clouds. Finally, Hybris leverages strong metadata consistency to guarantee to Hybris applications strong data consistency without any modifications to the eventually consistent public clouds. We implemented Hybris in Java and evaluated it using a series of micro and macro-benchmarks. Our results show that Hybris significantly outperforms comparable multi-cloud storage systems and approaches the performance of bare-bone commodity public cloud storage.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2758391214",
    "type": "article"
  },
  {
    "title": "Modeling Drive-Managed SMR Performance",
    "doi": "https://doi.org/10.1145/3139242",
    "publication_date": "2017-11-30",
    "publication_year": 2017,
    "authors": "Mansour Shafaei; Mohammad Hossein Hajkazemi; Peter Desnoyers; Abutalib Aghayev",
    "corresponding_authors": "",
    "abstract": "Accurately modeling drive-managed Shingled Magnetic Recording (SMR) disks is a challenge, requiring an array of approaches including both existing disk modeling techniques as well as new techniques for inferring internal translation layer algorithms. In this work, we present the first predictive simulation model of a generally available drive-managed SMR disk. Despite the use of unknown proprietary algorithms in this device, our model that is derived from external measurements is able to predict mean latency within a few percent, and with an Root Mean Square (RMS) cumulative latency error of 25% or less for most workloads tested. These variations, although not small, are in most cases less than three times the drive-to-drive variation seen among seemingly identical drives.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2771690271",
    "type": "article"
  },
  {
    "title": "Persisting RB-Tree into NVM in a Consistency Perspective",
    "doi": "https://doi.org/10.1145/3177915",
    "publication_date": "2018-02-26",
    "publication_year": 2018,
    "authors": "Chundong Wang; Qingsong Wei; Lingkun Wu; Sibo Wang; Cheng Chen; Xiaokui Xiao; Jun Yang; Mingdi Xue; Yechao Yang",
    "corresponding_authors": "",
    "abstract": "Byte-addressable non-volatile memory (NVM) is going to reshape conventional computer systems. With advantages of low latency, byte-addressability, and non-volatility, NVM can be directly put on the memory bus to replace DRAM. As a result, both system and application softwares have to be adjusted to perceive the fact that the persistent layer moves up to the memory. However, most of the current in-memory data structures will be problematic with consistency issues if not well tuned with NVM. This article places emphasis on an important in-memory structure that is widely used in computer systems, i.e., the Red/Black-tree (RB-tree). Since it has a long and complicated update process, the RB-tree is prone to inconsistency problems with NVM. This article presents an NVM-compatible consistent RB-tree with a new technique named cascade-versioning . The proposed RB-tree (i) is all-time consistent and scalable and (ii) needs no recovery procedure after system crashes. Experiment results show that the RB-tree for NVM not only achieves the aim of consistency with insignificant spatial overhead but also yields comparable performance to an ordinary volatile RB-tree.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2793139821",
    "type": "article"
  },
  {
    "title": "An Attention-augmented Deep Architecture for Hard Drive Status Monitoring in Large-scale Storage Systems",
    "doi": "https://doi.org/10.1145/3340290",
    "publication_date": "2019-08-13",
    "publication_year": 2019,
    "authors": "Ji Wang; Weidong Bao; Lei Zheng; Xiaomin Zhu; Philip S. Yu",
    "corresponding_authors": "",
    "abstract": "Data centers equipped with large-scale storage systems are critical infrastructures in the era of big data. The enormous amount of hard drives in storage systems magnify the failure probability, which may cause tremendous loss for both data service users and providers. Despite a set of reactive fault-tolerant measures such as RAID, it is still a tough issue to enhance the reliability of large-scale storage systems. Proactive prediction is an effective method to avoid possible hard-drive failures in advance. A series of models based on the SMART statistics have been proposed to predict impending hard-drive failures. Nonetheless, there remain some serious yet unsolved challenges like the lack of explainability of prediction results. To address these issues, we carefully analyze a dataset collected from a real-world large-scale storage system and then design an attention-augmented deep architecture for hard-drive health status assessment and failure prediction. The deep architecture, composed of a feature integration layer, a temporal dependency extraction layer, an attention layer, and a classification layer, cannot only monitor the status of hard drives but also assist in failure cause diagnoses. The experiments based on real-world datasets show that the proposed deep architecture is able to assess the hard-drive status and predict the impending failures accurately. In addition, the experimental results demonstrate that the attention-augmented deep architecture can reveal the degradation progression of hard drives automatically and assist administrators in tracing the cause of hard drive failures.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2968044411",
    "type": "article"
  },
  {
    "title": "Classifying Data to Reduce Long-Term Data Movement in Shingled Write Disks",
    "doi": "https://doi.org/10.1145/2851505",
    "publication_date": "2016-02-24",
    "publication_year": 2016,
    "authors": "Stephanie Jones; Ahmed Amer; Ethan L. Miller; Darrell D. E. Long; Rekha Pitchumani; Christina R. Strong",
    "corresponding_authors": "",
    "abstract": "Shingled magnetic recording (SMR) is a means of increasing the density of hard drives that brings a new set of challenges. Due to the nature of SMR disks, updating in place is not an option. Holes left by invalidated data can only be filled if the entire band is reclaimed, and a poor band compaction algorithm could result in spending a lot of time moving blocks over the lifetime of the device. We propose using write frequency to separate blocks to reduce data movement and develop a band compaction algorithm that implements this heuristic. We demonstrate how our algorithm results in improved data management, resulting in an up to 45% reduction in required data movements when compared to naive approaches to band management.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2277341711",
    "type": "article"
  },
  {
    "title": "Does RAID Improve Lifetime of SSD Arrays?",
    "doi": "https://doi.org/10.1145/2764915",
    "publication_date": "2016-04-29",
    "publication_year": 2016,
    "authors": "Sangwhan Moon; A. L. Narasimha Reddy",
    "corresponding_authors": "",
    "abstract": "Parity protection at the system level is typically employed to compose reliable storage systems. However, careful consideration is required when SSD-based systems employ parity protection. First, additional writes are required for parity updates. Second, parity consumes space on the device, which results in write amplification from less efficient garbage collection at higher space utilization. This article analyzes the effectiveness of SSD-based RAID and discusses the potential benefits and drawbacks in terms of reliability. A Markov model is presented to estimate the lifetime of SSD-based RAID systems in different environments. In a small array, our results show that parity protection provides benefit only with considerably low space utilizations and low data access rates. However, in a large system, RAID improves data lifetime even when we take write amplification into account.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2345608625",
    "type": "article"
  },
  {
    "title": "Level Hashing",
    "doi": "https://doi.org/10.1145/3322096",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Pengfei Zuo; Yu Hua; Jie Wu",
    "corresponding_authors": "",
    "abstract": "Non-volatile memory (NVM) technologies as persistent memory are promising candidates to complement or replace DRAM for building future memory systems, due to having the advantages of high density, low power, and non-volatility. In main memory systems, hashing index structures are fundamental building blocks to provide fast query responses. However, hashing index structures originally designed for dynamic random access memory (DRAM) become inefficient for persistent memory due to new challenges including hardware limitations of NVM and the requirement of data consistency. To address these challenges, this article proposes level hashing , a write-optimized and high-performance hashing index scheme with low-overhead consistency guarantee and cost-efficient resizing. Level hashing provides a sharing-based two-level hash table, which achieves constant-scale worst-case time complexity for search, insertion, deletion, and update operations, and rarely incurs extra NVM writes. To guarantee the consistency with low overhead, level hashing leverages log-free consistency schemes for deletion, insertion, and resizing operations, and an opportunistic log-free scheme for update operation. To cost-efficiently resize this hash table, level hashing leverages an in-place resizing scheme that only needs to rehash 1/3 of buckets instead of the entire table to expand a hash table and rehash 2/3 of buckets to shrink a hash table, thus significantly improving the resizing performance and reducing the number of rehashed buckets. Extensive experimental results show that the level hashing speeds up insertions by 1.4×−3.0×, updates by 1.2×−2.1×, expanding by over 4.3×, and shrinking by over 1.4× while maintaining high search and deletion performance compared with start-of-the-art hashing schemes.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2949504530",
    "type": "article"
  },
  {
    "title": "The Case for Custom Storage Backends in Distributed Storage Systems",
    "doi": "https://doi.org/10.1145/3386362",
    "publication_date": "2020-05-18",
    "publication_year": 2020,
    "authors": "Abutalib Aghayev; Sage A. Weil; Michael Kuchnik; Mark Nelson; Gregory R. Ganger; George Amvrosiadis",
    "corresponding_authors": "",
    "abstract": "For a decade, the Ceph distributed file system followed the conventional wisdom of building its storage backend on top of local file systems. This is a preferred choice for most distributed file systems today, because it allows them to benefit from the convenience and maturity of battle-tested code. Ceph’s experience, however, shows that this comes at a high price. First, developing a zero-overhead transaction mechanism is challenging. Second, metadata performance at the local level can significantly affect performance at the distributed level. Third, supporting emerging storage hardware is painstakingly slow. Ceph addressed these issues with BlueStore, a new backend designed to run directly on raw storage devices. In only two years since its inception, BlueStore outperformed previous established backends and is adopted by 70% of users in production. By running in user space and fully controlling the I/O stack, it has enabled space-efficient metadata and data checksums, fast overwrites of erasure-coded data, inline compression, decreased performance variability, and avoided a series of performance pitfalls of local file systems. Finally, it makes the adoption of backward-incompatible storage hardware possible, an important trait in a changing storage landscape that is learning to embrace hardware diversity.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3030629024",
    "type": "article"
  },
  {
    "title": "TrueErase",
    "doi": "https://doi.org/10.1145/2854882",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Sarah Diesburg; Christopher Meyers; Mark Stanovich; An-I Andy Wang; Geoff Kuenning",
    "corresponding_authors": "",
    "abstract": "One important aspect of privacy is the ability to securely delete sensitive data from electronic storage in such a way that it cannot be recovered; we call this action secure deletion . Short of physically destroying the entire storage medium, existing software secure-deletion solutions tend to be piecemeal at best -- they may only work for one type of storage or file system, may force the user to delete all files instead of selected ones, may require the added complexities of encryption and key storage, may require extensive changes and additions to the computer's operating system or storage firmware, and may not handle system crashes gracefully. We present TrueErase, a holistic secure-deletion framework for individual systems that contain sensitive data. Through design, implementation, verification, and evaluation on both a hard drive and NAND flash, TrueErase shows that it is possible to construct a per-file, secure-deletion framework that can accommodate different storage media and legacy file systems, require limited changes to legacy systems, and handle common crash scenarios. TrueErase can serve as a building block by cryptographic systems that securely delete information by erasing encryption keys. The overhead is dependent on spatial locality, number of sensitive files, and workload (computational- or I/O-bound).",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2406802995",
    "type": "article"
  },
  {
    "title": "Tools for Predicting the Reliability of Large-Scale Storage Systems",
    "doi": "https://doi.org/10.1145/2911987",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Robert J. Hall",
    "corresponding_authors": "Robert J. Hall",
    "abstract": "Data-intensive applications require extreme scaling of their underlying storage systems. Such scaling, together with the fact that storage systems must be implemented in actual data centers, increases the risk of data loss from failures of underlying components. Accurate engineering requires quantitatively predicting reliability, but this remains challenging due to the need to account for extreme scale, redundancy scheme type and strength, distribution architecture, and component dependencies. This article introduces CQS im -R, a tool suite for predicting the reliability of large-scale storage system designs and deployments. CQS im -R includes (a) direct calculations based on an only-drives-fail failure model and (b) an event-based simulator for detailed prediction that handles failures of and failure dependencies among arbitrary (drive or nondrive) components. These are based on a common combinatorial framework for modeling placement strategies. The article demonstrates CQS im -R using models of common storage systems, including replicated and erasure coded designs. New results, such as the poor reliability scaling of spread-placed systems and a quantification of the impact of data center distribution and rack-awareness on reliability, demonstrate the usefulness and generality of the tools. Analysis and empirical studies show the tools’ soundness, performance, and scalability.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2516510200",
    "type": "article"
  },
  {
    "title": "The Reliability of Modern File Systems in the face of SSD Errors",
    "doi": "https://doi.org/10.1145/3375553",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Shehbaz Jaffer; Stathis Maneas; Andy A. Hwang; Bianca Schroeder",
    "corresponding_authors": "",
    "abstract": "As solid state drives (SSDs) are increasingly replacing hard disk drives, the reliability of storage systems depends on the failure modes of SSDs and the ability of the file system layered on top to handle these failure modes. While the classical paper on IRON File Systems provides a thorough study of the failure policies of three file systems common at the time, we argue that 13 years later it is time to revisit file system reliability with SSDs and their reliability characteristics in mind, based on modern file systems that incorporate journaling, copy-on-write, and log-structured approaches and are optimized for flash. This article presents a detailed study, spanning ext4, Btrfs, and F2FS, and covering a number of different SSD error modes. We develop our own fault injection framework and explore over 1,000 error cases. Our results indicate that 16% of these cases result in a file system that cannot be mounted or even repaired by its system checker. We also identify the key file system metadata structures that can cause such failures, and, finally, we recommend some design guidelines for file systems that are deployed on top of SSDs.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3033256959",
    "type": "article"
  },
  {
    "title": "<i> B <sup>3</sup> </i> -Tree",
    "doi": "https://doi.org/10.1145/3394025",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Hokeun Cha; Moohyeon Nam; Kibeom Jin; Jiwon Seo; Beomseok Nam",
    "corresponding_authors": "",
    "abstract": "In this work, we propose B3-tree, a hybrid index for persistent memory that leverages the byte-addressability of the in-memory index and the page locality of B-trees. As in the byte-addressable in-memory index, B3-tree is updated by 8-byte store instructions. Also, as in disk-based index, B3-tree is failure-atomic since it makes every 8-byte store instruction transform a consistent index into another consistent index without the help of expensive logging. Since expensive logging becomes unnecessary, the number of cacheline flush instructions required for B3-tree is significantly reduced. Our performance study shows that B3-tree outperforms other state-of-the-art persistent indexes in terms of insert and delete performance. While B3-tree shows slightly worse performance for point query performance, the range query performance of B3-tree is 2x faster than FAST and FAIR B-tree because the leaf page size of B3-tree can be set to 8x larger than that of FAST and FAIR B-tree without degrading insertion performance. We also show that read transactions can access B3-tree without acquiring a shared lock because B3-tree remains always consistent while a sequence of 8-byte write operations are making changes to it. As a result, B3-tree provides high concurrency level comparable to FAST and FAIR B-tree.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3041215642",
    "type": "article"
  },
  {
    "title": "ctFS: Replacing File Indexing with Hardware Memory Translation through Contiguous File Allocation for Persistent Memory",
    "doi": "https://doi.org/10.1145/3565026",
    "publication_date": "2022-11-09",
    "publication_year": 2022,
    "authors": "Ruibin Li; Xiang Ren; Xu Zhao; Siwei He; Michael Stumm; Ding Yuan",
    "corresponding_authors": "",
    "abstract": "Persistent byte-addressable memory (PM) is poised to become prevalent in future computer systems. PMs are significantly faster than disk storage, and accesses to PMs are governed by the Memory Management Unit (MMU) just as accesses with volatile RAM. These unique characteristics shift the bottleneck from I/O to operations such as block address lookup—for example, in write workloads, up to 45% of the overhead in ext4-DAX is due to building and searching extent trees to translate file offsets to addresses on persistent memory. We propose a novel contiguous file system, ctFS, that eliminates most of the overhead associated with indexing structures such as extent trees in the file system. ctFS represents each file as a contiguous region of virtual memory, hence a lookup from the file offset to the address is simply an offset operation, which can be efficiently performed by the hardware MMU at a fraction of the cost of software-maintained indexes. Evaluating ctFS on real-world workloads such as LevelDB shows it outperforms ext4-DAX and SplitFS by 3.6× and 1.8×, respectively.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4308731338",
    "type": "article"
  },
  {
    "title": "FlatLSM: Write-Optimized LSM-Tree for PM-Based KV Stores",
    "doi": "https://doi.org/10.1145/3579855",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Kewen He; Yujie An; Yijing Luo; Xiaoguang Liu; Gang Wang",
    "corresponding_authors": "",
    "abstract": "The Log-Structured Merge Tree (LSM-Tree) is widely used in key-value (KV) stores because of its excwrite performance. But LSM-Tree-based KV stores still have the overhead of write-ahead log and write stall caused by slow L 0 flush and L 0 - L 1 compaction. New byte-addressable, persistent memory (PM) devices bring an opportunity to improve the write performance of LSM-Tree. Previous studies on PM-based LSM-Tree have not fully exploited PM’s “dual role” of main memory and external storage. In this article, we analyze two strategies of memtables based on PM and the reasons write stall problems occur in the first place. Inspired by the analysis result, we propose FlatLSM, a specially designed flat LSM-Tree for non-volatile memory based KV stores. First, we propose PMTable with separated index and data. The PM Log utilizes the Buffer Log to store KVs of size less than 256B. Second, to solve the write stall problem, FlatLSM merges the volatile memtables and the persistent L 0 into large PMTables, which can reduce the depth of LSM-Tree and concentrate I/O bandwidth on L 0 - L 1 compaction. To mitigate write stall caused by flushing large PMTables to SSD, we propose a parallel flush/compaction algorithm based on KV separation. We implemented FlatLSM based on RocksDB and evaluated its performance on Intel’s latest PM device, the Intel Optane DC PMM with the state-of-the-art PM-based LSM-Tree KV stores, FlatLSM improves the throughput 5.2× on random write workload and 2.55× on YCSB-A.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4317659229",
    "type": "article"
  },
  {
    "title": "eZNS: Elastic Zoned Namespace for Enhanced Performance Isolation and Device Utilization",
    "doi": "https://doi.org/10.1145/3653716",
    "publication_date": "2024-04-12",
    "publication_year": 2024,
    "authors": "Jae‐Hong Min; Chenxingyu Zhao; Ming Liu; Arvind Krishnamurthy",
    "corresponding_authors": "",
    "abstract": "Emerging Zoned Namespace (ZNS) SSDs, providing the coarse-grained zone abstraction, hold the potential to significantly enhance the cost efficiency of future storage infrastructure and mitigate performance unpredictability. However, existing ZNS SSDs have a static zoned interface, making them in-adaptable to workload runtime behavior, unscalable to underlying hardware capabilities, and interfering with co-located zones. Applications either under-provision the zone resources yielding unsatisfied throughput, create over-provisioned zones and incur costs, or experience unexpected I/O latencies. We propose eZNS, an elastic-ZNS interface that exposes an adaptive zone with predictable characteristics. eZNS comprises two major components: a zone arbiter that manages zone allocation and active resources on the control plane, and a hierarchical I/O scheduler with read congestion control and write admission control on the data plane. Together, eZNS enables the transparent use of a ZNS SSD and closes the gap between application requirements and zone interface properties. Our evaluations over RocksDB demonstrate that eZNS outperforms a static zoned interface by 17.7% and 80.3% in throughput and tail latency, respectively, at most.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4394766056",
    "type": "article"
  },
  {
    "title": "The Design of Fast Delta Encoding for Delta Compression Based Storage Systems",
    "doi": "https://doi.org/10.1145/3664817",
    "publication_date": "2024-05-14",
    "publication_year": 2024,
    "authors": "Haoliang Tan; Wen Xia; Xiangyu Zou; Cai Deng; Qing Liao; Zhaoquan Gu",
    "corresponding_authors": "",
    "abstract": "Delta encoding is a data reduction technique capable of calculating the differences (i.e., delta) among very similar files and chunks. It is widely used for various applications, such as synchronization replication, backup/archival storage, cache compression, and so on. However, delta encoding is computationally costly due to its time-consuming word-matching operations for delta calculation. Existing delta encoding approaches either run at a slow encoding speed, such as Xdelta and Zdelta, or at a low compression ratio, such as Ddelta and Edelta. In this article, we propose Gdelta, a fast delta encoding approach with a high compression ratio. The key idea behind Gdelta is the combined use of five techniques: (1) employing an improved Gear-based rolling hash to replace Adler32 hash for fast scanning overlapping words of similar chunks, (2) adopting a quick array-based indexing for word-matching, (3) applying a sampling indexing scheme to reduce the cost of traditional building full indexes for base chunks’ words, (4) skipping unmatched words to accelerate delta encoding through non-redundant areas, and (5) last but not least, after word-matching, further batch compressing the remainder to improve the compression ratio. Our evaluation results driven by seven real-world datasets suggest that Gdelta achieves encoding/decoding speedups of 3.5X∼25X over the classic Xdelta and Zdelta approaches while increasing the compression ratio by about 10%∼240%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396904087",
    "type": "article"
  },
  {
    "title": "Efficiently Enlarging RDMA-Attached Memory with SSD",
    "doi": "https://doi.org/10.1145/3700772",
    "publication_date": "2024-10-21",
    "publication_year": 2024,
    "authors": "Zhe Yang; Qing Wang; Xiaojian Liao; Youyou Lu; Keji Huang; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "RDMA-based in-memory storage systems offer high performance but are restricted by the capacity of physical memory. In this paper, we propose TeRM to extend RDMA-attached memory with SSD. TeRM achieves fast remote access on the SSD-extended memory by eliminating page faults of RDMA NIC and CPU from the critical path. We also introduce a set of techniques to reduce the consumption of CPU and network resources. Evaluation shows that TeRM performs close to the performance of the ideal upper bound where all pages are pinned in the physical memory. Compared with existing approaches TeRM significantly improves the performance of unmodified RDMA-based storage systems, including a file system and a key-value system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403609169",
    "type": "article"
  },
  {
    "title": "Storage Abstractions for SSDs: The Past, Present, and Future",
    "doi": "https://doi.org/10.1145/3708992",
    "publication_date": "2024-12-30",
    "publication_year": 2024,
    "authors": "Xiang-Qun Zhang; Janki Bhimani; Shuyi Pei; Eunji Lee; Sungjin Lee; Yoon Jae Seong; E. Kim; Changho Choi; Eyee Hyun Nam; Jongmoo Choi; Bryan S. Kim",
    "corresponding_authors": "",
    "abstract": "This paper traces the evolution of SSD (solid-state drive) interfaces, examining the transition from the block storage paradigm inherited from hard disk drives to SSD-specific standards customized to flash memory. Early SSDs conformed to the block abstraction for compatibility with the existing software storage stack, but studies and deployments show that this limits the performance potential for SSDs. As a result, new SSD-specific interface standards emerged to not only capitalize on the low latency and abundant internal parallelism of SSDs, but also include new command sets that diverge from the longstanding block abstraction. We first describe flash memory technology in the context of the block storage abstraction and the components within an SSD that provide the block storage illusion. We then describe the genealogy and relationships among academic research and industry standardization efforts for SSDs, along with some of their rise and fall in popularity. We classify these work into four evolving branches: (1) extending block abstraction with host-SSD hints/directives; (2) enhancing host-level control over SSDs; (3) offloading host-level management to SSDs; and (4) making SSDs byte-addressable. By dissecting these trajectories, the paper also sheds light on the emerging challenges and opportunities, providing a roadmap for future research and development in SSD technologies.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4405915827",
    "type": "article"
  },
  {
    "title": "Project Silica: Towards Sustainable Cloud Archival Storage in Glass",
    "doi": "https://doi.org/10.1145/3708996",
    "publication_date": "2024-12-31",
    "publication_year": 2024,
    "authors": "Patrick Anderson; Erika Aranas; Youssef Assaf; R. Behrendt; Richard Black; Marco Caballero; Pashmina Cameron; Burcu Canakci; Andromachi Chatzieleftheriou; Rebekah Storan Clarke; James Clegg; Daniel Cletheroe; Bridgette Cooper; Thales De Carvalho; Tim Deegan; Austin Donnelly; Rokas Drevinskas; Alexander L. Gaunt; Christos Gkantsidis; Ariel Gomez Diaz; Istvan Haller; Freddie Hong; Teodora Ilieva; Shashidhar Joshi; Russell Joyce; W. Minster Kunkel; David Lara; Sergey Legtchenko; Fanglin Linda Liu; Bruno Magalhaes; Alana Marzoev; Marvin McNett; Jayashree Mohan; Michael Myrah; Truong Nguyen; Sebastian Nowozin; Aaron Ogus; Hiske Overweg; Antony Rowstron; Maneesh Sah; Masaaki Sakakura; P. Scholtz; Nina Schreiner; Omer Sella; Adam Smith; Ioan Stefanovici; David Sweeney; Benn C. Thomsen; Govert Verkes; Phil Wainman; Jonathan Westcott; Luke Weston; Charles Whittaker; Pablo Wilke Berenguer; Hugh Williams; Thomas Winkler; Stefan Winzeck",
    "corresponding_authors": "",
    "abstract": "Sustainable and cost-effective long-term storage remains an unsolved problem. The most widely used storage technologies today are magnetic (hard disk drives and tape). They use media that degrades over time and has a limited lifetime, which leads to inefficient, wasteful, and costly solutions for long-lived data. This paper presents Silica: the first cloud storage system for archival data underpinned by quartz glass, an extremely resilient media that allows data to be left in situ indefinitely. The hardware and software of Silica have been co-designed and co-optimized from the media up to the service level with sustainability as a primary objective. The design follows a cloud-first, data-driven methodology underpinned by principles derived from analyzing the archival workload of a large public cloud service. Silica can support a wide range of archival storage workloads and ushers in a new era of sustainable, cost-effective storage.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4405918781",
    "type": "article"
  },
  {
    "title": "Consistent and automatic replica regeneration",
    "doi": "https://doi.org/10.1145/1044956.1044958",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Haifeng Yu; Amin Vahdat",
    "corresponding_authors": "",
    "abstract": "Reducing management costs and improving the availability of large-scale distributed systems require automatic replica regeneration , that is, creating new replicas in response to replica failures. A major challenge to regeneration is maintaining consistency when the replica group changes. Doing so is particularly difficult across the wide area where failure detection is complicated by network congestion and node overload.In this context, this article presents Om, the first read/write peer-to-peer, wide-area storage system that achieves high availability and manageability through online automatic regeneration while still preserving consistency guarantees. We achieve these properties through the following techniques. First, by utilizing the limited view divergence property in today's Internet and by adopting the witness model , Om is able to regenerate from any single replica, rather than requiring a majority quorum, at the cost of a small (10 −6 in our experiments) probability of violating consistency during each regeneration. As a result, Om can deliver high availability with a small number of replicas, while traditional designs would significantly increase the number of replicas. Next, we distinguish failure-free reconfigurations from failure-induced ones, enabling common reconfigurations to proceed with a single round of communication. Finally, we use a lease graph among the replicas and a two-phase write protocol to optimize for reads, so that reads in Om can be processed by any single replica. Experiments on PlanetLab show that consistent regeneration in Om completes in approximately 20 seconds.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2130078562",
    "type": "article"
  },
  {
    "title": "Minimum density RAID-6 codes",
    "doi": "https://doi.org/10.1145/1970338.1970340",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "James S. Plank; Adam L. Buchsbaum; Bradley T. Vander Zanden",
    "corresponding_authors": "",
    "abstract": "RAID-6 codes protect disk array storage systems from two-disk failures. This article presents a complete treatment of a class of RAID-6 codes, called minimum density RAID-6 codes , that have an optimal blend of performance properties. There are two families of minimal density RAID-6 codes: Blaum-Roth codes and Liberation codes, and a separate special-purpose code called the Liber8tion code. The first of these have been known since the late 1990's, while the latter two are new constructions. In this article, we motivate, demonstrate, and evaluate the minimum density codes, comparing them to EVENODD and RDP codes, which represent the state-of-the-art in RAID-6. Following that, we prove that the codes indeed fit the RAID-6 methodology, and cite their implementation in an open-source library.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2028794704",
    "type": "article"
  },
  {
    "title": "A hybrid flash translation layer with adaptive merge for SSDs",
    "doi": "https://doi.org/10.1145/1970338.1970339",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Gyudong Shim; Young‐Woo Park; Kyu Ho Park",
    "corresponding_authors": "",
    "abstract": "The Flash Translation Layer (FTL) in Solid-State Disks (SSDs) maps logical addresses to physical addresses for disk drive virtualization. In order to reduce garbage collection overhead, we propose full associative striped block-level mapping. In addition, an adaptive merge is proposed to avoid excessive data block reconstructions during garbage collection. With these mechanisms, the write latency is improved up to 78% in comparison with the previous multichannel hybrid FTLs in a sample PC trace. The performance improvements stem from 52% reduced garbage collection.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2115002476",
    "type": "article"
  },
  {
    "title": "Understanding and Alleviating the Impact of the Flash Address Translation on Solid State Devices",
    "doi": "https://doi.org/10.1145/3051123",
    "publication_date": "2017-05-22",
    "publication_year": 2017,
    "authors": "You Zhou; Fei Wu; Ping Huang; Xubin He; Changsheng Xie; Jian Zhou",
    "corresponding_authors": "",
    "abstract": "Flash-based solid state devices (SSDs) have been widely employed in consumer and enterprise storage systems. However, the increasing SSD capacity imposes great pressure on performing efficient logical to physical address translation in a page-level flash translation layer (FTL). Existing schemes usually employ a built-in RAM to store mapping information, called mapping cache , to speed up the address translation. Since only a fraction of the mapping table can be cached due to limited cache space, a large number of extra flash accesses are required for cache management and garbage collection, degrading the performance and lifetime of an SSD. In this paper, we first apply analytical models to investigate the key factors that incur extra flash accesses during address translation. Then, we propose a novel page-level FTL with an efficient translation page-level caching mechanism, named TPFTL , to minimize the extra flash accesses. TPFTL employs a two-level least recently used (LRU) list with space-efficient optimizations to organize cached mapping entries. Inspired by the models, we further design a workload-adaptive loading policy combined with an efficient replacement policy to increase the cache hit rate and reduce the writebacks of replaced dirty entries. Finally, we evaluate TPFTL using extensive trace-driven simulations. Our evaluation results show that compared to the state-of-the-art FTLs, TPFTL significantly reduces the extra operations caused by address translation, achieving reductions on system response time and write amplification by up to 27.1% and 32.2%, respectively.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2619660230",
    "type": "article"
  },
  {
    "title": "CGraph",
    "doi": "https://doi.org/10.1145/3319406",
    "publication_date": "2019-04-20",
    "publication_year": 2019,
    "authors": "Yu Zhang; Jin Zhao; Xiaofei Liao; Hai Jin; Lin Gu; Haikun Liu; Bingsheng He; Ligang He",
    "corresponding_authors": "",
    "abstract": "Distributed graph processing platforms usually need to handle massive Concurrent iterative Graph Processing (CGP) jobs for different purposes. However, existing distributed systems face high ratio of data access cost to computation for the CGP jobs, which incurs low throughput. We observed that there are strong spatial and temporal correlations among the data accesses issued by different CGP jobs, because these concurrently running jobs usually need to repeatedly traverse the shared graph structure for the iterative processing of each vertex. Based on this observation, this article proposes a distributed storage and processing system CGraph for the CGP jobs to efficiently handle the underlying static/evolving graph for high throughput. It uses a data-centric load-trigger-pushing model, together with several optimizations, to enable the CGP jobs to efficiently share the graph structure data in the cache/memory and their accesses by fully exploiting such correlations, where the graph structure data is decoupled from the vertex state associated with each job. It can deliver much higher throughput for the CGP jobs by effectively reducing their average ratio of data access cost to computation. Experimental results show that CGraph improves the throughput of the CGP jobs by up to 3.47× in comparison with existing solutions on distributed platforms.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2940923397",
    "type": "article"
  },
  {
    "title": "Mitigating Synchronous I/O Overhead in File Systems on Open-Channel SSDs",
    "doi": "https://doi.org/10.1145/3319369",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Youyou Lu; Jiwu Shu; Jiacheng Zhang",
    "corresponding_authors": "",
    "abstract": "Synchronous I/O has long been a design challenge in file systems. Although open-channel solid state drives (SSDs) provide better performance and endurance to file systems, they still suffer from synchronous I/Os due to the amplified writes and worse hot/cold data grouping. The reason lies in the controversy design choices between flash write and read/erase operations. While fine-grained logging improves performance and endurance in writes, it hurts indexing and data grouping efficiency in read and erase operations. In this article, we propose a flash-friendly data layout by introducing a built-in persistent staging layer to provide balanced read, write, and garbage collection performance. Based on this, we design a new flash file system (FS) named StageFS , which decouples the content and structure updates. Content updates are logically logged to the staging layer in a persistence-efficient way, which achieves better write performance and lower write amplification. The updated contents are reorganized into the normal data area for structure updates, with improved hot/cold grouping and in a page-level indexing way, which is more friendly to read and garbage collection operations. Evaluation results show that, compared to recent flash-friendly file system (F2FS), StageFS effectively improves performance by up to 211.4% and achieves low garbage collection overhead for workloads with frequent synchronization.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2948028886",
    "type": "article"
  },
  {
    "title": "Enabling Efficient Updates in KV Storage via Hashing",
    "doi": "https://doi.org/10.1145/3340287",
    "publication_date": "2019-08-13",
    "publication_year": 2019,
    "authors": "Yongkun Li; Helen H. W. Chan; Patrick P. C. Lee; Yinlong Xu",
    "corresponding_authors": "",
    "abstract": "Persistent key-value (KV) stores mostly build on the Log-Structured Merge (LSM) tree for high write performance, yet the LSM-tree suffers from the inherently high I/O amplification. KV separation mitigates I/O amplification by storing only keys in the LSM-tree and values in separate storage. However, the current KV separation design remains inefficient under update-intensive workloads due to its high garbage collection (GC) overhead in value storage. We propose HashKV, which aims for high update performance atop KV separation under update-intensive workloads. HashKV uses hash-based data grouping , which deterministically maps values to storage space to make both updates and GC efficient. We further relax the restriction of such deterministic mappings via simple but useful design extensions. We extensively evaluate various design aspects of HashKV. We show that HashKV achieves 4.6× update throughput and 53.4% less write traffic compared to the current KV separation design. In addition, we demonstrate that we can integrate the design of HashKV with state-of-the-art KV stores and improve their respective performance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2969132550",
    "type": "article"
  },
  {
    "title": "MultiLanes",
    "doi": "https://doi.org/10.1145/2801155",
    "publication_date": "2016-06-16",
    "publication_year": 2016,
    "authors": "Junbin Kang; Chunming Hu; Tianyu Wo; Ye Zhai; Benlong Zhang; Jinpeng Huai",
    "corresponding_authors": "",
    "abstract": "OS-level virtualization is often used for server consolidation in data centers because of its high efficiency. However, the sharing of storage stack services among the colocated containers incurs contention on shared kernel data structures and locks within I/O stack, leading to severe performance degradation on manycore platforms incorporating fast storage technologies (e.g., SSDs based on nonvolatile memories). This article presents MultiLanes, a virtualized storage system for OS-level virtualization on manycores. MultiLanes builds an isolated I/O stack on top of a virtualized storage device for each container to eliminate contention on kernel data structures and locks between them, thus scaling them to manycores. Meanwhile, we propose a set of techniques to tune the overhead induced by storage-device virtualization to be negligible, and to scale the virtualized devices to manycores on the host, which itself scales poorly. To reduce the contention within each single container, we further propose SFS, which runs multiple file-system instances through the proposed virtualized storage devices, distributes all files under each directory among the underlying file-system instances, then stacks a unified namespace on top of them. The evaluation of our prototype system built for Linux container (LXC) on a 32-core machine with both a RAM disk and a modern flash-based SSD demonstrates that MultiLanes scales much better than Linux in micro- and macro-benchmarks, bringing significant performance improvements, and that MultiLanes with SFS can further reduce the contention within each single container.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2425594935",
    "type": "article"
  },
  {
    "title": "Optimizing File Systems with Fine-grained Metadata Journaling on Byte-addressable NVM",
    "doi": "https://doi.org/10.1145/3060147",
    "publication_date": "2017-05-22",
    "publication_year": 2017,
    "authors": "Cheng Chen; Jun Yang; Qingsong Wei; Chundong Wang; Mingdi Xue",
    "corresponding_authors": "",
    "abstract": "Journaling file systems have been widely adopted to support applications that demand data consistency. However, we observed that the overhead of journaling can cause up to 48.2% performance drop under certain kinds of workloads. On the other hand, the emerging high-performance, byte-addressable Non-volatile Memory (NVM) has the potential to minimize such overhead by being used as the journal device. The traditional journaling mechanism based on block devices is nevertheless unsuitable for NVM due to the write amplification of metadata journal we observed. In this article, we propose a fine-grained metadata journal mechanism to fully utilize the low-latency byte-addressable NVM so that the overhead of journaling can be significantly reduced. Based on the observation that conventional block-based metadata journal contains up to 90% clean metadata that is unnecessary to be journalled, we design a fine-grained journal format for byte-addressable NVM which contains only modified metadata. Moreover, we redesign the process of transaction committing, checkpointing, and recovery in journaling file systems utilizing the new journal format. Therefore, thanks to the reduced amount of ordered writes for journals, the overhead of journaling can be reduced without compromising the file system consistency. To evaluate our fine-grained metadata journaling mechanism, we have implemented a journaling file system prototype based on Ext4 and JBD2 in Linux. Experimental results show that our NVM-based fine-grained metadata journaling is up to 15.8 × faster than the traditional approach under FileBench workloads.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2619578132",
    "type": "article"
  },
  {
    "title": "Lazy Exact Deduplication",
    "doi": "https://doi.org/10.1145/3078837",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Jingwei Ma; Rebecca J. Stones; Yuxiang Ma; Jingui Wang; Junjie Ren; Gang Wang; Xiaoguang Liu",
    "corresponding_authors": "",
    "abstract": "Deduplication aims to reduce duplicate data in storage systems by removing redundant copies of data blocks, which are compared to one another using fingerprints. However, repeated on-disk fingerprint lookups lead to high disk traffic, which results in a bottleneck. In this article, we propose a “lazy” data deduplication method, which buffers incoming fingerprints that are used to perform on-disk lookups in batches, with the aim of improving subsequent prefetching. In deduplication in general, prefetching is used to improve the cache hit rate by exploiting locality within the incoming fingerprint stream. For lazy deduplication, we design a buffering strategy that preserves locality in order to facilitate prefetching. Furthermore, as the proportion of deduplication time spent on I/O decreases, the proportion spent on fingerprint calculation and chunking increases. Thus, we also utilize parallel approaches (utilizing multiple CPU cores and a graphics processing unit) to further improve the overall performance. Experimental results indicate that the lazy method improves fingerprint identification performance by over 50% compared with an “eager” method with the same data layout. The GPU improves the hash calculation by a factor of 4.6 and multithreaded chunking by a factor of 4.16. Deduplication performance can be improved by over 45% on SSD and 80% on HDD in the last round on the real datasets.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2625784407",
    "type": "article"
  },
  {
    "title": "Pannier",
    "doi": "https://doi.org/10.1145/3094785",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Cheng Li; Philip Shilane; Fred Douglis; Grant Wallace",
    "corresponding_authors": "",
    "abstract": "Classic caching algorithms leverage recency, access count, and/or other properties of cached blocks at per-block granularity. However, for media such as flash which have performance and wear penalties for small overwrites, implementing cache policies at a larger granularity is beneficial. Recent research has focused on buffering small blocks and writing in large granularities, sometimes called containers, but it has not explored the ramifications and best strategies for caching compound blocks consisting of logically distinct, but physically co-located, blocks. Containers may have highly diverse blocks, with mixtures of frequently accessed, infrequently accessed, and invalidated blocks. We propose and evaluate Pannier, a flash cache layer that provides high performance while extending flash lifespan. Pannier uses three main techniques: (1) leveraging block access counts to manage cache containers, (2) incorporating block liveness as a property to improve flash cache space efficiency, and (3) designing a multi-step feedback controller to ensure a flash cache reaches its desired lifespan while maintaining performance. Our evaluation shows that Pannier improves flash cache performance and extends lifespan beyond previous per-block and container-aware caching policies. More fundamentally, our investigation highlights the importance of creating new policies for caching compound blocks in flash.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2755039058",
    "type": "article"
  },
  {
    "title": "Sketching Volume Capacities in Deduplicated Storage",
    "doi": "https://doi.org/10.1145/3369737",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Danny Harnik; Moshik Hershcovitch; Yosef Shatsky; Amir Epstein; Ronen I. Kat",
    "corresponding_authors": "",
    "abstract": "The adoption of deduplication in storage systems has introduced significant new challenges for storage management. Specifically, the physical capacities associated with volumes are no longer readily available. In this work, we introduce a new approach to analyzing capacities in deduplicated storage environments. We provide sketch-based estimations of fundamental capacity measures required for managing a storage system: How much physical space would be reclaimed if a volume or group of volumes were to be removed from a system (the reclaimable capacity) and how much of the physical space should be attributed to each of the volumes in the system (the attributed capacity). Our methods also support capacity queries for volume groups across multiple storage systems, e.g., how much capacity would a volume group consume after being migrated to another storage system? We provide analytical accuracy guarantees for our estimations as well as empirical evaluations. Our technology is integrated into a prominent all-flash storage array and exhibits high performance even for very large systems. We also demonstrate how this method opens the door for performing placement decisions at the data-center level and obtaining insights on deduplication in the field.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3004612141",
    "type": "article"
  },
  {
    "title": "Power-optimized Deployment of Key-value Stores Using Storage Class Memory",
    "doi": "https://doi.org/10.1145/3511905",
    "publication_date": "2022-03-10",
    "publication_year": 2022,
    "authors": "Hiwot Tadese Kassa; Jason Akers; Mrinmoy Ghosh; Zhichao Cao; Vaibhav Gogte; Ronald Dreslinski",
    "corresponding_authors": "",
    "abstract": "High-performance flash-based key-value stores in data-centers utilize large amounts of DRAM to cache hot data. However, motivated by the high cost and power consumption of DRAM, server designs with lower DRAM-per-compute ratio are becoming popular. These low-cost servers enable scale-out services by reducing server workload densities. This results in improvements to overall service reliability, leading to a decrease in the total cost of ownership (TCO) for scalable workloads. Nevertheless, for key-value stores with large memory footprints, these reduced DRAM servers degrade performance due to an increase in both IO utilization and data access latency. In this scenario, a standard practice to improve performance for sharded databases is to reduce the number of shards per machine, which degrades the TCO benefits of reduced DRAM low-cost servers. In this work, we explore a practical solution to improve performance and reduce the costs and power consumption of key-value stores running on DRAM-constrained servers by using Storage Class Memories (SCM). SCMs in a DIMM form factor, although slower than DRAM, are sufficiently faster than flash when serving as a large extension to DRAM. With new technologies like Compute Express Link, we can expand the memory capacity of servers with high bandwidth and low latency connectivity with SCM. In this article, we use Intel Optane PMem 100 Series SCMs (DCPMM) in AppDirect mode to extend the available memory of our existing single-socket platform deployment of RocksDB (one of the largest key-value stores at Meta). We first designed a hybrid cache in RocksDB to harness both DRAM and SCM hierarchically. We then characterized the performance of the hybrid cache for three of the largest RocksDB use cases at Meta (ChatApp, BLOB Metadata, and Hive Cache). Our results demonstrate that we can achieve up to 80% improvement in throughput and 20% improvement in P95 latency over the existing small DRAM single-socket platform, while maintaining a 43–48% cost improvement over our large DRAM dual-socket platform. To the best of our knowledge, this is the first study of the DCPMM platform in a commercial data center.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4221109816",
    "type": "article"
  },
  {
    "title": "Building GC-free Key-value Store on HM-SMR Drives with ZoneFS",
    "doi": "https://doi.org/10.1145/3502846",
    "publication_date": "2022-07-22",
    "publication_year": 2022,
    "authors": "Yiwen Zhang; Ting Yao; Jiguang Wan; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "Host-managed shingled magnetic recording drives (HM-SMR) are advantageous in capacity to harness the explosive growth of data. For key-value (KV) stores based on log-structured merge trees (LSM-trees), the HM-SMR drive is an ideal solution owning to its capacity, predictable performance, and economical cost. However, building an LSM-tree-based KV store on HM-SMR drives presents severe challenges in maintaining the performance and space utilization efficiency due to the redundant cleaning processes for applications and storage devices (i.e., compaction and garbage collection). To eliminate the overhead of on-disk garbage collection (GC) and improve compaction efficiency, this article presents GearDB , a GC-free KV store tailored for HM-SMR drives. GearDB improves the write performance and space efficiency through three new techniques: a new on-disk data layout, compaction windows, and a novel gear compaction algorithm. We further augment the read performance of GearDB with a new SSTable layout and read ahead mechanism. We implement GearDB with LevelDB, and use zonefs to access a real HM-SMR drive. Our extensive experiments confirm that GearDB achieves both high performance and space efficiency, i.e., on average 1.7× and 1.5× better than LevelDB in random write and read, respectively, with up to 86.9% space efficiency.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4286716706",
    "type": "article"
  },
  {
    "title": "Tunable Encrypted Deduplication with Attack-resilient Key Management",
    "doi": "https://doi.org/10.1145/3510614",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Zuoru Yang; Jingwei Li; Yanjing Ren; Patrick P. C. Lee",
    "corresponding_authors": "",
    "abstract": "Conventional encrypted deduplication approaches retain the deduplication capability on duplicate chunks after encryption by always deriving the key for encryption/decryption from the chunk content, but such a deterministic nature causes information leakage due to frequency analysis. We present TED , a tunable encrypted deduplication primitive that provides a tunable mechanism for balancing the tradeoff between storage efficiency and data confidentiality. The core idea of TED is that its key derivation is based on not only the chunk content but also the number of duplicate chunk copies, such that duplicate chunks are encrypted by distinct keys in a controlled manner. In particular, TED allows users to configure a storage blowup factor, under which the information leakage quantified by an information-theoretic measure is minimized for any input workload. In addition, we extend TED with a distributed key management architecture and propose two attack-resilient key generation schemes that trade between performance and fault tolerance. We implement an encrypted deduplication prototype TEDStore to realize TED in networked environments. Evaluation on real-world file system snapshots shows that TED effectively balances the tradeoff between storage efficiency and data confidentiality, with small performance overhead.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4297294775",
    "type": "article"
  },
  {
    "title": "The <i>what</i> , The <i>from</i> , and The <i>to</i> : The Migration Games in Deduplicated Systems",
    "doi": "https://doi.org/10.1145/3565025",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Roei Kisous; Ariel Kolikant; Abhinav Duggal; Sarai Sheinvald; Gala Yadgar",
    "corresponding_authors": "",
    "abstract": "Deduplication reduces the size of the data stored in large-scale storage systems by replacing duplicate data blocks with references to their unique copies. This creates dependencies between files that contain similar content and complicates the management of data in the system. In this article, we address the problem of data migration, in which files are remapped between different volumes as a result of system expansion or maintenance. The challenge of determining which files and blocks to migrate has been studied extensively for systems without deduplication. In the context of deduplicated storage, however, only simplified migration scenarios have been considered. In this article, we formulate the general migration problem for deduplicated systems as an optimization problem whose objective is to minimize the system’s size while ensuring that the storage load is evenly distributed between the system’s volumes and that the network traffic required for the migration does not exceed its allocation. We then present three algorithms for generating effective migration plans, each based on a different approach and representing a different trade-off between computation time and migration efficiency. Our greedy algorithm provides modest space savings but is appealing thanks to its exceptionally short runtime. Its results can be improved by using larger system representations. Our theoretically optimal algorithm formulates the migration problem as an integer linear programming (ILP) instance. Its migration plans consistently result in smaller and more balanced systems than those of the greedy approach, although its runtime is long and, as a result, the theoretical optimum is not always found. Our clustering algorithm enjoys the best of both worlds: its migration plans are comparable to those generated by the ILP-based algorithm, but its runtime is shorter, sometimes by an order of magnitude. It can be further accelerated at a modest cost in the quality of its results.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4298005108",
    "type": "article"
  },
  {
    "title": "HLN-Tree: A memory-efficient B+-Tree with huge leaf nodes and locality predictors",
    "doi": "https://doi.org/10.1145/3707641",
    "publication_date": "2025-01-06",
    "publication_year": 2025,
    "authors": "André Brinkmann; Reza Salkhordeh; Florian Wiegert; Peng Wang; Xin Yao; Renhai Chen; Keji Huang; Gong Zhang",
    "corresponding_authors": "",
    "abstract": "Key-value stores in Cloud environments can contain more than 2 45 unique elements and be larger than 100 PByte. B + -Tree s are well suited for these larger-than-memory datasets and seamlessly index data stored on thousands of secondary storage devices. Unfortunately, it is often uneconomical to even store all inner tree nodes in memory for these dataset sizes. Therefore, lookup performance is affected by the additional IOs for reading inner nodes. This number of inner nodes can be reduced by increasing the size of leaf nodes. We propose HLN-Tree s, which support huge leaf nodes without increasing the IO sizes for individual index operations. They partition leaf nodes in arrays of independent subnodes and combine ideas from BD-trees with rebalancing, learning key deviations, and storing locality predictors. HLN-Tree s have been initially designed for uniform at random key distributions and support arbitrary key distributions through an additional layer of hashing in leaf nodes. HLN-Tree s decrease the number of inner nodes by up to 256 × for uniform at random key distributions and by 16 × to 64 × for arbitrary ones compared to B + -Tree s, while keeping their performance at the same level even at high concurrency levels. We show analytically and through real-world and synthetic benchmarks that HLN-Tree s also outperform state-of-the-art learned indexes for secondary storage.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406125223",
    "type": "article"
  },
  {
    "title": "The Past, Present, and Future of Storage Technologies (Part 1 of 2)",
    "doi": "https://doi.org/10.1145/3709140",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Geoff Kuenning; Youjip Won; Ming Zhao; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "This article traces the evolution of SSD (solid-state drive) interfaces, examining the transition from the block storage paradigm inherited from hard disk drives to SSD-specific standards customized to flash memory. Early SSDs conformed to the block ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406396769",
    "type": "article"
  },
  {
    "title": "WALSH: Write-Aggregating Log-Structured Hashing for Hybrid Memory",
    "doi": "https://doi.org/10.1145/3715010",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Yubo Liu; Yongfeng Wang; Zhiguang Chen; Yutong Lu; Ming Zhao",
    "corresponding_authors": "",
    "abstract": "Persistent memory (PM) brings important opportunities for improving data storage including the widely used hash tables. However, PM is not friendly to small writes, which causes existing PM hashes to suffer from high hardware write amplification. Hybrid memory offers the performance and concurrency of DRAM and the durability and capacity of PM, but existing hybrid memory hashes cannot deliver high performance, low DRAM footprint, and fast recovery at the same time. This paper proposes WALSH, a flat hash with novel log-structured separate chaining designs to optimize the performance while ensuring low DRAM footprint and fast recovery. To address the overhead of hash resizing and garbage collection (GC), WALSH further proposes partial resizing/GC mechanisms and a 4-phase protocol for concurrent hash operations. As a result, WALSH is the first flat index for hybrid memory with embedded write aggregation ability. A comprehensive evaluation shows that WALSH substantially outperforms state-of-the-art hybrid memory hashes; e.g., its insert throughput is up to 2.4X that of related works while saving more than 87% of DRAM. WALSH also provides efficient recovery; e.g., it can recover a dataset with 1 billion objects in just a few seconds.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406803270",
    "type": "article"
  },
  {
    "title": "HM-ORAM: A Lightweight Crash-consistent ORAM Framework on Hybrid Memory System",
    "doi": "https://doi.org/10.1145/3715009",
    "publication_date": "2025-01-25",
    "publication_year": 2025,
    "authors": "Gang Liu; Xiao Zheng; Kangxun Li; Rujia Wang",
    "corresponding_authors": "",
    "abstract": "Byte-addressable non-volatile memory (NVM) is a promising alternative technology for main memory, allowing the processor to access persistent data in the main memory directly. Systems with emerging NVM as the main memory still suffer from information leakage, performance, and endurance challenges. Fortunately, Oblivious RAM (ORAM) provides provable secure address obfuscation and encryption and can be used to build a secure memory system. However, it is challenging to support crash consistency with the increasing use of NVM as persistent memory. Current state-of-the-art PS-ORAM systems are designed based on pure NVM memory systems, which suffer from low performance. Different from the previous PS-ORAM system, in this work, we consider a hybrid memory system (both NVM and DRAM) and aim to build a crash-consistent ORAM framework with low overhead. We propose the HM-ORAM framework, which includes a novel ORAM tree-level partition scheme, a lightweight data persistency architecture, and a secure access protocol. It makes full advantage of both hybrid memory technologies to achieve the design requirements to support crash consistency and persistence of ORAM systems with minimal write overhead. Without compromising security, HM-ORAM can outperform an NVM-based ORAM (i.e., NVM-ORAM) system by 1.23 × and 1.39 × in non-recursive and recursive implementations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406825379",
    "type": "article"
  },
  {
    "title": "Scalable and High-Performance Large-Scale Dynamic Graph Storage and Processing System",
    "doi": "https://doi.org/10.1145/3715332",
    "publication_date": "2025-01-25",
    "publication_year": 2025,
    "authors": "Rui Wang; Weixu Zong; Shuibing He; Yongkun Li; Yinlong Xu",
    "corresponding_authors": "",
    "abstract": "Existing in-memory graph storage systems that rely on DRAM have scalability issues because of the limited capacity and volatile nature of DRAM. The emerging persistent memory (PMEM) offers us a chance to solve these issues through its larger capacity and non-volatile characteristics. However, simply adapting existing DRAM-based graph storage systems to PMEM would result in inefficient PMEM stores and accesses, including high read and write amplification to PMEM, imbalanced work division for PMEM accesses, and costly remote PMEM access across NUMA nodes. These issues severely limit the performance of large graph processing. In this paper, we aim to achieve scalable and high-performance graph processing in PMEM. We first propose an XPLine-friendly graph storage model that uses vertex-centric graph buffering, hierarchical vertex buffer managing, and in-place vertex block merging to optimize PMEM graph storage. Furthermore, we develop a scalable graph processing model that leverages multi-threaded work dividing and NUMA-friendly graph accessing to optimize PMEM graph accesses. Based on these techniques, we implement XPGraph , a PMEM-based graph storage system for large-scale evolving graphs, and several variants for different system settings. Our experiments demonstrate that XPGraph surpasses the state-of-the-art in-memory graph storage system on a PMEM-based system by 3.07 × to 4.99 × in update performance and up to 5.87 × in query performance, and performs much better in highly parallel multi-threaded scenarios.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406826556",
    "type": "article"
  },
  {
    "title": "Analyzing Request Volatility of I/O Temporal Behaviors in Mobile Storage Workloads",
    "doi": "https://doi.org/10.1145/3722117",
    "publication_date": "2025-03-06",
    "publication_year": 2025,
    "authors": "Qiang Zou; Bo Mao; Suzhen Wu; Yujuan Tan; Donghong Qin",
    "corresponding_authors": "",
    "abstract": "The design and performance optimization of flash-based storage subsystems are crucial for improving the system performance of Android-based smartphones. However, it highly relies on wisdom derived from mobile storage workload studies of smartphone applications. From the temporal perspective, our burstiness diagnosis reveals that the arrival processes of I/O requests in 33 smartphone applications, are significantly bursty, especially for read requests. This paper studies the correlation of inter-arrival times of read and write requests, and compares the correlations for read and write requests in the four same types of mobile applications. We first observe that read requests of 85% of applications and write requests of 88% of applications present a certain degree of correlation over a longer time range. Then, we further conduct Hurst parameter estimation for these mobile applications with mature statistical tools. All estimated Hurst parameters are larger than 0.5, confirming the existence of self-similarity in a majority of mobile application workloads. Finally, we deploy a flexible I/O request generator for smartphone applications based on the parameters measured from actual traces. Experimental results show that the proposed generator can accurately generate request sequences for various mobile applications and more faithfully characterize the heavy-tail properties of I/O activities in mobile storage workloads than traditional models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408198695",
    "type": "article"
  },
  {
    "title": "Simplicity as the Ultimate Principle: The Art of Garbage Collection Management in SSDs Inspired by Natural Data Behavior",
    "doi": "https://doi.org/10.1145/3725219",
    "publication_date": "2025-03-19",
    "publication_year": 2025,
    "authors": "Keyu Wang; Huailiang Tan; Keqin Li",
    "corresponding_authors": "",
    "abstract": "As solid-state drives (SSDs) are increasingly used in various computing environments, effective garbage collection (GC) management is crucial for enhancing performance and extending lifespan. Existing GC strategies rely on complex data categorization techniques to distinguish between hot and cold data. This process is not only computationally expensive but also inefficient. This paper introduces a revolutionary simplified GC management method, which we call SUP-GC, based on the core design principle that simplicity is the ultimate principle. SUP-GC requires almost no computation and naturally redefines the data storage pattern. All newly written data is defaulted as hot data and stored directly in hot data blocks; data that needs to be moved during the GC process is considered cold data and uniformly migrated to cold data blocks. Our strategy eliminates the need for precise but resource-intensive real-time analysis of data states and instead adopts a storage strategy that is highly consistent with the natural properties of data. This intuitive partitioning significantly reduces the need for complex judgments about data states, thereby optimizing the storage management process. Experiments and designs on real SSDs have shown that our SUP-GC strategy significantly outperforms existing mainstream GC methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408622785",
    "type": "article"
  },
  {
    "title": "A High-Performance and Scalable Userspace Log-Structured File System for Modern SSDs",
    "doi": "https://doi.org/10.1145/3728645",
    "publication_date": "2025-04-09",
    "publication_year": 2025,
    "authors": "Wenbao Jia; Dejun Jiang; Jin Xiong",
    "corresponding_authors": "",
    "abstract": "We present AugeFS , a scalable userspace log-structured file system for modern SSDs. AugeFS re-architects the file system stack to address three critical challenges: inefficient control plane, limited metadata scalability, and underutilized device bandwidth. First, we propose a shared and protected address space within the userspace of accessing applications to run AugeFS , which enables the high-performance data plane and efficient control plane. Second, we design a scalable LSM-tree based key-value store called MetaDB to organize small-sized metadata in AugeFS . To improve the metadata scalability, MetaDB employs parallel request processing to reduce thread synchronization overhead and fine-grained parallel write-ahead log to eliminate false sharing in metadata persistence. Finally, AugeFS distributes files into different domains. To reduce contention, we maintain space management metadata for each domain independently, which helps scale data performance and improve the device IO utilization. Moreover, AugeFS designs an asynchronous IO stack for fsync to reduce the latency of synchronous writes. The evaluation results show that AugeFS significantly improves both metadata scalability and data scalability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409280366",
    "type": "article"
  },
  {
    "title": "HaParallel: Hit Ratio-Aware Parallel Aggressive Eviction Cache Management Algorithm for SSDs",
    "doi": "https://doi.org/10.1145/3728644",
    "publication_date": "2025-04-09",
    "publication_year": 2025,
    "authors": "Lianzheng Su; Ming‐Wei Lin; Bo Mao; Jianpeng Zhang; Zeshui Xu",
    "corresponding_authors": "",
    "abstract": "Solid-state drives (SSDs) can be classified into two types: with and without built-in cache. In terms of performance, SSDs featuring cache exhibit a substantial performance advantage over their non-cache counterparts. The main focus of this paper is the management of built-in cache in SSDs. From a large number of previous studies, we observe that cache hit ratios remain relatively modest for the majority of workloads. First, based on this observation, we adopt an aggressive eviction policy, diverging from traditional eviction algorithms that follow an on-demand eviction policy. Next, considering the temporal locality and parallelism of cached data, we introduce multi-level linked lists to organize cached data. In this way, a smaller computational load can be used to increase the probability of triggering advanced commands. Finally, drawing inspiration from congestion control algorithm in computer networking, we design a cache hit ratio-aware unit. This unit can employ varying degrees of aggressive eviction policies based on its own state. The aim is to maximize the execution of advanced commands while limiting the impact of the aggressive eviction policy on the hit ratio. Our experimental simulations on real workloads show a substantial improvement in average response time compared to LRU, VBBMS, and Req-block, with our method achieving reductions of 19.7%, 19.4%, and 29.9%, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409280370",
    "type": "article"
  },
  {
    "title": "BLA: Byzantine-Tolerant Lazy Auditing Framework for Decentralized Storage Data Integrity",
    "doi": "https://doi.org/10.1145/3731542",
    "publication_date": "2025-04-22",
    "publication_year": 2025,
    "authors": "Tengfei Li; Minghao Yin; Juncheng Hu",
    "corresponding_authors": "",
    "abstract": "With the rise of blockchain technology, the trend toward decentralization has spread to the field of remote storage, leading to the emergence of decentralized storage as a promising model. This change is highlighted by its features of open and fair access, reduced dependence on intermediaries, and strong privacy protections. However, similar to centralized storage, the decentralization of data management presents challenges, including the separation of ownership and control, along with the need for integrity auditing on externally managed data. The current popular centralised auditing model for the mainstream cloud storage cannot be directly used for decentralized storage environments. Additionally, Homomorphic Verification Tag (HVT)-based auditing models encounter significant problems such as high computational costs and inefficient auditing processes. In response to these needs, we introduce a novel Byzantine-tolerant Lazy Auditing framework (BLA) to ensure data integrity in decentralized storage settings. A key innovation is the hierarchical architecture used: the upper level employs a simplified Practical Byzantine Fault Tolerance (PBFT) protocol to help nodes reach a consensus on data integrity audits. At the lower level, nodes are grouped into clusters based on criteria such as accessibility, organized using a block design strategy. This approach reduces unnecessary information exchange during the auditing process. It maximizes parallel processing and strengthens fault tolerance and system resilience. By distributing data, it also reduces the impact of node failures. Our theoretical analyses and empirical evaluations clearly show that BLA reduces communication complexity compared to conventional PBFT protocols. Additionally, when compared to traditional HVT-based schemes, BLA demonstrates better storage efficiency and improved computational performance, making it a viable and effective solution for data integrity auditing in decentralized storage systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409653203",
    "type": "article"
  },
  {
    "title": "NVM in Data Storage: A Post-Optane Future",
    "doi": "https://doi.org/10.1145/3731454",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Sajad Karim; Johannes Wünsche; Michael Kühn; Gunter Saake; David Broneske",
    "corresponding_authors": "",
    "abstract": "The dynamic evolution of non-volatile memory (NVM) technologies from Read-Only Memory (ROM) to flash memory, and recent innovations in Magnetoresistive RAM (MRAM), Phase Change Memory (PCM), and Resistive RAM (ReRAM) signify a pivotal shift in data storage capabilities and applications. This progression, marked by enhancements in performance and density, fills the gap between Dynamic RAM (DRAM) and flash storage, meeting the demands of data-rich domains such as high-performance computing and databases. The integration of NVM like the formerly commercially available Optane Persistent Memory (PMem) has introduced a paradigm shift in storage system design, addressing challenges related to latency, data proximity, memory constraints, and energy efficiency which are critical in computing. Despite facing hurdles, such as performance asymmetry in Optane PMem, the potential of NVM in data storage systems has been shown with research focusing on speeding up indexing, small data accesses and cache-less I/O paths. We review existing and emerging NVM technologies based on their physical properties and application in data storage. We underscore the significance of NVM in offering unique access characteristics that address the limitations of traditional storage devices, thereby extending the data storage landscape. We discuss the programming challenges and hardware-software strategies crucial for NVM’s seamless integration and widespread industry adoption. Additionally, we discuss the discontinuation of Optane, the ongoing challenges and the need for storage system designs to adapt to non-block based storage to leverage NVM’s performance benefits. We conclude that while advancements in NVM promise improved performance and endurance, practical availability depends on addressing challenges such as density, heat management, and reliability. Support in the existing software ecosystem is also crucial for its widespread adoption. Furthermore, Compute Express Link (CXL) is highlighted as a significant development that can streamline the memory hierarchy and support the adoption of NVM, enabling more flexible designs and effective usage of storage media in next-generation data storage systems. Overall, CXL offers a promising method to include NVM in future systems. However, in the meantime, data storage systems will rely on NVMe SSDs due to their performance and availability. But, we encourage further research into integrating NVM devices due to data management constraints with conventional block-based data management and advocate for the availability of NVM-ready software to users.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409812068",
    "type": "article"
  },
  {
    "title": "Enhancing the Performance of Next-Generation SSD Arrays: A Holistic Approach",
    "doi": "https://doi.org/10.1145/3736588",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Jie Zhang; Shushu Yi; Xiurui Pan; Yiming Xu; Qiao Li; Qiang Li; Chenxi Wang; Bo Mao; Myoungsoo Jung",
    "corresponding_authors": "",
    "abstract": "All-flash array (AFA) is a popular approach to aggregate the capacity of multiple solid-state drives (SSDs) while guaranteeing fault tolerance. Unfortunately, existing AFA engines inflict substantial software overheads on the I/O path, such as the user-kernel context switches and AFA internal tasks (e.g., parity preparation), thereby failing to adopt next-generation high-performance SSDs. Tackling this challenge, we propose ScalaAFA, a unique holistic design of AFA engine that can extend the throughput of next-generation SSD arrays in scale with low CPU costs. We incorporate ScalaAFA into user space to avoid user-kernel context switches while harnessing SSD built-in resources for handling AFA internal tasks. Specifically, in adherence to the lock-free principle of existing user-space storage framework, ScalaAFA substitutes the traditional locks with an efficient message-passing-based permission management scheme to facilitate inter-thread synchronization. Considering the CPU burden imposed by background I/O and parity computation, ScalaAFA proposes to offload these tasks to SSDs. To mitigate host-SSD communication overheads in offloading, ScalaAFA takes a novel data placement policy that enables transparent data gathering and in-situ parity computation. ScalaAFA also addresses the write amplification and metadata persistence issues and avoids GC-caused latency spikes, by thoroughly exploiting SSD architectural innovations. Comprehensive evaluation results indicate that ScalaAFA can achieve 2.5 × write throughput and reduce average write latency by a significant 52.7%, compared to the state-of-the-art AFA engines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410519164",
    "type": "article"
  },
  {
    "title": "MTree: A Tiering-based Key-Value Store Powered by High-performance Hierarchical Data Management",
    "doi": "https://doi.org/10.1145/3736587",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Hui Sun; Y. Chen; Yingjun Yu; Yongtao Deng; Yinliang Yue; Song Jiang; Xiao Qin",
    "corresponding_authors": "",
    "abstract": "Key-value stores (KV stores) anchored on log-structured merge trees (LSM-tree) provide much improved performance under write-intensive workloads but exhibit significant write amplification (WA). The tiering compaction strategy is widely adopted in KV stores to reduce the WA. However, there are three imminent issues in tiering-based KV stores. First, excessive levels in the LSM-tree structure result in high read and write amplification. Second, without key partitioning support, these KV stores increase write tail latency. Relying solely on a hash-based key partitioning scheme is insufficient, as it does not support range queries. Furthermore, a KV store with only a key range-based partitioning scheme overlooks the distribution of keys within the key space.. Third, existing KV stores with level optimization can incur high costs. A common approach to reducing the number of LSM-tree levels is to increase the MemTable size. While this reduces the number of levels, it requires more memory, leading to higher costs. Therefore, it is crucial to balance the trade-off between level optimization and monetary expenses. To address these issues, we propose a tiering-based key-value store, leveraging high-performance hierarchical data management. The key space of our proposed KV store is partitioned into m ultiple partitions based on key ranges, with an LSM- tree in each partition. We refer to this KV store as MTree in this paper. MTree reduces the number of levels in the LSM-Tree structure close to one without increasing the monetary cost. In MTree, a hierarchical structure that combines the log and LSM-tree curtails the number of levels in the LSM-tree by accumulating the data in the log without increasing the cost. With the hierarchical structure in place, we devise a long- and short-term key density distribution-aware partitioning scheme for the key range. This scheme dynamically adjusts the size of partitioning, balancing data in the partition and storing most data in large-sized logs. Then, MTree reduces the number of levels in the LSM-tree, thereby optimizing the read/write amplification. The experimental results show that MTree limits the WA to as low as two. MTree enhances the random write throughput of the state-of-the-art KV stores by up to 6.9 ×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410519415",
    "type": "article"
  },
  {
    "title": "Disaggregated Memory for File-backed Pages",
    "doi": "https://doi.org/10.1145/3736585",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Daegyu Han; J. W. Nam; Hokeun Cha; Changdae Kim; Kwangwon Koh; Taehoon Kim; Sang-Hoon Kim; Beomseok Nam",
    "corresponding_authors": "",
    "abstract": "To explore the opportunity of expanding the page cache using disaggregated memory for file-backed pages, this study presents BalloonStasher, an RDMA-based disaggregated memory for data-intensive applications. Utilizing the ephemeral nature of the page cache, BalloonStasher dynamically adapts to the changing page cache demands of multiple clients. BalloonStasher supports one-sided RDMA-based memory pooling or two-sided RDMA-based memory sharing when using memory nodes. Additionally, it also supports peer memory mode, which utilizes the idle memory of peer nodes. Our extensive performance study compares the benefits and limitations of the three cache modes, and shows that BalloonStasher can mitigate the memory underutilization problem and improve performance of data-intensive applications by a large margin.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410564852",
    "type": "article"
  },
  {
    "title": "Physical vs. Logical Indexing with IDEA: Inverted Deduplication-Aware Index",
    "doi": "https://doi.org/10.1145/3729426",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "A J Levi; Philip Shilane; Sarai Sheinvald; Gala Yadgar",
    "corresponding_authors": "",
    "abstract": "In the realm of information retrieval, the need to maintain reliable term-indexing has grown more acute in recent years, with vast amounts of ever-growing online data used for data mining and natural language processing, and searched by a large number of search-engine users. At the same time, an increasing portion of primary storage systems employ data deduplication, where duplicate logical data chunks are replaced with references to a unique physical copy. We show that indexing deduplicated data with deduplication-oblivious mechanisms might result in extreme inefficiencies: the index size would increase in proportion to the logical data size, regardless of its duplication ratio, consuming excessive storage and memory and slowing down lookups. In addition, the logically sequential accesses during index creation would be transformed into random and redundant accesses to the physical chunks. Indeed, to the best of our knowledge, term indexing is not supported by any deduplicating storage system. In this paper, we propose the design of a deduplication-aware term-index that addresses these challenges. IDEA maps terms to the unique chunks that contain them, and maps each chunk to the files in which it is contained. This basic design concept improves the index performance and can support advanced functionalities such as inline indexing, result ranking, and proximity search. Our prototype implementation based on Lucene (the search engine at the core of Elasticsearch) shows that IDEA can reduce the index size and indexing time by up to 73% and 94%, respectively, and reduce term-lookup latency by up to 82% and 59% for single and multi-term queries, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410564994",
    "type": "article"
  },
  {
    "title": "Lustre Unveiled: Evolution, Design, Advancements, and Current Trends",
    "doi": "https://doi.org/10.1145/3736583",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Anjus George; Andreas Dilger; Michael J. Brim; Robert Mohr; Amir Shehata; Jong Youl Choi; Ahmad Maroof Karimi; Jesse Hanley; James Q. Simmons; Dominic Manno; Verónica Melesse Vergara; Sarp Oral; Christopher Zimmer",
    "corresponding_authors": "",
    "abstract": "The Lustre filesystem serves as a vital element in high-performance parallel storage, meeting the rising demands of scientific, research, and enterprise environments. Widely deployed across HPC environments, ranging from small-scale applications in AI/ML, to domains like oil and gas, drug discovery, and meteorology, and manufacturing, Lustre addresses the universal challenge of efficiently accessing vast and ever-increasing volumes of data. Lustre is the filesystem of choice on six out of the top 10 fastest supercomputers in the world today, over 65% of the top 100, and also for over 60% of the top 500. Despite its widespread popularity, there is a lack of a complete and up-to-date reference, covering Lustre’s evolution, design, and various advancements made over the years. In this journal, we aim to fill this gap by providing a comprehensive journey of Lustre, including its history with significant contributions to HPC, detailed architecture and design elements, exploration of advancements added through its evolution, and future directions. Additionally, we present a comparison of Lustre with other prominent storage technologies of the era. To illustrate the current state of Lustre, we analyze several filesystem trends, including utilization, performance, and usage patterns on Orion, the Lustre filesystem on the first exascale supercomputer Frontier. We hope that this journal serves as a comprehensive educational reference for the current and future generations interested in HPC filesystem storage aspects.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410601015",
    "type": "article"
  },
  {
    "title": "Graceful CNN Model Degradation in Uncorrected Flash Storage for Embedded Edge Devices",
    "doi": "https://doi.org/10.1145/3747298",
    "publication_date": "2025-07-07",
    "publication_year": 2025,
    "authors": "Hung-Yi Chen; Jin-Wei Chang; Homg-Chih Lin; Li-Pin Chang",
    "corresponding_authors": "",
    "abstract": "Computing near the source of data has been proven effective in terms of energy conservation, latency improvement, and privacy preservation. With this, edge intelligence refers to local CNN inference in embedded edge devices. Because edge devices are highly resource-constrained, in practice they store CNN model(s) in external flash memory and load them (or part of) during runtime inference. However, flash memory is subject to time-related retention errors, and thus a dilemma is that without error correction, a CNN model in flash memory quickly deteriorates and becomes useless. On the other hand, strong error correction involves extra read sensing and read retrying. Due to the concern of reliability, the lineup of embedded serial flash memory offers low-density solutions only. In this study, we investigate graceful degradation of CNN models in uncorrected high-density flash storage for edge intelligence. Inspired by the observation that in popular CNN models, the majority of CNN weight parameters share a few monotonic bit patterns in their high-order bits, we propose mapping the frequent bit patterns to low flash-cell voltage levels for protection against retention errors. Furthermore, as CNN layers show different levels of tolerance to bit errors, we also propose using adaptive cell-bit density to provide non-uniform error protection among layers. We conducted experiments on popular CNN designs, including VGG16, ResNet50, and InceptionV3, under realistic effects of flash aging. Results show that with prior methods, the inference accuracy of ResNet50 degrades to 70.7% in just three months; by contrast, with our approach, the degradation is less than 1% and the accuracy remains at 91.6% after one year of retention.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412091390",
    "type": "article"
  },
  {
    "title": "Argus: A Precise and Efficient Resemblance Detection for Post-Deduplication Delta Compression",
    "doi": "https://doi.org/10.1145/3747839",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "H. Xu; Xiangyu Zou; Yunsheng Dong; Philip Shilane; Yanqi Pan; Cai Deng; Wen Xia",
    "corresponding_authors": "",
    "abstract": "For data reduction techniques used in storage systems, delta compression is often implemented after deduplication, having been shown to achieve a much higher compression ratio by efficiently detecting and compressing similar data chunks. Unfortunately, existing resemblance detection approaches cannot maintain both high throughput and data reduction ratio simultaneously since they either introduce heavy calculation overhead or generate useless features that reduce the accuracy of resemblance detection. In this paper, we propose Argus, a fast and precise resemblance detection approach using two primary techniques to improve data-reduction efficiency significantly. First, Argus utilizes a Bin-Wise Partitioning strategy, which separates the rolling hash values of each data chunk into different subsets according to the suffix bits of the hash value and generates features from these subsets. Thus, Argus generates features more efficiently, which both achieves high detection accuracy and improves feature generation speed. Second, based on the efficient Bin-Wise Partitioning strategy, Argus utilizes fine-grained Gear rolling hash and a Plain Feature strategy to manage the granularity of the content represented in the feature, increasing the probability of feature matching and catching as many similar chunks as possible. Consequently, Argus can achieve better detection accuracy, resulting in a much higher compression ratio than previous works while minimizing the computational overhead for resemblance detection. Our evaluation results driven by several real-world datasets suggest that, compared to the state-of-the-art approaches, Argus achieves up to 1.64 × (DeepSketch) and 2.29 × (Finesse, Odess, and N-Transform) higher delta compression ratio and achieves up to 19.9 × (N-Transform), 5.57 × (Finesse), and 1.18 × (Odess) faster feature generation speed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412393310",
    "type": "article"
  },
  {
    "title": "The Past, Present, and Future of Storage Technologies (part 2 of 2)",
    "doi": "https://doi.org/10.1145/3744914",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Geoff Kuenning; Youjip Won; Ming Zhao; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412557516",
    "type": "article"
  },
  {
    "title": "Achieving Both Performance and Reliability in An Asymmetric File System on Disaggregated Persistent Memory",
    "doi": "https://doi.org/10.1145/3760403",
    "publication_date": "2025-08-13",
    "publication_year": 2025,
    "authors": "Miao Cai; Junru Shen; Baoliu Ye",
    "corresponding_authors": "",
    "abstract": "The ultra-fast persistent memories (PMs) promise a practical solution towards high-performance distributed file systems. This paper examines and reveals a cascade of performance and reliability issues in the current PM provision scheme, which not only underutilizes fast PM devices but also leads to severe consequences, such as throughput degradation, load imbalance, and even service outage. To remedy these, we introduce Ethane+, a rack-scale, distributed file system built on disaggregated persistent memory (DPM). Through resource separation using fast data connection technologies, DPM achieves efficient and cost-effective PM sharing while supporting strong fault isolation. To unleash such hardware potentials, Ethane+ incorporates an asymmetric file system architecture inspired by the imbalanced resource provision feature of DPM. It splits a file system into a control-plane FS and a data-plane FS, and designs these two planes with dual goals of best hardware utilization and hardening file system reliability. Evaluation results demonstrate that Ethane+ reaps the DPM hardware benefits, performs up to 60 × better than modern distributed file systems, resists both software and hardware faults, and improves data-intensive application throughputs by up to 15 ×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413167321",
    "type": "article"
  },
  {
    "title": "A Tale of Two Paths: Optimizing Paravirtualized Storage I/O with eBPF",
    "doi": "https://doi.org/10.1145/3760404",
    "publication_date": "2025-08-13",
    "publication_year": 2025,
    "authors": "Li Wang; Shi Qiu; Jiuming Yan; Zhirong Shen; Qingbo Wu; Xin Yao; Meiling Wang; Renhai Chen; Yiming Zhang",
    "corresponding_authors": "",
    "abstract": "KVM is the dominant VM hypervisor on Linux, and relies on QEMU to realize the backends of the virtio family of devices such as virtio-blk. However, KVM/QEMU-based paravirtualization prolongs the guest I/O path with multiple context switches. As fast NVMe storage devices have been widely used, the software overhead becomes non-negligible. To shorten the I/O path, virtio-blk’s variations, vhost-kernel-blk and vhost-user-blk, respectively perform all guest I/O processing in kernel and user spaces. Unfortunately, they essentially forsake the collaboration between KVM and QEMU, sacrifice important QEMU features including live migration, snapshots, and flexible image format support. This paper presents EXO, an extension of virtio-blk for efficient KVM/QEMU-based storage paravirtualization. The insight is that no matter how complex the QEMU backend’s processing is, to handle a guest I/O request, the host storage stack only needs to know the request’s guest-to-host address mapping. Therefore, we preserve the original slow I/O path of virtio-blk as a fallback, and leverage eBPF to introduce an in-kernel fast path that directly queries the address mapping without switching to the user-space backend processing. Extensive evaluation shows that EXO achieves similar or even higher performance compared to the variations (vhost-kernel-blk/vhost-user-blk) of virtio-blk, while preserving virtio-blk’s flexibility, safety, and compatibility.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413167330",
    "type": "article"
  },
  {
    "title": "Synthetic Data Generation for Storage Trace Augmentation",
    "doi": "https://doi.org/10.1145/3767317",
    "publication_date": "2025-09-15",
    "publication_year": 2025,
    "authors": "Lu Pang; Krishna Kant",
    "corresponding_authors": "",
    "abstract": "Due to the increasingly data-intensive nature of the applications, the storage system performance continues to increase in importance and is often substantially responsible for the overall processing rate of the application. Fortunately, the storage technologies themselves are improving rapidly in numerous ways, from low-level read/write of bits in a device all the way to the management of the entire storage hierarchy in large enterprise and cloud settings. Studying many of the important issues in this entire spectrum often requires storage access traces from the storage server side, but these are often hard to come by. To address this gap, we present a method to generate synthetic traces using a novel generative adversarial network (GAN) architecture that captures the realism and diversity of real storage traces. The generated traces can be used to augment the existing workload traces of interest for a variety of storage system studies. We demonstrate how the proposed method can generate storage traces that have the overall characteristics of the real traces and yet provide behavioral diversity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414207658",
    "type": "article"
  },
  {
    "title": "HyTorC: Hybrid Address Translation for SSDs supporting Compression",
    "doi": "https://doi.org/10.1145/3767335",
    "publication_date": "2025-09-13",
    "publication_year": 2025,
    "authors": "Yu Zhang; Renhai Chen; Gong Zhang; Peng Wang; Xin Yao; Keji Huang; André Brinkmann",
    "corresponding_authors": "",
    "abstract": "High-capacity solid-state drives (SSDs) with expected capacities of one PByte and more will address cloud storage and archiving environments previously dominated by magnetic disks. These new applications are very cost-sensitive, so unnecessary overhead must be reduced as much as possible without sacrificing the performance advantages of SSDs. An expensive component within a scale-up SSD is the on-device memory to map host logical page numbers to flash pages. This paper therefore proposes HyTorC, which builds on the idea of S-FTL to represent sequentially stored logical pages using bitmaps instead of providing one entry per logical page. HyTorC extends this idea by introducing, for the first time in an FTL, compression of these bitmaps using run-length encoding, and by investigating the effects of background scrubbing to realign randomly written pages into contiguous runs. This background scrubbing allows HyTorC to keep large portions of the mapping table in block mapping mode, further reducing the memory footprint. HyTorC retains the flexibility of the page mapping scheme and supports compression of logical blocks within the SSD, allowing multiple compressed logical pages to be stored within a single physical page. HyTorC is fully implemented in an open-channel SSD. Our tests show that HyTorC can reduce memory consumption by an average of 98.9% over standard page mapping, 95.6% over the DFTL scheme, 87.3% over S-FTL, and 56.5% over the learned index-based approach LeaFTL for the Alibaba Cloud block traces, and by 98.6% over standard page mapping, 95.5% over the DFTL scheme, 84.1% over S-FTL, and 61.5% over LeaFTL for the Microsoft Research Cambridge traces. HyTorC focuses on the memory footprint of the FTL and not on performance. However, the performance evaluation shows that HyTorC achieves similar performance compared to page mapping and FTLs based on learned indexes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414269073",
    "type": "article"
  },
  {
    "title": "SrFTL: Leveraging Storage Semantics for Effective Ransomware Defense in Flash-based SSDs",
    "doi": "https://doi.org/10.1145/3767322",
    "publication_date": "2025-09-17",
    "publication_year": 2025,
    "authors": "Zhu Wei-dong; Grant Hernandez; Washington García; Dave Tian; Sara Rampazzi; Kevin Butler",
    "corresponding_authors": "",
    "abstract": "Ransomware attacks have become increasingly frequent and high-profile, resulting in billions of dollars in data and operational losses annually. Current mechanisms typically deploy defenses in vulnerable operating systems, making them susceptible to advanced adversaries capable of compromising the OS. While implementing defense mechanisms within storage devices can address this vulnerability, they lack detection accuracy due to their inability to access data semantics, such as file system metadata. Moreover, these methods only expose block-level interfaces without file-level information, limiting the usability and practicality of data recovery management. Therefore, we develop SrFTL , a novel ransomware defense framework that allows leveraging data semantics for accurate ransomware detection and effective file-level data recovery against data compromise. Specifically, SrFTL employs defense enforcement within the flash translation layer (FTL) of SSDs. Then, SrFTL combines the secure enclave with the modified FTL through a secure channel to enable flexible ransomware defenses within the enclave. Finally, SrFTL deploys ransomware classification and data recovery defenses in the enclave, providing high detection accuracy and low-cost data recovery. Our evaluation demonstrates that SrFTL achieves zero false positives and negatives when detecting our collected real-world ransomware samples and benign applications, outperforming current FTL-level solutions (e.g., MimosaFTL). Moreover, SrFTL introduces on average a trivial performance overhead of 1.5% compared to a regular SSD. Finally, evaluating against multiple real-world ransomware samples, SrFTL enables fast data recovery with an average time of 9.3 seconds. SrFTL thus bridges the semantic gap between the FTL and OS-level file information to stop ransomware while maintaining the integrity and authenticity of employed defenses.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414290413",
    "type": "article"
  },
  {
    "title": "SquirrelFS: Using the Rust Compiler to Check File-System Crash Consistency",
    "doi": "https://doi.org/10.1145/3769109",
    "publication_date": "2025-09-23",
    "publication_year": 2025,
    "authors": "Hayley LeBlanc; Nathan Taylor; James Bornholt; Vijay Chidambaram",
    "corresponding_authors": "",
    "abstract": "This work introduces a new approach to building crash-safe file systems for persistent memory. We exploit the fact that Rust’s typestate pattern allows compile-time enforcement of a specific order of operations. We introduce a novel crash-consistency mechanism, Synchronous Soft Updates , that boils down crash safety to enforcing ordering among updates to file-system metadata. We employ this approach to build SquirrelFS , a new file system with crash-consistency guarantees that are checked at compile time . SquirrelFS avoids the need for separate proofs, instead incorporating correctness guarantees into the typestate itself. Compiling SquirrelFS only takes tens of seconds; successful compilation indicates crash consistency, while an error provides a starting point for fixing the bug. We evaluate SquirrelFS against state of the art file systems such as NOVA and WineFS, and find that SquirrelFS achieves similar or better performance on a wide range of benchmarks and applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414423822",
    "type": "article"
  },
  {
    "title": "From In-Place Updates to Out-of-Place Selections: Reconsidering Write Disturbance in Non-Volatile Memory",
    "doi": "https://doi.org/10.1145/3767319",
    "publication_date": "2025-09-12",
    "publication_year": 2025,
    "authors": "Shuyue Zhou; Ronglong Wu; Hao Li; Zhong‐Ning Lin; Chengshuo Zheng; Zhirong Shen; Yu-Huan ZHONG; Fulin Nan; Yiming Zhang; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Non-volatile memory (NVM) opens up new opportunities to resolve scaling restrictions of main memory, yet it is still hindered by the write disturbance (WD) problem. The WD problem mistakenly transforms the values of NVM cells, hence seriously deteriorating memory reliability and downgrading access performance. Existing studies mainly mitigate the WD problem via encoding WD-prone data patterns under in-place updates, yet we find that when turning to out-of-place updates, they can gain the potential to reduce more WD errors. We present LearnWD, an approach that mitigates the WD problem in NVM via coupling machine learning with out-of-place updates. LearnWD first employs clustering algorithms to classify the stale data based on the error proneness. To perform a write operation, LearnWD carefully examines the aggressivity of new data and the error proneness of stale data, so as to speculatively minimize the resulting WD errors. We conduct extensive experiments using 15 real-world data sets with different data types, showing that LearnWD can assist a variety of data encoding schemes to further reduce 19.5% of WD errors, shorten 10.1% of write latency, and extend 22.2% of write endurance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414591291",
    "type": "article"
  },
  {
    "title": "Storage optimization for large-scale distributed stream-processing systems",
    "doi": "https://doi.org/10.1145/1326542.1326547",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Kirsten Hildrum; Fred Douglis; Joel L. Wolf; Philip S. Yu; Lisa Fleischer; Akshay Katta",
    "corresponding_authors": "",
    "abstract": "We consider storage in an extremely large-scale distributed computer system designed for stream processing applications. In such systems, both incoming data and intermediate results may need to be stored to enable analyses at unknown future times. The quantity of data of potential use would dominate even the largest storage system. Thus, a mechanism is needed to keep the data most likely to be used. One recently introduced approach is to employ retention value functions, which effectively assign each data object a value that changes over time in a prespecified way [Douglis et al.2004]. Storage space for data entering the system is reclaimed automatically by deleting data of the lowest current value. In such large systems, there will naturally be multiple file systems available, each with different properties. Choosing the right file system for a given incoming stream of data presents a challenge. In this article we provide a novel and effective scheme for optimizing the placement of data within a distributed storage subsystem employing retention value functions. The goal is to keep the data of highest overall value, while simultaneously balancing the read load to the file system. The key aspects of such a scheme are quite different from those that arise in traditional file assignment problems. We further motivate this optimization problem and describe a solution, comparing its performance to other reasonable schemes via simulation experiments.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1979255157",
    "type": "article"
  },
  {
    "title": "A strategy to emulate NOR flash with NAND flash",
    "doi": "https://doi.org/10.1145/1807060.1807062",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Yuan-Hao Chang; Jian-Hong Lin; Jen-Wei Hsieh; Tei‐Wei Kuo",
    "corresponding_authors": "",
    "abstract": "This work is motivated by a strong market demand for the replacement of NOR flash memory with NAND flash memory to cut down the cost of many embedded-system designs, such as mobile phones. Different from LRU-related caching or buffering studies, we are interested in prediction-based prefetching based on given execution traces of application executions. An implementation strategy is proposed for the storage of the prefetching information with limited SRAM and run-time overheads. An efficient prediction procedure is presented based on information extracted from application executions to reduce the performance gap between NAND flash memory and NOR flash memory in reads. With the behavior of a target application extracted from a set of collected traces, we show that data access to NOR flash memory can respond effectively over the proposed implementation.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2052585202",
    "type": "article"
  },
  {
    "title": "Optimizing MEMS-based storage devices for mobile battery-powered systems",
    "doi": "https://doi.org/10.1145/1714454.1714455",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Mohammed G. Khatib; Pieter Hartel",
    "corresponding_authors": "",
    "abstract": "An emerging storage technology, called MEMS-based storage, promises nonvolatile storage devices with ultrahigh density, high rigidity, a small form factor, and low cost. For these reasons, MEMS-based storage devices are suitable for battery-powered mobile systems such as PDAs. For deployment in such systems, MEMS-based storage devices must consume little energy. This work mainly targets reducing the energy consumption of this class of devices. We derive the operation modes of a MEMS-based storage device and systemically devise a policy in each mode for energy saving. Three types of policies are presented: power management, shutdown, and data-layout policy. Combined, these policies reduce the total energy consumed by a MEMS-based storage device. A MEMS-based storage device that enforces these policies comes close to Flash with respect to energy consumption and response time. However, enhancement on the device level is still needed; we present some suggestions to resolve this issue.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2045086213",
    "type": "article"
  },
  {
    "title": "Optimizing energy and performance for server-class file system workloads",
    "doi": "https://doi.org/10.1145/1837915.1837918",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Priya Sehgal; Vasily Tarasov; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "Recently, power has emerged as a critical factor in designing components of storage systems, especially for power-hungry data centers. While there is some research into power-aware storage stack components, there are no systematic studies evaluating each component's impact separately. Various factors like workloads, hardware configurations, and software configurations impact the performance and energy efficiency of the system. This article evaluates the file system's impact on energy consumption and performance. We studied several popular Linux file systems, with various mount and format options, using the FileBench workload generator to emulate four server workloads: Web, database, mail, and fileserver, on two different hardware configurations. The file system design, implementation, and available features have a significant effect on CPU/disk utilization, and hence on performance and power. We discovered that default file system options are often suboptimal, and even poor. In this article we show that a careful matching of expected workloads and hardware configuration to a single software configuration—the file system—can improve power-performance efficiency by a factor ranging from 1.05 to 9.4 times.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2083429848",
    "type": "article"
  },
  {
    "title": "Reducing Repair Traffic in P2P Backup Systems",
    "doi": "https://doi.org/10.1145/2027066.2027070",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Zhen Huang; Ernst W. Biersack; Yuxing Peng",
    "corresponding_authors": "",
    "abstract": "Peer to peer backup systems store data on “unreliable” peers that can leave the system at any moment. In this case, the only way to assure durability of the data is to add redundancy using either replication or erasure codes. Erasure codes are able to provide the same reliability as replication requiring much less storage space. Erasure coding breaks the data into blocks that are encoded and then stored on different nodes. However, when storage nodes permanently abandon the system, new redundant blocks must be created, which is referred to as repair. For “classical” erasure codes, generating a new block requires the transmission of k blocks over the network, resulting in a high repair traffic. Recently, two new classes of erasure codes, Regenerating Codes and Hierarchical Codes, have been proposed that significantly reduce the repair traffic. Regenerating Codes reduce the amount of data uploaded by each peer involved in the repair, while Hierarchical Codes reduce the number of nodes participating in the repair. In this article we propose to combine these two codes to devise a new class of erasure codes called ER-Hierarchical Codes that combine the advantages of both.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2036422753",
    "type": "article"
  },
  {
    "title": "Challenges and Solutions for Tracing Storage Systems",
    "doi": "https://doi.org/10.1145/3149376",
    "publication_date": "2018-04-12",
    "publication_year": 2018,
    "authors": "Marc-André Vef; Vasily Tarasov; Dean Hildebrand; André Brinkmann",
    "corresponding_authors": "",
    "abstract": "IBM Spectrum Scale’s parallel file system General Parallel File System (GPFS) has a 20-year development history with over 100 contributing developers. Its ability to support strict POSIX semantics across more than 10K clients leads to a complex design with intricate interactions between the cluster nodes. Tracing has proven to be a vital tool to understand the behavior and the anomalies of such a complex software product. However, the necessary trace information is often buried in hundreds of gigabytes of by-product trace records. Further, the overhead of tracing can significantly impact running applications and file system performance, limiting the use of tracing in a production system. In this research article, we discuss the evolution of the mature and highly scalable GPFS tracing tool and present the exploratory study of GPFS’ new tracing interface, FlexTrace , which allows developers and users to accurately specify what to trace for the problem they are trying to solve. We evaluate our methodology and prototype, demonstrating that the proposed approach has negligible overhead, even under intensive I/O workloads and with low-latency storage devices.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2796663191",
    "type": "article"
  },
  {
    "title": "Cluster and Single-Node Analysis of Long-Term Deduplication Patterns",
    "doi": "https://doi.org/10.1145/3183890",
    "publication_date": "2018-05-11",
    "publication_year": 2018,
    "authors": "Zhen “Jason” Sun; Geoff Kuenning; Sonam Mandal; Philip Shilane; Vasily Tarasov; Nong Xiao; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "Deduplication has become essential in disk-based backup systems, but there have been few long-term studies of backup workloads. Most past studies either were of a small static snapshot or covered only a short period that was not representative of how a backup system evolves over time. For this article, we first collected 21 months of data from a shared user file system; 33 users and over 4,000 snapshots are covered. We then analyzed the dataset, examining a variety of essential characteristics across two dimensions: single-node deduplication and cluster deduplication. For single-node deduplication analysis, our primary focus was individual-user data. Despite apparently similar roles and behavior among all of our users, we found significant differences in their deduplication ratios. Moreover, the data that some users share with others had a much higher deduplication ratio than average. For cluster deduplication analysis, we implemented seven published data-routing algorithms and created a detailed comparison of their performance with respect to deduplication ratio, load distribution, and communication overhead. We found that per-file routing achieves a higher deduplication ratio than routing by super-chunk (multiple consecutive chunks), but it also leads to high data skew (imbalance of space usage across nodes). We also found that large chunking sizes are better for cluster deduplication, as they significantly reduce data-routing overhead, while their negative impact on deduplication ratios is small and acceptable. We draw interesting conclusions from both single-node and cluster deduplication analysis and make recommendations for future deduplication systems design.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2799944323",
    "type": "article"
  },
  {
    "title": "TDDFS",
    "doi": "https://doi.org/10.1145/3295461",
    "publication_date": "2019-02-05",
    "publication_year": 2019,
    "authors": "Zhichao Cao; Hao Wen; Xiongzi Ge; Jingwei Ma; Jim Diehl; David Hung-Chang Du",
    "corresponding_authors": "",
    "abstract": "With the rapid increase in the amount of data produced and the development of new types of storage devices, storage tiering continues to be a popular way to achieve a good tradeoff between performance and cost-effectiveness. In a basic two-tier storage system, a storage tier with higher performance and typically higher cost (the fast tier) is used to store frequently-accessed (active) data while a large amount of less-active data are stored in the lower-performance and low-cost tier (the slow tier). Data are migrated between these two tiers according to their activity. In this article, we propose a Tier-aware Data Deduplication-based File System, called TDDFS, which can operate efficiently on top of a two-tier storage environment. Specifically, to achieve better performance, nearly all file operations are performed in the fast tier. To achieve higher cost-effectiveness, files are migrated from the fast tier to the slow tier if they are no longer active, and this migration is done with data deduplication. The distinctiveness of our design is that it maintains the non-redundant (unique) chunks produced by data deduplication in both tiers if possible. When a file is reloaded (called a reloaded file) from the slow tier to the fast tier, if some data chunks of the file already exist in the fast tier, then the data migration of these chunks from the slow tier can be avoided. Our evaluation shows that TDDFS achieves close to the best overall performance among various file-tiering designs for two-tier storage systems.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2911310687",
    "type": "article"
  },
  {
    "title": "ZoneTier",
    "doi": "https://doi.org/10.1145/3335548",
    "publication_date": "2019-07-10",
    "publication_year": 2019,
    "authors": "Xuchao Xie; Liquan Xiao; David H. C. Du",
    "corresponding_authors": "",
    "abstract": "Integrating solid-state drives (SSDs) and host-aware shingled magnetic recording (HA-SMR) drives can potentially build a cost-effective high-performance storage system. However, existing SSD tiering and caching designs in such a hybrid system are not fully matched with the intrinsic properties of HA-SMR drives due to their lacking consideration of how to handle non-sequential writes (NSWs). We propose ZoneTier, a zone-based storage tiering and caching co-design, to effectively control all the NSWs by leveraging the host-aware property of HA-SMR drives. ZoneTier exploits real-time data layout of SMR zones to optimize zone placement, reshapes NSWs generated from zone demotions to SMR preferred sequential writes, and transforms the inevitable NSWs to cleaning-friendly write traffics for SMR zones. ZoneTier can be easily extended to match host-managed SMR drives using proactive cleaning policy. We implemented a prototype of ZoneTier with user space data management algorithms and real SSD and HA-SMR drives, which are manipulated by the functions provided by libzbc and libaio. Our experiments show that ZoneTier can reduce zone relocation overhead by 29.41% on average, shorten performance recovery time of HA-SMR drives from cleaning by up to 33.37%, and improve performance by up to 32.31% than existing hybrid storage designs.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2956336315",
    "type": "article"
  },
  {
    "title": "Leveraging Glocality for Fast Failure Recovery in Distributed RAM Storage",
    "doi": "https://doi.org/10.1145/3289604",
    "publication_date": "2019-02-18",
    "publication_year": 2019,
    "authors": "Yiming Zhang; Dongsheng Li; Ling Liu",
    "corresponding_authors": "",
    "abstract": "Distributed RAM storage aggregates the RAM of servers in data center networks (DCN) to provide extremely high I/O performance for large-scale cloud systems. For quick recovery of storage server failures, MemCube [53] exploits the proximity of the BCube network to limit the recovery traffic to the recovery servers’ 1-hop neighborhood. However, the previous design is applicable only to the symmetric BCube( n , k ) network with n k +1 nodes and has suboptimal recovery performance due to congestion and contention. To address these problems, in this article, we propose CubeX, which (i) generalizes the “1-hop” principle of MemCube for arbitrary cube-based networks and (ii) improves the throughput and recovery performance of RAM-based key-value (KV) store via cross-layer optimizations. At the core of CubeX is to leverage the glocality (= globality + locality) of cube-based networks: It scatters backup data across a large number of disks globally distributed throughout the cube and restricts all recovery traffic within the small local range of each server node. Our evaluation shows that CubeX not only efficiently supports RAM-based KV store for cube-based networks but also significantly outperforms MemCube and RAMCloud in both throughput and recovery time.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2997188720",
    "type": "article"
  },
  {
    "title": "Random Slicing",
    "doi": "https://doi.org/10.1145/2632230",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "Alberto Miranda; Sascha Effert; Yangwook Kang; Ethan L. Miller; Ivan Popov; André Brinkmann; Tom Friedetzky; Toni Cortés",
    "corresponding_authors": "",
    "abstract": "The ever-growing amount of data requires highly scalable storage solutions. The most flexible approach is to use storage pools that can be expanded and scaled down by adding or removing storage devices. To make this approach usable, it is necessary to provide a solution to locate data items in such a dynamic environment. This article presents and evaluates the Random Slicing strategy, which incorporates lessons learned from table-based, rule-based, and pseudo-randomized hashing strategies and is able to provide a simple and efficient strategy that scales up to handle exascale data. Random Slicing keeps a small table with information about previous storage system insert and remove operations, drastically reducing the required amount of randomness while delivering a perfect load distribution.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2067750024",
    "type": "article"
  },
  {
    "title": "Improving the Performance of Deduplication-Based Backup Systems via Container Utilization Based Hot Fingerprint Entry Distilling",
    "doi": "https://doi.org/10.1145/3459626",
    "publication_date": "2021-10-15",
    "publication_year": 2021,
    "authors": "Zhang Datong; Yuhui Deng; Yi Zhou; Yifeng Zhu; Xiao Qin",
    "corresponding_authors": "",
    "abstract": "Data deduplication techniques construct an index consisting of fingerprint entries to identify and eliminate duplicated copies of repeating data. The bottleneck of disk-based index lookup and data fragmentation caused by eliminating duplicated chunks are two challenging issues in data deduplication. Deduplication-based backup systems generally employ containers storing contiguous chunks together with their fingerprints to preserve data locality for alleviating the two issues, which is still inadequate. To address these two issues, we propose a container utilization based hot fingerprint entry distilling strategy to improve the performance of deduplication-based backup systems. We divide the index into three parts: hot fingerprint entries, fragmented fingerprint entries, and useless fingerprint entries. A container with utilization smaller than a given threshold is called a sparse container . Fingerprint entries that point to non-sparse containers are hot fingerprint entries. For the remaining fingerprint entries, if a fingerprint entry matches any fingerprint of forthcoming backup chunks, it is classified as a fragmented fingerprint entry. Otherwise, it is classified as a useless fingerprint entry. We observe that hot fingerprint entries account for a small part of the index, whereas the remaining fingerprint entries account for the majority of the index. This intriguing observation inspires us to develop a hot fingerprint entry distilling approach named HID . HID segregates useless fingerprint entries from the index to improve memory utilization and bypass disk accesses. In addition, HID separates fragmented fingerprint entries to make a deduplication-based backup system directly rewrite fragmented chunks, thereby alleviating adverse fragmentation. Moreover, HID introduces a feature to treat fragmented chunks as unique chunks. This feature compensates for the shortcoming that a Bloom filter cannot directly identify certain duplicated chunks (i.e., the fragmented chunks). To take full advantage of the preceding feature, we propose an evolved HID strategy called EHID . EHID incorporates a Bloom filter, to which only hot fingerprints are mapped. In doing so, EHID exhibits two salient features: (i) EHID avoids disk accesses to identify unique chunks and the fragmented chunks; (ii) EHID slashes the false positive rate of the integrated Bloom filter. These salient features push EHID into the high-efficiency mode. Our experimental results show our approach reduces the average memory overhead of the index by 34.11% and 25.13% when using the Linux dataset and the FSL dataset, respectively. Furthermore, compared with the state-of-the-art method HAR, EHID boosts the average backup throughput by up to a factor of 2.25 with the Linux dataset, and EHID reduces the average disk I/O traffic by up to 66.21% when it comes to the FSL dataset. EHID also marginally improves the system's restore performance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3207131661",
    "type": "article"
  },
  {
    "title": "Reprogramming 3D TLC Flash Memory based Solid State Drives",
    "doi": "https://doi.org/10.1145/3487064",
    "publication_date": "2022-01-29",
    "publication_year": 2022,
    "authors": "Congming Gao; Min Ye; Chun Jason Xue; Youtao Zhang; Liang Shi; Jiwu Shu; Jun Yang",
    "corresponding_authors": "",
    "abstract": "NAND flash memory-based SSDs have been widely adopted. The scaling of SSD has evolved from plannar (2D) to 3D stacking. For reliability and other reasons, the technology node in 3D NAND SSD is larger than in 2D, but data density can be increased via increasing bit-per-cell. In this work, we develop a novel reprogramming scheme for TLCs in 3D NAND SSD, such that a cell can be programmed and reprogrammed several times before it is erased. Such reprogramming can improve the endurance of a cell and the speed of programming, and increase the amount of bits written in a cell per program/erase cycle, i.e., effective capacity. Our work is the first to perform a real 3D NAND SSD test to validate the feasibility of the reprogram operation. From the collected data, we derive the restrictions of performing reprogramming due to reliability challenges. Furthermore, a reprogrammable SSD (ReSSD) is designed to structure reprogram operations. ReSSD is evaluated in a case study in RAID 5 system (RSS-RAID). Experimental results show that RSS-RAID can improve the endurance by 35.7%, boost write performance by 15.9%, and increase effective capacity by 7.71%, with negligible overhead compared with conventional 3D SSD-based RAID 5 system.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4210430893",
    "type": "article"
  },
  {
    "title": "InDe: An Inline Data Deduplication Approach via Adaptive Detection of Valid Container Utilization",
    "doi": "https://doi.org/10.1145/3568426",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Lifang Lin; Yuhui Deng; Yi Zhou; Yifeng Zhu",
    "corresponding_authors": "",
    "abstract": "Inline deduplication removes redundant data in real-time as data is being sent to the storage system. However, it causes data fragmentation: logically consecutive chunks are physically scattered across various containers after data deduplication. Many rewrite algorithms aim to alleviate the performance degradation due to fragmentation by rewriting fragmented duplicate chunks as unique chunks into new containers. Unfortunately, these algorithms determine whether a chunk is fragmented based on a simple pre-set fixed value, ignoring the variance of data characteristics between data segments. Accordingly, when backups are restored, they often fail to select an appropriate set of old containers for rewrite, generating a substantial number of invalid chunks in retrieved containers. To address this issue, we propose an inline deduplication approach for storage systems, called InDe , which uses a greedy algorithm to detect valid container utilization and dynamically adjusts the number of old container references in each segment. InDe fully leverages the distribution of duplicated chunks to improve the restore performance while maintaining high backup performance. We define an effectiveness metric, valid container referenced counts (VCRC) , to identify appropriate containers for the rewrite. We design a rewrite algorithm F-greedy that detects valid container utilization to rewrite low-VCRC containers. According to the VCRC distribution of containers, F-greedy dynamically adjusts the number of old container references to only share duplicate chunks with high-utilization containers for each segment, thereby improving the restore speed. To take full advantage of the above features, we further propose another rewrite algorithm called F-greedy+ based on adaptive interval detection of valid container utilization. F-greedy+ makes a more accurate estimation of the valid utilization of old containers by detecting trends of VCRC’s change in two directions and selecting referenced containers in the global scope. We quantitatively evaluate InDe using three real-world backup workloads. The experimental results show that compared with two state-of-the-art algorithms (Capping and SMR), our scheme improves the restore speed by 1.3×–2.4× while achieving almost the same backup performance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4309561352",
    "type": "article"
  },
  {
    "title": "Efficient Crash Consistency for NVMe over PCIe and RDMA",
    "doi": "https://doi.org/10.1145/3568428",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Xiaojian Liao; Youyou Lu; Zhe Yang; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "This article presents crash-consistent Non-Volatile Memory Express (ccNVMe), a novel extension of the NVMe that defines how host software communicates with the non-volatile memory (e.g., solid-state drive) across a PCI Express bus and RDMA-capable networks with both crash consistency and performance efficiency. Existing storage systems pay a huge tax on crash consistency, and thus cannot fully exploit the multi-queue parallelism and low latency of the NVMe and RDMA interfaces. ccNVMe alleviates this major bottleneck by coupling the crash consistency to the data dissemination. This new idea allows the storage system to achieve crash consistency by taking the free rides of the data dissemination mechanism of NVMe, using only two lightweight memory-mapped I/Os (MMIOs), unlike traditional systems that use complex update protocol and synchronized block I/Os. ccNVMe introduces a series of techniques including transaction-aware MMIO/doorbell and I/O command coalescing to reduce the PCIe traffic as well as to provide atomicity. We present how to build a high-performance and crash-consistent file system named MQFS atop ccNVMe. We experimentally show that MQFS increases the IOPS of RocksDB by 36% and 28% compared to a state-of-the-art file system and Ext4 without journaling, respectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4309561846",
    "type": "article"
  },
  {
    "title": "Reliability Evaluation of Erasure-coded Storage Systems with Latent Errors",
    "doi": "https://doi.org/10.1145/3568313",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Ilias Iliadis",
    "corresponding_authors": "Ilias Iliadis",
    "abstract": "Large-scale storage systems employ erasure-coding redundancy schemes to protect against device failures. The adverse effect of latent sector errors on the Mean Time to Data Loss (MTTDL) and the Expected Annual Fraction of Data Loss (EAFDL) reliability metrics is evaluated. A theoretical model capturing the effect of latent errors and device failures is developed, and closed-form expressions for the metrics of interest are derived. The MTTDL and EAFDL of erasure-coded systems are obtained analytically for (i) the entire range of bit error rates; (ii) the symmetric, clustered, and declustered data placement schemes; and (iii) arbitrary device failure and rebuild time distributions under network rebuild bandwidth constraints. The range of error rates that deteriorate system reliability is derived analytically. For realistic values of sector error rates, the results obtained demonstrate that MTTDL degrades, whereas, for moderate erasure codes, EAFDL remains practically unaffected. It is demonstrated that, in the range of typical sector error rates and for very powerful erasure codes, EAFDL degrades as well. It is also shown that the declustered data placement scheme offers superior reliability.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4309562626",
    "type": "article"
  },
  {
    "title": "KVRangeDB: Range Queries for a Hash-based Key–Value Device",
    "doi": "https://doi.org/10.1145/3582013",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Mian Qin; Qing Zheng; Jason Lee; Bradley W. Settlemyer; Fei Wen; Narasimha Reddy; Paul V. Gratz",
    "corresponding_authors": "",
    "abstract": "Key–value (KV) software has proven useful to a wide variety of applications including analytics, time-series databases, and distributed file systems. To satisfy the requirements of diverse workloads, KV stores have been carefully tailored to best match the performance characteristics of underlying solid-state block devices. Emerging KV storage device is a promising technology for both simplifying the KV software stack and improving the performance of persistent storage-based applications. However, while providing fast, predictable put and get operations, existing KV storage devices do not natively support range queries that are critical to all three types of applications described above. In this article, we present KVRangeDB, a software layer that enables processing range queries for existing hash-based KV solid-state disks (KVSSDs). As an effort to adapt to the performance characteristics of emerging KVSSDs, KVRangeDB implements log-structured merge tree key index that reduces compaction I/O, merges keys when possible, and provides separate caches for indexes and values. We evaluated the KVRangeDB under a set of representative workloads, and compared its performance with two existing database solutions: a Rocksdb variant ported to work with the KVSSD, and Wisckey, a key–value database that is carefully tuned for conventional block devices. On filesystem aging workloads, KVRangeDB outperforms Wisckey by 23.7× in terms of throughput and reduce CPU usage and external write amplifications by 14.3× and 9.8×, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4317659183",
    "type": "article"
  },
  {
    "title": "Owner-free Distributed Symmetric Searchable Encryption Supporting Conjunctive Queries",
    "doi": "https://doi.org/10.1145/3607255",
    "publication_date": "2023-07-05",
    "publication_year": 2023,
    "authors": "Qiuyun Tong; Xinghua Li; Yinbin Miao; Yunwei Wang; Ximeng Liu; Robert H. Deng",
    "corresponding_authors": "",
    "abstract": "Symmetric Searchable Encryption (SSE), as an ideal primitive, can ensure data privacy while supporting retrieval over encrypted data. However, existing multi-user SSE schemes require the data owner to share the secret key with all query users or always be online to generate search tokens. While there are some solutions to this problem, they have at least one weakness, such as non-supporting conjunctive query, result decryption assistance of the data owner, and unauthorized access. To solve the above issues, we propose an O wner-free Di stributed S ymmetric searchable encryption supporting C onjunctive query (ODiSC). Specifically, we first evaluate the Learning-Parity-with-Noise weak Pseudorandom Function (LPN-wPRF) in dual-cloud architecture to generate search tokens with the data owner free from sharing key and being online. Then, we provide fine-grained conjunctive query in the distributed architecture using additive secret sharing and symmetric-key hidden vector encryption. Finally, formal security analysis and empirical performance evaluation demonstrate that ODiSC is adaptively simulation-secure and efficient.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4383265341",
    "type": "article"
  },
  {
    "title": "FASTSync: A FAST Delta Sync Scheme for Encrypted Cloud Storage in High-bandwidth Network Environments",
    "doi": "https://doi.org/10.1145/3607536",
    "publication_date": "2023-07-07",
    "publication_year": 2023,
    "authors": "Suzhen Wu; Zhanhong Tu; Yuxuan Zhou; Zuocheng Wang; Zhirong Shen; Wei Chen; Wei Wang; Weichun Wang; Bo Mao",
    "corresponding_authors": "",
    "abstract": "More and more data are stored in cloud storage, which brings two major challenges. First, the modified files in the cloud should be quickly synchronized to ensure data consistency, e.g., delta synchronization (sync) achieves efficient cloud sync by synchronizing only the updated part of the file. Second, the huge data in the cloud needs to be deduplicated and encrypted, e.g., Message-Locked Encryption (MLE) implements data deduplication by encrypting the content among different users. However, when combined, a few updates in the content can cause large sync traffic amplification for both keys and ciphertext in the MLE-based cloud storage, significantly degrading the cloud sync efficiency. A feature-based encryption sync scheme, FeatureSync, is proposed to address the delta amplification problem. However, with further improvement of the network bandwidth, the performance of FeatureSync stagnates. In our preliminary experimental evaluations, we find that the bottleneck of the computational overhead in the high-bandwidth network environments is the main bottleneck in FeatureSync. In this article, we propose an enhanced feature-based encryption sync scheme FASTSync to optimize the performance of FeatureSync in high-bandwidth network environments. The performance evaluations on a lightweight prototype implementation of FASTSync show that FASTSync reduces the cloud sync time by 70.3% and the encryption time by 37.3%, on average, compared with FeatureSync.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4383556186",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX FAST 2024",
    "doi": "https://doi.org/10.1145/3716633",
    "publication_date": "2025-02-22",
    "publication_year": 2025,
    "authors": "Xiaosong Ma; Youjip Won",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407849692",
    "type": "article"
  },
  {
    "title": "ZonesDB: Building Write-Optimized and Space-Adaptive Key-Value Store on Zoned Storage with Fragmented LSM Tree",
    "doi": "https://doi.org/10.1145/3715331",
    "publication_date": "2025-03-03",
    "publication_year": 2025,
    "authors": "Liang Yu-hong; Yingjia Wang; Tsun-Yu Yang; Matias Bjørling; Ming-Chang Yang",
    "corresponding_authors": "",
    "abstract": "The zoned storage has revolutionized the decades-old block storage in lowering the cost-per-gigabyte while enabling the host system to achieve better performance. With such benefit of cost and performance, we still require careful consideration on the endurance when deploying the applications on the zoned storage, since the modern storage tends to trade its endurance for larger capacity at lower cost. In this regard, although previous studies have deployed the log-structure merge (LSM-tree) based key-value (KV) store on the zoned storage, the LSM-tree based KV store can be suboptimal choice to build a cost-effective KV store on zoned storage since LSM-tree has a well-known problem of write amplification (WA). Therefore, based on the key insight that the Fragmented Log-Structured Merge tree (FLSM-tree) substantially alleviates the notorious write amplification problem of the classical LSM-tree and inherently complies with the sequential write constraint of zoned storage, FLSM-tree would be a promising design choice to build a cost-effective KV store on zoned storage. However, based on our investigation, deploying an FLSM-tree based KV store on zoned storage faces two challenges: the write amplification of the host-initiated garbage collection (GC) cancels out the low WA merit of FLSM-tree and FLSM-tree results in high space amplification to increase the cost. In this regard, this paper presents ZonesDB , a novel FLSM-tree based KV store that comes with a series of innovative “zone-aware” techniques for pursuing write optimality and space adaptability. Our evaluations, based on two types of production-grade zoned storage (i.e., ZNS SSD and HM-SMR HDD), reveal that ZonesDB can bring into play the low WA merit of FLSM-tree, deliver outstanding write performance, and mitigate the space amplification problem of FLSM-tree on zoned storage.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408116213",
    "type": "article"
  },
  {
    "title": "Towards Agile and Judicious Metadata Load Balancing for Ceph File System via Matrix-based Modeling",
    "doi": "https://doi.org/10.1145/3721483",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Xinyang Shao; Yiduo Wang; Cheng Li; Hengyu Liang; C. Y. Wang; Feng Yan; Yinlong Xu",
    "corresponding_authors": "",
    "abstract": "To scale out the massive metadata access, the Ceph distributed file system (CephFS) adopts a dynamic subtree partitioning method, splitting the hierarchical namespace and distributing subtrees across multiple metadata servers. However, this method suffers from a severe imbalance problem that may result in poor performance due to its inaccurate imbalance prediction, ignorance of workload characteristics, and unnecessary/invalid migration activities. To eliminate these inefficiencies, we propose Lunule, a novel CephFS metadata load balancer, which employs an imbalance factor model for accurately determining when to trigger re-balance and tolerate unharmful imbalanced situations. Lunule further adopts a workload-aware migration planner to appropriately select subtree migration candidates. Finally, we extend Lunule to Lunule + , which models metadata accesses into matrices, and employs matrix-based formulas for more accurate load prediction and re-balance decision. Compared to baselines, Lunule achieves better load balance, increases the metadata throughput by up to 315.8%, and shortens the tail job completion time by up to 64.6% for five real-world workloads and their mixture, respectively. Besides, Lunule is capable of handling the metadata cluster expansion and the workload growth, and scales linearly on a 16-node cluster. Compared to Lunule, Lunule + achieves up to 64.96% better metadata load balance, and 13.53-86.09% higher throughput.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408217336",
    "type": "article"
  },
  {
    "title": "An Efficient Delta Compression Framework Seamlessly Integrated into Inline Deduplication",
    "doi": "https://doi.org/10.1145/3721485",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Yucheng Zhang; Wenbin Zeng; Hong Jiang; Dan Feng; Zichen Xu; Shuibing He; Mingzhe Zhang; Dan Wu",
    "corresponding_authors": "",
    "abstract": "Delta compression can complement data deduplication by further minimizing redundancy through the compression of non-duplicate data chunks. When adding delta compression to deduplication-based backup systems, however, two primary challenges arise that degrade performance of inline deduplication. First, extra I/Os are introduced along the critical paths of backup and restoration for retrieving base chunks, slowing the system. Second, rewriting techniques prohibit specific data chunks from serving as base chunks for delta compression to improve restore performance, resulting in a loss of compression efficiency. In this paper, we introduce LoopDelta, a framework that seamlessly integrates delta compression into inline deduplication for backup storage, addressing the aforementioned challenges by using three techniques: (1) dual-locality-based similarity tracking leverages both logical and physical locality to detect most of the similar chunks, which, due to their locality, can be prefetched by piggybacking on routine operations during deduplication, thereby eliminating extra I/Os during backup; (2) cache-aware filter identifies base chunks requiring extra I/Os during restore and prevents their referencing, thus eliminating extra restore I/Os; and (3) inversed delta compression, which reverses the roles of base and target chunks in the traditional delta compression approach, thereby allowing for the delta compression of data chunks that are otherwise prohibited as base chunks due to rewriting techniques. Experiments show that LoopDelta increases the compression ratio by 1.28 to 11.33 times over basic deduplication, without significantly affecting backup throughput, and enhances restore performance by up to 3.57 times.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408218562",
    "type": "article"
  },
  {
    "title": "Building Stream-based Page Cache to Accelerate File Scanning on Fast Storage Devices",
    "doi": "https://doi.org/10.1145/3721486",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Zhiyue Li; Guangyan Zhang",
    "corresponding_authors": "",
    "abstract": "Buffered I/O via page cache is used for file scanning in many cases as page cache can provide buffering, data aggregation, I/O alignment, and prefetching transparently. However, our study indicates that employing page cache for file scanning on fast storage devices presents two performance issues: it offers limited I/O bandwidth that does not align with the performance of fast storage devices, and the intensive background writeback onto fast storage devices can significantly interfere with foreground I/O requests. In this paper, we propose StreamCache, a new page cache management system for file scanning on fast storage devices. StreamCache exploits three techniques to achieve high I/O performance. First, it uses a two-layer memory management method to accelerate page allocation by leveraging CPU cache locality. Second, it uses a stream-based page reclaiming method to lower the interference to foreground I/O requests. Third, it uses a lightweight stream tracking method to record the states of cached pages at the granularity of sequential streams to support stream-based page reclaiming. We implement StreamCache in XFS. Experimental results show that compared with existing methods, StreamCache can increase the I/O bandwidth of scientific applications by 44%, and reduce the checkpoint/restart time of large language models by 15.7% on average.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408220657",
    "type": "article"
  },
  {
    "title": "Advancing Archival Data Storage: The Promises and Challenges of DNA Storage System",
    "doi": "https://doi.org/10.1145/3723166",
    "publication_date": "2025-03-13",
    "publication_year": 2025,
    "authors": "Alex Sensintaffar; Yixun Wei; Li Ou; David H. C. Du; Bingzhe Li",
    "corresponding_authors": "",
    "abstract": "As the volume of data is rapidly produced every day, there is a need for the storage media to keep up with the growth rate of digital data created. Despite emerging storage solutions that have been proposed such as Solid State Drive (SSD) with quad-level cells (QLC) or penta-level cells (PLC), Shingled Magnetic Recording (SMR), LTO-tape, etc., these technologies still fall short of meeting the demand for preserving huge amounts of available data. Moreover, current storage solutions have a limited lifespan, often lasting just a few years. To ensure long-term preservation, data must be continuously migrated to new storage drives. Therefore, there is a need for alternative storage technologies that not only offer high storage capacity but also long persistency. In contrast to existing storage devices, Synthetic Deoxyribonucleic Acid (DNA) storage emerges as a promising candidate for archival data storage, offering both high-density storage capacity and the potential for long-term data preservation. In this paper, we will introduce DNA storage, discuss the capabilities of DNA storage based on the current biotechnologies, discuss possible improvements in DNA storage, and explore further improvements with future technologies. Currently, the limitations of DNA storage are due to its weaknesses including high error rates, long access latency, etc. In this paper, we will focus on possible DNA storage research issues based on its relevant bio and computer technologies. Also, we will provide potential solutions and forward-looking predictions about the development and the future of DNA storage. We will discuss DNA storage from the following five perspectives: 1) We will describe the basic background of DNA storage including the basic technologies of read/write DNA storage, data access processes such as Polymerase Chain Reaction (PCR) based random access, encoding schemes from digital data to DNA, and required DNA storage format. 2) We will describe the issues of DNA storage based on the current technologies including bio-constraints during the encoding process such as avoiding long homopolymers and containing certain GC contents, different types of errors in synthesis and sequencing processes, low practical capacity with the current technologies, slow read and write performance, and low encoding density for random accesses. 3) Based on the previously mentioned issues, we will summarize the current solutions for each issue, and also give and discuss the potential solutions based on the future technologies. 4) From a system perspective, we will discuss how the DNA storage system will look if the DNA storage becomes commercialized and is widely equipped in archive systems. Some questions will be discussed including i) How to efficiently index data in DNA storage? ii) What is a good storage hierarchical storage system with DNA storage? iii) What will DNA storage be like with the development of technology? 5) Finally, we will provide a comparison with other competitive technologies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408412593",
    "type": "article"
  },
  {
    "title": "Reflecting on the Past 17 Years of Shingled Magnetic Recording for Insights into Future Disk Transitions: A Survey",
    "doi": "https://doi.org/10.1145/3731453",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "Babar Khan; Andreas Koch",
    "corresponding_authors": "",
    "abstract": "Shingled magnetic recording (SMR) is a data storage recording technology used in modern hard disk drives (HDDs) to increase the areal density capacity (ADC) of underlying media. The research on SMR drives began around 2008, with the first SMR disk entering the market in 2013. We have performed an extensive survey on SMR research, encompassing over 100 scientific research papers spanning nearly 17 years. Our survey offers an in-depth analysis of the evolution of SMR disks, examining the different types of SMR architectures and the inherent performance challenges in existing SMR disks. We have also explored how SMR technology integrates with data storage solutions like RAID and Deduplication, including an examination of real-world use cases where hyperscalers have successfully leveraged SMR for large-scale data management. Furthermore, as storage demands continue to escalate, there is a notable shift from various HDD technologies towards Heat-Assisted Magnetic Recording (HAMR) disks, offering potential for increased storage densities beyond 1.5 Tbit / in 2 . To this end, our survey also briefly discusses the research contributing to two specific HAMR variants such as Shingled-HAMR and Heat Interlaced Magnetic Recording (HIMR).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409700971",
    "type": "article"
  },
  {
    "title": "JMStore: Joint Optimization of Computation and Storage Balancing in Multi-NDP Key-Value Stores with Hash-Based Data Distribution",
    "doi": "https://doi.org/10.1145/3744568",
    "publication_date": "2025-06-14",
    "publication_year": 2025,
    "authors": "Hui Sun; Huang Jh; Bo Chen; Yinliang Yue; Xiao Qin",
    "corresponding_authors": "",
    "abstract": "It is challenging to store and process massive unstructured data for key-value storage systems that require high concurrency, high performance, and low latency. Log-Structured Merge (LSM) trees-based Key-value stores or KV stores are widely adopted for enhanced write performance. Existing KV stores are primarily deployed on a CPU-centric architecture, which necessitates moving data to the CPU from memory or storage devices for processing. This is particularly problematic during the compaction process, which involves substantial data movement and rewrite. This tradition consumes bandwidth and computational resources, leading to write amplification and impairing system performance. Near-Data Processing (NDP) devices mitigate this issue by processing data at the storage location, thereby reducing data movement cost. Recognizing that the computational power of a single NDP device is insufficient for the demands of large-scale unstructured data processing, we propose JMStore – a multi-NDP key-value store based on a hash data organization. By offloading computational tasks to multiple NDP devices, JMStore collaboratively optimizes the compaction process to address the data movement issue as well as the mismatch between large-scale in workloads data and computational power on an NDP. We design a key-value store programming model and data organization for a multi-NDP architecture, enabling the system to leverage the hardware efficiency and parallelism of multiple NDP devices to significantly optimize system performance. We also propose a strategy to balance storage and computation resources under a hash layout. Compared to the latest single NDP KV store (PStore) and the multi-NDP KV store (MStore), JMStore demonstrates significant performance improvements under DB_Bench and YCSB-C with read-write mixed workloads: the peak improvements can reach a factor of 10.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411309339",
    "type": "article"
  },
  {
    "title": "P2Cache: Enhancing Data-Centric Applications via Application-Guided Management of OS Page Caches",
    "doi": "https://doi.org/10.1145/3736586",
    "publication_date": "2025-06-14",
    "publication_year": 2025,
    "authors": "Dusol Lee; I. H. Choi; Chanyoung Lee; Hyungsoo Jung; Jihong Kim",
    "corresponding_authors": "",
    "abstract": "Data-centric applications perform tasks that require intensive data processing and ample memory resources. These tasks have varying I/O access patterns, significantly impacted by the OS cache. Therefore, it is desirable to enable application-specific cache management without compromising memory efficiency. However, infusing user-level policies into the OS cache management is challenging because it is difficult to communicate application-level I/O semantics and access patterns to general-purpose OSs. This article addresses the challenge by enabling applications to safely convey their I/O semantics to OSs via eBPF, allowing for more application-specific control over the OS cache. To this end, we introduce P2Cache , a programmable OS page cache. P2Cache extends the Linux page cache with three new probe points (i.e., eviction , prefetching , and swapping ) that can support application-directed custom policies on OS cache management using eBPF programs. Our experimental results showed that P2Cache significantly enhanced the performance of an LLM inference, a graph processing application, and a database by up to 230%, 49%, and 18%, respectively, with minimal effort.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411309349",
    "type": "article"
  },
  {
    "title": "Looking Back to Move Forward: Unveiling the Mysteries of HBM Errors to Predict Future Failures",
    "doi": "https://doi.org/10.1145/3767333",
    "publication_date": "2025-09-12",
    "publication_year": 2025,
    "authors": "Shuyue Zhou; X. Hu; Ronglong Wu; Jiahao Lu; Zhirong Shen; Zikang Xu; Yue Yu; Yuze Jiang; Jiwu Shu; Kunlin Yang; F. Lin; Yiming Zhang",
    "corresponding_authors": "",
    "abstract": "High-bandwidth memory (HBM) is regarded as a promising technology for fundamentally overcoming the memory wall. It stacks up multiple DRAM dies vertically to dramatically improve the memory access bandwidth. However, this architecture also comes with more severe reliability issues, since HBM not only inherits error patterns of the conventional DRAM, but also introduces new error causes. In this paper, we conduct the first systematical study on HBM errors, which cover over 460 million error events collected from nineteen data centers and span over two years of deployment under a variety of services. Through error analyses and methodology validations, we confirm that the HBM exhibits different error patterns from conventional DRAM, in terms of spatial locality, temporal correlation, and sensor metrics which make empirical prediction models for DRAM error prediction ineffective for HBM. We design and implement Calchas , a hierarchical failure prediction framework for HBM based on our findings, which integrate spatial, temporal, and sensor information from various device levels to predict upcoming failures. The results demonstrate the feasibility of failure prediction across hierarchical levels.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414592239",
    "type": "article"
  },
  {
    "title": "Network file storage with graceful performance degradation",
    "doi": "https://doi.org/10.1145/1063786.1063788",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Anxiao Jiang; Jehoshua Bruck",
    "corresponding_authors": "",
    "abstract": "A file storage scheme is proposed for networks containing heterogeneous clients. In the scheme, the performance measured by file-retrieval delays degrades gracefully under increasingly serious faulty circumstances. The scheme combines coding with storage for better performance. The problem is NP-hard for general networks; and this article focuses on tree networks with asymmetric edges between adjacent nodes. A polynomial-time memory-allocation algorithm is presented, which determines how much data to store on each node, with the objective of minimizing the total amount of data stored in the network. Then a polynomial-time data-interleaving algorithm is used to determine which data to store on each node for satisfying the quality-of-service requirements in the scheme. By combining the memory-allocation algorithm with the data-interleaving algorithm, an optimal solution to realize the file storage scheme in tree networks is established.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2048902074",
    "type": "article"
  },
  {
    "title": "DISP",
    "doi": "https://doi.org/10.1145/1044956.1044960",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Daniel Ellard; James Megquier",
    "corresponding_authors": "",
    "abstract": "DISP is a practical client-server protocol for the distributed storage of immutable data objects. Unlike most other contemporary protocols, DISP permits applications to make explicit tradeoffs between total storage space, computational overhead, and guarantees of availability, integrity, and privacy on a per-object basis. Applications specify the degree of redundancy with which each item is encoded, what level of integrity checks are computed and stored with each item, and whether items are stored in an encrypted format. At one extreme, clients willing to pay the overhead are guaranteed privacy, integrity, and availability of data stored in the system as long as fewer than half the servers are Byzantine. At the other extreme, objects that do not require privacy or integrity in the face of Byzantine servers can be stored with very low computational and storage overhead.DISP is efficient in terms of message count, message size, and storage requirements: even in the worst case, the read and write protocols require a number of messages that are linear with respect to the number of servers. In terms of message size, DISP requires transferring only marginally more than L bytes to correctly read an object of size L , even in the face of Byzantine server failures. In this article we provide a description of DISP and an analysis of its fault-tolerant properties. We also analyze the complexity of the protocol and discuss several potential applications. We conclude with a description of our prototype implementation and measurements of its performance on commodity hardware.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2158487821",
    "type": "article"
  },
  {
    "title": "SOPA",
    "doi": "https://doi.org/10.1145/1807060.1807064",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Yang Wang; Jiwu Shu; Guangyan Zhang; Wei Xue; Weimin Zheng",
    "corresponding_authors": "",
    "abstract": "With the development of storage technology and applications, new caching policies are continuously being introduced. It becomes increasingly important for storage systems to be able to select the matched caching policy dynamically under varying workloads. This article proposes SOPA, a cache framework to adaptively select the matched policy and perform policy switches in storage systems. SOPA encapsulates the functions of a caching policy into a module, and enables online policy switching by policy reconstruction. SOPA then selects the policy matched with the workload dynamically by collecting and analyzing access traces. To reduce the decision-making cost, SOPA proposes an asynchronous decision making process. The simulation experiments show that no single caching policy performed well under all of the different workloads. With SOPA, a storage system could select the appropriate policy for different workloads. The real-system evaluation results show that SOPA reduced the average response time by up to 20.3% and 11.9% compared with LRU and ARC, respectively.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2085370226",
    "type": "article"
  },
  {
    "title": "A Low-cost Disk Solution Enabling LSM-tree to Achieve High Performance for Mixed Read/Write Workloads",
    "doi": "https://doi.org/10.1145/3162615",
    "publication_date": "2018-04-12",
    "publication_year": 2018,
    "authors": "Dejun Teng; Lei Guo; Rubao Lee; Chen Feng; Yanfeng Zhang; Siyuan Ma; Xiaodong Zhang",
    "corresponding_authors": "",
    "abstract": "LSM-tree has been widely used in data management production systems for write-intensive workloads. However, as read and write workloads co-exist under LSM-tree, data accesses can experience long latency and low throughput due to the interferences to buffer caching from the compaction, a major and frequent operation in LSM-tree. After a compaction, the existing data blocks are reorganized and written to other locations on disks. As a result, the related data blocks that have been loaded in the buffer cache are invalidated since their referencing addresses are changed, causing serious performance degradations. To re-enable high-speed buffer caching during intensive writes, we propose Log-Structured buffered-Merge tree (simplified as LSbM-tree) by adding a compaction buffer on disks to minimize the cache invalidations on buffer cache caused by compactions. The compaction buffer efficiently and adaptively maintains the frequently visited datasets. In LSbM, strong locality objects can be effectively kept in the buffer cache with minimum or no harmful invalidations. With the help of a small on-disk compaction buffer, LSbM achieves a high query performance by enabling effective buffer caching, while retaining all the merits of LSM-tree for write-intensive data processing and providing high bandwidth of disks for range queries. We have implemented LSbM based on LevelDB. We show that with a standard buffer cache and a hard disk, LSbM can achieve 2x performance improvement over LevelDB. We have also compared LSbM with other existing solutions to show its strong cache effectiveness.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2797922595",
    "type": "article"
  },
  {
    "title": "ROS",
    "doi": "https://doi.org/10.1145/3231599",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Wenrui Yan; Jie Yao; Qiang Cao; Changsheng Xie; Hong Jiang",
    "corresponding_authors": "",
    "abstract": "The combination of the explosive growth in digital data and the demand to preserve much of these data in the long term has made it imperative to find a more cost-effective way than HDD arrays and a more easily accessible way than tape libraries to store massive amounts of data. While modern optical discs are capable of guaranteeing more than 50-year data preservation without media replacement, individual optical discs’ lack of the performance and capacity relative to HDDs or tapes has significantly limited their use in datacenters. This article presents a Rack-scale Optical disc library System, or ROS in short, which provides a PB-level total capacity and inline accessibility on thousands of optical discs built within a 42U Rack. A rotatable roller and robotic arm separating and fetching discs are designed to improve disc placement density and simplify the mechanical structure. A hierarchical storage system based on SSDs, hard disks, and optical discs is proposed to effectively hide the delay of mechanical operation. However, an optical library file system (OLFS) based on FUSE is proposed to schedule mechanical operation and organize data on the tiered storage with a POSIX user interface to provide an illusion of inline data accessibility. We further optimize OLFS by reducing unnecessary user/kernel context switches inheriting from legacy FUSE framework. We evaluate ROS on a few key performance metrics, including operation delays of the mechanical structure and software overhead in a prototype PB-level ROS system. The results show that ROS stacked on Samba and FUSE as network-attached storage (NAS) mode almost saturates the throughput provided by underlying samba via 10GbE network for external users, as well as in this scenario provides about 53ms file write and 15ms read latency, exhibiting its inline accessibility. Besides, ROS is able to effectively hide and virtualize internal complex operational behaviors and be easily deployable in datacenters.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2901314489",
    "type": "article"
  },
  {
    "title": "Agility and Performance in Elastic Distributed Storage",
    "doi": "https://doi.org/10.1145/2668129",
    "publication_date": "2014-10-31",
    "publication_year": 2014,
    "authors": "Lianghong Xu; James Cipar; Elie Krevat; Alexey Tumanov; Nitin Gupta; Michael A. Kozuch; Gregory R. Ganger",
    "corresponding_authors": "",
    "abstract": "Elastic storage systems can be expanded or contracted to meet current demand, allowing servers to be turned off or used for other tasks. However, the usefulness of an elastic distributed storage system is limited by its agility: how quickly it can increase or decrease its number of servers. Due to the large amount of data they must migrate during elastic resizing, state of the art designs usually have to make painful trade-offs among performance, elasticity, and agility. This article describes the state of the art in elastic storage and a new system, called SpringFS, that can quickly change its number of active servers, while retaining elasticity and performance goals. SpringFS uses a novel technique, termed bounded write offloading , that restricts the set of servers where writes to overloaded servers are redirected. This technique, combined with the read offloading and passive migration policies used in SpringFS, minimizes the work needed before deactivation or activation of servers. Analysis of real-world traces from Hadoop deployments at Facebook and various Cloudera customers and experiments with the SpringFS prototype confirm SpringFS’s agility, show that it reduces the amount of data migrated for elastic resizing by up to two orders of magnitude, and show that it cuts the percentage of active servers required by 67--82%, outdoing state-of-the-art designs by 6--120%.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1966148203",
    "type": "article"
  },
  {
    "title": "ExaPlan",
    "doi": "https://doi.org/10.1145/3078839",
    "publication_date": "2017-05-22",
    "publication_year": 2017,
    "authors": "Ilias Iliadis; Jens Jelitto; Yusik Kim; Slaviša Sarafijanović; Vinodh Venkatesan",
    "corresponding_authors": "",
    "abstract": "Multi-tiered storage, where each tier consists of one type of storage device (e.g., SSD, HDD, or disk arrays), is a commonly used approach to achieve both high performance and cost efficiency in large-scale systems that need to store data with vastly different access characteristics. By aligning the access characteristics of the data, either fixed-sized extents or variable-sized files, to the characteristics of the storage devices, a higher performance can be achieved for any given cost. This article presents ExaPlan, a method to determine both the data-to-tier assignment and the number of devices in each tier that minimize the system’s mean response time for a given budget and workload. In contrast to other methods that constrain or minimize the system load, ExaPlan directly minimizes the system’s mean response time estimated by a queueing model. Minimizing the mean response time is typically intractable as the resulting optimization problem is both nonconvex and combinatorial in nature. ExaPlan circumvents this intractability by introducing a parameterized data placement approach that makes it a highly scalable method that can be easily applied to exascale systems. Through experiments that use parameters from real-world storage systems, such as CERN and LOFAR, it is demonstrated that ExaPlan provides solutions that yield lower mean response times than previous works. It supports standalone SSDs and HDDs as well as disk arrays as storage tiers, and although it uses a static workload representation, we provide empirical evidence that underlying dynamic workloads have invariant properties that can be deemed static for the purpose of provisioning a storage system. ExaPlan is also effective as a load-balancing tool used for placing data across devices within a tier, resulting in an up to 3.6-fold reduction of response time compared with a traditional load-balancing algorithm, such as the Longest Processing Time heuristic.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2617470760",
    "type": "article"
  },
  {
    "title": "vNFS",
    "doi": "https://doi.org/10.1145/3116213",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Ming Chen; Geetika Babu Bangera; Dean Hildebrand; Farhaan Jalia; Geoff Kuenning; Henry Nelson; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "Modern systems use networks extensively, accessing both services and storage across local and remote networks. Latency is a key performance challenge, and packing multiple small operations into fewer large ones is an effective way to amortize that cost, especially after years of significant improvement in bandwidth but not latency. To this end, the NFSv4 protocol supports a compounding feature to combine multiple operations. Yet compounding has been underused since its conception because the synchronous POSIX file-system API issues only one (small) request at a time. We propose vNFS , an NFSv4.1-compliant client that exposes a vectorized high-level API and leverages NFS compound procedures to maximize performance. We designed and implemented vNFS as a user-space RPC library that supports an assortment of bulk operations on multiple files and directories. We found it easy to modify several UNIX utilities, an HTTP/2 server, and Filebench to use vNFS. We evaluated vNFS under a wide range of workloads and network latency conditions, showing that vNFS improves performance even for low-latency networks. On high-latency networks, vNFS can improve performance by as much as two orders of magnitude.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2758007819",
    "type": "article"
  },
  {
    "title": "Ouroboros Wear Leveling for NVRAM Using Hierarchical Block Migration",
    "doi": "https://doi.org/10.1145/3139530",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Qingyue Liu; Peter Varman",
    "corresponding_authors": "",
    "abstract": "Emerging nonvolatile RAM (NVRAM) technologies have a limit on the number of writes that can be made to any cell, similar to the erasure limits in NAND Flash. This motivates the need for wear leveling techniques to distribute the writes evenly among the cells. Unlike NAND Flash, cells in NVRAM can be rewritten without the need for erasing the entire containing block, avoiding the issues of space reclamation and garbage collection, motivating alternate approaches to the problem. In this article, we propose a hierarchical wear-leveling model called Ouroboros wear leveling. Ouroboros uses a two-level strategy whereby frequent low-cost intraregion wear leveling at small granularity is combined with interregion wear leveling at a larger time interval and granularity. Ouroboros is a hybrid migration scheme that exploits correct demand predictions in making better wear-leveling decisions while using randomization to avoid wear-leveling attacks by deterministic access patterns. We also propose a way to optimize wear-leveling parameter settings to meet a target smoothness level under limited time and space overhead constraints for different memory architectures and trace characteristics. Several experiments are performed on synthetically generated memory traces with special characteristics, two block-level storage traces, and two memory-line-level memory traces. The results show that Ouroboros wear leveling can distribute writes smoothly across the whole NVRAM with no more than 0.2% space overhead and 0.52% time overhead for a 512GB memory.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2768544716",
    "type": "article"
  },
  {
    "title": "Determining Data Distribution for Large Disk Enclosures with 3-D Data Templates",
    "doi": "https://doi.org/10.1145/3342858",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Guangyan Zhang; Zhufan Wang; Xiaosong Ma; Songlin Yang; Zican Huang; Weimin Zheng",
    "corresponding_authors": "",
    "abstract": "Conventional RAID solutions with fixed layouts partition large disk enclosures so that each RAID group uses its own disks exclusively. This achieves good performance isolation across underlying disk groups, at the cost of disk under-utilization and slow RAID reconstruction from disk failures. We propose RAID+, a new RAID construction mechanism that spreads both normal I/O and reconstruction workloads to a larger disk pool in a balanced manner. Unlike systems conducting randomized placement, RAID+ employs deterministic addressing enabled by the mathematical properties of mutually orthogonal Latin squares, based on which it constructs 3-D data templates mapping a logical data volume to uniformly distributed disk blocks across all disks. While the total read/write volume remains unchanged, with or without disk failures, many more disk drives participate in data service and disk reconstruction. Our evaluation with a 60-drive disk enclosure using both synthetic and real-world workloads shows that RAID+ significantly speeds up data recovery while delivering better normal I/O performance and higher multi-tenant system throughput.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2998684111",
    "type": "article"
  },
  {
    "title": "Kreon",
    "doi": "https://doi.org/10.1145/3418414",
    "publication_date": "2021-01-18",
    "publication_year": 2021,
    "authors": "Anastasios Papagiannis; Giorgos Saloustros; Giorgos Xanthakis; Giorgos Kalaentzis; Pilar González‐Férez; Angelos Bilas",
    "corresponding_authors": "",
    "abstract": "Persistent key-value stores have emerged as a main component in the data access path of modern data processing systems. However, they exhibit high CPU and I/O overhead. Nowadays, due to power limitations, it is important to reduce CPU overheads for data processing. In this article, we propose Kreon , a key-value store that targets servers with flash-based storage, where CPU overhead and I/O amplification are more significant bottlenecks compared to I/O randomness. We first observe that two significant sources of overhead in key-value stores are: (a) The use of compaction in Log-Structured Merge-Trees (LSM-Tree) that constantly perform merging and sorting of large data segments and (b) the use of an I/O cache to access devices, which incurs overhead even for data that reside in memory. To avoid these, Kreon performs data movement from level to level by using partial reorganization instead of full data reorganization via the use of a full index per-level. Kreon uses memory-mapped I/O via a custom kernel path to avoid a user-space cache. For a large dataset, Kreon reduces CPU cycles/op by up to 5.8×, reduces I/O amplification for inserts by up to 4.61×, and increases insert ops/s by up to 5.3×, compared to RocksDB.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3123211294",
    "type": "article"
  },
  {
    "title": "NVMM-Oriented Hierarchical Persistent Client Caching for Lustre",
    "doi": "https://doi.org/10.1145/3404190",
    "publication_date": "2021-01-18",
    "publication_year": 2021,
    "authors": "Wen Cheng; Chunyan Li; Lingfang Zeng; Yingjin Qian; Xi Li; André Brinkmann",
    "corresponding_authors": "",
    "abstract": "In high-performance computing (HPC), data and metadata are stored on special server nodes and client applications access the servers’ data and metadata through a network, which induces network latencies and resource contention. These server nodes are typically equipped with (slow) magnetic disks, while the client nodes store temporary data on fast SSDs or even on non-volatile main memory (NVMM). Therefore, the full potential of parallel file systems can only be reached if fast client side storage devices are included into the overall storage architecture. In this article, we propose an NVMM-based hierarchical persistent client cache for the Lustre file system (NVMM-LPCC for short). NVMM-LPCC implements two caching modes: a read and write mode (RW-NVMM-LPCC for short) and a read only mode (RO-NVMM-LPCC for short). NVMM-LPCC integrates with the Lustre Hierarchical Storage Management (HSM) solution and the Lustre layout lock mechanism to provide consistent persistent caching services for I/O applications running on client nodes, meanwhile maintaining a global unified namespace of the entire Lustre file system. The evaluation results presented in this article show that NVMM-LPCC can increase the average read throughput by up to 35.80 times and the average write throughput by up to 9.83 times compared with the native Lustre system, while providing excellent scalability.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3124083054",
    "type": "article"
  },
  {
    "title": "Checking the Integrity of Transactional Mechanisms",
    "doi": "https://doi.org/10.1145/2675113",
    "publication_date": "2014-10-31",
    "publication_year": 2014,
    "authors": "Daniel Fryer; Mike Qin; Jack Sun; Kah Wai Lee; Angela Demke Brown; Ashvin Goel",
    "corresponding_authors": "",
    "abstract": "Data corruption is the most common consequence of file-system bugs. When such corruption occurs, offline check and recovery tools must be used, but they are error prone and cause significant downtime. Previously we showed that a runtime checker for the Ext3 file system can verify that metadata updates are consistent, helping detect corruption in metadata blocks at transaction commit time. However, corruption can still occur when a bug in the file system’s transactional mechanism loses, misdirects, or corrupts writes. We show that a runtime checker must enforce the atomicity and durability properties of the file system on every write, in addition to checking transactions at commit time, to provide the strong guarantee that every block write will maintain file system consistency. We identify the invariants that need to be enforced on journaling and shadow paging file systems to preserve the integrity of committed transactions. We also describe the key properties that make it feasible to check these invariants for a file system. Based on this characterization, we have implemented runtime checkers for Ext3 and Btrfs. Our evaluation shows that both checkers detect data corruption effectively, and they can be used during normal operation with low overhead.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4240191654",
    "type": "article"
  },
  {
    "title": "Data Sorting in Flash Memory",
    "doi": "https://doi.org/10.1145/2665067",
    "publication_date": "2015-03-20",
    "publication_year": 2015,
    "authors": "Chin-Hsien Wu; Kuo-Yi Huang",
    "corresponding_authors": "",
    "abstract": "Because flash memory now provides an economical solution for various portable devices and embedded systems, an NAND flash-based storage system has replaced the hard disk drive in many applications. Recently, the implementation of database systems using an NAND flash-based storage system has become an important research topic. In particular, the external sorting is an important operation in database systems. With the very distinctive characteristics of flash memory, the typical external sorting system that adopts a clustered sorting process can result in performance degradation and reduce the reliability of flash memory. In this article, we will propose an unclustered sorting method that considers the unique characteristics of flash memory, and we then propose a decision rule to exploit the advantages of both clustered and unclustered sorting. The decision rule can separate records according to their record length, sort them appropriately by the clustered and unclustered sorting, and merge the sorted results. The experimental results show that the proposed method can improve performance in an NAND flash-based storage system (i.e., solid-state drive).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1985621987",
    "type": "article"
  },
  {
    "title": "Z-MAP",
    "doi": "https://doi.org/10.1145/2629663",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Qingsong Wei; Cheng Chen; Mingdi Xue; Jun Yang",
    "corresponding_authors": "",
    "abstract": "Existing space management and address mapping schemes for flash-based Solid-State-Drive (SSD) operate either at page or block granularity, with inevitable limitations in terms of memory requirement, performance, garbage collection, and scalability. To overcome these limitations, we proposed a novel space management and address mapping scheme for flash referred to as Z-MAP, which manages flash space at granularity of Zone. Each Zone consists of multiple numbers of flash blocks. Leveraging workload classification, Z-MAP explores Page-mapping Zone (Page Zone) to store random data and handle a large number of partial updates, and Block-mapping Zone (Block Zone) to store sequential data and lower the overall mapping table. Zones are dynamically allocated and a mapping scheme for a Zone is determined only when it is allocated. Z-MAP uses a small part of Flash memory or phase change memory as a streaming Buffer Zone to log data sequentially and migrate data into Page Zone or Block Zone based on workload classification. A two-level address mapping is designed to reduce the overall mapping table and address translation latency. Z-MAP classifies data before it is permanently stored into Flash memory so that different workloads can be isolated and garbage collection overhead can be minimized. Z-MAP has been extensively evaluated by trace-driven simulation and a prototype implementation on OpenSSD. Our benchmark results conclusively demonstrate that Z-MAP can achieve up to 76% performance improvement, 81% mapping table reduction, and 88% garbage collection overhead reduction compared to existing Flash Translation Layer (FTL) schemes.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2025528816",
    "type": "article"
  },
  {
    "title": "Storage Workload Identification",
    "doi": "https://doi.org/10.1145/2818716",
    "publication_date": "2016-05-12",
    "publication_year": 2016,
    "authors": "Jayanta Kumar Basak; Kushal Wadhwani; Kaladhar Voruganti",
    "corresponding_authors": "",
    "abstract": "Storage workload identification is the task of characterizing a workload in a storage system (more specifically, network storage system—NAS or SAN) and matching it with the previously known workloads. We refer to storage workload identification as “workload identification” in the rest of this article. Workload identification is an important problem for cloud providers to solve because (1) providers can leverage this information to colocate similar workloads to make the system more predictable and (2) providers can identify workloads and subsequently give guidance to the subscribers as to associated best practices (with respect to configuration) for provisioning those workloads. Historically, people have identified workloads by looking at their read/write ratios, random/sequential ratios, block size, and interarrival frequency. Researchers are well aware that workload characteristics change over time and that one cannot just take a point in time view of a workload, as that will incorrectly characterize workload behavior. Increasingly, manual detection of workload signature is becoming harder because (1) it is difficult for a human to detect a pattern and (2) representing a workload signature by a tuple consisting of average values for each of the signature components leads to a large error. In this article, we present workload signature detection and a matching algorithm that is able to correctly identify workload signatures and match them with other similar workload signatures. We have tested our algorithm on nine different workloads generated using publicly available traces and on real customer workloads running in the field to show the robustness of our approach.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2368868422",
    "type": "article"
  },
  {
    "title": "Bridging Software-Hardware for CXL Memory Disaggregation in Billion-Scale Nearest Neighbor Search",
    "doi": "https://doi.org/10.1145/3639471",
    "publication_date": "2024-01-06",
    "publication_year": 2024,
    "authors": "Junhyeok Jang; Hanjin Choi; Hanyeoreum Bae; Seung-Jun Lee; Miryeong Kwon; Myoungsoo Jung",
    "corresponding_authors": "",
    "abstract": "We propose CXL-ANNS , a software-hardware collaborative approach to enable scalable approximate nearest neighbor search (ANNS) services. To this end, we first disaggregate DRAM from the host via compute express link (CXL) and place all essential datasets into its memory pool. While this CXL memory pool allows ANNS to handle billion-point graphs without an accuracy loss, we observe that the search performance significantly degrades because of CXL’s far-memory-like characteristics. To address this, CXL-ANNS considers the node-level relationship and caches the neighbors in local memory, which are expected to visit most frequently. For the uncached nodes, CXL-ANNS prefetches a set of nodes most likely to visit soon by understanding the graph traversing behaviors of ANNS. CXL-ANNS is also aware of the architectural structures of the CXL interconnect network and lets different hardware components collaborate with each other for the search. Furthermore, it relaxes the execution dependency of neighbor search tasks and allows ANNS to utilize all hardware in the CXL network in parallel. Our evaluation shows that CXL-ANNS exhibits 93.3% lower query latency than state-of-the-art ANNS platforms that we tested. CXL-ANNS also outperforms an oracle ANNS system that has unlimited local DRAM capacity by 68.0%, in terms of latency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390636178",
    "type": "article"
  },
  {
    "title": "A Contract-aware and Cost-effective LSM Store for Cloud Storage with Low Latency Spikes",
    "doi": "https://doi.org/10.1145/3643851",
    "publication_date": "2024-02-20",
    "publication_year": 2024,
    "authors": "Yuanhui Zhou; Jian Zhou; Kai Lü; Ling Zhan; Peng Xu; Peng Wu; Shuning Chen; Liu Xian; Jiguang Wan",
    "corresponding_authors": "",
    "abstract": "Cloud storage is gaining popularity because features such as pay-as-you-go significantly reduce storage costs. However, the community has not sufficiently explored its contract model and latency characteristics. As LSM-Tree-based key-value stores (LSM stores) become the building block for numerous cloud applications, how cloud storage would impact the performance of key-value accesses is vital. This study reveals the significant latency variances of Amazon Elastic Block Store (EBS) under various I/O pressures, which challenges LSM store read performance on cloud storage. To reduce the corresponding tail latency, we propose Calcspar, a contract-aware LSM store for cloud storage, which efficiently addresses the challenges by regulating the rate of I/O requests to cloud storage and absorbing surplus I/O requests with the data cache. We specifically developed a fluctuation-aware cache to lower the high latency brought on by workload fluctuations. Additionally, we build a congestion-aware IOPS allocator to reduce the impact of LSM store internal operations on read latency. We evaluated Calcspar on EBS with different real-world workloads and compared it to the cutting-edge LSM stores. The results show that Calcspar can significantly reduce tail latency while maintaining regular read and write performance, keeping the 99 th percentile latency under 550μs and reducing average latency by 66%. In addition, Calcspar has lower write prices and average latency compared to Cloud NoSQL services offered by cloud vendors.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391957429",
    "type": "article"
  },
  {
    "title": "Index Shipping for Efficient Replication in LSM Key-Value Stores with Hybrid KV Placement",
    "doi": "https://doi.org/10.1145/3658672",
    "publication_date": "2024-04-16",
    "publication_year": 2024,
    "authors": "Giorgos Stylianakis; Giorgos Saloustros; Orestis Chiotakis; Giorgos Xanthakis; Angelos Bilas",
    "corresponding_authors": "",
    "abstract": "Key-value (KV) stores based on the LSM tree have become a foundational layer in the storage stack of datacenters and cloud services. Current approaches for achieving reliability and availability favor reducing network traffic and send to replicas only new KV pairs. As a result, they perform costly compactions to reorganize data in both the primary and backup nodes, which increases device I/O traffic and CPU overhead, and eventually hurts overall system performance. In this article, we describe Tebis , an efficient LSM-based KV store that reduces I/O amplification and CPU overhead for maintaining the replica index. We use a primary-backup replication scheme that performs compactions only on the primary nodes and sends pre-built indexes to backup nodes, avoiding all compactions in backup nodes. Our approach includes an efficient mechanism to deal with pointer translation across nodes in the pre-built region index. Our results show that Tebis reduces resource utilization on backup nodes compared to performing full compactions: throughput is increased by 1.06 to 2.90×, CPU efficiency is increased by 1.21 to 2.78×, and I/O amplification is reduced by 1.7 to 3.27×, whereas network traffic increases by up to 1.32 to 3.76x.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394844937",
    "type": "article"
  },
  {
    "title": "ReadGuard: Integrated SSD Management for Priority-Aware Read Performance Differentiation",
    "doi": "https://doi.org/10.1145/3676884",
    "publication_date": "2024-07-25",
    "publication_year": 2024,
    "authors": "Myoungjun Chun; Myungsuk Kim; Dusol Lee; Jisung Park; Jihong Kim",
    "corresponding_authors": "",
    "abstract": "When multiple apps with different I/O priorities share a high-performance SSD, it is important to differentiate the I/O QoS level based on the I/O priority of each app. In this paper, we study how a modern flash-based SSD should be designed to support priority-aware read performance differentiation. From an in-depth evaluation study using 3D TLC SSDs, we observed that existing FTLs have several weaknesses that need to be improved for better read performance differentiation. In order to overcome the existing FTL weaknesses, we propose ReadGuard , a novel priority-aware SSD management technique that enables an FTL to manage its blocks in a fully read-latency-aware fashion. ReadGuard leverages a new read-latency-centric block quality marker that can accurately distinguish the read latency of a block and ensures that higher-quality blocks are used for higher-priority apps. ReadGuard extends an existing suspend/resume technique to handle collisions among reads. Our experimental results show that a ReadGuard -enabled SSD is effective in supporting differentiated read performance in modern 3D flash SSDs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400982938",
    "type": "article"
  },
  {
    "title": "Encrypted Data Reduction: Removing Redundancy from Encrypted Data in Outsourced Storage",
    "doi": "https://doi.org/10.1145/3685278",
    "publication_date": "2024-07-29",
    "publication_year": 2024,
    "authors": "Zhao Jia; Zuoru Yang; Jingwei Li; Patrick P. C. Lee",
    "corresponding_authors": "",
    "abstract": "Storage savings and data confidentiality are two primary goals for outsourced storage. However, encryption by design destroys the content redundancy within plaintext data, so there exist design tensions when combining encryption with data reduction techniques (i.e., deduplication, delta compression, and local compression). We present EDRStore, an outsourced storage system that realizes encrypted data reduction to achieve both storage savings and data confidentiality. EDRStore’s core idea is a careful design of the encryption and data reduction workflows. It proposes new key generation and encryption schemes to preserve the content similarity of encrypted data for deduplication and delta compression. It further proposes selective local compression based on content similarity, so as to achieve storage savings of encrypted data from both delta compression and local compression. Evaluation on real-world datasets shows that EDRStore achieves higher storage savings than existing encrypted storage approaches and incurs moderate performance overhead compared with plaintext storage.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401090613",
    "type": "article"
  },
  {
    "title": "Portably solving file races with hardness amplification",
    "doi": "https://doi.org/10.1145/1416944.1416948",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Dan Tsafrir; Tomer Hertz; David Wagner; Dilma Da Silva",
    "corresponding_authors": "",
    "abstract": "The file-system API of contemporary systems makes programs vulnerable to TOCTTOU (time-of-check-to-time-of-use) race conditions. Existing solutions either help users to detect these problems (by pinpointing their locations in the code), or prevent the problem altogether (by modifying the kernel or its API). But the latter alternative is not prevalent, and the former is just the first step: Programmers must still address TOCTTOU flaws within the limits of the existing API with which several important tasks cannot be accomplished in a portable straightforward manner. Recently, Dean and Hu [2004] addressed this problem and suggested a probabilistic hardness amplification approach that alleviated the matter. Alas, shortly after, Borisov et al. [2005] responded with an attack termed “filesystem maze” that defeated the new approach. We begin by noting that mazes constitute a generic way to deterministically win many TOCTTOU races (gone are the days when the probability was small). In the face of this threat, we: (1) develop a new user-level defense that can withstand mazes; and (2) show that our method is undefeated even by much stronger hypothetical attacks that provide the adversary program with ideal conditions to win the race (enjoying complete and instantaneous knowledge about the defending program's actions and being able to perfectly synchronize accordingly). The fact that our approach is immune to these unrealistic attacks suggests it can be used as a simple and portable solution to a large class of TOCTTOU vulnerabilities, without requiring modifications to the underlying operating system.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2112504626",
    "type": "article"
  },
  {
    "title": "Storing semi-structured data on disk drives",
    "doi": "https://doi.org/10.1145/1534912.1534915",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Medha Bhadkamkar; Fernando Farfán; Vagelis Hristidis; Raju Rangaswami",
    "corresponding_authors": "",
    "abstract": "Applications that manage semi-structured data are becoming increasingly commonplace. Current approaches for storing semi-structured data use existing storage machinery; they either map the data to relational databases, or use a combination of flat files and indexes. While employing these existing storage mechanisms provides readily available solutions, there is a need to more closely examine their suitability to this class of data. Particularly, retrofitting existing solutions for semi-structured data can result in a mismatch between the tree structure of the data and the access characteristics of the underlying storage device (disk drive). This study explores various possibilities in the design space of native storage solutions for semi-structured data by exploring alternative approaches that match application data access characteristics to those of the underlying disk drive. For evaluating the effectiveness of the proposed native techniques in relation to the existing solution, we experiment with XML data using the XPathMark benchmark. Extensive evaluation reveals the strengths and weaknesses of the proposed native data layout techniques. While the existing solutions work really well for deep-focused queries into a semi-structured document (those that result in retrieving entire subtrees), the proposed native solutions substantially outperform for the non-deep-focused queries, which we demonstrate are at least as important as the deep-focused. We believe that native data layout techniques offer a unique direction for improving the performance of semi-structured data stores for a variety of important workloads. However, given that the proposed native techniques require circumventing current storage stack abstractions, further investigation is warranted before they can be applied to general-purpose storage systems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1981387614",
    "type": "article"
  },
  {
    "title": "OrcFS",
    "doi": "https://doi.org/10.1145/3162614",
    "publication_date": "2018-04-12",
    "publication_year": 2018,
    "authors": "Jinsoo Yoo; Joontaek Oh; Seongjin Lee; Youjip Won; Jin-Yong Ha; Jong-Sung Lee; Junseok Shim",
    "corresponding_authors": "",
    "abstract": "In this work, we develop the Orchestrated File System (OrcFS) for Flash storage. OrcFS vertically integrates the log-structured file system and the Flash-based storage device to eliminate the redundancies across the layers. A few modern file systems adopt sophisticated append-only data structures in an effort to optimize the behavior of the file system with respect to the append-only nature of the Flash memory. While the benefit of adopting an append-only data structure seems fairly promising, it makes the stack of software layers full of unnecessary redundancies, leaving substantial room for improvement. The redundancies include (i) redundant levels of indirection (address translation), (ii) duplicate efforts to reclaim the invalid blocks (i.e., segment cleaning in the file system and garbage collection in the storage device), and (iii) excessive over-provisioning (i.e., separate over-provisioning areas in each layer). OrcFS eliminates these redundancies via distributing the address translation, segment cleaning (or garbage collection), bad block management, and wear-leveling across the layers. Existing solutions suffer from high segment cleaning overhead and cause significant write amplification due to mismatch between the file system block size and the Flash page size. To optimize the I/O stack while avoiding these problems, OrcFS adopts three key technical elements. First, OrcFS uses disaggregate mapping , whereby it partitions the Flash storage into two areas, managed by a file system and Flash storage, respectively, with different granularity. In OrcFS, the metadata area and data area are maintained by 4Kbyte page granularity and 256Mbyte superblock granularity. The superblock-based storage management aligns the file system section size, which is a unit of segment cleaning, with the superblock size of the underlying Flash storage. It can fully exploit the internal parallelism of the underlying Flash storage, exploiting the sequential workload characteristics of the log-structured file system. Second, OrcFS adopts quasi-preemptive segment cleaning to prohibit the foreground I/O operation from being interfered with by segment cleaning. The latency to reclaim the free space can be prohibitive in OrcFS due to its large file system section size, 256Mbyte. OrcFS effectively addresses this issue via adopting a polling-based segment cleaning scheme. Third, the OrcFS introduces block patching to avoid unnecessary write amplification in the partial page program. OrcFS is the enhancement of the F2FS file system. We develop a prototype OrcFS based on F2FS and server class SSD with modified firmware (Samsung 843TN). OrcFS reduces the device mapping table requirement to 1/465 and 1/4 compared with the page mapping and the smallest mapping scheme known to the public, respectively. Via eliminating the redundancy in the segment cleaning and garbage collection, the OrcFS reduces 1/3 of the write volume under heavy random write workload. OrcFS achieves 56% performance gain against EXT4 in varmail workload.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2796963089",
    "type": "article"
  },
  {
    "title": "L <scp>ib</scp> PM",
    "doi": "https://doi.org/10.1145/3278141",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Leonardo Mármol; Mohammad Ataur Rahman Chowdhury; Raju Rangaswami",
    "corresponding_authors": "",
    "abstract": "Persistent Memory devices present properties that are uniquely different from prior technologies for which applications have been built. Unfortunately, the conventional approach to building applications fail to either efficiently utilize these new devices or provide programmers a seamless development experience. We have built L ib PM, a Persistent Memory Library that implements an easy-to-use container abstraction for consuming PM. LibPM’s containers are data hosting units that can store arbitrarily complex data types while preserving their integrity and consistency. Consequently, L ib PM’s containers provide a generic interface to applications, allowing applications to store and manipulate arbitrarily structured data with strong durability and consistency properties, all without having to navigate all the myriad pitfalls of programming PM directly. By providing a simple and high-performing transactional update mechanism, L ib PM allows applications to manipulate persistent data at the speed of memory. The container abstraction and automatic persistent data discovery mechanisms within L ib PM also simplify porting legacy applications to PM. From a performance perspective, L ib PM closely matches and often exceeds the performance of state-of-the-art application libraries for PM. For instance, L ib PM ’s performance is 195× better for write intensive workloads and 2.6× better for read intensive workloads when compared with the state-of-the-art P mem .IO persistent memory library.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2902391296",
    "type": "article"
  },
  {
    "title": "FlashNet",
    "doi": "https://doi.org/10.1145/3239562",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Animesh Trivedi; Nikolas Ioannou; Bernard Metzler; Patrick Stuedi; Jonas Pfefferle; Kornilios Kourtis; Ioannis Koltsidas; Thomas Groß",
    "corresponding_authors": "",
    "abstract": "During the past decade, network and storage devices have undergone rapid performance improvements, delivering ultra-low latency and several Gbps of bandwidth. Nevertheless, current network and storage stacks fail to deliver this hardware performance to the applications, often due to the loss of I/O efficiency from stalled CPU performance. While many efforts attempt to address this issue solely on either the network or the storage stack, achieving high-performance for networked-storage applications requires a holistic approach that considers both. In this article, we present FlashNet, a software I/O stack that unifies high-performance network properties with flash storage access and management. FlashNet builds on RDMA principles and abstractions to provide a direct, asynchronous, end-to-end data path between a client and remote flash storage. The key insight behind FlashNet is to co-design the stack’s components (an RDMA controller, a flash controller, and a file system) to enable cross-stack optimizations and maximize I/O efficiency. In micro-benchmarks, FlashNet improves 4kB network I/O operations per second (IOPS by 38.6% to 1.22M, decreases access latency by 43.5% to 50.4μs, and prolongs the flash lifetime by 1.6-5.9× for writes. We illustrate the capabilities of FlashNet by building a Key-Value store and porting a distributed data store that uses RDMA on it. The use of FlashNet’s RDMA API improves the performance of KV store by 2× and requires minimum changes for the ported data store to access remote flash devices.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2903389234",
    "type": "article"
  },
  {
    "title": "Online availability upgrades for parity-based RAIDs through supplementary parity augmentations",
    "doi": "https://doi.org/10.1145/1970338.1970341",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Lei Tian; Qiang Cao; Hong Jiang; Dan Feng; Changsheng Xie; Qin Xin",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a simple but powerful online availability upgrade mechanism, S upplementary P arity A ugmentations( SPA ), to address the availability issue in parity-based RAID systems. The basic idea of SPA is to store and update the supplementary parity units on one or a few newly augmented spare disks for online RAID systems in the operational mode, thus achieving the goals of improving the reconstruction performance while tolerating multiple disk failures and latent sector errors simultaneously. By applying the exclusive OR operations appropriately among supplementary parity, full parity, and data units, SPA can reconstruct the data on the failed disks with a fraction of the original overhead that is proportional to the supplementary parity coverage, thus significantly reducing the overhead of data regeneration and decreasing recovery time in parity-based RAID systems. Our extensive trace-driven simulation study shows that SPA can significantly improve the reconstruction performance of the RAID5 and RAID5+0 systems, at an acceptable performance overhead imposed in the operational mode. Moreover, our reliability analytical modeling and sequential Monte-Carlo simulation demonstrate that SPA is consistently more than double the MTTDL of the RAID5 system and improves the reliability of the RAID5+0 system noticeably.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2153453736",
    "type": "article"
  },
  {
    "title": "An Analysis of Flash Page Reuse With WOM Codes",
    "doi": "https://doi.org/10.1145/3177886",
    "publication_date": "2018-02-26",
    "publication_year": 2018,
    "authors": "Gala Yadgar; Eitan Yaakobi; Fabio Margaglia; Yue Li; Alexander Yucovich; Nachum Bundak; Lior Gilon; Nir Yakovi; Assaf Schuster; André Brinkmann",
    "corresponding_authors": "",
    "abstract": "Flash memory is prevalent in modern servers and devices. Coupled with the scaling down of flash technology, the popularity of flash memory motivates the search for methods to increase flash reliability and lifetime. Erasures are the dominant cause of flash cell wear, but reducing them is challenging because flash is a write-once medium— memory cells must be erased prior to writing. An approach that has recently received considerable attention relies on write-once memory (WOM) codes, designed to accommodate additional writes on write-once media. However, the techniques proposed for reusing flash pages with WOM codes are limited in their scope. Many focus on the coding theory alone, whereas others suggest FTL designs that are application specific, or not applicable due to their complexity, overheads, or specific constraints of multilevel cell (MLC) flash. This work is the first that addresses all aspects of page reuse within an end-to-end analysis of a general-purpose FTL on MLC flash. We use a hardware evaluation setup to directly measure the short- and long-term effects of page reuse on SSD durability and energy consumption, and show that FTL design must explicitly take them into account. We then provide a detailed analytical model for deriving the optimal garbage collection policy for such FTL designs, and for predicting the benefit from reuse on realistic hardware and workload characteristics.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2789354333",
    "type": "article"
  },
  {
    "title": "UnistorFS",
    "doi": "https://doi.org/10.1145/3177918",
    "publication_date": "2018-02-26",
    "publication_year": 2018,
    "authors": "Shuo-Han Chen; Tseng‐Yi Chen; Yuan-Hao Chang; Hsin‐Wen Wei; Wei‐Kuan Shih",
    "corresponding_authors": "",
    "abstract": "With the advanced technology in persistent random access memory (PRAM), PRAM such as three-dimen-sional XPoint memory and Phase Change Memory (PCM) is emerging as a promising candidate for the next-generation medium for both (main) memory and storage. Previous works mainly focus on how to overcome the possible endurance issues of PRAM while both main memory and storage own a partition on the same PRAM device. However, a holistic software-level system design should be proposed to fully exploit the benefit of PRAM. This article proposes a &lt;underline&gt;uni&lt;/underline&gt;on &lt;underline&gt;stor&lt;/underline&gt;age &lt;underline&gt;f&lt;/underline&gt;ile &lt;underline&gt;s&lt;/underline&gt;ystem (UnistorFS), which aims to jointly manage the PRAM resource for main memory and storage. The proposed UnistorFS realizes the concept of using the PRAM resource as memory and storage interchangeably to achieve resource sharing while main memory and storage coexist on the same PRAM device with no partition or logical boundary. This approach not only enables PRAM resource sharing but also eliminates unnecessary data movements between main memory and storage since they are already in the same address space and can be accessed directly. At the same time, the proposed UnistorFS ensures the persistence of file data and sanity of the file system after power recycling. A series of experiments was conducted on a modified Linux kernel. The results show that the proposed UnistorFS can eliminate unnecessary memory accesses and outperform other PRAM-based file systems for 0.2--8.7 times in terms of read/write performance.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2789732395",
    "type": "article"
  },
  {
    "title": "Characterizing Output Bottlenecks of a Production Supercomputer",
    "doi": "https://doi.org/10.1145/3335205",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Bing Xie; Sarp Oral; Christopher Zimmer; Jong Youl Choi; David Dillow; Scott Klasky; Jay Lofstead; Norbert Podhorszki; Jeffrey S. Chase",
    "corresponding_authors": "",
    "abstract": "This article studies the I/O write behaviors of the Titan supercomputer and its Lustre parallel file stores under production load. The results can inform the design, deployment, and configuration of file systems along with the design of I/O software in the application, operating system, and adaptive I/O libraries. We propose a statistical benchmarking methodology to measure write performance across I/O configurations, hardware settings, and system conditions. Moreover, we introduce two relative measures to quantify the write-performance behaviors of hardware components under production load. In addition to designing experiments and benchmarking on Titan, we verify the experimental results on one real application and one real application I/O kernel, XGC and HACC IO, respectively. These two are representative and widely used to address the typical I/O behaviors of applications. In summary, we find that Titan’s I/O system is variable across the machine at fine time scales. This variability has two major implications. First, stragglers lessen the benefit of coupled I/O parallelism (striping). Peak median output bandwidths are obtained with parallel writes to many independent files, with no striping or write sharing of files across clients (compute nodes). I/O parallelism is most effective when the application—or its I/O libraries—distributes the I/O load so that each target stores files for multiple clients and each client writes files on multiple targets in a balanced way with minimal contention. Second, our results suggest that the potential benefit of dynamic adaptation is limited. In particular, it is not fruitful to attempt to identify “good locations” in the machine or in the file system: component performance is driven by transient load conditions and past performance is not a useful predictor of future performance. For example, we do not observe diurnal load patterns that are predictable.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3014932476",
    "type": "article"
  },
  {
    "title": "SlimCache",
    "doi": "https://doi.org/10.1145/3383124",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Yichen Jia; Zili Shao; Feng Chen",
    "corresponding_authors": "",
    "abstract": "Flash-based key-value caching is becoming popular in data centers for providing high-speed key-value services. These systems adopt slab-based space management on flash and provide a low-cost solution for key-value caching. However, optimizing cache efficiency for flash-based key-value cache systems is highly challenging, due to the huge number of key-value items and the unique technical constraints of flash devices. In this article, we present a dynamic on-line compression scheme, called SlimCache , to improve the cache hit ratio by virtually expanding the usable cache space through data compression. We have investigated the effect of compression granularity to achieve a balance between compression ratio and speed, and we leveraged the unique workload characteristics in key-value systems to efficiently identify and separate hot and cold data. To dynamically adapt to workload changes during runtime, we have designed an adaptive hot/cold area partitioning method based on a cost model. To avoid unnecessary compression, SlimCache also estimates data compressibility to determine whether the data are suitable for compression or not. We have implemented a prototype based on Twitter’s Fatcache. Our experimental results show that SlimCache can accommodate more key-value items in flash by up to 223.4%, effectively increasing throughput and reducing average latency by up to 380.1% and 80.7%, respectively.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3035208061",
    "type": "article"
  },
  {
    "title": "Hybrid Codes",
    "doi": "https://doi.org/10.1145/3407193",
    "publication_date": "2020-09-24",
    "publication_year": 2020,
    "authors": "Liuqing Ye; Dan Feng; Yuchong Hu; Xueliang Wei",
    "corresponding_authors": "",
    "abstract": "Erasure codes are being extensively deployed in practical storage systems to prevent data loss with low redundancy. However, these codes require excessive disk I/Os and network traffic for recovering unavailable data. Among all erasure codes, Minimum Storage Regenerating (MSR) codes can achieve optimal repair bandwidth under the minimum storage during recovery, but some open issues remain to be addressed before applying them in real systems. Facing with the huge burden during recovery, erasure-coded storage systems need to be developed with high repair efficiency. Aiming at this goal, a new class of coding scheme is introduced—Hybrid Regenerating Codes (Hybrid-RC). The codes utilize the superiority of MSR codes to compute a subset of data blocks while some other parity blocks are used for reliability maintenance. As a result, our design is near-optimal with respect to storage and network traffic and shows great improvements in recovery performance.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3088311959",
    "type": "article"
  },
  {
    "title": "Performance Modeling and Practical Use Cases for Black-Box SSDs",
    "doi": "https://doi.org/10.1145/3440022",
    "publication_date": "2021-05-30",
    "publication_year": 2021,
    "authors": "Joonsung Kim; Kanghyun Choi; Wonsik Lee; Jangwoo Kim",
    "corresponding_authors": "",
    "abstract": "Modern servers are actively deploying Solid-State Drives (SSDs) thanks to their high throughput and low latency. However, current server architects cannot achieve the full performance potential of commodity SSDs, as SSDs are complex devices designed for specific goals (e.g., latency, throughput, endurance, cost) with their internal mechanisms undisclosed to users. In this article, we propose SSDcheck , a novel SSD performance model to extract various internal mechanisms and predict the latency of next access to commodity black-box SSDs. We identify key performance-critical features (e.g., garbage collection, write buffering) and find their parameters (i.e., size, threshold) from each SSD by using our novel diagnosis code snippets. Then, SSDcheck constructs a performance model for a target SSD and dynamically manages the model to predict the latency of the next access. In addition, SSDcheck extracts and provides other useful internal mechanisms (e.g., fetch unit in multi-queue SSDs, background tasks triggering idle-time interval) for the storage system to fully exploit SSDs. By using those useful features and the performance model, we propose multiple practical use cases. Our evaluations show that SSDcheck’s performance model is highly accurate, and proposed use cases achieve significant performance improvement in various scenarios.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3168095980",
    "type": "article"
  },
  {
    "title": "Penalty- and Locality-aware Memory Allocation in Redis Using Enhanced AET",
    "doi": "https://doi.org/10.1145/3447573",
    "publication_date": "2021-05-28",
    "publication_year": 2021,
    "authors": "Cheng Pan; Xiaolin Wang; Yingwei Luo; Zhenlin Wang",
    "corresponding_authors": "",
    "abstract": "Due to large data volume and low latency requirements of modern web services, the use of an in-memory key-value (KV) cache often becomes an inevitable choice (e.g., Redis and Memcached). The in-memory cache holds hot data, reduces request latency, and alleviates the load on background databases. Inheriting from the traditional hardware cache design, many existing KV cache systems still use recency-based cache replacement algorithms, e.g., least recently used or its approximations. However, the diversity of miss penalty distinguishes a KV cache from a hardware cache. Inadequate consideration of penalty can substantially compromise space utilization and request service time. KV accesses also demonstrate locality, which needs to be coordinated with miss penalty to guide cache management. In this article, we first discuss how to enhance the existing cache model, the Average Eviction Time model, so that it can adapt to modeling a KV cache. After that, we apply the model to Redis and propose pRedis, Penalty- and Locality-aware Memory Allocation in Redis, which synthesizes data locality and miss penalty, in a quantitative manner, to guide memory allocation and replacement in Redis. At the same time, we also explore the diurnal behavior of a KV store and exploit long-term reuse. We replace the original passive eviction mechanism with an automatic dump/load mechanism, to smooth the transition between access peaks and valleys. Our evaluation shows that pRedis effectively reduces the average and tail access latency with minimal time and space overhead. For both real-world and synthetic workloads, our approach delivers an average of 14.0%∼52.3% latency reduction over a state-of-the-art penalty-aware cache management scheme, Hyperbolic Caching (HC), and shows more quantitative predictability of performance. Moreover, we can obtain even lower average latency (1.1%∼5.5%) when dynamically switching policies between pRedis and HC.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3170749704",
    "type": "article"
  },
  {
    "title": "Design of LSM-tree-based Key-value SSDs with Bounded Tails",
    "doi": "https://doi.org/10.1145/3452846",
    "publication_date": "2021-05-28",
    "publication_year": 2021,
    "authors": "Junsu Im; Jinwook Bae; Chanwoo Chung; Arvind Arvind; Sungjin Lee",
    "corresponding_authors": "",
    "abstract": "Key-value store based on a log-structured merge-tree (LSM-tree) is preferable to hash-based key-value store, because an LSM-tree can support a wider variety of operations and show better performance, especially for writes. However, LSM-tree is difficult to implement in the resource constrained environment of a key-value SSD (KV-SSD), and, consequently, KV-SSDs typically use hash-based schemes. We present PinK , a design and implementation of an LSM-tree-based KV-SSD, which compared to a hash-based KV-SSD, reduces 99th percentile tail latency by 73%, improves average read latency by 42%, and shows 37% higher throughput. The key idea in improving the performance of an LSM-tree in a resource constrained environment is to avoid the use of Bloom filters and instead, use a small amount of DRAM to keep/pin the top levels of the LSM-tree. We also find that PinK is able to provide a flexible design space for a wide range of KV workloads by leveraging the read-write tradeoff in LSM-trees.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3172366570",
    "type": "article"
  },
  {
    "title": "An Energy-Efficient and Reliable Storage Mechanism for Data-Intensive Academic Archive Systems",
    "doi": "https://doi.org/10.1145/2720021",
    "publication_date": "2015-03-20",
    "publication_year": 2015,
    "authors": "Tseng‐Yi Chen; Hsin‐Wen Wei; Tsung-Tai Yeh; Tsan‐sheng Hsu; Wei‐Kuan Shih",
    "corresponding_authors": "",
    "abstract": "Previous studies proposed energy-efficient solutions, such as multispeed disks and disk spin-down methods, to conserve power in their respective storage systems. However, in most cases, the authors did not analyze the reliability of their solutions. According to research conducted by Google and the IDEMA standard, frequently setting the disk status to standby mode will increase the disk’s Annual Failure Rate and reduce its lifespan. To resolve the issue, we propose an evaluation function called E 3 SaRC (Economic Evaluation of Energy Saving with Reliability Constraint), which considers the cost of hardware failure when applying energy-saving schemes. We also present an adaptive write cache mechanism called CacheRAID. The mechanism tries to mitigate the random access problems that implicitly exist in RAID techniques and thereby reduce the energy consumption of RAID disks. CacheRAID also addresses the issue of system reliability by applying a control mechanism to the spin-down algorithm. Our experimental results show that the CacheRAID storage system can reduce the power consumption of the conventional software RAID 5 system by 65% to 80%. Moreover, according to the E 3 SaRC measurement, the overall saved cost of CacheRAID is the largest among the systems that we compared.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2061610875",
    "type": "article"
  },
  {
    "title": "Exploiting Sequential and Temporal Localities to Improve Performance of NAND Flash-Based SSDs",
    "doi": "https://doi.org/10.1145/2905054",
    "publication_date": "2016-05-12",
    "publication_year": 2016,
    "authors": "Sungjin Lee; Dongkun Shin; Young‐Jin Kim; Jihong Kim",
    "corresponding_authors": "",
    "abstract": "NAND flash-based Solid-State Drives (SSDs) are becoming a viable alternative as a secondary storage solution for many computing systems. Since the physical characteristics of NAND flash memory are different from conventional Hard-Disk Drives (HDDs), flash-based SSDs usually employ an intermediate software layer, called a Flash Translation Layer (FTL). The FTL runs several firmware algorithms for logical-to-physical mapping, I/O interleaving, garbage collection, wear-leveling, and so on. These FTL algorithms not only have a great effect on storage performance and lifetime, but also determine hardware cost and data integrity. In general, a hybrid FTL scheme has been widely used in mobile devices because it exhibits high performance and high data integrity at a low hardware cost. Recently, a demand-based FTL based on page-level mapping has been rapidly adopted in high-performance SSDs. The demand-based FTL more effectively exploits the device-level parallelism than the hybrid FTL and requires a small amount of memory by keeping only popular mapping entries in DRAM. Because of this caching mechanism, however, the demand-based FTL is not robust enough for power failures and requires extra reads to fetch missing mapping entries from NAND flash. In this article, we propose a new flash translation layer called LAST++. The proposed LAST++ scheme is based on the hybrid FTL, thus it has the inherent benefits of the hybrid FTL, including low resource requirements, strong robustness for power failures, and high read performance. By effectively exploiting the locality of I/O references, LAST++ increases device-level parallelism and reduces garbage collection overheads. This leads to a great improvement of I/O performance and makes it possible to overcome the limitations of the hybrid FTL. Our experimental results show that LAST++ outperforms the demand-based FTL by 27% for writes and 7% for reads, on average, while offering higher robustness against sudden power failures. LAST++ also improves write performance by 39%, on average, over the existing hybrid FTL.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2371481495",
    "type": "article"
  },
  {
    "title": "Performance Bug Analysis and Detection for Distributed Storage and Computing Systems",
    "doi": "https://doi.org/10.1145/3580281",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Jiaxin Li; Yiming Zhang; Shan Lu; Haryadi S. Gunawi; Xiaohui Gu; Feng Huang; Dongsheng Li",
    "corresponding_authors": "",
    "abstract": "This article systematically studies 99 distributed performance bugs from five widely deployed distributed storage and computing systems (Cassandra, HBase, HDFS, Hadoop MapReduce and ZooKeeper). We present the TaxPerf database, which collectively organizes the analysis results as over 400 classification labels and over 2,500 lines of bug re-description. TaxPerf is classified into six bug categories (and 18 bug subcategories) by their root causes; resource, blocking, synchronization, optimization, configuration, and logic. TaxPerf can be used as a benchmark for performance bug studies and debug tool designs. Although it is impractical to automatically detect all categories of performance bugs in TaxPerf, we find that an important category of blocking bugs can be effectively solved by analysis tools. We analyze the cascading nature of blocking bugs and design an automatic detection tool called PCatch , which (i) performs program analysis to identify code regions whose execution time can potentially increase dramatically with the workload size; (ii) adapts the traditional happens-before model to reason about software resource contention and performance dependency relationship; and (iii) uses dynamic tracking to identify whether the slowdown propagation is contained in one job. Evaluation shows that PCatch can accurately detect blocking bugs of representative distributed storage and computing systems by observing system executions under small-scale workloads.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4317209671",
    "type": "article"
  },
  {
    "title": "A High-performance RDMA-oriented Learned Key-value Store for Disaggregated Memory Systems",
    "doi": "https://doi.org/10.1145/3620674",
    "publication_date": "2023-09-05",
    "publication_year": 2023,
    "authors": "Pengfei Li; Yu Hua; Pengfei Zuo; Zhangyu Chen; Jiajie Sheng",
    "corresponding_authors": "",
    "abstract": "Disaggregated memory systems separate monolithic servers into different components, including compute and memory nodes, to enjoy the benefits of high resource utilization, flexible hardware scalability, and efficient data sharing. By exploiting the high-performance RDMA (Remote Direct Memory Access), the compute nodes directly access the remote memory pool without involving remote CPUs. Hence, the ordered key-value (KV) stores (e.g., B-trees and learned indexes) keep all data sorted to provide range query services via the high-performance network. However, existing ordered KVs fail to work well on the disaggregated memory systems, due to either consuming multiple network roundtrips to search the remote data or heavily relying on the memory nodes equipped with insufficient computing resources to process data modifications. In this article, we propose a scalable RDMA-oriented KV store with learned indexes, called ROLEX, to coalesce the ordered KV store in the disaggregated systems for efficient data storage and retrieval. ROLEX leverages a retraining-decoupled learned index scheme to dissociate the model retraining from data modification operations via adding a bias and some data movement constraints to learned models. Based on the operation decoupling, data modifications are directly executed in compute nodes via one-sided RDMA verbs with high scalability. The model retraining is hence removed from the critical path of data modification and asynchronously executed in memory nodes by using dedicated computing resources. ROLEX efficiently alleviates the fragmentation and garbage collection issues, due to allocating and reclaiming space via fixed-size leaves that are accessed via the atomic-size leaf numbers. Our experimental results on YCSB and real-world workloads demonstrate that ROLEX achieves competitive performance on the static workloads, as well as significantly improving the performance on dynamic workloads by up to 2.2× over state-of-the-art schemes on the disaggregated memory systems. We have released the open-source codes for public use in GitHub.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386443138",
    "type": "article"
  },
  {
    "title": "Using MEMS-based storage in computer systems---device modeling and management",
    "doi": "https://doi.org/10.1145/1149976.1149978",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Bo Hong; Scott Brandt; Darrell D. E. Long; Ethan L. Miller; Ying Lin",
    "corresponding_authors": "",
    "abstract": "MEMS-based storage is an emerging nonvolatile secondary storage technology. It promises high performance, high storage density, and low power consumption. With fundamentally different architectural designs from magnetic disk, MEMS-based storage exhibits unique two-dimensional positioning behaviors and efficient power state transitions. We model these low-level, device-specific properties of MEMS-based storage and present request scheduling algorithms and power management strategies that exploit the full potential of these devices. Our simulations show that MEMS-specific device management policies can significantly improve system performance and reduce power consumption.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2078043391",
    "type": "article"
  },
  {
    "title": "Cheap recovery",
    "doi": "https://doi.org/10.1145/1044956.1044959",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Andrew C. Huang; Armando Fox",
    "corresponding_authors": "",
    "abstract": "Cluster hash tables (CHTs) are key components of many large-scale Internet services due to their highly-scalable performance and the prevalence of the type of data they store. Another advantage of CHTs is that they can be designed to be as self-managing as a cluster of stateless servers. One key to achieving this extreme manageability is reboot-based recovery that is predictably fast and has modest impact on system performance and availability. This \"cheap\" recovery mechanism simplifies management in two ways. First, it simplifies failure detection by lowering the cost of acting on false positives. This enables one to use statistical techniques to turn hard-to-catch failures, such as node degradation, into failure, followed by recovery. Second, cheap recovery simplifies capacity planning by recasting repartitioning as failure plus recovery to achieve zero-downtime incremental scaling. These low-cost recovery and scaling mechanisms make it possible for the system to be continuously self-adjusting, a key property of self-managing systems.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2100389034",
    "type": "article"
  },
  {
    "title": "Using MEMS-based storage in computer systems---MEMS storage architectures",
    "doi": "https://doi.org/10.1145/1138041.1138042",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Bo Hong; Feng Wang; Scott Brandt; Darrell D. E. Long; Thomas Schwarz",
    "corresponding_authors": "",
    "abstract": "As an emerging nonvolatile secondary storage technology, MEMS-based storage exhibits several desirable properties including high performance, high storage volumic density, low power consumption, low entry cost, and small form factor. However, MEMS-based storage provides a limited amount of storage per device and is likely to be more expensive than magnetic disk. Systems designers will therefore need to make trade-offs to achieve well-balanced designs. We present an architecture in which MEMS devices are organized into MEMS storage enclosures with online spares. Such enclosures are proven to be highly reliable storage building bricks with no maintenance during their economic lifetimes. We also demonstrate the effectiveness of using MEMS as another layer in the storage hierarchy, bridging the cost and performance gap between MEMS storage and disk. We show that using MEMS as a disk cache can significantly improve system performance and cost-performance ratio.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2100833658",
    "type": "article"
  },
  {
    "title": "Intelligent storage",
    "doi": "https://doi.org/10.1145/1168910.1168912",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Youjip Won; Hyungkyu Chang; Jaemin Ryu; Yongdai Kim; Junseok Shim",
    "corresponding_authors": "",
    "abstract": "In this work, we develop an intelligent storage system framework for soft real-time applications. Modern software systems consist of a collection of layers and information exchange across the layers is performed via well-defined interfaces. Due to the strictness and inflexibility of interface definition, it is not possible to pass the information specific to one layer to other layers. In practice, the exploitation of this information across the layers can greatly enhance the performance, reliability, and manageability of the system. We address the limitation of legacy interface definition via enabling intelligence in the storage system. The objective is to enable the lower-layer entity, for example, a physical or block device, to conjecture the semantic and contextual information of that application behavior which cannot be passed via the legacy interface. Based upon the knowledge obtained by the intelligence module, the system can perform a number of actions to improve the performance, reliability, security, and manageability of the system. Our intelligence storage system focuses on optimizing the I/O subsystem performance for a soft real-time application. Our intelligence framework consists of three components: the workload monitor, workload analyzer, and system optimizer. The workload monitor maintains a window of recent I/O requests and extracts feature vectors in regular intervals. The workload analyzer is trained to determine the class of the incoming workload by using the feature vector. The system optimizer performs various actions to tune the storage system for a given workload. We use confidence rate boosting to train the workload analyzer. This sophisticated learner achieves a higher than 97% accuracy of workload class prediction. We develop a prototype intelligence storage system on the legacy operating system platform. The system optimizer performs; (1) dynamic adjustment of the file-system-level read-ahead size; (2) dynamic adjustment of I/O request size; and (3) filtering of I/O requests. We examine the effect of this autonomic optimization via experimentation. We find that the storage level pro-active optimization greatly enhances the efficiency of the underlying storage system. The sophisticated intelligence module developed in this work does not restrict its usage for performance optimization. It can be effectively used as classification engine for generic autonomic computing environment, i.e. management, diagnosis, security and etc.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2071181159",
    "type": "article"
  },
  {
    "title": "Efficient cooperative backup with decentralized trust management",
    "doi": "https://doi.org/10.1145/2339118.2339119",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Nguyen H. Tran; Frank Chiang; Jinyang Li",
    "corresponding_authors": "",
    "abstract": "Existing backup systems are unsatisfactory: commercial backup services are reliable but expensive while peer-to-peer systems are cheap but offer limited assurance of data reliability. This article introduces Friendstore, a system that provides inexpensive and reliable backup by giving users the choice to store backup data only on nodes they trust (typically those owned by friends and colleagues). Because it is built on trusted nodes, Friendstore is not burdened by the complexity required to cope with potentially malicious participants. Friendstore only needs to detect and repair accidental data loss and to ensure balanced storage exchange. The disadvantage of using only trusted nodes is that Friendstore cannot achieve perfect storage utilization. Friendstore is designed for a heterogeneous environment where nodes have very different access link speeds and available disk spaces. To ensure long-term data reliability, a node with limited upload bandwidth refrains from storing more data than its calculated maintainable capacity. A high bandwidth node might be limited by its available disk space. We introduce a simple coding scheme, called XOR(1,2), which doubles a node's ability to store backup information in the same amount of disk space at the cost of doubling the amount of data transferred during restore. Analysis and simulations using long-term node activity traces show that a node can reliably back up tens of gigabytes of data even with low upload bandwidth.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2166439304",
    "type": "article"
  },
  {
    "title": "Exploiting I/O Reordering and I/O Interleaving to Improve Application Launch Performance",
    "doi": "https://doi.org/10.1145/3024094",
    "publication_date": "2017-02-25",
    "publication_year": 2017,
    "authors": "Yongsoo Joo; Sangsoo Park; Hyokyung Bahn",
    "corresponding_authors": "",
    "abstract": "Application prefetchers improve application launch performance through either I/O reordering or I/O interleaving. However, there has been no proposal to combine the two techniques together, missing the opportunity for further optimization. We present a new application prefetching technique to take advantage of both the approaches. We evaluated our method with a set of applications to demonstrate that it reduces cold start application launch time by 50%, which is an improvement of 22% from the I/O reordering technique.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2593643889",
    "type": "article"
  },
  {
    "title": "Writes Wrought Right, and Other Adventures in File System Optimization",
    "doi": "https://doi.org/10.1145/3032969",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Jun Yuan; Yang Zhan; William Jannen; Prashant Pandey; Amogh Akshintala; Kanchan Chandnani; Pooja Deo; Zardosht Kasheff; Leif Walsh; Michael A. Bender; Martı́n Farach-Colton; Rob Johnson; Bradley C. Kuszmaul; Donald E. Porter",
    "corresponding_authors": "",
    "abstract": "File systems that employ write-optimized dictionaries (WODs) can perform random-writes, metadata updates, and recursive directory traversals orders of magnitude faster than conventional file systems. However, previous WOD-based file systems have not obtained all of these performance gains without sacrificing performance on other operations, such as file deletion, file or directory renaming, or sequential writes. Using three techniques, late-binding journaling , zoning , and range deletion , we show that there is no fundamental trade-off in write-optimization. These dramatic improvements can be retained while matching conventional file systems on all other operations. BetrFS 0.2 delivers order-of-magnitude better performance than conventional file systems on directory scans and small random writes and matches the performance of conventional file systems on rename, delete, and sequential I/O. For example, BetrFS 0.2 performs directory scans 2.2 × faster, and small random writes over two orders of magnitude faster, than the fastest conventional file system. But unlike BetrFS 0.1, it renames and deletes files commensurate with conventional file systems and performs large sequential I/O at nearly disk bandwidth. The performance benefits of these techniques extend to applications as well. BetrFS 0.2 continues to outperform conventional file systems on many applications, such as as rsync, git-diff, and tar, but improves git-clone performance by 35% over BetrFS 0.1, yielding performance comparable to other file systems.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2602601325",
    "type": "article"
  },
  {
    "title": "Protocol-Aware Recovery for Consensus-Based Distributed Storage",
    "doi": "https://doi.org/10.1145/3241062",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Ramnatthan Alagappan; Aishwarya Ganesan; Eric Lee; Aws Albarghouthi; Vijay Chidambaram; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We introduce protocol-aware recovery (P ar ), a new approach that exploits protocol-specific knowledge to correctly recover from storage faults in distributed systems. We demonstrate the efficacy of P ar through the design and implementation of &lt;underline&gt;c&lt;/underline&gt;orruption-&lt;underline&gt;t&lt;/underline&gt;olerant &lt;underline&gt;r&lt;/underline&gt;ep&lt;underline&gt;l&lt;/underline&gt;ication (C trl ), a P ar mechanism specific to replicated state machine (RSM) systems. We experimentally show that the C trl versions of two systems, LogCabin and ZooKeeper, safely recover from storage faults and provide high availability, while the unmodified versions can lose data or become unavailable. We also show that the C trl versions achieve this reliability with little performance overheads.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2894672808",
    "type": "article"
  },
  {
    "title": "Exploiting Internal Parallelism for Address Translation in Solid-State Drives",
    "doi": "https://doi.org/10.1145/3239564",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Wei Xie; Yong Chen; Philip C. Roth",
    "corresponding_authors": "",
    "abstract": "Solid-state Drives (SSDs) have changed the landscape of storage systems and present a promising storage solution for data-intensive applications due to their low latency, high bandwidth, and low power consumption compared to traditional hard disk drives. SSDs achieve these desirable characteristics using internal parallelism —parallel access to multiple internal flash memory chips—and a Flash Translation Layer (FTL) that determines where data are stored on those chips so that they do not wear out prematurely. However, current state-of-the-art cache-based FTLs like the Demand-based Flash Translation Layer (DFTL) do not allow IO schedulers to take full advantage of internal parallelism, because they impose a tight coupling between the logical-to-physical address translation and the data access. To address this limitation, we introduce a new FTL design called Parallel-DFTL that works with the DFTL to decouple address translation operations from data accesses. Parallel-DFTL separates address translation and data access operations into different queues, allowing the SSD to use concurrent flash accesses for both types of operations. We also present a Parallel-LRU cache replacement algorithm to improve the concurrency of address translation operations. To compare Parallel-DFTL against existing FTL approaches, we present a Parallel-DFTL performance model and compare its predictions against those for DFTL and an ideal page-mapping approach. We also implemented the Parallel-DFTL approach in an SSD simulator using real device parameters, and used trace-driven simulation to evaluate Parallel-DFTL’s efficacy. Our evaluation results show that Parallel-DFTL improved the overall performance by up to 32% for the real IO workloads we tested, and by up to two orders of magnitude with synthetic test workloads. We also found that Parallel-DFTL is able to achieve reasonable performance with a very small cache size and that it provides the best benefit for those workloads with large request size or with high write ratio.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2904028580",
    "type": "article"
  },
  {
    "title": "Finding Bugs in File Systems with an Extensible Fuzzing Framework",
    "doi": "https://doi.org/10.1145/3391202",
    "publication_date": "2020-05-18",
    "publication_year": 2020,
    "authors": "Seulbae Kim; Meng Xu; Sanidhya Kashyap; Jungyeon Yoon; Wen Xu; Taesoo Kim",
    "corresponding_authors": "",
    "abstract": "File systems are too large to be bug free. Although handwritten test suites have been widely used to stress file systems, they can hardly keep up with the rapid increase in file system size and complexity, leading to new bugs being introduced. These bugs come in various flavors: buffer overflows to complicated semantic bugs. Although bug-specific checkers exist, they generally lack a way to explore file system states thoroughly. More importantly, no turnkey solution exists that unifies the checking effort of various aspects of a file system under one umbrella. In this article, to highlight the potential of applying fuzzing to find any type of file system bugs in a generic way, we propose H ydra , an extensible fuzzing framework. H ydra provides building blocks for file system fuzzing, including input mutators, feedback engines, test executors, and bug post-processors. As a result, developers only need to focus on building the core logic for finding bugs of their interests. We showcase the effectiveness of H ydra with four checkers that hunt crash inconsistency, POSIX violations, logic assertion failures, and memory errors. So far, H ydra has discovered 157 new bugs in Linux file systems, including three in verified file systems (FSCQ and Yxv6).",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3036168767",
    "type": "article"
  },
  {
    "title": "Strong and Efficient Consistency with Consistency-aware Durability",
    "doi": "https://doi.org/10.1145/3423138",
    "publication_date": "2021-01-18",
    "publication_year": 2021,
    "authors": "Aishwarya Ganesan; Ramnatthan Alagappan; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We introduce consistency-aware durability or C ad , a new approach to durability in distributed storage that enables strong consistency while delivering high performance. We demonstrate the efficacy of this approach by designing cross-client monotonic reads , a novel and strong consistency property that provides monotonic reads across failures and sessions in leader-based systems; such a property can be particularly beneficial in geo-distributed and edge-computing scenarios. We build O rca , a modified version of ZooKeeper that implements C ad and cross-client monotonic reads. We experimentally show that O rca provides strong consistency while closely matching the performance of weakly consistent ZooKeeper. Compared to strongly consistent ZooKeeper, O rca provides significantly higher throughput (1.8--3.3×) and notably reduces latency, sometimes by an order of magnitude in geo-distributed settings. We also implement C ad in Redis and show that the performance benefits are similar to that of C ad ’s implementation in ZooKeeper.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3121178565",
    "type": "article"
  },
  {
    "title": "Twizzler: A <i>Data-centric</i> OS for Non-volatile Memory",
    "doi": "https://doi.org/10.1145/3454129",
    "publication_date": "2021-05-30",
    "publication_year": 2021,
    "authors": "Daniel Bittman; Peter Alvaro; Pankaj Mehra; Darrell D. E. Long; Ethan L. Miller",
    "corresponding_authors": "",
    "abstract": "Byte-addressable, non-volatile memory (NVM) presents an opportunity to rethink the entire system stack. We present Twizzler, an operating system redesign for this near-future. Twizzler removes the kernel from the I/O path, provides programs with memory-style access to persistent data using small (64 bit), object-relative cross-object pointers, and enables simple and efficient long-term sharing of data both between applications and between runs of an application. Twizzler provides a clean-slate programming model for persistent data, realizing the vision of Unix in a world of persistent RAM. We show that Twizzler is simpler, more extensible, and more secure than existing I/O models and implementations by building software for Twizzler and evaluating it on NVM DIMMs. Most persistent pointer operations in Twizzler impose less than 0.5 ns added latency. Twizzler operations are up to <?TeX $13\\times$?> faster than Unix , and SQLite queries are up to <?TeX $4.2\\times$?> faster than on PMDK. YCSB workloads ran 1.1– <?TeX $2.9\\times$?> faster on Twizzler than on native and NVM-optimized SQLite backends.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3170498275",
    "type": "article"
  },
  {
    "title": "Two Reconfigurable NDP Servers: Understanding the Impact of Near-Data Processing on Data Center Applications",
    "doi": "https://doi.org/10.1145/3460201",
    "publication_date": "2021-10-15",
    "publication_year": 2021,
    "authors": "Xiaojia Song; Tao Xie; Stephen M. Fischer",
    "corresponding_authors": "",
    "abstract": "Existing near-data processing (NDP)-powered architectures have demonstrated their strength for some data-intensive applications. Data center servers, however, have to serve not only data-intensive but also compute-intensive applications. An in-depth understanding of the impact of NDP on various data center applications is still needed. For example, can a compute-intensive application also benefit from NDP? In addition, current NDP techniques focus on maximizing the data processing rate by always utilizing all computing resources at all times. Is this “always running in full gear” strategy consistently beneficial for an application? To answer these questions, we first propose two reconfigurable NDP-powered servers called RANS ( R econfigurable A RM-based N DP S erver) and RFNS ( R econfigurable F PGA-based N DP S erver). Next, we implement a single-engine prototype for each of them based on a conventional data center and then evaluate their effectiveness. Experimental results measured from the two prototypes are then extrapolated to estimate the properties of the two full-size reconfigurable NDP servers. Finally, several new findings are presented. For example, we find that while RANS can only benefit data-intensive applications, RFNS can offer benefits for both data-intensive and compute-intensive applications. Moreover, we find that for certain applications the reconfigurability of RANS/RFNS can deliver noticeable energy efficiency without any performance degradation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3207560564",
    "type": "article"
  },
  {
    "title": "Design and Prototype of a Solid-State Cache",
    "doi": "https://doi.org/10.1145/2629491",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "Mohit Saxena; Michael M. Swift",
    "corresponding_authors": "",
    "abstract": "The availability of high-speed solid-state storage has introduced a new tier into the storage hierarchy. Low-latency and high-IOPS solid-state drives (SSDs) cache data in front of high-capacity disks. However, most existing SSDs are designed to be a drop-in disk replacement, and hence are mismatched for use as a cache. This article describes FlashTier , a system architecture built upon a solid-state cache (SSC), which is a flash device with an interface designed for caching. Management software at the operating system block layer directs caching. The FlashTier design addresses three limitations of using traditional SSDs for caching. First, FlashTier provides a unified logical address space to reduce the cost of cache block management within both the OS and the SSD. Second, FlashTier provides a new SSC block interface to enable a warm cache with consistent data after a crash. Finally, FlashTier leverages cache behavior to silently evict data blocks during garbage collection to improve performance of the SSC. We first implement an SSC simulator and a cache manager in Linux to perform an in-depth evaluation and analysis of FlashTier's design techniques. Next, we develop a prototype of SSC on the OpenSSD Jasmine hardware platform to investigate the benefits and practicality of FlashTier design. Our prototyping experiences provide insights applicable to managing modern flash hardware, implementing other SSD prototypes and new OS storage stack interface extensions. Overall, we find that FlashTier improves cache performance by up to 168% over consumer-grade SSDs and up to 52% over high-end SSDs. It also improves flash lifetime for write-intensive workloads by up to 60% compared to SSD caches with a traditional flash interface.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2105465034",
    "type": "article"
  },
  {
    "title": "Can We Group Storage? Statistical Techniques to Identify Predictive Groupings in Storage System Accesses",
    "doi": "https://doi.org/10.1145/2738042",
    "publication_date": "2016-02-01",
    "publication_year": 2016,
    "authors": "Avani Wildani; Ethan L. Miller",
    "corresponding_authors": "",
    "abstract": "Storing large amounts of data for different users has become the new normal in a modern distributed cloud storage environment. Storing data successfully requires a balance of availability, reliability, cost, and performance. Typically, systems design for this balance with minimal information about the data that will pass through them. We propose a series of methods to derive groupings from data that have predictive value, informing layout decisions for data on disk. Unlike previous grouping work, we focus on dynamically identifying groupings in data that can be gathered from active systems in real time with minimal impact using spatiotemporal locality. We outline several techniques we have developed and discuss how we select particular techniques for particular workloads and application domains. Our statistical and machine-learning-based grouping algorithms answer questions such as “What can a grouping be based on?” and “Is a given grouping meaningful for a given application?” We design our models to be flexible and require minimal domain information so that our results are as broadly applicable as possible. We intend for this work to provide a launchpad for future specialized system design using groupings in combination with caching policies and architectural distinctions such as tiered storage to create the next generation of scalable storage systems.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2258405498",
    "type": "article"
  },
  {
    "title": "Bridging the digital divide: storage media + postal network = generic high-bandwidth communication",
    "doi": "https://doi.org/10.1145/1063786.1063791",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Nitin Garg; Sumeet Sobti; Junwen Lai; Fengzhou Zheng; Kai Li; Randolph Y. Wang; Arvind Krishnamurthy",
    "corresponding_authors": "",
    "abstract": "Making high-bandwidth Internet access pervasively available to a large worldwide audience is a difficult challenge, especially in many developing regions. As we wait for the uncertain takeoff of technologies that promise to improve the situation, we propose to explore an approach that is potentially more easily realizable: the use of digital storage media transported by the postal system as a general digital communication mechanism. We shall call such a system a Postmanet . Compared to more conventional wide-area connectivity options, the Postmanet has several important advantages, including wide global reach, great bandwidth potential, low cost, and ease of incremental adoption. While the idea of sending digital content via the postal system is not a new one, none of the existing attempts have turned the postal system into a generic and transparent communication channel that not only can cater to a wide array of applications, but also effectively manage the many idiosyncrasies associated with using the postal system. In the proposed Postmanet, we see two recurring themes at many different levels of the system. One is the simultaneous exploitation of the Internet and the postal system so we can combine their latency and bandwidth advantages. The other is the exploitation of the abundant capacity and bandwidth of the Postmanet to improve its latency, cost, and reliability.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2152117447",
    "type": "article"
  },
  {
    "title": "Modeling and improving security of a local disk system for write-intensive workloads",
    "doi": "https://doi.org/10.1145/1210596.1210598",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Mais Nijim; Xiao Qin; Tao Xie",
    "corresponding_authors": "",
    "abstract": "Since security is of critical importance for modern storage systems, it is imperative to protect stored data from being tampered with or disclosed. Although an increasing number of secure storage systems have been developed, there is no way to dynamically choose security services to meet disk requests' flexible security requirements. Furthermore, existing security techniques for disk systems are not suitable to guarantee desired response times of disk requests. We remedy this situation by proposing an adaptive strategy (referred to as AWARDS) that can judiciously select the most appropriate security service for each write request, while endeavoring to guarantee the desired response times of all disk requests. To prove the efficiency of the proposed approach, we build an analytical model to measure the probability that a disk request is completed before its desired response time. The model also can be used to derive the expected value of disk requests' security levels. Empirical results based on synthetic workloads as well as real I/O-intensive applications show that AWARDS significantly improves overall performance over an existing scheme by up to 358.9% (with an average of 213.4%).",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2142390967",
    "type": "article"
  },
  {
    "title": "Zoned-RAID",
    "doi": "https://doi.org/10.1145/1227835.1227836",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Seon Ho Kim; Hong Zhu; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "The RAID (Redundant Array of Inexpensive Disks) system has been widely used in practical storage applications for better performance, cost effectiveness, and reliability. This study proposes a novel variant of RAID named Zoned-RAID (Z-RAID). Z-RAID improves the performance of traditional RAID by utilizing the zoning property of modern disks which provides multiple zones with different data transfer rates within a disk. Z-RAID levels 1, 5, and 6 are introduced to enhance the effective data transfer rate of RAID levels 1, 5, and 6, respectively, by constraining the placement of data blocks in multizone disks. We apply the Z-RAID to a practical and popular application, streaming media server, that requires a high-data transfer rate as well as a high reliability. The analytical and experimental results demonstrate the superiority of Z-RAID to conventional RAID. Z-RAID provides a higher effective data transfer rate in normal mode with no disadvantage. In the presence of a disk failure, Z-RAID still performs as well as RAID.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2296698683",
    "type": "article"
  },
  {
    "title": "Request Bridging and Interleaving",
    "doi": "https://doi.org/10.1145/1970348.1970349",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "Dong In Shin; Young Jin Yu; Hyeong S. Kim; Hyeonsang Eom; Heon Y. Yeom",
    "corresponding_authors": "",
    "abstract": "Write-through caching in modern disk drives enables the protection of data in the event of power failures as well as from certain disk errors when the write-back cache does not. Host system can achieve these benefits at the price of significant performance degradation, especially for small disk writes. We present new block-level techniques to address the performance problem of write-through caching disks. Our techniques are strongly motivated by some interesting results when the disk-level caching is turned off. By extending the conventional request merging, request bridging increases the request size and amortizes the inherent delays in the disk drive across more bytes of data. Like sector interleaving, request interleaving rearranges requests to prevent the disk head from missing the target sector position in close proximity, and thus reduces disk latency. We have evaluated our block-level approach using a variety of I/O workloads and shown that it increases disk I/O throughput by up to about 50%. For some real-world workloads, the disk performance is comparable or even superior to that of using the write-back disk cache. In practice, our simple yet effective solutions achieve better tradeoffs between data reliability and disk performance when applied to write-through caching disks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1979519392",
    "type": "article"
  },
  {
    "title": "Generalized Optimal Response Time Retrieval of Replicated Data from Storage Arrays",
    "doi": "https://doi.org/10.1145/2491472.2491474",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Nihat Altiparmak; Ali Şaman Tosun",
    "corresponding_authors": "",
    "abstract": "Declustering techniques reduce query response times through parallel I/O by distributing data among parallel disks. Recently, replication-based approaches were proposed to further reduce the response time. Efficient retrieval of replicated data from multiple disks is a challenging problem. Existing retrieval techniques are designed for storage arrays with identical disks, having no initial load or network delay. In this article, we consider the generalized retrieval problem of replicated data where the disks in the system might be heterogeneous, the disks may have initial load, and the storage arrays might be located on different sites. We first formulate the generalized retrieval problem using a Linear Programming (LP) model and solve it with mixed integer programming techniques. Next, the generalized retrieval problem is formulated as a more efficient maximum flow problem. We prove that the retrieval schedule returned by the maximum flow technique yields the optimal response time and this result matches the LP solution. We also propose a low-complexity online algorithm for the generalized retrieval problem by not guaranteeing the optimality of the result. Performance of proposed and state of the art retrieval strategies are investigated using various replication schemes, query types, query loads, disk specifications, network delays, and initial loads.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2015652866",
    "type": "article"
  },
  {
    "title": "Transparent Online Storage Compression at the Block-Level",
    "doi": "https://doi.org/10.1145/2180905.2180906",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Yannis Klonatos; Thanos Makatos; Manolis Marazakis; Michail D. Flouris; Angelos Bilas",
    "corresponding_authors": "",
    "abstract": "In this work, we examine how transparent block-level compression in the I/O path can improve both the space efficiency and performance of online storage. We present ZBD , a block-layer driver that transparently compresses and decompresses data as they flow between the file-system and storage devices. Our system provides support for variable-size blocks, metadata caching, and persistence, as well as block allocation and cleanup. ZBD targets maintaining high performance, by mitigating compression and decompression overheads that can have a significant impact on performance by leveraging modern multicore CPUs through explicit work scheduling. We present two case-studies for compression. First, we examine how our approach can be used to increase the capacity of SSD-based caches, thus increasing their cost-effectiveness. Then, we examine how ZBD can improve the efficiency of online disk-based storage systems. We evaluate our approach in the Linux kernel on a commodity server with multicore CPUs, using PostMark, SPECsfs2008, TPC-C, and TPC-H. Preliminary results show that transparent online block-level compression is a viable option for improving effective storage capacity, it can improve I/O performance up to 80% by reducing I/O traffic and seek distance, and has a negative impact on performance, up to 34%, only when single-thread I/O latency is critical. In particular, for SSD-based caching, our results indicate that, in line with current technology trends, compressed caching trades off CPU utilization for performance and enhances SSD efficiency as a storage cache up to 99%.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2062730552",
    "type": "article"
  },
  {
    "title": "CDF-LDPC",
    "doi": "https://doi.org/10.1145/3017430",
    "publication_date": "2017-02-25",
    "publication_year": 2017,
    "authors": "Shigui Qi; Dan Feng; Nan Su; Linjun Mei; Jingning Liu",
    "corresponding_authors": "",
    "abstract": "The raw error rate of a Solid-State drive (SSD) increases gradually with the increase of Program/Erase (P/E) cycles, retention time, and read cycles. Traditional approaches often use Error Correction Code (ECC) to ensure the reliability of SSDs. For error-free flash memory pages, time costs spent on ECC are redundant and make read performance suboptimal. This article presents a CRC-Detect-First LDPC (CDF-LDPC) algorithm to optimize the read performance of SSDs. The basic idea is to bypass Low-Density Parity-Check (LDPC) decoding of error-free flash memory pages, which can be found using a Cyclic Redundancy Check (CRC) code. Thus, error-free pages can be read directly without sacrificing the reliability of SSDs. Experiment results show that the read performance is improved more than 50% compared with traditional approaches. In particular, when idle time of benchmarks and SSD parallelism are exploited, CDF-LDPC can be performed more efficiently. In this case, the read performance of SSDs can be improved up to about 80% compared to that of the state-of-art.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2591582708",
    "type": "article"
  },
  {
    "title": "Systematic Erasure Codes with Optimal Repair Bandwidth and Storage",
    "doi": "https://doi.org/10.1145/3109479",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Qing Liu; Dan Feng; Hong Jiang; Yuchong Hu; Tianfeng Jiao",
    "corresponding_authors": "",
    "abstract": "Erasure codes are widely used in distributed storage systems to prevent data loss. Traditional codes suffer from a typical repair-bandwidth problem in which the amount of data required to reconstruct the lost data, referred to as the repair bandwidth, is often far more than the theoretical minimum. While many novel codes have been proposed in recent years to reduce the repair bandwidth, these codes either require extra storage and computation overhead or are only applicable to some special cases. To address the weaknesses of the existing solutions to the repair-bandwidth problem, we propose Z Codes, a general family of codes capable of achieving the theoretical lower bound of repair bandwidth versus storage. To the best of our knowledge, the Z codes are the first general systematic erasure codes that jointly achieve optimal repair bandwidth and storage. Further, we generalize the Z codes to the GZ codes to gain the Maximum Distance Separable (MDS) property. Our evaluations of a real system indicate that Z/GZ and Reed-Solomon (RS) codes show approximately close encoding and repairing speeds, while GZ codes achieve over 37.5% response time reduction for repairing the same size of data, compared to the RS and Cauchy Reed-Solomon (CRS) codes.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2759157621",
    "type": "article"
  },
  {
    "title": "CosaFS",
    "doi": "https://doi.org/10.1145/3149482",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Lingfang Zeng; Zehao Zhang; Yang Wang; Dan Feng; Kenneth B. Kent",
    "corresponding_authors": "",
    "abstract": "In this article, we design and implement a cooperative shingle-aware file system, called CosaFS , on heterogeneous storage devices that mix solid-state drives (SSDs) and shingled magnetic recording (SMR) technology to improve the overall performance of storage systems. The basic idea of CosaFS is to classify objects as hot or cold objects based on a proposed Lookahead with Recency Weight scheme. If an object is identified as a hot (small) object, then it will be served by SSD. Otherwise, cold (large) objects are stored on SMR. For an SMR, large objects can be accessed in large sequential blocks, rendering the performance of their accesses comparable with that of accessing the same large sequential blocks as if they were stored on a hard drive. Small objects, such as inodes and directories, are stored on the SSD where “seeks” for such objects are nearly free. With thorough empirical studies, we demonstrate that CosaFS, as a cooperative shingle-aware file system, with metadata separation and cache-assistance, is a very effective way to handle the disk-based data demanded by the shingled writes and outperforms the device- and host-side shingle-aware file systems in terms of throughput, IOPS, and access latency as well.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2770600168",
    "type": "article"
  },
  {
    "title": "Write Energy Reduction for PCM via Pumping Efficiency Improvement",
    "doi": "https://doi.org/10.1145/3200139",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Huizhang Luo; Qing Liu; Jingtong Hu; Qiao Li; Liang Shi; Qingfeng Zhuge; Edwin H.‐M. Sha",
    "corresponding_authors": "",
    "abstract": "The emerging Phase Change Memory (PCM) is considered to be a promising candidate to replace DRAM as the next generation main memory due to its higher scalability and lower leakage power. However, the high write power consumption has become a major challenge in adopting PCM as main memory. In addition to the fact that writing to PCM cells requires high write current and voltage, current loss in the charge pumps also contributes a large percentage of high power consumption. The pumping efficiency of a PCM chip is a concave function of the write current. Leveraging the characteristics of the concave function, the overall pumping efficiency can be improved if the write current is uniform. In this article, we propose a peak-to-average (PTA) write scheme, which smooths the write current fluctuation by regrouping write units. In particular, we calculate the current requirements for each write unit by their values when they are evicted from the last level cache (LLC). When the write units are waiting in the memory controller, we regroup the write units by LLC-assisted PTA to reach the current-uniform goal. Experimental results show that LLC-assisted PTA achieved 13.4% of overall energy saving compared to the baseline.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2903120202",
    "type": "article"
  },
  {
    "title": "CrashMonkey and ACE",
    "doi": "https://doi.org/10.1145/3320275",
    "publication_date": "2019-04-20",
    "publication_year": 2019,
    "authors": "Jayashree Mohan; Ashlie Martinez; Soujanya Ponnapalli; Pandian Raju; Vijay Chidambaram",
    "corresponding_authors": "",
    "abstract": "We present C rash M onkey and A ce , a set of tools to systematically find crash-consistency bugs in Linux file systems. C rash M onkey is a record-and-replay framework which tests a given workload on the target file system by simulating power-loss crashes while the workload is being executed, and checking if the file system recovers to a correct state after each crash. A ce automatically generates all the workloads to be run on the target file system. We build C rash M onkey and A ce based on a new approach to test file-system crash consistency: bounded black-box crash testing ( B 3 ). B 3 tests the file system in a black-box manner using workloads of file-system operations. Since the space of possible workloads is infinite, B 3 bounds this space based on parameters such as the number of file-system operations or which operations to include, and exhaustively generates workloads within this bounded space. B 3 builds upon insights derived from our study of crash-consistency bugs reported in Linux file systems in the last 5 years. We observed that most reported bugs can be reproduced using small workloads of three or fewer file-system operations on a newly created file system, and that all reported bugs result from crashes after fsync()-related system calls. C rash M onkey and A ce are able to find 24 out of the 26 crash-consistency bugs reported in the last 5 years. Our tools also revealed 10 new crash-consistency bugs in widely used, mature Linux file systems, 7 of which existed in the kernel since 2014. Additionally, our tools found a crash-consistency bug in a verified file system, FSCQ. The new bugs result in severe consequences like broken rename atomicity, loss of persisted files and directories, and data loss.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2942185437",
    "type": "article"
  },
  {
    "title": "SolarDB",
    "doi": "https://doi.org/10.1145/3318158",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Τao Zhu; Zhuoyue Zhao; Feifei Li; Weining Qian; Aoying Zhou; Dong Xie; Ryan Stutsman; Haining Li; Huiqi Hu",
    "corresponding_authors": "",
    "abstract": "Efficient transaction processing over large databases is a key requirement for many mission-critical applications. Although modern databases have achieved good performance through horizontal partitioning, their performance deteriorates when cross-partition distributed transactions have to be executed. This article presents SolarDB, a distributed relational database system that has been successfully tested at a large commercial bank. The key features of SolarDB include (1) a shared-everything architecture based on a two-layer log-structured merge-tree; (2) a new concurrency control algorithm that works with the log-structured storage, which ensures efficient and non-blocking transaction processing even when the storage layer is compacting data among nodes in the background; and (3) find-grained data access to effectively minimize and balance network communication within the cluster. According to our empirical evaluations on TPC-C, Smallbank, and a real-world workload, SolarDB outperforms the existing shared-nothing systems by up to 50x when there are close to or more than 5% distributed transactions.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2954857688",
    "type": "article"
  },
  {
    "title": "Visualizing Block IO Workloads",
    "doi": "https://doi.org/10.1145/2651422",
    "publication_date": "2015-03-20",
    "publication_year": 2015,
    "authors": "Ohad Rodeh; Haim Helman; D. D. Chambliss",
    "corresponding_authors": "",
    "abstract": "Massive block IO systems are the workhorses powering many of today’s largest applications. Databases, health care systems, and virtual machine images are examples for block storage applications. The massive scale of these workloads, and the complexity of the underlying storage systems, makes it difficult to pinpoint problems when they occur. This work attempts to shed light on workload patterns through visualization, aiding our intuition. We describe our experience in the last 3 years of analyzing and visualizing customer traces from XIV, an IBM enterprise block storage system. We also present results from applying the same visualization technology to Linux filesystems. We show how visualization aids our understanding of workloads and how it assists in resolving customer performance problems.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2077601829",
    "type": "article"
  },
  {
    "title": "Thermal issues in disk drive design",
    "doi": "https://doi.org/10.1145/1138041.1138044",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Sudhanva Gurumurthi; Anand Sivasubramaniam",
    "corresponding_authors": "",
    "abstract": "The importance of pushing the performance envelope of disk drives continues to grow in the enterprise storage market. One of the most fundamental factors impacting disk drive design is heat dissipation, since it directly affects drive reliability. Until now, drive manufacturers have continued to meet the 40% annual growth target of the internal data-rates (IDR) by increasing RPMs and shrinking platter sizes, both of which have counteracting effects on the heat dissipation within a drive. In this article, we shall show that we are getting to a point where it is going to be very difficult to stay on this roadmap. We first present detailed models that capture the close relationships between capacity, performance, and thermal characteristics over time. Using these models, we quantify the drop-off in IDR growth rates over the next decade if we are to adhere to the thermal design envelope. We motivate the need for continued improvements in IDR by showing that the response times of real workloads can be improved by 30--60% with a 10K increase in the RPM for disks used in their respective storage systems. We then present two dynamic thermal management (DTM) techniques that can be used to buy back some of this IDR loss. The first DTM technique exploits the thermal slack between what the drive was intended to support and the currently lower operating temperature to ramp up the RPM. The second DTM technique assumes that the drive is only designed for average case operation and dynamically throttles its activities to remain within the thermal envelope.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2006940900",
    "type": "article"
  },
  {
    "title": "Building MEMS-based storage systems for streaming media",
    "doi": "https://doi.org/10.1145/1242520.1242523",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Raju Rangaswami; Zoran Dimitrijević; Edward Yi Chang; Klaus E. Schauser",
    "corresponding_authors": "",
    "abstract": "The performance of streaming media servers has been limited by the dual requirements of high disk throughput (to service more clients simultaneously) and low memory use (to decrease system cost). To achieve high disk throughput, disk drives must be accessed with large IOs to amortize disk access overhead. Large IOs imply an increased requirement of expensive DRAM, and, consequently, greater overall system cost. MEMS-based storage, an emerging storage technology, is predicted to offer a price-performance point between those of DRAM and disk drives. In this study, we propose storage architectures that use the relatively inexpensive MEMS-based storage devices as an intermediate layer (between DRAM and disk drives) for temporarily staging large disk IOs at a significantly lower cost. We present data layout mechanisms and synchronized IO scheduling algorithms for the real-time storage and retrieval of streaming data within such an augmented storage system. Analytical evaluation suggests that MEMS-augmented storage hierarchies can reduce the cost and improve the throughput of streaming servers significantly.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2014656287",
    "type": "article"
  },
  {
    "title": "FluidSMR: Adaptive Management for Hybrid SMR Drives",
    "doi": "https://doi.org/10.1145/3465404",
    "publication_date": "2021-10-15",
    "publication_year": 2021,
    "authors": "Fenggang Wu; Bingzhe Li; David Hung-Chang Du",
    "corresponding_authors": "",
    "abstract": "Hybrid Shingled Magnetic Recording (H-SMR) drives are the most recently developed SMR drives, which allow dynamic conversion of the recording format between Conventional Magnetic Recording (CMR) and SMR on a single disk drive. We identify the unique opportunities of H-SMR drives to manage the tradeoffs between performance and capacity, including the possibility of adjusting the SMR area capacity based on storage usage and the flexibility of dynamic data swapping between the CMR area and SMR area. We design and implement FluidSMR, an adaptive management scheme for hybrid SMR Drives, to fully utilize H-SMR drives under different workloads and capacity usages. FluidSMR has a two-phase allocation scheme to support a growing usage of the H-SMR drive. The scheme can intelligently determine the sizes of the CMR and the SMR space in an H-SMR drive based on the dynamic changing of workloads. Moreover, FluidSMR uses a cache in the CMR region, managed by a proposed loop-back log policy, to reduce the overhead of updates to the SMR region. Evaluations using enterprise traces demonstrate that FluidSMR outperforms baseline schemes in various workloads by decreasing the average I/O latency and effectively reducing/controlling the performance impact of the format conversion between CMR and SMR.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3207311093",
    "type": "article"
  },
  {
    "title": "YouChoose",
    "doi": "https://doi.org/10.1145/2027066.2027069",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Xuechen Zhang; Yuehai Xu; Song Jiang",
    "corresponding_authors": "",
    "abstract": "Currently the QoS requirements for storage systems are usually presented in the form of service-level agreement (SLA) to bound I/O measures such as latency and throughput of I/O requests. However, SLA is not an effective performance interface for users to specify their required I/O service quality for two major reasons. First, for users it is difficult to determine appropriate latency and throughput bounds to ensure their required application performance without resource over-provisioning. Second, for storage system administrators it is a challenge to estimate a user’s real resource demand because the specified SLA measures are not consistently correlated with the user’s resource demand. This makes resource provisioning and scheduling less informative and can greatly reduce system efficiency. We propose the concept of reference storage system (RSS), which can be a storage system chosen by users and whose performance can be measured offline and mimicked online, as a performance interface between applications and storage servers. By designating an RSS to represent I/O performance requirement, a user can expect the performance received from a shared storage server servicing his I/O workload is not worse than the performance received from the RSS servicing the same workload. The storage system is responsible for implementing the RSS interface. The key enabling techniques are a machine learning model that derives request-specific performance requirements and an RSS-centric scheduling that efficiently allocates resource among requests from different users. The proposed scheme, named as YouChoose , supports the user-chosen performance interface through efficiently implementing and migrating virtual storage devices in a host storage system. Our evaluation based on trace-driven simulations shows that YouChoose can precisely implement the RSS performance interface, achieve a strong performance assurance and isolation, and improve the efficiency of a consolidated storage system consisting of different types of storage devices.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2027746023",
    "type": "article"
  },
  {
    "title": "Hybrid Associative Flash Translation Layer for the Performance Optimization of Chip-Level Parallel Flash Memory",
    "doi": "https://doi.org/10.1145/2535931",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Se Jin Kwon; Hyung-Ju Cho; Tae‐Sun Chung",
    "corresponding_authors": "",
    "abstract": "Flash memory is used widely in the data storage market, particularly low-price MultiLevel Cell (MLC) flash memory, which has been adopted by large-scale storage systems despite its low performance. To overcome the poor performance of MLC flash memory, a system architecture has been designed to optimize chip-level parallelism. This design increases the size of the page unit and the block unit, thereby simultaneously executing operations on multiple chips. Unfortunately, its Flash Translation Layer (FTL) generates many unused sectors in each page, which leads to unnecessary write operations. Furthermore, it reuses an earlier log block scheme, although it generates many erase operations because of its low space utilization. To solve these problems, we propose a hybrid associative FTL (Hybrid-FTL) to enhance the performance of the chip-level parallel flash memory system. Hybrid-FTL reduces the number of write operations by utilizing all of the unused sectors. Furthermore, it reduces the overall number of erase operations by classifying data as hot, cold, or fragment data. Hybrid-FTL requires less mapping information in the DRAM and in the flash memory compared with previous FTL algorithms.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2089922579",
    "type": "article"
  },
  {
    "title": "Customizable SLO and Its Near-Precise Enforcement for Storage Bandwidth",
    "doi": "https://doi.org/10.1145/2998454",
    "publication_date": "2017-02-16",
    "publication_year": 2017,
    "authors": "Ning Li; Hong Jiang; Dan Feng; Zhan Shi",
    "corresponding_authors": "",
    "abstract": "Cloud service is being adopted as a utility for large numbers of tenants by renting Virtual Machines (VMs). But for cloud storage, unpredictable IO characteristics make accurate Service-Level-Objective (SLO) enforcement challenging. As a result, it has been very difficult to support simple-to-use and technology-agnostic SLO specifying a particular value for a specific metric (e.g., storage bandwidth). This is because the quality of SLO enforcement depends on performance error and fluctuation that measure the precision of SLO enforcement . High precision of SLO enforcement is critical for user-oriented performance customization and user experiences. To address this challenge, this article presents V-Cup, a framework for VM-oriented customizable SLO and its near-precise enforcement. It consists of multiple auto-tuners, each of which exports an interface for a tenant to customize the desired storage bandwidth for a VM and enable the storage bandwidth of the VM to converge on the target value with a predictable precision. We design and implement V-Cup in the Xen hypervisor based on the fair sharing scheduler for VM-level resource management. Our V-Cup prototype evaluation shows that it achieves satisfying performance guarantees through near-precise SLO enforcement.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2588541081",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on USENIX FAST 2016",
    "doi": "https://doi.org/10.1145/3039209",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2600464027",
    "type": "article"
  },
  {
    "title": "SmartCon",
    "doi": "https://doi.org/10.1145/2631922",
    "publication_date": "2015-03-20",
    "publication_year": 2015,
    "authors": "Jongmin Gim; Tae-Ho Hwang; Youjip Won; Krishna Kant",
    "corresponding_authors": "",
    "abstract": "Handling of storage IO in modern operating systems assumes that such devices are slow and CPU cycles are valuable. Consequently, to effectively exploit the underlying hardware resources, for example, CPU cycles, storage bandwidth and the like, whenever an IO request is issued to such device, the requesting thread is switched out in favor of another thread that may be ready to execute. Recent advances in nonvolatile storage technologies and multicore CPUs make both of these assumptions increasingly questionable, and an unconditional context switch is no longer desirable. In this article, we propose a novel mechanism called SmartCon, which intelligently decides whether to service a given IO request in interrupt-driven manner or busy-wait--based manner based on not only the device characteristics but also dynamic parameters such as IO latency, CPU utilization, and IO size. We develop an analytic performance model to project the performance of SmartCon for forthcoming devices. We implement SmartCon mechanism on Linux 2.6 and perform detailed evaluation using three different IO devices: Ramdisk, low-end SSD, and high-end SSD. We find that SmartCon yields up to a 39% performance gain over the mainstream block device approach for Ramdisk, and up to a 45% gain for PCIe-based SSD and SATA-based SSDs. We examine the detailed behavior of TLB, L1, L2 cache and show that SmartCon achieves significant improvement in all cache misbehaviors.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1466072757",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on <i>MSST</i> 2015",
    "doi": "https://doi.org/10.1145/2853993",
    "publication_date": "2016-01-13",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2235744184",
    "type": "article"
  },
  {
    "title": "GCTrees",
    "doi": "https://doi.org/10.1145/2857056",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Chris Dragga; Douglas J. Santry",
    "corresponding_authors": "",
    "abstract": "File-system snapshots have been a key component of enterprise storage management since their inception. Creating and managing them efficiently, while maintaining flexibility and low overhead, has been a constant struggle. Although the current state-of-the-art mechanism—hierarchical reference counting—performs reasonably well for traditional small-file workloads, these workloads are increasingly vanishing from the enterprise data center, replaced instead with virtual machine and database workloads. These workloads center around a few very large files, violating the assumptions that allow hierarchical reference counting to operate efficiently. To better cope with these workloads, we introduce Generational Chain Trees (GCTrees), a novel method of space management that uses concepts of block lineage across snapshots rather than explicit reference counting. As a proof of concept, we create a prototype file system—gcext4, a modified version of ext4 that uses GCTrees as a basis for snapshots and copy-on-write. In evaluating this prototype empirically, we find that although they have a somewhat higher overhead for traditional workloads, GCTrees have dramatically lower overhead than hierarchical reference counting for large-file workloads, improving by a factor of 34 or more in some cases. Furthermore, gcext4 performs comparably to ext4 across all workloads, showing that GCTrees impose minor cost for their benefits.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2314336249",
    "type": "article"
  },
  {
    "title": "Optimizing Storage Performance with Calibrated Interrupts",
    "doi": "https://doi.org/10.1145/3505139",
    "publication_date": "2022-02-28",
    "publication_year": 2022,
    "authors": "A.T. Tai; I Smolyar; Michael Wei; Dan Tsafrir",
    "corresponding_authors": "",
    "abstract": "After request completion, an I/O device must decide whether to minimize latency by immediately firing an interrupt or to optimize for throughput by delaying the interrupt, anticipating that more requests will complete soon and help amortize the interrupt cost. Devices employ adaptive interrupt coalescing heuristics that try to balance between these opposing goals. Unfortunately, because devices lack the semantic information about which I/O requests are latency-sensitive, these heuristics can sometimes lead to disastrous results. Instead, we propose addressing the root cause of the heuristics problem by allowing software to explicitly specify to the device if submitted requests are latency-sensitive. The device then “calibrates” its interrupts to completions of latency-sensitive requests. We focus on NVMe storage devices and show that it is natural to express these semantics in the kernel and the application and only requires a modest two-bit change to the device interface. Calibrated interrupts increase throughput by up to 35%, reduce CPU consumption by as much as 30%, and achieve up to 37% lower latency when interrupts are coalesced.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3177763227",
    "type": "article"
  },
  {
    "title": "WebAssembly-based Delta Sync for Cloud Storage Services",
    "doi": "https://doi.org/10.1145/3502847",
    "publication_date": "2022-08-30",
    "publication_year": 2022,
    "authors": "Jianwei Zheng; Zhenhua Li; Yuanhui Qiu; Hao Lin; Xiao He; Yang Li; Yunhao Liu",
    "corresponding_authors": "",
    "abstract": "Delta synchronization (sync) is crucial to the network-level efficiency of cloud storage services, especially when handling large files with small increments. Practical delta sync techniques are, however, only available for PC clients and mobile apps, but not web browsers—the most pervasive and OS-independent access method. To bridge this gap, prior work concentrates on either reversing the delta sync protocol or utilizing the native client, all striving around the tradeoffs among efficiency, applicability, and usability and thus forming an “impossible triangle.” Recently, we note the advent of WebAssembly (WASM) , a portable binary instruction format that is efficient in both encoding size and load time. In principle, the unique advantages of WASM can make web-based applications enjoy near-native runtime speed without significant cloud-side or client-side changes. Thus, we implement a straightforward WASM-based delta sync solution, WASMrsync, finding its quasi-asynchronous working manner and conventional In-situ Separate Memory Allocation greatly increase sync time and memory usage. To address them, we strategically devise sync-async code decoupling and streaming compilation, together with Informed In-place File Construction. The resulting solution, WASMrsync+, achieves comparable sync time as the state-of-the-art (most efficient) solution with nearly only half of memory usage, letting the “impossible triangle” reach a reconciliation.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4293547371",
    "type": "article"
  },
  {
    "title": "A Disk Failure Prediction Method Based on Active Semi-supervised Learning",
    "doi": "https://doi.org/10.1145/3523699",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Yang Zhou; Fang Wang; Dan Feng",
    "corresponding_authors": "",
    "abstract": "Disk failure has always been a major problem for data centers, leading to data loss. Current disk failure prediction approaches are mostly offline and assume that the disk labels required for training learning models are available and accurate. However, these offline methods are no longer suitable for disk failure prediction tasks in large-scale data centers. Behind this explosive amount of data, most methods do not consider whether it is not easy to get the label values during the training or the obtained label values are not completely accurate. These problems further restrict the development of supervised learning and offline modeling in disk failure prediction. In this article, Active Semi-supervised Learning Disk-failure Prediction ( ASLDP ), a novel disk failure prediction method is proposed, which uses active learning and semi-supervised learning. According to the characteristics of data in the disk lifecycle, ASLDP carries out active learning for those clear labeled samples, which selects valuable samples with the most significant probability uncertainty and eliminates redundancy. For those samples that are unclearly labeled or unlabeled, ASLDP uses semi-supervised learning for pre-labeled by calculating the conditional values of the samples and enhances the generalization ability by active learning. Compared with several state-of-the-art offline and online learning approaches, the results on four realistic datasets from Backblaze and Baidu demonstrate that ASLDP achieves stable failure detection rates of 80–85% with low false alarm rates. In addition, we use a dataset from Alibaba to evaluate the generality of ASLDP . Furthermore, ASLDP can overcome the problem of missing sample labels and data redundancy in large data centers, which are not considered and implemented in all offline learning methods for disk failure prediction to the best of our knowledge. Finally, ASLDP can predict the disk failure 4.9 days in advance with lower overhead and latency.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4297321247",
    "type": "article"
  },
  {
    "title": "Improving Storage Systems Using Machine Learning",
    "doi": "https://doi.org/10.1145/3568429",
    "publication_date": "2022-11-21",
    "publication_year": 2022,
    "authors": "Ibrahim Umit Akgun; Ali Selman Aydin; Andrew Burford; Michael McNeill; Michael Arkhangelskiy; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "Operating systems include many heuristic algorithms designed to improve overall storage performance and throughput. Because such heuristics cannot work well for all conditions and workloads, system designers resorted to exposing numerous tunable parameters to users—thus burdening users with continually optimizing their own storage systems and applications. Storage systems are usually responsible for most latency in I/O-heavy applications, so even a small latency improvement can be significant. Machine learning (ML) techniques promise to learn patterns, generalize from them, and enable optimal solutions that adapt to changing workloads. We propose that ML solutions become a first-class component in OSs and replace manual heuristics to optimize storage systems dynamically. In this article, we describe our proposed ML architecture, called KML. We developed a prototype KML architecture and applied it to two case studies: optimizing readahead and NFS read-size values. Our experiments show that KML consumes less than 4 KB of dynamic kernel memory, has a CPU overhead smaller than 0.2%, and yet can learn patterns and improve I/O throughput by as much as 2.3× and 15× for two case studies—even for complex, never-seen-before, concurrently running mixed workloads on different storage devices.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4309529650",
    "type": "article"
  },
  {
    "title": "Boosting Cache Performance by Access Time Measurements",
    "doi": "https://doi.org/10.1145/3572778",
    "publication_date": "2023-01-14",
    "publication_year": 2023,
    "authors": "Gil Einziger; Omri Himelbrand; Erez Waisbard",
    "corresponding_authors": "",
    "abstract": "Most modern systems utilize caches to reduce the average data access time and optimize their performance. Recently proposed policies implicitly assume uniform access times, but variable access times naturally appear in domains such as storage, web search, and DNS resolution. Our work measures the access times for various items and exploits variations in access times as an additional signal for caching algorithms. Using such a signal, we introduce adaptive access time-aware cache policies that consistently improve the average access time compared with the best alternative in diverse workloads. Our adaptive algorithm attains an average access time reduction of up to 46% in storage workloads, up to 16% in web searches, and 8.4% on average when considering all experiments in our study.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4315977482",
    "type": "article"
  },
  {
    "title": "Visibility Graph-based Cache Management for DRAM Buffer Inside Solid-state Drives",
    "doi": "https://doi.org/10.1145/3586576",
    "publication_date": "2023-03-03",
    "publication_year": 2023,
    "authors": "Zhibing Sha; Jun Li; Fengxiang Zhang; Min Huang; Zhigang Cai; François Trahay; Jianwei Liao",
    "corresponding_authors": "",
    "abstract": "Most solid-state drives (SSDs) adopt an on-board Dynamic Random Access Memory (DRAM) to buffer the write data, which can significantly reduce the amount of write operations committed to the flash array of SSD if data exhibits locality in write operations. This article focuses on efficiently managing the small amount of DRAM cache inside SSDs. The basic idea is to employ the visibility graph technique to unify both temporal and spatial locality of references of I/O accesses, for directing cache management in SSDs. Specifically, we propose to adaptively generate the visibility graph of cached data pages and then support batch adjustment of adjacent or nearby (hot) cached data pages by referring to the connection situations in the visibility graph. In addition, we propose to evict the buffered data pages in batches by also referring to the connection situations, to maximize the internal flushing parallelism of SSD devices without worsening I/O congestion. The trace-driven simulation experiments show that our proposal can yield improvements on cache hits by between 0.8 % and 19.8 %, and the overall I/O latency by 25.6 % on average, compared to state-of-the-art cache management schemes inside SSDs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4323043789",
    "type": "article"
  },
  {
    "title": "A Universal SMR-aware Cache Framework with Deep Optimization for DM-SMR and HM-SMR Disks",
    "doi": "https://doi.org/10.1145/3588442",
    "publication_date": "2023-03-21",
    "publication_year": 2023,
    "authors": "Diansen Sun; Ruixiong Tan; Yunpeng Chai",
    "corresponding_authors": "",
    "abstract": "To satisfy the enormous storage capacities required for big data, data centers have been adopting high-density shingled magnetic recording (SMR) disks. However, the weak fine-grained random write performance of SMR disks caused by their inherent write amplification and unbalanced read–write performance poses a severe challenge. Many studies have proposed solid-state drive (SSD) cache systems to address this issue. However, existing cache algorithms, such as the least recently used (LRU) algorithm, which is used to optimize cache popularity, and the MOST algorithm, which is used to optimize the write amplification factor, cannot exploit the full performance of the proposed cache systems because of their inappropriate optimization objectives. This article proposes a new SMR-aware cache framework called SAC+ to improve SMR-based hybrid storage. SAC+ integrates the two dominant types of SMR drives—namely, drive-managed and host-managed SMR drives—and provides a universal framework implementation. In addition, SAC+ integrally combines the drive characteristics to optimize I/O performance. The results of evaluations conducted using real-world traces indicate that SAC+ reduces the I/O time by 36–93% compared with state-of-the-art algorithms.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4328127907",
    "type": "article"
  },
  {
    "title": "Explorations and Exploitation for Parity-based RAIDs with Ultra-fast SSDs",
    "doi": "https://doi.org/10.1145/3627992",
    "publication_date": "2023-10-16",
    "publication_year": 2023,
    "authors": "Shucheng Wang; Qiang Cao; Hong Jiang; Ziyi Lu; Jie Yao; Yuxing Chen; Anqun Pan",
    "corresponding_authors": "",
    "abstract": "Following a conventional design principle that pays more fast-CPU-cycles for fewer slow-I/Os, popular software storage architecture Linux Multiple-Disk (MD) for parity-based RAID (e.g., RAID5 and RAID6) assigns one or more centralized worker threads to efficiently process all user requests based on multi-stage asynchronous control and global data structures, successfully exploiting characteristics of slow devices, e.g., Hard Disk Drives (HDDs). However, we observe that, with high-performance NVMe-based Solid State Drives (SSDs), even the recently added multi-worker processing mode in MD achieves only limited performance gain because of the severe lock contentions under intensive write workloads. In this paper, we propose a novel stripe-threaded RAID architecture, StRAID, assigning a dedicated worker thread for each stripe-write (one-for-one model) to sufficiently exploit high parallelism inherent among RAID stripes, multi-core processors, and SSDs. For the notoriously performance-punishing partial-stripe writes that induce extra read and write I/Os, StRAID presents a two-stage stripe write mechanism and a two-dimensional multi-log SSD buffer. All writes first are opportunistically batched in memory, and then are written into the primary RAID for aggregated full-stripe writes or conditionally redirected to the buffer for partial-stripe writes. These buffered data are strategically reclaimed to the primary RAID. We evaluate a StRAID prototype with a variety of benchmarks and real-world traces. StRAID is demonstrated to outperform MD by up to 5.8 times in write throughput.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387664655",
    "type": "article"
  },
  {
    "title": "Perseid: A Secondary Indexing Mechanism for LSM-Based Storage Systems",
    "doi": "https://doi.org/10.1145/3633285",
    "publication_date": "2023-11-17",
    "publication_year": 2023,
    "authors": "Jing Wang; Youyou Lu; Qing Wang; Yuhao Zhang; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "LSM-based storage systems are widely used for superior write performance on block devices. However, they currently fail to efficiently support secondary indexing, since a secondary index query operation usually needs to retrieve multiple small values, which scatter in multiple LSM components. In this work, we revisit secondary indexing in LSM-based storage systems with byte-addressable persistent memory (PM). Existing PM-based indexes are not directly competent for efficient secondary indexing. We propose Perseid , an efficient PM-based secondary indexing mechanism for LSM-based storage systems, which takes into account both characteristics of PM and secondary indexing. Perseid consists of (1) a specifically designed secondary index structure that achieves high-performance insertion and query, (2) a lightweight hybrid PM-DRAM and hash-based validation approach to filter out obsolete values with subtle overhead, and (3) two adapted optimizations on primary table searching issued from secondary indexes to accelerate non-index-only queries. Our evaluation shows that Perseid outperforms existing PM-based indexes by 3–7× and achieves about two orders of magnitude performance of state-of-the-art LSM-based secondary indexing techniques even if on PM instead of disks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388763421",
    "type": "article"
  },
  {
    "title": "Efficient Directory Mutations in a Full-Path-Indexed File System",
    "doi": "https://doi.org/10.1145/3241061",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Yang Zhan; Yizheng Jiao; Donald E. Porter; Alex Conway; Eric Knorr; Martı́n Farach-Colton; Michael A. Bender; Jun Yuan; William Jannen; Rob Johnson",
    "corresponding_authors": "",
    "abstract": "Full-path indexing can improve I/O efficiency for workloads that operate on data organized using traditional, hierarchical directories, because data is placed on persistent storage in scan order. Prior results indicate, however, that renames in a local file system with full-path indexing are prohibitively expensive. This article shows how to use full-path indexing in a file system to realize fast directory scans, writes, and renames. The article introduces a range-rename mechanism for efficient key-space changes in a write-optimized dictionary. This mechanism is encapsulated in the key-value Application Programming Interface (API) and simplifies the overall file system design. We implemented this mechanism in B ε -trees File System (BetrFS), an in-kernel, local file system for Linux. This new version, BetrFS 0.4, performs recursive greps 1.5x faster and random writes 1.2x faster than BetrFS 0.3, but renames are competitive with indirection-based file systems for a range of sizes. BetrFS 0.4 outperforms BetrFS 0.3, as well as traditional file systems, such as ext4, Extents File System (XFS), and Z File System (ZFS), across a variety of workloads.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2902405086",
    "type": "article"
  },
  {
    "title": "HIL",
    "doi": "https://doi.org/10.1145/3281030",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Jin‐Yong Choi; Eyee Hyun Nam; Yoon Jae Seong; Jin Hyuk Yoon; Sookwan Lee; Hong Seok Kim; Jeongsu Park; Yeong-Jae Woo; Sheayun Lee; Sang Lyul Min",
    "corresponding_authors": "",
    "abstract": "We present a framework called Hierarchically Interacting Logs (HIL) for constructing Flash Translation Layers (FTLs). The main goal of the HIL framework is to heal the Achilles heel —the crash recovery—of FTLs (hence, its name). Nonetheless, the framework itself is general enough to encompass not only block-mapped and page-mapped FTLs but also many of their variants, including hybrid ones, because of its compositional nature. Crash recovery within the HIL framework proceeds in two phases: structural recovery and functional recovery. During the structural recovery, residual effects due to program operations ongoing at the time of the crash are eliminated in an atomic manner using shadow paging. During the functional recovery, operations that would have been performed if there had been no crash are replayed in a redo-only fashion. Both phases operate in an idempotent manner, preventing repeated crashes during recovery from causing any additional problems. We demonstrate the practicality of the proposed HIL framework by implementing a prototype and showing that its performance during normal execution and also during crash recovery is at least as good as those of state-of-the-art SSDs.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2903421363",
    "type": "article"
  },
  {
    "title": "TxFS",
    "doi": "https://doi.org/10.1145/3318159",
    "publication_date": "2019-05-08",
    "publication_year": 2019,
    "authors": "Yige Hu; Zhiting Zhu; Ian Neal; Youngjin Kwon; Tianyu Cheng; Vijay Chidambaram; Emmett Witchel",
    "corresponding_authors": "",
    "abstract": "We introduce TxFS, a transactional file system that builds upon a file system’s atomic-update mechanism such as journaling. Though prior work has explored a number of transactional file systems, TxFS has a unique set of properties: a simple API, portability across different hardware, high performance, low complexity (by building on the file-system journal), and full ACID transactions. We port SQLite, OpenLDAP, and Git to use TxFS and experimentally show that TxFS provides strong crash consistency while providing equal or better performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2944513339",
    "type": "article"
  },
  {
    "title": "ShieldNVM",
    "doi": "https://doi.org/10.1145/3381835",
    "publication_date": "2020-05-18",
    "publication_year": 2020,
    "authors": "Fan Yang; Youmin Chen; Haiyu Mao; Youyou Lu; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Data encryption and authentication are essential for secure non-volatile memory (NVM). However, the introduced security metadata needs to be atomically written back to NVM along with data, so as to provide crash consistency, which unfortunately incurs high overhead. To support fine-grained data protection and fast recovery for a secure NVM system without compromising the performance, we propose ShieldNVM. It first proposes an epoch-based mechanism to aggressively cache the security metadata in the metadata cache while retaining the consistency of them in NVM. Deferred spreading is also introduced to reduce the calculating overhead for data authentication. Leveraging the ability of data hash message authentication codes, we can always recover the consistent but old security metadata to its newest version. By recording a limited number of dirty addresses of the security metadata, ShieldNVM achieves fast recovering the secure NVM system after crashes. Compared to Osiris, a state-of-the-art secure NVM, ShieldNVM reduces system runtime by 39.1% and hash message authentication code computation overhead by 80.5% on average over NVM workloads. When system crashes happen, ShieldNVM’s recovery time is orders of magnitude faster than Osiris. In addition, ShieldNVM also recovers faster than AGIT, which is the Osiris-based state-of-the-art mechanism addressing the recovery time of the secure NVM system. Once the recovery process fails, instead of dropping all data due to malicious attacks, ShieldNVM is able to detect and locate the area of the tampered data with the help of the tracked addresses.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3027071396",
    "type": "article"
  },
  {
    "title": "Dynamic Synchronous/Asynchronous Replication",
    "doi": "https://doi.org/10.1145/2508011",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Assaf Natanzon; Eitan Bachmat",
    "corresponding_authors": "",
    "abstract": "Online, remote, data replication is critical for today’s enterprise IT organization. Availability of data is key to the success of the organization. A few hours of downtime can cost from thousands to millions of dollars With increasing frequency, companies are instituting disaster recovery plans to ensure appropriate data availability in the event of a catastrophic failure or disaster that destroys a site (e.g. flood, fire, or earthquake). Synchronous and asynchronous replication technologies have been available for a long period of time. Synchronous replication has the advantage of no data loss, but due to latency, synchronous replication is limited by distance and bandwidth. Asynchronous replication on the other hand has no distance limitation, but leads to some data loss which is proportional to the data lag. We present a novel method, implemented within EMC Recover-Point, which allows the system to dynamically move between these replication options without any disruption to the I/O path. As latency grows, the system will move from synchronous replication to semi-synchronous replication and then to snapshot shipping. It returns to synchronous replication as more bandwidth is available and latency allows.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1969044144",
    "type": "article"
  },
  {
    "title": "A caching-oriented management design for the performance enhancement of solid-state drives",
    "doi": "https://doi.org/10.1145/2093139.2093142",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Yuan-Hao Chang; Cheng-Kang Hsieh; Po‐Chun Huang; Pi-Cheng Hsiu",
    "corresponding_authors": "",
    "abstract": "While solid-state drives are excellent alternatives to hard disks in mobile devices, a number of performance and reliability issues need to be addressed. In this work, we design an efficient flash management scheme for the performance improvement of low-cost MLC flash memory devices. Specifically, we design an efficient flash management scheme for multi-chipped flash memory devices with cache support, and develop a two-level address translation mechanism with an adaptive caching policy. We evaluated the approach on real workloads. The results demonstrate that it can improve the performance of multi-chipped solid-state drives through logical-to-physical mappings and concurrent accesses to flash chips.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1980386317",
    "type": "article"
  },
  {
    "title": "MFTL",
    "doi": "https://doi.org/10.1145/2180905.2180908",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Jen-Wei Hsieh; Chung‐Hsien Wu; Ge-Ming Chiu",
    "corresponding_authors": "",
    "abstract": "NAND flash memory has gained its popularity in a variety of applications as a storage medium due to its low power consumption, nonvolatility, high performance, physical stability, and portability. In particular, Multi-Level Cell (MLC) flash memory, which provides a lower cost and higher density solution, has occupied the largest part of NAND flash-memory market share. However, MLC flash memory also introduces new challenges: (1) Pages in a block must be written sequentially. (2) Information to indicate a page being obsoleted cannot be recorded in its spare area due to the limitation on the number of partial programming. Since most of applications access NAND flash memory under FAT file system, this article designs an MLC Flash Translation Layer (MFTL) for flash-memory storage systems which takes constraints of MLC flash memory and access behaviors of FAT file system into consideration. A series of trace-driven simulations was conducted to evaluate the performance of the proposed scheme. Although MFTL is designed for MLC flash memory and FAT file system, it is applicable to SLC flash memory and other file systems as well. Our experiment results show that the proposed MFTL could achieve a good performance for various access patterns even on SLC flash memory.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2027385589",
    "type": "article"
  },
  {
    "title": "Generalized X-code",
    "doi": "https://doi.org/10.1145/2339118.2339121",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Xianghong Luo; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Many RAID-6 codes have been proposed in the literature, but each has its limitations. Horizontal code has the ability to adapt to the arbitrary size of a disk array but its high computational complexity is a major shortcoming. In contrast, the computational complexity of vertical code (e.g. X-code) often achieves the theoretical optimality, but vertical code is limited to using a prime number as the size of the disk array In this article, we propose a novel efficient RAID-6 code for arbitrary size of disk array: generalized X-code. We move the redundant elements along their calculation diagonals in X-code onto two specific disks and change two data elements into redundant elements in order to realize our new code. The generalized X-code achieves optimal encoding and updating complexity and low decoding complexity; in addition, it has the ability to adapt to arbitrary size of disk array. Furthermore, we also provide a method for generalizing horizontal code to achieve optimal encoding and updating complexity while keeping the code's original ability to adapt to arbitrary size of disk array.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2055318782",
    "type": "article"
  },
  {
    "title": "Fast file existence checking in archiving systems",
    "doi": "https://doi.org/10.1145/1970343.1970345",
    "publication_date": "2011-06-01",
    "publication_year": 2011,
    "authors": "Sašo Tomažič; Vesna Pavlović; Jasna Milovanovic; Jaka Sodnik; Anton Kos; Sara Stančin; Veljko Milutinović",
    "corresponding_authors": "",
    "abstract": "This article presents a new Fast Hash-based File Existence Checking (FHFEC) method for archiving systems. During the archiving process, there are many submissions which are actually unchanged files that do not need to be re-archived. In this system, instead of comparing the entire files, only digests of the files are compared. Strong cryptographic hash functions with a low probability of collision can be used as digests. We propose a fast algorithm to check if a certain hash, that is, a corresponding file, is already stored in the system. The algorithm is based on dividing the whole domain of hashes into equally sized regions, and on the existence of a pointer array, which has exactly one pointer for each region. Each pointer points to the location of the first stored hash from the corresponding region and has a null value if no hash from that region exists. The entire structure can be stored in random access memory or, alternatively, on a dedicated hard disk. A statistical performance analysis has been performed that shows that in certain cases FHFEC performs nearly optimally. Extensive simulations have confirmed these analytical results. The performance of FHFEC has been compared to the performance of a binary search (BIS) and B+tree, which are commonly used in file systems and databases for table indices. The results show that FHFEC significantly outperforms both of them.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2073674525",
    "type": "article"
  },
  {
    "title": "LoneStar RAID",
    "doi": "https://doi.org/10.1145/2840810",
    "publication_date": "2016-01-07",
    "publication_year": 2016,
    "authors": "Matthias Grawinkel; Lars Nagel; André Brinkmann",
    "corresponding_authors": "",
    "abstract": "The need for huge storage archives rises with the ever growing creation of data. With today’s big data and data analytics applications, some of these huge archives become active in the sense that all stored data can be accessed at any time. Running and evolving these archives is a constant tradeoff between performance, capacity, and price. We present the LoneStar RAID, a disk-based storage architecture, which focuses on high reliability, low energy consumption, and cheap reads. It is designed for MAID systems with up to hundreds of disk drives per server and is optimized for “write once, read sometimes” workloads. We use dedicated data and parity disks, and export the data disks as individually accessible buckets. By intertwining disk groups into a two-dimensional RAID and improving single-disk reliability with intradisk redundancy, the system achieves an elastic fault tolerance that can at least recover all 3-disk failures. Furthermore, we integrate a cache to offload parity updates and a journal to track the RAID’s state. The LoneStar RAID scheme provides a mean time to data loss (MTTDL) that competes with today’s erasure codes and is optimized to require only a minimal set of running disk drives.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2229628996",
    "type": "article"
  },
  {
    "title": "H-Scale",
    "doi": "https://doi.org/10.1145/2822895",
    "publication_date": "2016-04-29",
    "publication_year": 2016,
    "authors": "Jiguang Wan; Peng Xu; Xubin He; Jibin Wang; Junyao Li; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "To satisfy the explosive growth of data in large-scale data centers, where redundant arrays of independent disks (RAIDs), especially RAID-5, are widely deployed, effective storage scaling and disk expansion methods are desired. However, a way to reduce the data migration overhead and maintain the reliability of the original RAID are major concerns of storage scaling. To address these problems, we propose a new RAID scaling scheme, H-Scale, to achieve fast RAID scaling via hybrid stripe layouts. H-Scale takes advantage of the loose restriction of stripe structures to choose migrated data and to create hybrid stripe structures. The main advantages of our scheme include: (1) dramatically reducing the data migration overhead and thus speeding up the scaling process, (2) maintaining the original RAID’s reliability, (3) balancing the workload among disks after scaling, and (4) providing a general scaling approach for different RAID levels. Our theoretical analysis show that H-Scale outperforms existing scaling solutions in terms of data migration, I/O overheads, and parity update operations. Evaluation results on a prototype implementation demonstrate that H-Scale speeds up the online scaling process by up to 60% under SPC traces, and similar improvements on scaling time and user response time are also achieved by evaluations using standard benchmarks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2346524372",
    "type": "article"
  },
  {
    "title": "An approach to virtual allocation in storage systems",
    "doi": "https://doi.org/10.1145/1210596.1210597",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Sukwoo Kang; A. L. Narasimha Reddy",
    "corresponding_authors": "",
    "abstract": "This article presents virtual allocation , a scheme for flexible storage allocation. Virtual allocation separates storage allocation from the file system. It employs an allocate-on-write strategy which lets applications fit into the actual usage of storage space, without regard to the configured file system size. This improves flexibility by allowing storage space to be shared across different file systems. This article presents the design of virtual allocation and its evaluation through benchmarks. To illustrate our approach, we implemented a prototype system on PCs running Linux. We present the results from the prototype implementation and its evaluation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2011248562",
    "type": "article"
  },
  {
    "title": "Tunable randomization for load management in shared-disk clusters",
    "doi": "https://doi.org/10.1145/1044956.1044962",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Changxun Wu; Randal Burns",
    "corresponding_authors": "",
    "abstract": "We develop and evaluate a system for load management in shared-disk file systems built on clusters of heterogeneous computers. It balances workload by moving file sets among cluster server nodes. It responds to changing server resources that arise from failure and recovery, and dynamically adding or removing servers. It also realizes performance consistency---nearly uniform performance across all servers. The system is adaptive and self-tuning. It operates without any a priori knowledge of workload properties, or the capabilities of the servers. Rather, it continuously tunes load placement using a technique called adaptive, nonuniform (ANU) randomization. ANU randomization realizes the scalability and metadata reduction benefits of hash-based, randomized placement techniques, while avoiding hashing's drawbacks: load skew, inability to cope with heterogeneity, and lack of tunability. ANU randomization outperforms virtual-processor approaches to load balancing, while reducing the amount of shared state among servers and the amount of load movement.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2171395472",
    "type": "article"
  },
  {
    "title": "Contributing storage using the transparent file system",
    "doi": "https://doi.org/10.1145/1288783.1288787",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "James Cipar; Mark D. Corner; Emery D. Berger",
    "corresponding_authors": "",
    "abstract": "Contributory applications allow users to donate unused resources on their personal computers to a shared pool. Applications such as SETI@home, Folding@home, and Freenet are now in wide use and provide a variety of services, including data processing and content distribution. However, while several research projects have proposed contributory applications that support peer-to-peer storage systems, their adoption has been comparatively limited. We believe that a key barrier to the adoption of contributory storage systems is that contributing a large quantity of local storage interferes with the principal user of the machine. To overcome this barrier, we introduce the Transparent File System (TFS). TFS provides background tasks with large amounts of unreliable storage—all of the currently available space—without impacting the performance of ordinary file access operations. We show that TFS allows a peer-to-peer contributory storage system to provide 40% more storage at twice the performance when compared to a user-space storage mechanism. We analyze the impact of TFS on replication in peer-to-peer storage systems and show that TFS does not appreciably increase the resources needed for file replication.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2050755086",
    "type": "article"
  },
  {
    "title": "QoS for storage subsystems using IEEE-1394",
    "doi": "https://doi.org/10.1145/1480439.1480441",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Chih‐Yuan Huang; Tei‐Wei Kuo; Ai‐Chun Pang",
    "corresponding_authors": "",
    "abstract": "IEEE-1394 is widely adopted in various commercial products for computing, communication, and entertainment. Although many services with Quality-of-Service (QoS) supports are now available in systems over IEEE-1394, little work is done for QoS-based resource allocation. In this article, we aim at the design of a bandwidth reservation mechanism and its policy for isochronous requests, such as those from cameras. We then address the QoS support issue for asynchronous requests, such as those from disks, and an analytic framework for probability-based QoS guarantees. This work is concluded by the proposing of a topology configuration algorithm for IEEE-1394 devices. The capability of the proposed methodology and the analytic framework are evaluated by a series of experiments over a Linux-based system prototype.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1966283434",
    "type": "article"
  },
  {
    "title": "Bidirectional Database Storage and SQL Query Exploiting RRAM-Based Process-in-Memory Structure",
    "doi": "https://doi.org/10.1145/3177917",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Yuliang Sun; Yu Wang; Huazhong Yang",
    "corresponding_authors": "",
    "abstract": "With the coming of the “Big Data” era, a high-energy-efficiency database is demanded for the Internet of things (IoT) application scenarios. The emerging Resistive Random Access Memory (RRAM) has been considered as an energy-efficient replacement of DRAM for next-generation main memory. In this article, we propose an RRAM-based SQL query unit with process-in-memory (PIM) characteristics. A bidirectional storage structure for a database in RRAM crossbar array is proposed that avoids redundant data transfer to cache and reduces cache miss rate compared with the storage method in DRAM for an in-memory database. The proposed RRAM-based SQL query unit can support a representative subset of SQL queries in memory and thus can further reduce the data transfer cost. The corresponding query optimization method is proposed to fully utilize the PIM characteristics. Simulation results show that the energy efficiency of the proposed RRAM-based SQL query unit is increased by 4 to 6 orders of magnitudes compared with the traditional architecture.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2790638889",
    "type": "article"
  },
  {
    "title": "Workload Characterization for Enterprise Disk Drives",
    "doi": "https://doi.org/10.1145/3151847",
    "publication_date": "2018-04-12",
    "publication_year": 2018,
    "authors": "Anil Kashyap",
    "corresponding_authors": "Anil Kashyap",
    "abstract": "The article presents an analysis of drive workloads from enterprise storage systems. The drive workloads are obtained from field return units from a cross-section of enterprise storage system vendors and thus provides a view of the workload characteristics over a wide spectrum of end-user applications. The workload parameters that have been characterized include transfer lengths, access patterns, throughput, and utilization. The study shows that reads are the dominant workload accounting for 80% of the accesses to the drive. Writes are dominated by short block random accesses while reads range from random to highly sequential. A trend analysis over the period 2010–2014 shows that the workload has remained fairly constant even as the capacities of the drives shipped has steadily increased. The study shows that the data stored on disk drives is relatively cold—on average less than 4% of the drive capacity is accessed in a given 2h interval.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2797752849",
    "type": "article"
  },
  {
    "title": "INSTalytics",
    "doi": "https://doi.org/10.1145/3369738",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Muthian Sivathanu; Midhul Vuppalapati; Bhargav S. Gulavani; Kaushik Sunder Rajan; Jyoti Leeka; Jayashree Mohan; Piyus Kedia",
    "corresponding_authors": "",
    "abstract": "We present the design, implementation, and evaluation of INSTalytics , a co-designed stack of a cluster file system and the compute layer, for efficient big-data analytics in large-scale data centers. INSTalytics amplifies the well-known benefits of data partitioning in analytics systems; instead of traditional partitioning on one dimension, INSTalytics enables data to be simultaneously partitioned on four different dimensions at the same storage cost, enabling a larger fraction of queries to benefit from partition filtering and joins without network shuffle. To achieve this, INSTalytics uses compute-awareness to customize the three-way replication that the cluster file system employs for availability. A new heterogeneous replication layout enables INSTalytics to preserve the same recovery cost and availability as traditional replication. INSTalytics also uses compute-awareness to expose a new sliced-read API that improves performance of joins by enabling multiple compute nodes to read slices of a data block efficiently via co-ordinated request scheduling and selective caching at the storage nodes. We have built a prototype implementation of INSTalytics in a production analytics stack, and we show that recovery performance and availability is similar to physical replication, while providing significant improvements in query performance, suggesting a new approach to designing cloud-scale big-data analytics systems.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3002057446",
    "type": "article"
  },
  {
    "title": "File system virtual appliances",
    "doi": "https://doi.org/10.1145/2339118.2339120",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Michael Abd-El-Malek; Matthew Wachs; James Cipar; Karan Sanghi; Gregory R. Ganger; Garth A. Gibson; Michael K. Reiter",
    "corresponding_authors": "",
    "abstract": "File system virtual appliances (FSVAs) address the portability headaches that plague file system (FS) developers. By packaging their FS implementation in a virtual machine (VM), separate from the VM that runs user applications, they can avoid the need to port the file system to each operating system (OS) and OS version. A small FS-agnostic proxy, maintained by the core OS developers, connects the FSVA to whatever OS the user chooses. This article describes an FSVA design that maintains FS semantics for unmodified FS implementations and provides desired OS and virtualization features, such as a unified buffer cache and VM migration. Evaluation of prototype FSVA implementations in Linux and NetBSD, using Xen as the virtual machine manager (VMM), demonstrates that the FSVA architecture is efficient, FS-agnostic, and able to insulate file system implementations from OS differences that would otherwise require explicit porting.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2011181054",
    "type": "article"
  },
  {
    "title": "Towards High-Performance SAN with Fast Storage Devices",
    "doi": "https://doi.org/10.1145/2577385",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "Jae Woo Choi; Dong In Shin; Young Jin Yu; Hyeonsang Eom; Heon Y. Yeom",
    "corresponding_authors": "",
    "abstract": "Storage area network (SAN) is one of the most popular solutions for constructing server environments these days. In these kinds of server environments, HDD-based storage usually becomes the bottleneck of the overall system, but it is not enough to merely replace the devices with faster ones in order to exploit their high performance. In other words, proper optimizations are needed to fully utilize their performance gains. In this work, we first adopted a DRAM-based SSD as a fast backend-storage in the existing SAN environment, and found significant performance degradation compared to its own capabilities, especially in the case of small-sized random I/O pattern, even though a high-speed network was used. We have proposed three optimizations to solve this problem: (1) removing software overhead in the SAN I/O path; (2) increasing parallelism in the procedures for handling I/O requests; and (3) adopting the temporal merge mechanism to reduce network overheads. We have implemented them as a prototype and found that our approaches make substantial performance improvements by up to 39% and 280% in terms of both the latency and bandwidth, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2014246963",
    "type": "article"
  },
  {
    "title": "Emulating goliath storage systems with David",
    "doi": "https://doi.org/10.1145/2078861.2078862",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Nitin Agrawal; Leo Arulraj; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "Benchmarking file and storage systems on large file-system images is important, but difficult and often infeasible. Typically, running benchmarks on such large disk setups is a frequent source of frustration for file-system evaluators; the scale alone acts as a strong deterrent against using larger, albeit realistic, benchmarks. To address this problem, we develop David: a system that makes it practical to run large benchmarks using modest amount of storage or memory capacities readily available on most computers. David creates a “compressed” version of the original file-system image by omitting all file data and laying out metadata more efficiently; an online storage model determines the runtime of the benchmark workload on the original uncompressed image. David works under any file system, as demonstrated in this article with ext3 and btrfs. We find that David reduces storage requirements by orders of magnitude; David is able to emulate a 1-TB target workload using only an 80 GB available disk, while still modeling the actual runtime accurately. David can also emulate newer or faster devices, for example, we show how David can effectively emulate a multidisk RAID using a limited amount of memory.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2160301640",
    "type": "article"
  },
  {
    "title": "Nap: Persistent Memory Indexes for NUMA Architectures",
    "doi": "https://doi.org/10.1145/3507922",
    "publication_date": "2022-01-29",
    "publication_year": 2022,
    "authors": "Qing Wang; Youyou Lu; Junru Li; Minhui Xie; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "We present Nap , a black-box approach that converts concurrent persistent memory (PM) indexes into non-uniform memory access (NUMA)-aware counterparts. Based on the observation that real-world workloads always feature skewed access patterns, Nap introduces a NUMA-aware layer (NAL) on the top of existing concurrent PM indexes, and steers accesses to hot items to this layer. The NAL maintains (1) per-node partial views in PM for serving insert/update/delete operations with failure atomicity and (2) a global view in DRAM for serving lookup operations. The NAL eliminates remote PM accesses to hot items without inducing extra local PM accesses. Moreover, to handle dynamic workloads, Nap adopts a fast NAL switch mechanism. We convert five state-of-the-art PM indexes using Nap . Evaluation on a four-node machine with Optane DC Persistent Memory shows that Nap can improve the throughput by up to 2.3× and 1.56× under write-intensive and read-intensive workloads, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210517351",
    "type": "article"
  },
  {
    "title": "The Concurrent Learned Indexes for Multicore Data Storage",
    "doi": "https://doi.org/10.1145/3478289",
    "publication_date": "2022-01-29",
    "publication_year": 2022,
    "authors": "Zhaoguo Wang; Haibo Chen; Youyun Wang; Chuzhe Tang; Huan Wang",
    "corresponding_authors": "",
    "abstract": "We present XIndex, which is a concurrent index library and designed for fast queries. It includes a concurrent ordered index (XIndex-R) and a concurrent hash index (XIndex-H). Similar to a recent proposal of the learned index, the indexes in XIndex use learned models to optimize index efficiency. Compared with the learned index, for the ordered index, XIndex-R is able to handle concurrent writes effectively and adapts its structure according to runtime workload characteristics. For the hash index, XIndex-H is able to avoid the resize operation blocking concurrent writes. Furthermore, the indexes in XIndex can index string keys much more efficiently than the learned index. We demonstrate the advantages of XIndex with YCSB, TPC-C (KV), which is a TPC-C-inspired benchmark for key-value stores, and micro-benchmarks. Compared with ordered indexes of Masstree and Wormhole, XIndex-R achieves up to 3.2× and 4.4× performance improvement on a 24-core machine. Compared with hash indexes of Intel TBB HashMap, XIndex-H achieves up to 3.1× speedup. The performance further improves by 91% after adding the optimizations on indexing string keys. The library is open-sourced. 1",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210819313",
    "type": "article"
  },
  {
    "title": "HintStor: A Framework to Study I/O Hints in Heterogeneous Storage",
    "doi": "https://doi.org/10.1145/3489143",
    "publication_date": "2022-03-10",
    "publication_year": 2022,
    "authors": "Xiongzi Ge; Zhichao Cao; David Hung-Chang Du; Pradeep Ganesan; Dennis Hahn",
    "corresponding_authors": "",
    "abstract": "To bridge the giant semantic gap between applications and modern storage systems, passing a piece of tiny and useful information, called I/O access hints, from upper layers to the storage layer may greatly improve application performance and ease data management in storage systems. This is especially true for heterogeneous storage systems that consist of multiple types of storage devices. Since ingesting external access hints will likely involve laborious modifications of legacy I/O stacks, it is very hard to evaluate the effect and take advantages of access hints. In this article, we design a generic and flexible framework, called HintStor, to quickly play with a set of I/O access hints and evaluate their impacts on heterogeneous storage systems. HintStor provides a new application/user-level interface, a file system plugin, and performs data management with a generic block storage data manager. We demonstrate the flexibility of HintStor by evaluating four types of access hints: file system data classification, stream ID, cloud prefetch, and I/O task scheduling on a Linux platform. The results show that HintStor can execute and evaluate various I/O access hints under different scenarios with minor modifications to the kernel and applications.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4220893202",
    "type": "article"
  },
  {
    "title": "SmartFVM: A Fast, Flexible, and Scalable Hardware-based Virtualization for Commodity Storage Devices",
    "doi": "https://doi.org/10.1145/3511213",
    "publication_date": "2022-04-12",
    "publication_year": 2022,
    "authors": "Dongup Kwon; Wonsik Lee; Dongryeong Kim; Junehyuk Boo; Jangwoo Kim",
    "corresponding_authors": "",
    "abstract": "A computational storage device incorporating a computation unit inside or near its storage unit is a highly promising technology to maximize a storage server’s performance. However, to apply such computational storage devices and take their full potential in virtualized environments, server architects must resolve a fundamental challenge: cost-effective virtualization . This critical challenge can be directly addressed by the following questions: (1) how to virtualize two different hardware units (i.e., computation and storage), and (2) how to integrate them to construct virtual computational storage devices, and (3) how to provide them to users. However, the existing methods for computational storage virtualization severely suffer from their low performance and high costs due to the lack of hardware-assisted virtualization support. In this work, we propose SmartFVM-Engine , an FPGA card designed to maximize the performance and cost-effectiveness of computational storage virtualization. SmartFVM-Engine introduces three key ideas to achieve the design goals. First, it achieves high virtualization performance by applying hardware-assisted virtualization to both computation and storage units. Second, it further improves the performance by applying hardware-assisted resource orchestration for the virtualized units. Third, it achieves high cost-effectiveness by dynamically constructing and scheduling virtual computational storage devices. To the best of our knowledge, this is the first work to implement a hardware-assisted virtualization mechanism for modern computational storage devices.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4223552288",
    "type": "article"
  },
  {
    "title": "Dynamic Synchronous/Asynchronous Replication",
    "doi": "https://doi.org/10.1145/2501620.2508011",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Assaf Natanzon; Eitan Bachmat",
    "corresponding_authors": "",
    "abstract": "Online, remote, data replication is critical for today’s enterprise IT organization. Availability of data is key to the success of the organization. A few hours of downtime can cost from thousands to millions of dollars With increasing frequency, companies are instituting disaster recovery plans to ensure appropriate data availability in the event of a catastrophic failure or disaster that destroys a site (e.g. flood, fire, or earthquake).",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4232005309",
    "type": "article"
  },
  {
    "title": "Donag: Generating Efficient Patches and Diffs for Compressed Archives",
    "doi": "https://doi.org/10.1145/3507919",
    "publication_date": "2022-07-27",
    "publication_year": 2022,
    "authors": "Michael J. May",
    "corresponding_authors": "Michael J. May",
    "abstract": "Differencing between compressed archives is a common task in file management and synchronization. Applications include source code distribution, application updates, and document synchronization. General purpose binary differencing tools can create and apply patches to compressed archives, but don’t consider the internal structure of the compressed archive or the file lifecycle. Therefore, they miss opportunities to save space based on the archive’s internal structure and metadata. To address the gap, we develop a content-aware, format independent theory for differencing on compressed archives and propose a canonical form and digest for compressed archives. Based on them, we present Donag, a content-aware differencing and patching algorithm that produces smaller patches than general purpose binary differencing tools on versioned archives by exploiting the compressed archives’ internal structure. Donag uses the VCDiff and BSDiff engines internally. We compare Donag’s patches to ones produced by bsdiff, xdelta3, and Delta++ on three classes of compressed archives: open-source code repositories, large and small applications, and office productivity documents (DOCX, XLSX, PPTX). Donag’s patches are typically 10% to 89% smaller than those produced by bsdiff, xdelta3, and Delta++, with reasonable memory overhead and throughput on commodity hardware. In the worst case, Donag’s patches are negligibly larger.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4288050403",
    "type": "article"
  },
  {
    "title": "DEFUSE: An Interface for Fast and Correct User Space File System Access",
    "doi": "https://doi.org/10.1145/3494556",
    "publication_date": "2022-08-30",
    "publication_year": 2022,
    "authors": "James Lembke; Pierre-Louis Roman; Patrick Eugster",
    "corresponding_authors": "",
    "abstract": "Traditionally, the only option for developers was to implement file systems (FSs) via drivers within the operating system kernel. However, there exists a growing number of file systems (FSs), notably distributed FSs for the cloud, whose interfaces are implemented solely in user space to (i) isolate FS logic, (ii) take advantage of user space libraries, and/or (iii) for rapid FS prototyping. Common interfaces for implementing FSs in user space exist, but they do not guarantee POSIX compliance in all cases, or suffer from considerable performance penalties due to high amounts of wait context switchs between kernel and user space processes. We propose DEFUSE: an interface for user space FSs that provides fast accesses while ensuring access correctness and requiring no modifications to applications. DEFUSE: achieves significant performance improvements over existing user space FS interfaces thanks to its novel design that drastically reduces the number of wait context switchs for FS accesses. Additionally, to ensure access correctness, DEFUSE: maintains POSIX compliance for FS accesses thanks to three novel concepts of bypassed file descriptor (FD) lookup , FD stashing , and user space paging . Our evaluation spanning a variety of workloads shows that by reducing the number of wait context switchs per workload from as many as 16,000 or 41,000 with filesystem in user space down to 9 on average, DEFUSE: increases performance 2× over existing interfaces for typical workloads and by as many as 10× in certain instances.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4293544710",
    "type": "article"
  },
  {
    "title": "Toward Fast and Scalable Random Walks over Disk-Resident Graphs via Efficient I/O Management",
    "doi": "https://doi.org/10.1145/3533579",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Rui Wang; Yongkun Li; Yinlong Xu; Hong Xie; John C. S. Lui; Shuibing He",
    "corresponding_authors": "",
    "abstract": "Traditional graph systems mainly use the iteration-based model, which iteratively loads graph blocks into memory for analysis so as to reduce random I/Os. However, this iteration-based model limits the efficiency and scalability of running random walk, which is a fundamental technique to analyze large graphs. In this article, we first propose a state-aware I/O model to improve the I/O efficiency of running random walk, then we develop a block-centric indexing and buffering scheme for managing walk data, and leverage an asynchronous walk updating strategy to improve random walk efficiency. We implement an I/O-efficient graph system, GraphWalker , which is efficient to handle very large disk-resident graphs and also scalable to run tens of billions of random walks with only a single commodity machine. Experiments show that GraphWalker can achieve more than an order of magnitude speedup when compared with DrunkardMob, which is tailored for random walks based on the classical graph system GraphChi, as well as two state-of-the-art single-machine graph systems, Graphene and GraFSoft. Furthermore, when compared with the most recent distributed system KnightKing, GraphWalker still achieves comparable performance with only a single machine, thereby making it a more cost-effective alternative.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4297320256",
    "type": "article"
  },
  {
    "title": "<scp>Ares</scp> : Adaptive, Reconfigurable, Erasure coded, Atomic Storage",
    "doi": "https://doi.org/10.1145/3510613",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Nicolas Nicolaou; Viveck R. Cadambe; N. Prakash; Andria Trigeorgi; Kishori M. Konwar; Muriel Médard; Nancy Lynch",
    "corresponding_authors": "",
    "abstract": "Emulating a shared atomic , read/write storage system is a fundamental problem in distributed computing. Replicating atomic objects among a set of data hosts was the norm for traditional implementations (e.g., [ 11 ]) in order to guarantee the availability and accessibility of the data despite host failures. As replication is highly storage demanding, recent approaches suggested the use of erasure-codes to offer the same fault-tolerance while optimizing storage usage at the hosts. Initial works focused on a fixed set of data hosts. To guarantee longevity and scalability, a storage service should be able to dynamically mask hosts failures by allowing new hosts to join, and failed host to be removed without service interruptions. This work presents the first erasure-code -based atomic algorithm, called Ares , which allows the set of hosts to be modified in the course of an execution. Ares is composed of three main components: (i) a reconfiguration protocol , (ii) a read/write protocol , and (iii) a set of data access primitives (DAPs) . The design of Ares is modular and is such to accommodate the usage of various erasure-code parameters on a per-configuration basis. We provide bounds on the latency of read/write operations and analyze the storage and communication costs of the Ares algorithm.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4297378356",
    "type": "article"
  },
  {
    "title": "PSA-Cache: A Page-state-aware Cache Scheme for Boosting 3D NAND Flash Performance",
    "doi": "https://doi.org/10.1145/3574324",
    "publication_date": "2022-12-06",
    "publication_year": 2022,
    "authors": "Shujie Pang; Yuhui Deng; Genxiong Zhang; Yi Zhou; Yaoqin Huang; Xiao Qin",
    "corresponding_authors": "",
    "abstract": "Garbage collection (GC) plays a pivotal role in the performance of 3D NAND flash memory, where Copyback has been widely used to accelerate valid page migration during GC. Unfortunately, copyback is constrained by the parity symmetry issue: data read from an odd/even page must be written to an odd/even page. After migrating two odd/even consecutive pages, a free page between the two migrated pages will be wasted. Such wasted pages noticeably lower free space on flash memory and cause extra GCs, thereby degrading solid-state-disk (SSD) performance. To address this problem, we propose a page-state-aware cache scheme called PSA-Cache , which prevents page waste to boost the performance of NAND Flash-based SSDs. To facilitate making write-back scheduling decisions, PSA-Cache regulates write-back priorities for cached pages according to the state of pages in victim blocks. With high write-back-priority pages written back to flash chips, PSA-Cache effectively fends off page waste by breaking odd/even consecutive pages in subsequent garbage collections. We quantitatively evaluate the performance of PSA-Cache in terms of the number of wasted pages, the number of GCs, and response time. We compare PSA-Cache with two state-of-the-art schemes, GCaR and TTflash, in addition to a baseline scheme LRU. The experimental results unveil that PSA-Cache outperforms the existing schemes. In particular, PSA-Cache curtails the number of wasted pages of GCaR and TTflash by 25.7% and 62.1%, respectively. PSA-Cache immensely cuts back the number of GC counts by up to 78.7% with an average of 49.6%. Furthermore, PSA-Cache slashes the average write response time by up to 85.4% with an average of 30.05%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4311644768",
    "type": "article"
  },
  {
    "title": "Exploiting Data-pattern-aware Vertical Partitioning to Achieve Fast and Low-cost Cloud Log Storage",
    "doi": "https://doi.org/10.1145/3643641",
    "publication_date": "2024-01-29",
    "publication_year": 2024,
    "authors": "Junyu Wei; Guangyan Zhang; Junchao Chen; Yang Wang; Weimin Zheng; Tingtao Sun; Jiesheng Wu; Jiangwei Jiang",
    "corresponding_authors": "",
    "abstract": "Cloud logs can be categorized into on-line, off-line, and near-line logs based on the access frequency. Among them, near-line logs are mainly used for debugging, which means they prefer a low query latency for better user experience. Besides, the storage system for near-line logs prefers a low overall cost including the storage cost to store compressed logs, and the computation cost to compress logs and execute queries. These requirements pose challenges to achieving fast and cheap cloud log storage. This article proposes LogGrep, the first log compression and query tool that exploits both static and runtime patterns to properly structurize and organize log data in fine-grained units. The key idea of LogGrep is “vertical partitioning”: it stores each log entry into multiple partitions by first parsing logs into variable vectors according to static patterns and then extracting runtime pattern(s) automatically within each variable vector. Based on such runtime patterns, LogGrep further decomposes the variable vectors into fine-grained units called “Capsules” and stamps each Capsule with a summary of its values. During the query process, LogGrep can avoid decompressing and scanning Capsules that cannot match the keywords, with the help of the extracted runtime patterns and the Capsule stamps. We further show that the interactive debugging can well utilize the advantages of the vertical-partitioning-based method and mitigate its weaknesses as well. To this end, LogGrep integrates incremental locating and partial reconstruction to mitigate the read amplification incurred by vertical-partitioning-based method. We evaluate LogGrep on 37 cloud logs from the production environment of Alibaba Cloud and the public datasets. The results show that LogGrep can reduce the query latency and the overall cost by an order of magnitude compared with state-of-the-art works. Such results have confirmed that it is worthwhile applying a more sophisticated vertical-partitioning-based method to accelerate queries on compressed cloud logs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391323409",
    "type": "article"
  },
  {
    "title": "Tarazu: An Adaptive End-to-end I/O Load-balancing Framework for Large-scale Parallel File Systems",
    "doi": "https://doi.org/10.1145/3641885",
    "publication_date": "2024-02-01",
    "publication_year": 2024,
    "authors": "Arnab K. Paul; Sarah Neuwirth; Bharti Wadhwa; Feiyi Wang; Sarp Oral; Ali R. Butt",
    "corresponding_authors": "",
    "abstract": "The imbalanced I/O load on large parallel file systems affects the parallel I/O performance of high-performance computing (HPC) applications. One of the main reasons for I/O imbalances is the lack of a global view of system-wide resource consumption. While approaches to address the problem already exist, the diversity of HPC workloads combined with different file striping patterns prevents widespread adoption of these approaches. In addition, load-balancing techniques should be transparent to client applications. To address these issues, we propose Tarazu , an end-to-end control plane where clients transparently and adaptively write to a set of selected I/O servers to achieve balanced data placement. Our control plane leverages real-time load statistics for global data placement on distributed storage servers, while our design model employs trace-based optimization techniques to minimize latency for I/O load requests between clients and servers and to handle multiple striping patterns in files. We evaluate our proposed system on an experimental cluster for two common use cases: the synthetic I/O benchmark IOR and the scientific application I/O kernel HACC-I/O. We also use a discrete-time simulator with real HPC application traces from emerging workloads running on the Summit supercomputer to validate the effectiveness and scalability of Tarazu in large-scale storage environments. The results show improvements in load balancing and read performance of up to 33% and 43%, respectively, compared to the state-of-the-art.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391449109",
    "type": "article"
  },
  {
    "title": "Design and Implementation of Deduplication on F2FS",
    "doi": "https://doi.org/10.1145/3662735",
    "publication_date": "2024-04-29",
    "publication_year": 2024,
    "authors": "Tiangmeng Zhang; Rufeng Chen; Zijing Li; Congming Gao; Chengke Wang; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Data deduplication technology has gained popularity in modern file systems due to its ability to eliminate redundant writes and improve storage space efficiency. In recent years, the flash-friendly file system (F2FS) has been widely adopted in flash memory-based storage devices, including smartphones, fast-speed servers, and Internet of Things. In this article, we propose F2DFS (deduplication-based F2FS), which introduces three main design contributions. First, F2DFS integrates inline and offline hybrid deduplication. Inline deduplication eliminates redundant writes and enhances flash device endurance, while offline deduplication mitigates the negative I/O performance impact and saves more storage space. Second, F2DFS follows the file system coupling design principle, effectively leveraging the potentials and benefits of both deduplication and native F2FS. Also, with the aid of this principle, F2DFS achieves high-performance and space-efficient incremental deduplication. Third, F2DFS adopts virtual indexing to mitigate deduplication-induced many-to-one mapping updates during the segment cleaning. We conducted comprehensive experimental comparisons between F2DFS, native F2FS, and other state-of-the-art deduplication schemes, using both synthetic and real-world workloads. For inline deduplication, F2DFS outperforms SmartDedup, Dmdedup, and ZFS, in terms of both I/O bandwidth performance and deduplication rates. And for offline deduplication, compared to SmartDedup, XFS, and BtrFS, F2DFS shows higher execution efficiency, lower resource usage, and greater storage space savings. Moreover, F2DFS demonstrates more efficient segment cleanings than native F2FS.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396237713",
    "type": "article"
  },
  {
    "title": "FSDedup: Feature-Aware and Selective Deduplication for Improving Performance of Encrypted Non-Volatile Main Memory",
    "doi": "https://doi.org/10.1145/3662736",
    "publication_date": "2024-05-01",
    "publication_year": 2024,
    "authors": "Chunfeng Du; Zihang Lin; Suzhen Wu; Yifei Chen; Jiapeng Wu; Wang Shengzhe; Weichun Wang; Qingfeng Wu; Bo Mao",
    "corresponding_authors": "",
    "abstract": "Enhancing the endurance, performance, and energy efficiency of encrypted Non-Volatile Main Memory (NVMM) can be achieved by minimizing written data through inline deduplication. However, existing approaches applying inline deduplication to encrypted NVMM suffer from substantial performance degradation due to high computing, memory footprint, and index-lookup overhead to generate, store, and query the cryptographic hash (fingerprint). In the preliminary ESD [ 14 ], we proposed the Error Correcting Code (ECC) assisted selective deduplication scheme, utilizing the ECC information as a fingerprint to identify similar data effectively and then leveraging the selective deduplication technique to eliminate a large amount of redundant data with high reference counts. In this article, we proposed FSDedup. Compared with ESD, FSDedup could leverage the prefetch cache to reduce the read overhead during similarity comparison and utilize the cache refresh mechanism to identify further and eliminate more redundant data. Extensive experimental evaluations demonstrate that FSDedup can enhance the performance of the NVMM system further than the ESD. Experimental results show that FSDedup can improve both write and read speed by up to 1.8×, enhance Instructions Per Cycle by up to 1.5×, and reduce energy consumption by up to 2.0×, compared to ESD.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396560796",
    "type": "article"
  },
  {
    "title": "Fastmove: A Comprehensive Study of On-Chip DMA and its Demonstration for Accelerating Data Movement in NVM-based Storage Systems",
    "doi": "https://doi.org/10.1145/3656477",
    "publication_date": "2024-05-06",
    "publication_year": 2024,
    "authors": "Jiahao Li; J. Su; L J Chen; Cheng Li; Kai Zhang; Liang Yang; Sam H. Noh; Yinlong Xu",
    "corresponding_authors": "",
    "abstract": "Data-intensive applications executing on NVM-based storage systems experience serious bottlenecks when moving data between DRAM and NVM. We advocate for the use of the long-existing but recently neglected on-chip DMA to expedite data movement with three contributions. First, we explore new latency-oriented optimization directions, driven by a comprehensive DMA study, to design a high-performance DMA module, which significantly lowers the I/O size threshold to observe benefits. Second, we propose a new data movement engine, Fastmove , that coordinates the use of the DMA along with the CPU with DDIO-aware strategies, judicious scheduling, and load splitting such that the DMA’s limitations are compensated, and the overall gains are maximized. Finally, with a general kernel-based design, simple APIs, and DAX file system integration, Fastmove allows applications to transparently exploit the DMA and its new features without code change. We run three data-intensive applications MySQL, GraphWalker, and Filebench atop NOVA , ext4-DAX , and XFS-DAX , with standard benchmarks like TPC-C, and popular graph algorithms like PageRank. Across single- and multi-socket settings, compared to the conventional CPU-only NVM accesses, Fastmove introduces to TPC-C with MySQL 1.13–2.16× speedups of peak throughput, reduces the average latency by 17.7–60.8%, and saves 37.1–68.9% CPU usage spent in data movement. It also shortens the execution time of graph algorithms with GraphWalker by 39.7–53.4%, and introduces 1.01–1.48× throughput speedups for Filebench.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396671130",
    "type": "article"
  },
  {
    "title": "LVMT: An Efficient Authenticated Storage for Blockchain",
    "doi": "https://doi.org/10.1145/3664818",
    "publication_date": "2024-05-16",
    "publication_year": 2024,
    "authors": "C. Li; Sidi Mohamed Beillahi; Guang Yang; Ming Wu; Wei Xu; Fan Long",
    "corresponding_authors": "",
    "abstract": "Authenticated storage access is the performance bottleneck of a blockchain, because each access can be amplified to potentially O (log n ) disk I/O operations in the standard Merkle Patricia Trie (MPT) storage structure. In this article, we propose a multi-Layer Versioned Multipoint Trie (LVMT), a novel high-performance blockchain storage with significantly reduced I/O amplifications. LVMT uses the authenticated multipoint evaluation tree vector commitment protocol to update commitment proofs in constant time. LVMT adopts a multi-layer design to support unlimited key–value pairs and stores version numbers instead of value hashes to avoid costly elliptic curve multiplication operations. In our experiment, LVMT outperforms the MPT in real Ethereum traces, delivering read and write operations 6× faster. It also boosts blockchain system execution throughput by up to 2.7×.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396956843",
    "type": "article"
  },
  {
    "title": "Flash-oriented Coded Storage: Research Status and Future Directions",
    "doi": "https://doi.org/10.1145/3708995",
    "publication_date": "2024-12-19",
    "publication_year": 2024,
    "authors": "Zhiyue Li; Guangyan Zhang; Yang Wang",
    "corresponding_authors": "",
    "abstract": "Flash-based solid-state drives (SSDs) have been widely adopted in various storage systems, manifesting better performance than their forerunner HDDs. However, the characteristics of flash media post some drawbacks when deploying SSD-based storage systems. First, flash media have limited program/erase cycles, making them vulnerable to media failures. Second, SSD foreground I/Os can suffer from inconsistent performance due to interference from background operations like garbage collection (GC). The major solution to the above problems is to introduce data redundancy. Redundant data can not only detect raw bit errors and recover lost data but also enable I/O scheduling to sidestep SSDs that are under performance degradation. Compared with multi-replica, data coding is a more space-efficient way to provide redundancy. However, it is more challenging to simultaneously achieve low access latency, consistent performance and fast recovery. This paper examines the design of coded storage in existing storage systems, with a focus on flash storage systems, and how they address these challenges. The coded storage techniques are categorized into in-device coding, cross-device coding, and cross-machine coding. They are designed for different scenarios and purposes, but share some design rationales in common. For each type of coded storage, we begin by presenting the theoretical bases, followed by an overview of how existing studies address the performance and endurance issues of coded storage from a systemic perspective. Finally, we review the history of coded storage, list several key insights from existing works, and speculate some promising directions for flash-oriented coded storage systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405580134",
    "type": "article"
  },
  {
    "title": "Multicollective I/O",
    "doi": "https://doi.org/10.1145/1168910.1168915",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Gokhan Memik; Mahmut Kandemir; Wei‐keng Liao; Alok Choudhary",
    "corresponding_authors": "",
    "abstract": "The increasing gap between processor cycle times and access times to storage devices makes it necessary to use powerful optimizations. This is especially true for applications in the parallel computing domain that frequently perform large amounts of file I/O. Collective I/O strategy that coordinates the processes to perform I/O on each other's behalf has demonstrated a significant performance improvement. This article proposes a new concept called Multicollective I/O (MCIO) that expands the collective I/O to allow data from multiple files to be requested in a single I/O request, in contrast to allowing only multiple segments for a single file to be specified together. MCIO considers multiple arrays simultaneously by having a more global view of the overall I/O behavior exhibited by parallel applications. This article shows that determining the optimal MCIO access pattern is an NP-complete problem, and proposes two different heuristics for the access pattern detection problem, also called the assignment problem. Both heuristics have been implemented within a runtime library, and tested using a large-scale scientific application. Our results show that MCIO outperforms collective I/O by as much as 87%. Our runtime library-based implementation can be used by application users as well as by optimizing compilers. Based on our results, we recommend that future library designers for I/O-intensive applications include MCIO in their suite of optimizations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2070987480",
    "type": "article"
  },
  {
    "title": "Reliability of SSDs in Enterprise Storage Systems",
    "doi": "https://doi.org/10.1145/3423088",
    "publication_date": "2021-01-13",
    "publication_year": 2021,
    "authors": "Stathis Maneas; Kaveh Mahdaviani; Tim Emami; Bianca Schroeder",
    "corresponding_authors": "",
    "abstract": "This article presents the first large-scale field study of NAND-based SSDs in enterprise storage systems (in contrast to drives in distributed data center storage systems). The study is based on a very comprehensive set of field data, covering 1.6 million SSDs of a major storage vendor (NetApp). The drives comprise three different manufacturers, 18 different models, 12 different capacities, and all major flash technologies (SLC, cMLC, eMLC, 3D-TLC). The data allows us to study a large number of factors that were not studied in prior works, including the effect of firmware versions, the reliability of TLC NAND, and the correlations between drives within a RAID system. This article presents our analysis, along with a number of practical implications derived from it.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3118571333",
    "type": "article"
  },
  {
    "title": "GoSeed: Optimal Seeding Plan for Deduplicated Storage",
    "doi": "https://doi.org/10.1145/3453301",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Aviv Nachman; Sarai Sheinvald; Ariel Kolikant; Gala Yadgar",
    "corresponding_authors": "",
    "abstract": "Deduplication decreases the physical occupancy of files in a storage volume by removing duplicate copies of data chunks, but creates data-sharing dependencies that complicate standard storage management tasks. Specifically, data migration plans must consider the dependencies between files that are remapped to new volumes and files that are not. Thus far, only greedy approaches have been suggested for constructing such plans, and it is unclear how they compare to one another and how much they can be improved. We set to bridge this gap for seeding —migration in which the target volume is initially empty. We prove that even this basic instance of data migration is NP-hard in the presence of deduplication. We then present GoSeed, a formulation of seeding as an integer linear programming (ILP) problem, and three acceleration methods for applying it to real-sized storage volumes. Our experimental evaluation shows that, while the greedy approaches perform well on “easy” problem instances, the cost of their solution can be significantly higher than that of GoSeed’s solution on “hard” instances, for which they are sometimes unable to find a solution at all.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3194658889",
    "type": "article"
  },
  {
    "title": "Efficient Free Space Reclamation in WAFL",
    "doi": "https://doi.org/10.1145/3125647",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Ram Kesavan; Rohit Singh; Travis Grusecki; Yuvraj Patel",
    "corresponding_authors": "",
    "abstract": "NetApp ® WAFL ® is a transactional file system that uses the copy-on-write mechanism to support fast write performance and efficient snapshot creation. However, copy-on-write increases the demand on the file system to find free blocks quickly, which makes rapid free space reclamation essential. Inability to find free blocks quickly may impede allocations for incoming writes. Efficiency is also important, because the task of reclaiming free space may consume CPU and other resources at the expense of client operations. In this article, we describe the evolution (over more than a decade) of the WAFL algorithms and data structures for reclaiming space with minimal impact to the overall performance of the storage appliance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2756734609",
    "type": "article"
  },
  {
    "title": "CacheSack: Theory and Experience of Google’s Admission Optimization for Datacenter Flash Caches",
    "doi": "https://doi.org/10.1145/3582014",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Tzu-Wei Yang; Seth Pollen; Mustafa Uysal; Arif Merchant; H. Wolfmeister; Junaid Khalid",
    "corresponding_authors": "",
    "abstract": "This article describes the algorithm, implementation, and deployment experience of CacheSack, the admission algorithm for Google datacenter flash caches. CacheSack minimizes the dominant costs of Google’s datacenter flash caches: disk IO and flash footprint. CacheSack partitions cache traffic into disjoint categories, analyzes the observed cache benefit of each subset, and formulates a knapsack problem to assign the optimal admission policy to each subset. Prior to this work, Google datacenter flash cache admission policies were optimized manually, with most caches using the Lazy Adaptive Replacement Cache algorithm. Production experiments showed that CacheSack significantly outperforms the prior static admission policies for a 7.7% improvement of the total cost of ownership, as well as significant improvements in disk reads (9.5% reduction) and flash wearout (17.8% reduction).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4317659274",
    "type": "article"
  },
  {
    "title": "Hybrid Block Storage for Efficient Cloud Volume Service",
    "doi": "https://doi.org/10.1145/3596446",
    "publication_date": "2023-05-08",
    "publication_year": 2023,
    "authors": "Yiming Zhang; Huiba Li; Shengyun Liu; Peng Huang",
    "corresponding_authors": "",
    "abstract": "The migration of traditional desktop and server applications to the cloud brings challenge of high performance, high reliability, and low cost to the underlying cloud storage. To satisfy the requirement, this article proposes a hybrid cloud-scale block storage system called Ursa . Trace analysis shows that the I/O patterns served by block storage have only limited locality to exploit. Therefore, instead of using solid state drives (SSDs) as a cache layer, Ursa proposes hybrid storage structure that directly stores primary replicas on SSDs and replicates backup replicas on hard disk drives (HDDs) . At the core of Ursa ’s hybrid storage design is an adaptive journal that can bridge the performance gap between primary SSDs and backup HDDs for random writes by transforming small backup writes into journal appends, which are then asynchronously replayed and merged to backup HDDs. To efficiently index the journal, we design a novel range-optimized merge-tree structure that combines a continuous range of keys into a single composite key {offset,length} . Ursa integrates the hybrid structure with designs for high reliability, scalability, and availability. Experiments show that Ursa in its hybrid mode achieves almost the same performance as in its SSD-only mode (storing all replicas on SSDs), and outperforms other block stores (Ceph and Sheepdog) even in their SSD-only mode while achieving much higher CPU efficiency (IOPS and throughput per core).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4375861735",
    "type": "article"
  },
  {
    "title": "A Scalable Wear Leveling Technique for Phase Change Memory",
    "doi": "https://doi.org/10.1145/3631146",
    "publication_date": "2023-10-30",
    "publication_year": 2023,
    "authors": "Wang Xu; Israel Koren",
    "corresponding_authors": "",
    "abstract": "Phase Change Memory (PCM), one of the recently proposed non-volatile memory technologies, has been suffering from low write endurance. For example, a single-layer PCM cell could only be written approximately 10 8 . This limits the lifetime of a PCM-based memory to a few days rather than years when memory-intensive applications are running. Wear leveling techniques have been proposed to improve the write endurance of a PCM. Among those techniques, the region-based start-gap (RBSG) scheme is widely cited as achieving the highest lifetime. Based on our experiments, RBSG can achieve 97% of the ideal lifetime, but only for relatively small memory sizes (e.g., 8–32GB). As the memory size goes up, RBSG becomes less effective and its expected percentage of the ideal lifetime reduces to less than 57% for a 2TB PCM. In this article, we propose a table-based wear leveling scheme called block grouping to enhance the write endurance of a PCM with a negligible overhead. Our research results show that with a proper configuration and adoption of partial writes (writing back only 64B subblocks instead of a whole row to the PCM arrays) and internal row shift (shifting the subblocks in a row periodically so no subblock in a row will be written repeatedly), the proposed block grouping scheme could achieve 95% of the ideal lifetime on average for the Rodinia, NPB, and SPEC benchmarks with less than 1.74% performance overhead and up to 0.18% hardware overhead. Moreover, our scheme is scalable and achieves the same percentage of ideal lifetime for PCM of size from 8GB to 2TB. We also show that the proposed scheme can better tolerate memory write attacks than WoLFRaM (Wear Leveling and Fault Tolerance for Resistive Memories) and RBSG for a PCM of size 32GB or higher. Finally, we integrate an error-correcting pointer technique into our proposed block grouping scheme to make the PCM fault tolerant against hard errors.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388094051",
    "type": "article"
  },
  {
    "title": "M-CLOCK",
    "doi": "https://doi.org/10.1145/3216730",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Minho Lee; Dong Hyun Kang; Young Ik Eom",
    "corresponding_authors": "",
    "abstract": "Phase Change Memory (PCM) has drawn great attention as a main memory due to its attractive characteristics such as non-volatility, byte-addressability, and in-place update. However, since the capacity of PCM is not fully mature yet, hybrid memory architecture that consists of DRAM and PCM has been suggested as a main memory. In addition, page replacement algorithm based on hybrid memory architecture is actively being studied, because existing page replacement algorithms cannot be used on hybrid memory architecture in that they do not consider the two weaknesses of PCM: high write latency and low endurance. In this article, to mitigate the above hardware limitations of PCM, we revisit the page cache layer for the hybrid memory architecture and propose a novel page replacement algorithm, called M-CLOCK, to improve the performance of hybrid memory architecture and the lifespan of PCM. In particular, M-CLOCK aims to reduce the number of PCM writes that negatively affect the performance of hybrid memory architecture . Experimental results clearly show that M-CLOCK outperforms the state-of-the-art page replacement algorithms in terms of the number of PCM writes and effective memory access time by up to 98% and 9.4 times, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2895267029",
    "type": "article"
  },
  {
    "title": "On the Lifecycle of the File",
    "doi": "https://doi.org/10.1145/3295463",
    "publication_date": "2019-02-18",
    "publication_year": 2019,
    "authors": "Michael J. May; Etamar Laron; Khalid Zoabi; Havah Gerhardt",
    "corresponding_authors": "",
    "abstract": "Users and Operating Systems (OSs) have vastly different views of files. OSs use files to persist data and structured information. To accomplish this, OSs treat files as named collections of bytes managed in hierarchical file systems. Despite their critical role in computing, little attention is paid to the lifecycle of the file, the evolution of file contents, or the evolution of file metadata. In contrast, users have rich mental models of files: they group files into projects, send data repositories to others, work on documents over time, and stash them aside for future use. Current OSs and Revision Control Systems ignore such mental models, persisting a selective, manually designated history of revisions. Preserving the mental model allows applications to better match how users view their files, making file processing and archiving tools more effective. We propose two mechanisms that OSs can adopt to better preserve the mental model: File Lifecycle Events (FLEs) that record a file’s progression and Complex File Events (CFEs) that combine them into meaningful patterns. We present the Complex File Events Engine (CoFEE), which uses file system monitoring and an extensible rulebase (Drools) to detect FLEs and convert them into complex ones. CFEs are persisted in NoSQL stores for later querying.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2916357048",
    "type": "article"
  },
  {
    "title": "An Exploratory Study on Software-Defined Data Center Hard Disk Drives",
    "doi": "https://doi.org/10.1145/3319405",
    "publication_date": "2019-05-21",
    "publication_year": 2019,
    "authors": "Li Yin; Xubin Chen; Ning Zheng; Jingpeng Hao; Tong Zhang",
    "corresponding_authors": "",
    "abstract": "This article presents a design framework aiming to reduce mass data storage cost in data centers. Its underlying principle is simple: Assume one may noticeably reduce the HDD manufacturing cost by significantly (i.e., at least several orders of magnitude) relaxing raw HDD reliability, which ensures the eventual data storage integrity via low-cost system-level redundancy. This is called system-assisted HDD bit cost reduction. To better utilize both capacity and random IOPS of HDDs, it is desirable to mix data with complementary requirements on capacity and random IOPS in each HDD. Nevertheless, different capacity and random IOPS requirements may demand different raw HDD reliability vs. bit cost trade-offs and hence different forms of system-assisted bit cost reduction. This article presents a software-centric design framework to realize data-adaptive system-assisted bit cost reduction for data center HDDs. Implementation is solely handled by the filesystem and demands only minor change of the error correction coding (ECC) module inside HDDs. Hence, it is completely transparent to all the other components in the software stack (e.g., applications, OS kernel, and drivers) and keeps fundamental HDD design practice (e.g., firmware, media, head, and servo) intact. We carried out analysis and experiments to evaluate its implementation feasibility and effectiveness. We integrated the design techniques into ext4 to further quantitatively measure its impact on system speed performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2946747365",
    "type": "article"
  },
  {
    "title": "The Composite-File File System",
    "doi": "https://doi.org/10.1145/3366684",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Shuanglong Zhang; Robert Le Roy; Leah E. Rumancik; An-I Andy Wang",
    "corresponding_authors": "",
    "abstract": "The design and implementation of traditional file systems typically use the one-to-one mapping of logical files to their physical metadata representations. File system optimizations generally follow this rigid mapping and miss opportunities for an entire class of optimizations. We designed, implemented, and evaluated a composite-file file system, which allows many-to-one mappings of files to metadata. Through exploring different mapping strategies, our empirical evaluation shows up to a 27% performance improvement under web server and software development workloads, for both disks and SSDs. This result demonstrates that our approach of relaxing file-to-metadata mapping is promising.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3033055137",
    "type": "article"
  },
  {
    "title": "Batch-file Operations to Optimize Massive Files Accessing",
    "doi": "https://doi.org/10.1145/3394286",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Yang Yang; Qiang Cao; Jie Yao; Hong Jiang; Yang Li",
    "corresponding_authors": "",
    "abstract": "Existing local file systems, designed to support a typical single-file access mode only, can lead to poor performance when accessing a batch of files, especially small files. This single-file mode essentially serializes accesses to batched files one by one, resulting in a large number of non-sequential, random, and often dependent I/Os between file data and metadata at the storage ends. Such access mode can further worsen the efficiency and performance of applications accessing massive files, such as data migration. We first experimentally analyze the root cause of such inefficiency in batch-file accesses. Then, we propose a novel batch-file access approach, referred to as BFO for its set of optimized Batch-File Operations , by developing novel BFOr and BFOw operations for fundamental read and write processes, respectively, using a two-phase access for metadata and data jointly. The BFO offers dedicated interfaces for batch-file accesses and additional processes integrated into existing file systems without modifying their structures and procedures. In addition, based on BFOr and BFOw, we also propose the novel batch-file migration BFOm to accelerate the data migration for massive small files. We implement a BFO prototype on ext4, one of the most popular file systems. Our evaluation results show that the batch-file read and write performances of BFO are consistently higher than those of the traditional approaches regardless of access patterns, data layouts, and storage media, under synthetic and real-world file sets. BFO improves the read performance by up to 22.4× and 1.8× with HDD and SSD, respectively, and it boosts the write performance by up to 111.4× and 2.9× with HDD and SSD, respectively. BFO also demonstrates consistent performance advantages for data migration in both local and remote situations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3041842376",
    "type": "article"
  },
  {
    "title": "Cache What You Need to Cache",
    "doi": "https://doi.org/10.1145/3397766",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Hua Wang; Jiawei Zhang; Ping Huang; Xinbo Yi; Bin Cheng; Ke Zhou",
    "corresponding_authors": "",
    "abstract": "The SSD has been playing a significantly important role in caching systems due to its high performance-to-cost ratio. Since the cache space is typically much smaller than that of the backend storage by one order of magnitude or even more, write density (defined as writes per unit time and space) of the SSD cache is therefore much more intensive than that of HDD storage, which brings about tremendous challenges to the SSD’s lifetime. Meanwhile, under social network workloads, quite a lot writes to the SSD cache are unnecessary. For example, our study on Tencent’s photo caching shows that about 61% of total photos are accessed only once, whereas they are still swapped in and out of the cache. Therefore, if we can predict these kinds of photos proactively and prevent them from entering the cache, we can eliminate unnecessary SSD cache writes and improve cache space utilization. To cope with the challenge, we put forward a “one-time-access criteria” that is applied to the cache space and further propose a “one-time-access-exclusion” policy. Based on these two techniques, we design a prediction-based classifier to facilitate the policy. Unlike the state-of-the-art history-based predictions, our prediction is non-history oriented, which is challenging to achieve good prediction accuracy. To address this issue, we integrate a decision tree into the classifier, extract social-related information as classifying features, and apply cost-sensitive learning to improve classification precision. Due to these techniques, we attain a prediction accuracy greater than 80%. Experimental results show that the one-time-access-exclusion approach results in outstanding cache performance in most aspects. Take LRU, for instance: applying our approach improves the hit rate by 4.4%, decreases the cache writes by 56.8%, and cuts the average access latency by 5.5%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3046282621",
    "type": "article"
  },
  {
    "title": "Streaming Data Reorganization at Scale with DeltaFS Indexed Massive Directories",
    "doi": "https://doi.org/10.1145/3415581",
    "publication_date": "2020-09-24",
    "publication_year": 2020,
    "authors": "Qing Zheng; Charles D. Cranor; Ankush Jain; Gregory R. Ganger; Garth A. Gibson; George Amvrosiadis; Bradley W. Settlemyer; Gary Grider",
    "corresponding_authors": "",
    "abstract": "Complex storage stacks providing data compression, indexing, and analytics help leverage the massive amounts of data generated today to derive insights. It is challenging to perform this computation, however, while fully utilizing the underlying storage media. This is because, while storage servers with large core counts are widely available, single-core performance and memory bandwidth per core grow slower than the core count per die. Computational storage offers a promising solution to this problem by utilizing dedicated compute resources along the storage processing path. We present DeltaFS Indexed Massive Directories (IMDs), a new approach to computational storage. DeltaFS IMDs harvest available (i.e., not dedicated) compute, memory, and network resources on the compute nodes of an application to perform computation on data. We demonstrate the efficiency of DeltaFS IMDs by using them to dynamically reorganize the output of a real-world simulation application across 131,072 CPU cores. DeltaFS IMDs speed up reads by 1,740× while only slightly slowing down the writing of data during simulation I/O for in situ data processing.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3088172273",
    "type": "article"
  },
  {
    "title": "Bridging Storage Semantics Using Data Labels and Asynchronous I/O",
    "doi": "https://doi.org/10.1145/3415579",
    "publication_date": "2020-10-14",
    "publication_year": 2020,
    "authors": "Anthony Kougkas; Hariharan Devarajan; Xian‐He Sun",
    "corresponding_authors": "",
    "abstract": "In the era of data-intensive computing, large-scale applications, in both scientific and the BigData communities, demonstrate unique I/O requirements leading to a proliferation of different storage devices and software stacks, many of which have conflicting requirements. Further, new hardware technologies and system designs create a hierarchical composition that may be ideal for computational storage operations. In this article, we investigate how to support a wide variety of conflicting I/O workloads under a single storage system. We introduce the idea of a Label , a new data representation, and, we present LABIOS: a new, distributed, Label- based I/O system. LABIOS boosts I/O performance by up to 17× via asynchronous I/O, supports heterogeneous storage resources, offers storage elasticity, and promotes in situ analytics and software defined storage support via data provisioning. LABIOS demonstrates the effectiveness of storage bridging to support the convergence of HPC and BigData workloads on a single platform.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3093360420",
    "type": "article"
  },
  {
    "title": "Design and Evaluation of a New Approach to RAID-0 Scaling",
    "doi": "https://doi.org/10.1145/2491054",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "Guangyan Zhang; Weimin Zheng; Keqin Li",
    "corresponding_authors": "",
    "abstract": "Scaling up a RAID-0 volume with added disks can increase its storage capacity and I/O bandwidth simultaneously. For preserving a round-robin data distribution, existing scaling approaches require all the data to be migrated. Such large data migration results in a long redistribution time as well as a negative impact on application performance. In this article, we present a new approach to RAID-0 scaling called FastScale. First, FastScale minimizes data migration, while maintaining a uniform data distribution. It moves only enough data blocks from old disks to fill an appropriate fraction of new disks. Second, FastScale optimizes data migration with access aggregation and lazy checkpoint. Access aggregation enables data migration to have a larger throughput due to a decrement of disk seeks. Lazy checkpoint minimizes the number of metadata writes without compromising data consistency. Using several real system disk traces, we evaluate the performance of FastScale through comparison with SLAS, one of the most efficient existing scaling approaches. The experiments show that FastScale can reduce redistribution time by up to 86.06% with smaller application I/O latencies. The experiments also illustrate that the performance of RAID-0 scaled using FastScale is almost identical to, or even better than, that of the round-robin RAID-0.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2035308424",
    "type": "article"
  },
  {
    "title": "Making the common case the only case with anticipatory memory allocation",
    "doi": "https://doi.org/10.1145/2078861.2078863",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Sundararaman Swaminathan; Yupu Zhang; Sriram Subramanian; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We present anticipatory memory allocation (AMA), a new method to build kernel code that is robust to memory-allocation failures. AMA avoids the usual difficulties in handling allocation failures through a novel combination of static and dynamic techniques. Specifically, a developer, with assistance from AMA static analysis tools, determines how much memory a particular call into a kernel subsystem will need, and then preallocates said amount immediately upon entry to the kernel; subsequent allocation requests are serviced from the preallocated pool and thus guaranteed never to fail. We describe the static and runtime components of AMA, and then present a thorough evaluation of Linux ext2-mfr, a case study in which we transform the Linux ext2 file system into a memory-failure robust version of itself. Experiments reveal that ext2-mfr avoids memory-allocation failures successfully while incurring little space or time overhead.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2130875695",
    "type": "article"
  },
  {
    "title": "Reparo: A Fast RAID Recovery Scheme for Ultra-large SSDs",
    "doi": "https://doi.org/10.1145/3450977",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Duwon Hong; Keonsoo Ha; Min-Seok Ko; Myoungjun Chun; Yoona Kim; Sungjin Lee; Jihong Kim",
    "corresponding_authors": "",
    "abstract": "A recent ultra-large SSD (e.g., a 32-TB SSD) provides many benefits in building cost-efficient enterprise storage systems. Owing to its large capacity, however, when such SSDs fail in a RAID storage system, a long rebuild overhead is inevitable for RAID reconstruction that requires a huge amount of data copies among SSDs. Motivated by modern SSD failure characteristics, we propose a new recovery scheme, called reparo , for a RAID storage system with ultra-large SSDs. Unlike existing RAID recovery schemes, reparo repairs a failed SSD at the NAND die granularity without replacing it with a new SSD, thus avoiding most of the inter-SSD data copies during a RAID recovery step. When a NAND die of an SSD fails, reparo exploits a multi-core processor of the SSD controller in identifying failed LBAs from the failed NAND die and recovering data from the failed LBAs. Furthermore, reparo ensures no negative post-recovery impact on the performance and lifetime of the repaired SSD. Experimental results using 32-TB enterprise SSDs show that reparo can recover from a NAND die failure about 57 times faster than the existing rebuild method while little degradation on the SSD performance and lifetime is observed after recovery.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3195023525",
    "type": "article"
  },
  {
    "title": "Treating the Storage Stack Like a Network",
    "doi": "https://doi.org/10.1145/3032968",
    "publication_date": "2017-02-16",
    "publication_year": 2017,
    "authors": "Ioan Stefanovici; Bianca Schroeder; Greg O’Shea; Eno Thereska",
    "corresponding_authors": "",
    "abstract": "In a data center, an IO from an application to distributed storage traverses not only the network but also several software stages with diverse functionality. This set of ordered stages is known as the storage or IO stack. Stages include caches, hypervisors, IO schedulers, file systems, and device drivers. Indeed, in a typical data center, the number of these stages is often larger than the number of network hops to the destination. Yet, while packet routing is fundamental to networks, no notion of IO routing exists on the storage stack. The path of an IO to an endpoint is predetermined and hard coded. This forces IO with different needs (e.g., requiring different caching or replica selection) to flow through a one-size-fits-all IO stack structure, resulting in an ossified IO stack. This article proposes sRoute, an architecture that provides a routing abstraction for the storage stack. sRoute comprises a centralized control plane and “sSwitches” on the data plane. The control plane sets the forwarding rules in each sSwitch to route IO requests at runtime based on application-specific policies. A key strength of our architecture is that it works with unmodified applications and Virtual Machines (VMs). This article shows significant benefits of customized IO routing to data center tenants: for example, a factor of 10 for tail IO latency, more than 60% better throughput for a customized replication protocol, a factor of 2 in throughput for customized caching, and enabling live performance debugging in a running system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2588368217",
    "type": "article"
  },
  {
    "title": "High-Performance General Functional Regenerating Codes with Near-Optimal Repair Bandwidth",
    "doi": "https://doi.org/10.1145/3051122",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Qing Liu; Dan Feng; Yuchong Hu; Zhan Shi; Min Fu",
    "corresponding_authors": "",
    "abstract": "Erasure codes are widely used in modern distributed storage systems to prevent data loss and server failures. Regenerating codes are a class of erasure codes that trade storage efficiency and computation for repair bandwidth reduction. However, their nonunified coding parameters and huge computational overhead prohibit their applications. Hence, we first propose a family of General Functional Regenerating (GFR) codes with uncoded repair, balancing storage efficiency and repair bandwidth with general parameters. The GFR codes take advantage of a heuristic repair algorithm, which makes efforts to employ as little repair bandwidth as possible to repair a single failure. Second, we also present a scheduled shift multiplication (SSM) algorithm, which accelerates the matrix product over the Galois field by scheduling the order of coding operations, so encoding and repairing of GFR codes can be executed by fast bitwise shifting and exclusive-OR. Compared to the traditional table-lookup multiplication algorithm, our SSM algorithm gains 1.2 to 2 X speedup in our experimental evaluations, with little effect on the repair success rate.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2626828312",
    "type": "article"
  },
  {
    "title": "SUPA",
    "doi": "https://doi.org/10.1145/3129901",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Dong-Jin Kim; Kyu Ho Park; Chan‐Hyun Youn",
    "corresponding_authors": "",
    "abstract": "To design the write buffer and flash translation layer (FTL) for a solid-state drive (SSD), previous studies have tried to increase overall SSD performance by parallel I/O and garbage collection overhead reduction. Recent works have proposed pattern-based managements, which uses the request size and read- or write-intensiveness to apply different policies to each type of data. In our observation, the locations of read and write requests are closely related, and the pattern of each type of data can be changed. In this work, we propose SUPA, a single unified read-write buffer and pattern-change-aware FTL on multi-channel SSD architecture. To increase both read and write hit ratios on the buffer based on locality, we use a single unified read-write buffer for both clean and dirty blocks. With proposed buffer, we can increase buffer hit ratio up to 8.0% and reduce 33.6% and 7.5% of read and write latencies, respectively. To handle pattern-changed blocks, we add a pattern handler between the buffer and the FTL, which monitors channel status and handles data by applying one of the two different policies according to the pattern changes. With pattern change handling process, we can reduce 1.0% and 15.4% of read and write latencies, respectively. In total, our evaluations show that SUPA can get up to 2.0 and 3.9 times less read and write latency, respectively, without loss of lifetime in comparison to previous works.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2769892223",
    "type": "article"
  },
  {
    "title": "Lightweight Robust Size Aware Cache Management",
    "doi": "https://doi.org/10.1145/3507920",
    "publication_date": "2022-06-02",
    "publication_year": 2022,
    "authors": "Gil Einziger; Ohad Eytan; Roy Friedman; Benjamin Manes",
    "corresponding_authors": "",
    "abstract": "Modern key-value stores, object stores, Internet proxy caches, and Content Delivery Networks (CDN) often manage objects of diverse sizes, e.g., blobs, video files of different lengths, images with varying resolutions, and small documents. In such workloads, size-aware cache policies outperform size-oblivious algorithms. Unfortunately, existing size-aware algorithms tend to be overly complicated and computationally expensive. Our work follows a more approachable pattern; we extend the prevalent (size-oblivious) TinyLFU cache admission policy to handle variable-sized items. Implementing our approach inside two popular caching libraries only requires minor changes. We show that our algorithms yield competitive or better hit-ratios and byte hit-ratios compared to the state-of-the-art size-aware algorithms such as AdaptSize, LHD, LRB, and GDSF. Further, a runtime comparison indicates that our implementation is faster by up to 3× compared to the best alternative, i.e., it imposes a much lower CPU overhead.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3162003969",
    "type": "article"
  },
  {
    "title": "A utility-based unified disk scheduling framework for shared mixed-media services",
    "doi": "https://doi.org/10.1145/1326542.1326546",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Akshat Verma; Rohit Jain; Sugata Ghosal",
    "corresponding_authors": "",
    "abstract": "We present a new disk scheduling framework to address the needs of a shared multimedia service that provides differentiated multilevel quality-of-service for mixed-media workloads. In such a shared service, requests from different users have different associated performance objectives and utilities, in accordance with the negotiated service-level agreements (SLAs). Service providers typically provision resources only for average workload intensity, so it becomes important to handle workload surges in a way that maximizes the utility of the served requests. We capture the performance objectives and utilities associated with these multiclass diverse workloads in a unified framework and formulate the disk scheduling problem as a reward maximization problem. We map the reward maximization problem to a minimization problem on graphs and, by novel use of graph-theoretic techniques, design a scheduling algorithm that is computationally efficient and optimal in the class of seek-optimizing algorithms. Comprehensive experimental studies demonstrate that the proposed algorithm outperforms other disk schedulers under all loads, with the performance improvement approaching 100% under certain high load conditions. In contrast to existing schedulers, the proposed scheduler is extensible to new performance objectives (workload type) and utilities by simply altering the reward functions associated with the requests.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2033637312",
    "type": "article"
  },
  {
    "title": "An SLC-Like Programming Scheme for MLC Flash Memory",
    "doi": "https://doi.org/10.1145/3129257",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Chien-Chung Ho; Yu-Ming Chang; Yuan-Hao Chang; Tei‐Wei Kuo",
    "corresponding_authors": "",
    "abstract": "Although the multilevel cell (MLC) technique is widely adopted by flash-memory vendors to boost the chip density and lower the cost, it results in serious performance and reliability problems. Different from past work, a new cell programming method is proposed to not only significantly improve chip performance but also reduce the potential bit error rate. In particular, a single-level cell (SLC)-like programming scheme is proposed to better explore the threshold-voltage relationship to denote different MLC bit information, which in turn drastically provides a larger window of threshold voltage similar to that found in SLC chips. It could result in less programming iterations and simultaneously a much less reliability problem in programming flash-memory cells. In the experiments, the new programming scheme could accelerate the programming speed up to 742% and even reduce the bit error rate up to 471% for MLC pages.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2789789552",
    "type": "article"
  },
  {
    "title": "Countering Fragmentation in an Enterprise Storage System",
    "doi": "https://doi.org/10.1145/3366173",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Ram Kesavan; Matthew Curtis-Maury; Vinay Devadas; Kesari Mishra",
    "corresponding_authors": "",
    "abstract": "As a file system ages, it can experience multiple forms of fragmentation. Fragmentation of the free space in the file system can lower write performance and subsequent read performance. Client operations as well as internal operations, such as deduplication, can fragment the layout of an individual file, which also impacts file read performance. File systems that allow sub-block granular addressing can gather intra-block fragmentation, which leads to wasted free space. Similarly, wasted space can also occur when a file system writes a collection of blocks out to object storage as a single large object, because the constituent blocks can become free at different times. The impact of fragmentation also depends on the underlying storage media. This article studies each form of fragmentation in the NetApp ® WAFL ® file system, and explains how the file system leverages a storage virtualization layer for defragmentation techniques that physically relocate blocks efficiently, including those in read-only snapshots. The article analyzes the effectiveness of these techniques at reducing fragmentation and improving overall performance across various storage media.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3013925573",
    "type": "article"
  },
  {
    "title": "A Lightweight Data Location Service for Nondeterministic Exascale Storage Systems",
    "doi": "https://doi.org/10.1145/2629451",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "Zhiwei Sun; Anthony Skjellum; Lee Ward; Matthew Leon Curry",
    "corresponding_authors": "",
    "abstract": "In this article, we present LWDLS, a lightweight data location service designed for Exascale storage systems (storage systems with order of 10 18 bytes) and geo-distributed storage systems (large storage systems with physically distributed locations). LWDLS provides a search-based data location solution, and enables free data placement, movement, and replication. In LWDLS, probe and prune protocols are introduced that reduce topology mismatch, and a heuristic flooding search algorithm (HFS) is presented that achieves higher search efficiency than pure flooding search while having comparable search speed and coverage to the pure flooding search. LWDLS is lightweight and scalable in terms of incorporating low overhead, high search efficiency, no global state, and avoiding periodic messages. LWDLS is fully distributed and can be used in nondeterministic storage systems and in deterministic storage systems to deal with cases where search is needed. Extensive simulations modeling large-scale High Performance Computing (HPC) storage environments provide representative performance outcomes. Performance is evaluated by metrics including search scope, search efficiency, and average neighbor distance. Results show that LWDLS is able to locate data efficiently with low cost of state maintenance in arbitrary network environments. Through these simulations, we demonstrate the effectiveness of protocols and search algorithm of LWDLS.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2011589434",
    "type": "article"
  },
  {
    "title": "A User-Friendly Log Viewer for Storage Systems",
    "doi": "https://doi.org/10.1145/2846101",
    "publication_date": "2016-05-12",
    "publication_year": 2016,
    "authors": "Jayanta Kumar Basak; P. C. Nagesh",
    "corresponding_authors": "",
    "abstract": "System log files contains messages emitted from several modules within a system and carries valuable information about the system state such as device status and error conditions and also about the various tasks within the system such as program names, execution path, including function names and parameters, and the task completion status. For customers with remote support, the system collects and transmits these logs to a central enterprise repository, where these are monitored for alerts, problem forecasting, and troubleshooting. Very large log files limit the interpretability for the support engineers. For an expert, a large volume of log messages may not pose any problem; however, an inexperienced person may get flummoxed due to the presence of a large number of log messages. Often it is desired to present the log messages in a comprehensive manner where a person can view the important messages first and then go into details if required. In this article, we present a user-friendly log viewer where we first hide the unimportant or inconsequential messages from the log file. A user can then click a particular hidden view and get the details of the hided messages. Messages with low utility are considered inconsequential as their removal does not impact the end user for the aforesaid purpose such as problem forecasting or troubleshooting. We relate the utility of a message to the probability of its appearance in the due context. We present machine-learning-based techniques that computes the usefulness of individual messages in a log file. We demonstrate identification and discarding of inconsequential messages to shrink the log size to acceptable limits. We have tested this over real-world logs and observed that eliminating such low value data can reduce the log files significantly (30% to 55%), with minimal error rates (7% to 20%). When limited user feedback is available, we show modifications to the technique to learn the user intent and accordingly further reduce the error.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2367199898",
    "type": "article"
  },
  {
    "title": "Scalable and fault-tolerant support for variable bit-rate data in the exedra streaming server",
    "doi": "https://doi.org/10.1145/1111609.1111611",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Stergios V. Anastasiadis; Kenneth C. Sevcik; Michael Stumm",
    "corresponding_authors": "",
    "abstract": "We describe the design and implementation of the Exedra continuous media server, and experimentally evaluate alternative resource management policies using a prototype system that we built. Exedra has been designed to provide scalable and efficient support for variable bit-rate media streams whose compression efficiency leads to reduced storage space and bandwidth requirements in comparison to constant bit-rate streams of equivalent quality. We examine alternative disk striping policies, and quantify the benefits of innovative techniques for storage space allocation, buffer management, and resource reservation, which we developed to achieve both predictability and high-performance in handling disk and network data transfers of variable size. Additionally, we investigate the differences between diverse data replication schemes over disk arrays, and compare methods for disk access time reservation that enable tolerance of disk failures at minimal cost. Overall, we demonstrate the feasibility of building network media servers that exploit the latest advances in media compression technology towards reducing the cost of wide-scale streaming services for stored data.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2018790444",
    "type": "article"
  },
  {
    "title": "Can Applications Recover from <b> <tt>fsync</tt> </b> Failures?",
    "doi": "https://doi.org/10.1145/3450338",
    "publication_date": "2021-05-30",
    "publication_year": 2021,
    "authors": "Anthony Rebello; Yuvraj Patel; Ramnatthan Alagappan; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We analyze how file systems and modern data-intensive applications react to fsync failures. First, we characterize how three Linux file systems (ext4, XFS, Btrfs) behave in the presence of failures. We find commonalities across file systems (pages are always marked clean, certain block writes always lead to unavailability) as well as differences (page content and failure reporting is varied). Next, we study how five widely used applications (PostgreSQL, LMDB, LevelDB, SQLite, Redis) handle fsync failures. Our findings show that although applications use many failure-handling strategies, none are sufficient: fsync failures can cause catastrophic outcomes such as data loss and corruption. Our findings have strong implications for the design of file systems and applications that intend to provide strong durability guarantees.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3171277139",
    "type": "article"
  },
  {
    "title": "Divide-and-conquer scheme for strictly optimal retrieval of range queries",
    "doi": "https://doi.org/10.1145/1629075.1629077",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Ali Şaman Tosun",
    "corresponding_authors": "Ali Şaman Tosun",
    "abstract": "Declustering distributes data among parallel disks to reduce retrieval cost using I/O parallelism. Many schemes were proposed for single copy declustering of spatial data. Recently, declustering using replication gained a lot of interest and several schemes with different properties were proposed. It is computationally expensive to verify optimality of replication schemes designed for range queries and existing schemes verify optimality for up to 50 disks. In this article, we propose a novel method to find replicated declustering schemes that render all spatial range queries optimal. The proposed scheme uses threshold based declustering, divisibility of large queries for optimization and optimistic approach to compute maximum flow. The proposed scheme is generic and works for any number of dimensions. Experimental results show that using 3 copies there exist allocations that render all spatial range queries optimal for up to 750 disks in 2 dimensions and with the exception of several values for up to 100 disks in 3 dimensions. The proposed scheme improves search for strictly optimal replicated declustering schemes significantly and will be a valuable tool to answer open problems on replicated declustering.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2065265499",
    "type": "article"
  },
  {
    "title": "Workload-based generation of administrator hints for optimizing database storage utilization",
    "doi": "https://doi.org/10.1145/1326542.1326545",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Kaushik Dutta; Raju Rangaswami; Sajib Kundu",
    "corresponding_authors": "",
    "abstract": "Database storage management at data centers is a manual, time-consuming, and error-prone task. Such management involves regular movement of database objects across storage nodes in an attempt to balance the I/O bandwidth utilization across disk drives. Achieving such balance is critical for avoiding I/O bottlenecks and thereby maximizing the utilization of the storage system. However, manual management of the aforesaid task, apart from increasing administrative costs, encumbers the greater risks of untimely and erroneous operations. We address the preceding concerns with STORM, an automated approach that combines low-overhead information gathering of database access and storage usage patterns with efficient analysis to generate accurate and timely hints for the administrator regarding data movement operations. STORM's primary objective is minimizing the volume of data movement required (to minimize potential down-time or reduction in performance) during the reconfiguration operation, with the secondary constraints of space and balanced I/O-bandwidth-utilization across the storage devices. We analyze and evaluate STORM theoretically, using a simulation framework, as well as experimentally. We show that the dynamic data layout reconfiguration problem is NP-hard and we present a heuristic that provides an approximate solution in O ( Nlog ( N / M ) + ( N / M ) 2 ) time, where M is the number of storage devices and N is the total number of database objects residing in the storage devices. A simulation study shows that the heuristic converges to an acceptable solution that is successful in balancing storage utilization with an accuracy that lies within 7% of the ideal solution. Finally, an experimental study demonstrates that the STORM approach can improve the overall performance of the TPC-C benchmark by as much as 22%, by reconfiguring an initial random, but evenly distributed, placement of database objects.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2165546830",
    "type": "article"
  },
  {
    "title": "D <scp>ude</scp> T <scp>x</scp>",
    "doi": "https://doi.org/10.1145/3177920",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Mengxing Liu; Mingxing Zhang; Kang Chen; Xuehai Qian; Yongwei Wu; Weimin Zheng; Jinglei Ren",
    "corresponding_authors": "",
    "abstract": "Emerging non-volatile memory (NVM) offers non-volatility, byte-addressability, and fast access at the same time. It is suggested that programs should access NVM directly through CPU load and store instructions. To guarantee crash consistency, durable transactions are regarded as a common choice of applications for accessing persistent memory data. However, existing durable transaction systems employ either undo logging , which requires a fence for every memory write, or redo logging , which requires intercepting all memory reads within transactions. Both approaches incur significant overhead. This article presents D ude T x , a crash-consistent durable transaction system that avoids the drawbacks of both undo and redo logging. D ude T x uses shadow DRAM to decouple the execution of a durable transaction into three fully asynchronous steps. The advantage is that only minimal fences and no memory read instrumentation are required. This design enables an out-of-the-box concurrency control mechanism, transactional memory or fine-grained locks, to be used as an independent component. The evaluation results show that D ude T x adds durability to a software transactional memory system with only 7.4%--24.6% throughput degradation. Compared to typical existing durable transaction systems, D ude T x provides 1.7× --4.4× higher throughput. Moreover, D ude T x can be implemented with hardware transactional memory or lock-based concurrency control, leading to a further 1.7× and 3.3× speedup, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2795857730",
    "type": "article"
  },
  {
    "title": "Bringing Order to Chaos",
    "doi": "https://doi.org/10.1145/3242091",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Youjip Won; Joontaek Oh; Jaemin Jung; Gyeongyeol Choi; Seongbae Son; Joo-Young Hwang; Sangyeun Cho",
    "corresponding_authors": "",
    "abstract": "This work is dedicated to eliminating the overhead required for guaranteeing the storage order in the modern IO stack. The existing block device adopts a prohibitively expensive approach in ensuring the storage order among write requests: interleaving the write requests with Transfer-and-Flush . For exploiting the cache barrier command for flash storage, we overhaul the IO scheduler, the dispatch module, and the filesystem so that these layers are orchestrated to preserve the ordering condition imposed by the application with which the associated data blocks are made durable. The key ingredients of Barrier-Enabled IO stack are Epoch-based IO scheduling , Order-Preserving Dispatch , and Dual-Mode Journaling . Barrier-enabled IO stack can control the storage order without Transfer-and-Flush overhead. We implement the barrier-enabled IO stack in server as well as in mobile platforms. SQLite performance increases by 270% and 75%, in server and in smartphone, respectively. In a server storage, BarrierFS brings as much as by 43 × and by 73× performance gain in MySQL and SQLite, respectively, against EXT4 via relaxing the durability of a transaction.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2895007362",
    "type": "article"
  },
  {
    "title": "CORES",
    "doi": "https://doi.org/10.1145/3321704",
    "publication_date": "2019-06-26",
    "publication_year": 2019,
    "authors": "Wen Weidong; Yang Li; Wenhai Li; Lingfeng Deng; Yanxiang He",
    "corresponding_authors": "",
    "abstract": "The relatively high cost of record deserialization is increasingly becoming the bottleneck of column-based storage systems in tree-structured applications [58]. Due to record transformation in the storage layer, unnecessary processing costs derived from fields and rows irrelevant to queries may be very heavy in nested schemas, significantly wasting the computational resources in large-scale analytical workloads. This leads to the question of how to reduce both the deserialization and IO costs of queries with highly selective filters following arbitrary paths in a nested schema. We present CORES (Column-Oriented Regeneration Embedding Scheme) to push highly selective filters down into column-based storage engines, where each filter consists of several filtering conditions on a field. By applying highly selective filters in the storage layer, we demonstrate that both the deserialization and IO costs could be significantly reduced. We show how to introduce fine-grained composition on filtering results. We generalize this technique by two pair-wise operations, rollup and drilldown, such that a series of conjunctive filters can effectively deliver their payloads in nested schema. The proposed methods are implemented on an open-source platform. For practical purposes, we highlight how to build a column storage engine and how to drive a query efficiently based on a cost model. We apply this design to the nested relational model especially when hierarchical entities are frequently required by ad hoc queries. The experiments, including a real workload and the modified TPCH benchmark, demonstrate that CORES improves the performance by 0.7×--26.9× compared to state-of-the-art platforms in scan-intensive workloads.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2954034700",
    "type": "article"
  },
  {
    "title": "Everyone Loves File",
    "doi": "https://doi.org/10.1145/3377877",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Bradley C. Kuszmaul; Matteo Frigo; Justin Mazzola Paluska; Alexander Sandler",
    "corresponding_authors": "",
    "abstract": "Oracle File Storage Service (FSS) is an elastic filesystem provided as a managed NFS service. A pipelined Paxos implementation underpins a scalable block store that provides linearizable multipage limited-size transactions. Above the block store, a scalable B-tree holds filesystem metadata and provides linearizable multikey limited-size transactions. Self-validating B-tree nodes and housekeeping operations performed as separate transactions allow each key in a B-tree transaction to require only one page in the underlying block transaction. The filesystem provides snapshots by using versioned key-value pairs. The system is programmed using a nonblocking lock-free programming style. Presentation servers maintain no persistent local state making them scalable and easy to failover. A non-scalable Paxos-replicated hash table holds configuration information required to bootstrap the system. An additional B-tree provides conversational multi-key minitransactions for control-plane information. The system throughput can be predicted by comparing an estimate of the network bandwidth needed for replication to the network bandwidth provided by the hardware. Latency on an unloaded system is about 4 times higher than a Linux NFS server backed by NVMe, reflecting the cost of replication. FSS has been in production since January 2018 and holds tens of thousands of customer file systems comprising many petabytes of data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3009472476",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on USENIX FAST 2019",
    "doi": "https://doi.org/10.1145/3372347",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Arif Merchant; Hakim Weatherspoon",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3014228925",
    "type": "article"
  },
  {
    "title": "Practical Quick File Server Migration",
    "doi": "https://doi.org/10.1145/3377322",
    "publication_date": "2020-05-23",
    "publication_year": 2020,
    "authors": "Keiichi Matsuzawa; Mitsuo Hayasaka; Takahiro Shinagawa",
    "corresponding_authors": "",
    "abstract": "Regular file server upgrades are indispensable to improve performance, robustness, and power consumption. In upgrading file servers, it is crucial to quickly migrate file-sharing services between heterogeneous servers with little downtime while minimizing performance interference. We present a practical quick file server migration scheme based on the postcopy approach that defers file copy until after switching servers. This scheme can (1) reduce downtime with on-demand file migration, (2) avoid performance interference using background migration, and (3) support heterogeneous servers with stub-based file management. We discuss several practical issues, such as intermittent crawling and traversal strategy, and present the solutions in our scheme. We also address several protocol-specific issues to achieve a smooth migration. This scheme is good enough to be adopted in production systems, as it has been demonstrated for several years in real operational environments. The performance evaluation demonstrates that the downtime is less than 3 seconds, and the first file access after switching servers does not cause a timeout in the default timeout settings; it takes less than 10 seconds in most cases and up to 84.55 seconds even in a large directory tree with a depth of 16 and a width of 1,000. Although the total migration time is approximately 3 times longer than the traditional precopy approach that copies all files in advance, our scheme allows the clients to keep accessing files with acceptable overhead. We also show that appropriate selection of traversal strategy reduces tail latency by 88%, and the overhead after the migration is negligible.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3044319774",
    "type": "article"
  },
  {
    "title": "Automatic Stream Identification to Improve Flash Endurance in Data Centers",
    "doi": "https://doi.org/10.1145/3470007",
    "publication_date": "2022-03-29",
    "publication_year": 2022,
    "authors": "Janki Bhimani; Zhengyu Yang; Jingpei Yang; Adnan Maruf; Ningfang Mi; Rajinikanth Pandurangan; Changho Choi; Vijay Balakrishnan",
    "corresponding_authors": "",
    "abstract": "The demand for high performance I/O in Storage-as-a-Service (SaaS) is increasing day by day. To address this demand, NAND Flash-based Solid-state Drives (SSDs) are commonly used in data centers as cache- or top-tiers in the storage rack ascribe to their superior performance compared to traditional hard disk drives (HDDs). Meanwhile, with the capital expenditure of SSDs declining and the storage capacity of SSDs increasing, all-flash data centers are evolving to serve cloud services better than SSD-HDD hybrid data centers. During this transition, the biggest challenge is how to reduce the Write Amplification Factor (WAF) as well as to improve the endurance of SSD since this device has a limited program/erase cycles. A specified case is that storing data with different lifetimes (i.e., I/O streams with similar temporal fetching patterns such as reaccess frequency) in one single SSD can cause high WAF, reduce the endurance, and downgrade the performance of SSDs. Motivated by this, multi-stream SSDs have been developed to enable data with a different lifetime to be stored in different SSD regions. The logic behind this is to reduce the internal movement of data—when garbage collection is triggered, there are high chances of having data blocks with either all the pages being invalid or valid. However, the limitation of this technology is that the system needs to manually assign the same streamID to data with a similar lifetime. Unfortunately, when data arrives, it is not known how important this data is and how long this data will stay unmodified. Moreover, according to our observation, with different definitions of a lifetime (i.e., different calculation formulas based on selected features previously exhibited by data, such as sequentiality, and frequency), streamID identification may have varying impacts on the final WAF of multi-stream SSDs. Thus, in this article, we first develop a portable and adaptable framework to study the impacts of different workload features and their combinations on write amplification. We then propose a feature-based stream identification approach, which automatically co-relates the measurable workload attributes (such as I/O size, I/O rate, and so on.) with high-level workload features (such as frequency, sequentiality, and so on.) and determines a right combination of workload features for assigning streamIDs . Finally, we develop an adaptable stream assignment technique to assign streamID for changing workloads dynamically. Our evaluation results show that our automation approach of stream detection and separation can effectively reduce the WAF by using appropriate features for stream assignment with minimal implementation overhead.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220864668",
    "type": "article"
  },
  {
    "title": "Kangaroo: Theory and Practice of Caching Billions of Tiny Objects on Flash",
    "doi": "https://doi.org/10.1145/3542928",
    "publication_date": "2022-06-13",
    "publication_year": 2022,
    "authors": "Sara McAllister; Benjamin Wagner vom Berg; Julian Tutuncu-Macias; Juncheng Yang; Sathya Gunasekar; Jimmy Lu; Daniel S. Berger; Nathan Beckmann; Gregory R. Ganger",
    "corresponding_authors": "",
    "abstract": "Many social-media and IoT services have very large working sets consisting of billions of tiny (≈100 B) objects. Large, flash-based caches are important to serving these working sets at acceptable monetary cost. However, caching tiny objects on flash is challenging for two reasons: (i) SSDs can read/write data only in multi-KB “pages” that are much larger than a single object, stressing the limited number of times flash can be written; and (ii) very few bits per cached object can be kept in DRAM without losing flash’s cost advantage. Unfortunately, existing flash-cache designs fall short of addressing these challenges: write-optimized designs require too much DRAM, and DRAM-optimized designs require too many flash writes. We present Kangaroo , a new flash-cache design that optimizes both DRAM usage and flash writes to maximize cache performance while minimizing cost. Kangaroo combines a large, set-associative cache with a small, log-structured cache. The set-associative cache requires minimal DRAM, while the log-structured cache minimizes Kangaroo’s flash writes. Experiments using traces from Meta and Twitter show that Kangaroo achieves DRAM usage close to the best prior DRAM-optimized design, flash writes close to the best prior write-optimized design, and miss ratios better than both. Kangaroo’s design is Pareto-optimal across a range of allowed write rates, DRAM sizes, and flash sizes, reducing misses by 29% over the state of the art. These results are corroborated by analytical models presented herein and with a test deployment of Kangaroo in a production flash cache at Meta.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4282542724",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2836327",
    "publication_date": "2015-11-21",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We introduce Skylight, a novel methodology that combines software and hardware techniques to reverse engineer key properties of drive-managed Shingled Magnetic Recording (SMR) drives. The software part of Skylight measures the latency of controlled I/O ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4252535048",
    "type": "paratext"
  },
  {
    "title": "TriCache: A User-Transparent Block Cache Enabling High-Performance Out-of-Core Processing with In-Memory Programs",
    "doi": "https://doi.org/10.1145/3583139",
    "publication_date": "2023-02-13",
    "publication_year": 2023,
    "authors": "Guanyu Feng; Huanqi Cao; Xiaowei Zhu; Bowen Yu; Yuanwei Wang; Zixuan Ma; Shengqi Chen; Wenguang Chen",
    "corresponding_authors": "",
    "abstract": "Out-of-core systems rely on high-performance cache sub-systems to reduce the number of I/O operations. Although the page cache in modern operating systems enables transparent access to memory and storage devices, it suffers from efficiency and scalability issues on cache misses, forcing out-of-core systems to design and implement their own cache components, which is a non-trivial task. This study proposes TriCache, a cache mechanism that enables in-memory programs to efficiently process out-of-core datasets without requiring any code rewrite. It provides a virtual memory interface on top of the conventional block interface to simultaneously achieve user transparency and sufficient out-of-core performance. A multi-level block cache design is proposed to address the challenge of per-access address translations required by a memory interface. It can exploit spatial and temporal localities in memory or storage accesses to render storage-to-memory address translation and page-level concurrency control adequately efficient for the virtual memory interface. Our evaluation shows that in-memory systems operating on top of TriCache can outperform Linux OS page cache by more than one order of magnitude, and can deliver performance comparable to or even better than that of corresponding counterparts designed specifically for out-of-core scenarios.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4320490937",
    "type": "article"
  },
  {
    "title": "Realizing Strong Determinism Contract on Log-Structured Merge Key-Value Stores",
    "doi": "https://doi.org/10.1145/3582695",
    "publication_date": "2023-02-24",
    "publication_year": 2023,
    "authors": "Miryeong Kwon; Seung-Jun Lee; Hyunkyu Choi; Joo-Young Hwang; Myoungsoo Jung",
    "corresponding_authors": "",
    "abstract": "We propose Vigil-KV , a hardware and software co-designed framework that eliminates long-tail latency almost perfectly by introducing strong latency determinism. To make Get latency deterministic, Vigil-KV first enables a predictable latency mode (PLM) interface on a real datacenter-scale NVMe SSD, having knowledge about the nature of the underlying flash technologies. Vigil-KV at the system-level then hides the non-deterministic time window (associated with SSD’s internal tasks and/or write services) by internally scheduling the different device states of PLM across multiple physical functions. Vigil-KV further schedules compaction/flush operations and client requests being aware of PLM’s restrictions thereby integrating strong latency determinism into LSM KVs. We implement Vigil-KV upon a 1.92TB NVMe SSD prototype and Linux 4.19.91, but other LSM KVs can adopt its concept. We evaluate diverse Facebook and Yahoo scenarios with Vigil-KV, and the results show that Vigil-KV can reducethe tail latency of a baseline KV system by 3.19× while reducing the average latency by 34%, on average.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4321786054",
    "type": "article"
  },
  {
    "title": "CostCounter: A Better Method for Collision Mitigation in Cuckoo Hashing",
    "doi": "https://doi.org/10.1145/3596910",
    "publication_date": "2023-05-12",
    "publication_year": 2023,
    "authors": "Haonan Wu; Shuxian Wang; Zhanfeng Jin; Yuhang Zhang; Ruyun Ma; S. Fan; R Chao",
    "corresponding_authors": "",
    "abstract": "Hardware is often required to support fast search and high-throughput applications. Consequently, the performance of search algorithms is limited by storage bandwidth. Hence, the search algorithm must be optimized accordingly. We propose a CostCounter (CC) algorithm based on cuckoo hashing and an Improved CostCounter (ICC) algorithm. A better path can be selected when collisions occur using a cost counter to record the kick-out situation. Our simulation results indicate that the CC and ICC algorithms can achieve more significant performance improvements than Random Walk (RW), Breadth First Search (BFS), and MinCounter (MC). With two buckets and two slots per bucket, under the 95% memory load rate of the maximum load rate, CC and ICC are optimized on read-write times over 20% and 80% compared to MC and BFS, respectively. Furthermore, the CC and ICC algorithms achieve a slight improvement in storage efficiency compared with MC. In addition, we implement RW, MC, and the proposed algorithms using fine-grained locking to support a high throughput rate. From the test on field programmable gate arrays, we verify the simulation results and our algorithms optimize the maximum throughput over 23% compared to RW and 9% compared to MC under 95% of the memory capacity. The test results indicate that our CC and ICC algorithms can achieve better performance in terms of hardware bandwidth and memory load efficiency without incurring a significant resource cost.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4376277786",
    "type": "article"
  },
  {
    "title": "The Security War in File Systems: An Empirical Study from A Vulnerability-centric Perspective",
    "doi": "https://doi.org/10.1145/3606020",
    "publication_date": "2023-07-17",
    "publication_year": 2023,
    "authors": "Jinghan Sun; Shaobo Li; Jun Xu; Jian Huang",
    "corresponding_authors": "",
    "abstract": "This article presents a systematic study on the security of modern file systems, following a vulnerability-centric perspective. Specifically, we collected 377 file system vulnerabilities committed to the CVE database in the past 20 years. We characterize them from four dimensions: why the vulnerabilities appear, how the vulnerabilities can be exploited, what consequences can arise, and how the vulnerabilities are fixed. This way, we build a deep understanding of the attack surfaces faced by file systems, the threats imposed by the attack surfaces, and the good and bad practices in mitigating the attacks in file systems. We envision that our study will bring insights towards the future development of file systems, the enhancement of file system security, and the relevant vulnerability-mitigating solutions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4384523606",
    "type": "article"
  },
  {
    "title": "Exploiting Flat Namespace to Improve File System Metadata Performance on Ultra-Fast, Byte-Addressable NVMs",
    "doi": "https://doi.org/10.1145/3620673",
    "publication_date": "2023-09-06",
    "publication_year": 2023,
    "authors": "Miao Cai; Junru Shen; Bin Tang; Hao Huang; Baoliu Ye",
    "corresponding_authors": "",
    "abstract": "The conventional file system provides a hierarchical namespace by structuring it as a directory tree. Tree-based namespace structure leads to inefficient file path walk and expensive namespace tree traversal, underutilizing ultra-low access latency and superior sequential performance provided by non-volatile memories (NVMs). This article proposes FlatFS+, an NVM file system that features a flat namespace architecture while providing a compatible hierarchical namespace view. FlatFS+ incorporates three novel techniques: the direct file path walk model, range-optimized B r tree, and compressed index key design with scan and write dual optimization, to fully exploit flat namespace to improve file system metadata performance on ultra-fast, byte-addressable NVMs. Evaluation results demonstrate that FlatFS+ achieves significant performance improvements for metadata-intensive benchmarks and real-world applications compared to other file systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386492886",
    "type": "article"
  },
  {
    "title": "From Missteps to Milestones: A Journey to Practical Fail-Slow Detection",
    "doi": "https://doi.org/10.1145/3617690",
    "publication_date": "2023-09-11",
    "publication_year": 2023,
    "authors": "Ruiming Lu; Erci Xu; Yiming Zhang; Fengyi Zhu; Zhaosheng Zhu; Mengtian Wang; Zongpeng Zhu; Guangtao Xue; Jiwu Shu; Minglu Li; Jiesheng Wu",
    "corresponding_authors": "",
    "abstract": "The newly emerging “fail-slow” failures plague both software and hardware where the victim components are still functioning yet with degraded performance. To address this problem, this article presents Perseus , a practical fail-slow detection framework for storage devices. Perseus leverages a light regression-based model to quickly pinpoint and analyze fail-slow failures at the granularity of drives. Within a 10-month close monitoring on 248K drives, Perseus managed to find 304 fail-slow cases. Isolating them can reduce the (node-level) 99.99th tail latency by 48%. We assemble a large-scale fail-slow dataset (including 41K normal drives and 315 verified fail-slow drives) from our production traces, based on which we provide root cause analysis on fail-slow drives covering a variety of ill-implemented scheduling, hardware defects, and environmental factors. We have released the dataset to the public for fail-slow study.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386603151",
    "type": "article"
  },
  {
    "title": "Empowering Storage Systems Research with NVMeVirt: A Comprehensive NVMe Device Emulator",
    "doi": "https://doi.org/10.1145/3625006",
    "publication_date": "2023-09-21",
    "publication_year": 2023,
    "authors": "Sang-Hoon Kim; Jaehoon Shim; E. LEE; Seongyeop Jeong; I. Kang; Jin‐Soo Kim",
    "corresponding_authors": "",
    "abstract": "There have been drastic changes in the storage device landscape recently. At the center of the diverse storage landscape lies the NVMe interface, which allows high-performance and flexible communication models required by these next-generation device types. However, its hardware-oriented definition and specification are bottlenecking the development and evaluation cycle for new revolutionary storage devices. Furthermore, existing emulators lack the capability to support the advanced storage configurations that are currently in the spotlight. In this article, we present NVMeVirt, a novel approach to facilitate software-defined NVMe devices. A user can define any NVMe device type with custom features, and NVMeVirt allows it to bridge the gap between the host I/O stack and the virtual NVMe device in software. We demonstrate the advantages and features of NVMeVirt by realizing various storage types and configurations, such as conventional SSDs, low-latency high-bandwidth NVM SSDs, zoned namespace SSDs, and key-value SSDs with the support of PCI peer-to-peer DMA and NVMe-oF target offloading. We also make cases for storage research with NVMeVirt, such as studying the performance characteristics of database engines and extending the NVMe specification for the improved key-value SSD performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386928504",
    "type": "article"
  },
  {
    "title": "Understanding Persistent-memory-related Issues in the Linux Kernel",
    "doi": "https://doi.org/10.1145/3605946",
    "publication_date": "2023-10-03",
    "publication_year": 2023,
    "authors": "Om Rameshwar Gatla; Duo Zhang; Wei Xu; Mai Zheng",
    "corresponding_authors": "",
    "abstract": "Persistent memory (PM) technologies have inspired a wide range of PM-based system optimizations. However, building correct PM-based systems is difficult due to the unique characteristics of PM hardware. To better understand the challenges as well as the opportunities to address them, this article presents a comprehensive study of PM-related issues in the Linux kernel. By analyzing 1,553 PM-related kernel patches in depth and conducting experiments on reproducibility and tool extension, we derive multiple insights in terms of PM patch categories, PM bug patterns, consequences, fix strategies, triggering conditions, and remedy solutions. We hope our results could contribute to the development of robust PM-based storage systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387306324",
    "type": "article"
  },
  {
    "title": "Improving Bandwidth Efficiency for Consistent Multistream Storage",
    "doi": "https://doi.org/10.1145/2435204.2435206",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "Andromachi Hatzieleftheriou; Stergios V. Anastasiadis",
    "corresponding_authors": "",
    "abstract": "Synchronous small writes play a critical role in system availability because they safely log recent state modifications for fast recovery from crashes. Demanding systems typically dedicate separate devices to logging for adequate performance during normal operation and redundancy during state reconstruction. However, storage stacks enforce page-sized granularity in data transfers from memory to disk. Thus, they consume excessive storage bandwidth to handle small writes, which hurts performance. The problem becomes worse, as filesystems often handle multiple concurrent streams, which effectively generate random I/O traffic. In a journaled filesystem, we introduce wasteless journaling as a mount mode that coalesces synchronous concurrent small writes of data into full page-sized journal blocks. Additionally, we propose selective journaling to automatically activate wasteless journaling on data writes with size below a fixed threshold. We implemented a functional prototype of our design over a widely-used filesystem. Our modes are compared against existing methods using microbenchmarks and application-level workloads on stand-alone servers and a multitier networked system. We examine synchronous and asynchronous writes. Coalescing small data updates to the journal sequentially preserves filesystem consistency while it reduces consumed bandwidth up to several factors, decreases recovery time up to 22%, and lowers write latency up to orders of magnitude.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2157472106",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2555948",
    "publication_date": "2013-11-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Scaling up a RAID-0 volume with added disks can increase its storage capacity and I/O bandwidth simultaneously. For preserving a round-robin data distribution, existing scaling approaches require all the data to be migrated. Such large data migration ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4255495964",
    "type": "paratext"
  },
  {
    "title": "Isotope",
    "doi": "https://doi.org/10.1145/3032967",
    "publication_date": "2017-02-16",
    "publication_year": 2017,
    "authors": "Ji-Yong Shin; Mahesh Balakrishnan; Tudor Marian; Hakim Weatherspoon",
    "corresponding_authors": "",
    "abstract": "Existing storage stacks are top heavy and expect little from block storage. As a result, new high-level storage abstractions—and new designs for existing abstractions—are difficult to realize, requiring developers to implement from scratch complex functionality such as failure atomicity and fine-grained concurrency control. In this article, we argue that pushing transactional isolation into the block store (in addition to atomicity and durability) is both viable and broadly useful, resulting in simpler high-level storage systems that provide strong semantics without sacrificing performance. We present Isotope, a new block store that supports ACID transactions over block reads and writes. Internally, Isotope uses a new multiversion concurrency control protocol that exploits fine-grained, subblock parallelism in workloads and offers both strict serializability and snapshot isolation guarantees. We implemented several high-level storage systems over Isotope, including two key-value stores that implement the LevelDB API over a hash table and B-tree, respectively, and a POSIX file system. We show that Isotope’s block-level transactions enable systems that are simple (100s of lines of code), robust (i.e., providing ACID guarantees), and fast (e.g., 415MB/s for random file writes). We also show that these systems can be composed using Isotope, providing applications with transactions across different high-level constructs such as files, directories, and key-value pairs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2588471848",
    "type": "article"
  },
  {
    "title": "Client-Side Journaling for Durable Shared Storage",
    "doi": "https://doi.org/10.1145/3149372",
    "publication_date": "2017-11-17",
    "publication_year": 2017,
    "authors": "Andromachi Hatzieleftheriou; Stergios V. Anastasiadis",
    "corresponding_authors": "",
    "abstract": "Hardware consolidation in the datacenter often leads to scalability bottlenecks from heavy utilization of critical resources, such as the storage and network bandwidth. Client-side caching on durable media is already applied at block level to reduce the storage backend load but has received criticism for added overhead, restricted sharing, and possible data loss at client crash. We introduce a journal to the kernel-level client of an object-based distributed filesystem to improve durability at high I/O performance and reduced shared resource utilization. Storage virtualization at the file interface achieves clear consistency semantics across data and metadata, supports native file sharing among clients, and provides flexible configuration of durable data staging at the host. Over a prototype that we have implemented, we experimentally quantify the performance and efficiency of the proposed Arion system in comparison to a production system. We run microbenchmarks and application-level workloads over a local cluster and a public cloud. We demonstrate reduced latency by 60% and improved performance up to 150% at reduced server network and disk bandwidth by 41% and 77%, respectively. The performance improvement reaches 92% for 16 relational databases as clients and gets as high as 11.3x with two key-value stores as clients.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2769787816",
    "type": "article"
  },
  {
    "title": "Experience from Two Years of Visualizing Flash with SSDPlayer",
    "doi": "https://doi.org/10.1145/3149356",
    "publication_date": "2017-11-17",
    "publication_year": 2017,
    "authors": "Gala Yadgar; Roman Shor",
    "corresponding_authors": "",
    "abstract": "Data visualization is a thriving field of computer science, with widespread impact on diverse scientific disciplines, from medicine and meteorology to visual data mining. Advances in large-scale storage systems, as well as low-level storage technology, played a significant role in accelerating the applicability and adoption of modern visualization techniques. Ironically, “the cobbler’s children have no shoes”: Researchers who wish to analyze storage systems and devices are usually limited to a variety of static histograms and basic displays. The dynamic nature of data movement on flash has motivated the introduction of SSDPlayer, a graphical tool for visualizing the various processes that cause data movement on solid-state drives (SSDs). In 2015, we used the initial version of SSDPlayer to demonstrate how visualization can assist researchers and developers in their understanding of modern, complex flash-based systems. While we continued to use SSDPlayer for analysis purposes, we found it extremely useful for education and presentation purposes as well. In this article, we describe our experience from two years of using, sharing, and extending SSDPlayer and how similar techniques can further advance storage systems research and education.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2770515094",
    "type": "article"
  },
  {
    "title": "hfplayer",
    "doi": "https://doi.org/10.1145/3149392",
    "publication_date": "2017-11-30",
    "publication_year": 2017,
    "authors": "Ali Akbar Haghdoost; Weiping He; Jerry Fredin; David H. C. Du",
    "corresponding_authors": "",
    "abstract": "We introduce new methods to replay intensive block I/O workloads more accurately. These methods can be used to reproduce realistic workloads for benchmarking, performance validation, and tuning of a high-performance block storage device/system. In this article, we study several sources in the stock operating system that introduce uncertainty in the workload replay. Based on the remedies of these findings, we design and develop a new replay tool called hfplayer that replays intensive block I/O workloads in a similar unscaled environment with more accuracy. To replay a given workload trace in a scaled environment with faster storage or host server, the dependency between I/O requests becomes crucial since the timing and ordering of I/O requests is expected to change according to these dependencies. Therefore, we propose a heuristic way of speculating I/O dependencies in a block I/O trace. Using the generated dependency graph, hfplayer tries to propagate I/O related performance gains appropriately along the I/O dependency chains and mimics the original application behavior when it executes in a scaled environment with slower or faster storage system and servers. We evaluate hfplayer with a wide range of workloads using several accuracy metrics and find that it produces better accuracy when compared to other replay approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2778288116",
    "type": "article"
  },
  {
    "title": "Introduction to special issue USENIX FAST 2007",
    "doi": "https://doi.org/10.1145/1288783.1288784",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2009216775",
    "type": "article"
  },
  {
    "title": "Introduction to special issue of USENIX FAST 2008",
    "doi": "https://doi.org/10.1145/1416944.1416945",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Mary Frances Baker",
    "corresponding_authors": "Mary Frances Baker",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2084511922",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1367829",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4248119854",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1353452",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Today's data storage systems are increasingly adopting low-cost disk drives that have higher capacity but lower reliability, leading to more frequent rebuilds and to a higher risk of unrecoverable media errors. We propose an efficient intradisk ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4252828197",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1288783",
    "publication_date": "2007-10-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4255268333",
    "type": "paratext"
  },
  {
    "title": "Editor-in Chief Letter",
    "doi": "https://doi.org/10.1145/3180478",
    "publication_date": "2018-02-26",
    "publication_year": 2018,
    "authors": "Sam H. Noh",
    "corresponding_authors": "Sam H. Noh",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2791247044",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on NVM and Storage",
    "doi": "https://doi.org/10.1145/3180480",
    "publication_date": "2018-02-26",
    "publication_year": 2018,
    "authors": "Chun Jason Xue; Michael M. Swift",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2792972914",
    "type": "article"
  },
  {
    "title": "Lerna",
    "doi": "https://doi.org/10.1145/3310368",
    "publication_date": "2019-02-28",
    "publication_year": 2019,
    "authors": "Mohamed M. Saad; Roberto Palmieri; Binoy Ravindran",
    "corresponding_authors": "",
    "abstract": "We present Lerna, an end-to-end tool that automatically and transparently detects and extracts parallelism from data-dependent sequential loops. Lerna uses speculation combined with a set of techniques including code profiling, dependency analysis, instrumentation, and adaptive execution. Speculation is needed to avoid conservative actions and detect actual conflicts. Lerna targets applications that are hard-to-parallelize due to data dependency. Our experimental study involves the parallelization of 13 applications with data dependencies. Results on a 24-core machine show an average of 2.7× speedup for micro-benchmarks and 2.5× for the macro-benchmarks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2922750061",
    "type": "article"
  },
  {
    "title": "LDJ",
    "doi": "https://doi.org/10.1145/3365918",
    "publication_date": "2019-11-28",
    "publication_year": 2019,
    "authors": "Dong Hyun Kang; Sang-Won Lee; Young Ik Eom",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a simple but practical and efficient optimization scheme for journaling in ext4, called lightweight data journaling ( LDJ ). By compressing journaled data prior to writing, LDJ can perform comparable to or even faster than the default ordered journaling (OJ) mode in ext4 on top of both HDDs and flash storage devices, while still guaranteeing the version consistency of the data journaling (DJ) mode. This surprising result can be explained with three main reasons. First, on modern storage devices, the sequential write pattern dominating in DJ mode is more and more high-performant than the random one in OJ mode. Second, the compression significantly reduces the amount of journal writes, which will in turn make the write completion faster and prolong the lifespan of storage devices. Third, the compression also enables the atomicity of each journal write without issuing an intervening FLUSH command between journal data blocks and commit block, thus halving the number of costly FLUSH calls in LDJ . We have prototyped our LDJ by slightly modifying the existing ext4 with jbd2 for journaling and also e2fsck for recovery; less than 300 lines of source code were changed. Also, we carried out a comprehensive evaluation using four standard benchmarks and three real applications. Our evaluation results clearly show that LDJ outperforms the OJ mode by up to 9.6× on the real applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2991897275",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX ATC 2023",
    "doi": "https://doi.org/10.1145/3635156",
    "publication_date": "2024-02-19",
    "publication_year": 2024,
    "authors": "Dan Williams; Julia Lawall",
    "corresponding_authors": "",
    "abstract": "The ten papers in this special section are revised and extended versions of papers presented at the 8th IEEE International Conference on Intelligent Transportation Systems (ITSC'05) held in Vienna, Austria, on September 13-16, 2005. The papers in this ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391946029",
    "type": "article"
  },
  {
    "title": "A Memory-Disaggregated Radix Tree",
    "doi": "https://doi.org/10.1145/3664289",
    "publication_date": "2024-05-08",
    "publication_year": 2024,
    "authors": "Xuchuan Luo; Pengfei Zuo; Jiacheng Shen; Jiazhen Gu; Xin Wang; Michael R. Lyu; Yangfan Zhou",
    "corresponding_authors": "",
    "abstract": "Disaggregated memory (DM) is an increasingly prevalent architecture with high resource utilization. It separates computing and memory resources into two pools and interconnects them with fast networks. Existing range indexes on DM are based on B+ trees, which suffer from large inherent read and write amplifications. The read and write amplifications rapidly saturate the network bandwidth, resulting in low request throughput and high access latency of B+ trees on DM. In this article, we propose that the radix tree is more suitable for DM than the B+ tree due to smaller read and write amplifications. However, constructing a radix tree on DM is challenging due to the costly lock-based concurrency control, the bounded memory-side IOPS, and the complicated computing-side cache validation. To address these challenges, we design SMART , the first radix tree for disaggregated memory with high performance. Specifically, we leverage (1) a hybrid concurrency control scheme including lock-free internal nodes and fine-grained lock-based leaf nodes to reduce lock overhead, (2) a computing-side read-delegation and write-combining technique to break through the IOPS upper bound by reducing redundant I/Os, and (3) a simple yet effective reverse check mechanism for computing-side cache validation. Experimental results show that SMART achieves 6.1× higher throughput under typical write-intensive workloads and 2.8× higher throughput under read-only workloads in YCSB benchmarks, compared with state-of-the-art B+ trees on DM.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396746001",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX OSDI 2023",
    "doi": "https://doi.org/10.1145/3654801",
    "publication_date": "2024-06-06",
    "publication_year": 2024,
    "authors": "Roxana Geambasu; Ed Nightingale",
    "corresponding_authors": "",
    "abstract": "This special section of the <italic>IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG)</italic> presents the five most highly rated papers from the 2023 IEEE Pacific Visualization Symposium (IEEE PacificVis), hosted in Seoul, Korea from ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399389317",
    "type": "article"
  },
  {
    "title": "Extremely-Compressed SSDs with I/O Behavior Prediction",
    "doi": "https://doi.org/10.1145/3677044",
    "publication_date": "2024-07-16",
    "publication_year": 2024,
    "authors": "Xiangyu Yao; Qiao Li; Kaihuan Lin; Xinbiao Gan; Jie Zhang; Congming Gao; Zhirong Shen; Quanqing Xu; Chuanhui Yang; Chun Jason Xue",
    "corresponding_authors": "",
    "abstract": "As the data volume continues to grow exponentially, there is an increasing demand for large storage system capacity. Data compression techniques effectively reduce the volume of written data, enhancing space efficiency. As a result, many modern SSDs have already incorporated data compression capabilities. However, data compression introduces additional processing overhead in critical I/O paths, potentially affecting system performance. Currently, most compression solutions in flash-based storage systems employ fixed compression algorithms for all incoming data without leveraging differences among various data access patterns. This leads to sub-optimal compression efficiency. This article proposes a data-type-aware Flash Translation Layer (DAFTL) scheme to maximize space efficiency without compromising system performance. First, we propose an I/O behavior prediction method to forecast future access on specific data. Then, DAFTL matches data types with distinct I/O behaviors to compression algorithms of varying intensities, achieving an optimal balance between performance and space efficiency. Specifically, it employs higher-intensity compression algorithms for less frequently accessed data to maximize space efficiency. For frequently accessed data, it utilizes lower-intensity but faster compression algorithms to maintain system performance. Finally, an improved compact compression method is proposed to effectively eliminate page fragmentation and further enhance space efficiency. Extensive evaluations using a variety of real-world workloads, as well as the workloads with real data we collected on our platforms, demonstrate that DAFTL achieves more data reductions than other approaches. When compared to the state-of-the-art compression schemes, DAFTL reduces the total number of pages written to the SSD by an average of 8%, 21.3%, and 25.6% for data with high, medium, and low compressibility, respectively. In the case of workloads with real data, DAFTL achieves an average reduction of 10.4% in the total number of pages written to SSD. Furthermore, DAFTL exhibits comparable or even improved read and write performance compared to other solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400701378",
    "type": "article"
  },
  {
    "title": "From SSDs Back to HDDs: Optimizing VDO to Support Inline Deduplication and Compression for HDDs as Primary Storage Media",
    "doi": "https://doi.org/10.1145/3678250",
    "publication_date": "2024-07-23",
    "publication_year": 2024,
    "authors": "Patrick Raaf; André Brinkmann; Eric Borba; Hossein Asadi; Sai Narasimhamurthy; John Bent; Mohamad El-Batal; Reza Salkhordeh",
    "corresponding_authors": "",
    "abstract": "Deduplication and compression are powerful techniques to reduce the ratio between the quantity of logical data stored and the physical amount of consumed storage. Deduplication can impose significant performance overheads, as duplicate detection for large systems induces random accesses to the backend storage. These random accesses have led to the concern that deduplication for primary storage and HDDs are not compatible. Most inline data reduction solutions are therefore optimized for SSDs and discourage their use for HDDs, even for sequential workloads. In this work, we show that these concerns are valid if and only if the lessons learned from deduplication research are not applied. We have therefore investigated data reduction solutions for primary storage based on the RedHat Virtual Disk Optimizer (VDO) and show that directly applying them can decrease sequential write performance for HDDs by 36×. We then show that slight modifications to VDO plus the integration of a very small SSD area significantly improve performance even beyond the performance without data reduction enabled, making HDDs more cost-efficient for a wide range of mostly sequential cloud workloads than SSDs. Additionally, these VDO optimizations do not require to maintain different code bases for HDDs and SSDs, and we therefore provide the first data reduction solution applicable to both storage media.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400919901",
    "type": "article"
  },
  {
    "title": "A Portable Linux-based Firmware for NVMe Computational Storage Devices",
    "doi": "https://doi.org/10.1145/3697352",
    "publication_date": "2024-09-25",
    "publication_year": 2024,
    "authors": "Rick Wertenbroek; Yann Thoma; Alberto Dassatti",
    "corresponding_authors": "",
    "abstract": "Over the years, interest in computational storage devices has been growing steadily. This is largely due to the rise of data-intensive applications, such as machine learning, online video distribution, astrophysics, and genomics. Moving compute operations closer to the data provides benefits in terms of scaling possibilities and energy efficiency. The development of computational storage devices has been limited by the need for specialized and complex hardware. In this work, we propose a portable Linux-based firmware framework for the development of NVMe computational storage devices. Our firmware runs on a variety of hardware platforms ranging from expensive FPGA solutions to inexpensive off-the-shelf single board computers. The firmware leverages the vast Linux software ecosystem to facilitate the development and prototyping of novel computational storage devices. We benchmark our firmware on multiple hardware platforms and demonstrate its versatility through several computational examples including a content-aware disk image search engine based on natural language processing and AI-driven image recognition.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402829368",
    "type": "article"
  },
  {
    "title": "Leveraging On-demand Processing to Co-optimize Scalability and Efficiency for Fully-external Graph Computation",
    "doi": "https://doi.org/10.1145/3701037",
    "publication_date": "2024-11-23",
    "publication_year": 2024,
    "authors": "Tsun-Yu Yang; Yizou Chen; Liang Yu-hong; Ming-Chang Yang",
    "corresponding_authors": "",
    "abstract": "Fully-external graph computation systems exhibit optimal scalability by computing the ever-growing, large-scale graph with constant amount of memory on a single machine. In particular, they keep the entire massive graph data in storage and iteratively load parts of them into memory for computation. Nevertheless, despite the merit of optimal scalability, their unreasonably-low efficiency often makes them uncompetitive, and even unpractical, to the other types of graph computation systems. The key rationale is that most existing fully-external graph computation systems over-emphasize retrieving graph data from storage through sequential access. Although this principle achieves high storage bandwidth, it often causes reading excessive and irrelevant data, which can severely degrade their overall efficiency. Therefore, this work presents Seraph, a fully-external graph computation system that achieves optimal S calability while toward satisfactory E fficiency improvement. Particularly, inspired by the modern storage offering comparable sequential and random access speeds, Seraph adopts the principle of on-demand processing to access the necessary graph data for saving I/O while enjoying the decent speed in random access. On the basis of this principle, Seraph further devises three practical designs to bring excellent performance leap to fully-external graph computation: 1) the hybrid format to represent the graph data for striking a good balance between I/O amount and access locality, 2) the vertex passing to enable efficient vertex updates on top of hybrid format, and 3) the selective pre-computation to re-use the loaded data for I/O reduction. Our evaluations reveal that Seraph notably outperforms other state-of-the-art fully-external systems under all the evaluated billion-scale graphs and representative graph algorithms by up to two orders of magnitude.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404650497",
    "type": "article"
  },
  {
    "title": "Evolving the Cloud Block Store with Performance, Elasticity, Availability, and Hardware Offloading",
    "doi": "https://doi.org/10.1145/3705925",
    "publication_date": "2024-12-04",
    "publication_year": 2024,
    "authors": "Erci Xu; Weidong Zhang; Qiuping Wang; Xiaolu Zhang; Yuesheng Gu; Zhenwei Lu; Tao Ouyang; G.F. Dong; Wenwen Peng; Z. S. Xu; Shuo Zhang; Dong Wu; Yilei Peng; Tianyun Wang; Haoran Zhang; Jiasheng Wang; Wenyuan Yan; Yuanyuan Dong; Wenhui Yao; Zhongjie Wu; Lingjun Zhu; Chao Shi; Yinhu Wang; R. Liu; WU Jun-ping; Jiaji Zhu; Jiesheng Wu",
    "corresponding_authors": "",
    "abstract": "In this paper, we qualitatively and quantitatively discuss the design choices, production experience, and lessons in building the Elastic Block Storage ( EBS ) at Alibaba Cloud over the past decade. To cope with hardware advancement and users’ demands, we shift our focus from design simplicity in EBS1 to high performance and space efficiency in EBS2 , and finally reducing network traffic amplification in EBS3 . In addition to the architectural evolutions, we also summarize development lessons and experiences as four topics, including: (i) achieving high elasticity in latency, throughput, IOPS and capacity; (ii) improving availability by minimizing the blast radius of individual, regional, and global failure events; (iii) identifying the motivations and key tradeoffs in various hardware offloading solutions; and (iv) identifying the pros/cons of alternative solutions and explaining why seemingly promising ideas would not work in practice.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405003459",
    "type": "article"
  },
  {
    "title": "A Dynamic Characteristic Aware Index Structure Optimized for Real-world Datasets",
    "doi": "https://doi.org/10.1145/3707642",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Jin Yang; Heejin Yoon; Gyeongchan Yun; Sam H. Noh; Young-ri Choi",
    "corresponding_authors": "",
    "abstract": "Many datasets in real life are complex and dynamic, that is, their key densities are varied over the whole key space and their key distributions change over time. It is challenging for an index structure to efficiently support all key operations for data management, in particular, search, insert, and scan, for such dynamic datasets. In this paper, we present DyTIS (Dynamic dataset Targeted Index Structure), an index that targets dynamic datasets. DyTIS, though based on the structure of Extendible hashing, leverages the CDF of the key distribution of a dataset, and learns and adjusts its structure as the dataset grows. The key novelty behind DyTIS is to group keys by the natural key order and maintain keys in sorted order in each bucket to support scan operations within a hash index. We also define what we refer to as a dynamic dataset and propose a means to quantify its dynamic characteristics. Our experimental results show that DyTIS provides higher performance than the state-of-the-art learned index for the dynamic datasets considered. We also analyze the effects of the dynamic characteristics of datasets, including sequential datasets, as well as the effect of multiple threads on the performance of the indexes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405547925",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1149976",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In the Outsourced Database (ODB) model, entities outsource their data management needs to a third-party service provider. Such a service provider offers mechanisms for its clients to create, store, update, and access (query) their databases. This work ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4231227649",
    "type": "paratext"
  },
  {
    "title": "Exploiting Nil-external Interfaces for Fast Replicated Storage",
    "doi": "https://doi.org/10.1145/3542821",
    "publication_date": "2022-06-06",
    "publication_year": 2022,
    "authors": "Aishwarya Ganesan; Ramnatthan Alagappan; Anthony Rebello; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "Do some storage interfaces enable higher performance than others? Can one identify and exploit such interfaces to realize high performance in storage systems? This article answers these questions in the affirmative by identifying nil-externality , a property of storage interfaces. A nil-externalizing (nilext) interface may modify state within a storage system but does not externalize its effects or system state immediately to the outside world. As a result, a storage system can apply nilext operations lazily, improving performance. In this article, we take advantage of nilext interfaces to build high-performance replicated storage. We implement Skyros , a nilext-aware replication protocol that offers high performance by deferring ordering and executing operations until their effects are externalized. We show that exploiting nil-externality offers significant benefit: For many workloads, Skyros provides higher performance than standard consensus-based replication. For example, Skyros offers 3× lower latency while providing the same high throughput offered by throughput-optimized Paxos.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4281720073",
    "type": "article"
  },
  {
    "title": "EMPRESS: Accelerating Scientific Discovery through Descriptive Metadata Management",
    "doi": "https://doi.org/10.1145/3523698",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Margaret Lawson; William Gropp; Jay Lofstead",
    "corresponding_authors": "",
    "abstract": "High-performance computing scientists are producing unprecedented volumes of data that take a long time to load for analysis. However, many analyses only require loading in the data containing particular features of interest and scientists have many approaches for identifying these features. Therefore, if scientists store information (descriptive metadata) about these identified features, then for subsequent analyses they can use this information to only read in the data containing these features. This can greatly reduce the amount of data that scientists have to read in, thereby accelerating analysis. Despite the potential benefits of descriptive metadata management, no prior work has created a descriptive metadata system that can help scientists working with a wide range of applications and analyses to restrict their reads to data containing features of interest. In this article, we present EMPRESS, the first such solution. EMPRESS offers all of the features needed to help accelerate discovery: It can accelerate analysis by up to 300 ×, supports a wide range of applications and analyses, is high-performing, is highly scalable, and requires minimal storage space. In addition, EMPRESS offers features required for a production-oriented system: scalable metadata consistency techniques, flexible system configurations, fault tolerance as a service, and portability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4297321141",
    "type": "article"
  },
  {
    "title": "<scp>Oasis</scp> : Controlling Data Migration in Expansion of Object-based Storage Systems",
    "doi": "https://doi.org/10.1145/3568424",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Yiming Zhang; Li Wang; Shun Gai; Qiwen Ke; Wenhao Li; Zhenlong Song; Guangtao Xue; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Object-based storage systems have been widely used for various scenarios such as file storage, block storage, blob (e.g., large videos) storage, and so on, where the data is placed among a large number of object storage devices (OSDs). Data placement is critical for the scalability of decentralized object-based storage systems. The state-of-the-art CRUSH placement method is a decentralized algorithm that deterministically places object replicas onto storage devices without relying on a central directory. While enjoying the benefits of decentralization such as high scalability, robustness, and performance, CRUSH-based storage systems suffer from uncontrolled data migration when expanding the capacity of the storage clusters (i.e., adding new OSDs), which is determined by the nature of CRUSH and will cause significant performance degradation when the expansion is nontrivial. This article presents MapX , a novel extension to CRUSH that uses an extra time-dimension mapping (from object creation times to cluster expansion times) for controlling data migration after cluster expansions. Each expansion is viewed as a new layer of the CRUSH map represented by a virtual node beneath the CRUSH root. MapX controls the mapping from objects onto layers by manipulating the timestamps of the intermediate placement groups (PGs). MapX is applicable to a large variety of object-based storage scenarios where object timestamps can be maintained as higher-level metadata. We have applied MapX to the state-of-the-art Ceph-RBD (RADOS Block Device) to implement a migration-controllable, decentralized object-based block store (called Oasis ). Oasis extends the RBD metadata structure to maintain and retrieve approximate object creation times (for migration control) at the granularity of expansion layers. Experimental results show that the MapX -based Oasis block store outperforms the CRUSH-based Ceph-RBD (which is busy in migrating objects after expansions) by 3.17× ∼ 4.31× in tail latency, and 76.3% (respectively, 83.8%) in IOPS for reads (respectively, writes).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4309562627",
    "type": "article"
  },
  {
    "title": "Principled Schedulability Analysis for Distributed Storage Systems Using Thread Architecture Models",
    "doi": "https://doi.org/10.1145/3574323",
    "publication_date": "2022-12-12",
    "publication_year": 2022,
    "authors": "Suli Yang; Jing Liu; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "In this article, we present an approach to systematically examine the schedulability of distributed storage systems, identify their scheduling problems, and enable effective scheduling in these systems. We use Thread Architecture Models (TAMs) to describe the behavior and interactions of different threads in a system, and show both how to construct TAMs for existing systems and utilize TAMs to identify critical scheduling problems. We specify three schedulability conditions that a schedulable TAM should satisfy: completeness, local enforceability, and independence; meeting these conditions enables a system to easily support different scheduling policies. We identify five common problems that prevent a system from satisfying the schedulability conditions, and show that these problems arise in existing systems such as HBase, Cassandra, MongoDB, and Riak, making it difficult or impossible to realize various scheduling disciplines. We demonstrate how to address these schedulability problems using both direct and indirect solutions, with different trade-offs. To show how to apply our approach to enable scheduling in realistic systems, we develop Tamed-HBase and Muzzled-HBase, sets of modifications to HBase that can realize the desired scheduling disciplines, including fairness and priority scheduling, even when presented with challenging workloads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4311159384",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on USENIX FAST 2015",
    "doi": "https://doi.org/10.1145/2825000",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "Ada Gavrilovska; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2090756596",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on USENIX FAST 2014",
    "doi": "https://doi.org/10.1145/2670792",
    "publication_date": "2014-10-31",
    "publication_year": 2014,
    "authors": "Bianca Schroeder; Eno Thereska",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2163282587",
    "type": "article"
  },
  {
    "title": "Frog",
    "doi": "https://doi.org/10.1145/2720022",
    "publication_date": "2015-07-24",
    "publication_year": 2015,
    "authors": "Zhang Ji; Xunfei Jiang; Xiao Qin; Wei‐Shinn Ku; Mohammed Alghamdi",
    "corresponding_authors": "",
    "abstract": "This article presents a framework, Frog, for Context-Based File Systems (CBFSs) that aim at simplifying the development of context-based file systems and applications. Unlike existing informed-based context-aware systems, Frog is a unifying informed-based framework that abstracts context-specific solutions as views, allowing applications to make view selections according to application behaviors. The framework can not only eliminate overheads induced by traditional context analysis, but also simplify the interactions between the context-based file systems and applications. Rather than propagating data through solution-specific interfaces, views in Frog can be selected by inserting their names in file path strings. With Frog in place, programmers can migrate an application from one solution to another by switching among views rather than changing programming interfaces. Since the data consistency issues are automatically enforced by the framework, file-system developers can focus their attention on context-specific solutions. We implement two prototypes to demonstrate the strengths and overheads of our design. Inspired by an observation that there are more than 50% of small files (&lt;4KB) in a file system, we create a Bi-context Archiving Virtual File System (BAVFS) that utilizes conservative and aggressive prefetching for the contexts of random and sequential reads. To improve the performance of random read-and-write operations, the Bi-context Hybrid Virtual File System (BHVFS) combines the update-in-place and update-out-of-place solutions for read-intensive and write-intensive contexts. Our experimental results show that the benefits of Frog-based CBFSs outweigh the overheads introduced by integrating multiple context-specific solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2220062819",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2747982",
    "publication_date": "2015-03-24",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Handling of storage IO in modern operating systems assumes that such devices are slow and CPU cycles are valuable. Consequently, to effectively exploit the underlying hardware resources, for example, CPU cycles, storage bandwidth and the like, whenever ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229677684",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2600090",
    "publication_date": "2014-03-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Storage area network (SAN) is one of the most popular solutions for constructing server environments these days. In these kinds of server environments, HDD-based storage usually becomes the bottleneck of the overall system, but it is not enough to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231627554",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2875132",
    "publication_date": "2016-02-26",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Shingled magnetic recording (SMR) is a means of increasing the density of hard drives that brings a new set of challenges. Due to the nature of SMR disks, updating in place is not an option. Holes left by invalidated data can only be filled if the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232331126",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2578042",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Journaling techniques are widely used in modern file systems as they provide high reliability and fast recovery from system failures. However, it reduces the performance benefit of buffer caching as journaling accounts for a bulk of the storage writes ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238336968",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2705611",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Fast algorithms are proposed for encoding and reconstructing data in RAID based on Reed-Solomon codes. The proposed approach is based on the cyclotomic fast Fourier transform algorithm and enables one to significantly reduce the number of expensive ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239292550",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2661087",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The ever-growing amount of data requires highly scalable storage solutions. The most flexible approach is to use storage pools that can be expanded and scaled down by adding or removing storage devices. To make this approach usable, it is necessary to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244856627",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2888404",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As the popularity of NAND flash expands in arenas from embedded systems to high-performance computing, a high-fidelity understanding of its specific properties becomes increasingly important. Further, with the increasing trend toward multiple-die, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245178612",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2809503",
    "publication_date": "2015-07-29",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents a framework, Frog, for Context-Based File Systems (CBFSs) that aim at simplifying the development of context-based file systems and applications. Unlike existing informed-based context-aware systems, Frog is a unifying informed-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251823732",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2932205",
    "publication_date": "2016-06-27",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "NAND flash memory–based solid state disks (SSDs) have been widely used in enterprise servers. However, flash memory has limited write endurance, as a block becomes unreliable after a finite number of program/erase cycles. Existing wear-leveling ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251869889",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2685385",
    "publication_date": "2014-10-31",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Practical storage systems often adopt erasure codes to tolerate device failures and sector failures, both of which are prevalent in the field. However, traditional erasure codes employ device-level redundancy to protect against sector failures, and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251980550",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2940403",
    "publication_date": "2016-08-29",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "One important aspect of privacy is the ability to securely delete sensitive data from electronic storage in such a way that it cannot be recovered; we call this action secure deletion. Short of physically destroying the entire storage medium, existing ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253352055",
    "type": "paratext"
  },
  {
    "title": "Copy-on-Abundant-Write for Nimble File System Clones",
    "doi": "https://doi.org/10.1145/3423495",
    "publication_date": "2021-01-29",
    "publication_year": 2021,
    "authors": "Yang Zhan; Alex Conway; Yizheng Jiao; Nirjhar Mukherjee; Ian Groombridge; Michael A. Bender; Martı́n Farach-Colton; William Jannen; Rob Johnson; Donald E. Porter; Jun Yuan",
    "corresponding_authors": "",
    "abstract": "Making logical copies, or clones, of files and directories is critical to many real-world applications and workflows, including backups, virtual machines, and containers. An ideal clone implementation meets the following performance goals: (1) creating the clone has low latency; (2) reads are fast in all versions (i.e., spatial locality is always maintained, even after modifications); (3) writes are fast in all versions; (4) the overall system is space efficient. Implementing a clone operation that realizes all four properties, which we call a nimble clone , is a long-standing open problem. This article describes nimble clones in B-ϵ-tree File System (BetrFS), an open-source, full-path-indexed, and write-optimized file system. The key observation behind our work is that standard copy-on-write heuristics can be too coarse to be space efficient, or too fine-grained to preserve locality. On the other hand, a write-optimized key-value store, such as a Bε-tree or an log-structured merge-tree (LSM)-tree, can decouple the logical application of updates from the granularity at which data is physically copied. In our write-optimized clone implementation, data sharing among clones is only broken when a clone has changed enough to warrant making a copy, a policy we call copy-on-abundant-write . We demonstrate that the algorithmic work needed to batch and amortize the cost of BetrFS clone operations does not erode the performance advantages of baseline BetrFS; BetrFS performance even improves in a few cases. BetrFS cloning is efficient; for example, when using the clone operation for container creation, BetrFS outperforms a simple recursive copy by up to two orders-of-magnitude and outperforms file systems that have specialized Linux Containers (LXC) backends by 3--4×.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3128227468",
    "type": "article"
  },
  {
    "title": "Programming Abstractions for Managing Workflows on Tiered Storage Systems",
    "doi": "https://doi.org/10.1145/3457119",
    "publication_date": "2021-10-25",
    "publication_year": 2021,
    "authors": "Devarshi Ghoshal; Lavanya Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Scientific workflows in High Performance Computing ( HPC ) environments are processing large amounts of data. The storage hierarchy on HPC systems is getting deeper, driven by new technologies (NVRAMs, SSDs, etc.) There is a need for new programming abstractions that allow users to seamlessly manage data at the workflow level on multi-tiered storage systems, and provide optimal workflow performance and use of storage resources. In previous work, we introduced a software architecture Managing Data on Tiered Storage for Scientific Workflows (MaDaTS ) that used a Virtual Data Space ( VDS ) abstraction to hide the complexities of the underlying storage system while allowing users to control data management strategies. In this article, we detail the data-centric programming abstractions that allow users to manage a workflow around its data on the storage layer. The programming abstractions simplify data management for scientific workflows on multi-tiered storage systems, without affecting workflow performance or storage capacity. We measure the overheads and effectiveness introduced by the programming abstractions of MaDaTS. Our results show that these abstractions can optimally use the storage capacity in lesser capacity storage tiers, and simplify data management without adding any performance overheads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3210253955",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2435204",
    "publication_date": "2013-03-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Enterprise and cloud data centers are comprised of tens of thousands of servers providing petabytes of storage to a large number of users and applications. At such a scale, these storage systems face two key challenges: (1) hot-spots due to the dynamic ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231562522",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1970338",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Flash Translation Layer (FTL) in Solid-State Disks (SSDs) maps logical addresses to physical addresses for disk drive virtualization. In order to reduce garbage collection overhead, we propose full associative striped block-level mapping. In ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232357261",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1970343",
    "publication_date": "2011-06-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The growing popularity of flash memory is expected to draw attention to the limitations of file-system performance over flash memory. This work was motivated by the modular designs of operating system components such as bus and device drivers. A filter-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238140269",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2501620",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modern computer systems have been built around the assumption that persistent storage is accessed via a slow, block-based interface. However, emerging nonvolatile memory technologies (sometimes referred to as storage class memory (SCM)), are poised to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247859571",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2027066",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Computational science applications are driving a demand for increasingly powerful storage systems. While many techniques are available for capturing the I/O behavior of individual application trial runs and specific components of the storage system, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249536019",
    "type": "paratext"
  },
  {
    "title": "Guest Editorial",
    "doi": "https://doi.org/10.1145/2027066.2027067",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "André Brinkmann; David Pease",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249885926",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2491472",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Declustering techniques reduce query response times through parallel I/O by distributing data among parallel disks. Recently, replication-based approaches were proposed to further reduce the response time. Efficient retrieval of replicated data from ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253220935",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1970348",
    "publication_date": "2011-07-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255055887",
    "type": "paratext"
  },
  {
    "title": "Guest editorial",
    "doi": "https://doi.org/10.1145/1837915.1837916",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Randal Burns; Kimberly Keeton",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1992845992",
    "type": "editorial"
  },
  {
    "title": "Introduction to special issue FAST 2009",
    "doi": "https://doi.org/10.1145/1629080.1629081",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Margo Seltzer; Ric Wheeler",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2037520984",
    "type": "article"
  },
  {
    "title": "Introduction to the special issue USENIX FAST 2012",
    "doi": "https://doi.org/10.1145/2385603.2385605",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Bill Bolosky; Jason Flinn",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2047331078",
    "type": "article"
  },
  {
    "title": "Editorial note",
    "doi": "https://doi.org/10.1145/2385603.2385604",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Darrell Long",
    "corresponding_authors": "Darrell Long",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4210916693",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2339118",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229621012",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2093139",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Solid State Drives (SSD's) have shown promise to be a candidate to replace traditional hard disk drives. The benefits of SSD's over HDD's include better durability, higher performance, and lower power consumption, but due to certain physical ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231622276",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1629080",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As increasing amounts of valuable information are produced and persist digitally, the ability to determine the origin of data becomes important. In science, medicine, commerce, and government, data provenance tracking is essential for rights protection, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235893406",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1534912",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236359357",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2180905",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this work, we examine how transparent block-level compression in the I/O path can improve both the space efficiency and performance of online storage. We present ZBD, a block-layer driver that transparently compresses and decompresses data as they ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239314954",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1714454",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242469290",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1629075",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Parity is a popular form of data protection in redundant arrays of inexpensive/independent disks (RAID). RAID5 dedicates one out of N disks to parity to mask single disk failures, that is, the contents of a block on a failed disk can be reconstructed by ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245060519",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2078861",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Benchmarking file and storage systems on large file-system images is important, but difficult and often infeasible. Typically, running benchmarks on such large disk setups is a frequent source of frustration for file-system evaluators; the scale alone ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245490172",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2385603",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Replicating data off site is critical for disaster recovery reasons, but the current approach of transferring tapes is cumbersome and error prone. Replicating across a wide area network (WAN) is a promising alternative, but fast network connections are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248157685",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1480439",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Kinesis is a novel data placement model for distributed storage systems. It exemplifies three design principles: structure (division of servers into a few failure-isolated segments), freedom of choice (freedom to allocate the best servers to store and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248382149",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1807060",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250823835",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1837915",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Latent sector errors (LSEs) refer to the situation where particular sectors on a drive become inaccessible. LSEs are a critical factor in data reliability, since a single LSE can lead to data loss when encountered during RAID reconstruction after a disk ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253593780",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1502777",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253997463",
    "type": "paratext"
  },
  {
    "title": "Introduction to the Special Issue on MSST 2016",
    "doi": "https://doi.org/10.1145/3078405",
    "publication_date": "2017-05-22",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2619114151",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on USENIX FAST 2017",
    "doi": "https://doi.org/10.1145/3131620",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Geoff Kuenning; Carl A. Waldspurger",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2766365900",
    "type": "article"
  },
  {
    "title": "GCMix",
    "doi": "https://doi.org/10.1145/3149373",
    "publication_date": "2017-11-17",
    "publication_year": 2017,
    "authors": "Sang-Hoon Kim; Jinhyuk Lee; Jin‐Soo Kim",
    "corresponding_authors": "",
    "abstract": "In multi-level cell (MLC) NAND flash memory, two logical pages are overlapped on a single physical page. Even after a logical page is programmed, the data can be corrupted if the programming of the coexisting logical page is interrupted. This phenomenon is called paired page interference. This article proposes a novel software technique to deal with the paired page interference without any additional hardware or extra page write. The proposed technique utilizes valid pages in the victim block during garbage collection (GC) as the backup against the interference, and pairs them with incoming pages written by the host. This approach eliminates undesirable page copy to backup pages against the interference. However, such a strategy has an adverse effect on the hot/cold separation policy, which is essential to improve the efficiency of GC. To limit the downside, we devise a metric to estimate the benefit of GCMix on-the-fly so that GCMix can be adaptively utilized only when the benefit outweighs the overhead. Evaluations using synthetic and real workloads show GCMix can effectively deal with the paired page interference, reducing the write amplification factor by up to 17.5%compared to the traditional technique, while providing comparable I/O performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2769333469",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Massive Storage Systems and Technology 2017",
    "doi": "https://doi.org/10.1145/3148596",
    "publication_date": "2017-11-30",
    "publication_year": 2017,
    "authors": "Aleatha Parker-Wood; Thomas Schwarz",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2775769991",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1227835",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The RAID (Redundant Array of Inexpensive Disks) system has been widely used in practical storage applications for better performance, cost effectiveness, and reliability. This study proposes a novel variant of RAID named Zoned-RAID (Z-RAID). Z-RAID ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230968908",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3160863",
    "publication_date": "2017-12-15",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Log-Structure Merge tree (LSM-tree) has been one of the mainstream indexes in key-value systems supporting a variety of write-intensive Internet applications in today’s data centers. However, the performance of LSM-tree is seriously hampered by ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239629771",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1416944",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240875378",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3098275",
    "publication_date": "2017-06-10",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Deduplication aims to reduce duplicate data in storage systems by removing redundant copies of data blocks, which are compared to one another using fingerprints. However, repeated on-disk fingerprint lookups lead to high disk traffic, which results in a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242002453",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1242520",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "An organization's data is often its most valuable asset, but today's file systems provide few facilities to ensure its safety. Databases, on the other hand, have long provided transactions. Transactions are useful because they provide atomicity, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245927064",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1326542",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The task of consistently and reliably replicating data is fundamental in distributed systems, and numerous existing protocols are able to achieve such replication efficiently. When called on to build a large-scale enterprise storage system with built-in ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246374150",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3141876",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recent research has shown that applications often incorrectly implement crash consistency. We present the Crash-Consistent File System (ccfs), a file system that improves the correctness of application-level crash consistency protocols while maintaining ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253954092",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3054178",
    "publication_date": "2017-03-24",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In a data center, an IO from an application to distributed storage traverses not only the network but also several software stages with diverse functionality. This set of ordered stages is known as the storage or IO stack. Stages include caches, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255405070",
    "type": "paratext"
  },
  {
    "title": "Localized Validation Accelerates Distributed Transactions on Disaggregated Persistent Memory",
    "doi": "https://doi.org/10.1145/3582012",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Ming Zhang; Yu Hua; Pengfei Zuo; Lurong Liu",
    "corresponding_authors": "",
    "abstract": "Persistent memory (PM) disaggregation significantly improves the resource utilization and failure isolation to build a scalable and cost-effective remote memory pool in modern data centers. However, due to offering limited computing power and overlooking the bandwidth and persistence properties of real PMs, existing distributed transaction schemes, which are designed for legacy DRAM-based monolithic servers, fail to efficiently work on the disaggregated PM. In this article, we propose FORD, a F ast O ne-sided R DMA-based D istributed transaction system for the new disaggregated PM architecture. FORD thoroughly leverages one-sided remote direct memory access to handle transactions for bypassing the remote CPU in the PM pool. To reduce the round trips, FORD batches the read and lock operations into one request to eliminate extra locking and validations for the read-write data. To accelerate the transaction commit, FORD updates all remote replicas in a single round trip with parallel undo logging and data visibility control. Moreover, considering the limited PM bandwidth, FORD enables the backup replicas to be read to alleviate the load on the primary replicas, thus improving the throughput. To efficiently guarantee the remote data persistency in the PM pool, FORD selectively flushes data to the backup replicas to mitigate the network overheads. Nevertheless, the original FORD wastes some validation round trips if the read-only data are not modified by other transactions. Hence, we further propose a localized validation scheme to transfer the validation operations for the read-only data from remote to local as much as possible to reduce the round trips. Experimental results demonstrate that FORD significantly improves the transaction throughput by up to 3× and decreases the latency by up to 87.4% compared with state-of-the-art systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4317659166",
    "type": "article"
  },
  {
    "title": "Editor-in-Chief Message",
    "doi": "https://doi.org/10.1145/3574325",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The last three years have been difficult, with the pandemic upending our lives and even our loved ones' health.But we are all trying our best to adapt to a \"new world\" and find innovative ways to work, study, and entertain.In this light, I would like to take this opportunity to express my appreciation to all those who have worked to make the ACM TOS the premier journal it is today, especially the outgoing Editor-in-Chief, Sam Noh.I would like to thank the",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4318618014",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX OSDI 2022",
    "doi": "https://doi.org/10.1145/3584363",
    "publication_date": "2023-03-06",
    "publication_year": 2023,
    "authors": "Marcos K. Aguilera; Hakim Weatherspoon",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4323312678",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX ATC 2022",
    "doi": "https://doi.org/10.1145/3582557",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Jiri Schindler; Noa Zilberman",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4362717794",
    "type": "article"
  },
  {
    "title": "Derrick: A Three-layer Balancer for Self-managed Continuous Scalability",
    "doi": "https://doi.org/10.1145/3594543",
    "publication_date": "2023-04-28",
    "publication_year": 2023,
    "authors": "Andrzej W. Jackowski; Leszek Gryz; Michał Wełnicki; Cezary Dubnicki; Konrad Iwanicki",
    "corresponding_authors": "",
    "abstract": "Data arrangement determines the capacity, resilience, and performance of a distributed storage system. A scalable self-managed system must place its data efficiently not only during stable operation but also after an expansion, planned downscaling, or device failures. In this article, we present Derrick, a data balancing algorithm addressing these needs, which has been developed for HYDRAstor, a highly scalable commercial storage system. Derrick makes its decisions quickly in case of failures but takes additional time to find a nearly optimal data arrangement and a plan for reaching it when the device population changes. Compared to balancing algorithms in two other state-of-the-art systems, Derrick provides better capacity utilization, reduced data movement, and improved performance. Moreover, it can be easily adapted to meet custom placement requirements.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4367313538",
    "type": "article"
  },
  {
    "title": "Block-level Image Service for the Cloud",
    "doi": "https://doi.org/10.1145/3620672",
    "publication_date": "2023-09-05",
    "publication_year": 2023,
    "authors": "Huiba Li; Zhihao Zhang; Yifan Yuan; Rui Du; Kai Ma; Lanzheng Liu; Yiming Zhang; Windsor Hsu",
    "corresponding_authors": "",
    "abstract": "Businesses increasingly need agile and elastic computing infrastructure to respond quickly to real-world situations. By offering efficient process-based virtualization and a layered image system, containers are designed to enable agile and elastic application deployment. However, creating or updating large container clusters is still slow due to the image downloading and unpacking process. In this article, we present DADI Image Service (DADI), a block-level image service for increased agility and elasticity in deploying applications. DADI replaces the waterfall model of starting containers (downloading image, unpacking image, starting container) with fine-grained on-demand transfer of remote images, realizing instant start of containers. To accelerate the cold start of containers, DADI designs a pull-based prefetching mechanism that allows a host to read necessary image data beforehand at the granularity of image layers. We design a peer-to-peer–based decentralized image sharing architecture to balance traffic among all the participating hosts and propose a pull-push collaborative prefetching mechanism to accelerate cold start. DADI efficiently supports various kinds of runtimes including cgroups, QEMU, and so on, further realizing “build once, run anywhere.” DADI has been deployed at scale in the production environment of Alibaba, serving one of the world’s largest ecommerce platforms. Performance results show that DADI can cold start 10,000 containers on 1,000 hosts within 4 s.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386443066",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX FAST 2023",
    "doi": "https://doi.org/10.1145/3612820",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Ashvin Goel; Dalit Naor",
    "corresponding_authors": "",
    "abstract": "This special section of the IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG) presents the five most highly rated papers from the 2022 IEEE Pacific Visualization Symposium (IEEE PacificVis). This year, IEEE PacificVis was scheduled to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388658341",
    "type": "article"
  },
  {
    "title": "An LSM Tree Augmented with B <sup>+</sup> Tree on Nonvolatile Memory",
    "doi": "https://doi.org/10.1145/3633475",
    "publication_date": "2023-12-02",
    "publication_year": 2023,
    "authors": "Donguk Kim; Jong-Sung Lee; Keun Soo Lim; Jun Heo; Tae Jun Ham; Jae W. Lee",
    "corresponding_authors": "",
    "abstract": "Modern log-structured merge (LSM) tree-based key-value stores are widely used to process update-heavy workloads effectively as the LSM tree sequentializes write requests to a storage device to maximize storage performance. However, this append-only approach leaves many outdated copies of frequently updated key-value pairs, which need to be routinely cleaned up through the operation called compaction . When the system load is modest, compaction happens in background. However, at a high system load, it can quickly become the major performance bottleneck. To address this compaction bottleneck and further improve the write throughput of LSM tree-based key-value stores, we propose LAB-DB, which augments the existing LSM tree with a pair of B + trees on byte-addressable nonvolatile memory (NVM). The auxiliary B + trees on NVM reduce both compaction frequency and compaction time, hence leading to lower compaction overhead for writes and fewer storage accesses for reads. According to our evaluation of LAB-DB on RocksDB with YCSB benchmarks, LAB-DB achieves 94% and 67% speedups on two write-intensive workloads (Workload A and F), and also a 43% geomean speedup on read-intensive YCSB Workload B, C, D, and E. This performance gain comes with a low cost of NVM whose size is just 0.6% of the entire dataset to demonstrate the scalability of LAB-DB with an ever increasing volume of future datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389269221",
    "type": "article"
  },
  {
    "title": "Empirical Evaluation and Enhancement of Enterprise Storage System Request Scheduling",
    "doi": "https://doi.org/10.1145/3193741",
    "publication_date": "2018-04-27",
    "publication_year": 2018,
    "authors": "Deng Zhou; Vania Fang; Tao Xie; Wen Pan; Ram Kesavan; Tony X. Lin; Naresh Patel",
    "corresponding_authors": "",
    "abstract": "Since little has been reported in the literature concerning enterprise storage system file-level request scheduling, we do not have enough knowledge about how various scheduling factors affect performance. Moreover, we are in lack of a good understanding on how to enhance request scheduling to adapt to the changing characteristics of workloads and hardware resources. To answer these questions, we first build a request scheduler prototype based on WAFL®, a mainstream file system running on numerous enterprise storage systems worldwide. Next, we use the prototype to quantitatively measure the impact of various scheduling configurations on performance on a NetApp®'s enterprise-class storage system. Several observations have been made. For example, we discover that in order to improve performance, the priority of write requests and non-preempted restarted requests should be boosted in some workloads. Inspired by these observations, we further propose two scheduling enhancement heuristics called SORD (size-oriented request dispatching) and QATS (queue-depth aware time slicing). Finally, we evaluate them by conducting a wide range of experiments using workloads generated by SPC-1 and SFS2014 on both HDD-based and all-flash platforms. Experimental results show that the combination of the two can noticeably reduce average request latency under some workloads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2802334693",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on USENIX FAST 2018",
    "doi": "https://doi.org/10.1145/3242152",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Nitin Agrawal; Raju Rangaswami",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2900794361",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on SYSTOR 2017",
    "doi": "https://doi.org/10.1145/3287097",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Peter Desnoyers; Eyal de Lara",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2903721908",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on ACM International Systems and Storage Conference (SYSTOR) 2018",
    "doi": "https://doi.org/10.1145/3313898",
    "publication_date": "2019-02-28",
    "publication_year": 2019,
    "authors": "Gala Yadgar; Donald E. Porter",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2941408608",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on the 2018 USENIX Annual Technical Conference (ATC’18)",
    "doi": "https://doi.org/10.1145/3322100",
    "publication_date": "2019-05-13",
    "publication_year": 2019,
    "authors": "Haryadi S. Gunawi; Benjamin Reed",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2945935634",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on OSDI’18",
    "doi": "https://doi.org/10.1145/3322101",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Andrea C. Arpaci-Dusseau; Geoffrey M. Voelker",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2946979298",
    "type": "article"
  },
  {
    "title": "ACM TOS Distinguished Reviewers",
    "doi": "https://doi.org/10.1145/3313879",
    "publication_date": "2019-02-28",
    "publication_year": 2019,
    "authors": "Sam H. Noh",
    "corresponding_authors": "Sam H. Noh",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2947077932",
    "type": "article"
  },
  {
    "title": "EIC Message",
    "doi": "https://doi.org/10.1145/3372345",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "editorial Free AccessEIC Message Share on ACM Transactions on StorageVolume 15Issue 4February 2020 Article No.: 22epp 1–2https://doi.org/10.1145/3372345Published:29 January 2020 0citation143DownloadsMetricsTotal Citations0Total Downloads143Last 12 Months69Last 6 weeks16 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4205398792",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3336116",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The relatively high cost of record deserialization is increasingly becoming the bottleneck of column-based storage systems in tree-structured applications [58]. Due to record transformation in the storage layer, unnecessary processing costs derived from ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230729438",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3311821",
    "publication_date": "2019-04-20",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Users and Operating Systems (OSs) have vastly different views of files. OSs use files to persist data and structured information. To accomplish this, OSs treat files as named collections of bytes managed in hierarchical file systems. Despite their ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235123262",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3373756",
    "publication_date": "2019-12-18",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236358784",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3326597",
    "publication_date": "2019-06-26",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We introduce TxFS, a transactional file system that builds upon a file system’s atomic-update mechanism such as journaling. Though prior work has explored a number of transactional file systems, TxFS has a unique set of properties: a simple API, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238075018",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3208078",
    "publication_date": "2018-05-25",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The reuse distance (least recently used (LRU) stack distance) is an essential metric for performance prediction and optimization of storage cache. Over the past four decades, there have been steady improvements in the algorithmic efficiency of reuse ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238513351",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1210596",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents virtual allocation, a scheme for flexible storage allocation. Virtual allocation separates storage allocation from the file system. It employs an allocate-on-write strategy which lets applications fit into the actual usage of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238994161",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3190860",
    "publication_date": "2018-04-04",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the advanced technology in persistent random access memory (PRAM), PRAM such as three-dimen-sional XPoint memory and Phase Change Memory (PCM) is emerging as a promising candidate for the next-generation medium for both (main) memory and storage. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239968612",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1168910",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "High-end computing is suffering a data deluge from experiments, simulations, and apparatus that creates overwhelming application dataset sizes. This has led to the proliferation of high-end mass storage systems, storage area clusters, and data centers. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242340173",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1044956.1044957",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Sreeranga P. Rajan",
    "corresponding_authors": "Sreeranga P. Rajan",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248076584",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3297750",
    "publication_date": "2018-12-15",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "During the past decade, network and storage devices have undergone rapid performance improvements, delivering ultra-low latency and several Gbps of bandwidth. Nevertheless, current network and storage stacks fail to deliver this hardware performance to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248922495",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1138041",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As an emerging nonvolatile secondary storage technology, MEMS-based storage exhibits several desirable properties including high performance, high storage volumic density, low power consumption, low entry cost, and small form factor. However, MEMS-based ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253782206",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3282875",
    "publication_date": "2018-11-26",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We introduce protocol-aware recovery (Par), a new approach that exploits protocol-specific knowledge to correctly recover from storage faults in distributed systems. We demonstrate the efficacy of Par through the design and implementation of <underline>...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254129013",
    "type": "paratext"
  },
  {
    "title": "Introduction to the Special Section on USENIX ATC 2019",
    "doi": "https://doi.org/10.1145/3383194",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Dahlia Malkhi; Dan Tsafrir",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3033880246",
    "type": "article"
  },
  {
    "title": "Spiffy",
    "doi": "https://doi.org/10.1145/3386368",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Kuei Sun; Daniel Fryer; Russell Wang; Sagar Patel; Joseph Chu; Matthew Lakier; Angela Demke Brown; Ashvin Goel",
    "corresponding_authors": "",
    "abstract": "Many file-system applications such as defragmentation tools, file-system checkers, or data recovery tools, operate at the storage layer. Today, developers of these file-system aware storage applications require detailed knowledge of the file-system format, which requires significant time to learn, often by trial and error, due to insufficient documentation or specification of the format. Furthermore, these applications perform ad-hoc processing of the file-system metadata, leading to bugs and vulnerabilities. We propose Spiffy, an annotation language for specifying the on-disk format of a file system. File-system developers annotate the data structures of a file system, and we use these annotations to generate a library that allows identifying, parsing, and traversing file-system metadata, providing support for both offline and online storage applications. This approach simplifies the development of storage applications that work across different file systems because it reduces the amount of file-system--specific code that needs to be written. We have written annotations for the Linux Ext4, Btrfs, and F2FS file systems, and developed several applications for these file systems, including a type-specific metadata corruptor, a file-system converter, an online storage layer cache that preferentially caches files for certain users, and a runtime file-system checker. Our experiments show that applications built with the Spiffy library for accessing file-system metadata can achieve good performance and are robust against file-system corruption errors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3042208251",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on SOSP 2019",
    "doi": "https://doi.org/10.1145/3395778",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3089433319",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on Computational Storage",
    "doi": "https://doi.org/10.1145/3425305",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "Jin-Soo Kim; Yang Seok Ki; Erik Riedel",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3106137244",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX OSDI 2021",
    "doi": "https://doi.org/10.1145/3507950",
    "publication_date": "2022-01-29",
    "publication_year": 2022,
    "authors": "Angela Demke Brown; Jay Lorch",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4210392805",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX ATC 2021",
    "doi": "https://doi.org/10.1145/3519550",
    "publication_date": "2022-04-17",
    "publication_year": 2022,
    "authors": "Irina Calciu; Geoff Kuenning",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4226231915",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on SOSP 2021",
    "doi": "https://doi.org/10.1145/3542850",
    "publication_date": "2022-08-31",
    "publication_year": 2022,
    "authors": "Sam H. Noh",
    "corresponding_authors": "Sam H. Noh",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4294238418",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX FAST 2022",
    "doi": "https://doi.org/10.1145/3564770",
    "publication_date": "2022-11-11",
    "publication_year": 2022,
    "authors": "Hildebrand Dean; Donald E. Porter",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4308915721",
    "type": "article"
  },
  {
    "title": "Improving the Endurance of Next Generation SSD’s using WOM-v Codes",
    "doi": "https://doi.org/10.1145/3565027",
    "publication_date": "2022-11-14",
    "publication_year": 2022,
    "authors": "Shehbaz Jaffer; Kaveh Mahdaviani; Bianca Schroeder",
    "corresponding_authors": "",
    "abstract": "High density Solid State Drives, such as QLC drives, offer increased storage capacity, but a magnitude lower Program and Erase (P/E) cycles, limiting their endurance and hence usability. We present the design and implementation of non-binary, Voltage-Based Write-Once-Memory (WOM-v) Codes to improve the lifetime of QLC drives. First, we develop a FEMU based simulator test-bed to evaluate the gains of WOM-v codes on real world workloads. Second, we propose and implement two optimizations, an efficient garbage collection mechanism and an encoding optimization to drastically improve WOM-v code endurance without compromising performance. Third, we propose analytical approaches to obtain estimates of the endurance gains under WOM-v codes. We analyze the Greedy garbage collection technique with uniform page access distribution and the Least Recently Written (LRW) garbage collection technique with skewed page access distribution in the context of WOM-v codes. We find that although both approaches overestimate the number of required erase operations, the model based on greedy garbage collection with uniform page access distribution provides tighter bounds. A careful evaluation, including microbenchmarks and trace-driven evaluation, demonstrates that WOM-v codes can reduce Erase cycles for QLC drives by 4.4×–11.1× for real world workloads with minimal performance overheads resulting in improved QLC SSD lifetime.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4309000239",
    "type": "article"
  },
  {
    "title": "Extending and Programming the NVMe I/O Determinism Interface for Flash Arrays",
    "doi": "https://doi.org/10.1145/3568427",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Huaicheng Li; Martin L. Putra; Ronald Shi; Fadhil I. Kurnia; Xing Lin; Jaeyoung Do; Achmad Imam Kistijantoro; Gregory R. Ganger; Haryadi S. Gunawi",
    "corresponding_authors": "",
    "abstract": "Predictable latency on flash storage is a long-pursuit goal, yet unpredictability stays due to the unavoidable disturbance from many well-known SSD internal activities. To combat this issue, the recent NVMe IO Determinism (IOD) interface advocates host-level controls to SSD internal management tasks. Although promising, challenges remain on how to exploit it for truly predictable performance. We present IODA , 1 an I/O deterministic flash array design built on top of small but powerful extensions to the IOD interface for easy deployment. IODA exploits data redundancy in the context of IOD for a strong latency predictability contract. In IODA , SSDs are expected to quickly fail an I/O on purpose to allow predictable I/Os through proactive data reconstruction. In the case of concurrent internal operations, IODA introduces busy remaining time exposure and predictable-latency-window formulation to guarantee predictable data reconstructions. Overall, IODA only adds five new fields to the NVMe interface and a small modification in the flash firmware while keeping most of the complexity in the host OS. Our evaluation shows that IODA improves the 95–99.99 th latencies by up to 75×. IODA is also the nearest to the ideal, no disturbance case compared to seven state-of-the-art preemption, suspension, GC coordination, partitioning, tiny-tail flash controller, prediction, and proactive approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4309561686",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX FAST 2020",
    "doi": "https://doi.org/10.1145/3442685",
    "publication_date": "2021-01-29",
    "publication_year": 2021,
    "authors": "Sam H. Noh; Brent Welch",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3127418780",
    "type": "article"
  },
  {
    "title": "Thanking the TOS Associated Editors and Reviewers",
    "doi": "https://doi.org/10.1145/3442683",
    "publication_date": "2021-01-29",
    "publication_year": 2021,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3133992299",
    "type": "article"
  },
  {
    "title": "Lightweight Dynamic Redundancy Control with Adaptive Encoding for Server-based Storage",
    "doi": "https://doi.org/10.1145/3456292",
    "publication_date": "2021-10-15",
    "publication_year": 2021,
    "authors": "Takayuki Fukatani; Hieu Le; Haruo Yokota",
    "corresponding_authors": "",
    "abstract": "With the recent performance improvements in commodity hardware, low-cost commodity server-based storage has become a practical alternative to dedicated-storage appliances. Because of the high failure rate of commodity servers, data redundancy across multiple servers is required in a server-based storage system. However, the extra storage capacity for this redundancy significantly increases the system cost. Although erasure coding (EC) is a promising method to reduce the amount of redundant data, it requires distributing and encoding data among servers. There remains a need to reduce the performance impact of these processes involving much network traffic and processing overhead. Especially, the performance impact becomes significant for random-intensive applications. In this article, we propose a new lightweight redundancy control for server-based storage. Our proposed method uses a new local filesystem-based approach that avoids distributing data by adding data redundancy to locally stored user data. Our method switches the redundancy method of user data between replication and EC according to workloads to improve capacity efficiency while achieving higher performance. Our experiments show up to 230% better online-transaction-processing performance for our method compared with CephFS, a widely used alternative system. We also confirmed that our proposed method prevents unexpected performance degradation while achieving better capacity efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3179353966",
    "type": "article"
  },
  {
    "title": "Toward Virtual Machine Image Management for Persistent Memory",
    "doi": "https://doi.org/10.1145/3450976",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Jiachen Zhang; Lixiao Cui; Peng Li; Xiaoguang Liu; Gang Wang",
    "corresponding_authors": "",
    "abstract": "Persistent memory’s (PM) byte-addressability and high capacity will also make it emerging for virtualized environment. Modern virtual machine monitors virtualize PM using either I/O virtualization or memory virtualization. However, I/O virtualization will sacrifice PM’s byte-addressability, and memory virtualization does not get the chance of PM image management. In this article, we enhance QEMU’s memory virtualization mechanism. The enhanced system can achieve both PM’s byte-addressability inside virtual machines and PM image management outside the virtual machines. We also design pcow , a virtual machine image format for PM, which is compatible with our enhanced memory virtualization and supports storage virtualization features including thin-provisioning, base image, snapshot, and striping. Address translation is performed with the help of the Extended Page Table, thus much faster than image formats implemented in I/O virtualization. We also optimize pcow considering PM’s characteristics. We perform exhaustive performance evaluations on an x86 server equipping with Intel’s Optane DC persistent memory. The evaluation demonstrates that our scheme boosts the overall performance by up to 50× compared with qcow2, an image format implemented in I/O virtualization, and brings almost no performance overhead compared with the native memory virtualization. The striping feature can also scale-out the virtual PM’s bandwidth performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3193581658",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX OSDI 2020",
    "doi": "https://doi.org/10.1145/3479434",
    "publication_date": "2021-08-31",
    "publication_year": 2021,
    "authors": "Shan Lu; J T Howell",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3196823315",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on USENIX FAST 2021",
    "doi": "https://doi.org/10.1145/3485449",
    "publication_date": "2021-10-15",
    "publication_year": 2021,
    "authors": "Marcos K. Aguilera; Gala Yadgar",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3206106223",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on USENIX ATC 2020",
    "doi": "https://doi.org/10.1145/3457170",
    "publication_date": "2021-05-28",
    "publication_year": 2021,
    "authors": "Ada Gavrilovska; Erez Zadok",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249904531",
    "type": "article"
  }
]