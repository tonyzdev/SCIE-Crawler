[
  {
    "title": "Bigtable",
    "doi": "https://doi.org/10.1145/1365815.1365816",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Fay W. Chang; Jay B. Dean; Sanjay Ghemawat; Wilson C. Hsieh; Deborah A. Wallach; Mike Burrows; Tushar Chandra; Andrew Fikes; Robert Gruber",
    "corresponding_authors": "",
    "abstract": "Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.",
    "cited_by_count": 3421,
    "openalex_id": "https://openalex.org/W1981420413",
    "type": "article"
  },
  {
    "title": "The part-time parliament",
    "doi": "https://doi.org/10.1145/279227.279229",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliament's protocol provides a new way of implementing the state machine approach to the design of distributed systems.",
    "cited_by_count": 2689,
    "openalex_id": "https://openalex.org/W2075854425",
    "type": "article"
  },
  {
    "title": "A logic of authentication",
    "doi": "https://doi.org/10.1145/77648.77649",
    "publication_date": "1990-02-01",
    "publication_year": 1990,
    "authors": "Michael T. Burrows; Martı́n Abadi; Roger M. Needham",
    "corresponding_authors": "",
    "abstract": "Authentication protocols are the basis of security in many distributed systems, and it is therefore essential to ensure that these protocols function correctly. Unfortunately, their design has been extremely error prone. Most of the protocols found in the literature contain redundancies or security flaws. A simple logic has allowed us to describe the beliefs of trustworthy parties involved in authentication protocols and the evolution of these beliefs as a consequence of communication. We have been able to explain a variety of authentication protocols formally, to discover subtleties and errors in them, and to suggest improvements. In this paper we present the logic and then give the results of our analysis of four published protocols, chosen either because of their practical importance or because they serve to illustrate our method.",
    "cited_by_count": 2484,
    "openalex_id": "https://openalex.org/W2010939995",
    "type": "article"
  },
  {
    "title": "Distributed snapshots",
    "doi": "https://doi.org/10.1145/214451.214456",
    "publication_date": "1985-02-01",
    "publication_year": 1985,
    "authors": "K. Mani Chandy; Leslie Lamport",
    "corresponding_authors": "",
    "abstract": "This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,” “ the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing.",
    "cited_by_count": 2469,
    "openalex_id": "https://openalex.org/W2131053137",
    "type": "article"
  },
  {
    "title": "The click modular router",
    "doi": "https://doi.org/10.1145/354871.354874",
    "publication_date": "2000-08-01",
    "publication_year": 2000,
    "authors": "Eddie Kohler; Robert Morris; Benjie Chen; John Jannotti; M. Frans Kaashoek",
    "corresponding_authors": "",
    "abstract": "Clicks is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements . Individual elements implement simple router functions like packet classification, queuing, scheduling, and interfacing with network devices. A router configurable is a directed graph with elements at the vertices; packets flow along the edges of the graph. Several features make individual elements more powerful and complex configurations easier to write, including pull connections, which model packet flow drivn by transmitting hardware devices, and flow-based router context, which helps an element locate other interesting elements. Click configurations are modular and easy to extend. A standards-compliant Click IP router has 16 elements on its forwarding path; some of its elements are also useful in Ethernet switches and IP tunnelling configurations. Extending the IP router to support dropping policies, fairness among flows, or Differentiated Services simply requires adding a couple of element at the right place. On conventional PC hardware, the Click IP router achieves a maximum loss-free forwarding rate of 333,000 64-byte packets per second, demonstrating that Click's modular and flexible architecture is compatible with good performance.",
    "cited_by_count": 2444,
    "openalex_id": "https://openalex.org/W2010365467",
    "type": "article"
  },
  {
    "title": "Practical byzantine fault tolerance and proactive recovery",
    "doi": "https://doi.org/10.1145/571637.571640",
    "publication_date": "2002-10-07",
    "publication_year": 2002,
    "authors": "Miguel Castro; Barbara Liskov",
    "corresponding_authors": "",
    "abstract": "Our growing reliance on online services accessible on the Internet demands highly available systems that provide correct service without interruptions. Software bugs, operator mistakes, and malicious attacks are a major cause of service interruptions and they can cause arbitrary behavior, that is, Byzantine faults. This article describes a new replication algorithm, BFT, that can be used to build highly available systems that tolerate Byzantine faults. BFT can be used in practice to implement real services: it performs well, it is safe in asynchronous environments such as the Internet, it incorporates mechanisms to defend against Byzantine-faulty clients, and it recovers replicas proactively. The recovery mechanism allows the algorithm to tolerate any number of faults over the lifetime of the system provided fewer than 1/3 of the replicas become faulty within a small window of vulnerability. BFT has been implemented as a generic program library with a simple interface. We used the library to implement the first Byzantine-fault-tolerant NFS file system, BFS. The BFT library and BFS perform well because the library incorporates several important optimizations, the most important of which is the use of symmetric cryptography to authenticate messages. The performance results show that BFS performs 2% faster to 24% slower than production implementations of the NFS protocol that are not replicated. This supports our claim that the BFT library can be used to build practical systems that tolerate Byzantine faults.",
    "cited_by_count": 2368,
    "openalex_id": "https://openalex.org/W2114579022",
    "type": "article"
  },
  {
    "title": "End-to-end arguments in system design",
    "doi": "https://doi.org/10.1145/357401.357402",
    "publication_date": "1984-11-01",
    "publication_year": 1984,
    "authors": "Jerome H. Saltzer; David P. Reed; David D. Clark",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on End-to-end arguments in system design Authors: J. H. Saltzer M.I.T. Laboratory for Computer Science, 545 Technology Square, Cambridge, MA M.I.T. Laboratory for Computer Science, 545 Technology Square, Cambridge, MAView Profile , D. P. Reed Software Arts, Inc., 27 Mica Lane, Wellesley, MA Software Arts, Inc., 27 Mica Lane, Wellesley, MAView Profile , D. D. Clark M.I.T. Laboratory for Computer Science, 545 Technology Square, Cambridge, MA M.I.T. Laboratory for Computer Science, 545 Technology Square, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 4Nov. 1984 pp 277–288https://doi.org/10.1145/357401.357402Online:01 November 1984Publication History 1,259citation21,347DownloadsMetricsTotal Citations1,259Total Downloads21,347Last 12 Months1,460Last 6 weeks358 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2158,
    "openalex_id": "https://openalex.org/W2018501701",
    "type": "article"
  },
  {
    "title": "Implementing remote procedure calls",
    "doi": "https://doi.org/10.1145/2080.357392",
    "publication_date": "1984-02-01",
    "publication_year": 1984,
    "authors": "Andrew Birrell; Bruce Jay Nelson",
    "corresponding_authors": "",
    "abstract": "article Free AccessImplementing remote procedure calls Authors: Andrew D. Birrell Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CAView Profile , Bruce Jay Nelson Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 1February 1984 pp 39–59https://doi.org/10.1145/2080.357392Published:01 February 1984Publication History 1,242citation16,381DownloadsMetricsTotal Citations1,242Total Downloads16,381Last 12 Months1,177Last 6 weeks183 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 1869,
    "openalex_id": "https://openalex.org/W2106035061",
    "type": "article"
  },
  {
    "title": "Eraser",
    "doi": "https://doi.org/10.1145/265924.265927",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Stefan Savage; Michael T. Burrows; Greg Nelson; Patrick Sobalvarro; Thomas E. Anderson",
    "corresponding_authors": "",
    "abstract": "Multithreaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This article describes a new tool, called Eraser, for dynamically detecting data races in lock-based multithreaded programs. Eraser uses binary rewriting techniques to monitor every shared-monory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multithreaded Web search engine, that demonstrate the effectiveness of this approach.",
    "cited_by_count": 1589,
    "openalex_id": "https://openalex.org/W1972544179",
    "type": "article"
  },
  {
    "title": "Design and evaluation of a wide-area event notification service",
    "doi": "https://doi.org/10.1145/380749.380767",
    "publication_date": "2001-08-01",
    "publication_year": 2001,
    "authors": "Antonio Carzaniga; David S. Rosenblum; Alexander L. Wolf",
    "corresponding_authors": "",
    "abstract": "The components of a loosely coupled system are typically designed to operate by generating and responding to asynchronous events. An event notification service is an application-independent infrastructure that supports the construction of event-based systems, whereby generators of events publish event notifications to the infrastructure and consumers of events subscribe with the infrastructure to receive relevant notifications. The two primary services that should be provided to components by the infrastructure are notification selection (i. e., determining which notifications match which subscriptions) and notification delivery (i.e., routing matching notifications from publishers to subscribers). Numerous event notification services have been developed for local-area networks, generally based on a centralized server to select and deliver event notifications. Therefore, they suffer from an inherent inability to scale to wide-area networks, such as the Internet, where the number and physical distribution of the service's clients can quickly overwhelm a centralized solution. The critical challenge in the setting of a wide-area network is to maximize the expressiveness in the selection mechanism without sacrificing scalability in the delivery mechanism. This paper presents SIENA, an event notification service that we have designed and implemented to exhibit both expressiveness and scalability. We describe the service's interface to applications, the algorithms used by networks of servers to select and deliver event notifications, and the strategies used to optimize performance. We also present results of simulation studies that examine the scalability and performance of the service.",
    "cited_by_count": 1454,
    "openalex_id": "https://openalex.org/W2131185826",
    "type": "article"
  },
  {
    "title": "Scale and performance in a distributed file system",
    "doi": "https://doi.org/10.1145/35037.35059",
    "publication_date": "1988-02-01",
    "publication_year": 1988,
    "authors": "John H. Howard; Michael L. Kazar; Sherri G. Menees; David A. Nichols; Mahadev Satyanarayanan; Robert N. Sidebotham; Michael J. West",
    "corresponding_authors": "",
    "abstract": "The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrews ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystems NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.",
    "cited_by_count": 1434,
    "openalex_id": "https://openalex.org/W2005373714",
    "type": "article"
  },
  {
    "title": "A class of generalized stochastic Petri nets for the performance evaluation of multiprocessor systems",
    "doi": "https://doi.org/10.1145/190.191",
    "publication_date": "1984-05-01",
    "publication_year": 1984,
    "authors": "Marco Ajmone Marsan; G. Conte; Gianfranco Balbo",
    "corresponding_authors": "",
    "abstract": "article A class of generalized stochastic Petri nets for the performance evaluation of multiprocessor systems Share on Authors: Marco Ajmone Marsan Politecnico di Torino, Turin, Italy Politecnico di Torino, Turin, ItalyView Profile , Gianni Conte Politecnico di Torino, Turin, Italy Politecnico di Torino, Turin, ItalyView Profile , Gianfranco Balbo Univ. di Torino, Turin, Italy Univ. di Torino, Turin, ItalyView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 2May 1984 pp 93–122https://doi.org/10.1145/190.191Online:01 May 1984Publication History 969citation1,939DownloadsMetricsTotal Citations969Total Downloads1,939Last 12 Months50Last 6 weeks9 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1341,
    "openalex_id": "https://openalex.org/W2079636926",
    "type": "article"
  },
  {
    "title": "Algorithms for scalable synchronization on shared-memory multiprocessors",
    "doi": "https://doi.org/10.1145/103727.103729",
    "publication_date": "1991-02-01",
    "publication_year": 1991,
    "authors": "John Mellor‐Crummey; Michael L. Scott",
    "corresponding_authors": "",
    "abstract": "Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible flag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory. We present a new scalable algorithm for spin locks that generates 0(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than a swap-with-memory instruction. We also present a new scalable barrier algorithm that generates 0(1) remote references per processor reaching the barrier, and observe that two previously-known barriers can likewise be cast in a form that spins only on locally-accessible flag variables. None of these barrier algorithms requires hardware support beyond the usual atomicity of memory reads and writes. We compare the performance of our scalable algorithms with other software approaches to busy-wait synchronization on both a Sequent Symmetry and a BBN Butterfly. Our principal conclusion is that contention due to synchronization need not be a problem in large-scale shared-memory multiprocessors. The existence of scalable algorithms greatly weakens the case for costly special-purpose hardware support for synchronization, and provides a case against so-called “dance hall” architectures, in which shared memory locations are equally far from all processors. — From the Authors' Abstract",
    "cited_by_count": 1304,
    "openalex_id": "https://openalex.org/W2001738739",
    "type": "article"
  },
  {
    "title": "The design and implementation of a log-structured file system",
    "doi": "https://doi.org/10.1145/146941.146943",
    "publication_date": "1992-02-01",
    "publication_year": 1992,
    "authors": "Mendel Rosenblum; John K. Ousterhout",
    "corresponding_authors": "",
    "abstract": "This paper presents a new technique for disk storage management called a log-structured file system . A log-structured file system writes all modifications to disk sequentially in a log-like structure, thereby speeding up both file writing and crash recovery. The log is the only structure on disk; it contains indexing information so that files can be read back from the log efficiently. In order to maintain large free areas on disk for fast writing, we divide the log into segments and use a segment cleaner to compress the live information from heavily fragmented segments. We present a series of simulations that demonstrate the efficiency of a simple cleaning policy based on cost and benefit. We have implemented a prototype log-structured file system called Sprite LFS; it outperforms current Unix file systems by an order of magnitude for small-file writes while matching or exceeding Unix performance for reads and large writes. Even when the overhead for cleaning is included, Sprite LFS can use 70% of the disk bandwidth for writing, whereas Unix file systems typically can use only 5–10%.",
    "cited_by_count": 1232,
    "openalex_id": "https://openalex.org/W2108183412",
    "type": "article"
  },
  {
    "title": "Multicast routing in datagram internetworks and extended LANs",
    "doi": "https://doi.org/10.1145/78952.78953",
    "publication_date": "1990-05-01",
    "publication_year": 1990,
    "authors": "S. Deering; David R. Cheriton",
    "corresponding_authors": "",
    "abstract": "Multicasting, the transmission of a packet to a group of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting internetwork . To address this limitation, we specify extensions to two common internetwork routing algorithms—distance-vector routing and link-state routing—to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.",
    "cited_by_count": 1197,
    "openalex_id": "https://openalex.org/W2079994203",
    "type": "article"
  },
  {
    "title": "Memory coherence in shared virtual memory systems",
    "doi": "https://doi.org/10.1145/75104.75105",
    "publication_date": "1989-11-01",
    "publication_year": 1989,
    "authors": "Kai Li; Paul Hudak",
    "corresponding_authors": "",
    "abstract": "The memory coherence problem in designing and implementing a shared virtual memory on loosely coupled multiprocessors is studied in depth. Two classes of algorithms, centralized and distributed, for solving the problem are presented. A prototype shared virtual memory on an Apollo ring based on these algorithms has been implemented. Both theoretical and practical results show that the memory coherence problem can indeed be solved efficiently on a loosely coupled multiprocessor.",
    "cited_by_count": 1178,
    "openalex_id": "https://openalex.org/W2044902313",
    "type": "article"
  },
  {
    "title": "TaintDroid",
    "doi": "https://doi.org/10.1145/2619091",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "William Enck; Peter Gilbert; Seungyeop Han; Vasant Tendulkar; Byung-Gon Chun; Landon P. Cox; Jaeyeon Jung; Patrick McDaniel; Anmol Sheth",
    "corresponding_authors": "",
    "abstract": "Today’s smartphone operating systems frequently fail to provide users with visibility into how third-party applications collect and share their private data. We address these shortcomings with TaintDroid, an efficient, system-wide dynamic taint tracking and analysis system capable of simultaneously tracking multiple sources of sensitive data. TaintDroid enables realtime analysis by leveraging Android’s virtualized execution environment. TaintDroid incurs only 32% performance overhead on a CPU-bound microbenchmark and imposes negligible overhead on interactive third-party applications. Using TaintDroid to monitor the behavior of 30 popular third-party Android applications, in our 2010 study we found 20 applications potentially misused users’ private information; so did a similar fraction of the tested applications in our 2012 study. Monitoring the flow of privacy-sensitive data with TaintDroid provides valuable input for smartphone users and security service firms seeking to identify misbehaving applications.",
    "cited_by_count": 1105,
    "openalex_id": "https://openalex.org/W2060692877",
    "type": "article"
  },
  {
    "title": "Disconnected operation in the Coda File System",
    "doi": "https://doi.org/10.1145/146941.146942",
    "publication_date": "1992-02-01",
    "publication_year": 1992,
    "authors": "James J. Kistler; Mahadev Satyanarayanan",
    "corresponding_authors": "",
    "abstract": "Disconnected operation is a mode of operation that enables a client to continue accessing critical data during temporary failures of a shared data repository. An important, though not exclusive, application of disconnected operation is in supporting portable computers. In this paper, we show that disconnected operation is feasible, efficient and usable by describing its design and implementation in the Coda File System. The central idea behind our work is that caching of data , now widely used for performance, can also be exploited to improve availability.",
    "cited_by_count": 1093,
    "openalex_id": "https://openalex.org/W2124074197",
    "type": "article"
  },
  {
    "title": "Reliable communication in the presence of failures",
    "doi": "https://doi.org/10.1145/7351.7478",
    "publication_date": "1987-01-05",
    "publication_year": 1987,
    "authors": "Kenneth P. Birman; Thomas Joseph",
    "corresponding_authors": "",
    "abstract": "The design and correctness of a communication facility for a distributed computer system are reported on. The facility provides support for fault-tolerant process groups in the form of a family of reliable multicast protocols that can be used in both local- and wide-area networks. These protocols attain high levels of concurrency, while respecting application-specific delivery ordering constraints, and have varying cost and performance that depend on the degree of ordering desired. In particular, a protocol that enforces causal delivery orderings is introduced and shown to be a valuable alternative to conventional asynchronous communication protocols. The facility also ensures that the processes belonging to a fault-tolerant process group will observe consistent orderings of events affecting the group as a whole, including process failures, recoveries, migration, and dynamic changes to group properties like member rankings. A review of several uses for the protocols in the ISIS system, which supports fault-tolerant resilient objects and bulletin boards, illustrates the significant simplification of higher level algorithms made possible by our approach.",
    "cited_by_count": 1010,
    "openalex_id": "https://openalex.org/W2118153508",
    "type": "article"
  },
  {
    "title": "Lightweight causal and atomic group multicast",
    "doi": "https://doi.org/10.1145/128738.128742",
    "publication_date": "1991-08-01",
    "publication_year": 1991,
    "authors": "Kenneth P. Birman; André Schiper; Pat Stephenson",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Lightweight causal and atomic group multicast Authors: Kenneth Birman Cornell University Cornell UniversityView Profile , André Schiper Ecole Polytechnique Federal de Lausanne, Switzerland Ecole Polytechnique Federal de Lausanne, SwitzerlandView Profile , Pat Stephenson Cornell University Cornell UniversityView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 3Aug. 1991 pp 272–314https://doi.org/10.1145/128738.128742Published:01 August 1991Publication History 709citation2,727DownloadsMetricsTotal Citations709Total Downloads2,727Last 12 Months115Last 6 weeks25 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 969,
    "openalex_id": "https://openalex.org/W1985349143",
    "type": "article"
  },
  {
    "title": "High-speed switch scheduling for local-area networks",
    "doi": "https://doi.org/10.1145/161541.161736",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Thomas E. Anderson; Susan Owicki; James B. Saxe; Charles P. Thacker",
    "corresponding_authors": "",
    "abstract": "Current technology trends make it possible to build communication networks that can support high-performance distributed computing. This paper describes issues in the design of a prototype switch for an arbitrary topology point-to-point network with link speeds of up to 1 Gbit/s. The switch deals in fixed-length ATM-style cells, which it can process at a rate of 37 million cells per second. It provides high bandwidth and low latency for datagram traffic. In addition, it supports real-time traffic by providing bandwidth reservations with guaranteed latency bounds. The key to the switch's operation is a technique called parallel iterative matching , which can quickly identify a set of conflict-free cells for transmission in a time slot. Bandwidth reservations are accommodated in the switch by building a fixed schedule for transporting cells from reserved flows across the switch; parallel iterative matching can fill unused slots with datagram traffic. Finally, we note that parallel iterative matching may not allocate bandwidth fairly among flows of datagram traffic. We describe a technique called statistical matching , which can be used to ensure fairness at the switch and to support applications with rapidly changing needs for guaranteed bandwidth.",
    "cited_by_count": 888,
    "openalex_id": "https://openalex.org/W2126169247",
    "type": "article"
  },
  {
    "title": "Inferring Internet denial-of-service activity",
    "doi": "https://doi.org/10.1145/1132026.1132027",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "David Moore; Colleen Shannon; Douglas J. Brown; Geoffrey M. Voelker; Stefan Savage",
    "corresponding_authors": "",
    "abstract": "In this article, we seek to address a simple question: “How prevalent are denial-of-service attacks in the Internet?” Our motivation is to quantitatively understand the nature of the current threat as well as to enable longer-term analyses of trends and recurring patterns of attacks. We present a new technique, called “backscatter analysis,” that provides a conservative estimate of worldwide denial-of-service activity. We use this approach on 22 traces (each covering a week or more) gathered over three years from 2001 through 2004. Across this corpus we quantitatively assess the number, duration, and focus of attacks, and qualitatively characterize their behavior. In total, we observed over 68,000 attacks directed at over 34,000 distinct victim IP addresses---ranging from well-known e-commerce companies such as Amazon and Hotmail to small foreign ISPs and dial-up connections. We believe our technique is the first to provide quantitative estimates of Internet-wide denial-of-service activity and that this article describes the most comprehensive public measurements of such activity to date.",
    "cited_by_count": 794,
    "openalex_id": "https://openalex.org/W2081357650",
    "type": "article"
  },
  {
    "title": "Fail-stop processors",
    "doi": "https://doi.org/10.1145/357369.357371",
    "publication_date": "1983-08-01",
    "publication_year": 1983,
    "authors": "Richard D. Schlichting; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Fail-stop processors: an approach to designing fault-tolerant computing systems Authors: Richard D. Schlichting Department of Computer Science, University of Arizona, Tucson, Arizona Department of Computer Science, University of Arizona, Tucson, ArizonaView Profile , Fred B. Schneider Department of Computer Science, Cornell University, Ithaca, New York Department of Computer Science, Cornell University, Ithaca, New YorkView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 3August 1983 pp 222–238https://doi.org/10.1145/357369.357371Published:01 August 1983Publication History 568citation1,822DownloadsMetricsTotal Citations568Total Downloads1,822Last 12 Months145Last 6 weeks22 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 777,
    "openalex_id": "https://openalex.org/W2081409107",
    "type": "article"
  },
  {
    "title": "A fast file system for UNIX",
    "doi": "https://doi.org/10.1145/989.990",
    "publication_date": "1984-08-01",
    "publication_year": 1984,
    "authors": "Marshall Kirk McKusick; William N. Joy; Samuel J. Leffler; R. S. Fabry",
    "corresponding_authors": "",
    "abstract": "article Free AccessA fast file system for UNIX Authors: Marshall K. McKusick Computer Systems Research Group Computer Systems Research GroupView Profile , William N. Joy Computer Systems Research Group Computer Systems Research GroupView Profile , Samuel J. Leffler Computer Systems Research Group Computer Systems Research GroupView Profile , Robert S. Fabry Computer Systems Research Group Computer Systems Research GroupView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 3Aug. 1984 pp 181–197https://doi.org/10.1145/989.990Published:01 August 1984Publication History 511citation11,970DownloadsMetricsTotal Citations511Total Downloads11,970Last 12 Months920Last 6 weeks130 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 768,
    "openalex_id": "https://openalex.org/W2114167330",
    "type": "article"
  },
  {
    "title": "Fine-grained mobility in the Emerald system",
    "doi": "https://doi.org/10.1145/35037.42182",
    "publication_date": "1988-02-01",
    "publication_year": 1988,
    "authors": "Eric Jul; Henry M. Levy; Norman C. Hutchinson; Andrew P. Black",
    "corresponding_authors": "",
    "abstract": "Emerald is an object-based language and system designed for the construction of distributed programs. An explicit goal of Emerald is support for object mobility; objects in Emerald can freely move within the system to take advantage of distribution and dynamically changing environments. We say that Emerald has fine-grained mobility because Emerald objects can be small data objects as well as process objects. Fine-grained mobility allows us to apply mobility in new ways but presents implementation problems as well. This paper discusses the benefits of tine-grained mobility, the Emerald language and run-time mechanisms that support mobility, and techniques for implementing mobility that do not degrade the performance of local operations. Performance measurements of the current implementation are included.",
    "cited_by_count": 744,
    "openalex_id": "https://openalex.org/W2121134342",
    "type": "article"
  },
  {
    "title": "A √N algorithm for mutual exclusion in decentralized systems",
    "doi": "https://doi.org/10.1145/214438.214445",
    "publication_date": "1985-05-01",
    "publication_year": 1985,
    "authors": "Mamoru Maekawa",
    "corresponding_authors": "Mamoru Maekawa",
    "abstract": "An algorithm is presented that uses only c√N messages to create mutual exclusion in a computer network, where N is the number of nodes and c a constant between 3 and 5. The algorithm is symmetric and allows fully parallel operation.",
    "cited_by_count": 743,
    "openalex_id": "https://openalex.org/W2100198355",
    "type": "article"
  },
  {
    "title": "Optimistic recovery in distributed systems",
    "doi": "https://doi.org/10.1145/3959.3962",
    "publication_date": "1985-08-01",
    "publication_year": 1985,
    "authors": "Rob Strom; Shaula Yemini",
    "corresponding_authors": "",
    "abstract": "Optimistic Recovery is a new technique supporting application-independent transparent recovery from processor failures in distributed systems. In optimistic recovery communication, computation and checkpointing proceed asynchronously. Synchronization is replaced by causal dependency tracking , which enables a posteriori reconstruction of a consistent distributed system state following a failure using process rollback and message replay . Because there is no synchronization among computation, communication, and checkpointing, optimistic recovery can tolerate the failure of an arbitrary number of processors and yields better throughput and response time than other general recovery techniques whenever failures are infrequent.",
    "cited_by_count": 736,
    "openalex_id": "https://openalex.org/W2052915895",
    "type": "article"
  },
  {
    "title": "Gossip-based aggregation in large dynamic networks",
    "doi": "https://doi.org/10.1145/1082469.1082470",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Márk Jelasity; Alberto Montresor; Özalp Babaoğlu",
    "corresponding_authors": "",
    "abstract": "As computer networks increase in size, become more heterogeneous and span greater geographic distances, applications must be designed to cope with the very large scale, poor reliability, and often, with the extreme dynamism of the underlying network. Aggregation is a key functional building block for such applications: it refers to a set of functions that provide components of a distributed system access to global information including network size, average load, average uptime, location and description of hotspots, and so on. Local access to global information is often very useful, if not indispensable for building applications that are robust and adaptive. For example, in an industrial control application, some aggregate value reaching a threshold may trigger the execution of certain actions; a distributed storage system will want to know the total available free space; load-balancing protocols may benefit from knowing the target average load so as to minimize the load they transfer. We propose a gossip-based protocol for computing aggregate values over network components in a fully decentralized fashion. The class of aggregate functions we can compute is very broad and includes many useful special cases such as counting, averages, sums, products, and extremal values. The protocol is suitable for extremely large and highly dynamic systems due to its proactive structure---all nodes receive the aggregate value continuously, thus being able to track any changes in the system. The protocol is also extremely lightweight, making it suitable for many distributed applications including peer-to-peer and grid computing systems. We demonstrate the efficiency and robustness of our gossip-based protocol both theoretically and experimentally under a variety of scenarios including node and communication failures.",
    "cited_by_count": 720,
    "openalex_id": "https://openalex.org/W2138830906",
    "type": "article"
  },
  {
    "title": "Reliable broadcast protocols",
    "doi": "https://doi.org/10.1145/989.357400",
    "publication_date": "1984-08-01",
    "publication_year": 1984,
    "authors": "Jo-Mei Chang; N.F. Maxemchuk",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Reliable broadcast protocols Authors: Jo-Mei Chang AT&T Bell Laboratories, Murray Hill, NJ AT&T Bell Laboratories, Murray Hill, NJView Profile , N. F. Maxemchuk AT&T Bell Laboratories, Murray Hill, NJ AT&T Bell Laboratories, Murray Hill, NJView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 3Aug. 1984 pp 251–273https://doi.org/10.1145/989.357400Published:01 August 1984Publication History 531citation2,637DownloadsMetricsTotal Citations531Total Downloads2,637Last 12 Months167Last 6 weeks49 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 682,
    "openalex_id": "https://openalex.org/W1963615255",
    "type": "article"
  },
  {
    "title": "Astrolabe",
    "doi": "https://doi.org/10.1145/762483.762485",
    "publication_date": "2003-05-01",
    "publication_year": 2003,
    "authors": "Robbert van Renesse; Kenneth P. Birman; Werner Vogels",
    "corresponding_authors": "",
    "abstract": "Scalable management and self-organizational capabilities are emerging as central requirements for a generation of large-scale, highly dynamic, distributed applications. We have developed an entirely new distributed information management system called Astrolabe. Astrolabe collects large-scale system state, permitting rapid updates and providing on-the-fly attribute aggregation. This latter capability permits an application to locate a resource, and also offers a scalable way to track system state as it evolves over time. The combination of features makes it possible to solve a wide variety of management and self-configuration problems. This paper describes the design of the system with a focus upon its scalability. After describing the Astrolabe service, we present examples of the use of Astrolabe for locating resources, publish-subscribe, and distributed synchronization in large systems. Astrolabe is implemented using a peer-to-peer protocol, and uses a restricted form of mobile code based on the SQL query language for aggregation. This protocol gives rise to a novel consistency model. Astrolabe addresses several security considerations using a built-in PKI. The scalability of the system is evaluated using both simulation and experiments; these confirm that Astrolabe could scale to thousands and perhaps millions of nodes, with information propagation delays in the tens of seconds.",
    "cited_by_count": 666,
    "openalex_id": "https://openalex.org/W1997369208",
    "type": "article"
  },
  {
    "title": "Bimodal multicast",
    "doi": "https://doi.org/10.1145/312203.312207",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Kenneth P. Birman; Mark Hayden; Öznur Özkasap; Zhen Xiao; Mihai Budiu; Yaron Minsky",
    "corresponding_authors": "",
    "abstract": "There are many methods for making a multicast protocol “reliable.” At one end of the spectrum, a reliable multicast protocol might offer tomicity guarantees, such as all-or-nothing delivery, delivery ordering, and perhaps additional properties such as virtually synchronous addressing. At the other are protocols that use local repair to overcome transient packet loss in the network, offering “best effort” reliability. Yet none of this prior work has treated stability of multicast delivery as a basic reliability property, such as might be needed in an internet radio, television, or conferencing application. This article looks at reliability with a new goal: development of a multicast protocol which is reliable in a sense that can be rigorously quantified and includes throughput stability guarantees. We characterize this new protocol as a “bimodal multicast” in reference to its reliability model, which corresponds to a family of bimodal probability distributions. Here, we introduce the protocol, provide a theoretical analysis of its behavior, review experimental results, and discuss some candidate applications. These confirm that bimodal multicast is reliable, scalable, and that the protocol provides remarkably stable delivery throughput.",
    "cited_by_count": 665,
    "openalex_id": "https://openalex.org/W2296427920",
    "type": "article"
  },
  {
    "title": "Spanner",
    "doi": "https://doi.org/10.1145/2491245",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "James C. Corbett; Jay B. Dean; Michael Epstein; Andrew Fikes; Christopher Frost; J. J. Furman; Sanjay Ghemawat; Andrey Gubarev; Christopher Heiser; Peter Hochschild; Wilson C. Hsieh; Sebastian Kanthak; Eugene Kogan; Hongyi Li; Alexander Lloyd; Sergey Melnik; David Mwaura; David F. Nagle; Sean Quinlan; Rajesh Rao; Lindsay Rolig; Yasushi Saitō; Michal Szymaniak; Christopher M. Taylor; Ruth Wang; Dale Woodford",
    "corresponding_authors": "",
    "abstract": "Spanner is Google’s scalable, multiversion, globally distributed, and synchronously replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This article describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free snapshot transactions, and atomic schema changes, across all of Spanner.",
    "cited_by_count": 636,
    "openalex_id": "https://openalex.org/W3137759927",
    "type": "article"
  },
  {
    "title": "Cache coherence protocols: evaluation using a multiprocessor simulation model",
    "doi": "https://doi.org/10.1145/6513.6514",
    "publication_date": "1986-09-01",
    "publication_year": 1986,
    "authors": "James Archibald; Jean-Loup Baer",
    "corresponding_authors": "",
    "abstract": "Using simulation, we examine the efficiency of several distributed, hardware-based solutions to the cache coherence problem in shared-bus multiprocessors. For each of the approaches, the associated protocol is outlined. The simulation model is described, and results from that model are presented. The magnitude of the potential performance difference between the various approaches indicates that the choice of coherence solution is very important in the design of an efficient shared-bus multiprocessor, since it may limit the number of processors in the system.",
    "cited_by_count": 584,
    "openalex_id": "https://openalex.org/W2106626405",
    "type": "article"
  },
  {
    "title": "Cryptographic solution to a problem of access control in a hierarchy",
    "doi": "https://doi.org/10.1145/357369.357372",
    "publication_date": "1983-08-01",
    "publication_year": 1983,
    "authors": "Selim G. Akl; Peter Taylor",
    "corresponding_authors": "",
    "abstract": "article Free AccessCryptographic solution to a problem of access control in a hierarchy Authors: Selim G. Akl Department of Computing and Information Science, Queen's University, Kingston, Ontario, Canada K7 3N6 Department of Computing and Information Science, Queen's University, Kingston, Ontario, Canada K7 3N6View Profile , Peter D. Taylor Department of Mathematics and Statistics, Queen's University, Kingston, Ontario, Canada K7 3N6 Department of Mathematics and Statistics, Queen's University, Kingston, Ontario, Canada K7 3N6View Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 3August 1983 pp 239–248https://doi.org/10.1145/357369.357372Published:01 August 1983Publication History 398citation2,278DownloadsMetricsTotal Citations398Total Downloads2,278Last 12 Months291Last 6 weeks35 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 559,
    "openalex_id": "https://openalex.org/W2031338211",
    "type": "article"
  },
  {
    "title": "New directions in traffic measurement and accounting",
    "doi": "https://doi.org/10.1145/859716.859719",
    "publication_date": "2003-08-01",
    "publication_year": 2003,
    "authors": "Cristian Estan; George Varghese",
    "corresponding_authors": "",
    "abstract": "Accurate network traffic measurement is required for accounting, bandwidth provisioning and detecting DoS attacks. These applications see the traffic as a collection of flows they need to measure. As link speeds and the number of flows increase, keeping a counter for each flow is too expensive (using SRAM) or slow (using DRAM). The current state-of-the-art methods (Cisco's sampled NetFlow), which count periodically sampled packets are slow, inaccurate and resource-intensive. Previous work showed that at different granularities a small number of \"heavy hitters\" accounts for a large share of traffic. Our paper introduces a paradigm shift by concentrating the measurement process on large flows only---those above some threshold such as 0.1% of the link capacity.We propose two novel and scalable algorithms for identifying the large flows: sample and hold and multistage filters , which take a constant number of memory references per packet and use a small amount of memory. If M is the available memory, we show analytically that the errors of our new algorithms are proportional to 1/ M ; by contrast, the error of an algorithm based on classical sampling is proportional to 1/√ M , thus providing much less accuracy for the same amount of memory. We also describe optimizations such as early removal and conservative update that further improve the accuracy of our algorithms, as measured on real traffic traces, by an order of magnitude. Our schemes allow a new form of accounting called threshold accounting in which only flows above a threshold are charged by usage while the rest are charged a fixed fee. Threshold accounting generalizes usage-based and duration based pricing.",
    "cited_by_count": 521,
    "openalex_id": "https://openalex.org/W2012549717",
    "type": "article"
  },
  {
    "title": "Caching in the Sprite network file system",
    "doi": "https://doi.org/10.1145/35037.42183",
    "publication_date": "1988-02-01",
    "publication_year": 1988,
    "authors": "Michael N. Nelson; Brent Welch; John K. Ousterhout",
    "corresponding_authors": "",
    "abstract": "The Sprite network operating system uses large main-memory disk block caches to achieve high performance in its file system. It provides non-write-through file caching on both client and server machines. A simple cache consistency mechanism permits files to be shared by multiple clients without danger of stale data. In order to allow the file cache to occupy as much memory as possible, the file system of each machine negotiates with the virtual memory system over physical memory usage and changes the size of the file cache dynamically. Benchmark programs indicate that client caches allow diskless Sprite workstations to perform within O-12 percent of workstations with disks. In addition, client caching reduces server loading by 50 percent and network traffic by 90 percent.",
    "cited_by_count": 520,
    "openalex_id": "https://openalex.org/W2107460938",
    "type": "article"
  },
  {
    "title": "VirtualClock",
    "doi": "https://doi.org/10.1145/103720.103721",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Lixia Zhang",
    "corresponding_authors": "Lixia Zhang",
    "abstract": "One of the challenging research issues in building high-speed packet-switched networks is how to control the transmission rate of statistical data flows. This paper describes a new traffic control algorithm, VirtualClock , for high-speed network applications. VirtualClock monitors the average transmission rate of statistical data flows and provides every flow with guaranteed throughput and low queueing delay. It provides firewall protection among individual flows, as in a TDM system, while retaining the statistical multiplexing advantages of packet switching. Simulation results show that the VirtualClock algorithm meets all its design goals.",
    "cited_by_count": 513,
    "openalex_id": "https://openalex.org/W2099440788",
    "type": "article"
  },
  {
    "title": "Gossip-based peer sampling",
    "doi": "https://doi.org/10.1145/1275517.1275520",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Márk Jelasity; Spyros Voulgaris; Rachid Guerraoui; Anne-Marie Kermarrec; Maarten van Steen",
    "corresponding_authors": "",
    "abstract": "Gossip-based communication protocols are appealing in large-scale distributed applications such as information dissemination, aggregation, and overlay topology management. This paper factors out a fundamental mechanism at the heart of all these protocols: the peer-sampling service. In short, this service provides every node with peers to gossip with. We promote this service to the level of a first-class abstraction of a large-scale distributed system, similar to a name service being a first-class abstraction of a local-area system. We present a generic framework to implement a peer-sampling service in a decentralized manner by constructing and maintaining dynamic unstructured overlays through gossiping membership information itself. Our framework generalizes existing approaches and makes it easy to discover new ones. We use this framework to empirically explore and compare several implementations of the peer-sampling service. Through extensive simulation experiments we show that---although all protocols provide a good quality uniform random stream of peers to each node locally---traditional theoretical assumptions about the randomness of the unstructured overlays as a whole do not hold in any of the instances. We also show that different design decisions result in severe differences from the point of view of two crucial aspects: load balancing and fault tolerance. Our simulations are validated by means of a wide-area implementation.",
    "cited_by_count": 493,
    "openalex_id": "https://openalex.org/W2126147068",
    "type": "article"
  },
  {
    "title": "A fast mutual exclusion algorithm",
    "doi": "https://doi.org/10.1145/7351.7352",
    "publication_date": "1987-01-05",
    "publication_year": 1987,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "A new solution to the mutual exclusion problem is presented that, in the absence of contention, requires only seven memory accesses. It assumes atomic reads and atomic writes to shared registers.",
    "cited_by_count": 493,
    "openalex_id": "https://openalex.org/W2166071597",
    "type": "article"
  },
  {
    "title": "Continuous profiling",
    "doi": "https://doi.org/10.1145/265924.265925",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Jennifer M. Anderson; Lance M. Berc; Jeffrey A. Dean; Sanjay Ghemawat; Monika Henzinger; Shun-Tak A. Leung; Richard L. Sites; Mark T. Vandevoorde; Carl A. Waldspurger; William E. Weihl",
    "corresponding_authors": "",
    "abstract": "This article describes the Digital Continuous Profiling Infrastructure, a sampling-based profiling system designed to run continuously on production systems. The system supports multiprocessors, works on unmodified executables, and collects profiles for entire systems, including user programs, shared libraries, and the operating system kernel. Samples are collected at a high rate (over 5200 samples/sec. per 333MHz processor), yet with low overhead (1–3% slowdown for most workloads). Analysis tools supplied with the profiling system use the sample data to produce a precise and accurate accounting, down to the level of pipeline stalls incurred by individual instructions, of where time is bring spent. When instructions incur stalls, the tools identify possible reasons, such as cache misses, branch mispredictions, and functional unit contention. The fine-grained instruction-level analysis guides users and automated optimizers to the causes of performance problems and provides important insights for fixing them.",
    "cited_by_count": 490,
    "openalex_id": "https://openalex.org/W2153131460",
    "type": "article"
  },
  {
    "title": "Fast address lookups using controlled prefix expansion",
    "doi": "https://doi.org/10.1145/296502.296503",
    "publication_date": "1999-02-01",
    "publication_year": 1999,
    "authors": "V. Srinivasan; George Varghese",
    "corresponding_authors": "",
    "abstract": "Internet (IP) address lookup is a major bottleneck in high-performance routers. IP address lookup is challenging because it requires a longest matching prefix lookup. It is compounded by increasing routing table sizes, increased traffic, higher-speed links, and the migration to 128-bit IPv6 addresses. We describe how IP lookups and updates can be made faster using a set of of transformation techniques. Our main technique, controlled prefix expansion , transforms a set of prefixes into an equivalent set with fewer prefix lengths. In addition, we use optimization techniques based on dynamic programming, and local transformations of data structures to improve cache behavior. When applied to trie search, our techniques provide a range of algorithms ( Expanded Tries ) whose performance can be tuned. For example, using a processor with 1MB of L2 cache, search of the MaeEast database containing 38000 prefixes can be done in 3 L2 cache accesses. On a 300MHz Pentium II which takes 4 cycles for accessing the first word of the L2 cacheline, this algorithm has a worst-case search time of 180 nsec., a worst-case insert/delete time of 2.5 msec., and an average insert/delete time of 4 usec. Expanded tries provide faster search and faster insert/delete times than earlier lookup algirthms. When applied to Binary Search on Levels, our techniques improve worst-case search times by nearly a factor of 2 (using twice as much storage) for the MaeEast database. Our approach to algorithm design is based on measurements using the VTune tool on a Pentium to obtain dynamic clock cycle counts. Our techniques also apply to similar address lookup problems in other network protocols.",
    "cited_by_count": 466,
    "openalex_id": "https://openalex.org/W1968801809",
    "type": "article"
  },
  {
    "title": "A tree-based algorithm for distributed mutual exclusion",
    "doi": "https://doi.org/10.1145/58564.59295",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Kerry Raymond",
    "corresponding_authors": "Kerry Raymond",
    "abstract": "We present an algorithm for distributed mutual exclusion in a computer network of N nodes that communicate by messages rather than shared memory. The algorithm uses a spanning tree of the computer network, and the number of messages exchanged per critical section depends on the topology of this tree. However, typically the number of messages exchanged is O (log N ) under light demand, and reduces to approximately four messages under saturated demand. Each node holds information only about its immediate neighbors in the spanning tree rather than information about all nodes, and failed nodes can recover necessary information from their neighbors. The algorithm does not require sequence numbers as it operates correctly despite message overtaking.",
    "cited_by_count": 446,
    "openalex_id": "https://openalex.org/W2140606557",
    "type": "article"
  },
  {
    "title": "Scheduler activations",
    "doi": "https://doi.org/10.1145/146941.146944",
    "publication_date": "1992-02-01",
    "publication_year": 1992,
    "authors": "Thomas E. Anderson; Brian N. Bershad; Edward D. Lazowska; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "Threads are the vehicle for concurrency in many approaches to parallel programming. Threads can be supported either by the operating system kernel or by user-level library code in the application address space, but neither approach has been fully satisfactory. This paper addresses this dilemma. First, we argue that the performance of kernel threads is inherently worse than that of user-level threads, rather than this being an artifact of existing implementations; managing parallelism at the user level is essential to high-performance parallel computing. Next, we argue that the problems encountered in integrating user-level threads with other system services is a consequence of the lack of kernel support for user-level threads provided by contemporary multiprocessor operating systems; kernel threads are the wrong abstraction on which to support user-level management of parallelism. Finally, we describe the design, implementation, and performance of a new kernel interface and user-level thread package that together provide the same functionality as kernel threads without compromising the performance and flexibility advantages of user-level management of parallelism.",
    "cited_by_count": 439,
    "openalex_id": "https://openalex.org/W1966938284",
    "type": "article"
  },
  {
    "title": "Exploiting process lifetime distributions for dynamic load balancing",
    "doi": "https://doi.org/10.1145/263326.263344",
    "publication_date": "1997-08-01",
    "publication_year": 1997,
    "authors": "Mor Harchol‐Balter; Allen B. Downey",
    "corresponding_authors": "",
    "abstract": "We consider policies for CPU load balancing in networks of workstations. We address the question of whether preemptive migration (migrating active processes) is necessary, or whether remote execution (migrating processes only at the time of birth) is sufficient for load balancing. We show that resolving this issue is strongly tied to understanding the process lifetime distribution. Our measurements indicate that the distribution of lifetimes for a UNIX process is Pareto (heavy-tailed), with a consistent functional form over a variety of workloads. We show how to apply this distribution to derive a preemptive migration policy that requires no hand-tuned parameters. We used a trace-driven simulation to show that our preemptive migration strategy is far more effective than remote execution, even when the memory transfer cost is high.",
    "cited_by_count": 438,
    "openalex_id": "https://openalex.org/W2068666958",
    "type": "article"
  },
  {
    "title": "Shielding Applications from an Untrusted Cloud with Haven",
    "doi": "https://doi.org/10.1145/2799647",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "Andrew Baumann; Marcus Peinado; Galen Hunt",
    "corresponding_authors": "",
    "abstract": "Today’s cloud computing infrastructure requires substantial trust. Cloud users rely on both the provider’s staff and its globally distributed software/hardware platform not to expose any of their private data. We introduce the notion of shielded execution, which protects the confidentiality and integrity of a program and its data from the platform on which it runs (i.e., the cloud operator’s OS, VM, and firmware). Our prototype, Haven, is the first system to achieve shielded execution of unmodified legacy applications, including SQL Server and Apache, on a commodity OS (Windows) and commodity hardware. Haven leverages the hardware protection of Intel SGX to defend against privileged code and physical attacks such as memory probes, and also addresses the dual challenges of executing unmodified legacy binaries and protecting them from a malicious host. This work motivated recent changes in the SGX specification.",
    "cited_by_count": 436,
    "openalex_id": "https://openalex.org/W2061643296",
    "type": "article"
  },
  {
    "title": "Disco",
    "doi": "https://doi.org/10.1145/265924.265930",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Edouard Bugnion; Scott Devine; Kinshuk Govil; Mendel Rosenblum",
    "corresponding_authors": "",
    "abstract": "In this article we examine the problem of extending modern operating systems to run efficiently on large-scale shared-memory multiprocessors without a large implementation effort. Our approach brings back an idea popular in the 1970s: virtual machine monitors. We use virtual machines to run multiple commodity operating systems on a scalable multiprocessor. This solution addresses many of the challenges facing the system software for these machines. We demonstrate our approach with a prototype called Disco that runs multiple copies of Silicon Graphics' IRIX operating system on a multiprocessor. Our experience shows that the overheads of the monitor are small and that the approach provides scalability as well as the ability to deal with the nonuniform memory access time of these systems. To reduce the memory overheads associated with running multiple operating systems, virtual machines transparently share major data structures such as the program code and the file system buffer cache. We use the distributed-system support of modern operating systems to export a partial single system image to the users. The overall solution achieves most of the benefits of operating systems customized for scalable multiprocessors, yet it can be achieved with a significantly smaller implementation effort.",
    "cited_by_count": 428,
    "openalex_id": "https://openalex.org/W2006816934",
    "type": "article"
  },
  {
    "title": "Eliminating receive livelock in an interrupt-driven kernel",
    "doi": "https://doi.org/10.1145/263326.263335",
    "publication_date": "1997-08-01",
    "publication_year": 1997,
    "authors": "Jeffrey C. Mogul; K. K. Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "Most operating systems use interface interrupts to schedule network tasks. Interrupt-driven systems can provide low overhead and good latency at low offered load, but degrade significantly at higher arrival rates unless care is taken to prevent several pathologies. These are various forms of receive livelock , in which the system spends all of its time processing interrupts, to the exclusion of other necessary tasks. Under extreme conditions, no packets are delivered to the user application or the output of the system. To avoid livelock and related problems, an operating system must schedule network interrupt handling as carefully as it schedules process execution. We modified an interrupt-driven networking implementation to do so; this modification eliminates receive livelock without degrading other aspects of system performance. Our modifications include the use of polling when the system is heavily loaded, while retaining the use of interrupts ur.Jer lighter load. We present measurements demonstrating the success of our approach.",
    "cited_by_count": 417,
    "openalex_id": "https://openalex.org/W2014485836",
    "type": "article"
  },
  {
    "title": "Lightweight probabilistic broadcast",
    "doi": "https://doi.org/10.1145/945506.945507",
    "publication_date": "2003-10-10",
    "publication_year": 2003,
    "authors": "Patrick Eugster; Rachid Guerraoui; Sidath Handurukande; Petr Kouznetsov; Anne-Marie Kermarrec",
    "corresponding_authors": "",
    "abstract": "Gossip-based broadcast algorithms, a family of probabilistic broadcast algorithms, trade reliability guarantees against \"scalability\" properties. Scalability in this context has usually been expressed in terms of message throughput and delivery latency, but there has been little work on how to reduce the memory consumption for membership management and message buffering at large scale.This paper presents lightweight probabilistic broadcast ( lpbcast ), a novel gossip-based broadcast algorithm, which complements the inherent throughput scalability of traditional probabilistic broadcast algorithms with a scalable memory management technique. Our algorithm is completely decentralized and based only on local information: in particular, every process only knows a fixed subset of processes in the system and only buffers fixed \"most suitable\" subsets of messages. We analyze our broadcast algorithm stochastically and compare the analytical results both with simulations and concrete implementation measurements.",
    "cited_by_count": 405,
    "openalex_id": "https://openalex.org/W2014644972",
    "type": "article"
  },
  {
    "title": "Distributed deadlock detection",
    "doi": "https://doi.org/10.1145/357360.357365",
    "publication_date": "1983-05-01",
    "publication_year": 1983,
    "authors": "K. Mani Chandy; Jayadev Misra; Laura M. Haas",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Distributed deadlock detection Authors: K. Mani Chandy Department of Computer Sciences, University of Texas at Austin, Austin, TX Department of Computer Sciences, University of Texas at Austin, Austin, TXView Profile , Jayadev Misra Department of Computer Sciences, University of Texas at Austin, Austin, TX Department of Computer Sciences, University of Texas at Austin, Austin, TXView Profile , Laura M. Haas IBM Research, 5600 Cottle Road, San Jose, CA IBM Research, 5600 Cottle Road, San Jose, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 201 May 1983pp 144–156https://doi.org/10.1145/357360.357365Published:01 May 1983Publication History 304citation5,066DownloadsMetricsTotal Citations304Total Downloads5,066Last 12 Months556Last 6 weeks196 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 401,
    "openalex_id": "https://openalex.org/W2023408771",
    "type": "article"
  },
  {
    "title": "The HP AutoRAID hierarchical storage system",
    "doi": "https://doi.org/10.1145/225535.225539",
    "publication_date": "1996-02-01",
    "publication_year": 1996,
    "authors": "John Wilkes; Richard Golding; Carl Staelin; Tim Sullivan",
    "corresponding_authors": "",
    "abstract": "Configuring redundant disk arrays is a black art. To configure an array properly, a system administrator must understand the details of both the array and the workload it will support. Incorrect understanding of either, or changes in the workload over time, can lead to poor performance. We present a solution to this problem: a two-level storage hierarchy implemented inside a single disk-array controller. In the upper level of this hierarchy, two copies of active data are stored to provide full redundancy and excellent performance. In the lower level, RAID 5 parity protection is used to provide excellent storage cost for inactive data, at somewhat lower performance. The technology we describe in this article, know as HP AutoRAID, automatically and transparently manages migration of data blocks between these two levels as access patterns change. The result is a fully redundant storage system that is extremely easy to use, is suitable for a wide variety of workloads, is largely insensitive to dynamic workload changes, and performs much better than disk arrays with comparable numbers of spindles and much larger amounts of front-end RAM cache. Because the implementation of the HP AutoRAID technology is almost entirely in software, the additional hardware cost for these benefits is very small. We describe the HP AutoRAID technology in detail, provide performance data for an embodiment of it in a storage array, and summarize the results of simulation studies used to choose algorithms implemented in the array.",
    "cited_by_count": 399,
    "openalex_id": "https://openalex.org/W2035606364",
    "type": "article"
  },
  {
    "title": "Providing high availability using lazy replication",
    "doi": "https://doi.org/10.1145/138873.138877",
    "publication_date": "1992-11-01",
    "publication_year": 1992,
    "authors": "Rivka Ladin; Barbara Liskov; Liuba Shrira; Sanjay Ghemawat",
    "corresponding_authors": "",
    "abstract": "To provide high availability for services such as mail or bulletin boards, data must be replicated. One way to guarantee consistency of replicated data is to force service operations to occur in the same order at all sites, but this approach is expensive. For some applications a weaker causal operation order can preserve consistency while providing better performance. This paper describes a new way of implementing causal operations. Our technique also supports two other kinds of operations: operations that are totally ordered with respect to one another and operations that are totally ordered with respect to all other operations. The method performs well in terms of response time, operation-processing capacity, amount of stored state, and number and size of messages; it does better than replication methods based on reliable multicast techniques.",
    "cited_by_count": 397,
    "openalex_id": "https://openalex.org/W2156027558",
    "type": "article"
  },
  {
    "title": "A generic component model for building systems software",
    "doi": "https://doi.org/10.1145/1328671.1328672",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Geoff Coulson; Gordon S. Blair; Paul Grace; François Taı̈ani; Ackbar Joolia; Kevin Lee; Jó Ueyama; Thirunavukkarasu Sivaharan",
    "corresponding_authors": "",
    "abstract": "Component-based software structuring principles are now commonplace at the application level; but componentization is far less established when it comes to building low-level systems software. Although there have been pioneering efforts in applying componentization to systems-building, these efforts have tended to target specific application domains (e.g., embedded systems, operating systems, communications systems, programmable networking environments, or middleware platforms). They also tend to be targeted at specific deployment environments (e.g., standard personal computer (PC) environments, network processors, or microcontrollers). The disadvantage of this narrow targeting is that it fails to maximize the genericity and abstraction potential of the component approach. In this article, we argue for the benefits and feasibility of a generic yet tailorable approach to component-based systems-building that offers a uniform programming model that is applicable in a wide range of systems-oriented target domains and deployment environments. The component model, called OpenCom , is supported by a reflective runtime architecture that is itself built from components. After describing OpenCom and evaluating its performance and overhead characteristics, we present and evaluate two case studies of systems we have built using OpenCom technology, thus illustrating its benefits and its general applicability.",
    "cited_by_count": 379,
    "openalex_id": "https://openalex.org/W2051544187",
    "type": "article"
  },
  {
    "title": "A binary feedback scheme for congestion avoidance in computer networks",
    "doi": "https://doi.org/10.1145/78952.78955",
    "publication_date": "1990-05-01",
    "publication_year": 1990,
    "authors": "K. K. Ramakrishnan; Raj Jain",
    "corresponding_authors": "",
    "abstract": "We propose a scheme for congestion avoidance in networks using a connectionless protocol at the network layer. The scheme uses a minimal amount of feedback from the network to the users, who adjust the amount of traffic allowed into the network. The routers in the network detect congestion and set a congestion-indication bit on packets flowing in the forward direction. The congestion indication is communicated back to the users through the transport-level acknowledgment. The scheme is distributed, adapts to the dynamic state of the network, converges to the optimal operating point, is quite simple to implement, and has low overhead. The scheme maintains fairness in service provided to multiple sources. This paper presents the scheme and the analysis that went into the choice of the various decision mechanisms. We also address the performance of the scheme under transient changes in the network and pathological overload conditions.",
    "cited_by_count": 367,
    "openalex_id": "https://openalex.org/W2096597645",
    "type": "article"
  },
  {
    "title": "Distributed process groups in the V Kernel",
    "doi": "https://doi.org/10.1145/214438.214439",
    "publication_date": "1985-05-01",
    "publication_year": 1985,
    "authors": "David R. Cheriton; Willy Zwaenepoel",
    "corresponding_authors": "",
    "abstract": "The V kernel supports an abstraction of processes, with operations for interprocess communication, process management, and memory management. This abstraction is used as a software base for constructing distributed systems. As a distributed kernel, the V kernel makes intermachine boundaries largely transparent. In this environment of many cooperating processes on different machines, there are many logical groups of processes. Examples include the group of tile servers, a group of processes executing a particular job, and a group of processes executing a distributed parallel computation. In this paper we describe the extension of the V kernel to support process groups. Operations on groups include group interprocess communication, which provides an application-level abstraction of network multicast. Aspects of the implementation and performance, and initial experience with applications are discussed.",
    "cited_by_count": 362,
    "openalex_id": "https://openalex.org/W2134806330",
    "type": "article"
  },
  {
    "title": "Improving the reliability of commodity operating systems",
    "doi": "https://doi.org/10.1145/1047915.1047919",
    "publication_date": "2005-02-02",
    "publication_year": 2005,
    "authors": "Michael M. Swift; Brian N. Bershad; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "Despite decades of research in extensible operating system technology, extensions such as device drivers remain a significant cause of system failures. In Windows XP, for example, drivers account for 85% of recently reported failures.This article describes Nooks, a reliability subsystem that seeks to greatly enhance operating system (OS) reliability by isolating the OS from driver failures. The Nooks approach is practical: rather than guaranteeing complete fault tolerance through a new (and incompatible) OS or driver architecture, our goal is to prevent the vast majority of driver-caused crashes with little or no change to the existing driver and system code. Nooks isolates drivers within lightweight protection domains inside the kernel address space, where hardware and software prevent them from corrupting the kernel. Nooks also tracks a driver's use of kernel resources to facilitate automatic cleanup during recovery.To prove the viability of our approach, we implemented Nooks in the Linux operating system and used it to fault-isolate several device drivers. Our results show that Nooks offers a substantial increase in the reliability of operating systems, catching and quickly recovering from many faults that would otherwise crash the system. Under a wide range and number of fault conditions, we show that Nooks recovers automatically from 99% of the faults that otherwise cause Linux to crash.While Nooks was designed for drivers, our techniques generalize to other kernel extensions. We demonstrate this by isolating a kernel-mode file system and an in-kernel Internet service. Overall, because Nooks supports existing C-language extensions, runs on a commodity operating system and hardware, and enables automated recovery, it represents a substantial step beyond the specialized architectures and type-safe languages required by previous efforts directed at safe extensibility.",
    "cited_by_count": 355,
    "openalex_id": "https://openalex.org/W2153950928",
    "type": "article"
  },
  {
    "title": "Energy-aware lossless data compression",
    "doi": "https://doi.org/10.1145/1151690.1151692",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Kenneth C. Barr; Krste Asanović",
    "corresponding_authors": "",
    "abstract": "Wireless transmission of a single bit can require over 1000 times more energy than a single computation. It can therefore be beneficial to perform additional computation to reduce the number of bits transmitted. If the energy required to compress data is less than the energy required to send it, there is a net energy savings and an increase in battery life for portable computers. This article presents a study of the energy savings possible by losslessly compressing data prior to transmission. A variety of algorithms were measured on a StrongARM SA-110 processor. This work demonstrates that, with several typical compression algorithms, there is a actually a net energy increase when compression is applied before transmission. Reasons for this increase are explained and suggestions are made to avoid it. One such energy-aware suggestion is asymmetric compression , the use of one compression algorithm on the transmit side and a different algorithm for the receive path. By choosing the lowest-energy compressor and decompressor on the test platform, overall energy to send and receive data can be reduced by 11% compared with a well-chosen symmetric pair, or up to 57% over the default symmetric zlib scheme.",
    "cited_by_count": 336,
    "openalex_id": "https://openalex.org/W1964369900",
    "type": "article"
  },
  {
    "title": "Spanner",
    "doi": "https://doi.org/10.1145/2518037.2491245",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "James C. Corbett; Peter Hochschild; Wilson C. Hsieh; Sebastian Kanthak; Eugene Kogan; Hongyi Li; Alexander Lloyd; Sergey Melnik; David Mwaura; David F. Nagle; Sean Quinlan; Jay B. Dean; Rajesh Rao; Lindsay Rolig; Yasushi Saitō; Michal Szymaniak; Christopher M. Taylor; Ruth Wang; Dale Woodford; Michael Epstein; Andrew Fikes; Christopher Frost; J. J. Furman; Sanjay Ghemawat; Andrey Gubarev; Christopher Heiser",
    "corresponding_authors": "",
    "abstract": "Spanner is Google's scalable, multiversion, globally distributed, and synchronously replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This article describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free snapshot transactions, and atomic schema changes, across all of Spanner.",
    "cited_by_count": 326,
    "openalex_id": "https://openalex.org/W4237774489",
    "type": "article"
  },
  {
    "title": "A distributed mutual exclusion algorithm",
    "doi": "https://doi.org/10.1145/6110.214406",
    "publication_date": "1985-11-01",
    "publication_year": 1985,
    "authors": "I. Suzuki; Tadao Kasami",
    "corresponding_authors": "",
    "abstract": "A distributed algorithm is presented that realizes mutual exclusion among N nodes in a computer network. The algorithm requires at most N message exchanges for one mutual exclusion invocation. Accordingly, the delay to invoke mutual exclusion is smaller than in an algorithm of Ricart and Agrawala, which requires 2*( N - 1) message exchanges per invocation. A drawback of the algorithm is that the sequence numbers contained in the messages are unbounded. It is shown that this problem can be overcome by slightly increasing the number of message exchanges.",
    "cited_by_count": 324,
    "openalex_id": "https://openalex.org/W1980152580",
    "type": "article"
  },
  {
    "title": "Comprehensive formal verification of an OS microkernel",
    "doi": "https://doi.org/10.1145/2560537",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Gerwin Klein; June Andronick; Kevin Elphinstone; Toby Murray; Thomas Sewell; Rafal Kolanski; Gernot Heiser",
    "corresponding_authors": "",
    "abstract": "We present an in-depth coverage of the comprehensive machine-checked formal verification of seL4, a general-purpose operating system microkernel. We discuss the kernel design we used to make its verification tractable. We then describe the functional correctness proof of the kernel's C implementation and we cover further steps that transform this result into a comprehensive formal verification of the kernel: a formally verified IPC fastpath, a proof that the binary code of the kernel correctly implements the C semantics, a proof of correct access-control enforcement, a proof of information-flow noninterference, a sound worst-case execution time analysis of the binary, and an automatic initialiser for user-level systems that connects kernel-level access-control enforcement with reasoning about system behaviour. We summarise these results and show how they integrate to form a coherent overall analysis, backed by machine-checked, end-to-end theorems. The seL4 microkernel is currently not just the only general-purpose operating system kernel that is fully formally verified to this degree. It is also the only example of formal proof of this scale that is kept current as the requirements, design and implementation of the system evolve over almost a decade. We report on our experience in maintaining this evolving formally verified code base.",
    "cited_by_count": 320,
    "openalex_id": "https://openalex.org/W2163347957",
    "type": "article"
  },
  {
    "title": "AutoScale",
    "doi": "https://doi.org/10.1145/2382553.2382556",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Anshul Gandhi; Mor Harchol‐Balter; Ram Raghunathan; Michael A. Kozuch",
    "corresponding_authors": "",
    "abstract": "Energy costs for data centers continue to rise, already exceeding $15 billion yearly. Sadly much of this power is wasted. Servers are only busy 10--30% of the time on average, but they are often left on, while idle, utilizing 60% or more of peak power when in the idle state. We introduce a dynamic capacity management policy, AutoScale , that greatly reduces the number of servers needed in data centers driven by unpredictable, time-varying load, while meeting response time SLAs. AutoScale scales the data center capacity, adding or removing servers as needed. AutoScale has two key features: (i) it autonomically maintains just the right amount of spare capacity to handle bursts in the request rate; and (ii) it is robust not just to changes in the request rate of real-world traces, but also request size and server efficiency. We evaluate our dynamic capacity management approach via implementation on a 38-server multi-tier data center, serving a web site of the type seen in Facebook or Amazon, with a key-value store workload. We demonstrate that AutoScale vastly improves upon existing dynamic capacity management policies with respect to meeting SLAs and robustness.",
    "cited_by_count": 285,
    "openalex_id": "https://openalex.org/W2075233755",
    "type": "article"
  },
  {
    "title": "The RAMCloud Storage System",
    "doi": "https://doi.org/10.1145/2806887",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "John K. Ousterhout; Arjun Gopalan; Ashish Gupta; Ankita Kejriwal; Collin Lee; Behnam Montazeri; Diego Ongaro; Seo Jin Park; Henry Qin; Mendel Rosenblum; Stephen M. Rumble; Ryan Stutsman; Stephen Yang",
    "corresponding_authors": "",
    "abstract": "RAMCloud is a storage system that provides low-latency access to large-scale datasets. To achieve low latency, RAMCloud stores all data in DRAM at all times. To support large capacities (1PB or more), it aggregates the memories of thousands of servers into a single coherent key-value store. RAMCloud ensures the durability of DRAM-based data by keeping backup copies on secondary storage. It uses a uniform log-structured mechanism to manage both DRAM and secondary storage, which results in high performance and efficient memory usage. RAMCloud uses a polling-based approach to communication, bypassing the kernel to communicate directly with NICs; with this approach, client applications can read small objects from any RAMCloud storage server in less than 5μs, durable writes of small objects take about 13.5μs. RAMCloud does not keep multiple copies of data online; instead, it provides high availability by recovering from crashes very quickly (1 to 2 seconds). RAMCloud’s crash recovery mechanism harnesses the resources of the entire cluster working concurrently so that recovery performance scales with cluster size.",
    "cited_by_count": 263,
    "openalex_id": "https://openalex.org/W2074881976",
    "type": "article"
  },
  {
    "title": "Preserving and using context information in interprocess communication",
    "doi": "https://doi.org/10.1145/65000.65001",
    "publication_date": "1989-08-01",
    "publication_year": 1989,
    "authors": "Larry Peterson; Nick C. Buchholz; Richard D. Schlichting",
    "corresponding_authors": "",
    "abstract": "When processes in a network communicate, the messages they exchange define a partial ordering of externally visible events. While the significance of this partial order in distributed computing is well understood, it has not been made an explicit part of the communication substrate upon which distributed programs are implemented. This paper describes a new interprocess communication mechanism, called Psync , that explicitly encodes this partial ordering with each message. The paper shows how Psync can be efficiently implemented on an unreliable communications network, and it demonstrates how conversations serve as an elegant foundation for ordering messages exchanged in a distributed computation and for recovering from processor failures.",
    "cited_by_count": 331,
    "openalex_id": "https://openalex.org/W2044052675",
    "type": "article"
  },
  {
    "title": "Size-based scheduling to improve web performance",
    "doi": "https://doi.org/10.1145/762483.762486",
    "publication_date": "2003-05-01",
    "publication_year": 2003,
    "authors": "Mor Harchol‐Balter; Bianca Schroeder; Nikhil Bansal; Mukesh Agrawal",
    "corresponding_authors": "",
    "abstract": "Is it possible to reduce the expected response time of every request at a web server, simply by changing the order in which we schedule the requests? That is the question we ask in this paper.This paper proposes a method for improving the performance of web servers servicing static HTTP requests. The idea is to give preference to requests for small files or requests with short remaining file size, in accordance with the SRPT (Shortest Remaining Processing Time) scheduling policy.The implementation is at the kernel level and involves controlling the order in which socket buffers are drained into the network. Experiments are executed both in a LAN and a WAN environment. We use the Linux operating system and the Apache and Flash web servers.Results indicate that SRPT-based scheduling of connections yields significant reductions in delay at the web server. These result in a substantial reduction in mean response time and mean slowdown for both the LAN and WAN environments. Significantly, and counter to intuition, the requests for large files are only negligibly penalized or not at all penalized as a result of SRPT-based scheduling.",
    "cited_by_count": 320,
    "openalex_id": "https://openalex.org/W2152407381",
    "type": "article"
  },
  {
    "title": "RecPlay",
    "doi": "https://doi.org/10.1145/312203.312214",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Michiel Ronsse; Koen De Bosschere",
    "corresponding_authors": "",
    "abstract": "This article presents a practical solution for the cyclic debugging of nondeterministic parallel programs. The solution consists of a combination of record/replay with automatic on-the-fly data race detection. This combination enables us to limit the record phase to the more efficient recording of the synchronization operations, while deferring the time-consuming data race detection to the replay phase. As the record phase is highly efficient, there is no need to switch it off, hereby eliminating the possibility of Heisenbugs because tracing can be left on all the time. This article describes an implementation of the tools needed to support RecPlay.",
    "cited_by_count": 319,
    "openalex_id": "https://openalex.org/W2040851906",
    "type": "article"
  },
  {
    "title": "An analytical cache model",
    "doi": "https://doi.org/10.1145/63404.63407",
    "publication_date": "1989-05-01",
    "publication_year": 1989,
    "authors": "Anant Agarwal; John L. Hennessy; Mark Horowitz",
    "corresponding_authors": "",
    "abstract": "Trace-driven simulation and hardware measurement are the techniques most often used to obtain accurate performance figures for caches. The former requires a large amount of simulation time to evaluate each cache configuration while the latter is restricted to measurements of existing caches. An analytical cache model that uses parameters extracted from address traces of programs can efficiently provide estimates of cache performance and show the effects of varying cache parameters. By representing the factors that affect cache performance, we develop an analytical model that gives miss rates for a given trace as a function of cache size, degree of associativity, block size, subblock size, multiprogramming level, task switch interval, and observation interval. The predicted values closely approximate the results of trace-driven simulations, while requiring only a small fraction of the computation cost.",
    "cited_by_count": 311,
    "openalex_id": "https://openalex.org/W2144954274",
    "type": "article"
  },
  {
    "title": "An efficient and fault-tolerant solution for distributed mutual exclusion",
    "doi": "https://doi.org/10.1145/103727.103728",
    "publication_date": "1991-02-01",
    "publication_year": 1991,
    "authors": "Divyakant Agrawal; Amr El Abbadi",
    "corresponding_authors": "",
    "abstract": "In this paper, we present an efficient and fault-tolerant algorithm for generating quorums to solve the distributed mutual exclusion problem. The algorithm uses a logical tree organization of the network to generate tree quorums, which are logarithmic in the size of the network in the best case. Our approach is resilient to both site and communication failures, even when such failures lead to network partitioning. Furthermore, the algorithm exhibits a property of graceful degradation, i.e., it requires more messages only as the number of failures increase in the network. We describe how tree quorums can be used for various distributed applications for providing mutually exclusive access to a distributed resource, managing replicated objects, and atomically commiting a distributed transaction.",
    "cited_by_count": 304,
    "openalex_id": "https://openalex.org/W2127663058",
    "type": "article"
  },
  {
    "title": "Design and evaluation of a conit-based continuous consistency model for replicated services",
    "doi": "https://doi.org/10.1145/566340.566342",
    "publication_date": "2002-08-01",
    "publication_year": 2002,
    "authors": "Haifeng Yu; Amin Vahdat",
    "corresponding_authors": "",
    "abstract": "The tradeoffs between consistency, performance, and availability are well understood. Traditionally, however, designers of replicated systems have been forced to choose from either strong consistency guarantees or none at all. This paper explores the semantic space between traditional strong and optimistic consistency models for replicated services. We argue that an important class of applications can tolerate relaxed consistency, but benefit from bounding the maximum rate of inconsistent access in an application-specific manner. Thus, we develop a conit-based continuous consistency model to capture the consistency spectrum using three application-independent metrics, numerical error , order error , and staleness . We then present the design and implementation of TACT, a middleware layer that enforces arbitrary consistency bounds among replicas using these metrics. We argue that the TACT consistency model can simultaneously achieve the often conflicting goals of generality and practicality by describing how a broad range of applications can express their consistency semantics using TACT and by demonstrating that application-independent algorithms can efficiently enforce target consistency levels. Finally, we show that three replicated applications running across the Internet demonstrate significant semantic and performance benefits from using our framework.",
    "cited_by_count": 300,
    "openalex_id": "https://openalex.org/W2115806861",
    "type": "article"
  },
  {
    "title": "Using model checking to find serious file system errors",
    "doi": "https://doi.org/10.1145/1189256.1189259",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Junfeng Yang; Paul Twohey; Dawson Engler; Madanlal Musuvathi",
    "corresponding_authors": "",
    "abstract": "This article shows how to use model checking to find serious errors in file systems. Model checking is a formal verification technique tuned for finding corner-case errors by comprehensively exploring the state spaces defined by a system. File systems have two dynamics that make them attractive for such an approach. First, their errors are some of the most serious, since they can destroy persistent data and lead to unrecoverable corruption. Second, traditional testing needs an impractical, exponential number of test cases to check that the system will recover if it crashes at any point during execution. Model checking employs a variety of state-reducing techniques that allow it to explore such vast state spaces efficiently.We built a system, FiSC, for model checking file systems. We applied it to four widely-used, heavily-tested file systems: ext3, JFS, ReiserFS and XFS. We found serious bugs in all of them, 33 in total. Most have led to patches within a day of diagnosis. For each file system, FiSC found demonstrable events leading to the unrecoverable destruction of metadata and entire directories, including the file system root directory “/”.",
    "cited_by_count": 300,
    "openalex_id": "https://openalex.org/W2124877509",
    "type": "article"
  },
  {
    "title": "Lightweight remote procedure call",
    "doi": "https://doi.org/10.1145/77648.77650",
    "publication_date": "1990-02-01",
    "publication_year": 1990,
    "authors": "Brian N. Bershad; Thomas E. Anderson; Edward D. Lazowska; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "Lightweight Remote Procedure Call (LRPC) is a communication facility designed and optimized for communication between protection domains on the same machine. In contemporary small-kernel operating systems, existing RPC systems incur an unnecessarily high cost when used for the type of communication that predominates—between protection domains on the same machine. This cost leads system designers to coalesce weakly related subsystems into the same protection domain, trading safety for performance. By reducing the overhead of same-machine communication, LRPC encourages both safety and performance. LRPC combines the control transfer and communication model of capability systems with the programming semantics and large-grained protection model of RPC. LRPC achieves a factor-of-three performance improvement over more traditional approaches based on independent threads exchanging messages, reducing the cost of same-machine communication to nearly the lower bound imposed by conventional hardware. LRPC has been integrated into the Taos operating system of the DEC SRC Firefly multiprocessor workstation.",
    "cited_by_count": 293,
    "openalex_id": "https://openalex.org/W2042559279",
    "type": "article"
  },
  {
    "title": "Implementing atomic actions on decentralized data",
    "doi": "https://doi.org/10.1145/357353.357355",
    "publication_date": "1983-02-01",
    "publication_year": 1983,
    "authors": "David P. Reed",
    "corresponding_authors": "David P. Reed",
    "abstract": "article Free Access Share on Implementing atomic actions on decentralized data Author: David P. Reed Laboratory for Computer Science, Massachussetts Institute of Technology, Cambridge, MA Laboratory for Computer Science, Massachussetts Institute of Technology, Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 1Feb. 1983 pp 3–23https://doi.org/10.1145/357353.357355Published:01 February 1983Publication History 197citation1,598DownloadsMetricsTotal Citations197Total Downloads1,598Last 12 Months145Last 6 weeks21 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 292,
    "openalex_id": "https://openalex.org/W2076627572",
    "type": "article"
  },
  {
    "title": "Page placement algorithms for large real-indexed caches",
    "doi": "https://doi.org/10.1145/138873.138876",
    "publication_date": "1992-11-01",
    "publication_year": 1992,
    "authors": "R. E. Kessler; Mark D. Hill",
    "corresponding_authors": "",
    "abstract": "When a computer system supports both paged virtual memory and large real-indexed caches, cache performance depends in part on the main memory page placement. To date, most operating systems place pages by selecting an arbitrary page frame from a pool of page frames that have been made available by the page replacement algorithm. We give a simple model that shows that this naive (arbitrary) page placement leads to up to 30% unnecessary cache conflicts. We develop several page placement algorithms, called careful-mapping algorithms , that try to select a page frame (from the pool of available page frames) that is likely to reduce cache contention. Using trace-driven simulation, we find that careful mapping results in 10–20% fewer (dynamic) cache misses than naive mapping (for a direct-mapped real-indexed multimegabyte cache). Thus, our results suggest that careful mapping by the operating system can get about half the cache miss reduction that a cache size (or associativity) doubling can.",
    "cited_by_count": 291,
    "openalex_id": "https://openalex.org/W2112300321",
    "type": "article"
  },
  {
    "title": "The Totem single-ring ordering and membership protocol",
    "doi": "https://doi.org/10.1145/210223.210224",
    "publication_date": "1995-11-01",
    "publication_year": 1995,
    "authors": "Yair Amir; L.E. Moser; P. M. Melliar‐Smith; D. Agarwal; P. Ciarfella",
    "corresponding_authors": "",
    "abstract": "Fault-tolerant distributed systems are becoming more important, but in existing systems, maintaining the consistency of replicated data is quite expensive. The Totem single-ring protocol supports consistent concurrent operations by placing a total order on broadcast messages. This total order is derived from the sequence number in a token that circulates around a logical ring imposed on a set of processors in a broadcast domain. The protocol handles reconfiguration of the system when processors fail and restart or when the network partitions and remerges. Extended virtual synchrony ensures that processors deliver messages and configuration changes to the application in a consistent, systemwide total order. An effective flow control mechanism enables the Totem single-ring protocol to achieve message-ordering rates significantly higher than the best prior total-ordering protocols.",
    "cited_by_count": 288,
    "openalex_id": "https://openalex.org/W2077671984",
    "type": "article"
  },
  {
    "title": "COCA",
    "doi": "https://doi.org/10.1145/571637.571638",
    "publication_date": "2002-10-07",
    "publication_year": 2002,
    "authors": "Lidong Zhou; Fred B. Schneider; Robbert van Renesse",
    "corresponding_authors": "",
    "abstract": "COCA is a fault-tolerant and secure online certification authority that has been built and deployed both in a local area network and in the Internet. Extremely weak assumptions characterize environments in which COCA's protocols execute correctly: no assumption is made about execution speed and message delivery delays; channels are expected to exhibit only intermittent reliability; and with 3 t + 1 COCA servers up to t may be faulty or compromised. COCA is the first system to integrate a Byzantine quorum system (used to achieve availability) with proactive recovery (used to defend against mobile adversaries which attack, compromise, and control one replica for a limited period of time before moving on to another). In addition to tackling problems associated with combining fault-tolerance and security, new proactive recovery protocols had to be developed. Experimental results give a quantitative evaluation for the cost and effectiveness of the protocols.",
    "cited_by_count": 282,
    "openalex_id": "https://openalex.org/W2095202767",
    "type": "article"
  },
  {
    "title": "Hypervisor-based fault tolerance",
    "doi": "https://doi.org/10.1145/225535.225538",
    "publication_date": "1996-02-01",
    "publication_year": 1996,
    "authors": "Thomas Bressoud; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "Protocols to implement a fault-tolerant computing system are described. These protocols augment the hypervisor of a virtual-machine manager and coordinate a primary virtual machine with its backup. No modifications to the hardware, operating system, or application programs are required. A prototype system was constructed for HP's PA-RISC instruction-set architecture. Even though the prototype was not carefully tuned, it ran programs about a factor of 2 slower than a bare machine would.",
    "cited_by_count": 278,
    "openalex_id": "https://openalex.org/W2114488210",
    "type": "article"
  },
  {
    "title": "Sequential consistency versus linearizability",
    "doi": "https://doi.org/10.1145/176575.176576",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Hagit Attiya; Jennifer L. Welch",
    "corresponding_authors": "",
    "abstract": "The power of two well-known consistency conditions for shared-memory multiprocessors, sequential consistency and linearizability , is compared. The cost measure studied is the worst-case response time in distributed implementations of virtual shared memory supporting one of the two conditions. Three types of shared-memory objects are considered: read/write objects, FIFO queues, and stacks. If clocks are only approximately synchronized (or do not exist), then for all three object types it is shown that linearizability is more expensive than sequential consistency. We show that, for all three data types, the worst-case response time is very sensitive to the assumptions that are made about the timing information available to the system. Under the strong assumption that processes have perfectly synchronized clocks, it is shown that sequential consistency and linearizability are equally costly. We present upper bounds for linearizability and matching lower bounds for sequential consistency. The upper bounds are shown by presenting algorithms that use atomic broadcast in a modular fashion. The lower-bound proofs for the approximate case use the technique of “shifting,” first introduced for studying the clock synchronization problem.",
    "cited_by_count": 275,
    "openalex_id": "https://openalex.org/W2114925953",
    "type": "article"
  },
  {
    "title": "On the generation of cryptographically strong pseudorandom sequences",
    "doi": "https://doi.org/10.1145/357353.357357",
    "publication_date": "1983-02-01",
    "publication_year": 1983,
    "authors": "Adi Shamir",
    "corresponding_authors": "Adi Shamir",
    "abstract": "article Free AccessOn the generation of cryptographically strong pseudorandom sequences Author: Adi Shamir Department of Applied Mathematics, The Weizmann Institute of Science, Rehovot, Israel Department of Applied Mathematics, The Weizmann Institute of Science, Rehovot, IsraelView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 1Feb. 1983 pp 38–44https://doi.org/10.1145/357353.357357Published:01 February 1983Publication History 177citation1,633DownloadsMetricsTotal Citations177Total Downloads1,633Last 12 Months102Last 6 weeks12 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 273,
    "openalex_id": "https://openalex.org/W2087115651",
    "type": "article"
  },
  {
    "title": "Cache performance of operating system and multiprogramming workloads",
    "doi": "https://doi.org/10.1145/48012.48037",
    "publication_date": "1988-11-01",
    "publication_year": 1988,
    "authors": "Anant Agarwal; John L. Hennessy; Mark Horowitz",
    "corresponding_authors": "",
    "abstract": "Large caches are necessary in current high-performance computer systems to provide the required high memory bandwidth. Because a small decrease in cache performance can result in significant system performance degradation, accurately characterizing the performance of large caches is important. Although measurements on actual systems have shown that operating systems and multiprogramming can affect cache performance, previous studies have not focused on these effects. We have developed a program tracing technique called ATUM (Address Tracing Using Microcode) that captures realistic traces of multitasking workloads including the operating system. Examining cache behavior using these traces from a VAX processor shows that both the operating system and multiprogramming activity significantly degrade cache performance, with an even greater proportional impact on large caches. From a careful analysis of the causes of this degradation, we explore various techniques to reduce this loss. While seemingly little can be done to mitigate the effect of system references, multitasking cache miss activity can be substantially reduced with small hardware additions.",
    "cited_by_count": 268,
    "openalex_id": "https://openalex.org/W2015370755",
    "type": "article"
  },
  {
    "title": "Sharing and protection in a single-address-space operating system",
    "doi": "https://doi.org/10.1145/195792.195795",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Jeffrey S. Chase; Henry M. Levy; Michael J. Feeley; Edward D. Lazowska",
    "corresponding_authors": "",
    "abstract": "This article explores memory sharing and protection support in Opal, a single-address-space operating system designed for wide-address (64-bit) architectures. Opal threads execute within protection domains in a single shared virtual address space. Sharing is simplified, because addresses are context independent. There is no loss of protection, because addressability and access are independent; the right to access a segment is determined by the protection domain in which a thread executes. This model enables beneficial code-and data-sharing patterns that are currently prohibitive, due in part to the inherent restrictions of multiple address spaces, and in part to Unix programming style. We have designed and implemented an Opal prototype using the Mach 3.0 microkernel as a base. Our implementation demonstrates how a single-address-space structure can be supported alongside of other environments on a modern microkernel operating system, using modern wide-address architectures. This article justifies the Opal model and its goals for sharing and protection, presents the system and its abstractions, describes the prototype implementation, and reports experience with integrated applications.",
    "cited_by_count": 268,
    "openalex_id": "https://openalex.org/W2107082099",
    "type": "article"
  },
  {
    "title": "Concurrent programming without locks",
    "doi": "https://doi.org/10.1145/1233307.1233309",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Keir Fraser; Tim Harris",
    "corresponding_authors": "",
    "abstract": "Mutual exclusion locks remain the de facto mechanism for concurrency control on shared-memory data structures. However, their apparent simplicity is deceptive: It is hard to design scalable locking strategies because locks can harbor problems such as priority inversion, deadlock, and convoying. Furthermore, scalable lock-based systems are not readily composable when building compound operations. In looking for solutions to these problems, interest has developed in nonblocking systems which have promised scalability and robustness by eschewing mutual exclusion while still ensuring safety. However, existing techniques for building nonblocking systems are rarely suitable for practical use, imposing substantial storage overheads, serializing nonconflicting operations, or requiring instructions not readily available on today's CPUs. In this article we present three APIs which make it easier to develop nonblocking implementations of arbitrary data structures. The first API is a multiword compare-and-swap operation (MCAS) which atomically updates a set of memory locations. This can be used to advance a data structure from one consistent state to another. The second API is a word-based software transactional memory (WSTM) which can allow sequential code to be reused more directly than with MCAS and which provides better scalability when locations are being read rather than being updated. The third API is an object-based software transactional memory (OSTM). OSTM allows a simpler implementation than WSTM, but at the cost of reengineering the code to use OSTM objects. We present practical implementations of all three of these APIs, built from operations available across all of today's major CPU families. We illustrate the use of these APIs by using them to build highly concurrent skip lists and red-black trees. We compare the performance of the resulting implementations against one another and against high-performance lock-based systems. These results demonstrate that it is possible to build useful nonblocking data structures with performance comparable to, or better than, sophisticated lock-based designs.",
    "cited_by_count": 265,
    "openalex_id": "https://openalex.org/W2034963261",
    "type": "article"
  },
  {
    "title": "Byzantine generals in action",
    "doi": "https://doi.org/10.1145/190.357399",
    "publication_date": "1984-05-01",
    "publication_year": 1984,
    "authors": "Fred B. Schneider",
    "corresponding_authors": "Fred B. Schneider",
    "abstract": "article Free Access Share on Byzantine generals in action: implementing fail-stop processors Author: Fred B. Schneider Department of Computer Science, Cornell University, Ithaca, NY Department of Computer Science, Cornell University, Ithaca, NYView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 201 May 1984pp 145–154https://doi.org/10.1145/190.357399Published:01 May 1984Publication History 204citation1,155DownloadsMetricsTotal Citations204Total Downloads1,155Last 12 Months78Last 6 weeks9 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 265,
    "openalex_id": "https://openalex.org/W2089138728",
    "type": "article"
  },
  {
    "title": "Zyzzyva",
    "doi": "https://doi.org/10.1145/1658357.1658358",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Ramakrishna Kotla; Lorenzo Alvisi; Mike Dahlin; Allen Clement; Edmund Wong",
    "corresponding_authors": "",
    "abstract": "A longstanding vision in distributed systems is to build reliable systems from unreliable components. An enticing formulation of this vision is Byzantine Fault-Tolerant (BFT) state machine replication, in which a group of servers collectively act as a correct server even if some of the servers misbehave or malfunction in arbitrary (“Byzantine”) ways. Despite this promise, practitioners hesitate to deploy BFT systems, at least partly because of the perception that BFT must impose high overheads. In this article, we present Zyzzyva, a protocol that uses speculation to reduce the cost of BFT replication. In Zyzzyva, replicas reply to a client's request without first running an expensive three-phase commit protocol to agree on the order to process requests. Instead, they optimistically adopt the order proposed by a primary server, process the request, and reply immediately to the client. If the primary is faulty, replicas can become temporarily inconsistent with one another, but clients detect inconsistencies, help correct replicas converge on a single total ordering of requests, and only rely on responses that are consistent with this total order. This approach allows Zyzzyva to reduce replication overheads to near their theoretical minima and to achieve throughputs of tens of thousands of requests per second, making BFT replication practical for a broad range of demanding services.",
    "cited_by_count": 263,
    "openalex_id": "https://openalex.org/W2038345682",
    "type": "article"
  },
  {
    "title": "Tolerating failures of continuous-valued sensors",
    "doi": "https://doi.org/10.1145/128733.128735",
    "publication_date": "1990-11-01",
    "publication_year": 1990,
    "authors": "Keith Marzullo",
    "corresponding_authors": "Keith Marzullo",
    "abstract": "One aspect of fault-tolerance in process control programs is the ability to tolerate sensor failure. This paper presents a methodology for transforming a process control program that cannot tolerate sensor failures into one that can. Issues addressed include modifying specifications in order to accommodate uncertainty in sensor values and averaging sensor values in a fault-tolerant manner. In addition, a hierarchy of sensor failure models is identified, and both the attainable accuracy and the run-time complexity of sensor averaging with respect to this hierarchy is discussed.",
    "cited_by_count": 258,
    "openalex_id": "https://openalex.org/W1992554067",
    "type": "article"
  },
  {
    "title": "Synchronizing shared abstract types",
    "doi": "https://doi.org/10.1145/989.1188",
    "publication_date": "1984-08-01",
    "publication_year": 1984,
    "authors": "Peter Schwarz; Alfred Z. Spector",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Synchronizing shared abstract types Authors: Peter M. Schwarz Carnegie-Mellon Univ., Pittsburgh, PA Carnegie-Mellon Univ., Pittsburgh, PAView Profile , Alfred Z. Spector Carnegie-Mellon Univ., Pittsburgh, PA Carnegie-Mellon Univ., Pittsburgh, PAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 3Aug. 1984 pp 223–250https://doi.org/10.1145/989.1188Online:01 August 1984Publication History 187citation516DownloadsMetricsTotal Citations187Total Downloads516Last 12 Months15Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 254,
    "openalex_id": "https://openalex.org/W2067686439",
    "type": "article"
  },
  {
    "title": "Monitoring distributed systems",
    "doi": "https://doi.org/10.1145/13677.22723",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "Jeffrey J. Joyce; Greg Lomow; Konrad Slind; Brian Unger",
    "corresponding_authors": "",
    "abstract": "The monitoring of distributed systems involves the collection, interpretation, and display of information concerning the interactions among concurrently executing processes. This information and its display can support the debugging, testing, performance evaluation, and dynamic documentation of distributed systems. General problems associated with monitoring are outlined in this paper, and the architecture of a general purpose, extensible, distributed monitoring system is presented. Three approaches to the display of process interactions are described: textual traces, animated graphical traces, and a combination of aspects of the textual and graphical approaches. The roles that each of these approaches fulfill in monitoring and debugging distributed systems are identified and compared. Monitoring tools for collecting communication statistics, detecting deadlock, controlling the non-deterministic execution of distributed systems, and for using protocol specifications in monitoring are also described. Our discussion is based on experience in the development and use of a monitoring system within a distributed programming environment called Jade. Jade was developed within the Computer Science Department of the University of Calgary and is now being used to support teaching and research at a number of university and research organizations.",
    "cited_by_count": 251,
    "openalex_id": "https://openalex.org/W2093827646",
    "type": "article"
  },
  {
    "title": "A dynamic network architecture",
    "doi": "https://doi.org/10.1145/128899.128901",
    "publication_date": "1992-05-01",
    "publication_year": 1992,
    "authors": "Sean O’Malley; Larry Peterson",
    "corresponding_authors": "",
    "abstract": "Network software is a critical component of any distributed system. Because of its complexity, network software is commonly layered into a hierarchy of protocols, or more generally, into a protocol graph . Typical protocol graphs—including those standardized in the ISO and TCP/IP network architectures—share three important properties; the protocol graph is simple, the nodes of the graph (protocols) encapsulate complex functionality, and the topology of the graph is relatively static. This paper describes a new way to organize network software that differs from conventional architectures in all three of these properties. In our approach, the protocol graph is complex, individual protocols encapsulate a single function, and the topology of the graph is dynamic. The main contribution of this paper is to describe the ideas behind our new architecture, illustrate the advantages of using the architecture, and demonstrate that the architecture results in efficient network software.",
    "cited_by_count": 248,
    "openalex_id": "https://openalex.org/W2141421176",
    "type": "article"
  },
  {
    "title": "A quorum-consensus replication method for abstract data types",
    "doi": "https://doi.org/10.1145/6306.6308",
    "publication_date": "1986-02-10",
    "publication_year": 1986,
    "authors": "Maurice Herlihy",
    "corresponding_authors": "Maurice Herlihy",
    "abstract": "Replication can enhance the availability of data in distributed systems. This paper introduces a new method for managing replicated data. Unlike many methods that support replication only for uninterpreted files, this method systematically exploits type-specific properties of objects such as sets, queues, or directories to provide more effective replication. Each operation requires the cooperation of a certain number of sites for its successful completion. A quorum for an operation is any such set of sites. Necessary and sufficient constraints on quorum intersections are derived from an analysis of the data type's algebraic structure. A reconfiguration method is proposed that permits quorums to be changed dynamically. By taking advantage of type-specific properties in a general and systematic way, this method can realize a wider range of availability properties and more flexible reconfiguration than comparable replication methods.",
    "cited_by_count": 247,
    "openalex_id": "https://openalex.org/W2012848290",
    "type": "article"
  },
  {
    "title": "A file system for continuous media",
    "doi": "https://doi.org/10.1145/138873.138875",
    "publication_date": "1992-11-01",
    "publication_year": 1992,
    "authors": "David P. Anderson; Yoshitomo Osawa; Ramesh Govindan",
    "corresponding_authors": "",
    "abstract": "The Continuous Media File System, CMFS, supports real-time storage and retrieval of continuous media data (digital audio and video) on disk. CMFS clients read or write files in “sessions,” each with a guaranteed minimum data rate. Multiple sessions, perhaps with different rates, and non-real-time access can proceed concurrently. CMFS addresses several interrelated design issues; real-time semantics fo sessions, disk layout, an acceptance test for new sessions, and disk scheduling policy. We use simulation to compare different design choices.",
    "cited_by_count": 245,
    "openalex_id": "https://openalex.org/W2065811011",
    "type": "article"
  },
  {
    "title": "A dynamic processor allocation policy for multiprogrammed shared-memory multiprocessors",
    "doi": "https://doi.org/10.1145/151244.151246",
    "publication_date": "1993-05-01",
    "publication_year": 1993,
    "authors": "Cathy McCann; Raj Vaswani; John Zahorjan",
    "corresponding_authors": "",
    "abstract": "We propose and evaluate empirically the performance of a dynamic processor-scheduling policy for multiprogrammed shared-memory multiprocessors. The policy is dynamic in that it reallocates processors from one parallel job to another based on the currently realized parallelism of those jobs. The policy is suitable for implementation in production systems in that: We have evaluated our scheduler and compared it to alternatives using a set of prototype implementations running on a Sequent Symmetry multiprocessor. Using a number of parallel applications with distinct qualitative behaviors, we have both evaluated the policies according to the major criterion of overall performance and examined a number of more general policy issues, including the advantage of “space sharing” over “time sharing” the processors of a multiprocessor, and the importance of cooperation between the kernel and the application in reallocating processors between jobs. We have also compared the policies according to other criteia important in real implementations, in particular, fairness and respone time to short, sequential requests. We conclude that a combination of performance and implementation considerations makes a compelling case for our dynamic scheduling policy.",
    "cited_by_count": 240,
    "openalex_id": "https://openalex.org/W2094587335",
    "type": "article"
  },
  {
    "title": "Converting thread-level parallelism to instruction-level parallelism via simultaneous multithreading",
    "doi": "https://doi.org/10.1145/263326.263382",
    "publication_date": "1997-08-01",
    "publication_year": 1997,
    "authors": "Jack Lo; Joel Emer; Henry M. Levy; Rebecca L. Stamm; Dean M. Tullsen; Susan J. Eggers",
    "corresponding_authors": "",
    "abstract": "To achieve high performance, contemporary computer systems rely on two forms of parallelism: instruction-level parallelism (ILP) and thread-level parallelism (TLP). Wide-issue super-scalar processors exploit ILP by executing multiple instructions from a single program in a single cycle. Multiprocessors (MP) exploit TLP by executing different threads in parallel on different processors. Unfortunately, both parallel processing styles statically partition processor resources, thus preventing them from adapting to dynamically changing levels of ILP and TLP in a program. With insufficient TLP, processors in an MP will be idle; with insufficient ILP, multiple-issue hardware on a superscalar is wasted. This article explores parallel processing on an alternative architecture, simultaneous multithreading (SMT), which allows multiple threads to complete for and share all of the processor's resources every cycle. The most compelling reason for running parallel applications on an SMT processor is its ability to use thread-level parallelism and instruction-level parallelism interchangeably. By permitting multiple threads to share the processor's functional units simultaneously, the processor can use both ILP and TLP to accommodate variations in parallelism. When a program has only a single thread, all of the SMT processor's resources can be dedicated to that thread; when more TLP exists, this parallelism can compensate for a lack of per-thread ILP. We examine two alternative on-chip parallel architectures for the next generation of processors. We compare SMT and small-scale, on-chip multiprocessors in their ability to exploit both ILP and TLP. First, we identify the hardware bottlenecks that prevent multiprocessors from effectively exploiting ILP. Then, we show that because of its dynamic resource sharing, SMT avoids these inefficiencies and benefits from being able to run more threads on a single processor. The use of TLP is especially advantageous when per-thread ILP is limited. The ease of adding additional thread contexts on an SMT (relative to adding additional processors on an MP) allows simultaneous multithreading to expose more parallelism, further increasing functional unit utilization and attaining a 52% average speedup (versus a four-processor, single-chip multiprocessor with comparable execution resources). This study also addresses an often-cited concern regarding the use of thread-level parallelism or multithreading: interference in the memory system and branch prediction hardware. We find the multiple threads cause interthread interference in the caches and place greater demands on the memory system, thus increasing average memory latencies. By exploiting threading-level parallelism, however, SMT hides these additional latencies, so that they only have a small impact on total program performance. We also find that for parallel applications, the additional threads have minimal effects on branch prediction.",
    "cited_by_count": 240,
    "openalex_id": "https://openalex.org/W2101366948",
    "type": "article"
  },
  {
    "title": "Minerva",
    "doi": "https://doi.org/10.1145/502912.502915",
    "publication_date": "2001-11-01",
    "publication_year": 2001,
    "authors": "Guillermo A. Alvarez; Elizabeth Borowsky; Susie Go; Theodore H. Romer; R. Becker-Szendy; Richard Golding; Arif Merchant; Mirjana Spasojevic; Alistair Veitch; John Wilkes",
    "corresponding_authors": "",
    "abstract": "Enterprise-scale storage systems, which can contain hundreds of host computers and storage devices and up to tens of thousands of disks and logical volumes, are difficult to design. The volume of choices that need to be made is massive, and many choices have unforeseen interactions. Storage system design is tedious and complicated to do by hand, usually leading to solutions that are grossly over-provisioned, substantially under-performing or, in the worst case, both.To solve the configuration nightmare, we present minerva: a suite of tools for designing storage systems automatically. Minerva uses declarative specifications of application requirements and device capabilities; constraint-based formulations of the various sub-problems; and optimization techniques to explore the search space of possible solutions.This paper also explores and evaluates the design decisions that went into Minerva, using specialized micro- and macro-benchmarks. We show that Minerva can successfully handle a workload with substantial complexity (a decision-support database benchmark). Minerva created a 16-disk design in only a few minutes that achieved the same performance as a 30-disk system manually designed by human experts. Of equal importance, Minerva was able to predict the resulting system's performance before it was built.",
    "cited_by_count": 238,
    "openalex_id": "https://openalex.org/W2062617656",
    "type": "article"
  },
  {
    "title": "Fault tolerance under UNIX",
    "doi": "https://doi.org/10.1145/58564.58565",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Anita Borg; Wolfgang Blau; Wolfgang Graetsch; Ferdinand Herrmann; Wolfgang Oberle",
    "corresponding_authors": "",
    "abstract": "The initial design for a distributed, fault-tolerant version of UNIX based on three-way atomic message transmission was presented in an earlier paper [3]. The implementation effort then moved from Auragen Systems 1 to Nixdorf Computer where it was completed. This paper describes the working system, now known as the TARGON/32. The original design left open questions in at least two areas: fault tolerance for server processes and recovery after a crash were briefly and inaccurately sketched, rebackup after recovery was not discussed at all. The fundamental design involving three-way message transmission has remained unchanged. However, in addition to important changes in the implementation, server backup has been redesigned and is now more consistent with that of normal user processes. Recovery and rebackup have been completed in a less centralized and thus more efficient manner than previously envisioned. In this paper we review important aspects of the original design and note how the implementation differs from our original ideas. We then focus on the backup and recovery for server processes and the changes and additions in the design and implementation of recovery and rebackup.",
    "cited_by_count": 235,
    "openalex_id": "https://openalex.org/W2109739361",
    "type": "article"
  },
  {
    "title": "Disk cache—miss ratio analysis and design considerations",
    "doi": "https://doi.org/10.1145/3959.3961",
    "publication_date": "1985-08-01",
    "publication_year": 1985,
    "authors": "Alan Jay Smith",
    "corresponding_authors": "Alan Jay Smith",
    "abstract": "The current trend of computer system technology is toward CPUs with rapidly increasing processing power and toward disk drives of rapidly increasing density, but with disk performance increasing very slowly if at all. The implication of these trends is that at some point the processing power of computer systems will be limited by the throughput of the input/output (I/O) system. A solution to this problem, which is described and evaluated in this paper, is disk cache . The idea is to buffer recently used portions of the disk address space in electronic storage. Empirically, it is shown that a large (e.g., 80-90 percent) fraction of all I/O requests are captured by a cache of an 8-Mbyte order-of-magnitude size for our workload sample. This paper considers a number of design parameters for such a cache (called cache disk or disk cache), including those that can be examined experimentally (cache location, cache size, migration algorithms, block sizes, etc.) and others (access time, bandwidth, multipathing, technology, consistency, error recovery, etc.) for which we have no relevant data or experiments. Consideration is given to both caches located in the I/O system, as with the storage controller, and those located in the CPU main memory. Experimental results are based on extensive trace-driven simulations using traces taken from three large IBM or IBM-compatible mainframe data processing installations. We find that disk cache is a powerful means of extending the performance limits of high-end computer systems.",
    "cited_by_count": 234,
    "openalex_id": "https://openalex.org/W2064421917",
    "type": "article"
  },
  {
    "title": "Shared resource matrix methodology",
    "doi": "https://doi.org/10.1145/357369.357374",
    "publication_date": "1983-08-01",
    "publication_year": 1983,
    "authors": "Richard A. Kemmerer",
    "corresponding_authors": "Richard A. Kemmerer",
    "abstract": "article Free Access Share on Shared resource matrix methodology: an approach to identifying storage and timing channels Author: Richard A. Kemmerer Computer Science Dept., University of California, Santa Barbara, CA Computer Science Dept., University of California, Santa Barbara, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 3pp 256–277https://doi.org/10.1145/357369.357374Published:01 August 1983Publication History 159citation1,712DownloadsMetricsTotal Citations159Total Downloads1,712Last 12 Months190Last 6 weeks34 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 233,
    "openalex_id": "https://openalex.org/W2166180744",
    "type": "article"
  },
  {
    "title": "VAXcluster",
    "doi": "https://doi.org/10.1145/214419.214421",
    "publication_date": "1986-05-01",
    "publication_year": 1986,
    "authors": "Nancy P. Kronenberg; Henry M. Levy; William D. Strecker",
    "corresponding_authors": "",
    "abstract": "A VAXcluster is a highly available and extensible configuration of VAX computers that operate as a single system. To achieve performance in a multicomputer environment, a new communications architecture, communications hardware, and distributed software were jointly designed. The software is a distributed version of the VAX/VMS operating system that uses a distributed lock manager to synchronize access to shared resources. The communications hardware includes a 70 megabit per second message-oriented interconnect and an interconnect port that performs communications tasks traditionally handled by software. Performance measurements show this structure to be highly efficient, for example, capable of sending and receiving 3000 messages per second on a VAX-11/780.",
    "cited_by_count": 226,
    "openalex_id": "https://openalex.org/W2117536183",
    "type": "article"
  },
  {
    "title": "Implementation and performance of integrated application-controlled file caching, prefetching, and disk scheduling",
    "doi": "https://doi.org/10.1145/235543.235544",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Pei Cao; Edward W. Felten; Anna R. Karlin; Kai Li",
    "corresponding_authors": "",
    "abstract": "As the performance gap between disks and micropocessors continues to increase, effective utilization of the file cache becomes increasingly immportant. Application-controlled file caching and prefetching can apply application-specific knowledge to improve file cache management. However, supporting application-controlled file caching and prefetching is nontrivial because caching and prefetching need to be integrated carefully, and the kernel needs to allocate cache blocks among processes appropriately. This article presents the design, implementation, and performance of a file system that integrates application-controlled caching, prefetching, and disk scheduling. We use a two-level cache management strategy. The kernel uses the LRU-SP (Least-Recently-Used with Swapping and Placeholders) policy to allocate blocks to processes, and each process integrates application-specific caching and prefetching based on the controlled-aggressive policy, an algorithm previously shown in a theoretical sense to be nearly optimal. Each process also improves its disk access latency by submittint its prefetches in batches so that the requests can be scheduled to optimize disk access performance. Our measurements show that this combination of techniques greatly improves the performance of the file system. We measured that the running time is reduced by 3% to 49% (average 26%) for single-process workloads and by 5% to 76% (average 32%) for multiprocess workloads.",
    "cited_by_count": 226,
    "openalex_id": "https://openalex.org/W2137935742",
    "type": "article"
  },
  {
    "title": "Fast and secure distributed read-only file system",
    "doi": "https://doi.org/10.1145/505452.505453",
    "publication_date": "2002-02-01",
    "publication_year": 2002,
    "authors": "Kevin Fu; M. Frans Kaashoek; David Mazières",
    "corresponding_authors": "",
    "abstract": "Internet users increasingly rely on publicly available data for everything from software installation to investment decisions. Unfortunately, the vast majority of public content on the Internet comes with no integrity or authenticity guarantees. This paper presents the self-certifying read-only file system, a content distribution system providing secure, scalable access to public, read-only data.The read-only file system makes the security of published content independent from that of the distribution infrastructure. In a secure area (perhaps off-line), a publisher creates a digitally signed database out of a file system's contents. The publisher then replicates the database on untrusted content-distribution servers, allowing for high availability.The read-only file system avoids performing any cryptographic operations on servers and keeps the overhead of cryptography low on clients, allowing servers to scale to a large number of clients. Measurements of an implementation show that an individual server running on a 550-Mhz Pentium III with FreeBSD can support 1,012 connections per second and 300 concurrent clients compiling a large software package.",
    "cited_by_count": 222,
    "openalex_id": "https://openalex.org/W2084445812",
    "type": "article"
  },
  {
    "title": "Analysis of benchmark characteristics and benchmark performance prediction",
    "doi": "https://doi.org/10.1145/235543.235545",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Rafael H. Saavedra; Alan Jay Smith",
    "corresponding_authors": "",
    "abstract": "Standard benchmarking provides to run-times for given programs on given machines, but fails to provide insight as to why those results were obtained (either in terms of machine or program characteristics) and fails to provide run-times for that program on some other machine, or some other programs on that machine. We have developed a machine-imdependent model of program execution to characterize both machine performance and program execution. By merging these machine and program characterizations, we can estimate execution time for arbitrary machine/program combinations. Our technique allows us to identify those operations, either on the machine or in the programs, which dominate the benchmark results. This information helps designers in improving the performance of future machines and users in tuning their applications to better utilize the performance of existing machines. Here we apply our methodology to characterize benchmarks and predict their execution times. We present extensive run-time statistics for a large set of benchmarks including the SPEC and Perfect Club suites. We show how these statistics can be used to identify important shortcoming in the programs. In addition, we give execution time estimates for a large sample of programs and machines and compare these against benchmark results. Finally, we develop a metric for program similarity that makes it possible to classify benchmarks with respect to a large set of characteristics.",
    "cited_by_count": 216,
    "openalex_id": "https://openalex.org/W2019436626",
    "type": "article"
  },
  {
    "title": "The LOCKSS peer-to-peer digital preservation system",
    "doi": "https://doi.org/10.1145/1047915.1047917",
    "publication_date": "2005-02-02",
    "publication_year": 2005,
    "authors": "Petros Maniatis; Mema Roussopoulos; TJ Giuli; David S. H. Rosenthal; Mary Baker",
    "corresponding_authors": "",
    "abstract": "The LOCKSS project has developed and deployed in a world-wide test a peer-to-peer system for preserving access to journals and other archival information published on the Web. It consists of a large number of independent, low-cost, persistent Web caches that cooperate to detect and repair damage to their content by voting in “opinion polls.” Based on this experience, we present a design for and simulations of a novel protocol for voting in systems of this kind. It incorporates rate limitation and intrusion detection to ensure that even some very powerful adversaries attacking over many years have only a small probability of causing irrecoverable damage before being detected.",
    "cited_by_count": 215,
    "openalex_id": "https://openalex.org/W2087314535",
    "type": "article"
  },
  {
    "title": "How to exchange (secret) keys",
    "doi": "https://doi.org/10.1145/357360.357368",
    "publication_date": "1983-05-01",
    "publication_year": 1983,
    "authors": "Manuel Blum",
    "corresponding_authors": "Manuel Blum",
    "abstract": "article Free Access Share on How to exchange (secret) keys Author: Manuel Blum Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 2pp 175–193https://doi.org/10.1145/357360.357368Published:01 May 1983Publication History 157citation1,360DownloadsMetricsTotal Citations157Total Downloads1,360Last 12 Months86Last 6 weeks17 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 214,
    "openalex_id": "https://openalex.org/W2026755693",
    "type": "article"
  },
  {
    "title": "File-system development with stackable layers",
    "doi": "https://doi.org/10.1145/174613.174616",
    "publication_date": "1994-02-01",
    "publication_year": 1994,
    "authors": "John Heidemann; Gerald J. Popek",
    "corresponding_authors": "",
    "abstract": "Filing services have experienced a number of innovations in recent years, but many of these promising ideas have failed to enter into broad use. One reason is that current filing environments present several barriers to new development. For example, file systems today typically stand alone instead of building on the work of others, and support of new filing services often requires changes that invalidate existing work. Stackable file-system design addresses these issues in several ways. Complex filing services are constructed from layer “building blocks,” each of which may be provided by independent parties. There are no syntactic constraints to layer order, and layers can occupy different address spaces, allowing very flexible layer configuration. Independent layer evolution and development are supported by an extensible interface bounding each layer. This paper discusses stackable layering in detail and presents design techniques it enables. We describe an implementation providing these facilities that exhibits very high performance. By lowering barriers to new filing design, stackable layering offers the potential of broad third-party file-system development not feasible today.",
    "cited_by_count": 204,
    "openalex_id": "https://openalex.org/W2054481902",
    "type": "article"
  },
  {
    "title": "Integrating security in a large distributed system",
    "doi": "https://doi.org/10.1145/65000.65002",
    "publication_date": "1989-08-01",
    "publication_year": 1989,
    "authors": "Mahadev Satyanarayanan",
    "corresponding_authors": "Mahadev Satyanarayanan",
    "abstract": "Andrew is a distributed computing environment that is a synthesis of the personal computing and timesharing paradigms. When mature, it is expected to encompass over 5,000 workstations spanning the Carnegie Mellon University campus. This paper examines the security issues that arise in such an environment and describes the mechanisms that have been developed to address them. These mechanisms include the logical and physical separation of servers and clients, support for secure communication at the remote procedure call level, a distributed authentication service, a file-protection scheme that combines access lists with UNIX mode bits, and the use of encryption as a basic building block. The paper also discusses the assumptions underlying security in Andrew and analyzes the vulnerability of the system. Usage experience reveals that resource control, particularly of workstation CPU cycles, is more important than originally anticipated and that the mechanisms available to address this issue are rudimentary.",
    "cited_by_count": 201,
    "openalex_id": "https://openalex.org/W2163758100",
    "type": "article"
  },
  {
    "title": "Depot",
    "doi": "https://doi.org/10.1145/2063509.2063512",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Prince Mahajan; Srinath Setty; Sangmin Lee; Allen Clement; Lorenzo Alvisi; Mike Dahlin; Michael Walfish",
    "corresponding_authors": "",
    "abstract": "This article describes the design, implementation, and evaluation of Depot, a cloud storage system that minimizes trust assumptions. Depot tolerates buggy or malicious behavior by any number of clients or servers, yet it provides safety and liveness guarantees to correct clients. Depot provides these guarantees using a two-layer architecture. First, Depot ensures that the updates observed by correct nodes are consistently ordered under Fork-Join-Causal consistency (FJC). FJC is a slight weakening of causal consistency that can be both safe and live despite faulty nodes. Second, Depot implements protocols that use this consistent ordering of updates to provide other desirable consistency, staleness, durability, and recovery properties. Our evaluation suggests that the costs of these guarantees are modest and that Depot can tolerate faults and maintain good availability, latency, overhead, and staleness even when significant faults occur.",
    "cited_by_count": 201,
    "openalex_id": "https://openalex.org/W3139430581",
    "type": "article"
  },
  {
    "title": "The Vesta parallel file system",
    "doi": "https://doi.org/10.1145/233557.233558",
    "publication_date": "1996-08-01",
    "publication_year": 1996,
    "authors": "Peter Corbett; Dror G. Feitelson",
    "corresponding_authors": "",
    "abstract": "The Vesta parallel file system is designed to provide parallel file access to application programs running on multicomputers with parallel I/O subsystems. Vesta uses a new abstraction of files: a file is not a sequence of bytes, but rather it can be partitioned into multiple disjoint sequences that are accessed in parallel. The partitioning—which can also be changed dynamically—reduces the need for synchronization and coordination during the access. Some control over the layout of data is also provided, so the layout can be matched with the anticipated access patterns. The system is fully implemented and forms the basis for the AIX Parallel I/O File System on the IBM SP2. The implementation does not compromise scalability or parallelism. In fact, all data accesses are done directly to the I/O node that contains the requested data, without any indirection or access to shared metadata. Disk mapping and caching functions are confined to each I/O node, so there is no need to keep data coherent across nodes. Performance measurements shown good scalability with increased resources. Moreover, different access patterns are show to achieve similar performance.",
    "cited_by_count": 200,
    "openalex_id": "https://openalex.org/W2068411009",
    "type": "article"
  },
  {
    "title": "Authentication in the Taos operating system",
    "doi": "https://doi.org/10.1145/174613.174614",
    "publication_date": "1994-02-01",
    "publication_year": 1994,
    "authors": "Edward Wobber; Martı́n Abadi; Michael T. Burrows; Butler Lampson",
    "corresponding_authors": "",
    "abstract": "We describe a design for security in a distributed system and its implementation. In our design, applications gain access to security services through a narrow interface. This interface provides a notion of identity that includes simple principals, groups, roles, and delegations. A new operating system component manages principals, credentials, and secure channels. It checks credentials according to the formal rules of a logic of authentication. Our implementation is efficient enough to support a substantial user community.",
    "cited_by_count": 200,
    "openalex_id": "https://openalex.org/W3005123096",
    "type": "article"
  },
  {
    "title": "Serverless network file systems",
    "doi": "https://doi.org/10.1145/225535.225537",
    "publication_date": "1996-02-01",
    "publication_year": 1996,
    "authors": "Thomas E. Anderson; Michael Dahlin; Jeanna M. Neefe; David A. Patterson; Drew Roselli; Randolph Y. Wang",
    "corresponding_authors": "",
    "abstract": "We propose a new paradigm for network file system design: serverless network file systems . While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Furthermore, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundatn data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client.",
    "cited_by_count": 194,
    "openalex_id": "https://openalex.org/W2039573521",
    "type": "article"
  },
  {
    "title": "System support for pervasive applications",
    "doi": "https://doi.org/10.1145/1035582.1035584",
    "publication_date": "2004-11-01",
    "publication_year": 2004,
    "authors": "Robert Grimm; Janet Davis; Eric Lemar; Adam Macbeth; Steven Swanson; Thomas E. Anderson; Brian N. Bershad; Gaetano Borriello; Steven D. Gribble; David Wetherall",
    "corresponding_authors": "",
    "abstract": "Pervasive computing provides an attractive vision for the future of computing. Computational power will be available everywhere. Mobile and stationary devices will dynamically connect and coordinate to seamlessly help people in accomplishing their tasks. For this vision to become a reality, developers must build applications that constantly adapt to a highly dynamic computing environment. To make the developers' task feasible, we present a system architecture for pervasive computing, called &lt;i&gt;one.world&lt;/i&gt;. Our architecture provides an integrated and comprehensive framework for building pervasive applications. It includes services, such as discovery and migration, that help to build applications and directly simplify the task of coping with constant change. We describe our architecture and its programming model and reflect on our own and others' experiences with using it.",
    "cited_by_count": 194,
    "openalex_id": "https://openalex.org/W2148009787",
    "type": "article"
  },
  {
    "title": "Measurement and modeling of computer reliability as affected by system activity",
    "doi": "https://doi.org/10.1145/6420.6422",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "R.K. Iyer; David J. Rossetti; M. C. Hsueh",
    "corresponding_authors": "",
    "abstract": "This paper demonstrates a practical approach to the study of the failure behavior of computer systems. Particular attention is devoted to the analysis of permanent failures. A number of important techniques, which may have general applicability in both failure and workload analysis, are brought together in this presentation. These include: smeared averaging of the workload data, clustering of like failures, and joint analysis of workload and failures. Approximately 17 percent of all failures affecting the CPU were estimated to be permanent. The manifestation of a permanent failure was found to be strongly correlated with the level and type of workload prior to the failure. Although, in strict terms, the results only relate to the manifestation of permanent failures and not to their occurrence, there are strong indications that permanent failures are both caused and discovered by increased activity. More measurements and experiments are necessary to determine their respective contributions to the measured workload/failure relationship.",
    "cited_by_count": 193,
    "openalex_id": "https://openalex.org/W2068171717",
    "type": "article"
  },
  {
    "title": "The string-to-string correction problem with block moves",
    "doi": "https://doi.org/10.1145/357401.357404",
    "publication_date": "1984-11-01",
    "publication_year": 1984,
    "authors": "Walter F. Tichy",
    "corresponding_authors": "Walter F. Tichy",
    "abstract": "article Free Access Share on The string-to-string correction problem with block moves Author: Walter F. Tichy Department of Computer Science, Purdue University, West Lafayette, IN Department of Computer Science, Purdue University, West Lafayette, INView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 4Nov. 1984 pp 309–321https://doi.org/10.1145/357401.357404Published:01 November 1984Publication History 147citation2,109DownloadsMetricsTotal Citations147Total Downloads2,109Last 12 Months84Last 6 weeks15 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 193,
    "openalex_id": "https://openalex.org/W2151325537",
    "type": "article"
  },
  {
    "title": "The S2E Platform",
    "doi": "https://doi.org/10.1145/2110356.2110358",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Vitaly Chipounov; Volodymyr Kuznetsov; George Candea",
    "corresponding_authors": "",
    "abstract": "This article presents S 2 E, a platform for analyzing the properties and behavior of software systems, along with its use in developing tools for comprehensive performance profiling, reverse engineering of proprietary software, and automated testing of kernel-mode and user-mode binaries. Conceptually, S 2 E is an automated path explorer with modular path analyzers: the explorer uses a symbolic execution engine to drive the target system down all execution paths of interest, while analyzers measure and/or check properties of each such path. S 2 E users can either combine existing analyzers to build custom analysis tools, or they can directly use S 2 E’s APIs. S 2 E’s strength is the ability to scale to large systems, such as a full Windows stack, using two new ideas: selective symbolic execution , a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and execution consistency models , a way to make principled performance/accuracy trade-offs during analysis. These techniques give S 2 E three key abilities: to simultaneously analyze entire families of execution paths instead of just one execution at a time; to perform the analyses in-vivo within a real software stack---user programs, libraries, kernel, drivers, etc.---instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.",
    "cited_by_count": 191,
    "openalex_id": "https://openalex.org/W2150990339",
    "type": "article"
  },
  {
    "title": "The STAMPede approach to thread-level speculation",
    "doi": "https://doi.org/10.1145/1082469.1082471",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "J. Gregory Steffan; Christopher B. Colohan; Antonia Zhai; Todd C. Mowry",
    "corresponding_authors": "",
    "abstract": "Multithreaded processor architectures are becoming increasingly commonplace: many current and upcoming designs support chip multiprocessing, simultaneous multithreading, or both. While it is relatively straightforward to use these architectures to improve the throughput of a multithreaded or multiprogrammed workload, the real challenge is how to easily create parallel software to allow single programs to effectively exploit all of this raw performance potential. One promising technique for overcoming this problem is Thread-Level Speculation (TLS) , which enables the compiler to optimistically create parallel threads despite uncertainty as to whether those threads are actually independent. In this article, we propose and evaluate a design for supporting TLS that seamlessly scales both within a chip and beyond because it is a straightforward extension of write-back invalidation-based cache coherence (which itself scales both up and down). Our experimental results demonstrate that our scheme performs well on single-chip multiprocessors where the first level caches are either private or shared. For our private-cache design, the program performance of two of 13 general purpose applications studied improves by 86% and 56%, four others by more than 8%, and an average across all applications of 16%---confirming that TLS is a promising way to exploit the naturally-multithreaded processing resources of future computer systems.",
    "cited_by_count": 187,
    "openalex_id": "https://openalex.org/W2037462607",
    "type": "article"
  },
  {
    "title": "Improving round-trip time estimates in reliable transport protocols",
    "doi": "https://doi.org/10.1145/118544.118549",
    "publication_date": "1991-11-01",
    "publication_year": 1991,
    "authors": "P. Karn; Craig Partridge",
    "corresponding_authors": "",
    "abstract": "article Free AccessImproving round-trip time estimates in reliable transport protocols Authors: Phil Karn Bell Communications Research, Morristown, NJ Bell Communications Research, Morristown, NJView Profile , Craig Partridge Bolt Beranek and Newman, Inc., Cambridge, MA Bolt Beranek and Newman, Inc., Cambridge, MAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 4Nov. 1991 pp 364–373https://doi.org/10.1145/118544.118549Published:01 November 1991Publication History 94citation1,450DownloadsMetricsTotal Citations94Total Downloads1,450Last 12 Months87Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W1984707196",
    "type": "article"
  },
  {
    "title": "A relational approach to monitoring complex systems",
    "doi": "https://doi.org/10.1145/42186.42323",
    "publication_date": "1988-05-01",
    "publication_year": 1988,
    "authors": "Richard T. Snodgrass",
    "corresponding_authors": "Richard T. Snodgrass",
    "abstract": "Monitoring is an essential part of many program development tools, and plays a central role in debugging, optimization, status reporting, and reconfiguration. Traditional monitoring techniques are inadequate when monitoring complex systems such as multiprocessors or distributed systems. A new approach is described in which a historical database forms the conceptual basis for the information processed by the monitor. This approach permits advances in specifying the low-level data collection, specifying the analysis of the collected data, performing the analysis, and displaying the results. Two prototype implementations demonstrate the feasibility of the approach.",
    "cited_by_count": 184,
    "openalex_id": "https://openalex.org/W2095748364",
    "type": "article"
  },
  {
    "title": "Backtracking intrusions",
    "doi": "https://doi.org/10.1145/1047915.1047918",
    "publication_date": "2005-02-02",
    "publication_year": 2005,
    "authors": "Samuel T. King; Peter M. Chen",
    "corresponding_authors": "",
    "abstract": "Analyzing intrusions today is an arduous, largely manual task because system administrators lack the information and tools needed to understand easily the sequence of steps that occurred in an attack. The goal of BackTracker is to identify automatically potential sequences of steps that occurred in an intrusion. Starting with a single detection point (e.g., a suspicious file), BackTracker identifies files and processes that could have affected that detection point and displays chains of events in a dependency graph. We use BackTracker to analyze several real attacks against computers that we set up as honeypots. In each case, BackTracker is able to highlight effectively the entry point used to gain access to the system and the sequence of steps from that entry point to the point at which we noticed the intrusion. The logging required to support BackTracker added 9% overhead in running time and generated 1.2 GB per day of log data for an operating-system intensive workload.",
    "cited_by_count": 180,
    "openalex_id": "https://openalex.org/W2293069947",
    "type": "article"
  },
  {
    "title": "MIDDLE-R",
    "doi": "https://doi.org/10.1145/1113574.1113576",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Marta Patiño-Martı́nez; Ricardo Jiménez; Bettina Kemme; Gustavo Alonso",
    "corresponding_authors": "",
    "abstract": "The widespread use of clusters and Web farms has increased the importance of data replication. In this article, we show how to implement consistent and scalable data replication at the middleware level. We do this by combining transactional concurrency control with group communication primitives. The article presents different replication protocols, argues their correctness, describes their implementation as part of a generic middleware, Middle-R, and proves their feasibility with an extensive performance evaluation. The solution proposed is well suited for a variety of applications including Web farms and distributed object platforms.",
    "cited_by_count": 174,
    "openalex_id": "https://openalex.org/W1991152532",
    "type": "article"
  },
  {
    "title": "The S/Net's Linda kernel",
    "doi": "https://doi.org/10.1145/214419.214420",
    "publication_date": "1986-05-01",
    "publication_year": 1986,
    "authors": "Nicholas Carriero; David Gelernter",
    "corresponding_authors": "",
    "abstract": "Linda is a parallel programming language that differs from other parallel languages in its simplicity and in its support for distributed data structures. The S/Net is a multicomputer, designed and built at AT&amp;T Bell Laboratories, that is based on a fast, word-parallel bus interconnect. We describe the Linda-supporting communication kernel we have implemented on the S/Net. The implementation suggests that Linda's unusual shared-memory-like communication primitives can be made to run well in the absence of physically shared memory; the simplicity of the language and of our implementation's logical structure suggest that similar Linda implementations might readily be constructed on related architectures. We outline the language, and programming methodologies based on distributed data structures; we then describe the implementation, and the performance both of the Linda primitives themselves and of a simple S/Net-Linda matrix-multiplication program designed to exercise them.",
    "cited_by_count": 174,
    "openalex_id": "https://openalex.org/W2156762091",
    "type": "article"
  },
  {
    "title": "A mechanistic performance model for superscalar out-of-order processors",
    "doi": "https://doi.org/10.1145/1534909.1534910",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Stijn Eyerman; Lieven Eeckhout; Tejas S. Karkhanis; James E. Smith",
    "corresponding_authors": "",
    "abstract": "A mechanistic model for out-of-order superscalar processors is developed and then applied to the study of microarchitecture resource scaling. The model divides execution time into intervals separated by disruptive miss events such as branch mispredictions and cache misses. Each type of miss event results in characterizable performance behavior for the execution time interval. By considering an interval's type and length (measured in instructions), execution time can be predicted for the interval. Overall execution time is then determined by aggregating the execution time over all intervals. The mechanistic model provides several advantages over prior modeling approaches, and, when estimating performance, it differs from detailed simulation of a 4-wide out-of-order processor by an average of 7%. The mechanistic model is applied to the general problem of resource scaling in out-of-order superscalar processors. First, we use the model to determine size relationships among microarchitecture structures in a balanced processor design. Second, we use the mechanistic model to study scaling of both pipeline depth and width in balanced processor designs. We corroborate previous results in this area and provide new results. For example, we show that at optimal design points, the pipeline depth times the square root of the processor width is nearly constant. Finally, we consider the behavior of unbalanced, overprovisioned processor designs based on insight gained from the mechanistic model. We show that in certain situations an overprovisioned processor may lead to improved overall performance. Designs where a processor's dispatch width is wider than its issue width are of particular interest.",
    "cited_by_count": 172,
    "openalex_id": "https://openalex.org/W1986491730",
    "type": "article"
  },
  {
    "title": "A security model for military message systems",
    "doi": "https://doi.org/10.1145/989.991",
    "publication_date": "1984-08-01",
    "publication_year": 1984,
    "authors": "Carl E. Landwehr; Constance Heitmeyer; John McLean",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A security model for military message systems Authors: Carl E. Landwehr Naval Research Laboratory Naval Research LaboratoryView Profile , Constance L. Heitmeyer Naval Research Laboratory Naval Research LaboratoryView Profile , John McLean Naval Research Laboratory Naval Research LaboratoryView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 3Aug. 1984 pp 198–222https://doi.org/10.1145/989.991Published:01 August 1984Publication History 137citation1,819DownloadsMetricsTotal Citations137Total Downloads1,819Last 12 Months48Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 169,
    "openalex_id": "https://openalex.org/W2103078852",
    "type": "article"
  },
  {
    "title": "Decoupled access/execute computer architectures",
    "doi": "https://doi.org/10.1145/357401.357403",
    "publication_date": "1984-11-01",
    "publication_year": 1984,
    "authors": "James E. Smith",
    "corresponding_authors": "James E. Smith",
    "abstract": "article Free AccessDecoupled access/execute computer architectures Author: James E. Smith Department of Electrical and Computer Engineering, University of Wisconsin, Madison, WI Department of Electrical and Computer Engineering, University of Wisconsin, Madison, WIView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 4Nov. 1984 pp 289–308https://doi.org/10.1145/357401.357403Published:01 November 1984Publication History 134citation1,916DownloadsMetricsTotal Citations134Total Downloads1,916Last 12 Months155Last 6 weeks26 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 167,
    "openalex_id": "https://openalex.org/W1969648707",
    "type": "article"
  },
  {
    "title": "801 storage: architecture and programming",
    "doi": "https://doi.org/10.1145/35037.42270",
    "publication_date": "1988-02-01",
    "publication_year": 1988,
    "authors": "Albert Chang; Mark Mergen",
    "corresponding_authors": "",
    "abstract": "Based on novel architecture, the 801 minicomputer project has developed a low-level storage manager that can significantly simplify storage programming in subsystems and applications. The storage manager embodies three ideas: (1) large virtual storage , to contain all temporary data and permanent files for the active programs; (2) the innovation of database storage , which has implicit properties of access serializability and atomic update, similar to those of database transaction systems; and (3) access to all storage, including files, by the usual operations and types of a high-level programming language . The IBM RT PC implements the hardware architecture necessary for these storage facilities in its storage controller (MMU). The storage manager and language elements required, as well as subsystems and applications that use them, have been implemented and studied in a prototype operating system called CPR, that runs on the RT PC. Low cost and good performance are achieved in both hardware and software. The design is intended to be extensible across a wide performance/cost spectrum.",
    "cited_by_count": 164,
    "openalex_id": "https://openalex.org/W2057811740",
    "type": "article"
  },
  {
    "title": "Contention-Aware Scheduling on Multicore Systems",
    "doi": "https://doi.org/10.1145/1880018.1880019",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Sergey Blagodurov; Sergey Zhuravlev; Alexandra Fedorova",
    "corresponding_authors": "",
    "abstract": "Contention for shared resources on multicore processors remains an unsolved problem in existing systems despite significant research efforts dedicated to this problem in the past. Previous solutions focused primarily on hardware techniques and software page coloring to mitigate this problem. Our goal is to investigate how and to what extent contention for shared resource can be mitigated via thread scheduling. Scheduling is an attractive tool, because it does not require extra hardware and is relatively easy to integrate into the system. Our study is the first to provide a comprehensive analysis of contention-mitigating techniques that use only scheduling. The most difficult part of the problem is to find a classification scheme for threads, which would determine how they affect each other when competing for shared resources. We provide a comprehensive analysis of such classification schemes using a newly proposed methodology that enables to evaluate these schemes separately from the scheduling algorithm itself and to compare them to the optimal. As a result of this analysis we discovered a classification scheme that addresses not only contention for cache space, but contention for other shared resources, such as the memory controller, memory bus and prefetching hardware. To show the applicability of our analysis we design a new scheduling algorithm, which we prototype at user level, and demonstrate that it performs within 2% of the optimal. We also conclude that the highest impact of contention-aware scheduling techniques is not in improving performance of a workload as a whole but in improving quality of service or performance isolation for individual applications and in optimizing system energy consumption.",
    "cited_by_count": 154,
    "openalex_id": "https://openalex.org/W1964225254",
    "type": "article"
  },
  {
    "title": "Performance of the VAX-11/780 translation buffer",
    "doi": "https://doi.org/10.1145/214451.214455",
    "publication_date": "1985-02-01",
    "publication_year": 1985,
    "authors": "Douglas W. Clark; Joel Emer",
    "corresponding_authors": "",
    "abstract": "A virtual-address translation buffer (TB) is a hardware cache of recently used virtual-to-physical address mappings. The authors present the results of a set of measurements and simulations of translation buffer performance in the VAX-11/780. Two different hardware monitors were attached to VAX-11/780 computers, and translation buffer behavior was measured. Measurements were made under normal time-sharing use and while running reproducible synthetic time-sharing work loads. Reported measurements include the miss ratios of data and instruction references, the rate of TB invalidations due to context switches, and the amount of time taken to service TB misses. Additional hardware measurements were made with half the TB disabled. Trace-driven simulations of several programs were also run; the traces captured system activity as well as user-mode execution. Several variants of the 11/780 TB structure were simulated.",
    "cited_by_count": 154,
    "openalex_id": "https://openalex.org/W2023327539",
    "type": "article"
  },
  {
    "title": "The WaveScalar architecture",
    "doi": "https://doi.org/10.1145/1233307.1233308",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Steven Swanson; Andrew Schwerin; Martha Mercaldi; Andrew Petersen; Andrew Putnam; Ken Michelson; Mark Oskin; Susan J. Eggers",
    "corresponding_authors": "",
    "abstract": "Silicon technology will continue to provide an exponential increase in the availability of raw transistors. Effectively translating this resource into application performance, however, is an open challenge that conventional superscalar designs will not be able to meet. We present WaveScalar as a scalable alternative to conventional designs. WaveScalar is a dataflow instruction set and execution model designed for scalable, low-complexity/high-performance processors. Unlike previous dataflow machines, WaveScalar can efficiently provide the sequential memory semantics that imperative languages require. To allow programmers to easily express parallelism, WaveScalar supports pthread-style, coarse-grain multithreading and dataflow-style, fine-grain threading. In addition, it permits blending the two styles within an application, or even a single function. To execute WaveScalar programs, we have designed a scalable, tile-based processor architecture called the WaveCache. As a program executes, the WaveCache maps the program's instructions onto its array of processing elements (PEs). The instructions remain at their processing elements for many invocations, and as the working set of instructions changes, the WaveCache removes unused instructions and maps new ones in their place. The instructions communicate directly with one another over a scalable, hierarchical on-chip interconnect, obviating the need for long wires and broadcast communication. This article presents the WaveScalar instruction set and evaluates a simulated implementation based on current technology. For single-threaded applications, the WaveCache achieves performance on par with conventional processors, but in less area. For coarse-grain threaded applications the WaveCache achieves nearly linear speedup with up to 64 threads and can sustain 7--14 multiply-accumulates per cycle on fine-grain threaded versions of well-known kernels. Finally, we apply both styles of threading to equake from Spec2000 and speed it up by 9x compared to the serial version.",
    "cited_by_count": 148,
    "openalex_id": "https://openalex.org/W2142501475",
    "type": "article"
  },
  {
    "title": "QoS-Aware scheduling in heterogeneous datacenters with paragon",
    "doi": "https://doi.org/10.1145/2556583",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Christina Delimitrou; Christos Kozyrakis",
    "corresponding_authors": "",
    "abstract": "Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty of matching applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online, and do not scale beyond a few applications. We present Paragon, an online and scalable DC scheduler that is heterogeneity- and interference-aware. Paragon is derived from robust analytical methods, and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown incoming workload with respect to heterogeneity and interference in multiple shared resources. It does so by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. After the initial application placement, Paragon monitors application behavior and adjusts the scheduling decisions at runtime to avoid performance degradations. Additionally, we design ARQ, a multiclass admission control protocol that constrains application waiting time. ARQ queues applications in separate classes based on the type of resources they need and avoids long queueing delays for easy-to-satisfy workloads in highly-loaded scenarios. Paragon scales to tens of thousands of servers and applications with marginal scheduling overheads in terms of time or state. We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious, and least-loaded schedulers only provide similar guarantees for 14%, 11%, and 3% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.",
    "cited_by_count": 146,
    "openalex_id": "https://openalex.org/W2015099024",
    "type": "article"
  },
  {
    "title": "The Next 700 BFT Protocols",
    "doi": "https://doi.org/10.1145/2658994",
    "publication_date": "2015-01-20",
    "publication_year": 2015,
    "authors": "Pierre-Louis Aublin; Rachid Guerraoui; Nikola Knežević; Vivien Quéma; Marko Vukolić",
    "corresponding_authors": "",
    "abstract": "We present Abstract (ABortable STate mAChine replicaTion), a new abstraction for designing and reconfiguring generalized replicated state machines that are, unlike traditional state machines, allowed to abort executing a client’s request if “something goes wrong.” Abstract can be used to considerably simplify the incremental development of efficient Byzantine fault-tolerant state machine replication ( BFT ) protocols that are notorious for being difficult to develop. In short, we treat a BFT protocol as a composition of Abstract instances. Each instance is developed and analyzed independently and optimized for specific system conditions. We illustrate the power of Abstract through several interesting examples. We first show how Abstract can yield benefits of a state-of-the-art BFT protocol in a less painful and error-prone manner. Namely, we develop AZyzzyva , a new protocol that mimics the celebrated best-case behavior of Zyzzyva using less than 35% of the Zyzzyva code. To cover worst-case situations, our abstraction enables one to use in AZyzzyva any existing BFT protocol. We then present Aliph , a new BFT protocol that outperforms previous BFT protocols in terms of both latency (by up to 360%) and throughput (by up to 30%). Finally, we present R-Aliph , an implementation of Aliph that is robust , that is, whose performance degrades gracefully in the presence of Byzantine replicas and Byzantine clients.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2124037649",
    "type": "article"
  },
  {
    "title": "Approximate Storage in Solid-State Memories",
    "doi": "https://doi.org/10.1145/2644808",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "Adrian Sampson; Jacob Nelson; Karin Strauß; Luís Ceze",
    "corresponding_authors": "",
    "abstract": "Memories today expose an all-or-nothing correctness model that incurs significant costs in performance, energy, area, and design complexity. But not all applications need high-precision storage for all of their data structures all of the time. This article proposes mechanisms that enable applications to store data approximately and shows that doing so can improve the performance, lifetime, or density of solid-state memories. We propose two mechanisms. The first allows errors in multilevel cells by reducing the number of programming pulses used to write them. The second mechanism mitigates wear-out failures and extends memory endurance by mapping approximate data onto blocks that have exhausted their hardware error correction resources. Simulations show that reduced-precision writes in multilevel phase-change memory cells can be 1.7 × faster on average and using failed blocks can improve array lifetime by 23% on average with quality loss under 10%.",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W2053709612",
    "type": "article"
  },
  {
    "title": "Arrakis",
    "doi": "https://doi.org/10.1145/2812806",
    "publication_date": "2015-11-02",
    "publication_year": 2015,
    "authors": "Simon Peter; Jialin Li; Irene Zhang; Dan R. K. Ports; Doug Woos; Arvind Krishnamurthy; Thomas E. Anderson; Timothy Roscoe",
    "corresponding_authors": "",
    "abstract": "Recent device hardware trends enable a new approach to the design of network server operating systems. In a traditional operating system, the kernel mediates access to device hardware by server applications to enforce process isolation as well as network and disk security. We have designed and implemented a new operating system, Arrakis, that splits the traditional role of the kernel in two. Applications have direct access to virtualized I/O devices, allowing most I/O operations to skip the kernel entirely, while the kernel is re-engineered to provide network and disk protection without kernel mediation of every operation. We describe the hardware and software changes needed to take advantage of this new abstraction, and we illustrate its power by showing improvements of 2 to 5 × in latency and 9 × throughput for a popular persistent NoSQL store relative to a well-tuned Linux implementation.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2265366104",
    "type": "article"
  },
  {
    "title": "Ryoan",
    "doi": "https://doi.org/10.1145/3231594",
    "publication_date": "2017-11-30",
    "publication_year": 2017,
    "authors": "Tyler Hunt; Zhiting Zhu; Yuanzhong Xu; Simon Peter; Emmett Witchel",
    "corresponding_authors": "",
    "abstract": "Users of modern data-processing services such as tax preparation or genomic screening are forced to trust them with data that the users wish to keep secret. Ryoan 1 protects secret data while it is processed by services that the data owner does not trust. Accomplishing this goal in a distributed setting is difficult, because the user has no control over the service providers or the computational platform. Confining code to prevent it from leaking secrets is notoriously difficult, but Ryoan benefits from new hardware and a request-oriented data model. Ryoan provides a distributed sandbox, leveraging hardware enclaves (e.g., Intel’s software guard extensions (SGX) [40]) to protect sandbox instances from potentially malicious computing platforms. The protected sandbox instances confine untrusted data-processing modules to prevent leakage of the user’s input data. Ryoan is designed for a request-oriented data model, where confined modules only process input once and do not persist state about the input. We present the design and prototype implementation of Ryoan and evaluate it on a series of challenging problems including email filtering, health analysis, image processing and machine translation.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2904364493",
    "type": "article"
  },
  {
    "title": "IO-Lite",
    "doi": "https://doi.org/10.1145/332799.332895",
    "publication_date": "2000-02-01",
    "publication_year": 2000,
    "authors": "Vivek S. Pai; Peter Druschel; Willy Zwaenepoel",
    "corresponding_authors": "",
    "abstract": "This article presents the design, implementation, and evaluation of IO -Lite, a unified I/O buffering and caching system for general-purpose operating systems. IO-Lite unifies all buffering and caching in the system, to the extent permitted by the hardware. In particular, it allows applications, the interprocess communication system, the file system, the file cache, and the network subsystem to safely and concurrently share a single physical copy of the data. Protection and security are maintained through a combination of access control and read-only sharing. IO-Lite eliminates all copying and multiple buffering of I/O data, and enables various cross-subsystem optimizations. Experiments with a Web server show performance improvements between 40 and 80% on real workloads as a result of IO-Lite.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W2045808511",
    "type": "article"
  },
  {
    "title": "The Zebra striped network file system",
    "doi": "https://doi.org/10.1145/210126.210131",
    "publication_date": "1995-08-01",
    "publication_year": 1995,
    "authors": "John H. Hartman; John K. Ousterhout",
    "corresponding_authors": "",
    "abstract": "Zebra is a network file system that increases throughput by striping the file data across multiple servers. Rather than striping each file separately, Zebra forms all the new data from each client into a single stream, which it then stripes using an approach similar to a log-structured file system. This provides high performance for writes of small files as well as for reads and writes of large files. Zebra also writes parity information in each stripe in the style of RAID disk arrays; this increases storage costs slightly, but allows the system to continue operation while a single storage server is unavailable. A prototype implementation of Zebra, built in the Sprite operating system, provides 4–5 times the throughput of the standard Sprite file system or NFS for large files and a 15–300% improvement for writing small files.",
    "cited_by_count": 178,
    "openalex_id": "https://openalex.org/W2613324619",
    "type": "article"
  },
  {
    "title": "Disk-directed I/O for MIMD multiprocessors",
    "doi": "https://doi.org/10.1145/244764.244766",
    "publication_date": "1997-02-01",
    "publication_year": 1997,
    "authors": "David Kotz",
    "corresponding_authors": "David Kotz",
    "abstract": "Many scientific applications that run on today's multiprocessors, such as weather forecasting and seismic analysis, are bottlenecked by their file-I/O needs. Even if the multiprocessor is configured with sufficient I/O hardware, the file system software often fails to provide the available bandwidth to the application. Although libraries and enhanced file system interfaces can make a significant improvement, we believe that fundamental changes are needed in the file server software. We propose a new technique, disk-directed I/O, to allow the disk servers to determine the flow of data for maximum performance. Our simulations show that tremendous performance gains are possible both for simple reads and writes and for an out-of-core application. Indeed, our disk-directed I/O technique provided consistent high performance that was largely independent of data distribution and obtained up to 93% of peak disk bandwidth. It was as much as 18 times faster than either a typical parallel file system or a two-phase-I/O library.",
    "cited_by_count": 177,
    "openalex_id": "https://openalex.org/W1978513924",
    "type": "article"
  },
  {
    "title": "<i>Firmato</i>",
    "doi": "https://doi.org/10.1145/1035582.1035583",
    "publication_date": "2004-11-01",
    "publication_year": 2004,
    "authors": "Yair Bartal; Alain Mayer; Kobbi Nissim; Avishai Wool",
    "corresponding_authors": "",
    "abstract": "In recent years packet-filtering firewalls have seen some impressive technological advances (e.g., stateful inspection, transparency, performance, etc.) and wide-spread deployment. In contrast, firewall and security &lt;i&gt;management&lt;/i&gt; technology is lacking. In this paper we present &lt;i&gt;Firmato&lt;/i&gt;, a firewall management toolkit, with the following distinguishing properties and components: (1) an entity-relationship model containing, in a unified form, global knowledge of the security policy and of the network topology; (2) a model definition language, which we use as an interface to define an instance of the entity-relationship model; (3) a model compiler, translating the global knowledge of the model into firewall-specific configuration files; and (4) a graphical firewall rule illustrator. We implemented a prototype of our toolkit to work with several commercially available firewall products. This prototype was used to control an operational firewall for several months. We believe that our approach is an important step toward streamlining the process of configuring and managing firewalls, especially in complex, multi-firewall installations.",
    "cited_by_count": 168,
    "openalex_id": "https://openalex.org/W2014254858",
    "type": "article"
  },
  {
    "title": "Secure program partitioning",
    "doi": "https://doi.org/10.1145/566340.566343",
    "publication_date": "2002-08-01",
    "publication_year": 2002,
    "authors": "Steve Zdancewic; Lantian Zheng; Nathaniel Nystrom; Andrew C. Myers",
    "corresponding_authors": "",
    "abstract": "This paper presents secure program partitioning, a language-based technique for protecting confidential data during computation in distributed systems containing mutually untrusted hosts. Confidentiality and integrity policies can be expressed by annotating programs with security types that constrain information flow; these programs can then be partitioned automatically to run securely on heterogeneously trusted hosts. The resulting communicating subprograms collectively implement the original program, yet the system as a whole satisfies the security requirements of participating principals without requiring a universally trusted host machine. The experience in applying this methodology and the performance of the resulting distributed code suggest that this is a promising way to obtain secure distributed computation.",
    "cited_by_count": 168,
    "openalex_id": "https://openalex.org/W2148144728",
    "type": "article"
  },
  {
    "title": "Managing battery lifetime with energy-aware adaptation",
    "doi": "https://doi.org/10.1145/986533.986534",
    "publication_date": "2004-05-01",
    "publication_year": 2004,
    "authors": "Jason Flinn; Mahadev Satyanarayanan",
    "corresponding_authors": "",
    "abstract": "We demonstrate that a collaborative relationship between the operating system and applications can be used to meet user-specified goals for battery duration. We first describe a novel profiling-based approach for accurately measuring application and system energy consumption. We then show how applications can dynamically modify their behavior to conserve energy. We extend the Linux operating system to yield battery lifetimes of user-specified duration. By monitoring energy supply and demand and by maintaining a history of application energy use, the approach can dynamically balance energy conservation and application quality. Our evaluation shows that this approach can meet goals that extend battery life by as much as 30%.",
    "cited_by_count": 166,
    "openalex_id": "https://openalex.org/W2085357353",
    "type": "article"
  },
  {
    "title": "Techniques for reducing consistency-related communication in distributed shared-memory systems",
    "doi": "https://doi.org/10.1145/210126.210127",
    "publication_date": "1995-08-01",
    "publication_year": 1995,
    "authors": "John B. Carter; John K. Bennett; Willy Zwaenepoel",
    "corresponding_authors": "",
    "abstract": "Distributed shared memory (DSM) is an abstraction of shared memory on a distributed-memory machine. Hardware DSM systems support this abstraction at the architecture level; software DSM systems support the abstraction within the runtime system. One of the key problems in building an efficient software DSM system is to reduce the amount of communication needed to keep the distributed memories consistent. In this article we present four techniques for doing so: software release consistency; multiple consistency protocols; write-shared protocols; and an update-with-timeout mechanism. These techniques have been implemented in the Munin DSM system. We compare the performance of seven Munin application programs: first to their performance when implemented using message passing, and then to their performance when running on a conventional software DSM system that does not embody the preceding techniques. On a 16-processor cluster of workstations, Munin's performance is within 5% of message passing for four out of the seven applications. For the other three, performance is within 29 to 33%. Detailed analysis of two of these three applications indicates that the addition of a function-shipping capability would bring their performance to within 7% of the message-passing performance. Compared to a conventional DSM system, Munin achieves performance improvements ranging from a few to several hundred percent, depending on the application.",
    "cited_by_count": 165,
    "openalex_id": "https://openalex.org/W2059736952",
    "type": "article"
  },
  {
    "title": "Ordered and reliable multicast communication",
    "doi": "https://doi.org/10.1145/128738.128741",
    "publication_date": "1991-08-01",
    "publication_year": 1991,
    "authors": "Héctor García-Molina; Annemarie Spauster",
    "corresponding_authors": "",
    "abstract": "article Free AccessOrdered and reliable multicast communication Authors: Hector Garcia-Molina Princeton University Princeton UniversityView Profile , AnneMarie Spauster Smith College Smith CollegeView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 3Aug. 1991 pp 242–271https://doi.org/10.1145/128738.128741Published:01 August 1991Publication History 127citation1,110DownloadsMetricsTotal Citations127Total Downloads1,110Last 12 Months182Last 6 weeks67 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 165,
    "openalex_id": "https://openalex.org/W2080551325",
    "type": "article"
  },
  {
    "title": "Cooperative shared memory",
    "doi": "https://doi.org/10.1145/161541.161544",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Mark D. Hill; James R. Larus; Steven K. Reinhardt; David A. Wood",
    "corresponding_authors": "",
    "abstract": "We believe the paucity of massively parallel, shared-memory machines follows from the lack of a shared-memory programming performance model that can inform programmers of the cost of operations (so they can avoid expensive ones) and can tell hardware designers which cases are common (so they can build simple hardware to optimize them). Cooperative shared memory, our approach to shared-memory design, addresses this problem. Our initial implementation of cooperative shared memory uses a simple programming model, called Check-In/Check-Out (CICO), in conjunction with even simpler hardware, called Dir 1 SW. In CICO, programs bracket uses of shared data with a check_in directive terminating the expected use of the data. A cooperative prefetch directive helps hide communication latency. Dir 1 SW is a minimal directory protocol that adds little complexity to message-passing hardware, but efficiently supports programs written within the CICO model.",
    "cited_by_count": 158,
    "openalex_id": "https://openalex.org/W2081644593",
    "type": "article"
  },
  {
    "title": "Debugging heterogeneous distributed systems using event-based models of behavior",
    "doi": "https://doi.org/10.1145/200912.200913",
    "publication_date": "1995-02-01",
    "publication_year": 1995,
    "authors": "Peter Bates",
    "corresponding_authors": "Peter Bates",
    "abstract": "We describe a high-level debugging approach, Event-Based Behavioral Abstraction (EBBA), in which debugging is treated as a process of creating models of expected program behaviors and comparing these to the actual behaviors exhibited by the program. The use of EBBA techniques can enhance debugging-tool transparency, reduce latency and uncertainty for fundamental debugging activities, and accommodate diverse, heterogeneous architectures. Using events and behavior models as a basic mechanism provides a uniform view of heterogeneous systems and enables analysis to be performed in well-defined ways. Their use also enables EBBA users to extend and reuse knowledge gained in solving previous problems to new situations. We describe our behavior-modeling algorithm that matches actual behavior to models and automates many behavior analysis steps. The algorithm matches behavior in as many ways as possible and resolves these to return the best match to the user. It deals readily with partial behavior matches and incomplete information. In particular, we describe a tool set we have built. The tool set has been used to investigate the behavior of a wide range of programs. The tools are modular and can be distributed readily throughout a system.",
    "cited_by_count": 156,
    "openalex_id": "https://openalex.org/W2062613080",
    "type": "article"
  },
  {
    "title": "Neural methods for dynamic branch prediction",
    "doi": "https://doi.org/10.1145/571637.571639",
    "publication_date": "2002-10-07",
    "publication_year": 2002,
    "authors": "Daniel A. Jiménez; Calvin Lin",
    "corresponding_authors": "",
    "abstract": "This article presents a new and highly accurate method for branch prediction. The key idea is to use one of the simplest possible neural methods, the perceptron, as an alternative to the commonly used two-bit counters. The source of our predictor's accuracy is its ability to use long history lengths, because the hardware resources for our method scale linearly, rather than exponentially, with the history length. We describe two versions of perceptron predictors, and we evaluate these predictors with respect to five well-known predictors. We show that for a 4 KB hardware budget, a simple version of our method that uses a global history achieves a misprediction rate of 4.6% on the SPEC 2000 integer benchmarks, an improvement of 26% over gshare . We also introduce a global/local version of our predictor that is 14% more accurate than the McFarling-style hybrid predictor of the Alpha 21264. We show that for hardware budgets of up to 256 KB, this global/local perceptron predictor is more accurate than Evers' multicomponent predictor, so we conclude that ours is the most accurate dynamic predictor currently available. To explore the feasibility of our ideas, we provide a circuit-level design of the perceptron predictor and describe techniques that allow our complex predictor to operate quickly. Finally, we show how the relatively complex perceptron predictor can be used in modern CPUs by having it override a simpler, quicker Smith predictor, providing IPC improvements of 15.8% over gshare and 5.7% over the McFarling hybrid predictor.",
    "cited_by_count": 155,
    "openalex_id": "https://openalex.org/W2025106479",
    "type": "article"
  },
  {
    "title": "File access performance of diskless workstations",
    "doi": "https://doi.org/10.1145/6420.6423",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Edward D. Lazowska; John Zahorjan; David R. Cheriton; Willy Zwaenepoel",
    "corresponding_authors": "",
    "abstract": "This paper studies the performance of single-user workstations that access files remotely over a local area network. From the environmental, economic, and administrative points of view, workstations that are diskless or that have limited secondary storage are desirable at the present time. Even with changing technology, access to shared data will continue to be important. It is likely that some performance penalty must be paid for remote rather than local file access. Our objectives are to assess this penalty and to explore a number of design alternatives that can serve to minimize it. Our approach is to use the results of measurement experiments to parameterize queuing network performance models. These models then are used to assess performance under load and to evahrate design alternatives. The major conclusions of our study are: (1) A system of diskless workstations with a shared file server can have satisfactory performance. By this, we mean performance comparable to that of a local disk in the lightly loaded case, and the ability to support substantial numbers of client workstations without significant degradation. As with any shared facility, good design is necessary to minimize queuing delays under high load. (2) The key to efficiency is protocols that allow volume transfers at every interface (e.g., between client and server, and between disk and memory at the server) and at every level (e.g., between client and server at the level of logical request/response and at the level of local area network packet size). However, the benefits of volume transfers are limited to moderate sizes (8-16 kbytes) by several factors. (3) From a performance point of view, augmenting the capabilities of the shared file server may be more cost effective than augmenting the capabilities of the client workstations. (4) Network contention should not be a performance problem for a lo-Mbit network and 100 active workstations in a software development environment.",
    "cited_by_count": 151,
    "openalex_id": "https://openalex.org/W2108783024",
    "type": "article"
  },
  {
    "title": "A digital multisignature scheme using bijective public-key cryptosystems",
    "doi": "https://doi.org/10.1145/48012.48246",
    "publication_date": "1988-11-01",
    "publication_year": 1988,
    "authors": "Tatsuaki Okamoto",
    "corresponding_authors": "Tatsuaki Okamoto",
    "abstract": "A new digital multisignature scheme using bijective public-key cryptosystems that overcomes the problems of previous signature schemes used for multisignatures is proposed. The principal features of this scheme are (1) the length of a multisignature message is nearly equivalent to that for a singlesignature message; (2) by using a one-way hash function, multisignature generation and verification are processed in an efficient manner; (3) the order of signing is not restricted; and (4) this scheme can be constructed on any bijective public-key cryptosystem as well as the RSA scheme. In addition, it is shown that the new scheme is considered as safe as the public-key cryptosystem used in this new scheme. Some variations based on the scheme are also presented.",
    "cited_by_count": 147,
    "openalex_id": "https://openalex.org/W1970943434",
    "type": "article"
  },
  {
    "title": "The information structure of distributed mutual exclusion algorithms",
    "doi": "https://doi.org/10.1145/24068.28052",
    "publication_date": "1987-08-01",
    "publication_year": 1987,
    "authors": "Beverly A. Sanders",
    "corresponding_authors": "Beverly A. Sanders",
    "abstract": "The concept of an information structure is introduced as a unifying principle behind several of the numerous algorithms that have been proposed for the distributed mutual exclusion problem. This approach allows the development of a generalized mutual exclusion algorithm that accepts a particular information structure at initialization and realizes both known and new algorithms as special cases. Two simple performance metrics of a realized algorithm can be obtained directly from the information structure. A new failure recovery mechanism called local recovery, which requires no coordination between nodes and no additional messages beyond that needed for failure detection, is introduced.",
    "cited_by_count": 144,
    "openalex_id": "https://openalex.org/W2132780832",
    "type": "article"
  },
  {
    "title": "Generating test cases for real-time systems from logic specifications",
    "doi": "https://doi.org/10.1145/210223.210226",
    "publication_date": "1995-11-01",
    "publication_year": 1995,
    "authors": "Dino Mandrioli; Sandro Morasca; Angelo Morzenti",
    "corresponding_authors": "",
    "abstract": "We address the problem of automated derivation of functional test cases for real-time systems, by introducing techniques for generating test cases from formal specifications written in TRIO, a language that extends classical temporal logic to deal explicitly with time measures. We describe an interactive tool that has been built to implement these techniques, based on interpretation algorithms of the TRIO language. Several heuristic criteria are suggested to reduce drastically the size of the test cases that are generated. Experience in the use of the tool on real-life cases is reported.",
    "cited_by_count": 143,
    "openalex_id": "https://openalex.org/W1979248560",
    "type": "article"
  },
  {
    "title": "A continuum of disk scheduling algorithms",
    "doi": "https://doi.org/10.1145/7351.8929",
    "publication_date": "1987-01-05",
    "publication_year": 1987,
    "authors": "Robert Geist; Stephen H. Daniel",
    "corresponding_authors": "",
    "abstract": "A continuum of disk scheduling algorithms, V( R ), having endpoints V(0) = SSTF and V(1) = SCAN, is defined. V( R ) maintains a current SCAN direction (in or out) and services next the request with the smallest effective distance. The effective distance of a request that lies in the current direction is its physical distance (in cylinders) from the read/write head. The effective distance of a request in the opposite direction is its physical distance plus R x (total number of cylinders on the disk). By use of simulation methods, it is shown that this definitional continuum also provides a continuum in performance, both with respect to the mean and with respect to the standard deviation of request waiting time. For objective functions that are linear combinations of the two measures, μ w + ko w , intermediate points of the continuum are seen to provide performance uniformly superior to both SSTF and SCAN. A method of implementing V( R ) and the results of its experimental use in a real system are presented.",
    "cited_by_count": 143,
    "openalex_id": "https://openalex.org/W2029903384",
    "type": "article"
  },
  {
    "title": "Coyote",
    "doi": "https://doi.org/10.1145/292523.292524",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Nina Bhatti; Matti Hiltunen; Richard D. Schlichting; Wanda Chiu",
    "corresponding_authors": "",
    "abstract": "Communication-oriented abstractions such as atomic multicast, group RPC, and protocols for location-independent mobile computing can simplify the development of complex applications built on distributed systems. This article describes Coyote, a system that supports the construction of highly modular and configurable versions of such abstractions. Coyote extends the notion of protocol objects and hierarchical composition found in existing systems with support for finer-grain microprotocol objects and a nonhierarchical composition scheme for use within a single layer of a protocol stack. A customized service is constructed by selecting microprotocols based on their semantic guarantees and configuring them together with a standard runtime system to form a composite protocol implementing the service. This composite protocol is then composed hierarchically with other protocols to form a complete network subsystem. The overall approach is described and illustrated with examples of services that have been constructed using Coyote, including atomic multicast, group RPC, membership, and mobile computing protocols. A prototype implementation based on extending x -kernel version 3.2 running on Mach 3.0 with support for microprotocols is also presented, together with performance results from a suite of microprotocols from which over 60 variants of group RPC can be constructed.",
    "cited_by_count": 142,
    "openalex_id": "https://openalex.org/W2047686393",
    "type": "article"
  },
  {
    "title": "Footprints in the cache",
    "doi": "https://doi.org/10.1145/29868.32979",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Dominique Thiébaut; Harold S. Stone",
    "corresponding_authors": "",
    "abstract": "This paper develops an analytical model for cache-reload transients and compares the model to observations based on several address traces. The cache-reload transient is the set of cache misses that occur when a process is reinitiated after being suspended temporarily. For example, an interrupt program that runs periodically experiences a reload transient at each initiation. The reload transient depends on the cache size and on the sizes of the footprints in the cache of the competing programs, where a program footprint is defined to be the set of lines in the cache in active use by the program. The model shows that the size of the transient is related to the normal distribution function. A simulation based on program-address traces shows excellent agreement between the model and the observations.",
    "cited_by_count": 141,
    "openalex_id": "https://openalex.org/W2094877030",
    "type": "article"
  },
  {
    "title": "Performance of the Firefly RPC",
    "doi": "https://doi.org/10.1145/77648.77653",
    "publication_date": "1990-02-01",
    "publication_year": 1990,
    "authors": "Michael Schroeder; Michael T. Burrows",
    "corresponding_authors": "",
    "abstract": "In this paper we report on the performance of the remote procedure call (RPC) implementation for the Firefly multiprocessor and analyze the implementation to account precisely for all measured latency. From the analysis and measurements, we estimate how much faster RPC could be if certain improvements were made. The elapsed time for an intermachine call to a remote procedure that accepts no arguments and produces no results is 2.66 ms. The elapsed time for an RPC that has a single 1440-byte result (the maximum result that will fit in a single packet) is 6.35 ms. Maximum intermachine throughput of application program data using RPC is 4.65 Mbits/s, achieved with four threads making parallel RPCs that return the maximum-size result that fits in a single RPC result packet. CPU utilization at maximum throughput is about 1.2 CPU seconds per second on the calling machine and a little less on the server. These measurements are for RPCs from user space on one machine to user space on another, using the installed system and a 10 Mbit/s Ethernet. The RPC packet exchange protocol is built on IP/UDP, and the times include calculating and verifying UDP checksums. The Fireflies used in the tests had 5 MicroVAX II processors and a DEQNA Ethernet controller.",
    "cited_by_count": 140,
    "openalex_id": "https://openalex.org/W2024564841",
    "type": "article"
  },
  {
    "title": "BASE",
    "doi": "https://doi.org/10.1145/859716.859718",
    "publication_date": "2003-08-01",
    "publication_year": 2003,
    "authors": "Miguel Castro; Rodrigo Rodrigues; Barbara Liskov",
    "corresponding_authors": "",
    "abstract": "Software errors are a major cause of outages and they are increasingly exploited in malicious attacks. Byzantine fault tolerance allows replicated systems to mask some software errors but it is expensive to deploy. This paper describes a replication technique, BASE, which uses abstraction to reduce the cost of Byzantine fault tolerance and to improve its ability to mask software errors. BASE reduces cost because it enables reuse of off-the-shelf service implementations. It improves availability because each replica can be repaired periodically using an abstract view of the state stored by correct replicas, and because each replica can run distinct or nondeterministic service implementations, which reduces the probability of common mode failures. We built an NFS service where each replica can run a different off-the-shelf file system implementation, and an object-oriented database where the replicas ran the same, nondeterministic implementation. These examples suggest that our technique can be used in practice---in both cases, the implementation required only a modest amount of new code, and our performance results indicate that the replicated services perform comparably to the implementations that they reuse.",
    "cited_by_count": 140,
    "openalex_id": "https://openalex.org/W2131314877",
    "type": "article"
  },
  {
    "title": "The evolution of Coda",
    "doi": "https://doi.org/10.1145/507052.507053",
    "publication_date": "2002-05-01",
    "publication_year": 2002,
    "authors": "Mahadev Satyanarayanan",
    "corresponding_authors": "Mahadev Satyanarayanan",
    "abstract": "Failure-resilient, scalable, and secure read-write access to shared information by mobile and static users over wireless and wired networks is a fundamental computing challenge. In this article, we describe how the Coda file system has evolved to meet this challenge through the development of mechanisms for server replication, disconnected operation, adaptive use of weak connectivity, isolation-only transactions, translucent caching, and opportunistic exploitation of hardware surrogates. For each mechanism, the article explains how usage experience with it led to the insights for another mechanism. It also shows how Coda has been influenced by the work of other researchers and by industry. The article closes with a discussion of the technical and nontechnical lessons that can be learned from the evolution of the system.",
    "cited_by_count": 139,
    "openalex_id": "https://openalex.org/W2074871523",
    "type": "article"
  },
  {
    "title": "Recovering device drivers",
    "doi": "https://doi.org/10.1145/1189256.1189257",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Michael M. Swift; Muthukaruppan Annamalai; Brian N. Bershad; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "This article presents a new mechanism that enables applications to run correctly when device drivers fail. Because device drivers are the principal failing component in most systems, reducing driver-induced failures greatly improves overall reliability. Earlier work has shown that an operating system can survive driver failures [Swift et al. 2005], but the applications that depend on them cannot. Thus, while operating system reliability was greatly improved, application reliability generally was not.To remedy this situation, we introduce a new operating system mechanism called a shadow driver . A shadow driver monitors device drivers and transparently recovers from driver failures. Moreover, it assumes the role of the failed driver during recovery. In this way, applications using the failed driver, as well as the kernel itself, continue to function as expected.We implemented shadow drivers for the Linux operating system and tested them on over a dozen device drivers. Our results show that applications and the OS can indeed survive the failure of a variety of device drivers. Moreover, shadow drivers impose minimal performance overhead. Lastly, they can be introduced with only modest changes to the OS kernel and with no changes at all to existing device drivers.",
    "cited_by_count": 139,
    "openalex_id": "https://openalex.org/W2116283852",
    "type": "article"
  },
  {
    "title": "Performance evaluation of the Orca shared-object system",
    "doi": "https://doi.org/10.1145/273011.273014",
    "publication_date": "1998-02-01",
    "publication_year": 1998,
    "authors": "Henri E. Bal; R.A.F. Bhoedjang; Rutger F. H. Hofman; Ceriel J. H. Jacobs; Koen Langendoen; Tim Rühl; M. Frans Kaashoek",
    "corresponding_authors": "",
    "abstract": "Orca is a portable, object-based distributed shared memory (DSM) system. This article studies and evaluates the design choices made in the Orca system and compares Orca with other DSMs. The article gives a quantitative analysis of Orca's coherence protocol (based on write-updates with function shipping), the totally ordered group communication protocol, the strategy for object placement, and the all-software, user-space architecture. Performance measurements for 10 parallel applications illustrate the trade-offs made in the design of Orca and show that essentially the right design decisions have been made. A write-update protocol with function shipping is effective for Orca, especially since it is used in combination with techniques that avoid replicating objects that have a low read/write ratio. The overhead of totally ordered group communication on application performance is low. The Orca system is able to make near-optimal decisions for object placement and replication. In addition, the article compares the performance of Orca with that of a page-based DSM (TreadMarks) and another object-based DSM (CRL). It also analyzes the communication overhead of the DSMs for several applications. All performance measurements are done on a 32-node Pentium Pro cluster with Myrinet and Fast Ethernet networks. The results show that Orca programs send fewer messages and less data than the TreadMarks and CRL programs and obtain better speedups.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2002694006",
    "type": "article"
  },
  {
    "title": "Limits to low-latency communication on high-speed networks",
    "doi": "https://doi.org/10.1145/151244.151247",
    "publication_date": "1993-05-01",
    "publication_year": 1993,
    "authors": "Chandramohan A. Thekkath; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "The throughput of local area networks is rapidly increasing. For example, the bandwidth of new ATM networks and FDDI token rings is an order of magnitude greater than that of Ethernets. Other network technologies promise a bandwidth increase of yet another order of magnitude in several years. However, in distributed systems, lowered latency rather than increased throughput is often of primary concern. This paper examines the system-level effects of newer high-speed network technologies on low-latency, cross-machine communications. To evaluate a number of influences, both hardware and software, we designed and implemented a new remote procedure call system targeted at providing low latency. We then ported this system to several hardware platforms (DECstation and SPARCstation) with several different networks and controllers (ATM, FDDI, and Ethernet). Comparing these systems allows us to explore the performance impact of alternative designs in the communication system with respect to achieving low latency, e.g., the network, the network controller, the hose architecture and cache system, and the kernel and user-level runtime software. Our RPC system, which achieves substantially reduced call times (170 μseconds on an ATM network using DECstation 5000/200 hosts), allows us to isolate those components of next-generation networks and controllers that still stand in the way of low-latency communication. We demonstrate that new-generation processor technology and software design can reduce small-packet RPC times to near network-imposed limits, making network and controller design more crucial than ever to achieving truly low-latency communication.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W2049631480",
    "type": "article"
  },
  {
    "title": "Measuring thin-client performance using slow-motion benchmarking",
    "doi": "https://doi.org/10.1145/592637.592640",
    "publication_date": "2003-01-10",
    "publication_year": 2003,
    "authors": "Jason Nieh; S. Jae Yang; Н. А. Новик",
    "corresponding_authors": "",
    "abstract": "Modern thin-client systems are designed to provide the same graphical interfaces and applications available on traditional desktop computers while centralizing administration and allowing more efficient use of computing resources. Despite the rapidly increasing popularity of these client-server systems, there are few reliable analyses of their performance. Industry standard benchmark techniques commonly used for measuring desktop system performance are ill-suited for measuring the performance of thin-client systems because these benchmarks only measure application performance on the server, not the actual user-perceived performance on the client. To address this problem, we have developed slow-motion benchmarking, a new measurement technique for evaluating thin-client systems. In slow-motion benchmarking, performance is measured by capturing network packet traces between a thin client and its respective server during the execution of a slow-motion version of a conventional benchmark application. These results can then be used either independently or in conjunction with conventional benchmark results to yield an accurate and objective measure of the performance of thin-client systems. We have demonstrated the effectiveness of slow-motion benchmarking by using this technique to measure the performance of several popular thin-client systems in various network environments on Web and multimedia workloads. Our results show that slow-motion benchmarking solves the problems with using conventional benchmarks on thin-client systems and is an accurate tool for analyzing the performance of these systems.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W2071266360",
    "type": "article"
  },
  {
    "title": "Energy-efficient CPU scheduling for multimedia applications",
    "doi": "https://doi.org/10.1145/1151690.1151693",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Wanghong Yuan; Klara Nahrstedt",
    "corresponding_authors": "",
    "abstract": "This article presents the design, implementation, and evaluation of EScheduler , an energy-efficient soft real-time CPU scheduler for multimedia applications running on a mobile device. EScheduler seeks to minimize the total energy consumed by the device while meeting multimedia timing requirements. To achieve this goal, EScheduler integrates dynamic voltage scaling into the traditional soft real-time CPU scheduling: It decides at what CPU speed to execute applications in addition to when to execute what applications. EScheduler makes these scheduling decisions based on the probability distribution of cycle demand of multimedia applications and obtains their demand distribution via online profiling.We have implemented EScheduler in the Linux kernel and evaluated it on a laptop with a variable-speed CPU and typical multimedia codecs. Our experimental results show four findings: first, the cycle demand distribution of our studied codecs is stable or changes slowly. This stability implies the feasibility to perform our proposed energy-efficient scheduling with low overhead. Second, EScheduler delivers soft performance guarantees to these codecs by bounding their deadline miss ratio under the application-specific performance requirements. Third, EScheduler reduces the total energy of the laptop by 14.4% to 37.2% relative to the scheduling algorithm without voltage scaling and by 2% to 10.5% relative to voltage scaling algorithms without considering the demand distribution. Finally, EScheduler saves energy by 2% to 5% by explicitly considering the discrete CPU speeds and the corresponding total power of the whole laptop, rather than assuming continuous speeds and cubic speed-power relationship.",
    "cited_by_count": 132,
    "openalex_id": "https://openalex.org/W1988302931",
    "type": "article"
  },
  {
    "title": "Metascheduling for continuous media",
    "doi": "https://doi.org/10.1145/152864.152866",
    "publication_date": "1993-08-01",
    "publication_year": 1993,
    "authors": "David P. Anderson",
    "corresponding_authors": "David P. Anderson",
    "abstract": "Next-generation distributed systems will support continuous media (digital audio and video) in the same framework as other data. Many applications that use continuous media need guaranteed end-to-end performance (bounds on throughput and delay). To reliably support these requirements, system components such as CPU schedulers, networks, and file systems must offer performance guarantees. A metascheduler coordinates these components, negotiating end-to-end guarantees on behalf of clients. The CM-resource model , described in this paper, provides a basis for such a metascheduler. It defines a workload parameterization, an abstract interface to resources, and an algorithm for reserving multiple resources. The model uses an economic approach to dividing end-to-end delay, and it allows system components to “work ahead,” improving the performance of nonreal-time workload.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2050945375",
    "type": "article"
  },
  {
    "title": "Computation and communication in R*",
    "doi": "https://doi.org/10.1145/2080.357390",
    "publication_date": "1984-02-01",
    "publication_year": 1984,
    "authors": "Bruce G. Lindsay; Laura M. Haas; C. Mohan; Paul F. Wilms; Robert A. Yost",
    "corresponding_authors": "",
    "abstract": "article Free AccessComputation and communication in R*: a distributed database manager Authors: Bruce G. Lindsay IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CA IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CAView Profile , Laura M. Haas IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CA IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CAView Profile , C. Mohan IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CA IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CAView Profile , Paul F. Wilms IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CA IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CAView Profile , Robert A. Yost IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CA IBM San Jose Research Laboratory, 5600 Cottle Road, San Jose, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 1February 1984 pp 24–38https://doi.org/10.1145/2080.357390Published:01 February 1984Publication History 85citation775DownloadsMetricsTotal Citations85Total Downloads775Last 12 Months54Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 130,
    "openalex_id": "https://openalex.org/W2035933658",
    "type": "article"
  },
  {
    "title": "Experience with Grapevine: the growth of a distributed system",
    "doi": "https://doi.org/10.1145/2080.2081",
    "publication_date": "1984-02-01",
    "publication_year": 1984,
    "authors": "Michael Schroeder; Andrew Birrell; Roger M. Needham",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Experience with Grapevine: the growth of a distributed system Authors: Michael D. Schroeder Xerox Palo Alto Research Center, Palo Alto, CA Xerox Palo Alto Research Center, Palo Alto, CAView Profile , Andrew D. Birrell Xerox Palo Alto Research Center, Palo Alto, CA Xerox Palo Alto Research Center, Palo Alto, CAView Profile , Roger M. Needham Xerox Palo Alto Research Center, Palo Alto, CA Xerox Palo Alto Research Center, Palo Alto, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 1February 1984 pp 3–23https://doi.org/10.1145/2080.2081Published:01 February 1984Publication History 83citation4,829DownloadsMetricsTotal Citations83Total Downloads4,829Last 12 Months121Last 6 weeks76 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2127851689",
    "type": "article"
  },
  {
    "title": "Labels and event processes in the Asbestos operating system",
    "doi": "https://doi.org/10.1145/1314299.1314302",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Steve VanDeBogart; Petros Efstathopoulos; Eddie Kohler; Maxwell Krohn; Cliff Frey; David Ziegler; Frans Kaashoek; Robert Morris; David Mazières",
    "corresponding_authors": "",
    "abstract": "Asbestos, a new operating system, provides novel labeling and isolation mechanisms that help contain the effects of exploitable software flaws. Applications can express a wide range of policies with Asbestos's kernel-enforced labels, including controls on interprocess communication and system-wide information flow. A new event process abstraction defines lightweight, isolated contexts within a single process, allowing one process to act on behalf of multiple users while preventing it from leaking any single user's data to others. A Web server demonstration application uses these primitives to isolate private user data. Since the untrusted workers that respond to client requests are constrained by labels, exploited workers cannot directly expose user data except as allowed by application policy. The server application requires 1.4 memory pages per user for up to 145,000 users and achieves connection rates similar to Apache, demonstrating that additional security can come at an acceptable cost.",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W2083228150",
    "type": "article"
  },
  {
    "title": "Nonblocking memory management support for dynamic-sized data structures",
    "doi": "https://doi.org/10.1145/1062247.1062249",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Maurice Herlihy; Victor Luchangco; Paul Martin; Mark Moir",
    "corresponding_authors": "",
    "abstract": "Conventional dynamic memory management methods interact poorly with lock-free synchronization. In this article, we introduce novel techniques that allow lock-free data structures to allocate and free memory dynamically using any thread-safe memory management library. Our mechanisms are lock-free in the sense that they do not allow a thread to be prevented from allocating or freeing memory by the failure or delay of other threads. We demonstrate the utility of these techniques by showing how to modify the lock-free FIFO queue implementation of Michael and Scott to free unneeded memory. We give experimental results that show that the overhead introduced by such modifications is moderate, and is negligible under low contention.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W1974122097",
    "type": "article"
  },
  {
    "title": "Gaining efficiency in transport services by appropriate design and implementation choices",
    "doi": "https://doi.org/10.1145/13677.13678",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "R. Watson; Sandy A. Mamrak",
    "corresponding_authors": "",
    "abstract": "End-to-end transport protocols continue to be an active area of research and development involving (1) design and implementation of special-purpose protocols, and (2) reexamination of the design and implementation of general-purpose protocols. This work is motivated by the perceived low bandwidth and high delay, CPU, memory, and other costs of many current general-purpose transport protocol designs and implementations. This paper examines transport protocol mechanisms and implementation issues and argues that general-purpose transport protocols can be effective in a wide range of distributed applications because (1) many of the mechanisms used in the special-purpose protocols can also be used in general-purpose protocol designs and implementations, (2) special-purpose designs have hidden costs, and (3) very special operating system environments, overall system loads, application response times, and interaction patterns are required before general-purpose protocols are the main system performance bottlenecks.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W1989427509",
    "type": "article"
  },
  {
    "title": "On the performance of wide-area thin-client computing",
    "doi": "https://doi.org/10.1145/1132026.1132029",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Albert M. Lai; Jason Nieh",
    "corresponding_authors": "",
    "abstract": "While many application service providers have proposed using thin-client computing to deliver computational services over the Internet, little work has been done to evaluate the effectiveness of thin-client computing in a wide-area network. To assess the potential of thin-client computing in the context of future commodity high-bandwidth Internet access, we have used a novel, noninvasive slow-motion benchmarking technique to evaluate the performance of several popular thin-client computing platforms in delivering computational services cross-country over Internet2. Our results show that using thin-client computing in a wide-area network environment can deliver acceptable performance over Internet2, even when client and server are located thousands of miles apart on opposite ends of the country. However, performance varies widely among thin-client platforms and not all platforms are suitable for this environment. While many thin-client systems are touted as being bandwidth efficient, we show that network latency is often the key factor in limiting wide-area thin-client performance. Furthermore, we show that the same techniques used to improve bandwidth efficiency often result in worse overall performance in wide-area networks. We characterize and analyze the different design choices in the various thin-client platforms and explain which of these choices should be selected for supporting wide-area computing services.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2089115820",
    "type": "article"
  },
  {
    "title": "Performance analysis of TLS Web servers",
    "doi": "https://doi.org/10.1145/1124153.1124155",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Cristian Coarfa; Peter Druschel; Dan S. Wallach",
    "corresponding_authors": "",
    "abstract": "TLS is the protocol of choice for securing today's e-commerce and online transactions but adding TLS to a Web server imposes a significant overhead relative to an insecure Web server on the same platform. We perform a comprehensive study of the performance costs of TLS. Our methodology is to profile TLS Web servers with trace-driven workloads, replace individual components inside TLS with no-ops, and measure the observed increase in server throughput. We estimate the relative costs of each TLS processing stage, identifying the areas for which future optimizations would be worthwhile. Our results show that while the RSA operations represent the largest performance cost in TLS Web servers, they do not solely account for TLS overhead. RSA accelerators are effective for e-commerce site workloads since they experience low TLS session reuse. Accelerators appear to be less effective for sites where all the requests are handled by a TLS server because they have a higher session reuse rate. In this case, investing in a faster CPU might provide a greater boost in performance. Our experiments show that having a second CPU is at least as useful as an RSA accelerator. Our results seem to suggest that, as CPUs become faster, the cryptographic costs of TLS will become dwarfed by the CPU costs of the nonsecurity aspects of a Web server. Optimizations aimed at general purpose Web servers should continue to be a focus of research and would benefit secure Web servers as well.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2130741436",
    "type": "article"
  },
  {
    "title": "The integration of virtual memory management and interprocess communication in Accent",
    "doi": "https://doi.org/10.1145/214419.214422",
    "publication_date": "1986-05-01",
    "publication_year": 1986,
    "authors": "Robert Fitzgerald; Richard F. Rashid",
    "corresponding_authors": "",
    "abstract": "The integration of virtual memory management and interprocess communication in the Accent network operating system kernel is examined. The design and implementation of the Accent memory management system is discussed and its performance, both on a series of message-oriented benchmarks and in normal operation, is analyzed in detail.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W2001703874",
    "type": "article"
  },
  {
    "title": "Cache Performance in the VAX-11/780",
    "doi": "https://doi.org/10.1145/357353.357356",
    "publication_date": "1983-02-01",
    "publication_year": 1983,
    "authors": "Douglas W. Clark",
    "corresponding_authors": "Douglas W. Clark",
    "abstract": "article Free AccessCache Performance in the VAX-11/780 Author: Douglas W. Clark Digital Equipment Corporation, 295 Foster Street, Littleton, MA Digital Equipment Corporation, 295 Foster Street, Littleton, MAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 1Feb. 1983 pp 24–37https://doi.org/10.1145/357353.357356Published:01 February 1983Publication History 102citation800DownloadsMetricsTotal Citations102Total Downloads800Last 12 Months51Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W1986209913",
    "type": "article"
  },
  {
    "title": "Secure communication using remote procedure calls",
    "doi": "https://doi.org/10.1145/214451.214452",
    "publication_date": "1985-02-01",
    "publication_year": 1985,
    "authors": "Andrew Birrell",
    "corresponding_authors": "Andrew Birrell",
    "abstract": "Research on encryption-based secure communication protocols has reached a stage where it is feasible to construct end-to-end secure protocols. The design of such a protocol, built as part of a remote procedure call package, is described. The security abstraction presented to users of the package, the authentication mechanisms, and the protocol for encrypting and verifying remote calls are also described.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2163684994",
    "type": "article"
  },
  {
    "title": "Bringing Virtualization to the x86 Architecture with the Original VMware Workstation",
    "doi": "https://doi.org/10.1145/2382553.2382554",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Edouard Bugnion; Scott Devine; Mendel Rosenblum; Jeremy Sugerman; Edward Y. Wang",
    "corresponding_authors": "",
    "abstract": "This article describes the historical context, technical challenges, and main implementation techniques used by VMware Workstation to bring virtualization to the x86 architecture in 1999. Although virtual machine monitors (VMMs) had been around for decades, they were traditionally designed as part of monolithic, single-vendor architectures with explicit support for virtualization. In contrast, the x86 architecture lacked virtualization support, and the industry around it had disaggregated into an ecosystem, with different vendors controlling the computers, CPUs, peripherals, operating systems, and applications, none of them asking for virtualization. We chose to build our solution independently of these vendors. As a result, VMware Workstation had to deal with new challenges associated with (i) the lack of virtualization support in the x86 architecture, (ii) the daunting complexity of the architecture itself, (iii) the need to support a broad combination of peripherals, and (iv) the need to offer a simple user experience within existing environments. These new challenges led us to a novel combination of well-known virtualization techniques, techniques from other domains, and new techniques. VMware Workstation combined a hosted architecture with a VMM. The hosted architecture enabled a simple user experience and offered broad hardware compatibility. Rather than exposing I/O diversity to the virtual machines, VMware Workstation also relied on software emulation of I/O devices. The VMM combined a trap-and-emulate direct execution engine with a system-level dynamic binary translator to efficiently virtualize the x86 architecture and support most commodity operating systems. By relying on x86 hardware segmentation as a protection mechanism, the binary translator could execute translated code at near hardware speeds. The binary translator also relied on partial evaluation and adaptive retranslation to reduce the overall overheads of virtualization. Written with the benefit of hindsight, this article shares the key lessons we learned from building the original system and from its later evolution.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2090590366",
    "type": "article"
  },
  {
    "title": "Improving Software Diagnosability via Log Enhancement",
    "doi": "https://doi.org/10.1145/2110356.2110360",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Ding Yuan; Jing Zheng; Soyeon Park; Yuanyuan Zhou; Stefan Savage",
    "corresponding_authors": "",
    "abstract": "Diagnosing software failures in the field is notoriously difficult, in part due to the fundamental complexity of troubleshooting any complex software system, but further exacerbated by the paucity of information that is typically available in the production setting. Indeed, for reasons of both overhead and privacy, it is common that only the run-time log generated by a system (e.g., syslog) can be shared with the developers. Unfortunately, the ad-hoc nature of such reports are frequently insufficient for detailed failure diagnosis. This paper seeks to improve this situation within the rubric of existing practice. We describe a tool, LogEnhancer that automatically “enhances” existing logging code to aid in future post-failure debugging. We evaluate LogEnhancer on eight large, real-world applications and demonstrate that it can dramatically reduce the set of potential root failure causes that must be considered while imposing negligible overheads.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2096761130",
    "type": "article"
  },
  {
    "title": "GPUfs",
    "doi": "https://doi.org/10.1145/2553081",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Mark Silberstein; Bryan Ford; Idit Keidar; Emmett Witchel",
    "corresponding_authors": "",
    "abstract": "As GPU hardware becomes increasingly general-purpose, it is quickly outgrowing the traditional, constrained GPU-as-coprocessor programming model. This article advocates for extending standard operating system services and abstractions to GPUs in order to facilitate program development and enable harmonious integration of GPUs in computing systems. As an example, we describe the design and implementation of GPUFs, a software layer which provides operating system support for accessing host files directly from GPU programs. GPUFs provides a POSIX-like API, exploits GPU parallelism for efficiency, and optimizes GPU file access by extending the host CPU's buffer cache into GPU memory. Our experiments, based on a set of real benchmarks adapted to use our file system, demonstrate the feasibility and benefits of the GPUFs approach. For example, a self-contained GPU program that searches for a set of strings throughout the Linux kernel source tree runs over seven times faster than on an eight-core CPU.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2013100386",
    "type": "article"
  },
  {
    "title": "Soft updates",
    "doi": "https://doi.org/10.1145/350853.350863",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "Gregory R. Ganger; Marshall Kirk McKusick; Craig A. N. Soules; Yale N. Patt",
    "corresponding_authors": "",
    "abstract": "Metadata updates, such as file creation and block allocation, have consistently been identified as a source of performance, integrity, security, and availability problems for file systems. Soft updates is an implementation technique for low-cost sequencing of fine-grained updates to write-back cache blocks. Using soft updates to track and enforce metadata update dependencies, a file system can safely use delayed writes for almost all file operations. This article describes soft updates, their incorporation into the 4.4BSD fast file system, and the resulting effects on the sytem. We show that a disk-based file system using soft updates achieves memory-based file system performance while providing stronger integrity and security guarantees than most disk-based file systems. For workloads that frequently perform updates on metadata (such as creating and deleting files), this improves performance by more than a factor of two and up to a factor of 20 when compared to the conventional synchronous write approach and by 4-19% when compared to an aggressive write-ahead logging approach. In addition, soft updates can improve file system availablity by relegating crash-recovery assistance (e.g., the fsck utility) to an optional and background role, reducing file system recovery time to less than one second.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2138664028",
    "type": "article"
  },
  {
    "title": "A logic for reasoning about security",
    "doi": "https://doi.org/10.1145/146937.146940",
    "publication_date": "1992-08-01",
    "publication_year": 1992,
    "authors": "Janice Glasgow; Glenn H. MacEwen; Prakash Panangaden",
    "corresponding_authors": "",
    "abstract": "A formal framework called Security Logic ( SL ) is developed for specifying and reasoning about security policies and for verifying that system designs adhere to such policies. Included in this modal logic framework are definitions of knowledge, permission, and obligation . Permission is used to specify secrecy policies and obligation to specify integrity policies. The combination of policies is addressed and examples based on policies from the current literature are given.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W1991305430",
    "type": "article"
  },
  {
    "title": "Adaptive block rearrangement",
    "doi": "https://doi.org/10.1145/201045.201046",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Sedat Akyürek; Kenneth Salem",
    "corresponding_authors": "",
    "abstract": "An adaptive technique for reducing disk seek times is described. The technique copies frequently referenced blocks from their original locations to reserved space near the middle of the disk. Reference frequencies need not be known in advance. Instead, they are estimated by monitoring the stream of arriving requests. Trace-driven simulations show that seek times can be cut substantially by copying only a small number of blocks using this technique. The technique has been implemented by modifying a UNIX device driver. No modifications are required to the file system that uses the driver.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W2160356600",
    "type": "article"
  },
  {
    "title": "Diffracting trees",
    "doi": "https://doi.org/10.1145/235543.235546",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Nir Shavit; Asaph Zemach",
    "corresponding_authors": "",
    "abstract": "Shared counters are among the most basic coordination structures in multiprocessor conputation, with applications ranging from barrier synchronization to concurrent-data-structure design. This article introduces diffracting trees, novel data structures for share counting and load balancing in a distributed/parallel environment. Empirical evidence, collected on a simulated distributed shared-memory machine and several simulated message-passing architectures, shows that diffracting trees scale better and are more robust than both combining trees and counting networks, currently the most effective known methods for implementing concurrent counters in software. The use of a randomized coordination method together with a combinatorial data structure overcomes the resiliency drawbacks of combining trees. Our simulations show that to handle the same load, diffracting trees and counting networks should have a similar width w , yet the depth of a diffracting tree is O (log w ), whereas counting networks have depth O (log 2 w ). Diffracting trees have already been used to implement highly efficient producer/consumer queues, and we believe diffraction will prove to be an effective alternative paradigm to combining and queue-locking in the design of many concurrent data structures.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W2294090198",
    "type": "article"
  },
  {
    "title": "Smart packets",
    "doi": "https://doi.org/10.1145/332799.332893",
    "publication_date": "2000-02-01",
    "publication_year": 2000,
    "authors": "B. Schwartz; Alden W. Jackson; W. Timothy Strayer; Wenyi Zhou; R. Dennis Rockwell; Craig Partridge",
    "corresponding_authors": "",
    "abstract": "This article introduces Smart Packets and describes the smart Packets architecture, the packet formats, the language and its design goals, and security considerations. Smart Packets is an Active Networks project focusing on applying active networks technology to network management and monitoring. Messages in active networks are programs that are executed at nodes on the path to one or more target hosts. Smart Packets programs are written in a tightly encoded, safe language specifically designed to support network management and avoid dangerous constructs and accesses. Smart Packets improves the management of large complex networks by (1) moving management decision points closer to the node being managed, (2) targeting specific aspects of the node for information rather than exhaustive collection via polling, and (3) abstracting the management concepts to language constructs, allowing nimble network control.",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2064160745",
    "type": "article"
  },
  {
    "title": "Real-time computing with lock-free shared objects",
    "doi": "https://doi.org/10.1145/253145.253159",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "James H. Anderson; Srikanth Ramamurthy; Kevin Jeffay",
    "corresponding_authors": "",
    "abstract": "This article considers the use of lock-free shared objects within hard real-time systems. As the name suggests, lock-free shared objects are distinguished by the fact that they are accessed without locking. As such, they do not give rise to priority inversions, a key advantage over conventional, lock-based object-sharing approaches. Despite this advantage, it is not immediately apparent that lock-free shared objects can be employed if tasks must adhere to strict timing constraints. In particular, lock-free object implementations permit concurrent operations to interfere with each other, and repeated interferences can cause a given operation to take an arbitrarily long time to complete. The main contribution of this article is to show that such interferences can be bounded by judicious scheduling. This work pertains to periodic, hard real-time tasks that share lock-free objects on a uniprocessor. In the first part of the article, scheduling conditions are derived for such tasks, for both static and dynamic priority schemes. Based on these conditions, it is formally shown that lock-free shared objects often incur less overhead than object implementations based on wait-free algorithms or lock-based schemes. In the last part of the article, this conclusion is validated experimentally through work involving a real-time desktop videoconferencing system.",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W1966901681",
    "type": "article"
  },
  {
    "title": "Experimental comparison of memory management policies for NUMA multiprocessors",
    "doi": "https://doi.org/10.1145/118544.118546",
    "publication_date": "1991-11-01",
    "publication_year": 1991,
    "authors": "Richard P. LaRowe; Carla Schlatter Ellis",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Experimental comparison of memory management policies for NUMA multiprocessors Authors: Richard P. Larowe Encore Computer Corp., Marlboro, MA Encore Computer Corp., Marlboro, MAView Profile , Carla Schlatter Ellis Duke Univ., Durham, NC Duke Univ., Durham, NCView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 4pp 319–363https://doi.org/10.1145/118544.118546Published:01 November 1991Publication History 70citation825DownloadsMetricsTotal Citations70Total Downloads825Last 12 Months47Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2079746131",
    "type": "article"
  },
  {
    "title": "The TickerTAIP parallel RAID architecture",
    "doi": "https://doi.org/10.1145/185514.185517",
    "publication_date": "1994-08-01",
    "publication_year": 1994,
    "authors": "Pei Cao; Swee Boon Lin; Shivakumar Venkataraman; John Wilkes",
    "corresponding_authors": "",
    "abstract": "Traditional disk arrays have a centralized architecture, with a single controller through which all requests flow. Such a controller is a single point of failure, and its performance limits the maximum number of disks to which the array can scale. We describe TickerTAIP, a parallel architecture for disk arrays that distributes the controller functions across several loosely coupled processors. The result is better scalability, fault tolerance, and flexibility. This article presents the TickerTAIP architecture and an evaluation of its behavior. We demonstrate the feasibility by a working example, describe a family of distributed algorithms for calculating RAID parity, discuss techniques for establishing request atomicity, sequencing, and recovery, and evaluate the performance of the TickerTAIP design in both absolute terms and by comparison to a centralized RAID implementation. We also analyze the effects of including disk-level request-scheduling algorithms inside the array. We conclude that the Ticker TAIP architectural approach is feasible, useful, and effective.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2018292377",
    "type": "article"
  },
  {
    "title": "Soft timers",
    "doi": "https://doi.org/10.1145/354871.354872",
    "publication_date": "2000-08-01",
    "publication_year": 2000,
    "authors": "Mohit Aron; Peter Druschel",
    "corresponding_authors": "",
    "abstract": "This paper proposes and evaluates soft timers, a new operating system facility that allows the efficient scheduling of software events at agranularity down to tens of microseconds. Soft timers can be used to avoid interrupts and reduce context switches associated with network processing, without sacrificing low communication delays. More specifically, soft timers enable transport protocols like TCP to efficiently perform rate-based clocking of packet transmissions. Experiments indicate that soft timers allow a server to employ rate-based clocking with little CPU overhead (2-6%) at high aggregate bandwidths. Soft timers can also be used to perform network polling, which eliminates network interrupts and increases the memory access locality of the network subsystem without sacrificing delay. Experiments show that this technique can improve the throughput of a Web server by up to 25%.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2111248450",
    "type": "article"
  },
  {
    "title": "Sentinel scheduling",
    "doi": "https://doi.org/10.1145/161541.159765",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Scott Mahlke; William Y. Chen; Roger A. Bringmann; Richard E. Hank; Wen‐mei Hwu; B. Ramakrishna Rau; Michael Schlansker",
    "corresponding_authors": "",
    "abstract": "Speculative execution is an important source of parallelism for VLIW and superscalar processors. A serious challenge with compiler-controlled speculative execution is to efficiently handle exceptions for speculative instructions. In this article, a set of architectural features and compile-time scheduling support collectively referred to as sentinel scheduling is introduced. Sentinel scheduling provides an effective framework for both compiler-controlled speculative execution and exception handling. All program exceptions are accurately detected and reported in a timely manner with sentinel scheduling. Recovery from exceptions is also ensured with the model. Experimental results show the effectiveness of sentinel scheduling for exploiting instruction-level parallelism and overhead associated with exception handling.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W2075651377",
    "type": "article"
  },
  {
    "title": "Recovery management in QuickSilver",
    "doi": "https://doi.org/10.1145/35037.35060",
    "publication_date": "1988-02-01",
    "publication_year": 1988,
    "authors": "Rober Haskin; Yoni Malachi; Gregory Chan",
    "corresponding_authors": "",
    "abstract": "This paper describes QuickSilver, developed at the IBM Almaden Research Center, which uses atomic transactions as a unified failure recovery mechanism for a client-server structured distributed system. Transactions allow failure atomicity for related activities at a single server or at a number of independent servers. Rather than bundling transaction management into a dedicated language or recoverable object manager, Quicksilver exposes the basic commit protocol and log recovery primitives, allowing clients and servers to tailor their recovery techniques to their specific needs. Servers can implement their own log recovery protocols rather than being required to use a system-defined protocol. These decisions allow servers to make their own choices to balance simplicity, efficiency, and recoverability.",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W1988127782",
    "type": "article"
  },
  {
    "title": "User-level interprocess communication for shared memory multiprocessors",
    "doi": "https://doi.org/10.1145/103720.114701",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Brian N. Bershad; Thomas E. Anderson; Edward D. Lazowska; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on User-level interprocess communication for shared memory multiprocessors Authors: Brian N. Bershad Carnegie-Mellon Univ., Pittsburgh, PA Carnegie-Mellon Univ., Pittsburgh, PAView Profile , Thomas E. Anderson Univ. of Washington, Seattle Univ. of Washington, SeattleView Profile , Edward D. Lazowska Univ. of Washington, Seattle Univ. of Washington, SeattleView Profile , Henry M. Levy Univ. of Washington, Seattle Univ. of Washington, SeattleView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 2May 1991 pp 175–198https://doi.org/10.1145/103720.114701Online:01 May 1991Publication History 55citation1,416DownloadsMetricsTotal Citations55Total Downloads1,416Last 12 Months137Last 6 weeks15 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W2144807611",
    "type": "article"
  },
  {
    "title": "Static grouping of small objects to enhance performance of a paged virtual memory",
    "doi": "https://doi.org/10.1145/190.194",
    "publication_date": "1984-05-01",
    "publication_year": 1984,
    "authors": "James W. Stamos",
    "corresponding_authors": "James W. Stamos",
    "abstract": "article Free Access Share on Static grouping of small objects to enhance performance of a paged virtual memory Author: James W. Stamos Xerox Palo Alto Research Center, Palo Alto, CA Xerox Palo Alto Research Center, Palo Alto, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 2pp 155–180https://doi.org/10.1145/190.194Published:01 May 1984Publication History 72citation644DownloadsMetricsTotal Citations72Total Downloads644Last 12 Months25Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2083276428",
    "type": "article"
  },
  {
    "title": "The performance of multiversion concurrency control algorithms",
    "doi": "https://doi.org/10.1145/6513.6517",
    "publication_date": "1986-09-01",
    "publication_year": 1986,
    "authors": "Michael J. Carey; Waleed A. Muhanna",
    "corresponding_authors": "",
    "abstract": "A number of multiversion concurrency control algorithms have been proposed in the past few years. These algorithms use previous versions of data items in order to improve the level of achievable concurrency. This paper describes a simulation study of the performance of several multiversion concurrency control algorithms, investigating the extent to which they provide increases in the level of concurrency and also the CPU, I/O, and storage costs resulting from the use of multiple versions. The multiversion algorithms are compared with regard to performance with their single-version counterparts and also with each other. It is shown that each multiversion algorithm offers significant performance improvements despite the additional disk accesses involved in accessing old versions of data; the nature of the improvement depends on the algorithm in question. It is also shown that the storage overhead for maintaining old versions that may be required by ongoing transactions is not all that large under most circumstances. Finally, it is demonstrated that it is important for version maintenance to be implemented efficiently, as otherwise the cost of maintaining old versions could outweigh their concurrency benefits.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2004735493",
    "type": "article"
  },
  {
    "title": "The automatic improvement of locality in storage systems",
    "doi": "https://doi.org/10.1145/1113574.1113577",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Windsor W. Hsu; Alan Jay Smith; Honesty C. Young",
    "corresponding_authors": "",
    "abstract": "Disk I/O is increasingly the performance bottleneck in computer systems despite rapidly increasing disk data transfer rates. In this article, we propose Automatic Locality-Improving Storage (ALIS), an introspective storage system that automatically reorganizes selected disk blocks based on the dynamic reference stream to increase effective storage performance. ALIS is based on the observations that sequential data fetch is far more efficient than random access, that improving seek distances produces only marginal performance improvements, and that the increasingly powerful processors and large memories in storage systems have ample capacity to reorganize the data layout and redirect the accesses so as to take advantage of rapid sequential data transfer. Using trace-driven simulation with a large set of real workloads, we demonstrate that ALIS considerably outperforms prior techniques, improving the average read performance by up to 50% for server workloads and by about 15% for personal computer workloads. We also show that the performance improvement persists as disk technology evolves. Since disk performance in practice is increasing by only about 8% per year, the benefit of ALIS may correspond to as much as several years of technological progress.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2080232995",
    "type": "article"
  },
  {
    "title": "On the power of cascade ciphers",
    "doi": "https://doi.org/10.1145/214438.214442",
    "publication_date": "1985-05-01",
    "publication_year": 1985,
    "authors": "Shimon Even; Oded Goldreich",
    "corresponding_authors": "",
    "abstract": "The unicity distance of a cascade of random ciphers, with respect to known plaintext attack, is shown to be the sum of the key lengths. A time-space trade-off for the exhaustive cracking of a cascade of ciphers is shown. The structure of the set of permutations realized by a cascade is studied; it is shown that only l .2 k exhaustive experiments are necessary to determine the behavior of a cascade of l stages, each having k key bits. It is concluded that the cascade of random ciphers is not a random cipher. Yet, it is shown that, with high probability, the number of permutations realizable by a cascade of l random ciphers, each having k key bits, is 2 lk . Next, it is shown that two stages are not worse than one, by a simple reduction of the cracking problem of any of the stages to the cracking problem of the cascade. Finally, it is shown that proving a nonpolynomial lower bound on the cracking problem of long cascades is a hard task, since such a bound implies that P ≉ NP .",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W1994995146",
    "type": "article"
  },
  {
    "title": "Low cost management of replicated data in fault-tolerant distributed systems",
    "doi": "https://doi.org/10.1145/6306.6309",
    "publication_date": "1986-02-10",
    "publication_year": 1986,
    "authors": "Thomas Joseph; Kenneth P. Birman",
    "corresponding_authors": "",
    "abstract": "Many distributed systems replicate data for fault tolerance or availability. In such systems, a logical update on a data item results in a physical update on a number of copies. The synchronization and communication required to keep the copies of replicated data consistent introduce a delay when operations are performed. In this paper, we describe a technique that relaxes the usual degree of synchronization, permitting replicated data items to be updated concurrently with other operations, while at the same time ensuring that correctness is not violated. The additional concurrency thus obtained results in better response time when performing operations on replicated data. We also discuss how this technique performs in conjunction with a roll-back and a roll-forward failure recovery mechanism.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W1996715632",
    "type": "article"
  },
  {
    "title": "Automated anomaly detection and performance modeling of enterprise applications",
    "doi": "https://doi.org/10.1145/1629087.1629089",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Ludmila Cherkasova; Kivanc Ozonat; Ningfang Mi; Julie Symons; Evgenia Smirni",
    "corresponding_authors": "",
    "abstract": "Automated tools for understanding application behavior and its changes during the application lifecycle are essential for many performance analysis and debugging tasks. Application performance issues have an immediate impact on customer experience and satisfaction. A sudden slowdown of enterprise-wide application can effect a large population of customers, lead to delayed projects, and ultimately can result in company financial loss. Significantly shortened time between new software releases further exacerbates the problem of thoroughly evaluating the performance of an updated application. Our thesis is that online performance modeling should be a part of routine application monitoring. Early, informative warnings on significant changes in application performance should help service providers to timely identify and prevent performance problems and their negative impact on the service. We propose a novel framework for automated anomaly detection and application change analysis. It is based on integration of two complementary techniques: (i) a regression-based transaction model that reflects a resource consumption model of the application, and (ii) an application performance signature that provides a compact model of runtime behavior of the application. The proposed integrated framework provides a simple and powerful solution for anomaly detection and analysis of essential performance changes in application behavior. An additional benefit of the proposed approach is its simplicity: It is not intrusive and is based on monitoring data that is typically available in enterprise production environments. The introduced solution further enables the automation of capacity planning and resource provisioning tasks of multitier applications in rapidly evolving IT environments.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2054506507",
    "type": "article"
  },
  {
    "title": "Rx",
    "doi": "https://doi.org/10.1145/1275517.1275519",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Feng Qin; Joseph Tucek; Yuanyuan Zhou; Jagadeesan Sundaresan",
    "corresponding_authors": "",
    "abstract": "Many applications demand availability. Unfortunately, software failures greatly reduce system availability. Prior work on surviving software failures suffers from one or more of the following limitations: required application restructuring, inability to address deterministic software bugs, unsafe speculation on program execution, and long recovery time. This paper proposes an innovative safe technique, called Rx, which can quickly recover programs from many types of software bugs, both deterministic and nondeterministic. Our idea, inspired from allergy treatment in real life, is to rollback the program to a recent checkpoint upon a software failure, and then to reexecute the program in a modified environment. We base this idea on the observation that many bugs are correlated with the execution environment, and therefore can be avoided by removing the “allergen” from the environment. Rx requires few to no modifications to applications and provides programmers with additional feedback for bug diagnosis. We have implemented Rx on Linux. Our experiments with five server applications that contain seven bugs of various types show that Rx can survive six out of seven software failures and provide transparent fast recovery within 0.017--0.16 seconds, 21--53 times faster than the whole program restart approach for all but one case (CVS). In contrast, the two tested alternatives, a whole program restart approach and a simple rollback and reexecution without environmental changes, cannot successfully recover the four servers (Squid, Apache, CVS, and ypserv) that contain deterministic bugs, and have only a 40% recovery rate for the server (MySQL) that contains a nondeterministic concurrency bug. Additionally, Rx's checkpointing system is lightweight, imposing small time and space overheads.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2122502383",
    "type": "article"
  },
  {
    "title": "Proactive obfuscation",
    "doi": "https://doi.org/10.1145/1813654.1813655",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Tom Roeder; Fred B. Schneider",
    "corresponding_authors": "",
    "abstract": "Proactive obfuscation is a new method for creating server replicas that are likely to have fewer shared vulnerabilities. It uses semantics-preserving code transformations to generate diverse executables, periodically restarting servers with these fresh versions. The periodic restarts help bound the number of compromised replicas that a service ever concurrently runs, and therefore proactive obfuscation makes an adversary's job harder. Proactive obfuscation was used in implementing two prototypes: a distributed firewall based on state-machine replication and a distributed storage service based on quorum systems. Costs intrinsic to supporting proactive obfuscation in replicated systems were evaluated by measuring the performance of these prototypes. The results show that employing proactive obfuscation adds little to the cost of replica-management protocols.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2294009246",
    "type": "article"
  },
  {
    "title": "CORFU",
    "doi": "https://doi.org/10.1145/2535930",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Mahesh Balakrishnan; Dahlia Malkhi; John D. Davis; Vijayan Prabhakaran; Michael Wei; Ted Wobber",
    "corresponding_authors": "",
    "abstract": "CORFU is a global log which clients can append-to and read-from over a network. Internally, CORFU is distributed over a cluster of machines in such a way that there is no single I/O bottleneck to either appends or reads. Data is fully replicated for fault tolerance, and a modest cluster of about 16--32 machines with SSD drives can sustain 1 million 4-KByte operations per second. The CORFU log enabled the construction of a variety of distributed applications that require strong consistency at high speeds, such as databases, transactional key-value stores, replicated state machines, and metadata services.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2074586513",
    "type": "article"
  },
  {
    "title": "L4 Microkernels",
    "doi": "https://doi.org/10.1145/2893177",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Gernot Heiser; Kevin Elphinstone",
    "corresponding_authors": "",
    "abstract": "The L4 microkernel has undergone 20 years of use and evolution. It has an active user and developer community, and there are commercial versions that are deployed on a large scale and in safety-critical systems. In this article we examine the lessons learnt in those 20 years about microkernel design and implementation. We revisit the L4 design articles and examine the evolution of design and implementation from the original L4 to the latest generation of L4 kernels. We specifically look at seL4, which has pushed the L4 model furthest and was the first OS kernel to undergo a complete formal verification of its implementation as well as a sound analysis of worst-case execution times. We demonstrate that while much has changed, the fundamental principles of minimality, generality, and high inter-process communication (IPC) performance remain the main drivers of design and implementation decisions.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2326370147",
    "type": "article"
  },
  {
    "title": "The Scalable Commutativity Rule",
    "doi": "https://doi.org/10.1145/2699681",
    "publication_date": "2015-01-20",
    "publication_year": 2015,
    "authors": "Austin T. Clements; M. Frans Kaashoek; Nickolai Zeldovich; Robert Morris; Eddie Kohler",
    "corresponding_authors": "",
    "abstract": "What opportunities for multicore scalability are latent in software interfaces, such as system call APIs? Can scalability challenges and opportunities be identified even before any implementation exists, simply by considering interface specifications? To answer these questions, we introduce the scalable commutativity rule: whenever interface operations commute, they can be implemented in a way that scales. This rule is useful throughout the development process for scalable multicore software, from the interface design through implementation, testing, and evaluation. This article formalizes the scalable commutativity rule. This requires defining a novel form of commutativity, SIM commutativity , that lets the rule apply even to complex and highly stateful software interfaces. We also introduce a suite of software development tools based on the rule. Our Commuter tool accepts high-level interface models, generates tests of interface operations that commute and hence could scale, and uses these tests to systematically evaluate the scalability of implementations. We apply Commuter to a model of 18 POSIX file and virtual memory system operations. Using the resulting 26,238 scalability tests, Commuter highlights Linux kernel problems previously observed to limit application scalability and identifies previously unknown bottlenecks that may be triggered by future workloads or hardware. Finally, we apply the scalable commutativity rule and Commuter to the design and implementation sv6, a new POSIX-like operating system. sv6’s novel file and virtual memory system designs enable it to scale for 99% of the tests generated by Commuter . These results translate to linear scalability on an 80-core x86 machine for applications built on sv6’s commutative operations.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2069852267",
    "type": "article"
  },
  {
    "title": "The IX Operating System",
    "doi": "https://doi.org/10.1145/2997641",
    "publication_date": "2016-12-09",
    "publication_year": 2016,
    "authors": "Adam Belay; George Prekas; Mia Primorac; Ana Klimovic; Samuel Grossman; Christos Kozyrakis; Edouard Bugnion",
    "corresponding_authors": "",
    "abstract": "The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and μs-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present ix , a dataplane operating system that provides high I/O performance and high resource efficiency while maintaining the protection and isolation benefits of existing kernels. ix uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The dataplane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to dataplane instances, processing bounded batches of packets to completion, and eliminating coherence traffic and multicore synchronization. The control plane dynamically adjusts core allocations and voltage/frequency settings to meet service-level objectives. We demonstrate that ix outperforms Linux and a user-space network stack significantly in both throughput and end-to-end latency. Moreover, ix improves the throughput of a widely deployed, key-value store by up to 6.4× and reduces tail latency by more than 2× . With three varying load patterns, the control plane saves 46%--54% of processor energy, and it allows background jobs to run at 35%--47% of their standalone throughput.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2560047851",
    "type": "article"
  },
  {
    "title": "A Virtualized Separation Kernel for Mixed-Criticality Systems",
    "doi": "https://doi.org/10.1145/2935748",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "Richard West; Ye Li; Eric Missimer; Matthew Danish",
    "corresponding_authors": "",
    "abstract": "Multi- and many-core processors are becoming increasingly popular in embedded systems. Many of these processors now feature hardware virtualization capabilities, as found on the ARM Cortex A15 and x86 architectures with Intel VT-x or AMD-V support. Hardware virtualization provides a way to partition physical resources, including processor cores, memory, and I/O devices, among guest virtual machines (VMs). Each VM is then able to host tasks of a specific criticality level, as part of a mixed-criticality system with different timing and safety requirements. However, traditional virtual machine systems are inappropriate for mixed-criticality computing. They use hypervisors to schedule separate VMs on physical processor cores. The costs of trapping into hypervisors to multiplex and manage machine physical resources on behalf of separate guests are too expensive for many time-critical tasks. Additionally, traditional hypervisors have memory footprints that are often too large for many embedded computing systems. In this article, we discuss the design of the Quest-V separation kernel, which partitions services of different criticality levels across separate VMs, or sandboxes . Each sandbox encapsulates a subset of machine physical resources that it manages without requiring intervention from a hypervisor. In Quest-V, a hypervisor is only needed to bootstrap the system, recover from certain faults, and establish communication channels between sandboxes. This not only reduces the memory footprint of the most privileged protection domain but also removes it from the control path during normal system operation, thereby heightening security.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2464364540",
    "type": "article"
  },
  {
    "title": "Improving Resource Efficiency at Scale with Heracles",
    "doi": "https://doi.org/10.1145/2882783",
    "publication_date": "2016-05-05",
    "publication_year": 2016,
    "authors": "David Lo; Liqun Cheng; Rama Govindaraju; Parthasarathy Ranganathan; Christos Kozyrakis",
    "corresponding_authors": "",
    "abstract": "User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy efficiency of large-scale datacenters. With the slowdown in technology scaling caused by the sunsetting of Moore’s law, it becomes important to address this opportunity. We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. We evaluate Heracles using production latency-critical and batch workloads from Google and demonstrate average server utilizations of 90% without latency violations across all the load and colocation scenarios that we evaluated.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2347137561",
    "type": "article"
  },
  {
    "title": "Validating JIT Compilers via Compilation Space Exploration",
    "doi": "https://doi.org/10.1145/3715102",
    "publication_date": "2025-02-14",
    "publication_year": 2025,
    "authors": "Cong Li; Yanyan Jiang; Chang Xu; Zhendong Su",
    "corresponding_authors": "",
    "abstract": "We introduce the concept of compilation space as a new pivot for the comprehensive validation of just-in-time (JIT) compilers in modern language virtual machines (LVMs). The compilation space of a program, encompasses a wide range of equivalent JIT-compilation choices, which can be cross-validated to ensure the correctness of the program’s JIT compilations. To thoroughly explore the compilation space in a lightweight and LVM-agnostic manner, we strategically mutate test programs with JIT-relevant but semantics-preserving code constructs, aiming to provoke diverse JIT compilation optimizations. We primarily implement this approach in Artemis , a tool for validating Java Virtual Machines (JVMs). Within three months, Artemis successfully discovered 85 bugs in three widely used production JVMs — HotSpot, OpenJ9, and the Android Runtime — where 53 were already confirmed or fixed and many of which were classified as critical. It is noteworthy that all reported bugs concern JIT compilers, highlighting the effectiveness and practicality of our technique. Building on the promising results with JVMs, we experimentally applied our technique to a state-of-the-art JavaScript Engine (JSE) fuzzer called Fuzzilli, aiming to augment it to find mis-compilation bugs without significantly sacrificing its ability to detect crashes. Our experiments demonstrate that our enhanced version of Fuzzilli namely Apollo could achieve comparable code coverage with a considerably smaller number of generated programs with a similar number of crashes. Additionally, Apollo successfully uncovered four mis-compilations in JavaScriptCore and SpiderMonkey within seven days. Following Artemis ’ and Apollo ’s success, we are expecting that the generality and practicability of our approach will make it broadly applicable for understanding and validating the JIT compilers of other LVMs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4407583718",
    "type": "article"
  },
  {
    "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
    "doi": "https://doi.org/10.1145/3768628",
    "publication_date": "2025-09-20",
    "publication_year": 2025,
    "authors": "Chao Jin; Zili Zhang; Xuanlin Jiang; Fangyue Liu; S. Liu; Xuanzhe Liu; Xin Jin",
    "corresponding_authors": "",
    "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated substantial advancements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, the retrieval step introduces long sequence generation and extra data dependency, resulting in long end-to-end latency. Our analysis benchmarks current RAG systems and reveals that, while the retrieval step poses performance challenges, it also offers optimization opportunities through its retrieval pattern and streaming search behavior. We propose RAGCache, a latency-optimized serving system tailored for RAG. RAGCache leverages the retrieval pattern to organize and cache the intermediate states of retrieved knowledge in a knowledge tree across the GPU and host memory hierarchy, reducing LLM generation time. RAGCache employs dynamic speculative pipelining to exploit the streaming search behavior, overlapping retrieval with LLM generation to minimize end-to-end latency. We implement RAGCache based on vLLM and Faiss, and evaluate it on both open-source and production datasets. Experimental results demonstrate that RAGCache reduces the time to first token (TTFT) by up to 4 × and improves the throughput by up to 2.1 × compared to vLLM integrated with Faiss.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4414371540",
    "type": "article"
  },
  {
    "title": "Scheduler-conscious synchronization",
    "doi": "https://doi.org/10.1145/244764.244765",
    "publication_date": "1997-02-01",
    "publication_year": 1997,
    "authors": "Leonidas Kontothanassis; Robert W. Wisniewski; Michael L. Scott",
    "corresponding_authors": "",
    "abstract": "Efficient synchronization is important for achieving good performance in parallel programs, especially on large-scale multiprocessors. Most synchronization algorithms have been designed to run on a dedicated machine, with one application process per processor, and can suffer serious performance degradation in the presence of multiprogramming. Problems arise when running processes block or, worse, busy-wait for action on the part of a process that the scheduler has chosen not to run. We show that these problems are particularly severe for scalable synchronization algorithms based on distributed data structures. We then describe and evaluate a set of algorithms that perform well in the presence of multiprogramming while maintaining good performance on dedicated machines. We consider both large and small machines, with a particular focus on scalability, and examine mutual-exclusion locks, reader-writer locks, and barriers. Our algorithms vary in the degree of support required from the kernel scheduler. We find that while it is possible to avoid pathological performance problems using previously proposed kernel mechanisms, a modest additional widening of the kernel/user interface can make scheduler-conscious synchronization algorithms significantly simpler and faster, with performance on dedicated machines comparable to that of scheduler-oblivious algorithms.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W1988780491",
    "type": "article"
  },
  {
    "title": "Specifying and using a partitionable group communication service",
    "doi": "https://doi.org/10.1145/377769.377776",
    "publication_date": "2001-05-01",
    "publication_year": 2001,
    "authors": "Alan Fekete; Nancy Lynch; Alex A. Shvartsman",
    "corresponding_authors": "",
    "abstract": "Group communication services are becoming accepted as effective building blocks for the construction of fault-tolerant distributed applications. Many specifications for group communication services have been proposed. However, there is still no agreement about what these specifications should say, especially in cases where the services are partitionable , i.e., where communication failures may lead to simultaneous creation of groups with disjoint memberships, such that each group is unware of the existence of any other group. In this paper, we present a new, succinct specification for a view-oriented partitionable group communication service. The service associates each message with a particular view of the group membership. All send and receive events for a message occur within the associated view. The service provides a total order on the messages within each view, and each processor receives a prefix of this order. Our specification separates safety requirements from performance and fault-tolerance requirements. The safety requirements are expressed by an abstract, global state machine . To present the performance and fault-tolerance requirements, we include failure-status input actions in the specification; we then give properties saying that consensus on the view and timely message delivery are guaranteed in an execution provided that the execution stabilizes to a situation in which the failure-status stops changing and corresponds to consistently partioned system. Because consensus is not required in every execution, the specification is not subject to the existing impossibility results for partionable systems. Our specification has a simple implementation, based on the membership algorithm of Christian and Schmuck. We show the utility of the specification by constructing an ordered-broadcast application, using an algorithm (based on algorithms of Amir, Dolev, Keidar, and others) that reconciles information derived from different instantiations of the group. The application manages the view-change activity to build a shared sequence of messages, i.e., the per-view total orders of the group service are combined to give a universal total order. We prove the correctness and analyze the performance and fault-tolerance of the resulting application.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2006007456",
    "type": "article"
  },
  {
    "title": "Algorithms for unboundedly parallel simulations",
    "doi": "https://doi.org/10.1145/128738.128739",
    "publication_date": "1991-08-01",
    "publication_year": 1991,
    "authors": "Albert Greenberg; Boris D. Lubachevsky; Isi Mitrani",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Algorithms for unboundedly parallel simulations Authors: Albert G. Greenberg AT&T Bell Laboratories, Murray Hill, NJ AT&T Bell Laboratories, Murray Hill, NJView Profile , Boris D. Lubachevsky University of Newcastle, Newcastle upon Tyne, NE1 7RU, UK University of Newcastle, Newcastle upon Tyne, NE1 7RU, UKView Profile , Isi Mitrani View Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 3Aug. 1991 pp 201–221https://doi.org/10.1145/128738.128739Online:01 August 1991Publication History 70citation404DownloadsMetricsTotal Citations70Total Downloads404Last 12 Months9Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2027110062",
    "type": "article"
  },
  {
    "title": "Quickly finding near-optimal storage designs",
    "doi": "https://doi.org/10.1145/1113574.1113575",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Eric Anderson; Susan Spence; Ram Swaminathan; Mahesh Kallahalla; Qian Wang",
    "corresponding_authors": "",
    "abstract": "Despite the importance of storage in enterprise computer systems, there are few adequate tools to design and configure a storage system to meet application data requirements efficiently. Storage system design involves choosing the disk arrays to use, setting the configuration options on those arrays, and determining an efficient mapping of application data onto the configured system. This is a complex process because of the multitude of disk array configuration options, and the need to take into account both capacity and potentially contending I/O performance demands when placing the data. Thus, both existing tools and administrators using rules of thumb often generate designs that are of poor quality.This article presents the Disk Array Designer (DAD), which is a tool that can be used both to guide administrators in their design decisions and to automate the design process. DAD uses a generalized best-fit bin packing heuristic with randomization and backtracking to search efficiently through the huge number of possible design choices. It makes decisions using device models that estimate storage system performance. We evaluate DAD's designs based on traces from a variety of database, filesystem, and e-mail workloads. We show that DAD can handle the difficult task of configuring midrange and high-end disk arrays, even with complex real-world workloads. We also show that DAD quickly generates near-optimal storage system designs, improving in both speed and quality over previous tools.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2082871877",
    "type": "article"
  },
  {
    "title": "Specialization tools and techniques for systematic optimization of system software",
    "doi": "https://doi.org/10.1145/377769.377778",
    "publication_date": "2001-05-01",
    "publication_year": 2001,
    "authors": "Dylan McNamee; Jonathan Walpole; Calton Pu; Crispin Cowan; Charles Krasic; Ashvin Goel; Perry Wagle; Charles Consel; Gilles Muller; Renauld Marlet",
    "corresponding_authors": "",
    "abstract": "Specialization has been recognized as a powerful technique for optimizing operating systems. However, specialization has not been broadly applied beyond the research community because current techniques based on manual specialization, are time-consuming and error-prone. The goal of the work described in this paper is to help operating system tuners perform specialization more easily. We have built a specialization toolkit that assists the major tasks of specializing operating systems. We demonstrate the effectiveness of the toolkit by applying it to three diverse operating system components. We show that using tools to assist specialization enables significant performance optimizations without error-prone manual modifications. Our experience with the toolkit suggests new ways of designing systems that combine high performance and clean structure.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2164319287",
    "type": "article"
  },
  {
    "title": "Using value prediction to increase the power of speculative execution hardware",
    "doi": "https://doi.org/10.1145/290409.290411",
    "publication_date": "1998-08-01",
    "publication_year": 1998,
    "authors": "Freddy Gabbay; Avi Mendelson",
    "corresponding_authors": "",
    "abstract": "This article presents an experimental and analytical study of value prediction and its impact on speculative execution in superscalar microprocessors. Value prediction is a new paradigm that suggests predicting outcome values of operations (at run-time ) and using these predicted values to trigger the execution of true-data-dependent operations speculatively. As a result, stals to memory locations can be reduced and the amount of instruction-level parallelism can be extended beyond the limits of the program's dataflow graph. This article examines the characteristics of the value prediction concept from two perspectives: (1) the related phenomena that are reflected in the nature of computer programs and (2) the significance of these phenomena to boosting instruction-level parallelism of superscalar microprocessors that support speculative execution. In order to better understand these characteristics, our work combines both analytical and experimental studies.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2053687647",
    "type": "article"
  },
  {
    "title": "Dynamic adaptation of real-time software",
    "doi": "https://doi.org/10.1145/103720.103723",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Thomas E. Bihari; Karsten Schwan",
    "corresponding_authors": "",
    "abstract": "In large, dynamic, real-time computer systems, it is frequently most cost effective to employ different software performance and reliability techniques at different levels of granularity, at different times, or within different subsystems. These techniques may include regulation of redundancy and resource allocation, multiversion and multipath execution, adjustments of program attributes such as time-out periods and others. The management of software in such systems is a difficult task. Software that may be adapted to meet varying performance and reliability requirements offers a solution. A REal-time Software Adaptation System (RESAS) includes a uniform model of adaptable software and provides the tool necessary for programmers to implement algorithms that choose and enact adaptations in real time. RESAS has been implemented on a testbed consisting of a multiprocessor and an attached workstation, and adaptation algorithms have been developed that address the problem of adapting software to achieve two goals: software execution within specified time constraints and software resiliency with respect to computer hardware failures.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2082649304",
    "type": "article"
  },
  {
    "title": "Implicit coscheduling",
    "doi": "https://doi.org/10.1145/380749.380764",
    "publication_date": "2001-08-01",
    "publication_year": 2001,
    "authors": "Andrea C. Arpaci-Dusseau",
    "corresponding_authors": "Andrea C. Arpaci-Dusseau",
    "abstract": "In modern distributed systems, coordinated time-sharing is required for communicating processes to leverage the performance of switch-based networks and low-overhead protocols. Coordinated time-sharing has traditionally been achieved with gang scheduling or explicit coscheduling, implementations of which often suffer from many deficiencies: multiple points of failure, high context-switch overheads, and poor interaction with client-server, interactive, and I/O -intensive workloads. Implicit coscheduling dynamically coordinates communicating processes across distributed machines without these structural deficiencies. In implicit coscheduling, no communication is required across operating systems schedulers; instead, cooperating processes achieve coordination by reacting to implicit information carried by communication existing within the parallel application. The implementation of this approach is simple and allows participating nodes to act autonomously. We introduce two key mechanisms in implicit coscheduling. The first is conditional two-phase waiting, a generalization of traditional two-phase waiting in which spin-time may be increased depending upon events occuring while the process waits. The second is an extension to stride scheduling that provides preemption and is fair to processes that block. To demonstrate that implicit coscheduling performs well, we show results from an extensive set of simulation and implementation experiments. To exercise the conditional two-phase waiting algorithm, we examine three workloads: bulk-synchronous and continuous-communication synthetic applications and application kernels written in the Split-C language. To exercise the local scheduler, we examine competing jobs with different communication characteristics. We demonstrate that our implementation scales well with the number of jobs and workstations and is robust to process placement. Our experiments show that implicit coscheduling is effective and fair for a wide range of workloads; most perform within 30% of an idealized model of gang scheduling.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2055228661",
    "type": "article"
  },
  {
    "title": "Increasing availability under mutual exclusion constraints with dynamic vote reassignment",
    "doi": "https://doi.org/10.1145/75104.75107",
    "publication_date": "1989-11-01",
    "publication_year": 1989,
    "authors": "Daniel Barbará; Héctor García-Molina; Annemarie Spauster",
    "corresponding_authors": "",
    "abstract": "Voting is used commonly to enforce mutual exclusion in distributed systems. Each node is assigned a number of votes, and only the group with a majority of votes is allowed to perform a restricted operation. This paper describes techniques for dynamically reassigning votes upon node or link failure, in an attempt to make the system more resilient to future failures. We focus on autonomous methods for achieving this, that is, methods that allow the nodes to make independent choices about changing their votes and picking new vote values, rather than group consensus techniques that require tight coordination among the remaining nodes. Protocols are given which allow nodes to install new vote values while still maintaining mutual exclusion requirements. The lemmas and theorems to validate the protocols are presented. A simple example shows how to apply the method to a database object-locking scheme; the protocols, however, are versatile and general purpose , and can be used for any application requiring mutual exclusion. In addition, policies are presented that allow nodes to autonomously select their new vote values. Simulation results are presented comparing the autonomous methods to static vote assignments and to group consensus strategies. These results demonstrate that under high failure rates, dynamic vote reassignment shows great improvement over a static assignment of votes in terms of availability. In addition, many autonomous methods for determining a new vote assignment yield almost as much availability as a group consensus method and at the same time are faster and more flexible.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2089248399",
    "type": "article"
  },
  {
    "title": "Decentralizing a global naming service for improved performance and fault tolerance",
    "doi": "https://doi.org/10.1145/63404.63406",
    "publication_date": "1989-05-01",
    "publication_year": 1989,
    "authors": "David R. Cheriton; Timothy Mann",
    "corresponding_authors": "",
    "abstract": "Naming is an important aspect of distributed system design. A naming system allows users and programs to assign character-string names to objects, and subsequently use the names to refer to those objects. With the interconnection of clusters of computers by wide-area networks and internetworks, the domain over which naming systems must function is growing to encompass the entire world. In this paper we address the problem of a global naming system, proposing a three-level naming architecture that consists of global, administrational, and managerial naming mechanisms, each optimized to meet the performance, reliability, and security requirements at its own level. We focus in particular on a decentralized approach to the lower levels, in which naming is handled directly by the managers of the named objects. Client-name caching and multicast are exploited to implement name mapping with almost optimum performance and fault tolerance. We also show how the naming system can be made secure. Our conclusions are bolstered by experience with an implementation in the V distributed operating system.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2123837909",
    "type": "article"
  },
  {
    "title": "The MVA priority approximation",
    "doi": "https://doi.org/10.1145/357401.357406",
    "publication_date": "1984-11-01",
    "publication_year": 1984,
    "authors": "Raymond M. Bryant; A. E. Krzesinski; M. Seetha Lakshmi; K. Mani Chandy",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on The MVA priority approximation Authors: Raymond M. Bryant IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NYView Profile , Anthony E. Krzesinski Institute for Applied Computer Science, University of Stellenbosch, 7600 Stellenbosch, South Africa Institute for Applied Computer Science, University of Stellenbosch, 7600 Stellenbosch, South AfricaView Profile , M. Seetha Lakshmi IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY IBM Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NYView Profile , K. Mani Chandy Department of Computer Science, The University of Texas at Austin, Austin, TX Department of Computer Science, The University of Texas at Austin, Austin, TXView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 4Nov. 1984 pp 335–359https://doi.org/10.1145/357401.357406Published:01 November 1984Publication History 51citation510DownloadsMetricsTotal Citations51Total Downloads510Last 12 Months30Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2057136734",
    "type": "article"
  },
  {
    "title": "Minimizing expected energy consumption in real-time systems through dynamic voltage scaling",
    "doi": "https://doi.org/10.1145/1314299.1314300",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Ruibin Xu; Daniel Mossé; Rami Melhem",
    "corresponding_authors": "",
    "abstract": "Many real-time systems, such as battery-operated embedded devices, are energy constrained. A common problem for these systems is how to reduce energy consumption in the system as much as possible while still meeting the deadlines; a commonly used power management mechanism by these systems is dynamic voltage scaling (DVS). Usually, the workloads executed by these systems are variable and, more often than not, unpredictable. Because of the unpredictability of the workloads, one cannot guarantee to minimize the energy consumption in the system. However, if the variability of the workloads can be captured by the probability distribution of the computational requirement of each task in the system, it is possible to achieve the goal of minimizing the expected energy consumption in the system. In this paper, we investigate DVS schemes that aim at minimizing expected energy consumption for frame-based hard real-time systems. Our investigation considers various DVS strategies (i.e., intra-task DVS, inter-task DVS, and hybrid DVS) and both an ideal system model (i.e., assuming unrestricted continuous frequency, well-defined power-frequency relation, and no speed change overhead) and a realistic system model (i.e., the processor provides a set of discrete speeds, no assumption is made on power-frequency relation, and speed change overhead is considered). The highlights of the investigation are two practical DVS schemes: Practical PACE (PPACE) for a single task and Practical Inter-Task DVS (PITDVS2) for general frame-based systems. Evaluation results show that our proposed schemes outperform and achieve significant energy savings over existing schemes.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2039088753",
    "type": "article"
  },
  {
    "title": "A key distribution protocol using event markers",
    "doi": "https://doi.org/10.1145/357369.357373",
    "publication_date": "1983-08-01",
    "publication_year": 1983,
    "authors": "Rocio Bauer; Tom Berson; R. J. Feiertag",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A key distribution protocol using event markers Authors: R. K. Bauer Sytek, Inc., 1225 Charleston Rd., Mountain View, CA Sytek, Inc., 1225 Charleston Rd., Mountain View, CAView Profile , T. A. Berson Sytek, Inc., 1225 Charleston Rd., Mountain View, CA Sytek, Inc., 1225 Charleston Rd., Mountain View, CAView Profile , R. J. Feiertag Sytek, Inc., 1225 Charleston Rd., Mountain View, CA Sytek, Inc., 1225 Charleston Rd., Mountain View, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 3August 1983 pp 249–255https://doi.org/10.1145/357369.357373Published:01 August 1983Publication History 49citation872DownloadsMetricsTotal Citations49Total Downloads872Last 12 Months32Last 6 weeks11 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1969090042",
    "type": "article"
  },
  {
    "title": "RaWMS - Random Walk Based Lightweight Membership Service for Wireless Ad Hoc Networks",
    "doi": "https://doi.org/10.1145/1365815.1365817",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Ziv Bar-Yossef; Roy Friedman; Gabriel Kliot",
    "corresponding_authors": "",
    "abstract": "This article presents RaWMS, a novel lightweight random membership service for ad hoc networks. The service provides each node with a partial uniformly chosen view of network nodes. Such a membership service is useful, for example, in data dissemination algorithms, lookup and discovery services, peer sampling services, and complete membership construction. The design of RaWMS is based on a novel reverse random walk (RW) sampling technique. The article includes a formal analysis of both the reverse RW sampling technique and RaWMS and verifies it through a detailed simulation study. In addition, RaWMS is compared both analytically and by simulations with a number of other known methods such as flooding and gossip-based techniques.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2066750882",
    "type": "article"
  },
  {
    "title": "The costs and limits of availability for replicated services",
    "doi": "https://doi.org/10.1145/1124153.1124156",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Haifeng Yu; Amin Vahdat",
    "corresponding_authors": "",
    "abstract": "As raw system performance continues to improve at exponential rates, the utility of many services is increasingly limited by availability rather than performance. A key approach to improving availability involves replicating the service across multiple, wide-area sites. However, replication introduces well-known trade-offs between service consistency and availability. Thus, this article explores the benefits of dynamically trading consistency for availability using a continuous consistency model . In this model, applications specify a maximum deviation from strong consistency on a per-replica basis. In this article, we: i) evaluate the availability of a prototype replication system running across the Internet as a function of consistency level, consistency protocol, and failure characteristics, ii) demonstrate that simple optimizations to existing consistency protocols result in significant availability improvements (more than an order of magnitude in some scenarios), iii) use our experience with these optimizations to prove tight upper bound on the availability of services, and iv) show that maximizing availability typically entails remaining as close to strong consistency as possible during times of good connectivity, resulting in a communication versus availability trade-off.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2099105350",
    "type": "article"
  },
  {
    "title": "Disk arm movement in anticipation of future requests",
    "doi": "https://doi.org/10.1145/99926.99930",
    "publication_date": "1990-08-01",
    "publication_year": 1990,
    "authors": "Richard P. King",
    "corresponding_authors": "Richard P. King",
    "abstract": "When a disk drive's access arm is idle, it may not be at the ideal location. In anticipation of future requests, movement to some other location may be advantageous. The effectiveness of anticipatory disk arm movement is explored. Various operating conditions are considered, and the reduction in seek distances and request response times is determined for them. Suppose that successive requests are independent and uniformly distributed. By bringing the arm to the middle of its range of motion when it is idle, the expected seek distance can be reduced by 25 percent. Nonlinearity in time versus distance can whittle that 25 percent reduction down to a 13 percent reduction in seek time. Nonuniformity in request location, nonPoisson arrival processes, and high arrival rates can whittle the reduction down to nothing. However, techniques are discussed that maximize those savings that are still possible under those circumstances. Various systems with multiple arms are analyzed. Usually, it is best to spread out the arms over the disk area. The both arms should be brought to the middle.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2141834169",
    "type": "article"
  },
  {
    "title": "Adaptive work-stealing with parallelism feedback",
    "doi": "https://doi.org/10.1145/1394441.1394443",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Kunal Agrawal; Charles E. Leiserson; Yuxiong He; Wen Jing Hsu",
    "corresponding_authors": "",
    "abstract": "Multiprocessor scheduling in a shared multiprogramming environment can be structured as two-level scheduling, where a kernel-level job scheduler allots processors to jobs and a user-level thread scheduler schedules the work of a job on its allotted processors. We present a randomized work-stealing thread scheduler for fork-join multithreaded jobs that provides continual parallelism feedback to the job scheduler in the form of requests for processors. Our A-STEAL algorithm is appropriate for large parallel servers where many jobs share a common multiprocessor resource and in which the number of processors available to a particular job may vary during the job's execution. Assuming that the job scheduler never allots a job more processors than requested by the job's thread scheduler, A-STEAL guarantees that the job completes in near-optimal time while utilizing at least a constant fraction of the allotted processors. We model the job scheduler as the thread scheduler's adversary, challenging the thread scheduler to be robust to the operating environment as well as to the job scheduler's administrative policies. For example, the job scheduler might make a large number of processors available exactly when the job has little use for them. To analyze the performance of our adaptive thread scheduler under this stringent adversarial assumption, we introduce a new technique called trim analysis, which allows us to prove that our thread scheduler performs poorly on no more than a small number of time steps, exhibiting near-optimal behavior on the vast majority. More precisely, suppose that a job has work T 1 and span T ∞ . On a machine with P processors, A-STEAL completes the job in an expected duration of O ( T 1 / P˜ + T ∞ + L lg P ) time steps, where L is the length of a scheduling quantum, and P˜ denotes the O ( T ∞ + L lg P )-trimmed availability. This quantity is the average of the processor availability over all time steps except the O ( T ∞ + L lg P ) time steps that have the highest processor availability. When the job's parallelism dominates the trimmed availability, that is, P˜ &lt; T 1 / T ∞ , the job achieves nearly perfect linear speedup. Conversely, when the trimmed mean dominates the parallelism, the asymptotic running time of the job is nearly the length of its span, which is optimal. We measured the performance of A-STEAL on a simulated multiprocessor system using synthetic workloads. For jobs with sufficient parallelism, our experiments confirm that A-STEAL provides almost perfect linear speedup across a variety of processor availability profiles. We compared A-STEAL with the ABP algorithm, an adaptive work-stealing thread scheduler developed by Arora et al. [1998] which does not employ parallelism feedback. On moderately to heavily loaded machines with large numbers of processors, A-STEAL typically completed jobs more than twice as quickly as ABP, despite being allotted the same number or fewer processors on every step, while wasting only 10% of the processor cycles wasted by ABP.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1976922833",
    "type": "article"
  },
  {
    "title": "The profile naming service",
    "doi": "https://doi.org/10.1145/48012.48013",
    "publication_date": "1988-11-01",
    "publication_year": 1988,
    "authors": "Larry Peterson",
    "corresponding_authors": "Larry Peterson",
    "abstract": "Profile is a descriptive naming service used to identify users and organizations. This paper presents a structural overview of Profile's three major components: a confederation of attribute-based name servers, a name space abstraction that unifies the name servers, and a user interface that integrates the name space with existing naming systems. Each name server is an independent authority that allows clients to describe users and organizations with a multiplicity of attributes; the name space abstraction is a client program that implements a discipline for searching a sequence of name servers; and the interface provides a tool with which users build customized commands. Experience with an implementation in the DARPA/NSF Internet demonstrates that Profile is a feasible and effective mechanism for naming users and organizations in a large internet.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2064453341",
    "type": "article"
  },
  {
    "title": "High-speed implementations of rule-based systems",
    "doi": "https://doi.org/10.1145/63404.63405",
    "publication_date": "1989-05-01",
    "publication_year": 1989,
    "authors": "Aman Gupta; Charles L. Forgy; Allen Newell",
    "corresponding_authors": "",
    "abstract": "Rule-based systems are widely used in artificial intelligence for modeling intelligent behavior and building expert systems. Most rule-based programs, however, are extremely computation intensive and run quite slowly. The slow speed of execution has prohibited the use of rule-based systems in domains requiring high performance and real-time response. In this paper we explore various methods for speeding up the execution of rule-based systems. In particular, we examine the role of parallelism in the high-speed execution of rule-based systems and study the architectural issues in the design of computers for rule-based systems. Our results show that contrary to initial expectations, the speed-up that can be obtained from parallelism is quite limited, only about tenfold. The reasons for the small speed-up are: (1) the small number of rules relevant to each change to data memory; (2) the large variation in the processing requirements of relevant rules; and (3) the small number of changes made to data memory between synchronization steps. Furthermore, we observe that to obtain this limited factor of tenfold speed-up, it is necessary to exploit parallelism at a very fine granularity. We propose that a suitable architecture to exploit such fine-grain parallelism is a shared-memory multiprocessor with 32-64 processors. Using such a multiprocessor, it is possible to obtain execution speeds of about 3800 rule-firings/set. This speed is significantly higher than that obtained by other proposed parallel implementations of rule-based systems.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2022247428",
    "type": "article"
  },
  {
    "title": "Sinfonia",
    "doi": "https://doi.org/10.1145/1629087.1629088",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "Marcos K. Aguilera; Arif Merchant; Mehul A. Shah; Alistair Veitch; Christos Karamanolis",
    "corresponding_authors": "",
    "abstract": "We propose a new paradigm for building scalable distributed systems. Our approach does not require dealing with message-passing protocols, a major complication in existing distributed systems. Instead, developers just design and manipulate data structures within our service called Sinfonia. Sinfonia keeps data for applications on a set of memory nodes, each exporting a linear address space. At the core of Sinfonia is a new minitransaction primitive that enables efficient and consistent access to data, while hiding the complexities that arise from concurrency and failures. Using Sinfonia, we implemented two very different and complex applications in a few months: a cluster file system and a group communication service. Our implementations perform well and scale to hundreds of machines.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2070300501",
    "type": "article"
  },
  {
    "title": "A VLSI layout for a pipelined Dadda multiplier",
    "doi": "https://doi.org/10.1145/357360.357366",
    "publication_date": "1983-05-01",
    "publication_year": 1983,
    "authors": "P. Cappello; Kenneth Steiglitz",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A VLSI layout for a pipelined Dadda multiplier Authors: Peter R. Cappello Dept. of Computer Science, University of California, Santa Barbara, CA Dept. of Computer Science, University of California, Santa Barbara, CAView Profile , Kenneth Steiglitz Dept. of Electrical Engineering and Computer Science, Princeton University, Princeton, NJ Dept. of Electrical Engineering and Computer Science, Princeton University, Princeton, NJView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 2pp 157–174https://doi.org/10.1145/357360.357366Published:01 May 1983Publication History 52citation1,330DownloadsMetricsTotal Citations52Total Downloads1,330Last 12 Months62Last 6 weeks25 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2075255008",
    "type": "article"
  },
  {
    "title": "DieCast",
    "doi": "https://doi.org/10.1145/1963559.1963560",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Diwaker Gupta; Kashi Venkatesh Vishwanath; Marvin McNett; Amin Vahdat; Ken Yocum; Alex C. Snoeren; Geoffrey M. Voelker",
    "corresponding_authors": "",
    "abstract": "Large-scale network services can consist of tens of thousands of machines running thousands of unique software configurations spread across hundreds of physical networks. Testing such services for complex performance problems and configuration errors remains a difficult problem. Existing testing techniques, such as simulation or running smaller instances of a service, have limitations in predicting overall service behavior at such scales. Testing large services should ideally be done at the same scale and configuration as the target deployment, which can be technically and economically infeasible. We present DieCast , an approach to scaling network services in which we multiplex all of the nodes in a given service configuration as virtual machines across a much smaller number of physical machines in a test harness. We show how to accurately scale CPU, network, and disk to provide the illusion that each VM matches a machine in the original service in terms of both available computing resources and communication behavior. We present the architecture and evaluation of a system we built to support such experimentation and discuss its limitations. We show that for a variety of services---including a commercial high-performance cluster-based file system---and resource utilization levels, DieCast matches the behavior of the original service while using a fraction of the physical resources.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2048014508",
    "type": "article"
  },
  {
    "title": "Efficient Testing of Recovery Code Using Fault Injection",
    "doi": "https://doi.org/10.1145/2063509.2063511",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Paul Dan Marinescu; George Candea",
    "corresponding_authors": "",
    "abstract": "A critical part of developing a reliable software system is testing its recovery code. This code is traditionally difficult to test in the lab, and, in the field, it rarely gets to run; yet, when it does run, it must execute flawlessly in order to recover the system from failure. In this article, we present a library-level fault injection engine that enables the productive use of fault injection for software testing. We describe automated techniques for reliably identifying errors that applications may encounter when interacting with their environment, for automatically identifying high-value injection targets in program binaries, and for producing efficient injection test scenarios. We present a framework for writing precise triggers that inject desired faults, in the form of error return codes and corresponding side effects, at the boundary between applications and libraries. These techniques are embodied in LFI, a new fault injection engine we are distributing http://lfi.epfl.ch. This article includes a report of our initial experience using LFI. Most notably, LFI found 12 serious, previously unreported bugs in the MySQL database server, Git version control system, BIND name server, Pidgin IM client, and PBFT replication system with no developer assistance and no access to source code. LFI also increased recovery-code coverage from virtually zero up to 60% entirely automatically without requiring new tests or human involvement.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W1973883609",
    "type": "article"
  },
  {
    "title": "Power Limitations and Dark Silicon Challenge the Future of Multicore",
    "doi": "https://doi.org/10.1145/2324876.2324879",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Hadi Esmaeilzadeh; Emily Blem; Renée St. Amant; Karthikeyan Sankaralingam; Doug Burger",
    "corresponding_authors": "",
    "abstract": "Since 2004, processor designers have increased core counts to exploit Moore’s Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area/performance and power/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21% of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50%. Through 2024, only 7.9× average speedup is possible across commonly used parallel workloads for the topologies we study, leaving a nearly 24-fold gap from a target of doubled performance per generation.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2049334739",
    "type": "article"
  },
  {
    "title": "Efficient Reuse Distance Analysis of Multicore Scaling for Loop-Based Parallel Programs",
    "doi": "https://doi.org/10.1145/2427631.2427632",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Meng-Ju Wu; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "Reuse Distance (RD) analysis is a powerful memory analysis tool that can potentially help architects study multicore processor scaling. One key obstacle, however, is that multicore RD analysis requires measuring Concurrent Reuse Distance (CRD) and Private-LRU-stack Reuse Distance (PRD) profiles across thread-interleaved memory reference streams. Sensitivity to memory interleaving makes CRD and PRD profiles architecture dependent, preventing them from analyzing different processor configurations. For loop-based parallel programs, CRD and PRD profiles shift coherently across RD values with core count scaling because interleaving threads are symmetric. Simple techniques can predict such shifting, making the analysis of numerous multicore configurations from a small set of CRD and PRD profiles feasible. Given the ubiquity of parallel loops, such techniques will be extremely valuable for studying future large multicore designs. This article investigates using RD analysis to efficiently analyze multicore cache performance for loop-based parallel programs, making several contributions. First, we provide an in-depth analysis on how CRD and PRD profiles change with core count scaling. Second, we develop techniques to predict CRD and PRD profile scaling, in particular employing reference groups [Zhong et al. 2003] to predict coherent shift, demonstrating 90% or greater prediction accuracy. Third, our CRD and PRD profile analyses define two application parameters with architectural implications: C core is the minimum shared cache capacity that “contains” locality degradation due to core count scaling, and C share is the capacity at which shared caches begin to provide a cache-miss reduction compared to private caches. And fourth, we apply CRD and PRD profiles to analyze multicore cache performance. When combined with existing problem scaling prediction, our techniques can predict shared LLC MPKI (private L2 cache MPKI) to within 10.7% (13.9%) of simulation across 1,728 (1,440) configurations using only 36 measured CRD (PRD) profiles.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2020035025",
    "type": "article"
  },
  {
    "title": "SKMD",
    "doi": "https://doi.org/10.1145/2798725",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "Janghaeng Lee; Mehrzad Samadi; Yongjun Park; Scott Mahlke",
    "corresponding_authors": "",
    "abstract": "Heterogeneous computing on CPUs and GPUs has traditionally used fixed roles for each device: the GPU handles data parallel work by taking advantage of its massive number of cores while the CPU handles non data-parallel work, such as the sequential code or data transfer management. This work distribution can be a poor solution as it underutilizes the CPU, has difficulty generalizing beyond the single CPU-GPU combination, and may waste a large fraction of time transferring data. Further, CPUs are performance competitive with GPUs on many workloads, thus simply partitioning work based on the fixed roles may be a poor choice. In this article, we present the single-kernel multiple devices (SKMD) system, a framework that transparently orchestrates collaborative execution of a single data-parallel kernel across multiple asymmetric CPUs and GPUs. The programmer is responsible for developing a single data-parallel kernel in OpenCL, while the system automatically partitions the workload across an arbitrary set of devices, generates kernels to execute the partial workloads, and efficiently merges the partial outputs together. The goal is performance improvement by maximally utilizing all available resources to execute the kernel. SKMD handles the difficult challenges of exposed data transfer costs and the performance variations GPUs have with respect to input size. On real hardware, SKMD achieves an average speedup of 28% on a system with one multicore CPU and two asymmetric GPUs compared to a fastest device execution strategy for a set of popular OpenCL kernels.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1987773847",
    "type": "article"
  },
  {
    "title": "The Arm Triple Core Lock-Step (TCLS) Processor",
    "doi": "https://doi.org/10.1145/3323917",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Xabier Iturbe; Balaji Venu; Emre Özer; Jean-Luc Poupat; Grégoire Gimenez; Hans-Ulrich Zurek",
    "corresponding_authors": "",
    "abstract": "The Arm Triple Core Lock-Step (TCLS) architecture is the natural evolution of Arm Cortex-R Dual Core Lock-Step (DCLS) processors to increase dependability, predictability, and availability in safety-critical and ultra-reliable applications. TCLS is simple, scalable, and easy to deploy in applications where Arm DCLS processors are widely used (e.g., automotive), as well as in new sectors where the presence of Arm technology is incipient (e.g., enterprise) or almost non-existent (e.g., space). Specifically in space, COTS Arm processors provide optimal power-to-performance, extensibility, evolvability, software availability, and ease of use, especially in comparison with the decades old rad-hard computing solutions that are still in use. This article discusses the fundamentals of an Arm Cortex-R5 based TCLS processor, providing key functioning and implementation details. The article shows that the TCLS architecture keeps the use of rad-hard technology to a minimum, namely, using rad-hard by design standard cell libraries only to protect the critical parts that account for less than 4% of the entire TCLS solution. Moreover, when exposure to radiation is relatively low, such as in terrestrial applications or even satellites operating in Low Earth Orbits (LEO), the system could be implemented entirely using commercial cell libraries, relying on the radiation mitigation methods implemented on the TCLS to cope with sporadic soft errors in its critical parts. The TCLS solution allows thus to significantly reduce chip manufacturing costs and keep pace with advances in low power consumption and high density integration by leveraging commercial semiconductor processes, while matching the reliability levels and improving availability that can be achieved using extremely expensive rad-hard semiconductor processes. Finally, the article describes a TRL4 proof-of-concept TCLS-based System-on-Chip (SoC) that has been prototyped and tested to power the computer on-board an Airbus Defence and Space telecom satellite. When compared to the currently used processor solution by Airbus, the TCLS-based SoC results in a more than 5× performance increase and cuts power consumption by more than half.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2950730845",
    "type": "article"
  },
  {
    "title": "Building Consistent Transactions with Inconsistent Replication",
    "doi": "https://doi.org/10.1145/3269981",
    "publication_date": "2017-11-30",
    "publication_year": 2017,
    "authors": "Irene Zhang; Naveen Sharma; Adriana Szekeres; Arvind Krishnamurthy; Dan R. K. Ports",
    "corresponding_authors": "",
    "abstract": "Application programmers increasingly prefer distributed storage systems with strong consistency and distributed transactions (e.g., Google’s Spanner) for their strong guarantees and ease of use. Unfortunately, existing transactional storage systems are expensive to use—in part, because they require costly replication protocols, like Paxos, for fault tolerance. In this article, we present a new approach that makes transactional storage systems more affordable: We eliminate consistency from the replication protocol, while still providing distributed transactions with strong consistency to applications. We present the Transactional Application Protocol for Inconsistent Replication (TAPIR), the first transaction protocol to use a novel replication protocol, called inconsistent replication , that provides fault tolerance without consistency. By enforcing strong consistency only in the transaction protocol, TAPIR can commit transactions in a single round-trip and order distributed transactions without centralized coordination. We demonstrate the use of TAPIR in a transactional key-value store, TAPIR-KV . Compared to conventional systems, TAPIR-KV provides better latency and better throughput.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2996998733",
    "type": "article"
  },
  {
    "title": "Pivot Tracing",
    "doi": "https://doi.org/10.1145/3208104",
    "publication_date": "2017-11-30",
    "publication_year": 2017,
    "authors": "Jonathan Mace; Ryan Roelke; Rodrigo Fonseca",
    "corresponding_authors": "",
    "abstract": "Monitoring and troubleshooting distributed systems is notoriously difficult; potential problems are complex, varied, and unpredictable. The monitoring and diagnosis tools commonly used today—logs, counters, and metrics—have two important limitations: what gets recorded is defined a priori , and the information is recorded in a component- or machine-centric way, making it extremely hard to correlate events that cross these boundaries. This article presents Pivot Tracing, a monitoring framework for distributed systems that addresses both limitations by combining dynamic instrumentation with a novel relational operator: the happened-before join. Pivot Tracing gives users, at runtime, the ability to define arbitrary metrics at one point of the system, while being able to select, filter, and group by events meaningful at other parts of the system, even when crossing component or machine boundaries. We have implemented a prototype of Pivot Tracing for Java-based systems and evaluate it on a heterogeneous Hadoop cluster comprising HDFS, HBase, MapReduce, and YARN. We show that Pivot Tracing can effectively identify a diverse range of root causes such as software bugs, misconfiguration, and limping hardware. We show that Pivot Tracing is dynamic, extensible, and enables cross-tier analysis between inter-operating applications, with low execution overhead.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2902547091",
    "type": "article"
  },
  {
    "title": "UNIQ",
    "doi": "https://doi.org/10.1145/3444943",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Chaim Baskin; Natan Liss; Eli Schwartz; Evgenii Zheltonozhskii; Raja Giryes; Alex Bronstein; Avi Mendelson",
    "corresponding_authors": "",
    "abstract": "We present a novel method for neural network quantization. Our method, named UNIQ , emulates a non-uniform k -quantile quantizer and adapts the model to perform well with quantized weights by injecting noise to the weights at training time. As a by-product of injecting noise to weights, we find that activations can also be quantized to as low as 8-bit with only a minor accuracy degradation. Our non-uniform quantization approach provides a novel alternative to the existing uniform quantization techniques for neural networks. We further propose a novel complexity metric of number of bit operations performed (BOPs), and we show that this metric has a linear relation with logic utilization and power. We suggest evaluating the trade-off of accuracy vs. complexity (BOPs). The proposed method, when evaluated on ResNet18/34/50 and MobileNet on ImageNet, outperforms the prior state of the art both in the low-complexity regime and the high accuracy regime. We demonstrate the practical applicability of this approach, by implementing our non-uniformly quantized CNN on FPGA.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3146781226",
    "type": "article"
  },
  {
    "title": "Moshe",
    "doi": "https://doi.org/10.1145/566340.566341",
    "publication_date": "2002-08-01",
    "publication_year": 2002,
    "authors": "Idit Keidar; Jeremy B. Sussman; Keith Marzullo; Danny Dolev",
    "corresponding_authors": "",
    "abstract": "We present Moshe, a novel scalable group membership algorithm built specifically for use in wide area networks (WANs), which can suffer partitions. Moshe is designed with three new significant features that are important in this setting: it avoids delivering views that reflect out-of-date memberships; it requires a single round of messages in the common case; and it employs a client-server design for scalability. Furthermore, Moshe's interface supplies the hooks needed to provide clients with full virtual synchrony semantics. We have implemented Moshe on top of a network event mechanism also designed specifically for use in a WAN. In addition to specifying the properties of the algorithm and proving that this specification is met, we provide empirical results of an implementation of Moshe running over the Internet. The empirical results justify the assumptions made by our design and exhibit good performance. In particular, Moshe terminates within a single communication round over 98% of the time. The experimental results also lead to interesting observations regarding the performance of membership algorithms over the Internet.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2024627782",
    "type": "article"
  },
  {
    "title": "Tolerating latency in multiprocessors through compiler-inserted prefetching",
    "doi": "https://doi.org/10.1145/273011.273021",
    "publication_date": "1998-02-01",
    "publication_year": 1998,
    "authors": "Todd C. Mowry",
    "corresponding_authors": "Todd C. Mowry",
    "abstract": "The large latency of memory accesses in large-scale shared-memory multiprocessors is a key obstacle to achieving high processor utilization. Software-controlled prefetching is a technique for tolerating memory latency by explicitly executing instructions to move data close to the processor before the data are actually needed. To minimize the burden on the programmer, compiler support is needed to automatically insert prefetch instructions into the code. A key challenge when inserting prefetches is ensuring that the overheads of prefetching do not outweigh the benefits. While previous studies have demonstrated the effectiveness of hand-inserted prefetching in multiprocessor applications, the benefit of compiler-inserted prefetching in practice has remained an open question. This article proposes and evaluates a new compiler algorithm for inserting prefetches into multiprocessor code. The proposed algorithm attempts to minimize overheads by only issuing prefetches for references that are predicted to suffer cache misses. The algorithm can prefetch both dense-matrix and sparse-matrix codes, thus covering a large fraction of scientific applications. We have implemented our algorithm in the SUIF(Stanford University Intermediate Format) optimizing compiler. The results of our detailed architectural simulations demonstrate that compiler-inserted prefetching can improve the speed of some parallel applications by as much as a factor of two.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2043613230",
    "type": "article"
  },
  {
    "title": "A SMART scheduler for multimedia applications",
    "doi": "https://doi.org/10.1145/762483.762484",
    "publication_date": "2003-05-01",
    "publication_year": 2003,
    "authors": "Jason Nieh; Monica S. Lam",
    "corresponding_authors": "",
    "abstract": "Real-time applications such as multimedia audio and video are increasingly populating the workstation desktop. To support the execution of these applications in conjunction with traditional non-real-time applications, we have created SMART, a Scheduler for Multimedia And Real-Time applications. SMART supports applications with time constraints, and provides dynamic feedback to applications to allow them to adapt to the current load. In addition, the support for real-time applications is integrated with the support for conventional computations. This allows the user to prioritize across real-time and conventional computations, and dictate how the processor is to be shared among applications of the same priority. As the system load changes, SMART adjusts the allocation of resources dynamically and seamlessly. It can dynamically shed real-time computations and regulate the execution rates of real-time tasks when the system is overloaded, while providing better value in underloaded conditions than previously proposed schemes.We have implemented SMART in the Solaris UNIX operating system and measured its performance against other schedulers commonly used in research and practice in executing real-time, interactive, and batch applications. Our experimental results demonstrate SMART's superior performance over fair queueing and UNIX SVR4 schedulers in supporting multimedia applications.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2160284242",
    "type": "article"
  },
  {
    "title": "Implications of hierarchical N-body methods for multiprocessor architectures",
    "doi": "https://doi.org/10.1145/201045.201050",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Jaswinder Pal Singh; John L. Hennessy; Anoop Gupta",
    "corresponding_authors": "",
    "abstract": "To design effective large-scale multiprocessors, designers need to understand the characteristics of the applications that will use the machines. Application characteristics of particular interest include the amount of communication relative to computation, the structure of the communication, and the local cache and memory requirements, as well as how these characteristics scale with larger problems and machines. One important class of applications is based on hierarchical N-body methods, which are used to solve a wide range of scientific and engineering problems efficiently. Important characteristics of these methods include the nonuniform and dynamically changing nature of the domains to which they are applied, and their use of long-range, irregular communication. This article examines the key architectural implications of representative applications that use the two dominant hierarchical N-body methods: the Barnes-Hut Method and the Fast Multipole Method. We first show that exploiting temporal locality on accesses to communicated data is critical to obtaining good performance on these applications and then argue that coherent caches on shared-address-space machines exploit this locality both automatically and very effectively. Next, we examine the implications of scaling the applications to run on larger machines. We use scaling methods that reflect the concerns of the application scientist and find that this leads to different conclusions about how communication traffic and local cache and memory usage scale than scaling based only on data set size. In particular, we show that under the most realistic form of scaling, both the communication-to-computation ratio as well as the working-set size (and hence the ideal cache size per processor) grow slowly as larger problems are run on larger machines. Finally, we examine the effects of using the two dominant abstractions for interprocessor communication: a shared address space and explicit message passing between private address spaces. We show that the lack of an efficiently supported shared address space will substantially increase the programming complexity and performance overheads for these applications.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W1985364893",
    "type": "article"
  },
  {
    "title": "Parallel program performance prediction using deterministic task graph analysis",
    "doi": "https://doi.org/10.1145/966785.966788",
    "publication_date": "2004-02-01",
    "publication_year": 2004,
    "authors": "Vikram Adve; Mary K. Vernon",
    "corresponding_authors": "",
    "abstract": "In this article, we consider analytical techniques for predicting detailed performance characteristics of a single shared memory parallel program for a particular input. Analytical models for parallel programs have been successful at providing simple qualitative insights and bounds on program scalability, but have been less successful in practice for providing detailed insights and metrics for program performance (leaving these to measurement or simulation). We develop a conceptually simple modeling technique called deterministic task graph analysis that provides detailed performance prediction for shared-memory programs with arbitrary task graphs, a wide variety of task scheduling policies, and significant communication and resource contention. Unlike many previous models that are stochastic models, our model assumes deterministic task execution times (while retaining the use of stochastic models for communication and resource contention). This assumption is supported by a previous study of the influence of nondeterministic delays in parallel programs.We evaluate our model in three ways. First, an experimental evaluation shows that our analysis technique is accurate and efficient for a variety of shared-memory programs, including programs with large and/or complex task graphs, sophisticated task scheduling, highly nonuniform task times, and significant communication and resource contention. The results also show that the deterministic assumption is crucial to permit accurate and yet efficient analysis of these programs. Second, we use three example programs to illustrate the predictive capabilities of the model. In two cases, broad insights and detailed metrics from the model are used to suggest improvements in load-balancing and the model quickly and accurately predicts the impact of these changes. In the third case, the model provides novel insights into the impact of program design changes that improve communication locality as well as load-balancing, via new (but general-purpose) metrics. Finally, we present results from a comparison of our model and representative stochastic models, and use these to characterize the conditions under which a deterministic model or stochastic models would be appropriate.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2056026474",
    "type": "article"
  },
  {
    "title": "Value-based clock gating and operation packing",
    "doi": "https://doi.org/10.1145/350853.350856",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "David Brooks; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "The large address space needs of many current applications have pushed processor designs toward 64-bit word widths. Although full 64-bit addresses and operations are indeed sometimes needed, arithmetic operations on much smaller quantities are still more common. In fact, another instruction set trend has been the introduction of instructions geared toward subword operations on 16-bit quantities. For examples, most major processors now include instruction set support for multimedia operations allowing parallel execution of several subword operations in the same ALU. This article presents our observations demonstrating that operations on “narrow-width” quantities are common not only in multimedia codes, but also in more general workloads. In fact, across the SPECint95 benchmarks, over half the integer operation executions require 16 bits or less. Based on this data, we propose two hardware mechanisms that dynamically recognize and capitalize on these narrow-width operations. The first, power-oriented optimization reduces processor power consumption by using operand-value-based clock gating to turn off portions of arithmetic units that will be unused by narrow-width operations. This optimization results in a 45%-60% reduction in the integer unit's power consumption for the SPECint95 and MediaBench benchmark suites. Applying this optimization to SPECfp95 benchmarks results in slightly smaller power reductions, but still seems warranted. These reductions in integer unit power consumption equate to a 5%-10% full-chip power savings. Our second, performance-oriented optimization improves processor performance by packing together narrow-width operations so that they share a single arithmetic unit. Conceptually similar to a dynamic form of MMX, this optimization offers speedups of 4.3%-6.2% for SPECint95 and 8.0%-10.4% for MediaBench. Overall, these optimizations highlight an increasing opportunity for value-based optimizations to improve both power and performance in current microprocessors.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W1976735509",
    "type": "article"
  },
  {
    "title": "Access normalization",
    "doi": "https://doi.org/10.1145/161541.159766",
    "publication_date": "1993-11-01",
    "publication_year": 1993,
    "authors": "Wei Li; Keshav Pingali",
    "corresponding_authors": "",
    "abstract": "In scalable parallel machines, processors can make local memory accesses much faster than they can make remote memory accesses. Additionally, when a number of remote accesses must be made, it is usually more efficient to use block transfers of data rather than to use many small messages. To run well on such machines, software must exploit these features. We believe it is too onerous for a programmer to do this by hand, so we have been exploring the use of restructuring compiler technology for this purpose. In this article, we start with a language like HPF-Fortran with user-specified data distribution and develop a systematic loop transformation strategy called access normalization that restructures loop nests to exploit locality and block transfers. We demonstrate the power of our techniques using routines from the BLAS (Basic Linear Algebra Subprograms) library. An important feature of our approach is that we model loop transformation using invertible matrices and integer lattice theory.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W1998246731",
    "type": "article"
  },
  {
    "title": "Scalable high-speed prefix matching",
    "doi": "https://doi.org/10.1145/502912.502914",
    "publication_date": "2001-11-01",
    "publication_year": 2001,
    "authors": "Marcel Waldvogel; George Varghese; Jon Turner; Bernhard Plattner",
    "corresponding_authors": "",
    "abstract": "Finding the longest matching prefix from a database of keywords is an old problem with a number of applications, ranging from dictionary searches to advanced memory management to computational geometry. But perhaps today's most frequent best matching prefix lookups occur in the Internet, when forwarding packets from router to router. Internet traffic volume and link speeds are rapidly increasing; at the same time, a growing user population is increasing the size of routing tables against which packets must be matched. Both factors make router prefix matching extremely performance critical.In this paper, we introduce a taxonomy for prefix matching technologies, which we use as a basis for describing, categorizing, and comparing existing approaches. We then present in detail a fast scheme using binary search over hash tables, which is especially suited for matching long addresses, such as the 128 bit addresses proposed for use in the next generation Internet Protocol, IPv6. We also present optimizations that exploit the structure of existing databases to further improve access time and reduce storage space.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2147629974",
    "type": "article"
  },
  {
    "title": "UFO",
    "doi": "https://doi.org/10.1145/290409.290410",
    "publication_date": "1998-08-01",
    "publication_year": 1998,
    "authors": "Albert D. Alexandrov; Maximilian Ibel; Klaus E. Schauser; Chris J. Scheiman",
    "corresponding_authors": "",
    "abstract": "In this article we show how to extend a wide range of functionality of standard operation systems completely at the user level. Our approach works by intercepting selected system calls at the user level, using tracing facilities such as the /proc file system provided by many Unix operating systems. The behavior of some intercepted system calls is then modified to implement new functionality. This approach does not require any relinking or recompilation of existing applications. In fact, the extensions can even be dynamically “installed” into already running processes. The extensions work completely at the user level and install without system administrator assistance. Individual users can choose what extensions to run, in effect creating a personalized operating system view for themselves. We used this approach to implement a global file system, called Ufo, which allows users to treat remote files exactly as if they were local. Currently, Ufo supports file access through the FTP and HTTP protocols and allows new protocols to be plugged in. While several other projects have implemented global file system abstractions, they all require either changes to the operating system or modifications to standard libraries. The article gives a detailed performance analysis of our approach to extending the OS and establishes that Ufo introduces acceptable overhead for common applications even though intercepting individual system calls incurs a high cost.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2050870325",
    "type": "article"
  },
  {
    "title": "Compiler-based I/O prefetching for out-of-core applications",
    "doi": "https://doi.org/10.1145/377769.377774",
    "publication_date": "2001-05-01",
    "publication_year": 2001,
    "authors": "Angela Demke Brown; Todd C. Mowry; Orran Krieger",
    "corresponding_authors": "",
    "abstract": "Current operating systems offer poor performance when a numeric application's working set does not fit in main memory. As a result, programmers who wish to solve “out-of-core” problems efficiently are typically faced with the onerous task of rewriting an application to use explicit I/O operations (e.g., read/write). In this paper, we propose and evaluate a fully automatic technique which liberates the programmer from this task, provides high performance, and requires only minimal changes to current operating systems. In our scheme the compiler provides the crucial information on future access patterns without burdening the programmer; the operating system supports nonbinding prefetch and release hints for managing I/O; and the operating systems cooperates with a run-time layer to accelerate performance by adapting to dynamic behavior and minimizing prefetch overhead. This approach maintains the abstraction of unlimited virtual memory for the programmer, gives the compiler the flexibility to aggressively insert prefetches ahead of references, and gives the operating system the flexibility to arbitrate between the competing resource demands of multiple applications. We implemented our compiler analysis within the SUIF compiler, and used it to target implementations of our run-time and OS support on both research and commercial systems (Hurricane and IRIX 6.5, respectively). Our experimental results show large performance gains for out-of-core scientific applications on both systems: more than 50% of the I/O stall time has been eliminated in most cases, thus translating into overall speedups of roughly twofold in many cases.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2146247503",
    "type": "article"
  },
  {
    "title": "A security architecture for fault-tolerant systems",
    "doi": "https://doi.org/10.1145/195792.195823",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Michael K. Reiter; Kenneth P. Birman; Robbert van Renesse",
    "corresponding_authors": "",
    "abstract": "Process groups are a common abstraction for fault-tolerant computing in distributed systems. We present a security architecture that extends the process group into a security abstraction. Integral parts of this architecture are services that securely and fault tolerantly support cryptographic key distribution. Using replication only when necessary, and introducing novel replication techniques when it was necessary, we have constructed these services both to be easily defensible against attack and to permit key distribution despite the transient unavailability of a substantial number of servers. We detail the design and implementation of these services and the secure process group abstraction they support. We also give preliminary performance figures for some common group operations.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2012219340",
    "type": "article"
  },
  {
    "title": "Efficient trace-driven simulation methods for cache performance analysis",
    "doi": "https://doi.org/10.1145/128738.128740",
    "publication_date": "1991-08-01",
    "publication_year": 1991,
    "authors": "Wen-Hann Wang; Jean-Loup Baer",
    "corresponding_authors": "",
    "abstract": "article Free AccessEfficient trace-driven simulation methods for cache performance analysis Authors: Wen-Hann Wang IBM AWD Future Systems Technology IBM AWD Future Systems TechnologyView Profile , Jean-Loup Baer University of Washington University of WashingtonView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 3pp 222–241https://doi.org/10.1145/128738.128740Published:01 August 1991Publication History 43citation751DownloadsMetricsTotal Citations43Total Downloads751Last 12 Months38Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2075183718",
    "type": "article"
  },
  {
    "title": "The Totem multiple-ring ordering and topology maintenance protocol",
    "doi": "https://doi.org/10.1145/279227.279228",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "D. Agarwal; L.E. Moser; P. M. Melliar‐Smith; R.K. Budhia",
    "corresponding_authors": "",
    "abstract": "The Totem multiple-ring protocol provides reliable totally ordered delivery of messages across multiple local-area networks interconnected by gateways. This consistent message order is maintained in the presence of network partitioning and remerging, and of processor failure and recovery. The protocol provides accurate topology change information as part of the global total order of messages. It addresses the issue of scalability and achieves a latency that increases logarithmically with system size by exploiting process group locality and selective forwarding of messages through the gateways. Pseudocode for the protocol and an evaluation of its performance are given. —Authors' Abstract",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2085963802",
    "type": "article"
  },
  {
    "title": "An empirical study of a wide-area distributed file system",
    "doi": "https://doi.org/10.1145/227695.227698",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Mirjana Spasojevic; Mahadev Satyanarayanan",
    "corresponding_authors": "",
    "abstract": "The evolution of the Andrew File System (AFS) into a wide-area distributed file system has encouraged collaboration and information dissemination on a much broader scale than ever before. We examine AFS as a provider of wide-area file services to over 100 organizations around the world. We discuss usage characteristics of AFS derived from empirical measurements of the system. Our observations indicate that AFS provides robust and efficient data access in its current configuration, thus confirming its viability as a design point for wide-area distributed file systems.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W2087922754",
    "type": "article"
  },
  {
    "title": "Managing stored voice in the Etherphone system",
    "doi": "https://doi.org/10.1145/35037.35038",
    "publication_date": "1988-02-01",
    "publication_year": 1988,
    "authors": "Douglas B. Terry; Daniel C. Swinehart",
    "corresponding_authors": "",
    "abstract": "The voice manager in the Etherphone system provides facilities for recording, editing, and playing stored voice in a distributed personal-computing environment. It provides the basis for applications such as voice mail, annotation of multimedia documents, and voice editing using standard text-editing techniques. To facilitate sharing, the voice manager stores voice on a special voice file server that is accessible via the local internet. Operations for editing a passage of recorded voice simply build persistent data structures to represent the edited voice. These data structures, implementing an abstraction called voice ropes , are stored in a server database and consist of lists of intervals within voice files. Clients refer to voice ropes solely by reference. Interests , additional persistent data structures maintained by the server, serve two purposes: First, they provide a sort of directory service for managing the voice ropes that have been created. More importantly, they provide a reliable reference-counting mechanism, permitting the garbage collection of voice ropes that are no longer needed. These interests are grouped into classes; for some important classes, obsolete interests can be detected and deleted by a class-specific algorithm that runs periodically.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2044780953",
    "type": "article"
  },
  {
    "title": "The Alpine file system",
    "doi": "https://doi.org/10.1145/6110.6111",
    "publication_date": "1985-11-01",
    "publication_year": 1985,
    "authors": "Mark R. Brown; K. N. Kolling; Edward Taft",
    "corresponding_authors": "",
    "abstract": "Alpine is a file system that supports atomic transactions and is designed to operate as a service on a computer network. Alpine's primary purpose is to store files that represent databases. An important secondary goal is to store ordinary files representing documents, program modules, and the like. Unlike other file servers described in the literature, Alpine uses a log-based technique to implement atomic file update. Another unusual aspect of Alpine is that it performs all communication via a general-purpose remote procedure call facility. Both of these decisions have worked out well. This paper describes Alpine's design and implementation, and evaluates the system in light of our experience to date. Alpine is written in Cedar, a strongly typed modular programming language that includes garbage-collected storage. We report on using the Cedar language and programming environment to develop Alpine.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2038598470",
    "type": "article"
  },
  {
    "title": "Response times in level-structured systems",
    "doi": "https://doi.org/10.1145/24068.24069",
    "publication_date": "1987-08-01",
    "publication_year": 1987,
    "authors": "Paul K. Harter",
    "corresponding_authors": "Paul K. Harter",
    "abstract": "Real-time programs are among the most critical programs in use today, yet they are also among the worst understood and the most difficult to verify. Validation of real-time systems is nonetheless extremely important in view of the high costs associated with failure in typical application areas. We present here a method for deriving response-time properties in complex systems with a level structure based on priority. The method involves a level-by-level examination of the system, in which information distilled from each successive level is used to adjust the results for later levels. The results obtained at each level of the system are not affected by later analyses, which obviates having to consider a complex system as a whole.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2064280844",
    "type": "article"
  },
  {
    "title": "High-performance operating system primitives for robotics and real-time control systems",
    "doi": "https://doi.org/10.1145/24068.24070",
    "publication_date": "1987-08-01",
    "publication_year": 1987,
    "authors": "Karsten Schwan; Tom Bihari; Bruce W. Weide; Gregor Taulbee",
    "corresponding_authors": "",
    "abstract": "To increase speed and reliability of operation, multiple computers are replacing uniprocessors and wired-logic controllers in modern robots and industrial control systems. However, performance increases are not attained by such hardware alone. The operating software controlling the robots or control systems must exploit the possible parallelism of various control tasks in order to perform the necessary computations within given real-time and reliability constraints. Such software consists of both control programs written by application programmers and operating system software offering means of task scheduling, intertask communication, and device control. The Generalized Executive for real-time Multiprocessor applications (GEM) is an operating system that addresses several requirements of operating software. First, when using GEM, programmers can select one of two different types of tasks differing in size, called processes and microprocesses. Second, the scheduling calls offered by GEM permit the implementation of several models of task interaction. Third, GEM supports multiple models of communication with a parameterized communication mechanism. Fourth, GEM is closely coupled to prototype real-time programming environments that provide programming support for the models of computation offered by the operating system. GEM is being used on a multiprocessor with robotics application software of substantial size and complexity.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W1998443198",
    "type": "article"
  },
  {
    "title": "Remote pipes and procedures for efficient distributed communication",
    "doi": "https://doi.org/10.1145/45059.45061",
    "publication_date": "1988-08-01",
    "publication_year": 1988,
    "authors": "David K. Gifford; Nathan Glasser",
    "corresponding_authors": "",
    "abstract": "We describe a new communication model for distributed systems that combines the advantages of remote procedure call with the efficient transfer of bulk data. Three ideas form the basis of this model. First, remote procedures are first-class values which can be freely exchanged among nodes, thus enabling a greater variety of protocols to be directly implemented in a remote procedure call framework. Second, a new type of abstract object, called a pipe , allows bulk data and incremental results to be efficiently transported in a type-safe manner. Unlike procedure calls, pipe calls do not return values and do not block a caller. Data sent down a pipe is received by the pipe's sink node in the order sent. Third, the relative sequencing of pipes and procedures can be controlled by combining them into channel groups . Calls on the members of a channel group are guaranteed to be processed in order. Application experience with this model, which we call the Channel Model , is reported. Derived performance bounds and experimental measures demonstrate k pipe calls can perform min ( 1 + ( r / p ), k ) times faster than k procedure calls, where r is the total roundtrip remote communication time and p is the procedure execution time.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2005671536",
    "type": "article"
  },
  {
    "title": "A discipline for constructing multiphase communication protocols",
    "doi": "https://doi.org/10.1145/6110.214400",
    "publication_date": "1985-11-01",
    "publication_year": 1985,
    "authors": "C.-H. Chow; Mohamed G. Gouda; Simon S. Lam",
    "corresponding_authors": "",
    "abstract": "Many communication protocols can be observed to go through different phases performing a distinct function in each phase. A multiphase model for such protocols is presented. A phase is formally defined to be a network of communicating finite-state machines with certain desirable correctness properties; these include proper termination and freedom from deadlocks and unspecified receptions. A multifunction protocol is constructed by first constructing separate phases to perform its different functions. It is shown how to connect these phases together to realize the multifunction protocol so that the resulting network of communicating finite state machines is also a phase (i.e., it possesses the desirable properties defined for phases). The modularity inherent in multiphase protocols facilitates not only their construction but also their understanding and modification. An abundance of protocols have been found in the literature that can be constructed as multiphase protocols. Three examples are presented here: two versions of IBM's BSC protocol for data link control and a token ring network protocol.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2044311901",
    "type": "article"
  },
  {
    "title": "The vulnerability of vote assignments",
    "doi": "https://doi.org/10.1145/6420.6421",
    "publication_date": "1986-08-01",
    "publication_year": 1986,
    "authors": "Daniel Barbará; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "In a faulty distributed system, voting is commonly used to achieve mutual exclusion among groups of nodes. Each node is assigned a number of votes, and any group with a majority of votes can perform the critical operations. Vote assignments can have a significant impact on system reliability, and in this paper we study the vote assignment problem. To compare vote assignments we define two deterministic measures, node and edge vulnerability. We present various properties of these measures and discuss how they can be computed. For these measures we discuss the selection of the best assignment and propose heuristics to identify good candidate assignments.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W1971617887",
    "type": "article"
  },
  {
    "title": "Experience distributing objects in an SMMP OS",
    "doi": "https://doi.org/10.1145/1275517.1275518",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Jonathan Appavoo; Dilma Da Silva; Orran Krieger; Marc Auslander; M. Ostrowski; Bryan S. Rosenburg; Amos Waterland; Robert W. Wisniewski; Jimi Xenidis; Michael Stumm; Livio Soares",
    "corresponding_authors": "",
    "abstract": "Designing and implementing system software so that it scales well on shared-memory multiprocessors (SMMPs) has proven to be surprisingly challenging. To improve scalability, most designers to date have focused on concurrency by iteratively eliminating the need for locks and reducing lock contention. However, our experience indicates that locality is just as, if not more, important and that focusing on locality ultimately leads to a more scalable system. In this paper, we describe a methodology and a framework for constructing system software structured for locality, exploiting techniques similar to those used in distributed systems. Specifically, we found two techniques to be effective in improving scalability of SMMP operating systems: ( i ) an object-oriented structure that minimizes sharing by providing a natural mapping from independent requests to independent code paths and data structures, and ( ii ) the selective partitioning, distribution, and replication of object implementations in order to improve locality. We describe concrete examples of distributed objects and our experience implementing them. We demonstrate that the distributed implementations improve the scalability of operating-system-intensive parallel workloads.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2088723365",
    "type": "article"
  },
  {
    "title": "DDoS defense by offense",
    "doi": "https://doi.org/10.1145/1731060.1731063",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Michael Walfish; Mythili Vutukuru; Hari Balakrishnan; David R. Karger; Scott Shenker",
    "corresponding_authors": "",
    "abstract": "This article presents the design, implementation, analysis, and experimental evaluation of speak-up , a defense against application-level distributed denial-of-service (DDoS), in which attackers cripple a server by sending legitimate-looking requests that consume computational resources (e.g., CPU cycles, disk). With speak-up, a victimized server encourages all clients, resources permitting, to automatically send higher volumes of traffic . We suppose that attackers are already using most of their upload bandwidth so cannot react to the encouragement. Good clients, however, have spare upload bandwidth so can react to the encouragement with drastically higher volumes of traffic. The intended outcome of this traffic inflation is that the good clients crowd out the bad ones, thereby capturing a much larger fraction of the server's resources than before. We experiment under various conditions and find that speak-up causes the server to spend resources on a group of clients in rough proportion to their aggregate upload bandwidths, which is the intended result.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W1984479361",
    "type": "article"
  },
  {
    "title": "Throughput optimal total order broadcast for cluster environments",
    "doi": "https://doi.org/10.1145/1813654.1813656",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "Rachid Guerraoui; Ron R. Levy; Bastian Pochon; Vivien Quéma",
    "corresponding_authors": "",
    "abstract": "Total order broadcast is a fundamental communication primitive that plays a central role in bringing cheap software-based high availability to a wide range of services. This article studies the practical performance of such a primitive on a cluster of homogeneous machines. We present LCR, the first throughput optimal uniform total order broadcast protocol. LCR is based on a ring topology. It only relies on point-to-point inter-process communication and has a linear latency with respect to the number of processes. LCR is also fair in the sense that each process has an equal opportunity of having its messages delivered by all processes. We benchmark a C implementation of LCR against Spread and JGroups, two of the most widely used group communication packages. LCR provides higher throughput than the alternatives, over a large number of scenarios.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2121898964",
    "type": "article"
  },
  {
    "title": "Leveraging Core Specialization via OS Scheduling to Improve Performance on Asymmetric Multicore Systems",
    "doi": "https://doi.org/10.1145/2166879.2166880",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Juan Carlos Sáez; Alexandra Fedorova; David Koufaty; Manuel Prieto",
    "corresponding_authors": "",
    "abstract": "Asymmetric multicore processors (AMPs) consist of cores with the same ISA (instruction-set architecture), but different microarchitectural features, speed, and power consumption. Because cores with more complex features and higher speed typically use more area and consume more energy relative to simpler and slower cores, we must use these cores for running applications that experience significant performance improvements from using those features. Having cores of different types in a single system allows optimizing the performance/energy trade-off. To deliver this potential to unmodified applications, the OS scheduler must map threads to cores in consideration of the properties of both. Our work describes a Comprehensive scheduler for Asymmetric Multicore Processors (CAMP) that addresses shortcomings of previous asymmetry-aware schedulers. First, previous schedulers catered to only one kind of workload properties that are crucial for scheduling on AMPs; either efficiency or thread-level parallelism (TLP), but not both. CAMP overcomes this limitation showing how using both efficiency and TLP in synergy in a single scheduling algorithm can improve performance. Second, most existing schedulers relying on models for estimating how much faster a thread executes on a “fast” vs. “slow” core (i.e., the speedup factor ) were specifically designed for AMP systems where cores differ only in clock frequency. However, more realistic AMP systems include cores that differ more significantly in their features. To demonstrate the effectiveness of CAMP on more realistic scenarios, we augmented the CAMP scheduler with a model that predicts the speedup factor on a real AMP prototype that closely matches future asymmetric systems.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2059530893",
    "type": "article"
  },
  {
    "title": "Optimizing the Block I/O Subsystem for Fast Storage Devices",
    "doi": "https://doi.org/10.1145/2619092",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Young Jin Yu; Dong In Shin; Woong Shin; Nae Young Song; Jae Woo Choi; Hyeong Seog Kim; Hyeonsang Eom; Heon Y. Yeom",
    "corresponding_authors": "",
    "abstract": "Fast storage devices are an emerging solution to satisfy data-intensive applications. They provide high transaction rates for DBMS, low response times for Web servers, instant on-demand paging for applications with large memory footprints, and many similar advantages for performance-hungry applications. In spite of the benefits promised by fast hardware, modern operating systems are not yet structured to take advantage of the hardware’s full potential. The software overhead caused by an OS, negligible in the past, adversely impacts application performance, lessening the advantage of using such hardware. Our analysis demonstrates that the overheads from the traditional storage-stack design are significant and cannot easily be overcome without modifying the hardware interface and adding new capabilities to the operating system. In this article, we propose six optimizations that enable an OS to fully exploit the performance characteristics of fast storage devices. With the support of new hardware interfaces, our optimizations minimize per-request latency by streamlining the I/O path and amortize per-request latency by maximizing parallelism inside the device. We demonstrate the impact on application performance through well-known storage benchmarks run against a Linux kernel with a customized SSD. We find that eliminating context switches in the I/O path decreases the software overhead of an I/O request from 20 microseconds to 5 microseconds and a new request merge scheme called Temporal Merge enables the OS to achieve 87% to 100% of peak device performance, regardless of request access patterns or types. Although the performance improvement by these optimizations on a standard SATA-based SSD is marginal (because of its limited interface and relatively high response times), our sensitivity analysis suggests that future SSDs with lower response times will benefit from these changes. The effectiveness of our optimizations encourages discussion between the OS community and storage vendors about future device interfaces for fast storage devices.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2007929969",
    "type": "article"
  },
  {
    "title": "A File Is Not a File",
    "doi": "https://doi.org/10.1145/2324876.2324878",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Tyler Harter; Chris Dragga; Michael Vaughn; Andrea C. Arpaci-Dusseau; Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "",
    "abstract": "We analyze the I/O behavior of iBench , a new collection of productivity and multimedia application workloads. Our analysis reveals a number of differences between iBench and typical file-system workload studies, including the complex organization of modern files, the lack of pure sequential access, the influence of underlying frameworks on I/O patterns, the widespread use of file synchronization and atomic operations, and the prevalence of threads. Our results have strong ramifications for the design of next generation local and cloud-based storage systems.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2080332495",
    "type": "article"
  },
  {
    "title": "DoublePlay",
    "doi": "https://doi.org/10.1145/2110356.2110359",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Kaushik Veeraraghavan; Dongyoon Lee; Benjamin Wester; Jessica Ouyang; Peter M. Chen; Jason Flinn; Satish Narayanasamy",
    "corresponding_authors": "",
    "abstract": "Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order of or the values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals ( epochs ) of the program concurrently on separate processors. This strategy, which we call uniparallelism , makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15% with two worker threads and 28% with four threads.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W1972304371",
    "type": "article"
  },
  {
    "title": "A Differential Approach to Undefined Behavior Detection",
    "doi": "https://doi.org/10.1145/2699678",
    "publication_date": "2015-03-11",
    "publication_year": 2015,
    "authors": "Xi Wang; Nickolai Zeldovich; M. Frans Kaashoek; Armando Solar-Lezama",
    "corresponding_authors": "",
    "abstract": "This article studies undefined behavior arising in systems programming languages such as C/C++. Undefined behavior bugs lead to unpredictable and subtle systems behavior, and their effects can be further amplified by compiler optimizations. Undefined behavior bugs are present in many systems, including the Linux kernel and the Postgres database. The consequences range from incorrect functionality to missing security checks. This article proposes a formal and practical approach that finds undefined behavior bugs by finding “unstable code” in terms of optimizations that leverage undefined behavior. Using this approach, we introduce a new static checker called S tack that precisely identifies undefined behavior bugs. Applying S tack to widely used systems has uncovered 161 new bugs that have been confirmed and fixed by developers.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1987099882",
    "type": "article"
  },
  {
    "title": "Derecho",
    "doi": "https://doi.org/10.1145/3302258",
    "publication_date": "2018-05-31",
    "publication_year": 2018,
    "authors": "Sagar Jha; Jonathan Behrens; Theo Gkountouvas; Mae Milano; Weijia Song; Edward Tremel; Robbert van Renesse; Sydney Zink; Kenneth P. Birman",
    "corresponding_authors": "",
    "abstract": "Cloud computing services often replicate data and may require ways to coordinate distributed actions. Here we present Derecho, a library for such tasks. The API provides interfaces for structuring applications into patterns of subgroups and shards, supports state machine replication within them, and includes mechanisms that assist in restart after failures. Running over 100Gbps RDMA, Derecho can send millions of events per second in each subgroup or shard and throughput peaks at 16GB/s, substantially outperforming prior solutions. Configured to run purely on TCP, Derecho is still substantially faster than comparable widely used, highly-tuned, standard tools. The key insight is that on modern hardware (including non-RDMA networks), data-intensive protocols should be built from non-blocking data-flow components.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2929031942",
    "type": "article"
  },
  {
    "title": "Reliability Analysis of SSDs Under Power Fault",
    "doi": "https://doi.org/10.1145/2992782",
    "publication_date": "2016-11-01",
    "publication_year": 2016,
    "authors": "Mai Zheng; Joseph Tucek; Feng Qin; Mark Lillibridge; Bill W. Zhao; Elizabeth S. Yang",
    "corresponding_authors": "",
    "abstract": "Modern storage technology (solid-state disks (SSDs), NoSQL databases, commoditized RAID hardware, etc.) brings new reliability challenges to the already-complicated storage stack. Among other things, the behavior of these new components during power faults—which happen relatively frequently in data centers—is an important yet mostly ignored issue in this dependability-critical area. Understanding how new storage components behave under power fault is the first step towards designing new robust storage systems. In this article, we propose a new methodology to expose reliability issues in block devices under power faults. Our framework includes specially designed hardware to inject power faults directly to devices, workloads to stress storage components, and techniques to detect various types of failures. Applying our testing framework, we test 17 commodity SSDs from six different vendors using more than three thousand fault injection cycles in total. Our experimental results reveal that 14 of the 17 tested SSD devices exhibit surprising failure behaviors under power faults, including bit corruption, shorn writes, unserializable writes, metadata corruption, and total device failure.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2548592627",
    "type": "article"
  },
  {
    "title": "GiantSan: Efficient Operation-Level Memory Sanitization with Segment Folding",
    "doi": "https://doi.org/10.1145/3742426",
    "publication_date": "2025-06-14",
    "publication_year": 2025,
    "authors": "Hao Ling; Heqing Huang; Chengpeng Wang; Yuandao Cai; Charles Zhang",
    "corresponding_authors": "",
    "abstract": "Memory safety sanitizers, the sharp weapon for detecting invalid memory operations during execution, employ runtime metadata to model the memory and help find memory errors hidden in the programs. However, location-based methods, the most widely deployed memory sanitization methods thanks to high compatibility, face the low protection density issue: the number of bytes safeguarded by one metadata is limited. As a result, numerous memory accesses require loading excessive metadata, leading to a high runtime overhead. To address this issue, we propose a new shadow encoding with segment folding to increase the protection density. Specifically, we characterize neighboring bytes with identical metadata by building novel summaries, called folded segments , on those bytes to reduce unnecessary metadata loadings. The new encoding uses less metadata to safeguard large memory regions with fewer instructions than existing works, speeding up memory sanitization. We implement our designed technique as GiantSan . Our evaluation using the SPEC CPU 2017 benchmark shows that GiantSan outperforms the state-of-the-art sanitization methods with 61.37% and 41.94% less runtime overhead than ASan and ASan–, respectively. Moreover, under the same redzone setting, GiantSan detects 463 fewer false negative cases than ASan and ASan-- in testing the real-world project PHP.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4411309194",
    "type": "article"
  },
  {
    "title": "Fast and flexible application-level networking on exokernel systems",
    "doi": "https://doi.org/10.1145/505452.505455",
    "publication_date": "2002-02-01",
    "publication_year": 2002,
    "authors": "Gregory R. Ganger; Dawson Engler; M. Frans Kaashoek; Héctor M. Briceño; Russell Hunt; Thomas Pinckney",
    "corresponding_authors": "",
    "abstract": "Application-level networking is a promising software organization for improving performance and functionality for important network services. The Xok/ExOS exokernel system includes application-level support for standard network services, while at the same time allowing application writers to specialize networking services. This paper describes how Xok/ExOS's kernel mechanisms and library operating system organization achieve this flexibility, and retrospectively shares our experiences and lessons learned (both positive and negative). It also describes how we used this flexibility to build and specialize three network data services: the Cheetah HTTP server, the webswamp Web benchmarking tool, and an application-level TCP forwarder. Overall measurements show large performance improvements relative to similar services built on conventional interfaces, in each case reaching the maximum possible end-to-end performance for the experimental platform. For example, Cheetah provides factor of 2--4 increases in throughput compared to highly tuned socket-based implementations and factor of 3--8 increases compared to conventional systems. Webswamp can offer loads that are two to eight times heavier. The TCP forwarder provides 50--300% higher throughput while also providing end-to-end TCP semantics that cannot be achieved with POSIX sockets. With more detailed measurements and profiling, these overall performance improvements are also broken down and attributed to the specific specializations described, providing server writers with insights into where to focus their optimization efforts.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2121160206",
    "type": "article"
  },
  {
    "title": "Memory system performance of programs with intensive heap allocation",
    "doi": "https://doi.org/10.1145/210126.210129",
    "publication_date": "1995-08-01",
    "publication_year": 1995,
    "authors": "Amer Diwan; David Tarditi; J. Eliot B. Moss",
    "corresponding_authors": "",
    "abstract": "Heap allocation with copying garbage collection is a general storage management technique for programming languages. It is believed to have poor memory system performance. To investigate this, we conducted an in-depth study of the memory system performance of heap allocation for memory systems found on many machines. We studied the performance of mostly functional Standard ML programs which made heavy use of heap allocation. We found that most machines support heap allocation poorly. However, with the appropriate memory system organization, heap allocation can have good performance. The memory system property crucial for achieving good performance was the ability to allocate and initialize a new object into the cache without a penalty. This can be achieved by having subblock by placement with a subblock size of one word with a write-allocate policy, along with fast page-mode writes or a write buffer. For caches with subblock placement, the data cache overhead was under 9% for a 64K or larger data cache; without subblock placement the overhead was often higher than 50%.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2154787154",
    "type": "article"
  },
  {
    "title": "Lightweight recoverable virtual memory",
    "doi": "https://doi.org/10.1145/174613.174615",
    "publication_date": "1994-02-01",
    "publication_year": 1994,
    "authors": "Mahadev Satyanarayanan; Henry H. Mashburn; Puneet Kumar; David C. Steere; James J. Kistler",
    "corresponding_authors": "",
    "abstract": "Recoverable virtual memory refers to regions of a virtual address space on which transactional guarantees are offered. This article describes RVM, an efficient, portable, and easily used implementation of recoverable virtual memory for Unix environments. A unique characteristic of RVM is that it allows independent control over the transactional properties of atomicity, permanence, and serializability. This leads to considerable flexibility in the use of RVM, potentially enlarging the range of applications that can benefit from transactions. It also simplifies the layering of functionality such as nesting and distribution. The article shows that RVM performs well over its intended range of usage even though it does not benefit from specialized operating system support. It also demonstrates the importance of intra- and inter-transaction optimizations.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W4231885549",
    "type": "article"
  },
  {
    "title": "Waiting algorithms for synchronization in large-scale multiprocessors",
    "doi": "https://doi.org/10.1145/152864.152869",
    "publication_date": "1993-08-01",
    "publication_year": 1993,
    "authors": "Beng-Hong Lim; Anant Agarwal",
    "corresponding_authors": "",
    "abstract": "Through analysis and experiments, this paper investigates two-phase waiting algorithms to minimize the cost of waiting for synchronization in large-scale multiprocessors. In a two-phase algorithm, a thread first waits by polling a synchronization variable. If the cost of polling reaches a limit L poll and further waiting is necessary, the thread is blocked, incurring an additional fixed cost, B . The choice of L poll is a critical determinant of the performance of two-phase algorithms. We focus on methods for statically determining L poll because the run-time overhead of dynamically determining L poll can be comparable to the cost of blocking in large-scale multiprocessor systems with lightweight threads. Our experiments show that always-block ( L poll = 0) is a good waiting algorithm with performance that is usually close to the best of the algorithms compared. We show that even better performance can be achieved with a static choice of L poll based on knowledge of likely wait-time distributions. Motivated by the observation that different synchronization types exhibit different wait-time distributions, we prove that a static choice of L poll can yield close to optimal on-line performance against an adversary that is restricted to choosing wait times from a fixed family of probability distributions. This result allows us to make an optimal static choice of L poll based on synchronization type. For exponentially distributed wait times, we prove that setting L poll = 1n(e-1) B results in a waiting cost that is no more than e/(e-1) times the cost of an optimal off-line algorithm. For uniformly distributed wait times, we prove that setting L poll =1/2(square root of 5 -1) B results in a waiting cost that is no more than (square root of 5 + 1)/2 (the golden ratio) times the cost of an optimal off-line algorithm. Experimental measurements of several parallel applications on the Alewife multiprocessor simulator corroborate our theoretical findings.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2073100321",
    "type": "article"
  },
  {
    "title": "Portable run-time support for dynamic object-oriented parallel processing",
    "doi": "https://doi.org/10.1145/227695.227696",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Andrew Grimshaw; Jon Weissman; W. Timothy Strayer",
    "corresponding_authors": "",
    "abstract": "Mentat is an object-oriented parallel processing system designed to simplify the task of writing portable parallel programs for parallel machines and workstation networks. The Mentat compiler and run-time system work together to automatically manage the communication and synchronization between objects. The run-time system marshals member function arguments, schedules objects on processors, and dynamically constructs and executes large-grain data dependence graphs. In this article we present the Mentat run-time system. We focus on three aspects—the software architecture, including the interface to the compiler and the structure and interaction of the principle components of the run-time system; the run-time overhead on a component-by-component basis for two platforms, a Sun SparcStation 2 and an Intel Paragon; and an analysis of the minimum granularity required for application programs to overcome the run-time overhead.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2040500024",
    "type": "article"
  },
  {
    "title": "A coherent distributed file cache with directory write-behind",
    "doi": "https://doi.org/10.1145/176575.176577",
    "publication_date": "1994-05-01",
    "publication_year": 1994,
    "authors": "Timothy Mann; Andrew Birrell; Andy Hisgen; Charles Jerian; Garret Swart",
    "corresponding_authors": "",
    "abstract": "Extensive caching is a key feature of the Echo distributed file system. Echo client machines maintain coherent caches of file and directory data and properties, with write-behind (delayed write-back) of all cached information. Echo specifies ordering constraints on this write-behind, enabling applications to store and maintain consistent data structures in the file system even when crashes or network faults prevent some writes from being completed. In this paper we describe the Echo cache's coherence and ordering semantics, show how they can improve the performance and consistency of applications, explain how they are implemented. We also discuss the general problem of reliably notifying applications and users when write-behind is lost; we addressed this problem as part of the Echo design, but did not find a fully satisfactory solution.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2144554450",
    "type": "article"
  },
  {
    "title": "Measuring system normality",
    "doi": "https://doi.org/10.1145/507052.507054",
    "publication_date": "2002-05-01",
    "publication_year": 2002,
    "authors": "Mark Burgess; Hårek Haugerud; Sigmund Straumsnes; Trond Reitan",
    "corresponding_authors": "",
    "abstract": "A comparative analysis of transaction time-series is made, for light to moderately loaded hosts, motivated by the problem of anomaly detection in computers. Criteria for measuring the statistical state of hosts are examined. Applying a scaling transformation to the measured data, it is found that the distribution of fluctuations about the mean is closely approximated by a steady-state, maximum-entropy distribution, modulated by a periodic variation. The shape of the distribution, under these conditions, depends on the dimensionless ratio of the daily/weekly periodicity and the correlation length of the data. These values are persistent or even invariant. We investigate the limits of these conclusions, and how they might be applied in anomaly detection.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2014366246",
    "type": "article"
  },
  {
    "title": "Efficient (stack) algorithms for analysis of write-back and sector memories",
    "doi": "https://doi.org/10.1145/58564.59296",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "James G. Thompson; Alan Jay Smith",
    "corresponding_authors": "",
    "abstract": "For the class of replacement algorithms known as stack algorithms, existing analysis techniques permit the computation of memory miss ratios for all memory sizes simultaneously in one pass over a memory reference string. We extend the class of computations possible by this methodology in two ways. First, we show how to compute the effects of copy-backs in write-back caches. The key observation here is that a given block is clean for all memory sizes less than or equal to C blocks and is dirty for all larger memory sizes. Our technique permits efficient computations for algorithms or systems using periodic write-back and/or block deletion. The second extension permits stack analysis simulation for sector (or subblock) caches in which a sector (associated with an address tag) consists of subsectors (or subblocks) that can be loaded independently. The key observation here is that a subsector is present only in caches of size C or greater. Load forward prefetching in a sector cache is shown to be a stack algorithm and is easily simulated using our technique. Running times for our methods are only slightly higher than for a simulation of a single memory size using nonstack techniques.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2068660999",
    "type": "article"
  },
  {
    "title": "Determining the last process to fail",
    "doi": "https://doi.org/10.1145/214451.214453",
    "publication_date": "1985-02-01",
    "publication_year": 1985,
    "authors": "Dale Skeen",
    "corresponding_authors": "Dale Skeen",
    "abstract": "A total failure occurs whenever all processes cooperatively executing a distributed task fail before the task completes. A frequent prerequisite for recovery from a total failure is identification of the last set (LAST) of processes to fail. Necessary and sufficient conditions are derived here for computing LAST from the local failure data of recovered processes. These conditions are then translated into procedures for deciding LAST membership, using either complete or incomplete failure data. The choice of failure data is itself dictated by two requirements: (1) it can be cheaply maintained, and (2) it must afford maximum fault-tolerance in the sense that the expected number of recoveries required for identifying LAST is minimized.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2020014766",
    "type": "article"
  },
  {
    "title": "Performance bound hierarchies for queueing networks",
    "doi": "https://doi.org/10.1145/357360.357363",
    "publication_date": "1983-05-01",
    "publication_year": 1983,
    "authors": "Derek L. Eager; Kenneth C. Sevcik",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Performance bound hierarchies for queueing networks Authors: Derek L. Eager Computer Systems Research Group, Sandford Fleming Building, University of Toronto, Toronto, Ontario M5S 1A4, Canada Computer Systems Research Group, Sandford Fleming Building, University of Toronto, Toronto, Ontario M5S 1A4, CanadaView Profile , Kenneth C. Sevcik Computer Systems Research Group, Sandford Fleming Building, University of Toronto, Toronto, Ontario M5S 1A4, Canada Computer Systems Research Group, Sandford Fleming Building, University of Toronto, Toronto, Ontario M5S 1A4, CanadaView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 2May 1983 pp 99–115https://doi.org/10.1145/357360.357363Published:01 May 1983Publication History 47citation371DownloadsMetricsTotal Citations47Total Downloads371Last 12 Months15Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2076134079",
    "type": "article"
  },
  {
    "title": "Cache performance of operating systems and multiprogramming",
    "doi": null,
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "A. Argawal",
    "corresponding_authors": "A. Argawal",
    "abstract": "",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2295771410",
    "type": "article"
  },
  {
    "title": "Speculative execution in a distributed file system",
    "doi": "https://doi.org/10.1145/1189256.1189258",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Edmund B. Nightingale; Peter M. Chen; Jason Flinn",
    "corresponding_authors": "",
    "abstract": "Speculator provides Linux kernel support for speculative execution. It allows multiple processes to share speculative state by tracking causal dependencies propagated through interprocess communication. It guarantees correct execution by preventing speculative processes from externalizing output, for example, sending a network message or writing to the screen, until the speculations on which that output depends have proven to be correct. Speculator improves the performance of distributed file systems by masking I/O latency and increasing I/O throughput. Rather than block during a remote operation, a file system predicts the operation's result, then uses Speculator to checkpoint the state of the calling process and speculatively continue its execution based on the predicted result. If the prediction is correct, the checkpoint is discarded; if it is incorrect, the calling process is restored to the checkpoint, and the operation is retried. We have modified the client, server, and network protocol of two distributed file systems to use Speculator. For PostMark and Andrew-style benchmarks, speculative execution results in a factor of 2 performance improvement for NFS over local area networks and an order of magnitude improvement over wide area networks. For the same benchmarks, Speculator enables the Blue File System to provide the consistency of single-copy file semantics and the safety of synchronous I/O, yet still outperform current distributed file systems with weaker consistency and safety.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2047418149",
    "type": "article"
  },
  {
    "title": "Computational algorithms for state-dependent queueing networks",
    "doi": "https://doi.org/10.1145/357353.357359",
    "publication_date": "1983-02-01",
    "publication_year": 1983,
    "authors": "Charles H. Sauer",
    "corresponding_authors": "Charles H. Sauer",
    "abstract": "article Free AccessComputational algorithms for state-dependent queueing networks Author: Charles H. Sauer IBM, Department 450, Building 984, 11400 Burnet Road, Austin, TX IBM, Department 450, Building 984, 11400 Burnet Road, Austin, TXView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 1pp 67–92https://doi.org/10.1145/357353.357359Published:01 February 1983Publication History 30citation677DownloadsMetricsTotal Citations30Total Downloads677Last 12 Months22Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1964081384",
    "type": "article"
  },
  {
    "title": "Optimal routing in closed queuing networks",
    "doi": "https://doi.org/10.1145/357377.357381",
    "publication_date": "1983-11-01",
    "publication_year": 1983,
    "authors": "Hiroshi Kobayashi; Mário Gerla",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Optimal routing in closed queuing networks Authors: Hiroshi Kobayashi 16 Suwazaka, KDD-211, Tsurumi-ku, Yokohama 230, Japan 16 Suwazaka, KDD-211, Tsurumi-ku, Yokohama 230, JapanView Profile , Mario Gerla BH 3732H, Computer Science Dept., U.C.L.A., Los Angeles, CA BH 3732H, Computer Science Dept., U.C.L.A., Los Angeles, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 4Nov. 1983 pp 294–310https://doi.org/10.1145/357377.357381Online:01 November 1983Publication History 35citation612DownloadsMetricsTotal Citations35Total Downloads612Last 12 Months20Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2014028771",
    "type": "article"
  },
  {
    "title": "Rethink the sync",
    "doi": "https://doi.org/10.1145/1394441.1394442",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Edmund B. Nightingale; Kaushik Veeraraghavan; Peter M. Chen; Jason Flinn",
    "corresponding_authors": "",
    "abstract": "We introduce external synchrony , a new model for local file I/O that provides the reliability and simplicity of synchronous I/O, yet also closely approximates the performance of asynchronous I/O. An external observer cannot distinguish the output of a computer with an externally synchronous file system from the output of a computer with a synchronous file system. No application modification is required to use an externally synchronous file system. In fact, application developers can program to the simpler synchronous I/O abstraction and still receive excellent performance. We have implemented an externally synchronous file system for Linux, called xsyncfs. Xsyncfs provides the same durability and ordering-guarantees as those provided by a synchronously mounted ext3 file system. Yet even for I/O-intensive benchmarks, xsyncfs performance is within 7% of ext3 mounted asynchronously . Compared to ext3 mounted synchronously, xsyncfs is up to two orders of magnitude faster.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2058535975",
    "type": "article"
  },
  {
    "title": "Load Balancing Content-Based Publish/Subscribe Systems",
    "doi": "https://doi.org/10.1145/1880018.1880020",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Alex King Yeung Cheung; Hans‐Arno Jacobsen",
    "corresponding_authors": "",
    "abstract": "Distributed content-based publish/subscribe systems suffer from performance degradation and poor scalability caused by uneven load distributions typical in real-world applications. The reason for this shortcoming is the lack of a load balancing scheme. This article proposes a load balancing solution specifically tailored to the needs of content-based publish/subscribe systems that is distributed, dynamic, adaptive, transparent, and accommodates heterogeneity. The solution consists of three key contributions: a load balancing framework, a novel load estimation algorithm, and three offload strategies. A working prototype of our solution is built on an open-sourced content-based publish/subscribe system and evaluated on PlanetLab, a cluster testbed, and in simulations. Real-life experiment results show that the proposed load balancing solution is efficient with less than 0.2% overhead; effective in distributing and balancing load originating from a single server to all available servers in the network; and capable of preventing overloads to preserve system stability, availability, and quality of service.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2000718212",
    "type": "article"
  },
  {
    "title": "The PowerNap Server Architecture",
    "doi": "https://doi.org/10.1145/1925109.1925112",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "David Meisner; Brian T. Gold; Thomas F. Wenisch",
    "corresponding_authors": "",
    "abstract": "Data center power consumption is growing to unprecedented levels: the EPA estimates U.S. data centers will consume 100 billion kilowatt hours annually by 2011. Much of this energy is wasted in idle systems: in typical deployments, server utilization is below 30%, but idle servers still consume 60% of their peak power draw. Typical idle periods---though frequent---last seconds or less, confounding simple energy-conservation approaches. In this article, we propose PowerNap, an energy-conservation approach where the entire system transitions rapidly between a high-performance active state and a near-zero-power idle state in response to instantaneous load. Rather than requiring fine-grained power-performance states and complex load-proportional operation from individual system components, PowerNap instead calls for minimizing idle power and transition time, which are simpler optimization goals. Based on the PowerNap concept, we develop requirements and outline mechanisms to eliminate idle power waste in enterprise blade servers. Because PowerNap operates in low-efficiency regions of current blade center power supplies, we introduce the Redundant Array for Inexpensive Load Sharing (RAILS), a power provisioning approach that provides high conversion efficiency across the entire range of PowerNap’s power demands. Using utilization traces collected from enterprise-scale commercial deployments, we demonstrate that, together, PowerNap and RAILS reduce average server power consumption by 74%.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2114153586",
    "type": "article"
  },
  {
    "title": "SEATTLE",
    "doi": "https://doi.org/10.1145/1925109.1925110",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Changhoon Kim; Matthew Caesar; Jennifer Rexford",
    "corresponding_authors": "",
    "abstract": "IP networks today require massive effort to configure and manage. Ethernet is vastly simpler to manage, but does not scale beyond small local area networks. This article describes an alternative network architecture called SEATTLE that achieves the best of both worlds: The scalability of IP combined with the simplicity of Ethernet. SEATTLE provides plug-and-play functionality via flat addressing, while ensuring scalability and efficiency through shortest-path routing and hash-based resolution of host information. In contrast to previous work on identity-based routing, SEATTLE ensures path predictability, controllability, and stability, thus simplifying key network-management operations, such as capacity planning, traffic engineering, and troubleshooting. We performed a simulation study driven by real-world traffic traces and network topologies, and used Emulab to evaluate a prototype of our design based on the Click and XORP open-source routing platforms. Our experiments show that SEATTLE efficiently handles network failures and host mobility, while reducing control overhead and state requirements by roughly two orders of magnitude compared with Ethernet bridging.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2060114188",
    "type": "article"
  },
  {
    "title": "EventGuard",
    "doi": "https://doi.org/10.1145/2063509.2063510",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Mudhakar Srivatsa; Ling Liu; Arun Iyengar",
    "corresponding_authors": "",
    "abstract": "Publish-subscribe (pub-sub) is an emerging paradigm for building a large number of distributed systems. A wide area pub-sub system is usually implemented on an overlay network infrastructure to enable information dissemination from publishers to subscribers. Using an open overlay network raises several security concerns such as: confidentiality and integrity, authentication, authorization and Denial-of-Service (DoS) attacks. In this article we present EventGuard, a framework for building secure wide-area pub-sub systems. The EventGuard architecture is comprised of three key components: (1) a suite of security guards that can be seamlessly plugged-into a content-based pub-sub system, (2) a scalable key management algorithm to enforce access control on subscribers, and (3) a resilient pub-sub network design that is capable of scalable routing, handling message dropping-based DoS attacks, and node failures. The design of EventGuard mechanisms aims at providing security guarantees while maintaining the system’s overall simplicity, scalability, and performance metrics. We describe an implementation of the EventGuard pub-sub system to show that EventGuard is easily stackable on any content-based pub-sub core. We present detailed experimental results that quantify the overhead of the EventGuard pub-sub system and demonstrate its resilience against various attacks.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2000934606",
    "type": "article"
  },
  {
    "title": "Fay",
    "doi": "https://doi.org/10.1145/2382553.2382555",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Úlfar Erlingsson; Marcus Peinado; Simon Peter; Mihai Budiu; Gloria Mainar-Ruiz",
    "corresponding_authors": "",
    "abstract": "Fay is a flexible platform for the efficient collection, processing, and analysis of software execution traces. Fay provides dynamic tracing through use of runtime instrumentation and distributed aggregation within machines and across clusters. At the lowest level, Fay can be safely extended with new tracing primitives, including even untrusted, fully optimized machine code, and Fay can be applied to running user-mode or kernel-mode software without compromising system stability. At the highest level, Fay provides a unified, declarative means of specifying what events to trace, as well as the aggregation, processing, and analysis of those events. We have implemented the Fay tracing platform for Windows and integrated it with two powerful, expressive systems for distributed programming. Our implementation is easy to use, can be applied to unmodified production systems, and provides primitives that allow the overhead of tracing to be greatly reduced, compared to previous dynamic tracing platforms. To show the generality of Fay tracing, we reimplement, in experiments, a range of tracing strategies and several custom mechanisms from existing tracing frameworks. Fay shows that modern techniques for high-level querying and data-parallel processing of disagreggated data streams are well suited to comprehensive monitoring of software execution in distributed systems. Revisiting a lesson from the late 1960s [Deutsch and Grant 1971], Fay also demonstrates the efficiency and extensibility benefits of using safe, statically verified machine code as the basis for low-level execution tracing. Finally, Fay establishes that, by automatically deriving optimized query plans and code for safe extensions, the expressiveness and performance of high-level tracing queries can equal or even surpass that of specialized monitoring tools.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1997787524",
    "type": "article"
  },
  {
    "title": "SnowFlock",
    "doi": "https://doi.org/10.1145/1925109.1925111",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "H. Andrés Lagar-Cavilla; Joseph A. Whitney; Roy Bryant; Philip Patchin; Michael Brudno; Eyal de Lara; Stephen M. Rumble; Mahadev Satyanarayanan; Adin Scannell",
    "corresponding_authors": "",
    "abstract": "A basic building block of cloud computing is virtualization. Virtual machines (VMs) encapsulate a user’s computing environment and efficiently isolate it from that of other users. VMs, however, are large entities, and no clear APIs exist yet to provide users with programatic, fine-grained control on short time scales. We present SnowFlock, a paradigm and system for cloud computing that introduces VM cloning as a first-class cloud abstraction. VM cloning exploits the well-understood and effective semantics of UNIX fork. We demonstrate multiple usage models of VM cloning: users can incorporate the primitive in their code, can wrap around existing toolchains via scripting, can encapsulate the API within a parallel programming framework, or can use it to load-balance and self-scale clustered servers. VM cloning needs to be efficient to be usable. It must efficiently transmit VM state in order to avoid cloud I/O bottlenecks. We demonstrate how the semantics of cloning aid us in realizing its efficiency: state is propagated in parallel to multiple VM clones, and is transmitted during runtime, allowing for optimizations that substantially reduce the I/O load. We show detailed microbenchmark results highlighting the efficiency of our optimizations, and macrobenchmark numbers demonstrating the effectiveness of the different usage models of SnowFlock.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2077733196",
    "type": "article"
  },
  {
    "title": "Aggressive Datacenter Power Provisioning with Batteries",
    "doi": "https://doi.org/10.1145/2427631.2427633",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Sriram Govindan; Di Wang; Anand Sivasubramaniam; Bhuvan Urgaonkar",
    "corresponding_authors": "",
    "abstract": "Datacenters spend $10--25 per watt in provisioning their power infrastructure, regardless of the watts actually consumed. Since peak power needs arise rarely, provisioning power infrastructure for them can be expensive. One can, thus, aggressively underprovision infrastructure assuming that simultaneous peak draw across all equipment will happen rarely. The resulting nonzero probability of emergency events where power needs exceed provisioned capacity, however small, mandates graceful reaction mechanisms to cap the power draw instead of leaving it to disruptive circuit breakers/fuses. Existing strategies for power capping use temporal knobs local to a server that throttle the rate of execution (using power modes), and/or spatial knobs that redirect/migrate excess load to regions of the datacenter with more power headroom. We show these mechanisms to have performance degrading ramifications, and propose an entirely orthogonal solution that leverages existing UPS batteries to temporarily augment the utility supply during emergencies. We build an experimental prototype to demonstrate such power capping on a cluster of 8 servers, each with an individual battery, and implement several online heuristics in the context of different datacenter workloads to evaluate their effectiveness in handling power emergencies. We show that our battery-based solution can: (i) handle emergencies of short durations on its own, (ii) supplement existing reaction mechanisms to enhance their efficacy for longer emergencies, and (iii) create more slack for shifting applications temporarily to nonpeak durations.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2053618194",
    "type": "article"
  },
  {
    "title": "Fast In-Memory Transaction Processing Using RDMA and HTM",
    "doi": "https://doi.org/10.1145/3092701",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Haibo Chen; Rong Chen; Xingda Wei; Jiaxin Shi; Yanzhe Chen; Zhaoguo Wang; Binyu Zang; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "DrTM is a fast in-memory transaction processing system that exploits advanced hardware features such as remote direct memory access (RDMA) and hardware transactional memory (HTM). To achieve high efficiency, it mostly offloads concurrency control such as tracking read/write accesses and conflict detection into HTM in a local machine and leverages the strong consistency between RDMA and HTM to ensure serializability among concurrent transactions across machines. To mitigate the high probability of HTM aborts for large transactions, we design and implement an optimized transaction chopping algorithm to decompose a set of large transactions into smaller pieces such that HTM is only required to protect each piece. We further build an efficient hash table for DrTM by leveraging HTM and RDMA to simplify the design and notably improve the performance. We describe how DrTM supports common database features like read-only transactions and logging for durability. Evaluation using typical OLTP workloads including TPC-C and SmallBank shows that DrTM has better single-node efficiency and scales well on a six-node cluster; it achieves greater than 1.51, 34 and 5.24, 138 million transactions per second for TPC-C and SmallBank on a single node and the cluster, respectively. Such numbers outperform a state-of-the-art single-node system (i.e., Silo) and a distributed transaction system (i.e., Calvin) by at least 1.9X and 29.6X for TPC-C.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2736285633",
    "type": "article"
  },
  {
    "title": "GPUnet",
    "doi": "https://doi.org/10.1145/2963098",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Mark Silberstein; Sangman Kim; Seonggu Huh; Xinya Zhang; Yige Hu; Amir Wated; Emmett Witchel",
    "corresponding_authors": "",
    "abstract": "Despite the popularity of GPUs in high-performance and scientific computing, and despite increasingly general-purpose hardware capabilities, the use of GPUs in network servers or distributed systems poses significant challenges. GPUnet is a native GPU networking layer that provides a socket abstraction and high-level networking APIs for GPU programs. We use GPUnet to streamline the development of high-performance, distributed applications like in-GPU-memory MapReduce and a new class of low-latency, high-throughput GPU-native network services such as a face verification server.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2521121845",
    "type": "article"
  },
  {
    "title": "Quantifying loop nest locality using SPEC'95 and the perfect benchmarks",
    "doi": "https://doi.org/10.1145/329466.329484",
    "publication_date": "1999-11-01",
    "publication_year": 1999,
    "authors": "Kathryn S. McKinley; Olivier Temam",
    "corresponding_authors": "",
    "abstract": "This article analyzes and quantifies the locality characteristics of numerical loop nests in order to suggest future directions for architecture and software cache optimizations. Since most programs spend the majority of their time in nests, the vast majority of cache optimization techniques target loop nests. In contrast, the locality characteristics that drive these optimizations are usually collected across the entire application rather than at the nest level. Researchers have studied numerical codes for so long that a number of commonly held assertions have emerged on their locality characteristics. In light of these assertions, we use the SPEC'95 and Perfect Benchmarks to take a new look at measuring locality on numerical codes based on references, loop nests, and program locality properties. Our results show that several popular assertions are at best overstatements. For example, although most reuse is within a loop nest, in line with popular assertions, most misses are internest capacity misses, and they correspond to potential reuse between nearby loop nests. In addition, we find that temporal and spatial reuse have balanced roles within a loop nest and that most reuse across nests and the entire program is temporal. These results are consistent with high hit rates (80% or more hits), but go against the commonly held assumption that spatial reuse dominates. Our locality measurements reveal important differences between loop nests and programs, refute some popular assertions, and provide new insights for the compiler writer and the architect.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2050152807",
    "type": "article"
  },
  {
    "title": "Hint-based cooperative caching",
    "doi": "https://doi.org/10.1145/362670.362675",
    "publication_date": "2000-11-01",
    "publication_year": 2000,
    "authors": "Prasenjit Sarkar; John H. Hartman",
    "corresponding_authors": "",
    "abstract": "This article presents the design, implementation, and measurement of a hint-based cooperative caching file system. Hints allow clients to make decisions based on local state, enabling a loosely coordinated system that is simple to implement. The resulting performance is comparable to that of existing tightly coordinated algorithms that use global state, but with less overhead. Simulations show that the block access times of our system are as good as those of the existing algorithms, while reducing manager load by more than a factor of seven, block lookup traffic by nearly a factor of two-thirds, and replacement traffic a factor of five. To verify our simulation results in a real system with real users, we implemented a prototype and measured its performance for one week. Although the simulation and prototype environments were very different, the prototype system mirrored the simulation results by exhibiting reduced overhead and high hint accuracy. Furthermore, hint-based cooperative caching reduced the average block access time to almost half that of NFS.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2021221734",
    "type": "article"
  },
  {
    "title": "Cellular disco",
    "doi": "https://doi.org/10.1145/354871.354873",
    "publication_date": "2000-08-01",
    "publication_year": 2000,
    "authors": "Kinshuk Govil; Dan Teodosiu; Yongqiang Huang; Mendel Rosenblum",
    "corresponding_authors": "",
    "abstract": "Despite the fact that large-scale shared-memory multiprocessors have been commercially available for several years, system software that fully utilizes all their features is still not available, mostly due to the complexity and cost of making the required changes to the operating system. A recently proposed approach, called Disco, substantially reduces this development cost by using a virtual machine monitor that laverages the existing operating system technology. In this paper we present a system called Cellular Disco that extends the Disco work to provide all the advantages of the hardware partitioning and scalable operating system approaches. We argue that Cellular Disco can achieve these benefits at only a small fraction of the development cost of modifying the operating system. Cellular Disco effectively turns a large-scale shared-memory multiprocessor into a virtual cluster that supports fault containment and heterogeneity, while avoiding operating system scalability bottlenecks. Yet at the same time, Cellular Disco preserves the benefits of a shared-memory multiprocessor by implementing dynamic, fine-grained resource sharing, and by allowing users to overcommit resources such as processors and memory. This hybrid approach requires a scalable resource manager that makes local decisions with limited information while still providing good global performance and fault containment. In this paper we describe our experience with a Cellular Disco prototype on a 32-processor SGI Origin 2000 system. We show that the execution time penalty for this approach is low, typically within 10% of the best available commercial operating system formost workloads, and that it can manage the CPU and memory resources of the machine significantly better than the hardware partitioning approach.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W1981647592",
    "type": "article"
  },
  {
    "title": "Interposed request routing for scalable network storage",
    "doi": "https://doi.org/10.1145/505452.505454",
    "publication_date": "2002-02-01",
    "publication_year": 2002,
    "authors": "Darrell C. Anderson; Jeffrey S. Chase; Amin Vahdat",
    "corresponding_authors": "",
    "abstract": "This paper explores interposed request routing in Slice, a new storage system architecture for high-speed networks incorporating network-attached block storage. Slice interposes a request switching filter---called a μ proxy ---along each client's network path to the storage service (e.g., in a network adapter or switch). The μproxy intercepts request traffic and distributes it across a server ensemble. We propose request routing schemes for I/O and file service traffic, and explore their effect on service structure. The Slice prototype uses a packet filter μproxy to virtualize the standard Network File System (NFS) protocol, presenting to NFS clients a unified shared file volume with scalable bandwidth and capacity. Experimental results from the industry-standard SPECsfs97 workload demonstrate that the architecture enables construction of powerful network-attached storage services by aggregating cost-effective components on a switched Gigabit Ethernet LAN.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2050559702",
    "type": "article"
  },
  {
    "title": "Set-associative cache simulation using generalized binomial trees",
    "doi": "https://doi.org/10.1145/200912.200918",
    "publication_date": "1995-02-01",
    "publication_year": 1995,
    "authors": "Rabin A. Sugumar; Santosh G. Abraham",
    "corresponding_authors": "",
    "abstract": "Set-associative caches are widely used in CPU memory hierarchies, I/O subsystems, and file systems to reduce average access times. This article proposes an efficient simulation technique for simulating a group of set-associative caches in a single pass through the address trace, where all caches have the same line size but varying associativities and varying number of sets. The article also introduces a generalization of the ordinary binomial tree and presents a representation of caches in this class using the Generalized Binomial Tree (gbt). The tree representation permits efficient search and update of the caches. Theoretically, the new algorithm, GBF_LS, based on the gbt structure, always takes fewer comparisons than the two earlier algorithms for the same class of caches: all-associativity and generalized forest simulation. Experimentally, the new algorithm shows performance gains in the range of 1.2 to 3.8 over the earlier algorithms on address traces of the SPEC benchmarks. A related algorithm for simulating multiple alternative direct-mapped caches with fixed cache size, but varying line size, is also presented.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2015937460",
    "type": "article"
  },
  {
    "title": "Parity logging disk arrays",
    "doi": "https://doi.org/10.1145/185514.185516",
    "publication_date": "1994-08-01",
    "publication_year": 1994,
    "authors": "Daniel Stodolsky; Mark Holland; William V. Courtright; Garth A. Gibson",
    "corresponding_authors": "",
    "abstract": "Parity-encoded redundant disk arrays provide highly reliable, cost-effective secondary storage with high performance for reads and large writes. Their performance on small writes, however, is much worse than mirrored disks—the traditional, highly reliable, but expensive organization for secondary storage. Unfortunately, small writes are a substantial portion of the I/O workload of many important, demanding applications such as on-line transaction processing. This paper presents parity logging , a novel solution to the small-write problem for redundant disk arrays. Parity logging applies journalling techniques to reduce substantially the cost of small writes. We provide detailed models of parity logging and competing schemes—mirroring, floating storage, and RAID level 5—and verify these models by simulation. Parity logging provides performance competitive with mirroring, but with capacity overhead close to the minimum offered by RAID level 5. Finally, parity logging can exploit data caching more effectively than all three alternative approaches.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2106587909",
    "type": "article"
  },
  {
    "title": "Delivery of time-critical messages using a multiple copy approach",
    "doi": "https://doi.org/10.1145/128899.128902",
    "publication_date": "1992-05-01",
    "publication_year": 1992,
    "authors": "Parameswaran Ramanathan; Kang G. Shin",
    "corresponding_authors": "",
    "abstract": "Reliable and timely delivery of messages between processing nodes is essential in distributed real-time systems. Failure to deliver a message within its deadline usually forces the system to undertake a recovery action, which introduces some cost (or overhead) to the system. This recovery cost can be very high, especially when the recovery action fails due to lack of time or resources. Proposed in this paper is a scheme to minimize the expected cost incurred as a result of messages failing to meet their deadlines. The scheme is intended for distributed real-time systems, especially with a point-to-point interconnection topology. The goal of minimizing the expected cost is achieved by sending multiple copies of a message through disjoint routes and thus increasing the probability of successful message delivery within the deadline. However, as the number of copies increases, the message traffic on the network increases, thereby increasing the delivery time for each of the copies. There is therefore a tradeoff between the number of copies of each message and the expected cost incurred as a result of messages missing their deadlines. The number of copies of each message to be sent is determined by optimizing this tradeoff. Simulation results for a hexagonal mesh and a hypercube topology indicate that the expected cost can be lowered substantially by the proposed scheme.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W1964054155",
    "type": "article"
  },
  {
    "title": "Efficient at-most-once messages based on synchronized clocks",
    "doi": "https://doi.org/10.1145/103720.103722",
    "publication_date": "1991-05-01",
    "publication_year": 1991,
    "authors": "Barbara Liskov; Liuba Shrira; John Wroclawski",
    "corresponding_authors": "",
    "abstract": "This paper describes a new at-most-once message passing protocol that provides guaranteed detection of duplicate messages even when the receiver has no state stored for the sender. It also discusses how to use at-most-once messages to implement higher-level primitives such as at-once-remote procedure calls and sequenced bytestream protocols. Our performance measurements indicate that at-most-once RPCs can provide at the same cost as less desirable forms of RPCs that do not guarantee at-most-once execution. Our method is based on the assumption that clocks throughout the system are loosely synchronized. Modern clock synchronization protocols provide good bounds on clock skew with high probability; our method depends on the bound for performance but not for correctness.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2137509374",
    "type": "article"
  },
  {
    "title": "Scalable concurrent counting",
    "doi": "https://doi.org/10.1145/210223.210225",
    "publication_date": "1995-11-01",
    "publication_year": 1995,
    "authors": "Maurice Herlihy; Beng-Hong Lim; Nir Shavit",
    "corresponding_authors": "",
    "abstract": "The notion of counting is central to a number of basic multiprocessor coordination problems, such as dynamic load balancing, barrier synchronization, and concurrent data structure design. We investigate the scalability of a variety of counting techniques for large-scale multiprocessors. We compare counting techniques based on: (1) spin locks, (2) message passing, (3) distributed queues, (4) software combining trees, and (5) counting networks. Our comparison is based on a series of simple benchmarks on a simulated 64-processor Alewife machine, a distributed-memory multiprocessor currently under development at MIT. Although locking techniques are known to perform well on small-scale, bus-based multiprocessors, serialization limits performance, and contention can degrade performance. Both counting networks and combining trees outperform the other methods substantially by avoiding serialization and alleviating contention, although combining-tree throughput is more sensitive to variations in load. A comparison of shared-memory and message-passing implementations of counting networks and combining trees shows that message-passing implementations have substantially higher throughput.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W1992679913",
    "type": "article"
  },
  {
    "title": "The measured performance of personal computer operating systems",
    "doi": "https://doi.org/10.1145/225535.225536",
    "publication_date": "1996-02-01",
    "publication_year": 1996,
    "authors": "J. Bradley Chen; Yasuhiro Endo; Kee Chan; David Mazières; António Dias; Margo Seltzer; Michael D. Smith",
    "corresponding_authors": "",
    "abstract": "This article presents a comparative study of the performance of three operating systems that run on the personal computer architecture derived form the IBM-PC. The operating systems, Windows for Workgroups, Windows NT, and NetBSD (a freely available variant of the UNIX operating system), cover a broad range of system functionality and user requirements, from a single-address-space model to full protection with preemptive multitasking. Our measurements are enable by hardware counters in Intel's Pentium processor that permit measurement of a broad range of processor events including instruction counts and on-chip cache miss counts. We use both microbenchmarks, which expose specific difference between the systems, and application workloads, which provide an indication of expected end-to-end performance. Our microbenchmark results show that accessing system functionality is often more expensive in Windows for Workgroups than in the other two systems due to frequent changes in machine mode and the use of system call hooks. When running native applications, Windows NT is more efficient than Windows, but it incurs overhead similar to that of a microkernel, since its application interface (the Win32 API) is implemented as a user-level server. Overall, system functionality can be accessed most efficiently in NetBSD; we attribute this to its monolithic structure and to the absence of the complications created by hardware backward-compatibility requirements in the other systems. Measurements of application performance show that although the impact of these differences is significant in terms of instruction counts and other hardware events (often a factor of 2 to 7 difference between the systems), overall performance is sometimes determined by the functionality provided by specific subsystems, such as the graphics subsystem or the file system buffer cache.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2031771926",
    "type": "article"
  },
  {
    "title": "FLIP: an internetwork protocol for supporting distributed systems",
    "doi": "https://doi.org/10.1145/151250.151253",
    "publication_date": "1993-02-01",
    "publication_year": 1993,
    "authors": "M. Frans Kaashoek; Robbert van Renesse; Hans van Staveren; Andrew S. Tanenbaum",
    "corresponding_authors": "",
    "abstract": "Most modern network protocols give adequate support for traditional applications such as file transfer and remote login. Distributed applications, however, have different requirements (e.g., efficient at-most-once remote procedure call even in the face of processor failures). Instead of using ad hoc protocols to meet each of the new requirements, we have designed a new protocol, called the Fast Local Internet Protocol (FLIP), that provides a clean and simple integrated approach to these new requirements. FLIP is an unreliable message protocol that provides both point-to-point communication and multicast communication, and requires almost no network management. Furthermore, by using FLIP we have simplified higher-level protocols such as remote procedure call and group communication, and enhanced support for process migration and security. A prototype implementation of FLIP has been built as part of the new kernel for the Amoeba distributed operating system, and is in daily use. Measurements of its performance are presented.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2137028319",
    "type": "article"
  },
  {
    "title": "Let caches decay",
    "doi": "https://doi.org/10.1145/507052.507055",
    "publication_date": "2002-05-01",
    "publication_year": 2002,
    "authors": "Zhigang Hu; Stefanos Kaxiras; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "Power dissipation is increasingly important in CPUs ranging from those intended for mobile use, all the way up to high-performance processors for highend servers. Although the bulk of the power dissipated is dynamic switching power, leakage power is also beginning to be a concern. Chipmakers expect that in future chip generations, leakage's proportion of total chip power will increase significantly. This article examines methods for reducing leakage power within the cache memories of the CPU. Because caches comprise much of a CPU chip's area and transistor counts, they are reasonable targets for attacking leakage. We discuss policies and implementations for reducing cache leakage by invalidating and \"turning off\" cache lines when they hold data not likely to be reused. In particular, our approach is targeted at the generational nature of cache line usage. That is, cache lines typically have a flurry of frequent use when first brought into the cache, and then have a period of \"dead time\" before they are evicted. By devising effective, low-power ways of deducing dead time, our results show that in many cases we can reduce L1 cache leakage energy by 4x in SPEC2000 applications without having an impact on performance. Because our decay-based techniques have notions of competitive online algorithms at their roots, their energy usage can be theoretically bounded at within a factor of two of the optimal oracle-based policy. We also examine adaptive decay-based policies that make energy-minimizing policy choices on a per-application basis by choosing appropriate decay intervals individually for each cache line. Our proposed adaptive policies effectively reduce L1 cache leakage energy by 5x for the SPEC2000 with only negligible degradations in performance.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2028283779",
    "type": "article"
  },
  {
    "title": "A programmable interface language for heterogeneous distributed systems",
    "doi": "https://doi.org/10.1145/29868.29870",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Joseph R. Falcone",
    "corresponding_authors": "Joseph R. Falcone",
    "abstract": "The 1980s have witnessed the emergence of a new architecture for computing based on networks of personal computer workstations. The performance requirements of such systems of workstations places a strain on traditional approaches to network architecture. The integration of diverse systems into this environment introduces functional compatibility issues that are not present in homogeneous networks. Effective prescriptions for functional compatibility, therefore, must go beyond the communication paradigms used in present distributed systems, such as remote procedure calls. This paper proposes a distributed system architecture in which communication follows a programming paradigm. In this architecture a programming language provides remote service interfaces for the heterogeneous distributed system environment. This language is a flexible and efficient medium for implementing service function protocols. In essence, clients and servers communicate by programming one another.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2043893719",
    "type": "article"
  },
  {
    "title": "Public protection of software",
    "doi": "https://doi.org/10.1145/29868.29872",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Amir Herzberg; Shlomit S. Pinter",
    "corresponding_authors": "",
    "abstract": "One of the overwhelming problems that software producers must contend with is the unauthorized use and distribution of their products. Copyright laws concerning software are rarely enforced, thereby causing major losses to the software companies. Technical means of protecting software from illegal duplication are required, but the available means are imperfect. We present protocols that enable software protection, without causing substantial overhead in distribution and maintenance. The protocols may be implemented by a conventional cryptosystem, such as the DES, or by a public key cryptosystem, such as the RSA. Both implementations are proved to satisfy required security criteria.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2055794506",
    "type": "article"
  },
  {
    "title": "Multidimensional voting",
    "doi": "https://doi.org/10.1145/118544.118552",
    "publication_date": "1991-11-01",
    "publication_year": 1991,
    "authors": "Mustaque Ahamad; Mostafa Ammar; Shun Yan Cheung",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Multidimensional voting Authors: Mustaque Ahamad Georgia Institute of Technology, Atlanta Georgia Institute of Technology, AtlantaView Profile , Mostafa H. Ammar Georgia Institute of Technology, Atlanta Georgia Institute of Technology, AtlantaView Profile , Shun Yan Cheung Emory Univ., Atlanta, GA Emory Univ., Atlanta, GAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 4Nov. 1991 pp 399–431https://doi.org/10.1145/118544.118552Online:01 November 1991Publication History 37citation564DownloadsMetricsTotal Citations37Total Downloads564Last 12 Months17Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2296286881",
    "type": "article"
  },
  {
    "title": "Consistent Database Replication at the Middleware Level",
    "doi": null,
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Marta Patiño-Martı́nez",
    "corresponding_authors": "Marta Patiño-Martı́nez",
    "abstract": "The widespread use of clusters and web farms has increased the importance of data replication. In this paper, we show how to implement consistent and scalable data replication at the middleware level. We do this by combining transactional concurrency control with group communication primitives. The paper presents different replication protocols, argues their correctness, describes their implementation as part of a generic middleware tool, and proves their feasibility with an extensive performance evaluation. The solution proposed is well suited for a variety of applications including web farms and distributed object platforms.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W154768151",
    "type": "article"
  },
  {
    "title": "An HDLC protocol specification and its verification using image protocols",
    "doi": "https://doi.org/10.1145/357377.357384",
    "publication_date": "1983-11-01",
    "publication_year": 1983,
    "authors": "A. Udaya Shankar; Simon S. Lam",
    "corresponding_authors": "",
    "abstract": "article Free AccessAn HDLC protocol specification and its verification using image protocols Authors: A. Udaya Shankar Department of Computer Science, University of Maryland, College Park, MD Department of Computer Science, University of Maryland, College Park, MDView Profile , Simon S. Lam Department of Computer Science, University of Texas at Austin, Austin, TX Department of Computer Science, University of Texas at Austin, Austin, TXView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 4Nov. 1983 pp 331–368https://doi.org/10.1145/357377.357384Published:01 November 1983Publication History 37citation967DownloadsMetricsTotal Citations37Total Downloads967Last 12 Months84Last 6 weeks34 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2137203882",
    "type": "article"
  },
  {
    "title": "UIO: a uniform I/O system interface for distributed systems",
    "doi": "https://doi.org/10.1145/7351.7353",
    "publication_date": "1987-01-05",
    "publication_year": 1987,
    "authors": "David R. Cheriton",
    "corresponding_authors": "David R. Cheriton",
    "abstract": "A uniform I/O interface allows programs to be written relatively independently of specific I/O services and yet work with a wide variety of the I/O services available in a distributed environment. Ideally, the interface provides this uniform access without excessive complexity in the interface or loss of performance. However, a uniform interface does not arise from careful design of individual system interfaces alone; it requires explicit definition. In this paper, the UIO (uniform I/O) system interface that has been used for the past five years in the V distributed operating system is described, with the focus on the key design issues. This interface provides several extensions beyond the I/O interface of UNIX™, including support for record I/O, locking, atomic transactions, and replication, as well as attributes that indicate whether optional semantics and operations are available. Experience in using and implementing this interface with a variety of different I/O services is described, along with the performance of both local and network I/O. It is concluded that the UIO interface provides a uniform I/O system interface with significant functionality, wide applicability, and no significant performance penalty.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2034963492",
    "type": "article"
  },
  {
    "title": "On the reliability of consensus-based fault-tolerant distributed computing systems",
    "doi": "https://doi.org/10.1145/29868.31332",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Özalp Babaoğlu",
    "corresponding_authors": "Özalp Babaoğlu",
    "abstract": "The designer of a fault-tolerant distributed system faces numerous alternatives. Using a stochastic model of processor failure times, we investigate design choices such as replication level, protocol running time, randomized versus deterministic protocols, fault detection, and authentication. We use the probability with which a system produces the correct output as our evaluation criterion. This contrasts with previous fault-tolerance results that guarantee correctness only if the percentage of faulty processors in the system can be bounded. Our results reveal some subtle and counterintuitive interactions between the design parameters and system reliability.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2072961560",
    "type": "article"
  },
  {
    "title": "Selective replication",
    "doi": "https://doi.org/10.1145/1658357.1658359",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "Xavier Vera; Jaume Abella; Javier Carretero; Antonio González",
    "corresponding_authors": "",
    "abstract": "Soft errors are an important challenge in contemporary microprocessors. Modern processors have caches and large memory arrays protected by parity or error detection and correction codes. However, today's failure rate is dominated by flip flops, latches, and the increasing sensitivity of combinational logic to particle strikes. Moreover, as Chip Multi-Processors (CMPs) become ubiquitous, meeting the FIT budget for new designs is becoming a major challenge. Solutions based on replicating threads have been explored deeply; however, their high cost in performance and energy make them unsuitable for current designs. Moreover, our studies based on a typical configuration for a modern processor show that focusing on the top 5 most vulnerable structures can provide up to 70% reduction in FIT rate. Therefore, full replication may overprotect the chip by reducing the FIT much below budget. We propose Selective Replication , a lightweight-reconfigurable mechanism that achieves a high FIT reduction by protecting the most vulnerable instructions with minimal performance and energy impact. Low performance degradation is achieved by not requiring additional issue slots and reissuing instructions only during the time window between when they are retirable and they actually retire. Coverage can be reconfigured online by replicating only a subset of the instructions (the most vulnerable ones). Instructions' vulnerability is estimated based on the area they occupy and the time they spend in the issue queue. By changing the vulnerability threshold, we can adjust the trade-off between coverage and performance loss. Results for an out-of-order processor configured similarly to Intel® Core™ Micro-Architecture show that our scheme can achieve over 65% FIT reduction with less than 4% performance degradation with small area and complexity overhead.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2032797509",
    "type": "article"
  },
  {
    "title": "A Hierarchical Thread Scheduler and Register File for Energy-Efficient Throughput Processors",
    "doi": "https://doi.org/10.1145/2166879.2166882",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Mark Gebhart; Daniel Johnson; David Tarjan; Stephen W. Keckler; William J. Dally; Erik Lindholm; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "Modern graphics processing units (GPUs) employ a large number of hardware threads to hide both function unit and memory access latency. Extreme multithreading requires a complex thread scheduler as well as a large register file, which is expensive to access both in terms of energy and latency. We present two complementary techniques for reducing energy on massively-threaded processors such as GPUs. First, we investigate a two-level thread scheduler that maintains a small set of active threads to hide ALU and local memory access latency and a larger set of pending threads to hide main memory latency. Reducing the number of threads that the scheduler must consider each cycle improves the scheduler’s energy efficiency. Second, we propose replacing the monolithic register file found on modern designs with a hierarchical register file. We explore various trade-offs for the hierarchy including the number of levels in the hierarchy and the number of entries at each level. We consider both a hardware-managed caching scheme and a software-managed scheme, where the compiler is responsible for orchestrating all data movement within the register file hierarchy. Combined with a hierarchical register file, our two-level thread scheduler provides a further reduction in energy by only allocating entries in the upper levels of the register file hierarchy for active threads. Averaging across a variety of real world graphics and compute workloads, the active thread count can be reduced by a factor of 4 with minimal impact on performance and our most efficient three-level software-managed register file hierarchy reduces register file energy by 54%.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2018150881",
    "type": "article"
  },
  {
    "title": "Management of Multilevel, Multiclient Cache Hierarchies with Application Hints",
    "doi": "https://doi.org/10.1145/1963559.1963561",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Gala Yadgar; Michael Factor; Kai Li; Assaf Schuster",
    "corresponding_authors": "",
    "abstract": "Multilevel caching, common in many storage configurations, introduces new challenges to traditional cache management: data must be kept in the appropriate cache and replication avoided across the various cache levels. Additional challenges are introduced when the lower levels of the hierarchy are shared by multiple clients. Sharing can have both positive and negative effects. While data fetched by one client can be used by another client without incurring additional delays, clients competing for cache buffers can evict each other’s blocks and interfere with exclusive caching schemes. We present a global noncentralized, dynamic and informed management policy for multiple levels of cache, accessed by multiple clients. Our algorithm, MC 2 , combines local, per client management with a global, system-wide scheme, to emphasize the positive effects of sharing and reduce the negative ones. Our local management scheme, Karma , uses readily available information about the client’s future access profile to save the most valuable blocks, and to choose the best replacement policy for them. The global scheme uses the same information to divide the shared cache space between clients, and to manage this space. Exclusive caching is maintained for nonshared data and is disabled when sharing is identified. Previous studies have partially addressed these challenges through minor changes to the storage interface. We show that all these challenges can in fact be addressed by combining minor interface changes with smart allocation and replacement policies. We show the superiority of our approach through comparison to existing solutions, including LRU, ARC, MultiQ, LRU-SP, and Demote, as well as a lower bound on optimal I/O response times. Our simulation results demonstrate better cache performance than all other solutions and up to 87% better performance than LRU on representative workloads.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2040706084",
    "type": "article"
  },
  {
    "title": "Quantifying the Mismatch between Emerging Scale-Out Applications and Modern Processors",
    "doi": "https://doi.org/10.1145/2382553.2382557",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Michael Ferdman; Almutaz Adileh; Onur Kocberber; Stavros Volos; Mohammad Alisafaee; Djordje Jevdjic; Cansu Kaynak; Adrian Popescu; Anastasia Ailamaki; Babak Falsafi",
    "corresponding_authors": "",
    "abstract": "Emerging scale-out workloads require extensive amounts of computational resources. However, data centers using modern server hardware face physical constraints in space and power, limiting further expansion and calling for improvements in the computational density per server and in the per-operation energy. Continuing to improve the computational resources of the cloud while staying within physical constraints mandates optimizing server efficiency to ensure that server hardware closely matches the needs of scale-out workloads. In this work, we introduce CloudSuite, a benchmark suite of emerging scale-out workloads. We use performance counters on modern servers to study scale-out workloads, finding that today’s predominant processor microarchitecture is inefficient for running these workloads. We find that inefficiency comes from the mismatch between the workload needs and modern processors, particularly in the organization of instruction and data memory systems and the processor core microarchitecture. Moreover, while today’s predominant microarchitecture is inefficient when executing scale-out workloads, we find that continuing the current trends will further exacerbate the inefficiency in the future. In this work, we identify the key microarchitectural needs of scale-out workloads, calling for a change in the trajectory of server processors that would lead to improved computational density and power efficiency in data centers.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2095196235",
    "type": "article"
  },
  {
    "title": "ISA Wars",
    "doi": "https://doi.org/10.1145/2699682",
    "publication_date": "2015-03-11",
    "publication_year": 2015,
    "authors": "Emily Blem; J. Menon; Thiruvengadam Vijayaraghavan; Karthikeyan Sankaralingam",
    "corresponding_authors": "",
    "abstract": "RISC versus CISC wars raged in the 1980s when chip area and processor design complexity were the primary constraints and desktops and servers exclusively dominated the computing landscape. Today, energy and power are the primary design constraints and the computing landscape is significantly different: Growth in tablets and smartphones running ARM (a RISC ISA) is surpassing that of desktops and laptops running x86 (a CISC ISA). Furthermore, the traditionally low-power ARM ISA is entering the high-performance server market, while the traditionally high-performance x86 ISA is entering the mobile low-power device market. Thus, the question of whether ISA plays an intrinsic role in performance or energy efficiency is becoming important again, and we seek to answer this question through a detailed measurement-based study on real hardware running real applications. We analyze measurements on seven platforms spanning three ISAs (MIPS, ARM, and x86) over workloads spanning mobile, desktop, and server computing. Our methodical investigation demonstrates the role of ISA in modern microprocessors’ performance and energy efficiency. We find that ARM, MIPS, and x86 processors are simply engineering design points optimized for different levels of performance, and there is nothing fundamentally more energy efficient in one ISA class or the other. The ISA being RISC or CISC seems irrelevant.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1988220683",
    "type": "article"
  },
  {
    "title": "Fast and Portable Locking for Multicore Architectures",
    "doi": "https://doi.org/10.1145/2845079",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Jean-Pierre Lozi; Florian David; Gaël Thomas; Julia Lawall; Gilles Muller",
    "corresponding_authors": "",
    "abstract": "The scalability of multithreaded applications on current multicore systems is hampered by the performance of lock algorithms, due to the costs of access contention and cache misses. The main contribution presented in this article is a new locking technique, Remote Core Locking (RCL), that aims to accelerate the execution of critical sections in legacy applications on multicore architectures. The idea of RCL is to replace lock acquisitions by optimized remote procedure calls to a dedicated server hardware thread. RCL limits the performance collapse observed with other lock algorithms when many threads try to acquire a lock concurrently and removes the need to transfer lock-protected shared data to the hardware thread acquiring the lock, because such data can typically remain in the server’s cache. Other contributions presented in this article include a profiler that identifies the locks that are the bottlenecks in multithreaded applications and that can thus benefit from RCL, and a reengineering tool that transforms POSIX lock acquisitions into RCL locks. Eighteen applications were used to evaluate RCL: the nine applications of the SPLASH-2 benchmark suite, the seven applications of the Phoenix 2 benchmark suite, Memcached, and Berkeley DB with a TPC-C client. Eight of these applications are unable to scale because of locks and benefit from RCL on an ×86 machine with four AMD Opteron processors and 48 hardware threads. By using RCL instead of Linux POSIX locks, performance is improved by up to 2.5 times on Memcached, and up to 11.6 times on Berkeley DB with the TPC-C client. On a SPARC machine with two Sun Ultrasparc T2+ processors and 128 hardware threads, three applications benefit from RCL. In particular, performance is improved by up to 1.3 times with respect to Solaris POSIX locks on Memcached, and up to 7.9 times on Berkeley DB with the TPC-C client.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2227501301",
    "type": "article"
  },
  {
    "title": "Manageability, availability, and performance in porcupine",
    "doi": "https://doi.org/10.1145/354871.354875",
    "publication_date": "2000-08-01",
    "publication_year": 2000,
    "authors": "Yasushi Saitō; Brian N. Bershad; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "This paper describes the motivation, design and performance of Porcupine, a scalable mail server. The goal of Porcupine is to provide a highly available and scalable electronic mail service using a large cluster of commodity PCs. We designed Porcupine to be easy to manage by emphasizing dynamic load balancing, automatic configuration, and graceful degradation in the presence of failures. Key to the system's manageability, availability, and performance is that sessions, data, and underlying services are distributed homogeneously and dynamically across nodes in a cluster.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2010445731",
    "type": "article"
  },
  {
    "title": "CHAOS <sup>arc</sup> : kernel support for multiweight objects, invocations, and atomicity in real-time multiprocessor applications",
    "doi": "https://doi.org/10.1145/151250.151252",
    "publication_date": "1993-02-01",
    "publication_year": 1993,
    "authors": "Ahmed Gheith; Karsten Schwan",
    "corresponding_authors": "",
    "abstract": "CHAOS arc is an object-based multiprocessor operating system kernel that provides primitives with which programmers may easily construct objects of differing types and object invocations of differing semantics, targeting multiprocessor systems, and real-time applications. The CHAOS arc can guarantee desired performance and functionality levels of selected computations in real-time applications. Such guarantees can be made despite possible uncertainty in execution environments by allowing programs to adapt in performance and functionality to varying operating conditions. This paper reviews the primitives offered by CHAOS arc and demonstrates how the required elements of the CHAOS arc real-time kernel are constructed with those primitives.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W1964988644",
    "type": "article"
  },
  {
    "title": "Design tradeoffs for software-managed TLBs",
    "doi": "https://doi.org/10.1145/185514.185515",
    "publication_date": "1994-08-01",
    "publication_year": 1994,
    "authors": "Richard Uhlig; David F. Nagle; Tim Stanley; Trevor Mudge; Stuart Sechrest; Richard B. Brown",
    "corresponding_authors": "",
    "abstract": "An increasing number of architectures provide virtual memory support through software-managed TLBs. However, software management can impose considerable penalties that are highly dependent on the operating system's structure and its use of virtual memory. This work explores software-managed TLB design tradeoffs and their interaction with a range of monolithic and microkernel operating systems. Through hardware monitoring and simulation, we explore TLB performance for benchmarks running on a MIPS R2000-based workstation running Ultrix, OSF/1, and three versions of Mach 3.0.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2107872701",
    "type": "article"
  },
  {
    "title": "Effective cache prefetching on bus-based multiprocessors",
    "doi": "https://doi.org/10.1145/200912.201006",
    "publication_date": "1995-02-01",
    "publication_year": 1995,
    "authors": "Dean M. Tullsen; Susan J. Eggers",
    "corresponding_authors": "",
    "abstract": "Compiler-directed cache prefetching has the potential to hide much of the high memory latency seen by current and future high-performance processors. However, prefetching is not without costs, particularly on a shared-memory multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory latencies and data sharing. We simulate the effects of a compiler-directed prefetching algorithm, running on a range of bus-based multiprocessors. We show that, despite a high memory latency, this architecture does not necessarily support prefetching well, in some cases actually causing performance degradations. We pinpoint several problems with prefetching on a shared-memory architecture (additional conflict misses, no reduction in the data-sharing traffic and associated latencies, a multiprocessor's greater sensitivity to memory utilization and the sensitivity of the cache hit rate to prefetch distance) and measure their effect on performance. We then solve those problems through architectural techniques and heuristics for prefetching that could be easily incorporated into a compiler: (1) victim caching, which eliminates most of the cache conflict misses caused by prefetching in a direct-mapped cache, (2) special prefetch algorithms for shared data, which significantly improve the ability of our basic prefetching algorithm to prefetch individual misses, and (3) compiler-based shared-data restructuring, which eliminates many of the invalidation misses the basic prefetching algorithm does not predict. The combined effect of these improvements is to make prefetching effective over a much wider range of memory architectures.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1991582153",
    "type": "article"
  },
  {
    "title": "An Internet multicast system for the stock market",
    "doi": "https://doi.org/10.1145/380749.380771",
    "publication_date": "2001-08-01",
    "publication_year": 2001,
    "authors": "N.F. Maxemchuk; David Shur",
    "corresponding_authors": "",
    "abstract": "We are moving toward an international, 24-hour, distributed, electronic stock exchange. The exchange will use the global Internet, or internet technology. This system is a natural application of multicast because there are a large number of receivers that should receive the same information simultaneously. The data requirements for the stock exchange are discussed. The current multicast protocols lack the reliability, fairness, and scalability needed in this application. We describe a distributed architecture and a timed reliable multicast protocol, TRMP, that has the appropriate characteristics. We consider three applications: (1) A unified stock ticker of the transactions that are being conducted on the various physical and electronic exchanges. Our objective is to deliver the same combined ticker reliability and simultaneously to all receivers, anywhere in the world. (2) A unified sequence of buy and sell offers that are delivered to a single exchange or a collection of exchanges. Our objective is to give all traders the same fair access to an exchange independent of their relative distances to the exchange or delay and loss characteristics of the international network. (3) A distributed, electronic trading floor that can replace the current exchanges. This application has the fairness attributes of the first two applications and uses TRMP to conduct irrefutable, distributed trades.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2000393451",
    "type": "article"
  },
  {
    "title": "Fault-tolerance in air traffic control systems",
    "doi": "https://doi.org/10.1145/233557.233559",
    "publication_date": "1996-08-01",
    "publication_year": 1996,
    "authors": "Flaviu Cristian; Bob Dancey; Jon Dehn",
    "corresponding_authors": "",
    "abstract": "The distributed real-time system services developed by Lockheed Martin's Air Traffic Management group serve the infrastructure for a number of air traffic control systems. Either completed development or under development are the US Federal Aviation Administration's Display System Replacement (DSR) system, the UK Civil Aviation Authority's New Enroute Center (NERC) system, and the Republic of China's Air Traffic Control Automated System (ATCAS). These systems are intended to replace present en route systems over the next decade. High availability of air traffic control services is an essential requirement of these systems. This article discusses the general approach to fault-tolerance adopted in this infrastructure, by reviewing some of the questions which were asked during the system design, various alternative solutions considered, and the reasons for the design choices made. The aspects of this infrastructure chosen for the individual ATC systems mentioned above, along with the status of those systems, are presented in the Section 11 of the article.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2035532773",
    "type": "article"
  },
  {
    "title": "Deriving protocol specifications from service specifications including parameters",
    "doi": "https://doi.org/10.1145/128733.128734",
    "publication_date": "1990-11-01",
    "publication_year": 1990,
    "authors": "Reinhard Gotzhein; Gregor von Bochmann",
    "corresponding_authors": "",
    "abstract": "The service specification concept has acquired an increasing level of recognition by protocol designers. This architectural concept influences the methodology applied to service and protocol definition. Since the protocol is seen as the logical implementation of the service, one can ask whether it is possible to formally derive the specification of a protocol providing a given service. This paper addresses this question and presents an algorithm for deriving a protocol specification from a given service specification. It is assumed that services are described by expressions, where names identifying both service primitives and previously defined services are composed using operators for sequence, parallelism and alternative. Services and service primitives may have input and output parameters. Composition of services from predefined services and service primitives is also permitted. The expression defining the service is the basis for the protocol derivation process. The algorithm presented fully automates the derivation process. Future work will focus on the optimization of traffic between protocol entities and on applications.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2028032512",
    "type": "article"
  },
  {
    "title": "Performance effects of architectural complexity in the Intel 432",
    "doi": "https://doi.org/10.1145/45059.214411",
    "publication_date": "1988-08-01",
    "publication_year": 1988,
    "authors": "Robert P. Colwell; Edward F. Gehringer; E. Douglas Jensen",
    "corresponding_authors": "",
    "abstract": "The Intel 432 is noteworthy as an architecture incorporating a large amount of functionality that most other systems perform by software. It has, in effect, “migrated” this functionality from the software into the microcode and hardware. The benefits of functional migration have recently been a subject of intense controversy, with critics claiming that a complex architecture is inherently less efficient than a simple architecture with good software support. This paper examines the performance impact of the incorporation of several kinds of functionality into the Intel 432. Among these are the addressing structure, the caches, instruction alignment, the buses, and the way that garbage collection is handled. A set of several benchmarks is used to quantify the performance effect of each of these decisions. The results indicate that the 432 could have been speeded up very significantly if a small number of implementation decisions had been made differently, and if incrementally better technology had been used in its construction. Even with these modifications, however, the 432 would still have only one-fourth to one times the speed of its contemporaries. These figures may represent the real cost of the 432's style of object-based programming environment.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2077631114",
    "type": "article"
  },
  {
    "title": "The Liberty Simulation Environment",
    "doi": "https://doi.org/10.1145/1151690.1151691",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Manish Vachharajani; Neil Vachharajani; David A. Penry; Jason Blome; Sharad Malik; David I. August",
    "corresponding_authors": "",
    "abstract": "In digital hardware system design, the quality of the product is directly related to the number of meaningful design alternatives properly considered. Unfortunately, existing modeling methodologies and tools have properties which make them less than ideal for rapid and accurate design-space exploration. This article identifies and evaluates the shortcomings of existing methods to motivate the Liberty Simulation Environment (LSE). LSE is a high-level modeling tool engineered to address these limitations, allowing for the rapid construction of accurate high-level simulation models. LSE simplifies model specification with low-overhead component-based reuse techniques and an abstraction for timing control. As part of a detailed description of LSE, this article presents these features, their impact on model specification effort, their implementation, and optimizations created to mitigate their otherwise deleterious impact on simulator execution performance.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2062581063",
    "type": "article"
  },
  {
    "title": "PACS",
    "doi": "https://doi.org/10.1145/357369.357370",
    "publication_date": "1983-08-01",
    "publication_year": 1983,
    "authors": "Tsutomu Hoshino; Toshio Kawai; Tomonori Shirakawa; Junchi Higashino; Akira YAMAOKA; Hachidai Ito; Takashi Sato; Kazuo Sawada",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on PACS: a parallel microprocessor array for scientific calculations Authors: Tsutomu Hoshino Institute of Engineering Mechanics, University of Tsukuba, 1-1-1, Tenodai, Sakura, Niihari, Ibaraki-Ken 305, Japan Institute of Engineering Mechanics, University of Tsukuba, 1-1-1, Tenodai, Sakura, Niihari, Ibaraki-Ken 305, JapanView Profile , Toshio Kawai Department of Physics, Keio University, 3-14-1, Hiyoshi, Kohokuku, Yokohama-shi 223, Japan Department of Physics, Keio University, 3-14-1, Hiyoshi, Kohokuku, Yokohama-shi 223, JapanView Profile , Tomonori Shirakawa Institute of Engineering Mechanics, University of Tsukuba, 1-1-1, Tenodai, Sakura, Niihari, Ibaraki-Ken 305, Japan Institute of Engineering Mechanics, University of Tsukuba, 1-1-1, Tenodai, Sakura, Niihari, Ibaraki-Ken 305, JapanView Profile , Junchi Higashino Central Research Laboratory, Hitachi Ltd., 1-280, Higashi Koigakubo, Kokubunji-shi, Tokyo 185, Japan Central Research Laboratory, Hitachi Ltd., 1-280, Higashi Koigakubo, Kokubunji-shi, Tokyo 185, JapanView Profile , Akira Yamaoka Central Research Laboratory, Hitachi Ltd., 1-280, Higashi Koigakubo, Kokubunji-shi, Tokyo 185, Japan Central Research Laboratory, Hitachi Ltd., 1-280, Higashi Koigakubo, Kokubunji-shi, Tokyo 185, JapanView Profile , Hachidai Ito Fuchu Works, Toshiba Corp., 1, Toshiba-cho, Fuchu-shi, Tokyo 183, Japan Fuchu Works, Toshiba Corp., 1, Toshiba-cho, Fuchu-shi, Tokyo 183, JapanView Profile , Takashi Sato INTEC Inc., 3-37-18, Hatagaya, Shibuya-Ku, Tokyo 151, Japan INTEC Inc., 3-37-18, Hatagaya, Shibuya-Ku, Tokyo 151, JapanView Profile , Kazuo Sawada FUJITSU FANUC Ltd., 3-5-1, Asahigaoka, Hino-shi, Tokyo 191, Japan FUJITSU FANUC Ltd., 3-5-1, Asahigaoka, Hino-shi, Tokyo 191, JapanView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 3August 1983 pp 195–221https://doi.org/10.1145/357369.357370Published:01 August 1983Publication History 34citation421DownloadsMetricsTotal Citations34Total Downloads421Last 12 Months12Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2009895650",
    "type": "article"
  },
  {
    "title": "Concurrency versus availability: atomicity mechanisms for replicated data",
    "doi": "https://doi.org/10.1145/24068.27643",
    "publication_date": "1987-08-01",
    "publication_year": 1987,
    "authors": "Maurice Herlihy",
    "corresponding_authors": "Maurice Herlihy",
    "abstract": "A replicated object is a typed data object that is stored redundantly at multiple locations to enhance availability. Most techniques for managing replicated data have a two-level structure: At the higher level, a replica-control protocol reconstructs the object's state from its distributed components, and at the lower level, a standard concurrency-control protocol synchronizes accesses to the individual components. This paper explores an alternative approach to managing replicated data by presenting two replication methods in which concurrency control and replica management are handled by a single integrated protocol. These integrated protocols permit more concurrency than independent protocols, and they allow availability and concurrency to be traded off: Constraints on concurrency may be relaxed if constraints on availability are tightened, and vice versa. In general, constraints on concurrency and availability cannot be minimized simultaneously.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2022960291",
    "type": "article"
  },
  {
    "title": "Specifying memory consistency of write buffer multiprocessors",
    "doi": "https://doi.org/10.1145/1189736.1189737",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Lisa Highám; LillAnne Jackson; Jalal Kawash",
    "corresponding_authors": "",
    "abstract": "Write buffering is one of many successful mechanisms that improves the performance and scalability of multiprocessors. However, it leads to more complex memory system behavior, which cannot be described using intuitive consistency models, such as Sequential Consistency. It is crucial to provide programmers with a specification of the exact behavior of such complex memories. This article presents a uniform framework for describing systems at different levels of abstraction and proving their equivalence. The framework is used to derive and prove correct simple specifications in terms of program-level instructions of the sparc total store order and partial store order memories.The framework is also used to examine the sparc relaxed memory order. We show that it is not a memory consistency model that corresponds to any implementation on a multiprocessor that uses write-buffers, even though we suspect that the sparc version 9 specification of relaxed memory order was intended to capture a general write-buffer architecture. The same technique is used to show that Coherence does not correspond to a write-buffer architecture. A corollary, which follows from the relationship between Coherence and Alpha, is that any implementation of Alpha consistency using write-buffers cannot produce all possible Alpha computations. That is, there are some computations that satisfy the Alpha specification but cannot occur in the given write-buffer implementation.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1982078374",
    "type": "article"
  },
  {
    "title": "Vigilante",
    "doi": "https://doi.org/10.1145/1455258.1455259",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Manuel Costa; Jon Crowcroft; Miguel Castro; Antony Rowstron; Lidong Zhou; Lintao Zhang; Paul Barham",
    "corresponding_authors": "",
    "abstract": "Worm containment must be automatic because worms can spread too fast for humans to respond. Recent work proposed network-level techniques to automate worm containment; these techniques have limitations because there is no information about the vulnerabilities exploited by worms at the network level. We propose Vigilante, a new end-to-end architecture to contain worms automatically that addresses these limitations. In Vigilante, hosts detect worms by instrumenting vulnerable programs to analyze infection attempts. We introduce dynamic data-flow analysis : a broad-coverage host-based algorithm that can detect unknown worms by tracking the flow of data from network messages and disallowing unsafe uses of this data. We also show how to integrate other host-based detection mechanisms into the Vigilante architecture. Upon detection, hosts generate self-certifying alerts (SCAs), a new type of security alert that can be inexpensively verified by any vulnerable host. Using SCAs, hosts can cooperate to contain an outbreak, without having to trust each other. Vigilante broadcasts SCAs over an overlay network that propagates alerts rapidly and resiliently. Hosts receiving an SCA protect themselves by generating filters with vulnerability condition slicing : an algorithm that performs dynamic analysis of the vulnerable program to identify control-flow conditions that lead to successful attacks. These filters block the worm attack and all its polymorphic mutations that follow the execution path identified by the SCA. Our results show that Vigilante can contain fast-spreading worms that exploit unknown vulnerabilities, and that Vigilante's filters introduce a negligible performance overhead. Vigilante does not require any changes to hardware, compilers, operating systems, or the source code of vulnerable programs; therefore, it can be used to protect current software binaries.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2109077605",
    "type": "article"
  },
  {
    "title": "Memory scheduling for modern microprocessors",
    "doi": "https://doi.org/10.1145/1314299.1314301",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Ibrahim Hur; Calvin Lin",
    "corresponding_authors": "",
    "abstract": "The need to carefully schedule memory operations has increased as memory performance has become increasingly important to overall system performance. This article describes the adaptive history-based (AHB) scheduler, which uses the history of recently scheduled operations to provide three conceptual benefits: (1) it allows the scheduler to better reason about the delays associated with its scheduling decisions, (2) it provides a mechanism for combining multiple constraints, which is important for increasingly complex DRAM structures, and (3) it allows the scheduler to select operations so that they match the program's mixture of Reads and Writes, thereby avoiding certain bottlenecks within the memory controller. We have previously evaluated this scheduler in the context of the IBM Power5. When compared with the state of the art, this scheduler improves performance by 15.6%, 9.9%, and 7.6% for the Stream, NAS, and commercial benchmarks, respectively. This article expands our understanding of the AHB scheduler in a variety of ways. Looking backwards, we describe the scheduler in the context of prior work that focused exclusively on avoiding bank conflicts, and we show that the AHB scheduler is superior for the IBM Power5, which we argue will be representative of future microprocessor memory controllers. Looking forwards, we evaluate this scheduler in the context of future systems by varying a number of microarchitectural features and hardware parameters. For example, we show that the benefit of this scheduler increases as we move to multithreaded environments.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1999136361",
    "type": "article"
  },
  {
    "title": "Hill-climbing SMT processor resource distribution",
    "doi": "https://doi.org/10.1145/1482619.1482620",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "Seungryul Choi; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "The key to high performance in Simultaneous MultiThreaded (SMT) processors lies in optimizing the distribution of shared resources to active threads. Existing resource distribution techniques optimize performance only indirectly. They infer potential performance bottlenecks by observing indicators, like instruction occupancy or cache miss counts, and take actions to try to alleviate them. While the corrective actions are designed to improve performance, their actual performance impact is not known since end performance is never monitored. Consequently, potential performance gains are lost whenever the corrective actions do not effectively address the actual bottlenecks occurring in the pipeline. We propose a different approach to SMT resource distribution that optimizes end performance directly. Our approach observes the impact that resource distribution decisions have on performance at runtime, and feeds this information back to the resource distribution mechanisms to improve future decisions. By evaluating many different resource distributions, our approach tries to learn the best distribution over time. Because we perform learning online, learning time is crucial. We develop a hill-climbing algorithm that quickly learns the best distribution of resources by following the performance gradient within the resource distribution space. We also develop several ideal learning algorithms to enable deeper insights through limit studies. This article conducts an in-depth investigation of hill-climbing SMT resource distribution using a comprehensive suite of 63 multiprogrammed workloads. Our results show hill-climbing outperforms ICOUNT, FLUSH, and DCRA (three existing SMT techniques) by 11.4%, 11.5%, and 2.8%, respectively, under the weighted IPC metric. A limit study conducted using our ideal learning algorithms shows our approach can potentially outperform the same techniques by 19.2%, 18.0%, and 7.6%, respectively, thus demonstrating additional room exists for further improvement. Using our ideal algorithms, we also identify three bottlenecks that limit online learning speed: local maxima, phased behavior, and interepoch jitter. We define metrics to quantify these learning bottlenecks, and characterize the extent to which they occur in our workloads. Finally, we conduct a sensitivity study, and investigate several extensions to improve our hill-climbing technique.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2065262858",
    "type": "article"
  },
  {
    "title": "Probabilistic quorum systems in wireless Ad Hoc networks",
    "doi": "https://doi.org/10.1145/1841313.1841315",
    "publication_date": "2008-09-30",
    "publication_year": 2008,
    "authors": "Roy Friedman; Gabriel Kliot; Chen Avin",
    "corresponding_authors": "",
    "abstract": "Quorums are a basic construct in solving many fundamental distributed computing problems. One of the known ways of making quorums scalable and efficient is by weakening their intersection guarantee to being probabilistic. This article explores several access strategies for implementing probabilistic quorums in ad hoc networks. In particular, we present the first detailed study of asymmetric probabilistic biquorum systems, that allow to mix different access strategies and different quorums sizes, while guaranteeing the desired intersection probability. We show the advantages of asymmetric probabilistic biquorum systems in ad hoc networks. Such an asymmetric construction is also useful for other types of networks with nonuniform access costs (e.g, peer-to-peer networks). The article includes a formal analysis of these approaches backed up by an extensive simulation-based study. The study explores the impact of various parameters such as network size, network density, mobility speed, and churn. In particular, we show that one of the strategies that uses random walks exhibits the smallest communication overhead, thus being very attractive for ad hoc networks.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2146515084",
    "type": "article"
  },
  {
    "title": "Predicting and preventing inconsistencies in deployed distributed systems",
    "doi": "https://doi.org/10.1145/1731060.1731062",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Maysam Yabandeh; Nikola Knežević; Dejan Kostić; Viktor Kunčak",
    "corresponding_authors": "",
    "abstract": "We propose a new approach for developing and deploying distributed systems, in which nodes predict distributed consequences of their actions and use this information to detect and avoid errors. Each node continuously runs a state exploration algorithm on a recent consistent snapshot of its neighborhood and predicts possible future violations of specified safety properties. We describe a new state exploration algorithm, consequence prediction, which explores causally related chains of events that lead to property violation. This article describes the design and implementation of this approach, termed CrystalBall. We evaluate CrystalBall on RandTree, BulletPrime, Paxos, and Chord distributed system implementations. We identified new bugs in mature Mace implementations of three systems. Furthermore, we show that if the bug is not corrected during system development, CrystalBall is effective in steering the execution away from inconsistent states at runtime.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1973139016",
    "type": "article"
  },
  {
    "title": "Parametric Content-Based Publish/Subscribe",
    "doi": "https://doi.org/10.1145/2465346.2465347",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "K. R. Jayaram; Patrick Eugster; Chamikara Jayalath",
    "corresponding_authors": "",
    "abstract": "Content-based publish/subscribe (CPS) is an appealing abstraction for building scalable distributed systems, e.g., message boards, intrusion detectors, or algorithmic stock trading platforms. Recently, CPS extensions have been proposed for location-based services like vehicular networks, mobile social networking, and so on. Although current CPS middleware systems are dynamic in the way they support the joining and leaving of publishers and subscribers, they fall short in supporting subscription adaptations. These are becoming increasingly important across many CPS applications. In algorithmic high frequency trading, for instance, stock price thresholds that are of interest to a trader change rapidly, and gains directly hinge on the reaction time to relevant fluctuations rather than fixed values. In location-aware applications, a subscription is a function of the subscriber location (e.g. GPS coordinates), which inherently changes during motion. The common solution for adapting a subscription consists of a resubscription, where a new subscription is issued and the superseded one canceled. This incurs substantial overhead in CPS middleware systems, and leads to missed or duplicated events during the transition. In this article, we explore the concept of parametric subscriptions for capturing subscription adaptations. We discuss desirable and feasible guarantees for corresponding support, and propose novel algorithms for updating routing mechanisms effectively and efficiently in classic decentralized CPS broker overlay networks. Compared to resubscriptions, our algorithms significantly improve the reaction time to subscription updates without hampering throughput or latency under high update rates. We also propose and evaluate approximation techniques to detect and mitigate pathological cases of high frequency subscription oscillations, which could significantly decrease the throughput of CPS systems thereby affecting other subscribers. We analyze the benefits of our support through implementations of our algorithms in two CPS systems, and by evaluating our algorithms on two different application scenarios.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2084351852",
    "type": "article"
  },
  {
    "title": "TritonSort",
    "doi": "https://doi.org/10.1145/2427631.2427634",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Alexander Rasmussen; George Porter; Michael Conley; Harsha V. Madhyastha; Radhika Niranjan Mysore; Alexander Pucher; Amin Vahdat",
    "corresponding_authors": "",
    "abstract": "We present TritonSort, a highly efficient, scalable sorting system. It is designed to process large datasets, and has been evaluated against as much as 100TB of input data spread across 832 disks in 52 nodes at a rate of 0.938TB/min. When evaluated against the annual Indy GraySort sorting benchmark, TritonSort is 66% better in absolute performance and has over six times the per-node throughput of the previous record holder. When evaluated against the 100TB Indy JouleSort benchmark, TritonSort sorted 9703 records/Joule. In this article, we describe the hardware and software architecture necessary to operate TritonSort at this level of efficiency. Through careful management of system resources to ensure cross-resource balance, we are able to sort data at approximately 80% of the disks’ aggregate sequential write speed. We believe the work holds a number of lessons for balanced system design and for scale-out architectures in general. While many interesting systems are able to scale linearly with additional servers, per-server performance can lag behind per-server capacity by more than an order of magnitude. Bridging the gap between high scalability and high performance would enable either significantly less expensive systems that are able to do the same work or provide the ability to address significantly larger problem sets with the same infrastructure.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2082322254",
    "type": "article"
  },
  {
    "title": "BlueDBM",
    "doi": "https://doi.org/10.1145/2898996",
    "publication_date": "2016-06-30",
    "publication_year": 2016,
    "authors": "Sang-Woo Jun; Ming Liu; Sungjin Lee; Jamey Hicks; John Ankcorn; Myron King; Shuotao Xu; Arvind Arvind",
    "corresponding_authors": "",
    "abstract": "Complex data queries, because of their need for random accesses, have proven to be slow unless all the data can be accommodated in DRAM. There are many domains, such as genomics, geological data, and daily Twitter feeds, where the datasets of interest are 5TB to 20TB. For such a dataset, one would need a cluster with 100 servers, each with 128GB to 256GB of DRAM, to accommodate all the data in DRAM. On the other hand, such datasets could be stored easily in the flash memory of a rack-sized cluster. Flash storage has much better random access performance than hard disks, which makes it desirable for analytics workloads. However, currently available off-the-shelf flash storage packaged as SSDs does not make effective use of flash storage because it incurs a great amount of additional overhead during flash device management and network access. In this article, we present BlueDBM, a new system architecture that has flash-based storage with in-store processing capability and a low-latency high-throughput intercontroller network between storage devices. We show that BlueDBM outperforms a flash-based system without these features by a factor of 10 for some important applications. While the performance of a DRAM-centric system falls sharply even if only 5% to 10% of the references are to secondary storage, this sharp performance degradation is not an issue in BlueDBM. BlueDBM presents an attractive point in the cost/performance tradeoff for Big Data analytics.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2466434324",
    "type": "article"
  },
  {
    "title": "Lock–Unlock",
    "doi": "https://doi.org/10.1145/3301501",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Rachid Guerraoui; Hugo Guiroux; Renaud Lachaize; Vivien Quéma; Vasileios Trigonakis",
    "corresponding_authors": "",
    "abstract": "A plethora of optimized mutex lock algorithms have been designed over the past 25 years to mitigate performance bottlenecks related to critical sections and locks. Unfortunately, there is currently no broad study of the behavior of these optimized lock algorithms on realistic applications that consider different performance metrics, such as energy efficiency and tail latency. In this article, we perform a thorough and practical analysis of synchronization, with the goal of providing software developers with enough information to design fast, scalable, and energy-efficient synchronization in their systems. First, we perform a performance study of 28 state-of-the-art mutex lock algorithms, on 40 applications, on four different multicore machines. We consider not only throughput (traditionally the main performance metric) but also energy efficiency and tail latency, which are becoming increasingly important. Second, we present an in-depth analysis in which we summarize our findings for all the studied applications. In particular, we describe nine different lock-related performance bottlenecks, and we propose six guidelines helping software developers with their choice of a lock algorithm according to the different lock properties and the application characteristics. From our detailed analysis, we make several observations regarding locking algorithms and application behaviors, several of which have not been previously discovered: (i) applications stress not only the lock–unlock interface but also the full locking API (e.g., trylocks, condition variables); (ii) the memory footprint of a lock can directly affect the application performance; (iii) for many applications, the interaction between locks and scheduling is an important application performance factor; (vi) lock tail latencies may or may not affect application tail latency; (v) no single lock is systematically the best; (vi) choosing the best lock is difficult; and (vii) energy efficiency and throughput go hand in hand in the context of lock algorithms. These findings highlight that locking involves more considerations than the simple lock/unlock interface and call for further research on designing low-memory footprint adaptive locks that fully and efficiently support the full lock interface, and consider all performance metrics.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2921135130",
    "type": "article"
  },
  {
    "title": "Designing Future Warehouse-Scale Computers for Sirius, an End-to-End Voice and Vision Personal Assistant",
    "doi": "https://doi.org/10.1145/2870631",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Johann Hauswald; Michael A. Laurenzano; Yunqi Zhang; Hailong Yang; Yiping Kang; Cheng Li; Austin Rovinski; Arjun Khurana; Ronald Dreslinski; Trevor Mudge; Vinícius Petrucci; Lingjia Tang; Jason Mars",
    "corresponding_authors": "",
    "abstract": "As user demand scales for intelligent personal assistants (IPAs) such as Apple’s Siri, Google’s Google Now, and Microsoft’s Cortana, we are approaching the computational limits of current datacenter (DC) architectures. It is an open question how future server architectures should evolve to enable this emerging class of applications, and the lack of an open-source IPA workload is an obstacle in addressing this question. In this article, we present the design of Sirius, an open end-to-end IPA Web-service application that accepts queries in the form of voice and images, and responds with natural language. We then use this workload to investigate the implications of four points in the design space of future accelerator-based server architectures spanning traditional CPUs, GPUs, manycore throughput co-processors, and FPGAs. To investigate future server designs for Sirius, we decompose Sirius into a suite of eight benchmarks (Sirius Suite) comprising the computationally intensive bottlenecks of Sirius. We port Sirius Suite to a spectrum of accelerator platforms and use the performance and power trade-offs across these platforms to perform a total cost of ownership (TCO) analysis of various server design points. In our study, we find that accelerators are critical for the future scalability of IPA services. Our results show that GPU- and FPGA-accelerated servers improve the query latency on average by 8.5× and 15×, respectively. For a given throughput, GPU- and FPGA-accelerated servers can reduce the TCO of DCs by 2.3× and 1.3×, respectively.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2333671762",
    "type": "article"
  },
  {
    "title": "SPATA: Effective OS Bug Detection with Summary-Based, Alias-Aware and Path-Sensitive Typestate Analysis",
    "doi": "https://doi.org/10.1145/3695250",
    "publication_date": "2024-09-06",
    "publication_year": 2024,
    "authors": "T. F. Li; Jia-Ju Bai; Yulei Sui; Shi‐Min Hu",
    "corresponding_authors": "",
    "abstract": "Operating system (OS) is the cornerstone for computer systems. It manages hardware and provides fundamental service for user-level applications. Thus, detecting bugs in OSes is important to improve the reliability of computer systems. Static typestate analysis is a common technique for detecting various types of bugs, but it is often inaccurate or unscalable for large-size OS code, due to imprecision of identifying alias relationships as well as high costs of typestate tracking, path-feasibility validation and inter-procedural analysis. In this article, we present SPATA, a novel summary-based, alias-aware and path-sensitive typestate analysis framework to detect OS bugs. To identify precise alias relationships in the OS code, SPATA performs a path-based alias analysis based on control-flow paths and access paths. With these alias relationships, SPATA reduces the costs of typestate tracking and path-feasibility validation, to accelerate path-sensitive typestate analysis for accurate bug detection. Moreover, SPATA uses an alias-summary-based analysis to accelerate inter-procedural bug detection, without time-consuming alias analysis across functions. We have evaluated SPATA on the Linux kernel and three popular IoT OSes, and it finds 651 real bugs with a false positive rate of 18%. Besides, our alias-summary-based analysis achieves a 6.7x speedup in bug detection, compared to non-summary-based analysis.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4402310141",
    "type": "article"
  },
  {
    "title": "Effective fine-grain synchronization for automatically parallelized programs using optimistic synchronization primitives",
    "doi": "https://doi.org/10.1145/329466.329486",
    "publication_date": "1999-11-01",
    "publication_year": 1999,
    "authors": "Martin Rinard",
    "corresponding_authors": "Martin Rinard",
    "abstract": "This article presents our experience using optimistic synchronization to implement fine-grain atomic operations in the context of a parallelizing compiler for irregular, object-based computations. Our experience shows that the synchronization requirements of these programs differ significantly from those of traditional parallel computations, which use loop nests to access dense matrices using affine access functions. In addition to coarse-grain barrier synchronization, our irregular computations require synchronization primitives that support efficient fine-grain atomic operations. The standard implementation mechanism for atomic operations uses mutual exclusion locks. But the overhead of acquiring and releasing locks can reduce the performance. Locks can also consume significant amounts of memory. Optimistic synchronization primitives such as loud-linked/store conditional are an attractive alternative. They require no additional memory and eliminate the use of heavyweight blocking synchronization constructs. We evaluate the effectiveness of optimistic synchronization by comparing experimental results from two versions of a parallelizing compiler for irregular, object-based computations. One version generates code that uses mutual exclusion locks to make operations execute atomically. The other version generates code that uses mutual exclusion locks to make operations execute atomically. The other version uses optimistic synchronization. We used this compiler to automatically parallelize three irregular, object-based benchmark applications of interest to the scientific and engineering computation community. The presented experimental results indicate that the use of optimistic synchronization in this context can significantly reduce the memory consumption and improve the overall performance.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1999635542",
    "type": "article"
  },
  {
    "title": "Call graph prefetching for database applications",
    "doi": "https://doi.org/10.1145/945506.945509",
    "publication_date": "2003-10-10",
    "publication_year": 2003,
    "authors": "Murali Annavaram; Jignesh M. Patel; Edward S. Davidson",
    "corresponding_authors": "",
    "abstract": "With the continuing technological trend of ever cheaper and larger memory, most data sets in database servers will soon be able to reside in main memory. In this configuration, the performance bottleneck is likely to be the gap between the processing speed of the CPU and the memory access latency. Previous work has shown that database applications have large instruction and data footprints and hence do not use processor caches effectively. In this paper, we propose Call Graph Prefetching (CGP), an N instruction prefetching technique that analyzes the call graph of a database system and prefetches instructions from the function that is deemed likely to be called next. CGP capitalizes on the highly predictable function call sequences that are typical of database systems. CGP can be implemented either in software or in hardware. The software-based CGP ( CGP_S ) uses profile information to build a call graph, and uses the predictable call sequences in the call graph to determine which function to prefetch next. The hardware-based CGP( CGP_H ) uses a hardware table, called the Call Graph History Cache (CGHC), to dynamically store sequences of functions invoked during program execution, and uses that stored history when choosing which functions to prefetch.We evaluate the performance of CGP on sets of Wisconsin and TPC-H queries, as well as on CPU-2000 benchmarks. For most CPU-2000 applications the number of instruction cache (I-cache) misses were very few even without any prefetching, obviating the need for CGP. On the other hand, the database workloads do suffer a significant number of I-cache misses; CGP_S improves their performance by 23% and CGP_H by 26% over a baseline system that has already been highly tuned for efficient I-cache usage by using the OM tool. CGP, with or without OM, reduces the I-cache miss stall time by about 50% relative to O5+OM, taking us about half way from an already highly tuned baseline system toward perfect I-cache performance.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2008158664",
    "type": "article"
  },
  {
    "title": "HFS",
    "doi": "https://doi.org/10.1145/263326.263356",
    "publication_date": "1997-08-01",
    "publication_year": 1997,
    "authors": "Orran Krieger; Michael Stumm",
    "corresponding_authors": "",
    "abstract": "The Hurricane File System (HFS) is designed for (potentially large-scale) shared-memory multiprocessors. Its architecture is based on the principle that, in order to maximize performance for applications with diverse requirements, a file system must support a wide variety of file structures, file system policies, and I/O interfaces. Files in HFS are implemented using simple building blocks composed in potentially complex ways. This approach yields great flexibility, allowing an application to customize the structure and policies of a file to exactly meet its requirements. As an extreme example, HFS allows a file's structure to be optimized for concurrent random-access write-only operations by 10 threads, something no other file system can do. Similarly, the prefetching, locking, and file cache management policies can all be chosen to match an application's access pattern. In contrast, most parallel file systems support a single file structure and a small set of policies. We have implemented HFS as part of the Hurricane operating system running on the Hector shared-memory multiprocessor. We demonstrate that the flexibility of HFS comes with little processing or I/O overhead. We also show that for a number of file access patterns, HFS is able to deliver to the applications the full I/O bandwidth of the disks on our system.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2020802793",
    "type": "article"
  },
  {
    "title": "Chores: enhanced run-time support for shared-memory parallel computing",
    "doi": "https://doi.org/10.1145/151250.151251",
    "publication_date": "1993-02-01",
    "publication_year": 1993,
    "authors": "Derek L. Eager; John Jahorjan",
    "corresponding_authors": "",
    "abstract": "Parallel computing is increasingly important in the solution of large-scale numerical problems. The difficulty of efficiently hand-coding parallelism, and the limitations of parallelizing compilers, have nonetheless restricted its use by scientific programmers. In this paper we propose a new paradigm, chores , for the run-time support of parallel computing on shared-memory multiprocessors. We consider specifically uniform memory access shared-memory environments, although the chore paradigm should also be appropriate for use within the clusters of a large-scale nonuniform memory access machine. We argue that chore systems attain both the high efficiency of compiler approaches for the common case of data parallelism, and the flexibility and performance of user-level thread approaches for functional parallelism. These benefits are achieved within a single, simple conceptual model that almost entirely relieves the programmer and compiler from concerns of granularity, scheduling, and enforcement of synchronization constraints. Measurements of a prototype implementation demonstrate that the chore model can be supported more efficiently than can traditional approaches to either data or functional parallelism alone.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2069765416",
    "type": "article"
  },
  {
    "title": "Verified data transfer protocols with variable flow control",
    "doi": "https://doi.org/10.1145/65000.65003",
    "publication_date": "1989-08-01",
    "publication_year": 1989,
    "authors": "A. Udaya Shankar",
    "corresponding_authors": "A. Udaya Shankar",
    "abstract": "We present and verify a sliding window protocol which uses modulo- N sequence numbers to achieve reliable flow-controlled data transfer between a producer and a consumer connected by unreliable channels. The consumer's data needs are represented by a receive window whose size can vary with time. The producer entity sends segments of data words that lie within the consumer's receive window. The consumer entity sends acknowledgment, selective acknowledgment, and selective reject messages that inform the producer entity of the current receive window size, the data word next expected, and the reception (or lack of reception) of out-of-sequence data segments. Our protocol is, therefore, a proper extension of existing transport and data link protocol standards such as TCP, IS0 TP, HDLC, ADCCP, and so forth. We consider two types of unreliable channels. The first type, referred to as transport channels, can lose, duplicate, and reorder messages to an arbitrary extent, but impose an upper bound on message lifetimes (which can be very large, e.g., days). The second type, referred to as data link channels, can only lose messages. For both types of channels, we obtain the minimum value of N that ensures safe operation without imposing any constraints on the retransmissions of messages or on the data segment sizes. Thus, any retransmission or acknowledgment policy that optimizes the protocol's performance can be used. For transport channels, this value of N is a function of the maximum message transmission rate, the maximum message lifetime, and the maximum receive window size. For data link channels, this value of N is a function only of the maximum receive window size. We verify progress under three different liveness assumptions: retransmissions initiated by both entities, only by the producer entity, and only by the consumer entity. The protocol also satisfies a convenient noninterference safety property between the acknowledgement, selective acknowledgment, and selective reject messages. The protocols are specified as event-driven systems and verified hierarchically in two major stages. First, we verify that correct flow-controlled data transfer results if the sequence numbers in the channels satisfy certain correct interpretation bounds , irrespective of the types of errors that the channels may have. Second, for both transport and data link channels, we verify that the correct interpretation bounds are enforced by the corresponding minimum values of N . For the verification of the transport channel case, we use a system model with continuous measures of time in which real-time constraints can be formally specified and verified.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1980469422",
    "type": "article"
  },
  {
    "title": "Reliable scheduling in a TMR database system",
    "doi": "https://doi.org/10.1145/58564.59294",
    "publication_date": "1989-01-01",
    "publication_year": 1989,
    "authors": "Frank M. Pittelli; Héctor García-Molina",
    "corresponding_authors": "",
    "abstract": "A Triple Modular Redundant (TMR) system achieves high reliability by replicating data and all processing at three independent nodes. When TMR is used for database processing all nonfaulty computers must execute the same sequence of transactions, and this is ensured by a collection of processes known as schedulers . In this paper we study the implementation of efficient schedulers through analysis of various enhancements such as null transactions and message batching. The schedulers have been implemented in an experimental TMR system and the evaluation results are presented here.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2101675843",
    "type": "article"
  },
  {
    "title": "Concurrent reading and writing of clocks",
    "doi": "https://doi.org/10.1145/128733.128736",
    "publication_date": "1990-11-01",
    "publication_year": 1990,
    "authors": "Leslie Lamport",
    "corresponding_authors": "Leslie Lamport",
    "abstract": "As an exercise in synchronization without mutual exclusion, algorithms are developed to implement both a monotonic and a cyclic multiple-word clock that is updated by one process and read by one or more other processes.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2162248954",
    "type": "article"
  },
  {
    "title": "Conversation-based mail",
    "doi": "https://doi.org/10.1145/6513.6515",
    "publication_date": "1986-09-01",
    "publication_year": 1986,
    "authors": "Douglas E. Comer; Larry Peterson",
    "corresponding_authors": "",
    "abstract": "A new message communication paradigm based on conversations that provides an alternative to memo - and conference -based mail is described. A conversation-based message system groups messages into conversations, and orders messages within a conversation according to the context in which they were written. The message context relation leads to an efficient implementation of conversations in a distributed environment and supports a natural ordering of messages when viewed by the user. Experience with a prototype demonstrates the workability of conversation-based mail and suggests that conversations provide a powerful tool for message communication.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2021100126",
    "type": "article"
  },
  {
    "title": "A system for computer music performance",
    "doi": "https://doi.org/10.1145/77648.77652",
    "publication_date": "1990-02-01",
    "publication_year": 1990,
    "authors": "David P. Anderson; Ron Kuivila",
    "corresponding_authors": "",
    "abstract": "A computer music performance system (CMPS) is a computer system connected to input devices (including musical keyboards or other instruments) and to graphic and audio output devices. A human performer generates input events using the input devices. The CMPS responds to these events by computing and performing sequences of output actions whose intended timing is determined algorithmically. Because of the need for accurate timing of output actions, the scheduling requirements of a CMPS differ from those of general-purpose or conventional real-time systems. This paper describes the scheduling facilities of FORMULA, a CMPS used by many musicians. In addition to providing accurate timing of output action sequences, FORMULA provides other basic functions useful in musical applications: (1) per-process virtual time systems with independent relationships to real time; (2) process grouping mechanisms and language-level control structures with time-related semantics, and (3) integrated scheduling of tasks (such as compiling and editing) whose real-time constraints are less stringent than those of output action computations.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2046253050",
    "type": "article"
  },
  {
    "title": "The distributed deadlock detection algorithm",
    "doi": "https://doi.org/10.1145/6513.6516",
    "publication_date": "1986-09-01",
    "publication_year": 1986,
    "authors": "Dushan Z. Badal",
    "corresponding_authors": "Dushan Z. Badal",
    "abstract": "We propose a distributed deadlock detection algorithm for distributed computer systems. We consider two types of resources, depending on whether the remote resource lock granularity and mode can or cannot be determined without access to the remote resource site. We present the algorithm, its performance analysis, and an informal argument about its correctness. The proposed algorithm has a hierarchical design intended to detect the most frequent deadlocks with maximum efficiency.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2044770819",
    "type": "article"
  },
  {
    "title": "Transient behavior of cache memories",
    "doi": "https://doi.org/10.1145/357377.357379",
    "publication_date": "1983-11-01",
    "publication_year": 1983,
    "authors": "William D. Strecker",
    "corresponding_authors": "William D. Strecker",
    "abstract": "article Free AccessTransient behavior of cache memories Author: William D. Strecker Digital Equipment Corporation, 295 Foster Street, Littleton, MA Digital Equipment Corporation, 295 Foster Street, Littleton, MAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 4Nov. 1983 pp 281–293https://doi.org/10.1145/357377.357379Published:01 November 1983Publication History 23citation467DownloadsMetricsTotal Citations23Total Downloads467Last 12 Months15Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2117625644",
    "type": "article"
  },
  {
    "title": "The Design, Implementation, and Evaluation of Cells",
    "doi": "https://doi.org/10.1145/2324876.2324877",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Christoffer Dall; Jeremy Andrus; Alexander Van’t Hof; Oren Laadan; Jason Nieh",
    "corresponding_authors": "",
    "abstract": "Smartphones are increasingly ubiquitous, and many users carry multiple phones to accommodate work, personal, and geographic mobility needs. We present Cells , a virtualization architecture for enabling multiple virtual smartphones to run simultaneously on the same physical cellphone in an isolated, secure manner. Cells introduces a usage model of having one foreground virtual phone and multiple background virtual phones. This model enables a new device namespace mechanism and novel device proxies that integrate with lightweight operating system virtualization to multiplex phone hardware across multiple virtual phones while providing native hardware device performance. Cells virtual phone features include fully accelerated 3D graphics, complete power management features, and full telephony functionality with separately assignable telephone numbers and caller ID support. We have implemented a prototype of Cells that supports multiple Android virtual phones on the same phone. Our performance results demonstrate that Cells imposes only modest runtime and memory overhead, works seamlessly across multiple hardware devices including Google Nexus 1 and Nexus S phones, and transparently runs Android applications at native speed without any modifications.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2077773210",
    "type": "article"
  },
  {
    "title": "Fairness via Source Throttling",
    "doi": "https://doi.org/10.1145/2166879.2166881",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "Eiman Ebrahimi; Chang Joo Lee; Onur Mutlu; Yale N. Patt",
    "corresponding_authors": "",
    "abstract": "Cores in chip-multiprocessors (CMPs) share multiple memory subsystem resources. If resource sharing is unfair, some applications can be delayed significantly while others are unfairly prioritized. Previous research proposed separate fairness mechanisms for each resource. Such resource-based fairness mechanisms implemented independently in each resource can make contradictory decisions, leading to low fairness and performance loss. Therefore, a coordinated mechanism that provides fairness in the entire shared memory system is desirable. This article proposes a new approach that provides fairness in the entire shared memory system, thereby eliminating the need for and complexity of developing fairness mechanisms for each resource. Our technique, Fairness via Source Throttling (FST), estimates unfairness in the entire memory system . If unfairness is above a system-software-set threshold, FST throttles down cores causing unfairness by limiting the number of requests they create and the frequency at which they do. As such, our source-based fairness control ensures fairness decisions are made in tandem in the entire memory system. FST enforces thread priorities/weights, and enables system-software to enforce different fairness objectives in the memory system. Our evaluations show that FST provides the best system fairness and performance compared to three systems with state-of-the-art fairness mechanisms implemented in both shared caches and memory controllers.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2029102646",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue APLOS 2011",
    "doi": "https://doi.org/10.1145/2110356.2110357",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Todd C. Mowry",
    "corresponding_authors": "Todd C. Mowry",
    "abstract": "No abstract available.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2107839037",
    "type": "article"
  },
  {
    "title": "Mobile processors for energy-efficient web search",
    "doi": "https://doi.org/10.1145/2003690.2003693",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Vijay Janapa Reddi; Benjamin C. Lee; Trishul Chilimbi; Kushagra Vaid",
    "corresponding_authors": "",
    "abstract": "As cloud and utility computing spreads, computer architects must ensure continued capability growth for the data centers that comprise the cloud. Given megawatt scale power budgets, increasing data center capability requires increasing computing hardware energy efficiency. To increase the data center's capability for work, the work done per Joule must increase. We pursue this efficiency even as the nature of data center applications evolves. Unlike traditional enterprise workloads, which are typically memory or I/O bound, big data computation and analytics exhibit greater compute intensity. This article examines the efficiency of mobile processors as a means for data center capability. In particular, we compare and contrast the performance and efficiency of the Microsoft Bing search engine executing on the mobile-class Atom processor and the server-class Xeon processor. Bing implements statistical machine learning to dynamically rank pages, producing sophisticated search results but also increasing computational intensity. While mobile processors are energy-efficient, they exact a price for that efficiency. The Atom is 5× more energy-efficient than the Xeon when comparing queries per Joule. However, search queries on Atom encounter higher latencies, different page results, and diminished robustness for complex queries. Despite these challenges, quality-of-service is maintained for most, common queries. Moreover, as different computational phases of the search engine encounter different bottlenecks, we describe implications for future architectural enhancements, application tuning, and system architectures. After optimizing the Atom server platform, a large share of power and cost go toward processor capability. With optimized Atoms, more servers can fit in a given data center power budget. For a data center with 15MW critical load, Atom-based servers increase capability by 3.2× for Bing.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2110422349",
    "type": "article"
  },
  {
    "title": "Energy-Oriented Partial Desktop Virtual Machine Migration",
    "doi": "https://doi.org/10.1145/2699683",
    "publication_date": "2015-03-11",
    "publication_year": 2015,
    "authors": "Nilton Bila; Eric J. Wright; Eyal de Lara; Kaustubh Joshi; H. Andrés Lagar-Cavilla; Eunbyung Park; Ashvin Goel; Matti Hiltunen; Mahadev Satyanarayanan",
    "corresponding_authors": "",
    "abstract": "Modern offices are crowded with personal computers. While studies have shown these to be idle most of the time, they remain powered, consuming up to 60% of their peak power. Hardware-based solutions engendered by PC vendors (e.g., low-power states, Wake-on-LAN) have proved unsuccessful because, in spite of user inactivity, these machines often need to remain network active in support of background applications that maintain network presence. Recent proposals have advocated the use of consolidation of idle desktop Virtual Machines (VMs). However, desktop VMs are often large, requiring gigabytes of memory. Consolidating such VMs creates large network transfers lasting in the order of minutes and utilizes server memory inefficiently. When multiple VMs migrate concurrently, networks become congested, and the resulting migration latencies are prohibitive. We present partial VM migration, an approach that transparently migrates only the working set of an idle VM. It creates a partial replica of the desktop VM on the consolidation server by copying only VM metadata, and it transfers pages to the server on-demand, as the VM accesses them. This approach places desktop PCs in low-power mode when inactive and switches them to running mode when pages are needed by the VM running on the consolidation server. To ensure that desktops save energy, we have developed sleep scheduling and prefetching algorithms, as well as the context-aware selective resume framework, a novel approach to reduce the latency of power mode transition operations in commodity PCs. Jettison, our software prototype of partial VM migration for off-the-shelf PCs, can deliver 44--91% energy savings during idle periods of at least 10 minutes, while providing low migration latencies of about 4 seconds and migrating minimal state that is under an order of magnitude of the VM’s memory footprint.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1967420911",
    "type": "article"
  },
  {
    "title": "Market mechanisms for managing datacenters with heterogeneous microarchitectures",
    "doi": "https://doi.org/10.1145/2541258",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Marisabel Guevara; Benjamin Lubin; Benjamin C. Lee",
    "corresponding_authors": "",
    "abstract": "Specialization of datacenter resources brings performance and energy improvements in response to the growing scale and diversity of cloud applications. Yet heterogeneous hardware adds complexity and volatility to latency-sensitive applications. A resource allocation mechanism that leverages architectural principles can overcome both of these obstacles. We integrate research in heterogeneous architectures with recent advances in multi-agent systems. Embedding architectural insight into proxies that bid on behalf of applications, a market effectively allocates hardware to applications with diverse preferences and valuations. Exploring a space of heterogeneous datacenter configurations, which mix server-class Xeon and mobile-class Atom processors, we find an optimal heterogeneous balance that improves both welfare and energy-efficiency. We further design and evaluate twelve design points along the Xeon-to-Atom spectrum, and find that a mix of three processor architectures achieves a 12× reduction in response time violations relative to equal-power homogeneous systems.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2014163011",
    "type": "article"
  },
  {
    "title": "Optimizing Resource Management for Shared Microservices: A Scalable System Design",
    "doi": "https://doi.org/10.1145/3631607",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Shutian Luo; Chenyu Lin; Kejiang Ye; Guoyao Xu; Liping Zhang; Guodong Yang; Huanle Xu; Chengzhong Xu",
    "corresponding_authors": "",
    "abstract": "A common approach to improving resource utilization in data centers is to adaptively provision resources based on the actual workload. One fundamental challenge of doing this in microservice management frameworks, however, is that different components of a service can exhibit significant differences in their impact on end-to-end performance. To make resource management more challenging, a single microservice can be shared by multiple online services that have diverse workload patterns and SLA requirements. We present an efficient resource management system, namely Erms, for guaranteeing SLAs with high probability in shared microservice environments. Erms profiles microservice latency as a piece-wise linear function of the workload, resource usage, and interference. Based on this profiling, Erms builds resource scaling models to optimally determine latency targets for microservices with complex dependencies. Erms also designs new scheduling policies at shared microservices to further enhance resource efficiency. Experiments across microservice benchmarks as well as trace-driven simulations demonstrate that Erms can reduce SLA violation probability by 5× and more importantly, lead to a reduction in resource usage by 1.6×, compared to state-of-the-art approaches.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4388412703",
    "type": "article"
  },
  {
    "title": "A quantitative comparison of parallel computation models",
    "doi": "https://doi.org/10.1145/290409.290412",
    "publication_date": "1998-08-01",
    "publication_year": 1998,
    "authors": "B. Juurlink; Harry A. G. Wijshoff",
    "corresponding_authors": "",
    "abstract": "In recent years, a large number of parallel computation models have been proposed to replace the PRAM as the parallel computation model presented to the algorithm designer. Although mostly the theoretical justifications for these models are sound, and many algorithmic results where obtained through these models, little experimentation has been conducted to validate the effectiveness of these models for developing cost-effective algorithms and applications on existing hardware platforms. In this article a first attempt is made to perform a detailed experimental account on the preciseness of these models. The achieve this, three models (BSP, E-BSP, and BPRAM) were selected and validated on five parallel platforms (Cray T3E, Thinking Machines CM-5, Intel Paragon, MasPar MP-1, and Parsytec GCel). The work described in this article consists of three parts. First, the predictive capabilities of the models are investigated. Unlike previous experimental work, which mostly demonstrated a close match between the measuredd and predicted execution times, this article shows that there are several situations in which the models do not precisely predict the actual runtime behavior of an algorithm implementation. Second, a comparison between the models is provided in order to determine the model that induces that most efficient algorithms. Lastly, the performance achieved by the model-derived algorithms is compared with the performace attained by machine-specific algorithms in order to examine the effectiveness of deriving fast algorithms through the formalisms of the models.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2022192965",
    "type": "article"
  },
  {
    "title": "An architecture for packet-striping protocols",
    "doi": "https://doi.org/10.1145/329466.329471",
    "publication_date": "1999-11-01",
    "publication_year": 1999,
    "authors": "Adiseshu Hari; George Varghese; Guru Parulkar",
    "corresponding_authors": "",
    "abstract": "Link-striping algorithms are often used to overcome transmission bottlenecks in computer networks. Traditional striping algorithms suffer from two major disadvantages. They provide inadequate load sharing in the presence of variable-length packets, and may result in non-FIFO delivery of data. We describe a new family of link-striping algorithms that solves both problems. Our scheme applies to any layer that can provide multiple FIFO channels. We deal with variable-sized packets by showing how fair-queuing algorithms can be transformed into load-sharing algorithms. Our transformation results in practical load-sharing protocols, and shows a theoretical connection between two seemingly different problems. The same transformation can be applied to obtain load-sharing protocols for links with different capacities. We deal with the FIFO requirement for two separate cases. If a sequence number can be added to each packet, we show how to speed up packet processing by letting the receiver simulate the sender algorithm. If no header can be added, we show how to provide quasi FIFO delivery. Quasi FIFO is FIFO except during occasional periods of loss of synchronization. We argue that quasi FIFO is adequate for most applications. We also describe a simple technique for speedy restoration of synchronization in the event of loss. We develop an architectural framework for transparently embedding our protocol at the network level by striping IP packets across multiple physical interfaces. The resulting stripe protocol has been implemented within the NetBSD kernel. Our measurements and simulations show that the protocol offers scalable throughput even when striping is done over dissimilar links, and that the protocol synchronized quickly after packet loss. Measurements show performance improvements over conventional round-robin striping schemes and striping schemes that do not resequence packets. Some aspects of our solution have been implemented in Cisco's router operating system (IOS 11.3) in the context of Multilink PPP striping.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2048094279",
    "type": "article"
  },
  {
    "title": "A simple and efficient bus management scheme that supports continuous streams",
    "doi": "https://doi.org/10.1145/201045.201048",
    "publication_date": "1995-05-01",
    "publication_year": 1995,
    "authors": "Saied Hosseini-Khayat; Andreas D. Bovopoulos",
    "corresponding_authors": "",
    "abstract": "An efficient bandwidth management and access arbitration scheme for an I/O bus in a multimedia workstation is presented. It assumes that a multimedia workstation consists of a number of processing modules which are interconnected by a packet bus. The scheme is efficient in the sense that it allows the bus to support both continuous media transfers and regular random transactions in such a way that continuous streams can meet their real-time constraints independently of random traffic, and random traffic is not delayed significantly by continuous traffic except when traffic load is very high. Implementation guidelines are provided to show that the scheme is practical. Finally, the performance of this scheme is compared with alternative solutions through simulation.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2062003873",
    "type": "article"
  },
  {
    "title": "Distributed timestamp generation in planar lattice networks",
    "doi": "https://doi.org/10.1145/152864.152865",
    "publication_date": "1993-08-01",
    "publication_year": 1993,
    "authors": "Paul Ammann; Sushil Jajodia",
    "corresponding_authors": "",
    "abstract": "Timestamps are considered for distributed environments in which information flow is restricted to one direction through a planar lattice imposed on a network. For applications in such networks, existing timestamping algorithms require extension and modification. For example, in secure environments, typical timestamps provide a potential signaling channel between incomparable levels. In hierarchical databases, typical timestamps cause peripheral sites to unnecessarily affect the behavior at main sites. Algorithms are presented by which a network node may generate and compare timestamps using timestamp components maintained at dominated nodes in the network. The comparison relation is shown to be acyclic for timestamps produced by the generation algorithm. We discuss ways to safely relax the requirement that the network be a lattice. By example, we show how to modify a simple nonplanar lattice so that the generation algorithm can be applied. Uses of the timestamp generation algorithm in the motivating applications are outlined.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2035891203",
    "type": "article"
  },
  {
    "title": "Network locality at the scale of processes",
    "doi": "https://doi.org/10.1145/128899.128900",
    "publication_date": "1992-05-01",
    "publication_year": 1992,
    "authors": "Jeffrey C. Mogul",
    "corresponding_authors": "Jeffrey C. Mogul",
    "abstract": "Packets on a LAN can be viewed as a series of references to and from the objects they address. The amount of locality in this reference stream may be critical to the efficiency of network implementations, if the locality can be exploited through caching or scheduling mechanisms. Most previous studies have treated network locality with an addressing granularity of networks or individual hosts. This paper describes some experiments tracing locality at a finer grain, looking at references to individual processes, and with fine-grained time resolution. Observations of typical LANs show high per-process locality; that is, packets to a host usually arrive for the process that most recently sent a packet, and often with little intervening delay.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2072031182",
    "type": "article"
  },
  {
    "title": "A study of source-level compiler algorithms for automatic construction of pre-execution code",
    "doi": "https://doi.org/10.1145/1012268.1012270",
    "publication_date": "2004-08-01",
    "publication_year": 2004,
    "authors": "Dong-Keun Kim; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "Pre-execution is a promising latency tolerance technique that uses one or more helper threads running in spare hardware contexts ahead of the main computation to trigger long-latency memory operations early, hence absorbing their latency on behalf of the main computation. This article investigates several source-to-source C compilers for extracting pre-execution thread code automatically, thus relieving the programmer or hardware from this onerous task. We present an aggressive profile-driven compiler that employs three powerful algorithms for code extraction. First, program slicing removes non-critical code for computing cache-missing memory references. Second, prefetch conversion replaces blocking memory references with non-blocking prefetch instructions to minimize pre-execution thread stalls. Finally, speculative loop parallelization generates thread-level parallelism to tolerate the latency of blocking loads. In addition, we present four \"reduced\" compilers that employ less aggressive algorithms to simplify compiler implementation. Our reduced compilers rely on back-end code optimizations rather than program slicing to remove non-critical code, and use compile-time heuristics rather than profiling to approximate runtime information (e.g., cache-miss and loop-trip counts).We prototype our algorithms on the Stanford University Intermediate Format (SUIF) framework and a publicly available program slicer, called Unravel [Lyle and Wallace 1997]. Using our prototype, we undertake a performance evaluation of our compilers on a detailed architectural simulator of an 8-way out-of-order SMT processor with 4 hardware contexts, and 13 applications selected from the SPEC and Olden benchmark suites. Our most aggressive compiler improves the performance of 10 out of 13 applications, reducing execution time by 20.9%. Across all 13 applications, our aggressive compiler achieves a harmonic average speedup of 17.6%. For our reduced compilers, eliminating program slicing and relying on back-end optimizations degrades performance minimally, suggesting that effective pre-execution compilers can be built without program slicing. Furthermore, without cache-miss profiles, we still achieve good speedup, 15.5%, but without loop-trip count profiles, we achieve a speedup of only 7.7%. Finally, our results show compiler-based pre-execution can benefit multiprogrammed workloads. Simultaneously executing applications achieve higher throughput with pre-execution compared to no pre-execution. Due to contention for hardware contexts, however, time-slicing outperforms simultaneous execution in some cases where individual applications make heavy use of pre-execution threads.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1964278538",
    "type": "article"
  },
  {
    "title": "Disk file allocation based on the buddy system",
    "doi": "https://doi.org/10.1145/29868.29871",
    "publication_date": "1987-10-01",
    "publication_year": 1987,
    "authors": "Philip D. L. Koch",
    "corresponding_authors": "Philip D. L. Koch",
    "abstract": "The buddy system is known for its speed and simplicity. However, high internal and external fragmentation have made it unattractive for use in operating system file layout. A variant of the binary buddy system that reduces fragmentation is described. Files are allocated on up to t extents, and inoptimally allocated files are periodically reallocated. The Dartmouth Time-Sharing System (DTSS) uses this method. Several installations of DTSS, representing different classes of workload, are studied to measure the method's performance. Internal fragmentation varies from 2-6 percent, and external fragmentation varies from 0-10 percent for expected request sizes. Less than 0.1 percent of the CPU is spent executing the algorithm. In addition, most files are stored contiguously on disk. The mean number of extents per file is less than 1.5, and the upper bound is t . Compared to the tile layout method used by UNIX, the buddy system results in more efficient access but less efficient utilization of disk space. As disks become larger and less expensive per byte, strategies that achieve efficient I/O throughput at the expense of some storage loss become increasingly attractive.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2117144693",
    "type": "article"
  },
  {
    "title": "High-bandwidth data dissemination for large-scale distributed systems",
    "doi": "https://doi.org/10.1145/1328671.1328674",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Dejan Kostić; Alex C. Snoeren; Amin Vahdat; Ryan Braud; Charles Killian; James W. Anderson; Jeannie Albrecht; Adolfo Rodriguez; Erik Vandekieft",
    "corresponding_authors": "",
    "abstract": "This article focuses on the multireceiver data dissemination problem. Initially, IP multicast formed the basis for efficiently supporting such distribution. More recently, overlay networks have emerged to support point-to-multipoint communication. Both techniques focus on constructing trees rooted at the source to distribute content among all interested receivers. We argue, however, that trees have two fundamental limitations for data dissemination. First, since all data comes from a single parent, participants must often continuously probe in search of a parent with an acceptable level of bandwidth. Second, due to packet losses and failures, available bandwidth is monotonically decreasing down the tree. To address these limitations, we present Bullet, a data dissemination mesh that takes advantage of the computational and storage capabilities of end hosts to create a distribution structure where a node receives data in parallel from multiple peers. For the mesh to deliver improved bandwidth and reliability, we need to solve several key problems: (i) disseminating disjoint data over the mesh, (ii) locating missing content, (iii) finding who to peer with (peering strategy), (iv) retrieving data at the right rate from all peers (flow control), and (v) recovering from failures and adapting to dynamically changing network conditions. Additionally, the system should be self-adjusting and should have few user-adjustable parameter settings. We describe our approach to addressing all of these problems in a working, deployed system across the Internet. Bullet outperforms state-of-the-art systems, including BitTorrent, by 25-70% and exhibits strong performance and reliability in a range of deployment settings. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2140337761",
    "type": "article"
  },
  {
    "title": "Practical and low-overhead masking of failures of TCP-based servers",
    "doi": "https://doi.org/10.1145/1534909.1534911",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "Dmitrii Zagorodnov; Keith Marzullo; Lorenzo Alvisi; Thomas Bressoud",
    "corresponding_authors": "",
    "abstract": "This article describes an architecture that allows a replicated service to survive crashes without breaking its TCP connections. Our approach does not require modifications to the TCP protocol, to the operating system on the server, or to any of the software running on the clients. Furthermore, it runs on commodity hardware. We compare two implementations of this architecture (one based on primary/backup replication and another based on message logging) focusing on scalability, failover time, and application transparency. We evaluate three types of services: a file server, a Web server, and a multimedia streaming server. Our experiments suggest that the approach incurs low overhead on throughput, scales well as the number of clients increases, and allows recovery of the service in near-optimal time.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2047750400",
    "type": "article"
  },
  {
    "title": "Distributed hash sketches",
    "doi": "https://doi.org/10.1145/1482619.1482621",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum",
    "corresponding_authors": "",
    "abstract": "Counting items in a distributed system, and estimating the cardinality of multisets in particular, is important for a large variety of applications and a fundamental building block for emerging Internet-scale information systems. Examples of such applications range from optimizing query access plans in peer-to-peer data sharing, to computing the significance (rank/score) of data items in distributed information retrieval. The general formal problem addressed in this article is computing the network-wide distinct number of items with some property (e.g., distinct files with file name containing “spiderman”) where each node in the network holds an arbitrary subset, possibly overlapping the subsets of other nodes. The key requirements that a viable approach must satisfy are: (1) scalability towards very large network size, (2) efficiency regarding messaging overhead, (3) load balance of storage and access, (4) accuracy of the cardinality estimation, and (5) simplicity and easy integration in applications. This article contributes the DHS (Distributed Hash Sketches) method for this problem setting: a distributed, scalable, efficient, and accurate multiset cardinality estimator. DHS is based on hash sketches for probabilistic counting, but distributes the bits of each counter across network nodes in a judicious manner based on principles of Distributed Hash Tables, paying careful attention to fast access and aggregation as well as update costs. The article discusses various design choices, exhibiting tunable trade-offs between estimation accuracy, hop-count efficiency, and load distribution fairness. We further contribute a full-fledged, publicly available, open-source implementation of all our methods, and a comprehensive experimental evaluation for various settings.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2134420161",
    "type": "article"
  },
  {
    "title": "The SMesh wireless mesh network",
    "doi": "https://doi.org/10.1145/1841313.1841314",
    "publication_date": "2008-09-30",
    "publication_year": 2008,
    "authors": "Yair Amir; Claudiu Danilov; Raluca Musuăloiu-Elefteri; Nilo Rivera",
    "corresponding_authors": "",
    "abstract": "Wireless mesh networks extend the connectivity range of mobile devices by using multiple access points, some of them connected to the Internet, to create a mesh topology and forward packets over multiple wireless hops. However, the quality of service provided by the mesh is impaired by the delays and disconnections caused by handoffs, as clients move within the area covered by multiple access points. We present the architecture and protocols of SMesh, the first transparent wireless mesh system that offers seamless, fast handoff, supporting real-time applications such as interactive VoIP. The handoff and routing logic is done solely by the access points, and therefore connectivity is attainable by any 802.11 device. In SMesh, the entire mesh network is seen by the mobile clients as a single, omnipresent access point, giving the mobile clients the illusion that they are stationary. We use multicast for access points coordination and, during handoff transitions, we use more than one access point to handle the moving client. SMesh provides a hybrid routing protocol that optimizes routes over wireless and wired links in a multihomed environment. Experimental results on a fully deployed mesh network demonstrate the effectiveness of the SMesh architecture and its intra-domain and inter-domain handoff protocols.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2163936416",
    "type": "article"
  },
  {
    "title": "Optimizing General-Purpose CPUs for Energy-Efficient Mobile Web Computing",
    "doi": "https://doi.org/10.1145/3041024",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Yuhao Zhu; Vijay Janapa Reddi",
    "corresponding_authors": "",
    "abstract": "Mobile applications are increasingly being built using web technologies as a common substrate to achieve portability and to improve developer productivity. Unfortunately, web applications often incur large performance overhead, directly affecting the user quality-of-service (QoS) experience. Traditional techniques in improving mobile processor performance have mostly been adopting desktop-like design techniques such as increasing single-core microarchitecture complexity and aggressively integrating more cores. However, such a desktop-oriented strategy is likely coming to an end due to the stringent energy and thermal constraints that mobile devices impose. Therefore, we must pivot away from traditional mobile processor design techniques in order to provide sustainable performance improvement while maintaining energy efficiency. In this article, we propose to combine hardware customization and specialization techniques to improve the performance and energy efficiency of mobile web applications. We first perform design-space exploration (DSE) and identify opportunities in customizing existing general-purpose mobile processors, that is, tuning microarchitecture parameters. The thorough DSE also lets us discover sources of energy inefficiency in customized general-purpose architectures. To mitigate these inefficiencies, we propose, synthesize, and evaluate two new domain-specific specializations, called the Style Resolution Unit and the Browser Engine Cache. Our optimizations boost performance and energy efficiency at the same time while maintaining general-purpose programmability. As emerging mobile workloads increasingly rely more on web technologies, the type of optimizations we propose will become important in the future and are likely to have a long-lasting and widespread impact.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2602922424",
    "type": "article"
  },
  {
    "title": "SILK+ Preventing Latency Spikes in Log-Structured Merge Key-Value Stores Running Heterogeneous Workloads",
    "doi": "https://doi.org/10.1145/3380905",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Oana Balmau; Florin Dinu; Willy Zwaenepoel; Karan Gupta; Ravishankar Chandhiramoorthi; Diego Didona",
    "corresponding_authors": "",
    "abstract": "Log-Structured Merge Key-Value stores (LSM KVs) are designed to offer good write performance, by capturing client writes in memory, and only later flushing them to storage. Writes are later compacted into a tree-like data structure on disk to improve read performance and to reduce storage space use. It has been widely documented that compactions severely hamper throughput. Various optimizations have successfully dealt with this problem. These techniques include, among others, rate-limiting flushes and compactions, selecting among compactions for maximum effect, and limiting compactions to the highest level by so-called fragmented LSMs. In this article, we focus on latencies rather than throughput. We first document the fact that LSM KVs exhibit high tail latencies. The techniques that have been proposed for optimizing throughput do not address this issue, and, in fact, in some cases, exacerbate it. The root cause of these high tail latencies is interference between client writes, flushes, and compactions. Another major cause for tail latency is the heterogeneous nature of the workloads in terms of operation mix and item sizes whereby a few more computationally heavy requests slow down the vast majority of smaller requests. We introduce the notion of an Input/Output (I/O) bandwidth scheduler for an LSM-based KV store to reduce tail latency caused by interference of flushing and compactions and by workload heterogeneity. We explore three techniques as part of this I/O scheduler: (1) opportunistically allocating more bandwidth to internal operations during periods of low load, (2) prioritizing flushes and compactions at the lower levels of the tree, and (3) separating client requests by size and by data access path. SILK+ is a new open-source LSM KV that incorporates this notion of an I/O scheduler.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3033289476",
    "type": "article"
  },
  {
    "title": "Application-Tailored I/O with Streamline",
    "doi": "https://doi.org/10.1145/1963559.1963562",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Willem de Bruijn; Herbert Bos; Henri E. Bal",
    "corresponding_authors": "",
    "abstract": "Streamline is a stream-based OS communication subsystem that spans from peripheral hardware to userspace processes. It improves performance of I/O-bound applications (such as webservers and streaming media applications) by constructing tailor-made I/O paths through the operating system for each application at runtime. Path optimization removes unnecessary copying, context switching and cache replacement and integrates specialized hardware. Streamline automates optimization and only presents users a clear, concise job control language based on Unix pipelines. For backward compatibility Streamline also presents well known files, pipes and sockets abstractions. Observed throughput improvement over Linux 2.6.24 for networking applications is up to 30-fold, but two-fold is more typical.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2041755486",
    "type": "article"
  },
  {
    "title": "Fireflies",
    "doi": "https://doi.org/10.1145/2701418",
    "publication_date": "2015-05-22",
    "publication_year": 2015,
    "authors": "Håvard D. Johansen; Robbert van Renesse; Ýmir Vigfússon; Dag Johansen",
    "corresponding_authors": "",
    "abstract": "An attacker who controls a computer in an overlay network can effectively control the entire overlay network if the mechanism managing membership information can successfully be targeted. This article describes Fireflies, an overlay network protocol that fights such attacks by organizing members in a verifiable pseudorandom structure so that an intruder cannot incorrectly modify the membership views of correct members. Fireflies provides each member with a view of the entire membership, and supports networks with moderate total churn. We evaluate Fireflies using both simulations and PlanetLab to show that Fireflies is a practical approach for secure membership maintenance in such networks.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2245949953",
    "type": "article"
  },
  {
    "title": "Software Prefetching for Indirect Memory Accesses",
    "doi": "https://doi.org/10.1145/3319393",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Sam Ainsworth; Timothy M. Jones",
    "corresponding_authors": "",
    "abstract": "Many modern data processing and HPC workloads are heavily memory-latency bound. A tempting proposition to solve this is software prefetching, where special non-blocking loads are used to bring data into the cache hierarchy just before being required. However, these are difficult to insert to effectively improve performance, and techniques for automatic insertion are currently limited. This article develops a novel compiler pass to automatically generate software prefetches for indirect memory accesses, a special class of irregular memory accesses often seen in high-performance workloads. We evaluate this across a wide set of systems, all of which gain benefit from the technique. We then evaluate the extent to which good prefetch instructions are architecture dependent and the class of programs that are particularly amenable. Across a set of memory-bound benchmarks, our automated pass achieves average speedups of 1.3× for an Intel Haswell processor, 1.1× for both an ARM Cortex-A57 and Qualcomm Kryo, 1.2× for a Cortex-72 and an Intel Kaby Lake, and 1.35× for an Intel Xeon Phi Knight’s Landing, each of which is an out-of-order core, and performance improvements of 2.1× and 2.7× for the in-order ARM Cortex-A53 and first generation Intel Xeon Phi.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2941917372",
    "type": "article"
  },
  {
    "title": "H-Container: Enabling Heterogeneous-ISA Container Migration in Edge Computing",
    "doi": "https://doi.org/10.1145/3524452",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Tong Xing; Antonio Barbalace; Pierre Olivier; Mohamed Lamine Karaoui; Wei Wang; Binoy Ravindran",
    "corresponding_authors": "",
    "abstract": "Edge computing is a recent computing paradigm that brings cloud services closer to the client. Among other features, edge computing offers extremely low client/server latencies. To consistently provide such low latencies, services should run on edge nodes that are physically as close as possible to their clients. Thus, when the physical location of a client changes, a service should migrate between edge nodes to maintain proximity. Differently from cloud nodes, edge nodes integrate CPUs of different Instruction Set Architectures (ISAs), hence a program natively compiled for a given ISA cannot migrate to a server equipped with a CPU of a different ISA. This hinders migration to the closest node. We introduce H-Container, a system that migrates natively compiled containerized applications across compute nodes featuring CPUs of different ISAs. H-Container advances over existing heterogeneous-ISA migration systems by being (a) highly compatible – no user’s source-code nor compiler toolchain modifications are needed; (b) easily deployable – fully implemented in user space, thus without any OS or hypervisor dependency, and (c) largely Linux-compliant – it can migrate most Linux software, including server applications and dynamically linked binaries. H-Container targets Linux and its already-compiled executables, adopts LLVM, extends CRIU, and integrates with Docker. Experiments demonstrate that H-Container adds no overheads during program execution, while 10–100 ms are added during migration. Furthermore, we show the benefits of H-Container in real-world scenarios, demonstrating, for example, up to 94% increase in Redis throughput when client/server proximity is maintained through heterogeneous container migration.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4220980201",
    "type": "article"
  },
  {
    "title": "Hardware-Software Collaborative Tiered-Memory Management Framework for Virtualization",
    "doi": "https://doi.org/10.1145/3639564",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Sai Sha; Chuandong Li; Xiaolin Wang; Zhenlin Wang; Yingwei Luo",
    "corresponding_authors": "",
    "abstract": "The tiered-memory system can effectively expand the memory capacity for virtual machines (VMs). However, virtualization introduces new challenges specifically in enforcing performance isolation, minimizing context switching, and providing resource overcommit. None of the state-of-the-art designs consider virtualization and address these challenges; we observe that a VM with tiered memory incurs up to a 2× slowdown compared to a DRAM-only VM. We propose vTMM , a hardware-software collaborative tiered-memory management framework for virtualization. A key insight in vTMM is to leverage the unique system features in virtualization to meet the above challenges. vTMM automatically determines page hotness and migrates pages between fast and slow memory to achieve better performance. Specially, vTMM optimizes page tracking and migration based on page-modification logging (PML), a hardware-assisted virtualization mechanism, and adaptively distinguishes hot/cold pages through the page “temperature” sorting. vTMM also dynamically adjusts fast memory among multi-VMs on demand by using a memory pool. Further, vTMM tracks huge pages at regular-page granularity in hardware and splits/merges pages in software, realizing hybrid-grained page management and optimization. We implement and evaluate vTMM with single-grained page management on an Intel processor, and the hybrid-grained page management on a Sunway processor with hardware mode supporting hardware/software co-designs. Experiments show that vTMM outperforms existing tiered-memory management designs in virtualization.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390880278",
    "type": "article"
  },
  {
    "title": "Recovery in the Calypso file system",
    "doi": "https://doi.org/10.1145/233557.233560",
    "publication_date": "1996-08-01",
    "publication_year": 1996,
    "authors": "Murthy Devarakonda; B. Kish; Ajay Mohindra",
    "corresponding_authors": "",
    "abstract": "This article presents the deign and implementation of the recovery scheme in Calypso. Calypso is a cluster-optimized, distributed file system for UNIX clusters. As in Sprite and AFS, Calypso servers are stateful and scale well to a large number of clients. The recovery scheme in Calypso is nondisruptive, meaning that open files remain open, client modified data are saved, and in-flight operations are properly handled across server recover. The scheme uses distributed state amount the clients to reconstruct the server state on a backup node if disks are multiported or on the rebooted server node. It guarantees data consistency during recovery and provides congestion control. Measurements show that the state reconstruction can be quite fast: for example, in a 32-node cluster, when an average node contains state for about 420 files, the reconstruction time is about 3.3 seconds. However, the time to update a file system after a failure can be a major factor in the overall recovery time, even when using journaling techniques.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2074184402",
    "type": "article"
  },
  {
    "title": "Informing memory operations",
    "doi": "https://doi.org/10.1145/279227.279230",
    "publication_date": "1998-05-01",
    "publication_year": 1998,
    "authors": "Mark Horowitz; Margaret Martonosi; Todd C. Mowry; Michael D. Smith",
    "corresponding_authors": "",
    "abstract": "Memory latency is an important bottleneck in system performance that cannot be adequately solved by hardware alone. Several promising software techniques have been shown to address this problem successfully in specific situations. However, the generality of these software approaches has been limited because current architecturtes do not provide a fine-grained, low-overhead mechanism for observing and reacting to memory behavior directly. To fill this need, this article proposes a new class of memory operations called informing memory operations , which essentially consist of a memory operatin combined (either implicitly or explicitly) with a conditional branch-and-ink operation that is taken only if the reference suffers a cache miss. This article describes two different implementations of informing memory operations. One is based on a cache-outcome condition code, and the other is based on low-overhead traps. We find that modern in-order-issue and out-of-order-issue superscalar processors already contain the bulk of the necessary hardware support. We describe how a number of software-based memory optimizations can exploit informing memory operations to enhance performance, and we look at cache coherence with fine-grained access control as a case study. Our performance results demonstrate that the runtime overhead of invoking the informing mechanism on the Alpha 21164 and MIPS R10000 processors is generally small enough to provide considerable flexibility to hardware and software designers, and that the cache coherence application has improved performance compared to other current solutions. We believe that the inclusion of informing memory operations in future processors may spur even more innovative performance optimizations.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2144571563",
    "type": "article"
  },
  {
    "title": "Measurement and evaluation of the MIPS architecture and processor",
    "doi": "https://doi.org/10.1145/45059.45060",
    "publication_date": "1988-08-01",
    "publication_year": 1988,
    "authors": "Thomas R. Gross; John L. Hennessy; Steven A. Przybylski; Christopher Rowen",
    "corresponding_authors": "",
    "abstract": "MIPS is a 32-bit processor architecture that has been implemented as an nMOS VLSI chip. The instruction set architecture is RISC-based. Close coupling with compilers and efficient use of the instruction set by compiled programs were goals of the architecture. The MIPS architecture requires that the software implement some constraints in the design that are normally considered part of the hardware implementation. This paper presents experimental results on the effectiveness of this processor as a program host. Using sets of large and small benchmarks, the instruction and operand usage patterns are examined both for optimized and unoptimized code. Several of the architectural and organizational innovations in MIPS, including software pipeline scheduling, multiple-operation instructions, and word-based addressing, are examined in light of this data.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1999927257",
    "type": "article"
  },
  {
    "title": "Run-time adaptation in river",
    "doi": "https://doi.org/10.1145/592637.592639",
    "publication_date": "2003-01-10",
    "publication_year": 2003,
    "authors": "Remzi H. Arpaci-Dusseau",
    "corresponding_authors": "Remzi H. Arpaci-Dusseau",
    "abstract": "We present the design, implementation, and evaluation of run-time adaptation within the River dataflow programming environment. The goal of the River system is to provide adaptive mechanisms that allow database query-processing applications to cope with performance variations that are common in cluster platforms. We describe the system and its basic mechanisms, and carefully evaluate those mechanisms and their effectiveness. In our analysis, we answer four previously unanswered and important questions. Are the core run-time adaptive mechanisms effective, especially as compared to the ideal? What are the keys to making them work well? Can applications easily use these primitives? And finally, are there situations in which run-time adaptation is not sufficient? In performing our study, we utilize a three-pronged approach, comparing results from idealized models of system behavior, targeted simulations, and a prototype implementation. As well as providing insight on the positives and negatives of run-time adaptation both specifically in River and in a broader context, we also comment on the interplay of modeling, simulation, and implementation in system design.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2130622824",
    "type": "article"
  },
  {
    "title": "Reliable broadcast algorithms for HARTS",
    "doi": "https://doi.org/10.1145/118544.119233",
    "publication_date": "1991-11-01",
    "publication_year": 1991,
    "authors": "D.D. Kandlur; Kang G. Shin",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Reliable broadcast algorithms for HARTS Authors: Dilip D. Kandlur Univ. of Michigan, Ann Arbor Univ. of Michigan, Ann ArborView Profile , Kang G. Shin Univ. of Michigan, Ann Arbor Univ. of Michigan, Ann ArborView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 9Issue 4pp 374–398https://doi.org/10.1145/118544.119233Published:01 November 1991Publication History 21citation378DownloadsMetricsTotal Citations21Total Downloads378Last 12 Months15Last 6 weeks6 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2148669390",
    "type": "article"
  },
  {
    "title": "High-level language debugging for concurrent programs",
    "doi": "https://doi.org/10.1145/128733.128737",
    "publication_date": "1990-11-01",
    "publication_year": 1990,
    "authors": "G. Goldszmidt; Shaula Yemini; Shmuel Katz",
    "corresponding_authors": "",
    "abstract": "An integrated system design for debugging distributed programs written in concurrent high-level languages is described. A variety of user-interface, monitoring, and analysis tools integrated around a uniform process model are provided. Because the tools are language-based, the user does not have to deal with low-level implementation details of distribution and concurrency, and instead can focus on the logic of the program in terms of language-level objects and constructs. The tools provide facilities for experimentation with process scheduling, environment simulation, and nondeterministic selections. Presentation and analysis of the program's behavior are supported by history replay, state queries, and assertion checking. Assertions are formulated in linear time temporal logic, which is a logic particularly well suited to specify the behavior of distributed programs. The tools are separated into two sets. The language-specific tools are those that directly interact with programs for monitoring of and on-line experimenting with distributed programs. The language-independent tools are those that support off-line presentation and analysis of the monitored information. This separation makes the system applicable to a wide range of programming languages. In addition, the separation of interactive experimentation from off-line analysis provides for efficient exploitation of both user time and machine resources. The implementation of a debugging facility for OCCAM is described.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2024385413",
    "type": "article"
  },
  {
    "title": "“Topologies”—distributed objects on multicomputers",
    "doi": "https://doi.org/10.1145/78952.78954",
    "publication_date": "1990-05-01",
    "publication_year": 1990,
    "authors": "Karsten Schwan; Win Bo",
    "corresponding_authors": "",
    "abstract": "Application programs written for large-scale multicomputers with interconnection structures known to the programmer (e.g., hypercubes or meshes) use complex communication structures for connecting the applications' parallel tasks. Such structures implement a wide variety of functions, including the exchange of data or control information relevant to the task computations and/or the communications required for task synchronization, message forwarding/filtering under program control, and so on. Topology is a programming and operating system construct that allows programmers to describe and efficiently implement such functionality as distributed objects with well-defined operational interfaces. As with abstract data types, topologies may be reused by any application desiring their functionality. However, in contrast to other research in parallel or distributed object-based operating systems, internally, a topology may be an entirely distributed implementation of the object's functionality, consisting of a communication graph and type-specific computations, which are triggered by messages traversing the graph. Sample computations may perform additions or minimizations of the values traversing a topology, thereby computing a global sum or minimum. Similarly, computations may concatenate or filter messages in order to implement program monitoring, I/O, file storage, or virtual terminal services. Topologies are implemented as an extension of the Intel iPSC hypercube's operating system kernel and have been used with several, large-scale parallel application programs.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1976521264",
    "type": "article"
  },
  {
    "title": "A tree-structured mean value analysis algorithm",
    "doi": "https://doi.org/10.1145/214419.214423",
    "publication_date": "1986-05-01",
    "publication_year": 1986,
    "authors": "Ken Hoyme; Steven C. Bruell; P. V. Afshari; Richard Y. Kain",
    "corresponding_authors": "",
    "abstract": "In a recent paper, Lam and Lien described an algorithm called tree-convolution that can reduce the space and computation time required for evaluating sparse multiclass, product-form queueing networks. In this paper, we develop an exact algorithm based on mean value analysis (MVA) that is the counterpart of the tree-convolution algorithm. The order of reduction in storage and computation achieved by our new Tree-MVA algorithm compared to the standard MVA algorithm is the same order of reduction obtained by the tree-convolution algorithm over that of the standard convolution algorithm. Our Tree-MVA algorithm preserves the inherent simplicity of MVA based algorithms.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2079830529",
    "type": "article"
  },
  {
    "title": "The aggregate server method for analyzing serialization delays in computer systems",
    "doi": "https://doi.org/10.1145/357360.357364",
    "publication_date": "1983-05-01",
    "publication_year": 1983,
    "authors": "Subhash C. Agrawal; Jeffrey P. Buzen",
    "corresponding_authors": "",
    "abstract": "article Free AccessThe aggregate server method for analyzing serialization delays in computer systems Authors: Subhash C. Agrawal Dept. of Computer Sciences, Purdue University, West Lafayette, IN Dept. of Computer Sciences, Purdue University, West Lafayette, INView Profile , Jeffrey P. Buzen BGS Systems, Inc., One University Office Park, Waltham, MA BGS Systems, Inc., One University Office Park, Waltham, MAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 2May 1983 pp 116–143https://doi.org/10.1145/357360.357364Published:01 May 1983Publication History 19citation416DownloadsMetricsTotal Citations19Total Downloads416Last 12 Months25Last 6 weeks9 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1978349657",
    "type": "article"
  },
  {
    "title": "Scheduling real-time garbage collection on uniprocessors",
    "doi": "https://doi.org/10.1145/2003690.2003692",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Tomáš Kalibera; Filip Pizlo; Antony L. Hosking; Jan Vítek",
    "corresponding_authors": "",
    "abstract": "Managed languages such as Java and C# are increasingly being considered for hard real-time applications because of their productivity and software engineering advantages. Automatic memory management, or garbage collection, is a key enabler for robust, reusable libraries, yet remains a challenge for analysis and implementation of real-time execution environments. This article comprehensively compares leading approaches to hard real-time garbage collection. There are many design decisions involved in selecting a real-time garbage collection algorithm. For time-based garbage collectors on uniprocessors one must choose whether to use periodic , slack-based or hybrid scheduling. A significant impediment to valid experimental comparison of such choices is that commercial implementations use completely different proprietary infrastructures. We present Minuteman, a framework for experimenting with real-time collection algorithms in the context of a high-performance execution environment for real-time Java. We provide the first comparison of the approaches, both experimentally using realistic workloads, and analytically in terms of schedulability.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1967455109",
    "type": "article"
  },
  {
    "title": "Full-Stack Architecting to Achieve a Billion-Requests-Per-Second Throughput on a Single Key-Value Store Server Platform",
    "doi": "https://doi.org/10.1145/2897393",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Sheng Li; Hyeontaek Lim; Victor W. Lee; Jung Ho Ahn; Anuj Kalia; Michael Kaminsky; David G. Andersen; O Seongil; Sukhan Lee; Pradeep Dubey",
    "corresponding_authors": "",
    "abstract": "Distributed in-memory key-value stores (KVSs), such as memcached, have become a critical data serving layer in modern Internet-oriented data center infrastructure. Their performance and efficiency directly affect the QoS of web services and the efficiency of data centers. Traditionally, these systems have had significant overheads from inefficient network processing, OS kernel involvement, and concurrency control. Two recent research thrusts have focused on improving key-value performance. Hardware-centric research has started to explore specialized platforms including FPGAs for KVSs; results demonstrated an order of magnitude increase in throughput and energy efficiency over stock memcached. Software-centric research revisited the KVS application to address fundamental software bottlenecks and to exploit the full potential of modern commodity hardware; these efforts also showed orders of magnitude improvement over stock memcached. We aim at architecting high-performance and efficient KVS platforms, and start with a rigorous architectural characterization across system stacks over a collection of representative KVS implementations. Our detailed full-system characterization not only identifies the critical hardware/software ingredients for high-performance KVS systems but also leads to guided optimizations atop a recent design to achieve a record-setting throughput of 120 million requests per second (MRPS) (167MRPS with client-side batching) on a single commodity server. Our system delivers the best performance and energy efficiency (RPS/watt) demonstrated to date over existing KVSs including the best-published FPGA-based and GPU-based claims. We craft a set of design principles for future platform architectures, and via detailed simulations demonstrate the capability of achieving a billion RPS with a single server constructed following our principles.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2322351367",
    "type": "article"
  },
  {
    "title": "Computational Sprinting",
    "doi": "https://doi.org/10.1145/3014428",
    "publication_date": "2017-01-09",
    "publication_year": 2017,
    "authors": "Seyed Majid Zahedi; Songchun Fan; Matthew Faw; Elijah Cole; Benjamin C. Lee",
    "corresponding_authors": "",
    "abstract": "Computational sprinting is a class of mechanisms that boost performance but dissipate additional power. We describe a sprinting architecture in which many, independent chip multiprocessors share a power supply and sprints are constrained by the chips’ thermal limits and the rack’s power limits. Moreover, we present the computational sprinting game, a multi-agent perspective on managing sprints. Strategic agents decide whether to sprint based on application phases and system conditions. The game produces an equilibrium that improves task throughput for data analytics workloads by 4--6× over prior greedy heuristics and performs within 90% of an upper bound on throughput from a globally optimized policy.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2571250354",
    "type": "article"
  },
  {
    "title": "Efficient Control and Communication Paradigms for Coarse-Grained Spatial Architectures",
    "doi": "https://doi.org/10.1145/2754930",
    "publication_date": "2015-09-11",
    "publication_year": 2015,
    "authors": "Michael Pellauer; Angshuman Parashar; Michael Adler; Bushra Ahsan; R. Allmon; Neal Crago; Kermin Fleming; Mohit Gambhir; Aamer Jaleel; Tushar Krishna; Daniel Lustig; Stephen Maresh; В. В. Павлов; Rachid Rayess; Antonia Zhai; Joel Emer",
    "corresponding_authors": "",
    "abstract": "There has been recent interest in exploring the acceleration of nonvectorizable workloads with spatially programmed architectures that are designed to efficiently exploit pipeline parallelism. Such an architecture faces two main problems: how to efficiently control each processing element (PE) in the system, and how to facilitate inter-PE communication without the overheads of traditional shared-memory coherent memory. In this article, we explore solving these problems using triggered instructions and latency-insensitive channels. Triggered instructions completely eliminate the program counter (PC) and allow programs to transition concisely between states without explicit branch instructions. Latency-insensitive channels allow efficient communication of inter-PE control information while simultaneously enabling flexible code placement and improving tolerance for variable events such as cache accesses. Together, these approaches provide a unified mechanism to avoid overserialized execution, essentially achieving the effect of techniques such as dynamic instruction reordering and multithreading. Our analysis shows that a spatial accelerator using triggered instructions and latency-insensitive channels can achieve 8 × greater area-normalized performance than a traditional general-purpose processor. Further analysis shows that triggered control reduces the number of static and dynamic instructions in the critical paths by 62% and 64%, respectively, over a PC-style baseline, increasing the performance of the spatial programming approach by 2.0 ×.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1988150112",
    "type": "article"
  },
  {
    "title": "Faults in Linux 2.6",
    "doi": "https://doi.org/10.1145/2619090",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Nicolas Palix; Gaël Thomas; Suman Saha; Christophe Calvès; Gilles Muller; Julia L. Lawall",
    "corresponding_authors": "",
    "abstract": "In August 2011, Linux entered its third decade. Ten years before, Chou et al. published a study of faults found by applying a static analyzer to Linux versions 1.0 through 2.4.1. A major result of their work was that the drivers directory contained up to 7 times more of certain kinds of faults than other directories. This result inspired numerous efforts on improving the reliability of driver code. Today, Linux is used in a wider range of environments, provides a wider range of services, and has adopted a new development and release model. What has been the impact of these changes on code quality? To answer this question, we have transported Chou et al.’s experiments to all versions of Linux 2.6; released between 2003 and 2011. We find that Linux has more than doubled in size during this period, but the number of faults per line of code has been decreasing. Moreover, the fault rate of drivers is now below that of other directories, such as arch. These results can guide further development and research efforts for the decade to come. To allow updating these results as Linux evolves, we define our experimental protocol and make our checkers available.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3122133953",
    "type": "article"
  },
  {
    "title": "Customized information extraction as a basis for resource discovery",
    "doi": "https://doi.org/10.1145/227695.227697",
    "publication_date": "1996-05-01",
    "publication_year": 1996,
    "authors": "Darren Hardy; Michael F. Schwartz",
    "corresponding_authors": "",
    "abstract": "Indexing file contents is a powerful means of helping users locate documents, software, and other types of data among large repositories. In environments that contain many different types of data, content indexing requires type-specific processing to extract information effectively. We present a model for type-specific, user-customizable information extraction, and a system implementation called Essence . This software structure allows users to associate specialized extraction methods with ordinary files, providing the illusion of an object-oriented file system that encapsulates indexing methods within files. By exploiting the semantics of common file types, Essence generates compact yet representative file summaries that can be used to improve both browsing and indexing in resource discovery systems. Essence can extract information from most of the types of files found in common file systems, including files with nested structure (such as compressed “tar” files). Essence interoperates with a number of different search/index systems (such as WAIS and Glimpse), as part of the Harvest system.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2062477286",
    "type": "article"
  },
  {
    "title": "Decay-usage scheduling in multiprocessors",
    "doi": "https://doi.org/10.1145/292523.292535",
    "publication_date": "1998-11-01",
    "publication_year": 1998,
    "authors": "Dick Epema",
    "corresponding_authors": "Dick Epema",
    "abstract": "Decay-usage scheduling is a priority-aging time-sharing scheduling policy capable of dealing with a workload of both interactive and batch jobs by decreasing the priority of a job when it acquires CPU time, and by increasing its priority when it does not use the (a) CPU. In this article we deal with a decay-usage scheduling policy in multiprocessors modeled after widely used systems. The priority of a job consists of a base priority and a time-dependent component based on processor usage. Because t he priorities in our model are time dependent, a queuing-theoretic analysis—for instance, for the mean job response time—seems impossible. Still, it turns out that as a consequence of the scheduling policy, the shares of the available CPU time obtained by jobs converge, and a deterministic analysis for these shares is feasible: We show how for a fixed set of jobs with large processing demands, the steady-state shares can be obtained given the base priorities, and conversely, how to set the base priorities given the required shares. In addition, we analyze the relation between the values of the scheduler parameters and the level of control it can exercise over the steady-state share ratios, and we deal with the rate of convergence. We validate the model by simulations and by measurements of actual systems.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1998058281",
    "type": "article"
  },
  {
    "title": "Architectural and Compiler Support for Effective Instruction Prefetching: A Cooperative Approach.",
    "doi": null,
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Chi-Keung Luk; Todd C. Mowry",
    "corresponding_authors": "",
    "abstract": "Instruction cache miss latency is becoming an increasingly important performance bottleneck, especially for commercial applications. Although instruction prefetching is an attractive technique for tolerating this latency, we find that existing prefetching schemes are insufficient for modern superscalar processors, since they fail to issue prefetches early enough (particularly for nonsequential accesses). To overcome these limitations, we propose a new instruction prefetching technique whereby the hardware and software cooperate to hide the latency as follows. The hardware performs aggressive sequential prefetching combined with a novel prefetch filtering mechanism to allow it to get far ahead without polluting the cache. To hide the latency of nonsequential accesses, we propose and implement a novel compiler algorithm which automatically inserts instruction-prefetch instructions into the executable to prefetch the targets of control transfers far enough in advance. Our experimental results demonstrate that this new approach hides 50% or more of the latency remaining with the best previous techniques, while at the same time reduces the number of useless prefetches by a factor of six. We find that both the prefetch filtering and compiler-inserted prefetching components of our design are essential and complementary, and that the compiler can limit the code expansion to only 9% on average. In addition, we show that the performance of our technique can be further increased by using profiling information to help reduce cache conflicts and unnecessary prefetches. From an architectural perspective, these performance advantages are sustained over a range of common miss latencies and bandwidth. Finally, our technique is cost effective as well, since it delivers performance comparable to (or even better than) that of larger caches, but requires a much smaller hardware budget.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2025597060",
    "type": "article"
  },
  {
    "title": "Separating access control policy, enforcement, and functionality in extensible systems",
    "doi": "https://doi.org/10.1145/367742.367773",
    "publication_date": "2001-02-01",
    "publication_year": 2001,
    "authors": "Robert Grimm; Brian N. Bershad",
    "corresponding_authors": "",
    "abstract": "Extensible systems, such as Java or the SPIN extensible operating system, allow for units of code, or extensions, to be added to a running system in almost arbitrary fashion. Extensions closely interact through low-latency but type-safe interfaces to form a tightly integrated system. As extensions can come from arbitrary sources, not all of whom can be trusted to conform to an organization's security policy, such structuring raises the question of how security constraints are enforced in an extensible system. In this paper, we present an access control mechanism for extensible systems to address this problem. Our access control mechanism decomposes access control into a policy-neutral enforcement manager and a security policy manager, and it is transparent to extensions in the absence of security violations. It structures the system into protection domains, enforces protection domains through access control checks, and performs auditing of system operations. The access control mechanism works by inspecting extensions for their types and operations to determine which abstractions require protection and by redirecting procedure or method invocations to inject access control operations into the system. We describe the design of this access control mechanism, present an implementation within the SPIN extensible operating systems, and provide a qualitative as well as quantitative evaluation of the mechanism.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2082768964",
    "type": "article"
  },
  {
    "title": "Shared memory computing on clusters with symmetric multiprocessors and system area networks",
    "doi": "https://doi.org/10.1145/1082469.1082472",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Leonidas Kontothanassis; Robert Stets; Galen Hunt; Umit Rencuzogullari; Gautam Altekar; Sandhya Dwarkadas; Michael L. Scott",
    "corresponding_authors": "",
    "abstract": "Cashmere is a software distributed shared memory (S-DSM) system designed for clusters of server-class machines. It is distinguished from most other S-DSM projects by (1) the effective use of fast user-level messaging, as provided by modern system-area networks, and (2) a “two-level” protocol structure that exploits hardware coherence within multiprocessor nodes. Fast user-level messages change the tradeoffs in coherence protocol design; they allow Cashmere to employ a relatively simple directory-based coherence protocol. Exploiting hardware coherence within SMP nodes improves overall performance when care is taken to avoid interference with inter-node software coherence.We have implemented Cashmere on a Compaq AlphaServer/Memory Channel cluster, an architecture that provides fast user-level messages. Experiments indicate that a one-level, version of the Cashmere protocol provides performance comparable to, or slightly better than, that of TreadMarks' lazy release consistency. Comparisons to Compaq's Shasta protocol also suggest that while fast user-level messages make finer-grain software DSMs competitive, VM-based systems continue to outperform software-based access control for applications without extensive fine-grain sharing.Within the family of Cashmere protocols, we find that leveraging intranode hardware coherence provides a 37% performance advantage over a more straightforward one-level implementation. Moreover, contrary to our original expectations, noncoherent hardware support for remote memory writes, total message ordering, and broadcast, provide comparatively little in the way of additional benefits over just fast messaging for our application suite.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1975670601",
    "type": "article"
  },
  {
    "title": "Error bounds for performance prediction in queuing networks",
    "doi": "https://doi.org/10.1145/3959.3960",
    "publication_date": "1985-08-01",
    "publication_year": 1985,
    "authors": "Y. C. Tay; Rajan Suri",
    "corresponding_authors": "",
    "abstract": "Analytic models based on closed queuing networks (CQNS) are widely used for performance prediction in practical systems. In using such models, there is always a prediction error, that is, a difference between the predicted performance and the actual outcome. This prediction error is due both to modeling errors and estimation errors, the latter being the difference between the estimated values of the CQN parameters and the actual outcomes. This paper considers the second class of errors; in particular, it studies the effect of small estimation errors and provides bounds on prediction errors based on bounds on estimation errors. Estimation errors may be divided into two types: (1) the difference between the estimated value and the average value of the outcome, and (2) the deviation of the actual value from its average. The analysis first studies the sum of both types of errors, then the second type alone. The results are illustrated with three examples.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2025981063",
    "type": "article"
  },
  {
    "title": "A general framework for prefetch scheduling in linked data structures and its application to multi-chain prefetching",
    "doi": "https://doi.org/10.1145/986533.986536",
    "publication_date": "2004-05-01",
    "publication_year": 2004,
    "authors": "Seungryul Choi; Nicholas Kohout; Sumit Pamnani; Dong-Keun Kim; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "Pointer-chasing applications tend to traverse composite data structures consisting of multiple independent pointer chains. While the traversal of any single pointer chain leads to the serialization of memory operations, the traversal of independent pointer chains provides a source of memory parallelism. This article investigates exploiting such interchain memory parallelism for the purpose of memory latency tolerance, using a technique called multi--chain prefetching . Previous works [Roth et al. 1998;Roth and Sohi 1999] have proposed prefetching simple pointer-based structures in a multi--chain fashion. However, our work enables multi--chain prefetching for arbitrary data structures composed of lists, trees, and arrays.This article makes five contributions in the context of multi--chain prefetching. First, we introduce a framework for compactly describing linked data structure (LDS) traversals, providing the data layout and traversal code work information necessary for prefetching. Second, we present an off-line scheduling algorithm for computing a prefetch schedule from the LDS descriptors that overlaps serialized cache misses across separate pointer-chain traversals. Our analysis focuses on static traversals. We also propose using speculation to identify independent pointer chains in dynamic traversals. Third, we propose a hardware prefetch engine that traverses pointer-based data structures and overlaps multiple pointer chains according to the computed prefetch schedule. Fourth, we present a compiler that extracts LDS descriptors via static analysis of the application source code, thus automating multi--chain prefetching. Finally, we conduct an experimental evaluation of compiler-instrumented multi--chain prefetching and compare it against jump pointer prefetching [Luk and Mowry 1996], prefetch arrays [Karlsson et al. 2000], and predictor-directed stream buffers (PSB) [Sherwood et al. 2000].Our results show compiler-instrumented multi--chain prefetching improves execution time by 40% across six pointer-chasing kernels from the Olden benchmark suite [Rogers et al. 1995], and by 3% across four SPECint2000 benchmarks. Compared to jump pointer prefetching and prefetch arrays, multi--chain prefetching achieves 34% and 11% higher performance for the selected Olden and SPECint2000 benchmarks, respectively. Compared to PSB, multi--chain prefetching achieves 27% higher performance for the selected Olden benchmarks, but PSB outperforms multi--chain prefetching by 0.2% for the selected SPECint2000 benchmarks. An ideal PSB with an infinite Markov predictor achieves comparable performance to multi--chain prefetching, coming within 6% across all benchmarks. Finally, speculation can enable multi--chain prefetching for some dynamic traversal codes, but our technique loses its effectiveness when the pointer-chain traversal order is highly dynamic.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2054818614",
    "type": "article"
  },
  {
    "title": "Locking under Pfair scheduling",
    "doi": "https://doi.org/10.1145/1132026.1132028",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Philip Holman; James H. Anderson",
    "corresponding_authors": "",
    "abstract": "We present several locking synchronization protocols for Pfair-scheduled multiprocessor systems. We focus on two classes of protocols. The first class is only applicable in systems in which all critical sections are short relative to the length of the scheduling quantum. In this case, efficient synchronization can be achieved by ensuring that all locks have been released before tasks are preempted. This is accomplished by exploiting the quantum-based nature of Pfair scheduling, which provides a priori knowledge of all possible preemption points. The second and more general protocol class is applicable to any system. For this class, we consider the use of a client-server model. We also discuss the viability of inheritance-based protocols in Pfair-scheduled systems.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1979351188",
    "type": "article"
  },
  {
    "title": "Exploring the Tradeoffs between Programmability and Efficiency in Data-Parallel Accelerators",
    "doi": "https://doi.org/10.1145/2491464",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Yunsup Lee; Rimas Avižienis; Alex Bishara; Richard Xia; Derek Lockhart; Christopher Batten; Krste Asanović",
    "corresponding_authors": "",
    "abstract": "We present a taxonomy and modular implementation approach for data-parallel accelerators, including the MIMD, vector-SIMD, subword-SIMD, SIMT, and vector-thread (VT) architectural design patterns. We introduce Maven, a new VT microarchitecture based on the traditional vector-SIMD microarchitecture, that is considerably simpler to implement and easier to program than previous VT designs. Using an extensive design-space exploration of full VLSI implementations of many accelerator design points, we evaluate the varying tradeoffs between programmability and implementation efficiency among the MIMD, vector-SIMD, and VT patterns on a workload of compiled microbenchmarks and application kernels. We find the vector cores provide greater efficiency than the MIMD cores, even on fairly irregular kernels. Our results suggest that the Maven VT microarchitecture is superior to the traditional vector-SIMD architecture, providing both greater efficiency and easier programmability.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2066851213",
    "type": "article"
  },
  {
    "title": "SPIN",
    "doi": "https://doi.org/10.1145/3309987",
    "publication_date": "2018-05-31",
    "publication_year": 2018,
    "authors": "Shai Bergman; Tanya Brokhman; Tzachi Cohen; Mark Silberstein",
    "corresponding_authors": "",
    "abstract": "Recent GPUs enable Peer-to-Peer Direct Memory Access ( p 2 p ) from fast peripheral devices like NVMe SSDs to exclude the CPU from the data path between them for efficiency. Unfortunately, using p 2 p to access files is challenging because of the subtleties of low-level non-standard interfaces, which bypass the OS file I/O layers and may hurt system performance. Developers must possess intimate knowledge of low-level interfaces to manually handle the subtleties of data consistency and misaligned accesses. We present SPIN , which integrates p 2 p into the standard OS file I/O stack, dynamically activating p 2 p where appropriate, transparently to the user. It combines p 2 p with page cache accesses, re-enables read-ahead for sequential reads, all while maintaining standard POSIX FS consistency, portability across GPUs and SSDs, and compatibility with virtual block devices such as software RAID. We evaluate SPIN on NVIDIA and AMD GPUs using standard file I/O benchmarks, application traces, and end-to-end experiments. SPIN achieves significant performance speedups across a wide range of workloads, exceeding p 2 p throughput by up to an order of magnitude. It also boosts the performance of an aerial imagery rendering application by 2.6× by dynamically adapting to its input-dependent file access pattern, enables 3.3× higher throughput for a GPU-accelerated log server, and enables 29% faster execution for the highly optimized GPU-accelerated image collage with only 30 changed lines of code.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2937574366",
    "type": "article"
  },
  {
    "title": "Protocol Responsibility Offloading to Improve TCP Throughput in Virtualized Environments",
    "doi": "https://doi.org/10.1145/2491463",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Sahan Gamage; Ramana Rao Kompella; Dongyan Xu; Ardalan Kangarlou",
    "corresponding_authors": "",
    "abstract": "Virtualization is a key technology that powers cloud computing platforms such as Amazon EC2. Virtual machine (VM) consolidation, where multiple VMs share a physical host, has seen rapid adoption in practice, with increasingly large numbers of VMs per machine and per CPU core. Our investigations, however, suggest that the increasing degree of VM consolidation has serious negative effects on the VMs’ TCP performance. As multiple VMs share a given CPU, the scheduling latencies, which can be in the order of tens of milliseconds, substantially increase the typically submillisecond round-trip times (RTTs) for TCP connections in a datacenter, causing significant degradation in throughput. In this article, we propose a lightweight solution, called vPRO, that (a) offloads the VM’s TCP congestion control function to the driver domain to improve TCP transmit performance; and (b) offloads TCP acknowledgment functionality to the driver domain to improve the TCP receive performance. Our evaluation of a vPRO prototype on Xen suggests that vPRO substantially improves TCP receive and transmit throughputs with minimal per-packet CPU overhead. We further show that the higher TCP throughput leads to improvement in application-level performance, via experiments with Apache Olio, a Web 2.0 cloud application, and Intel MPI benchmark.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2162424584",
    "type": "article"
  },
  {
    "title": "K2",
    "doi": "https://doi.org/10.1145/2699676",
    "publication_date": "2015-06-08",
    "publication_year": 2015,
    "authors": "Felix Xiaozhu Lin; Zhen Wang; Lin Zhong",
    "corresponding_authors": "",
    "abstract": "Mobile System-on-Chips (SoC) that incorporate heterogeneous coherence domains promise high energy efficiency to a wide range of mobile applications, yet are difficult to program. To exploit the architecture, a desirable, yet missing capability is to replicate operating system (OS) services over multiple coherence domains with minimum inter-domain communication. In designing such an OS, we set three goals: to ease application development, to simplify OS engineering, and to preserve the current OS performance. To this end, we identify a shared-most OS model for multiple coherence domains: creating per-domain instances of core OS services with no shared state, while enabling other extended OS services to share state across domains. To test the model, we build K2, a prototype OS on the TI OMAP4 SoC, by reusing most of the Linux 3.4 source. K2 presents a single system image to applications with its two kernels running on top of the two coherence domains of OMAP4. The two kernels have independent instances of core OS services, such as page allocation and interrupt management, as coordinated by K2; the two kernels share most extended OS services, such as device drivers, whose state is kept coherent transparently by K2. Despite platform constraints and unoptimized code, K2 improves energy efficiency for light OS workloads by 8x-10x, while incurring less than 9% performance overhead for two device drivers shared between kernels. Our experiences with K2 show that the shared-most model is promising.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2264380155",
    "type": "article"
  },
  {
    "title": "Exploring the Tradeoffs between Programmability and Efficiency in Data-Parallel Accelerators",
    "doi": "https://doi.org/10.1145/2518037.2491464",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Yunsup Lee; Rimas Avižienis; Alex Bishara; Richard Xia; Derek Lockhart; Christopher Batten; Krste Asanović",
    "corresponding_authors": "",
    "abstract": "We present a taxonomy and modular implementation approach for data-parallel accelerators, including the MIMD, vector-SIMD, subword-SIMD, SIMT, and vector-thread (VT) architectural design patterns. We introduce Maven, a new VT microarchitecture based on the traditional vector-SIMD microarchitecture, that is considerably simpler to implement and easier to program than previous VT designs. Using an extensive design-space exploration of full VLSI implementations of many accelerator design points, we evaluate the varying tradeoffs between programmability and implementation efficiency among the MIMD, vector-SIMD, and VT patterns on a workload of compiled microbenchmarks and application kernels. We find the vector cores provide greater efficiency than the MIMD cores, even on fairly irregular kernels. Our results suggest that the Maven VT microarchitecture is superior to the traditional vector-SIMD architecture, providing both greater efficiency and easier programmability.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4242141188",
    "type": "article"
  },
  {
    "title": "A Small-Footprint Accelerator for Large-Scale Neural Networks",
    "doi": "https://doi.org/10.1145/2701417",
    "publication_date": "2015-05-22",
    "publication_year": 2015,
    "authors": "Tianshi Chen; Shijin Zhang; Shaoli Liu; Zidong Du; Tao Luo; Yuan Gao; Junjie Liu; Dongsheng Wang; Chengyong Wu; Ninghui Sun; Yunji Chen; Olivier Temam",
    "corresponding_authors": "",
    "abstract": "Machine-learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve toward heterogeneous multicores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have been focusing on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance, and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02mm&lt;sup&gt;2&lt;/sup&gt; and 485mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87 × faster, and it can reduce the total energy by 21.08 ×. The accelerator characteristics are obtained after layout at 65nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1785235667",
    "type": "article"
  },
  {
    "title": "Freezing-based Memory and Process Co-design for User Experience on Resource-limited Mobile Devices",
    "doi": "https://doi.org/10.1145/3714409",
    "publication_date": "2025-01-18",
    "publication_year": 2025,
    "authors": "Changlong Li; Zongwei Zhu; Yu Liang; Xuehai Zhou",
    "corresponding_authors": "",
    "abstract": "Mobile devices with limited resources are prevalent as they have a relatively low price. Providing a good user experience with limited resources has been a big challenge. This paper found that foreground applications are often unexpectedly interfered by background applications’ memory activities. Improving user experience on resource-limited mobile devices calls for a strong collaboration between memory and process management. This paper proposes a framework, Ice, to optimize the user experience on resource-limited mobile devices. With Ice, processes that will cause frequent refaults in the background are identified and frozen accordingly. The frozen application will be thawed when memory condition allows. Based on the proposed Ice, this paper shows that the refault can be further reduced by revisiting the LRU lists in the original kernel with app-freezing awareness (called Ice + ). Evaluation of resource-limited mobile devices demonstrates that the user experience is effectively improved with Ice. Specifically, Ice boosts the frame rate by 1.57x on average over the state-of-the-art. The frame rate is further enhanced by 5.14% on average with Ice + .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406552190",
    "type": "article"
  },
  {
    "title": "XpuTEE: A High-Performance and Practical Heterogeneous Trusted Execution Environment for GPUs",
    "doi": "https://doi.org/10.1145/3719653",
    "publication_date": "2025-02-26",
    "publication_year": 2025,
    "authors": "Shulin Fan; Zhichao Hua; Yubin Xia; Haibo Chen",
    "corresponding_authors": "",
    "abstract": "AI applications are employed in diverse scenarios, including data centers, personal computers, smart cars, and so on. Their privacy is threatened by the intricate software stacks and the potential malfeasance of system maintainers. The Trusted Execution Environment (TEE) has become popular for safeguarding applications from untrusted system software. However, AI applications are always speeded up with heterogeneous accelerators, e.g., GPU, which requires the TEE to be heterogeneous. A heterogeneous TEE should satisfy three requirements: 1) the joint heterogeneous abstraction that covers the CPU and GPUs and minimizes cooperation overhead among enclaves on them; 2) the high performance for supporting high-speed GPUs and introducing limited performance overhead; and 3) the compatibility with existing CPUs and GPUs so that existing machines can directly benefit from it. To meet the above requirements, this paper introduces XpuTEE, a practical and high-performance heterogeneous TEE system. XpuTEE provides a new abstraction called XpuEnclave, comprising the CEnclave to protect CPU-side logic and numerous XEnclaves to guard GPU tasks. XpuEnclave is a joint TEE crossing the CPU and connected GPUs, and it removes all cryptographic operations and extra memory copies for CPU-GPU communication, which allows XpuTEE to achieve high performance. The results demonstrate that XpuTEE has an average performance overhead of 2.48% for common AI applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407974266",
    "type": "article"
  },
  {
    "title": "Towards Serialization/Deserialization-free State Transfer in Serverless Workflows",
    "doi": "https://doi.org/10.1145/3725986",
    "publication_date": "2025-03-25",
    "publication_year": 2025,
    "authors": "Xingda Wei; Fangming Lu; Zhuobin Huang; Rong Chen; Mingyu Wu; Haibo Chen",
    "corresponding_authors": "",
    "abstract": "Serialization and deserialization dominate the state transfer time of serverless workflows, leading to substantial performance penalties when executing various serverless workflow applications. We identify the key reason for serialization and deserialization as a lack of ability to efficiently access the (remote) memory of another function. To this end, we propose RMMap , an OS primitive for remote memory map, which allows a serverless function to directly access the memory of another function, even if it is located remotely. RMMap is the first to completely eliminate serialization and deserialization overhead when transferring states between any pairs of functions in (unmodified) serverless workflows. To make remote memory map efficient and feasible, we co-design it with modern networking (RDMA), OS, language runtime, and serverless platform. Evaluations using real-world serverless workloads show that integrating RMMap with Knative reduces the serverless workflow execution time on Knative by up to 2.6 × and improves resource utilizations by 86.3%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408833178",
    "type": "article"
  },
  {
    "title": "Efficient Fault Tolerance for Stateful Serverless Computing with Asymmetric Logging",
    "doi": "https://doi.org/10.1145/3725985",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Sheng Qi; Haoyu Feng; Xuanzhe Liu; Xin Jin",
    "corresponding_authors": "",
    "abstract": "Serverless computing separates function execution from state management. Simple retry-based fault tolerance might corrupt the shared state with duplicate updates. Existing solutions employ log-based fault tolerance to achieve exactly-once semantics, where every single read or write to the external state is associated with a log for deterministic replay. However, logging is not a free lunch, which introduces considerable overhead to stateful serverless applications. We present Halfmoon, a serverless runtime system for fault-tolerant stateful serverless computing. Our key insight is that it is unnecessary to symmetrically log both reads and writes. Instead, it suffices to log either reads or writes, i.e., asymmetrically. We design two logging protocols that enforce exactly-once semantics while providing log-free reads and writes, which are suitable for read- and write-intensive workloads, respectively. We theoretically prove that the two protocols are log-optimal , i.e., no other protocols can achieve lower logging overhead than our protocols. We provide a criterion for choosing the right protocol for a given workload, and a pauseless switching mechanism to switch protocols for dynamic workloads. We implement a prototype of Halfmoon. Experiments show that Halfmoon achieves 20%–40% lower latency and 1.5–4.0 × lower logging overhead than the state-of-the-art solution Boki.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408920617",
    "type": "article"
  },
  {
    "title": "RegVault II: Achieving Hardware-Assisted Selective Kernel Data Randomization for Multiple Architectures",
    "doi": "https://doi.org/10.1145/3734521",
    "publication_date": "2025-05-06",
    "publication_year": 2025,
    "authors": "Ren Guo; Yangye Zhou; Jinyan Xu; Wenbo Shen; Yajin Zhou; Rui Chang",
    "corresponding_authors": "",
    "abstract": "Memory corruption vulnerabilities pose a significant threat to system security. The traditional paging-based approach cannot protect fine-grained runtime data (e.g., function pointers), which are often mixed with other data in memory. To protect the runtime data, data space randomization is proposed to encrypt the in-memory data so that the attacker cannot control the decrypted result. Unfortunately, current hardware does not provide dedicated support for fine-grained data encryption. This paper presents RegVault II, a cross-architectural hardware-assisted lightweight data randomization scheme for OS kernels. To achieve robust, fine-grained, and lightweight data protection, we first identify five required capabilities for efficient and secure data randomization. Guided by these requirements, we design and implement novel hardware primitives that provide cryptographically strong encryption and decryption, thus ensuring both confidentiality and integrity for register-grained data. At the software level, we propose identification- and annotation-based approaches to automatically mark sensitive data and instrument the corresponding load and store operations. We also introduce new techniques to protect the interrupt context and safeguard the sensitive data spilling. We implement RegVault II on an actual FPGA hardware board for RISC-V and on QEMU for Arm, applying it to protect six types of sensitive data in the Linux kernel. Our thorough security and performance evaluations show that RegVault II effectively defends against a broad range of kernel data attacks while incurring minimal performance overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410126006",
    "type": "article"
  },
  {
    "title": "Towards Scalable and Non-blocking Automata Processing on GPUs with ngAP",
    "doi": "https://doi.org/10.1145/3748646",
    "publication_date": "2025-07-15",
    "publication_year": 2025,
    "authors": "Tianao Ge; Tong Zhang; Hongyuan Liu",
    "corresponding_authors": "",
    "abstract": "Finite automata serve as compute kernels for various applications. Although GPUs offer massive parallelism, their potential for automata processing remains underutilized due to three challenges: (1) insufficient parallelism leading to underutilized threads; (2) redundant computation, as many states match repeated symbols; and (3) limited exploitation of temporal and spatial locality, caused by frequent remapping between threads and states and irregular memory accesses. We observe that processing automata “one-symbol-at-a-time” serializes execution and hinders scalability. To address this, we propose Non-blocking Automata Processing, which enables parallel processing of different input symbols and supports several optimizations: (1) prefetching computations to improve thread utilization by allowing multiple symbols to be processed in parallel; (2) memoization to eliminate repeated computations via table lookups; (3) privatizing computations to preserve thread-state mapping and improve temporal locality; and (4) deduplicating matchsets and encoding common automata patterns to reduce memory usage and improve spatial locality. Our experimental evaluation across 20 applications shows that our approach outperforms the state-of-the-art GPU automata processing engine by an average of 9.5 ×, and up to 1,613 ×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412423764",
    "type": "article"
  },
  {
    "title": "Lining up Garbage Collection and Application for a Far-Memory-Friendly Runtime",
    "doi": "https://doi.org/10.1145/3749283",
    "publication_date": "2025-07-18",
    "publication_year": 2025,
    "authors": "Shengkai Li; Chenxi Wang; Haonan Xue; Haoran Ma; Shi Liu; Yifan Qiao; Jonathan Eyolfson; Christian Navasca; Shan Lu; Guoqing Xu",
    "corresponding_authors": "",
    "abstract": "Far-memory techniques that enable applications to use remote memory are increasingly appealing in modern data centers, supporting applications’ large memory footprint and improving machines’ resource utilization. Unfortunately, most far-memory techniques focus on OS-level optimizations and are agnostic to managed runtimes and garbage collections (GC) underneath applications written in high-level languages. With different object-access patterns from applications, GC can severely interfere with existing far-memory techniques, breaking remote memory prefetching algorithms and causing severe local-memory misses. We developed MemLiner, a runtime technique that improves the performance of far-memory systems by aligning memory accesses from application and garbage collection threads so that they follow similar memory access paths, thereby (1) reducing the local-memory working set and (2) improving remote-memory prefetching through simplified memory access patterns. We implemented MemLiner in two widely used GCs in OpenJDK: G1 and Shenandoah. Our evaluation with a range of widely deployed cloud systems shows that MemLiner improves applications’ end-to-end performance by up to 3.3 × and reduces applications’ tail latency by up to 220.0 × .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413060570",
    "type": "article"
  },
  {
    "title": "<i>TraceScaler</i> : A Framework for Scaling Load in Real-World Traces for System Evaluation",
    "doi": "https://doi.org/10.1145/3760774",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Sultan Mahmud Sajal; Md Salman Estyak; Rubaba Hasan; Timothy Zhu; Bhuvan Urgaonkar; Siddhartha Sen",
    "corresponding_authors": "",
    "abstract": "Trace replay is a common approach for evaluating systems by rerunning historical traffic patterns, but it’s not always possible to find suitable real-world traces at the desired level of system load. To experiment with different loads, one needs to downscale a trace to decrease the load or upscale a trace to artificially increase the load. This paper expands upon our work, TraceUpscaler [92], by considering the interaction of upscaling and downscaling. In addition to evaluating upscaling with traces collected from a subset of the cluster, we also evaluate upscaling with traces that were downscaled with the state-of-the-art downscaling tool, TraceSplitter [91], to demonstrate that the upscaling and downscaling techniques are compatible and do not introduce unexpected artifacts in the scaling. In addition to comparing against prior approaches, we develop a novel upscaling technique, TraceOverlap , based on the idea of overlapping different time periods in a trace, where we identify the most similar time periods to overlap. Our evaluation demonstrates that TraceUpscaler and TraceOverlap are both more accurate in maintaining latency characteristics than prior approaches, with TraceUpscaler matching the original trace latency more closely. Finally, we provide a unified framework, TraceScaler , that combines TraceUpscaler with TraceSplitter to provide experimenters a common tool for their trace scaling needs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413286964",
    "type": "article"
  },
  {
    "title": "LazyLog: A New Shared Log Abstraction and Design for Modern Low-Latency Applications",
    "doi": "https://doi.org/10.1145/3750445",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Xuhao Luo; Shreesha G. Bhat; J. Hu; Ramnatthan Alagappan; Aishwarya Ganesan",
    "corresponding_authors": "",
    "abstract": "Shared logs offer linearizable total order across storage shards. However, they enforce this order eagerly upon ingestion, leading to high latencies. We observe that in many modern shared-log applications, while linearizable ordering is necessary, it is not required eagerly when ingesting data but only later when data is consumed. Further, readers are naturally decoupled in time from writers in these applications. Based on this insight, we propose LazyLog, a novel shared log abstraction. LazyLog lazily binds records (across shards) to linearizable global positions and enforces this before a log position can be read. Such lazy ordering enables low ingestion latencies. Given the time decoupling, LazyLog can establish the order well before reads arrive, minimizing overhead upon reads. We build two LazyLog systems that provide linearizable total order across shards. Our experiments show that LazyLog systems deliver significantly lower latencies than conventional, eager-ordering shared logs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413308167",
    "type": "article"
  },
  {
    "title": "CCKit: An open-source toolkit for cache coherent accelerators",
    "doi": "https://doi.org/10.1145/3763790",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Abishek Ramdas; David Cock; Michael Giardino; Dario Korolija; Anastasiia Ruzhanskaia; Daniel Schwyn; Adam Turowski; Gustavo Alonso; Timothy Roscoe",
    "corresponding_authors": "",
    "abstract": "The trend towards system specialization is leading to a proliferation of accelerators, exposing interconnects as serious bottlenecks, both in functionality and performance. As a result, several alternative approaches have been proposed which promise to expand the coherence domain beyond homogeneous sockets to rack scale heterogeneous systems. In parallel, GPU vendors have developed their own high bandwidth interconnects also aiming for heterogeneous coherence beyond the CPU. This expansion of the coherency domain raises many questions that remain unanswered, in particular, how devices other than CPUs will interact with the coherence protocol and whether applications can take advantage of these expanded domains. As protocols such as CXL are still evolving, it is important to explore alternative designs that go beyond what the commercial specifications dictate. For this purpose, we developed CCKit, an open-source, server-class toolkit comprising a complete cache coherency stack on reconfigurable accelerators. CCKit is more flexible than commercial products and its performance is highly competitive with hardware-based implementations, thus enabling important and novel application use-cases for expanded coherence domains. Experimental data from real workloads provide the ability to influence and expand future interconnects, protocols, and applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413602082",
    "type": "article"
  },
  {
    "title": "Analyzing Configuration Dependencies of File Systems",
    "doi": "https://doi.org/10.1145/3747177",
    "publication_date": "2025-09-03",
    "publication_year": 2025,
    "authors": "Tabassum Mahmud; Om Rameshwar Gatla; Duo Zhang; Christopher G. Love; Ryan Bumann; Varun S Girimaji; Mai Zheng",
    "corresponding_authors": "",
    "abstract": "File systems play an essential role in modern society for managing precious data. To meet diverse needs, they often support many configuration parameters. Such flexibility comes at the price of additional complexity which can lead to subtle configuration-related issues. To address this challenge, we study the configuration-related issues of two major file systems (i.e., Ext4 and XFS) in depth, and identify a prevalent pattern called multilevel configuration dependencies. Based on the study, we build an extensible tool called ConfD to extract the dependencies automatically, and create a set of plugins to address different configuration-related issues. Our experiments on Ext4, XFS and a modern copy-on-write file system (i.e., ZFS) show that ConfD was able to extract 160 configuration dependencies for the file systems with a low false positive rate. Moreover, the dependency-guided plugins can identify various configuration issues (e.g., mishandling of configurations, regression test failures induced by valid configurations). In addition, we also explore the applicability of ConfD on a popular storage engine (i.e., WiredTiger). We hope that this comprehensive analysis of configuration dependencies of storage systems can shed light on addressing configuration-related challenges for the system community in general.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413941639",
    "type": "article"
  },
  {
    "title": "SuperBench: A Proactive Validation System for Improving Reliability of Cloud AI Infrastructure",
    "doi": "https://doi.org/10.1145/3767334",
    "publication_date": "2025-09-13",
    "publication_year": 2025,
    "authors": "Yifan Xiong; Yanda Jiang; Ziyue Yang; Lei Qu; G. Zhao; S. Liu; Dong Zhong; Boris Pinzur; Jie Zhang; Yang Wang; Joaquin José; Hossein Pourreza; J. S. Baxter; Kushal Datta; Prabhat Ram; L. Joseph Melton; Joe Chau; Peng Cheng; Yongqiang Xiong; Lidong Zhou",
    "corresponding_authors": "",
    "abstract": "Reliability in cloud AI infrastructure is crucial for cloud service providers, prompting the widespread use of hardware redundancies. However, these redundancies can inadvertently lead to hidden degradation, known as “gray failure”, for AI workloads, significantly affecting end-to-end performance and concealing performance issues, which complicates root cause analysis for failures and regressions. We introduce SuperBench, a proactive validation system for AI infrastructure that mitigates hidden degradation caused by hardware redundancies and enhances overall reliability. SuperBench features a comprehensive benchmark suite, capable of evaluating individual hardware components and representing most real AI workloads. It comprises a Validator that learns benchmark criteria to pinpoint defective components clearly. Additionally, SuperBench incorporates a Selector to balance validation time and issue-related penalties, enabling optimal timing for validation execution with a tailored subset of benchmarks. Through testbed evaluation and simulation, we demonstrate that SuperBench can increase the mean time between incidents by up to 22.61 ×. SuperBench has been successfully deployed in Azure production, validating hundreds of thousands of GPUs every year.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414260593",
    "type": "article"
  },
  {
    "title": "Unified and Near-optimal Multi-GPU Cache for Embedding-based Deep Learning",
    "doi": "https://doi.org/10.1145/3767725",
    "publication_date": "2025-09-13",
    "publication_year": 2025,
    "authors": "Xiaoniu Song; Rong Chen; Haitao Song; Yiwen Zhang; Haibo Chen",
    "corresponding_authors": "",
    "abstract": "This paper presents UGache , a unified multi-GPU cache system designed for embedding-based deep learning (EmbDL). UGache is primarily motivated by the unique characteristics of EmbDL applications, namely read-only and skewed embedding accesses with affinity and predictability. UGache introduces a novel factored extraction mechanism that avoids bandwidth congestion to fully exploit high-speed cross-GPU interconnects (e.g., NVLink and NVSwitch). Based on a hotness metric, UGache also provides a near-optimal cache policy that balances local and remote access to minimize the extraction time for diverse GPU interconnect topologies. We have implemented UGache and integrated it into two representative frameworks, TensorFlow and PyTorch. Evaluation using two typical types of EmbDL applications, namely graph neural network (GNN) training and deep learning recommendation (DLR) inference, shows that UGache outperforms state-of-the-art replication and partition designs by an average of 1.93 × and 1.63 × (up to 5.25 × and 3.45 ×), respectively. Furthermore, we demonstrate the applicability of UGache ’s principle beyond embedding-based deep learning, with an example of text-to-image generation on an inference cluster.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414260714",
    "type": "article"
  },
  {
    "title": "Kauri: BFT Consensus with Pipelined Tree-Based Dissemination and Aggregation",
    "doi": "https://doi.org/10.1145/3769423",
    "publication_date": "2025-09-26",
    "publication_year": 2025,
    "authors": "Ray Neiheiser; Miguel Matos; Luı́s Rodrigues",
    "corresponding_authors": "",
    "abstract": "With the growing interest in blockchains, permissioned approaches to consensus have received increasing attention. Unfortunately, the BFT consensus algorithms that are the backbone of most of these blockchains scale poorly and offer limited throughput. In fact, many state-of-the-art BFT consensus algorithms require a single leader process to receive and validate votes from a quorum of processes and then broadcast the result, which is inherently non-scalable. Recent approaches avoid this bottleneck by using dissemination/aggregation trees to propagate values and collect and validate votes. However, the use of trees increases the round latency, which limits the throughput for deeper trees. In this paper we propose Kauri, a BFT communication abstraction that sustains high throughput as the system size grows by leveraging a novel pipelining technique to perform scalable dissemination and aggregation on trees. Furthermore, when the number of faults is moderate (arguably the most common case in practice), our construction is able to recover from faults in an optimal number of reconfiguration steps. We implemented and experimentally evaluated Kauri with up to 800 processes. Our results show that Kauri outperforms the throughput of state-of-the-art permissioned blockchain protocols, by up to 58x without compromising latency. Interestingly, in some cases, the parallelization provided by Kauri can also decrease the latency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414556059",
    "type": "article"
  },
  {
    "title": "LCL <sup>+</sup> : a Lock Chain Length-based Distributed Deadlock Detection and Resolution Service Built for OceanBase",
    "doi": "https://doi.org/10.1145/3768621",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Zhenkun Yang; Chen Qian; Xuwang Teng; Fanyu Kong; Fusheng Han; Quanqing Xu; Daokun Hu",
    "corresponding_authors": "",
    "abstract": "The problem of deadlock detection and resolution in database systems has been studied for decades. Although it has long been a mature feature of classical centralized database systems for many years, its use in distributed database systems remains in its infancy. A simple and fully distributed deadlock detection and resolution algorithm was proposed by Don P. Mitchell and Michael J. Merritt ( M&amp;M ), but its assumption that each process waits for only one resource at a time prevents it from being generally applicable. The distributed deadlock detection algorithm based on Lock Chain Length (LCL) surpasses the limitations of the M&amp;M algorithm. However, it is less effective in quickly detecting deadlocks that encompass both distributed and local deadlocks. In this paper, we introduce LCL + , an advanced and universally applicable algorithm specifically designed for the detection and resolution of resource deadlocks in distributed environments. This algorithm improves the efficiency of identifying distributed deadlocks by accelerating the detection of hybrid deadlocks. Our extensive experiments demonstrate that LCL + significantly outperforms its predecessor, LCL, in efficiency. In addition, it has been successfully implemented in the OceanBase distributed relational database system. Detailed analyses from multiple perspectives within OceanBase confirm that LCL + significantly improves the system’s scalability and ensures the provision of high-quality service.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414780079",
    "type": "article"
  },
  {
    "title": "A Comprehensive Study on Solving Memory Bloat Under Virtualization",
    "doi": "https://doi.org/10.1145/3769429",
    "publication_date": "2025-10-08",
    "publication_year": 2025,
    "authors": "Chuandong Li; Zhe Tang; Dong Liu; Zhihong Xue; Xiaolin Wang; Zhenlin Wang; Yingwei Luo; Diyu Zhou",
    "corresponding_authors": "",
    "abstract": "Huge pages are effective in reducing address translation overhead under virtualization. However, huge pages can lead to the memory bloat problem, which manifests in two primary forms: hot bloat and usage bloat. Hot Bloat occurs when accesses to a huge page are heavily skewed towards a small subset of base pages, leading the hypervisor to (mistakenly) classify the entire huge page as hot. Hot Bloat undermines several critical virtualization techniques, including tiered memory and page sharing. Usage bloat refers to the base pages within a huge page that has not yet been allocated, causing virtual machines (VMs) to demand excessive memory. Prior work addressing memory bloat either requires hardware modification or targets a specific scenario and is not applicable to a hypervisor. This paper presents HugeScope , a lightweight, effective and generic system that addresses the memory bloat problem under virtualization based on commodity hardware. HugeScope includes an efficient and precise page tracking mechanism, leveraging the other level of indirect memory translation in the hypervisor. HugeScope provides a generic framework to support page splitting and coalescing policies, considering the memory pressure, as well as the recency, frequency, and skewness of page access. Moreover, HugeScope is general and modular. It can not only be easily applied to various scenarios concerning hot bloat, including tiered memory management ( HS-TMM ) and page sharing ( HS-Share ), but also seamlessly expose its capabilities to VMs to address the usage bloat problem ( HS-HP ). Evaluation shows that HugeScope incurs less than 4% overhead, by addressing hot bloat, HS-TMM improves performance by up to 61% over vTMM while HS-Share saves 41% more memory than Ingens while offering comparable performance, and By addressing usage bloat, HS-HP can eliminate excessive memory usage, and achieve performance improvements of up to 11% over HawkEye.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414934564",
    "type": "article"
  },
  {
    "title": "Java consistency",
    "doi": "https://doi.org/10.1145/362670.362673",
    "publication_date": "2000-11-01",
    "publication_year": 2000,
    "authors": "Alex Gontmakher; Assaf Schuster",
    "corresponding_authors": "",
    "abstract": "The Java Language Specification (JLS) [Gosling et al. 1996] provides an operational definition for the consistency of shared variables. The definition remains unchanged in the JLS 2nd edition, currently under peer review, which relies on a specific abstract machine as its underlying model, is very complicated. Several subsequent works have tried to simplify and formalize it. However, these revised definitions are also operational, and thus have failed to highlight the intuition behind the original specification. In this work we provide a complete nonoperational specification for Java and for the JVM, excluding synchronized operations. We provide a simpler definition, in which we clearly distinguish the consistency model that is promised to the programmer from that which should be implemented in the JVM. This distinction, which was implicit in the original definition, is crucial for building the JVM. We find that the programmer model is strictly weaker than that of the JVM, and precisely define their discrepancy. Moreover, our definition is independent of any specific (or even abstract) machine, and can thus be used to verify JVM implementations and compiler optimizations on any platform. Finally, we show the precise range of consistency relaxations obtainable for the Java memory model when a certain compiler optimization— called prescient stores in JLS—is applicable.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1989373428",
    "type": "article"
  },
  {
    "title": "Strong loss tolerance of electronic coin systems",
    "doi": "https://doi.org/10.1145/253145.253282",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "Birgit Pfitzmann; Michael Waidner",
    "corresponding_authors": "",
    "abstract": "Untraceable electronic cash means prepaid digital payment systems, usually with offline payments, that protect user privacy. Such systems have recently been given considerable attention by both theory and development projects. However, in most current schemes, loss of a user device containing electronic cash implies a loss of money, just as with real cash. In comparison with credit schemes, this is considered a serious shortcoming. This article shows how untraceable electronic cash can be made loss tolerant, i.e., how the monetary value of the lost data can be recovered. Security against fraud and preservation of privacy are ensured; strong loss tolerance means that not even denial of recovery is possible. In particular, systems based on electronic coins are treated. We present general design principles and options and their instantiation in one concrete payment system. The measures are practical.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2069447774",
    "type": "article"
  },
  {
    "title": "Using histories to implement atomic objects",
    "doi": "https://doi.org/10.1145/75104.75106",
    "publication_date": "1989-11-01",
    "publication_year": 1989,
    "authors": "T.P. Ng",
    "corresponding_authors": "T.P. Ng",
    "abstract": "In this paper we describe an approach to implementing atomicity. Atomicity requires that computations appear to be all-or-nothing and executed in a serialization order . The approach we describe has three characteristics. First, it utilizes the semantics of an application to improve concurrency. Second, it reduces the complexity of application-dependent synchronization code by analyzing the process of writing it. Third, our approach hides the protocol used to arrive at a serialization order from the applications. As a result, different protocols can be used without affecting the applications. Our approach uses a history abstraction. The history captures the ordering relationship among concurrent computations. By determining what types of computations exist in the history and their parameters, a computation can determine whether it can proceed.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2101242158",
    "type": "article"
  },
  {
    "title": "Experiments in SR with different upcall program structures",
    "doi": "https://doi.org/10.1145/48012.48041",
    "publication_date": "1988-11-01",
    "publication_year": 1988,
    "authors": "M. Stella Atkins",
    "corresponding_authors": "M. Stella Atkins",
    "abstract": "This paper explores program designs for layered systems such as communication protocols and server/client systems that do not exhibit a strict hierarchy in their control flow. Clark has proposed structuring such systems, where both upward and downward control flow are required, to use efficient synchronous procedure calls between the layers whenever possible. The term upcall is used by Clark to describe this synchronous upward communication from server to client. Several techniques are possible for structuring such programs using upcalls. Comparisons are made by implementing a communication protocol described by Clark in three different ways. The first method implements all the protocol routines in a single large module. The second method structures the routines into modules occupying vertical slices of the protocol layers, and the third method structures the routines into modules corresponding to the protocol layers. Comparisons are made on two fronts: Preservation of modularity, in order to determine which method shows fault-tolerance and ease of programming, and program performance, which is a key motivation for the upcalls programming style. We conclude that the vertically layered protocol design is to be preferred unless there are many shared variables between the send-side and receive-side, as it is very efficient and provides the best protection of clients from each other. The horizontally layered design is the least efficient, but it is the easiest to program.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2003226440",
    "type": "article"
  },
  {
    "title": "Using certes to infer client response time at the web server",
    "doi": "https://doi.org/10.1145/966785.966787",
    "publication_date": "2004-02-01",
    "publication_year": 2004,
    "authors": "David P. Olshefski; Jason Nieh; Dakshi Agrawal",
    "corresponding_authors": "",
    "abstract": "As businesses continue to grow their World Wide Web presence, it is becoming increasingly vital for them to have quantitative measures of the mean client perceived response times of their web services. We present Certes (CliEnt Response Time Estimated by the Server), an online server-based mechanism that allows web servers to estimate mean client perceived response time, as if measured at the client. Certes is based on a model of TCP that quantifies the effect that connection drops have on mean client perceived response time by using three simple server-side measurements: connection drop rate, connection accept rate and connection completion rate. The mechanism does not require modifications to HTTP servers or web pages, does not rely on probing or third party sampling, and does not require client-side modifications or scripting. Certes can be used to estimate response times for any web content, not just HTML. We have implemented Certes and compared its response time estimates with those obtained with detailed client instrumentation. Our results demonstrate that Certes provides accurate server-based estimates of mean client response times in HTTP 1.0/1.1 environments, even with rapidly changing workloads. Certes runs online in constant time with very low overhead. It can be used at websites and server farms to verify compliance with service level objectives.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2100860375",
    "type": "article"
  },
  {
    "title": "A stateless approach to connection-oriented protocols",
    "doi": "https://doi.org/10.1145/1394441.1394444",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Alan Shieh; Andrew C. Myers; Emin Gün Sirer",
    "corresponding_authors": "",
    "abstract": "Traditional operating system interfaces and network protocol implementations force some system state to be kept on both sides of a connection. This state ties the connection to its endpoints, impedes transparent failover, permits denial-of-service attacks, and limits scalability. This article introduces a novel TCP-like transport protocol and a new interface to replace sockets that together enable all state to be kept on one endpoint, allowing the other endpoint, typically the server, to operate without any per-connection state. Called Trickles , this approach enables servers to scale well with increasing numbers of clients, consume fewer resources, and better resist denial-of-service attacks. Measurements on a full implementation in Linux indicate that Trickles achieves performance comparable to TCP/IP, interacts well with other flows, and scales well. Trickles also enables qualitatively different kinds of networked services. Services can be geographically replicated and contacted through an anycast primitive for improved availability and performance. Widely-deployed practices that currently have client-observable side effects, such as periodic server reboots, connection redirection, and failover, can be made transparent, and perform well, under Trickles. The protocol is secure against tampering and replay attacks, and the client interface is backward-compatible, requiring no changes to sockets-based client applications.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2067614093",
    "type": "article"
  },
  {
    "title": "On the design of perturbation-resilient atomic commit protocols for mobile transactions",
    "doi": "https://doi.org/10.1145/2003690.2003691",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Brahim Ayari; Abdelmajid Khelil; Neeraj Suri",
    "corresponding_authors": "",
    "abstract": "Distributed mobile transactions utilize commit protocols to achieve atomicity and consistent decisions. This is challenging, as mobile environments are typically characterized by frequent perturbations such as network disconnections and node failures. On one hand environmental constraints on mobile participants and wireless links may increase the resource blocking time of fixed participants. On the other hand frequent node and link failures complicate the design of atomic commit protocols by increasing both the transaction abort rate and resource blocking time. Hence, the deployment of classical commit protocols (such as two-phase commit) does not reasonably extend to distributed infrastructure-based mobile environments driving the need for perturbation-resilient commit protocols. In this article, we comprehensively consider and classify the perturbations of the wireless infrastructure-based mobile environment according to their impact on the outcome of commit protocols and on the resource blocking times. For each identified perturbation class a commit solution is provided. Consolidating these subsolutions, we develop a family of fault-tolerant atomic commit protocols that are tunable to meet the desired perturbation needs and provide minimized resource blocking times and optimized transaction commit rates. The framework is also evaluated using simulations and an actual testbed deployment.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2070002126",
    "type": "article"
  },
  {
    "title": "Reining in Long Tails in Warehouse-Scale Computers with Quick Voltage Boosting Using Adrenaline",
    "doi": "https://doi.org/10.1145/3054742",
    "publication_date": "2017-02-28",
    "publication_year": 2017,
    "authors": "Chang-Hong Hsu; Yunqi Zhang; Michael A. Laurenzano; David Meisner; Thomas F. Wenisch; Ronald Dreslinski; Jason Mars; Lingjia Tang",
    "corresponding_authors": "",
    "abstract": "Reducing the long tail of the query latency distribution in modern warehouse scale computers is critical for improving performance and quality of service (QoS) of workloads such as Web Search and Memcached. Traditional turbo boost increases a processor’s voltage and frequency during a coarse-grained sliding window, boosting all queries that are processed during that window. However, the inability of such a technique to pinpoint tail queries for boosting limits its tail reduction benefit. In this work, we propose Adrenaline , an approach to leverage finer-granularity (tens of nanoseconds) voltage boosting to effectively rein in the tail latency with query-level precision. Two key insights underlie this work. First, emerging finer granularity voltage/frequency boosting is an enabling mechanism for intelligent allocation of the power budget to precisely boost only the queries that contribute to the tail latency; second, per-query characteristics can be used to design indicators for proactively pinpointing these queries, triggering boosting accordingly. Based on these insights, Adrenaline effectively pinpoints and boosts queries that are likely to increase the tail distribution and can reap more benefit from the voltage/frequency boost. By evaluating under various workload configurations, we demonstrate the effectiveness of our methodology. We achieve up to a 2.50 × tail latency improvement for Memcached and up to a 3.03 × for Web Search over coarse-grained dynamic voltage and frequency scaling (DVFS) given a fixed boosting power budget. When optimizing for energy reduction, Adrenaline achieves up to a 1.81 × improvement for Memcached and up to a 1.99 × for Web Search over coarse-grained DVFS. By using the carefully chosen boost thresholds, Adrenaline further improves the tail latency reduction to 4.82 × over coarse-grained DVFS.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2599706750",
    "type": "article"
  },
  {
    "title": "Supercloud",
    "doi": "https://doi.org/10.1145/3132038",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Zhiming Shen; Qin Jia; Gur-Eyal Sela; Weijia Song; Hakim Weatherspoon; Robbert van Renesse",
    "corresponding_authors": "",
    "abstract": "Infrastructure-as-a-Service (IaaS) cloud providers hide available interfaces for virtual machine (VM) placement and migration, CPU capping, memory ballooning, page sharing, and I/O throttling, limiting the ways in which applications can optimally configure resources or respond to dynamically shifting workloads. Given these interfaces, applications could migrate VMs in response to diurnal workloads or changing prices, adjust resources in response to load changes, and so on. This article proposes a new abstraction that we call a Library Cloud and that allows users to customize the diverse available cloud resources to best serve their applications. We built a prototype of a Library Cloud that we call the Supercloud . The Supercloud encapsulates applications in a virtual cloud under users’ full control and can incorporate one or more availability zones within a cloud provider or across different providers. The Supercloud provides virtual machine, storage, and networking complete with a full set of management operations, allowing applications to optimize performance. In this article, we demonstrate various innovations enabled by the Library Cloud.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2761383491",
    "type": "article"
  },
  {
    "title": "An Instruction Set Architecture for Machine Learning",
    "doi": "https://doi.org/10.1145/3331469",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "Yunji Chen; Huiying Lan; Zidong Du; Shaoli Liu; Jinhua Tao; Dong Seog Han; Tao Luo; Qi Guo; Ling Li; Yuan Xie; Tianshi Chen",
    "corresponding_authors": "",
    "abstract": "Machine Learning (ML) are a family of models for learning from the data to improve performance on a certain task. ML techniques, especially recent renewed neural networks (deep neural networks), have proven to be efficient for a broad range of applications. ML techniques are conventionally executed on general-purpose processors (such as CPU and GPGPU), which usually are not energy efficient, since they invest excessive hardware resources to flexibly support various workloads. Consequently, application-specific hardware accelerators have been proposed recently to improve energy efficiency. However, such accelerators were designed for a small set of ML techniques sharing similar computational patterns, and they adopt complex and informative instructions (control signals) directly corresponding to high-level functional blocks of an ML technique (such as layers in neural networks) or even an ML as a whole. Although straightforward and easy to implement for a limited set of similar ML techniques, the lack of agility in the instruction set prevents such accelerator designs from supporting a variety of different ML techniques with sufficient flexibility and efficiency. In this article, we first propose a novel domain-specific Instruction Set Architecture (ISA) for NN accelerators, called Cambricon, which is a load-store architecture that integrates scalar, vector, matrix, logical, data transfer, and control instructions, based on a comprehensive analysis of existing NN techniques. We then extend the application scope of Cambricon from NN to ML techniques. We also propose an assembly language, an assembler, and runtime to support programming with Cambricon, especially targeting large-scale ML problems. Our evaluation over a total of 16 representative yet distinct ML techniques have demonstrated that Cambricon exhibits strong descriptive capacity over a broad range of ML techniques and provides higher code density than general-purpose ISAs such as x86, MIPS, and GPGPU. Compared to the latest state-of-the-art NN accelerator design DaDianNao [7] (which can only accommodate three types of NN techniques), our Cambricon-based accelerator prototype implemented in TSMC 65nm technology incurs only negligible latency/power/area overheads, with a versatile coverage of 10 different NN benchmarks and 7 other ML benchmarks. Compared to the recent prevalent ML accelerator PuDianNao, our Cambricon-based accelerator is able to support all the ML techniques as well as the 10 NNs but with only approximate 5.1% performance loss.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2968029517",
    "type": "article"
  },
  {
    "title": "Identifying Power-Efficient Multicore Cache Hierarchies via Reuse Distance Analysis",
    "doi": "https://doi.org/10.1145/2851503",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "Michael Badamo; Jeff Casarona; Minshu Zhao; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "To enable performance improvements in a power-efficient manner, computer architects have been building CPUs that exploit greater amounts of thread-level parallelism. A key consideration in such CPUs is properly designing the on-chip cache hierarchy. Unfortunately, this can be hard to do, especially for CPUs with high core counts and large amounts of cache. The enormous design space formed by the combinatorial number of ways in which to organize the cache hierarchy makes it difficult to identify power-efficient configurations. Moreover, the problem is exacerbated by the slow speed of architectural simulation, which is the primary means for conducting such design space studies. A powerful tool that can help architects optimize CPU cache hierarchies is reuse distance (RD) analysis. Recent work has extended uniprocessor RD techniques-i.e., by introducing concurrent RD and private-stack RD profiling—to enable analysis of different types of caches in multicore CPUs. Once acquired, parallel locality profiles can predict the performance of numerous cache configurations, permitting highly efficient design space exploration. To date, existing work on multicore RD analysis has focused on developing the profiling techniques and assessing their accuracy. Unfortunately, there has been no work on using RD analysis to optimize CPU performance or power consumption. This article investigates applying multicore RD analysis to identify the most power efficient cache configurations for a multicore CPU. First, we develop analytical models that use the cache-miss counts from parallel locality profiles to estimate CPU performance and power consumption. Although future scalable CPUs will likely employ multithreaded (and even out-of-order) cores, our current study assumes single-threaded in-order cores to simplify the models, allowing us to focus on the cache hierarchy and our RD-based techniques. Second, to demonstrate the utility of our techniques, we apply our models to optimize a large-scale tiled CPU architecture with a two-level cache hierarchy. We show that the most power efficient configuration varies considerably across different benchmarks, and that our locality profiles provide deep insights into why certain configurations are power efficient. We also show that picking the best configuration can provide significant gains, as there is a 2.01x power efficiency spread across our tiled CPU design space. Finally, we validate the accuracy of our techniques using detailed simulation. Among several simulated configurations, our techniques can usually pick the most power efficient configuration, or one that is very close to the best. In addition, across all simulated configurations, we can predict power efficiency with 15.2% error.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2326460788",
    "type": "article"
  },
  {
    "title": "Protocol Responsibility Offloading to Improve TCP Throughput in Virtualized Environments",
    "doi": "https://doi.org/10.1145/2518037.2491463",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Sahan Gamage; Ramana Rao Kompella; Dongyan Xu; Ardalan Kangarlou",
    "corresponding_authors": "",
    "abstract": "Virtualization is a key technology that powers cloud computing platforms such as Amazon EC2. Virtual machine (VM) consolidation, where multiple VMs share a physical host, has seen rapid adoption in practice, with increasingly large numbers of VMs per machine and per CPU core. Our investigations, however, suggest that the increasing degree of VM consolidation has serious negative effects on the VMs' TCP performance. As multiple VMs share a given CPU, the scheduling latencies, which can be in the order of tens of milliseconds, substantially increase the typically submillisecond round-trip times (RTTs) for TCP connections in a datacenter, causing significant degradation in throughput. In this article, we propose a lightweight solution, called vPRO, that (a) offloads the VM's TCP congestion control function to the driver domain to improve TCP transmit performance; and (b) offloads TCP acknowledgment functionality to the driver domain to improve the TCP receive performance. Our evaluation of a vPRO prototype on Xen suggests that vPRO substantially improves TCP receive and transmit throughputs with minimal per-packet CPU overhead. We further show that the higher TCP throughput leads to improvement in application-level performance, via experiments with Apache Olio, a Web 2.0 cloud application, and Intel MPI benchmark.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4234714894",
    "type": "article"
  },
  {
    "title": "Enabling Anonymous Online Streaming Analytics at the Network Edge",
    "doi": "https://doi.org/10.1145/3746130",
    "publication_date": "2025-06-23",
    "publication_year": 2025,
    "authors": "Yunming Xiao; Yanqi Gu; Yibo Zhao; Sen Lin; Aleksandar Kuzmanovic",
    "corresponding_authors": "",
    "abstract": "In recent years, content hyper-giants have increasingly deployed server infrastructure and services close to end-users within ”eyeball” networks. Still, online streaming analytics has largely remained unaffected by this trend. This is despite the fact that most of the “big data” is received in real-time and is most valuable at the time of arrival. The inability to process data at the network edge is caused by a common setting where user profiles, necessary for analytics, are stored deep in the data center backends. This setting also carries privacy concerns as such user profiles are individually identifiable, yet the users are almost blind to what data is associated with their identities and how the data is analyzed. In this paper, we revise this arrangement, and plant encrypted semantic cookies at the user end. By redesigning the cookie content without altering existing protocols, semantic cookies enable the capture and pre-processing of user data at edge ISPs or CDNs while preserving user anonymity. Additionally, lightweight cryptographic algorithms like partially homomorphic encryption can protect web providers’ proprietary data from CDNs. We present Snatch , a QUIC-based streaming analytics prototype that achieves up to 200x faster user analytics, with common-case improvements of 10-30x.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411535333",
    "type": "article"
  },
  {
    "title": "Whole-system Persistence Made Efficient with Tree-structured Checkpointing on Microkernel",
    "doi": "https://doi.org/10.1145/3742425",
    "publication_date": "2025-06-25",
    "publication_year": 2025,
    "authors": "Mingkai Dong; Fangnuo Wu; Gequan Mo; Haibo Chen",
    "corresponding_authors": "",
    "abstract": "Whole-system persistence promises simplified application deployment and near-instantaneous recovery. This can be implemented using single-level store (SLS) through periodic checkpointing of ephemeral state to persistent devices. However, traditional SLSs suffer from two main issues on checkpointing efficiency and external synchrony, which are critical for low-latency services with persistence need. In this article, we note that the decentralized state of microkernel-based systems can be exploited to simplify and optimize state checkpointing. To this end, we propose TreeSLS, a whole-system persistent microkernel that simplifies the whole-system state maintenance to a capability tree and a failure-resilient checkpoint manager. TreeSLS further exploits the emerging non-volatile memory to minimize checkpointing pause time by eliminating the distinction between ephemeral and persistent devices. With efficient state maintenance, TreeSLS further proposes delayed external visibility to provide transparent external synchrony with little overhead. Evaluation on microbenchmarks and real-world applications (e.g., Memcached, Redis, and RocksDB) show that TreeSLS can complete a whole-system persistence in around 100 μs and even take a checkpoint every 1 ms with reasonable overhead to applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411652475",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on SOSP 2023",
    "doi": "https://doi.org/10.1145/3744676",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "Jason Flinn; Margo Seltzer",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412913510",
    "type": "article"
  },
  {
    "title": "Real-time, Work-conserving GPU Scheduling for Concurrent DNN Inference",
    "doi": "https://doi.org/10.1145/3768622",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Mingcong Han; Rong Chen; Weihang Shen; Hanze Zhang; Jianguang Yang; Haibo Chen",
    "corresponding_authors": "",
    "abstract": "Many intelligent applications, such as autonomous driving and virtual reality, require running both latency-critical (real-time) and best-effort deep neural network (DNN) inference tasks to achieve both real-time and work-conserving on the GPU. However, commodity GPUs lack efficient preemptive scheduling support, and existing state-of-the-art approaches either have to monopolize GPU or let real-time tasks to wait for best-effort tasks to complete, resulting in low utilization, high latency, or both. This paper presents Reef , the first GPU-accelerated DNN inference serving system that achieves low-latency and work-conserving for concurrent real-time and best-effort tasks. Reef accomplishes this by enabling microsecond-scale kernel preemption and controlled concurrent execution in GPU scheduling. Reef is novel in two ways. First, based on the observation that DNN inference kernels are mostly idempotent, Reef devises a reset-based preemption scheme that launches a real-time kernel on the GPU by proactively killing and restoring best-effort kernels at microsecond-scale. Second, since DNN inference kernels have varied parallelism and predictable latency, Reef proposes a dynamic kernel padding mechanism that dynamically pads the real-time kernel with appropriate best-effort kernels to fully utilize the GPU with negligible overhead. Evaluation using a new DNN inference serving benchmark (DISB) with diverse workloads and a real-world trace on both NVIDIA and AMD GPUs shows that Reef only incurs less than 5% overhead in end-to-end latency for real-time tasks but increases the overall throughput by up to 1.53 ×, compared to scheduling tasks sequentially. To demonstrate the practical benefits of our approach, we compare Reef with Triton, a widely-adopted production-level serving system. Our evaluation shows that Reef outperforms Triton by 1.12 × to 5.20 × in end-to-end latency for real-time tasks, while maintaining comparable throughput.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414332261",
    "type": "article"
  },
  {
    "title": "A new approach to I/O performance evaluation",
    "doi": "https://doi.org/10.1145/195792.195812",
    "publication_date": "1994-11-01",
    "publication_year": 1994,
    "authors": "Peter M. Chen; David A. Patterson",
    "corresponding_authors": "",
    "abstract": "Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete; they do not stress the I/O system; and they do not help much in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, the benchmark automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of five workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to estimate quickly the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue that this method gives a far more accurate comparative performance evaluation than traditional single-point benchmarks. We apply our new evaluation technique by measuring a SPARCstation 1+ with one SCSI disk, an HP 730 with one SCSI-II disk, a DECstation 5000/200 running the Sprite LFS operating system with a three-disk disk array, a Convex C240 minisupercomputer with a four-disk disk array, and a Solbourne 5E/905 fileserver with a two-disk disk array.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1991807219",
    "type": "article"
  },
  {
    "title": "Architectural and compiler support for effective instruction prefetching",
    "doi": "https://doi.org/10.1145/367742.367786",
    "publication_date": "2001-02-01",
    "publication_year": 2001,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Instruction cache miss latency is becoming an increasingly important performance bottleneck, especially for commercial applications. Although instruction prefetching is an attractive technique for tolerating this latency, we find that existing prefetching schemes are insufficient for modern superscalar processors, since they fail to issue prefetches early enough (particularly for nonsequential accesses). To overcome these limitations, we propose a new instruction prefetching technique whereby the hardware and software cooperate to hide the latency as follows. The hardware performs aggressive sequential prefetching combined with a novel prefetch filtering mechanism to allow it to get far ahead without polluting the cache. To hide the latency of nonsequential accesses, we propose and implement a novel compiler algorithm which automatically inserts instruction-prefetch the targets of control transfers far enough in advance. Our experimental results demonstrate that this new approach hides 50% or more tof the latecy remaining with the best previous techniques, while at the same time reduces the number of useless prefetches by a factor of six. We find that both the prefetch filtering and compiler-inserted prefetching components of our design are essential and complementary, and that the compiler can limit the code expansion to only 9% on average. In addition, we show that the performance of our technique can be further increased by using profiling information to help reduce cache conflicts and unnecessary prefetches. From an architectural perspective, these performance advantages are sustained over a range of common miss latencies and bandwidth. Finally, our technique is cost effective as well, since it delivers performance comparable to (or even better than) that of larger caches, but requires a much smaller hardware budget.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4251328264",
    "type": "article"
  },
  {
    "title": "Independent general principles for constructing responsive software systems",
    "doi": "https://doi.org/10.1145/6306.6307",
    "publication_date": "1986-02-10",
    "publication_year": 1986,
    "authors": "Connie U. Smith",
    "corresponding_authors": "Connie U. Smith",
    "abstract": "Three general principles are presented that can be applied in early software life cycle stages for the definition of software requirements and designs with acceptable performance. They are genuine high-level considerations for meeting responsiveness goals without sacrificing understandability and maintainability, and without increasing development time and cost. The principles are derived from the interrelationships of two performance models: a queueing network based on computer system model and an execution graph software model. The performance effect of each of the principles is quantified using the models. Examples are given that illustrate how they can be applied to software systems.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2083306603",
    "type": "article"
  },
  {
    "title": "Mechanisms that enforce bounds on packet lifetimes",
    "doi": "https://doi.org/10.1145/357377.357382",
    "publication_date": "1983-11-01",
    "publication_year": 1983,
    "authors": "Lansing Sloan",
    "corresponding_authors": "Lansing Sloan",
    "abstract": "article Free AccessMechanisms that enforce bounds on packet lifetimes Author: Lansing Sloan L-60, Lawrence Livermore National Laboratory, P.O. Box 808, Livermore, CA L-60, Lawrence Livermore National Laboratory, P.O. Box 808, Livermore, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 4pp 311–330https://doi.org/10.1145/357377.357382Published:01 November 1983Publication History 15citation341DownloadsMetricsTotal Citations15Total Downloads341Last 12 Months27Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1983934003",
    "type": "article"
  },
  {
    "title": "Interprocess communication and processor dispatching on the Intel 432",
    "doi": "https://doi.org/10.1145/357353.357358",
    "publication_date": "1983-02-01",
    "publication_year": 1983,
    "authors": "George W. Cox; William M. Corwin; Konrad Lai; Fred J. Pollack",
    "corresponding_authors": "",
    "abstract": "article Free AccessInterprocess communication and processor dispatching on the Intel 432 Authors: George W. Cox Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, ORView Profile , William M. Corwin Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, ORView Profile , Konrad K. Lai Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, ORView Profile , Fred J. Pollack Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, ORView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 1Issue 1Feb. 1983 pp 45–66https://doi.org/10.1145/357353.357358Published:01 February 1983Publication History 15citation847DownloadsMetricsTotal Citations15Total Downloads847Last 12 Months34Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2089782950",
    "type": "article"
  },
  {
    "title": "Improved latency and accuracy for neural branch prediction",
    "doi": "https://doi.org/10.1145/1062247.1062250",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Daniel A. Jiménez",
    "corresponding_authors": "Daniel A. Jiménez",
    "abstract": "Microarchitectural prediction based on neural learning has received increasing attention in recent years. However, neural prediction remains impractical because its superior accuracy over conventional predictors is not enough to offset the cost imposed by its high latency. We present a new neural branch predictor that solves the problem from both directions: it is both more accurate and much faster than previous neural predictors. Our predictor improves accuracy by combining path and pattern history to overcome limitations inherent to previous predictors. It also has much lower latency than previous neural predictors. The result is a predictor with accuracy far superior to conventional predictors but with latency comparable to predictors from industrial designs. Our simulations show that a path-based neural predictor improves the instructions-per-cycle (IPC) rate of an aggressively clocked microarchitecture by 16% over the original perceptron predictor. One reason for the improved accuracy is the ability of our new predictor to learn linearly inseparable branches; we show that these branches account for 50% of all branches and almost all branch mispredictions.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2012897812",
    "type": "article"
  },
  {
    "title": "Comprehensive multiprocessor cache miss rate generation using multivariate models",
    "doi": "https://doi.org/10.1145/1062247.1062248",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Ilya Gluhovsky; B. O'Krafka",
    "corresponding_authors": "",
    "abstract": "This article presents a technique for taking a sparse set of cache simulation data and fitting a multivariate model to fill in the missing points over a broad region of cache configurations. We extend previous work by its applicability to multiple miss rate components and its ability to model a wide range of cache parameters, including size, associativity and sharing. Miss rate models are useful for broad design exploration in which many cache configurations cannot be simulated directly due to limitations of trace collection setups or available resources. We show the effectiveness of the technique by applying it to two commercial workloads and presenting miss rate data for a broad design space with cache size, associativity, sharing and number of processors as variables. The fitted data match the simulation data very well. The various curves show how a miss rate model is useful for not only estimating the performance of specific configurations, but also for providing insight into miss rate trends. Furthermore, this modeling methodology is robust in the presence of corrupted simulation data and variations in simulation data from multiple sources.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1978443597",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1731060.1731061",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Peter M. Chen",
    "corresponding_authors": "Peter M. Chen",
    "abstract": "editorial Free AccessEditorial Author: Peter M. Chen ACM Transactions on Computer Systems ACM Transactions on Computer SystemsView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 28Issue 1March 2010 Article No.: 1pp 1–3https://doi.org/10.1145/1731060.1731061Published:04 August 2010Publication History 0citation337DownloadsMetricsTotal Citations0Total Downloads337Last 12 Months11Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2624958153",
    "type": "editorial"
  },
  {
    "title": "Energy Analysis of Hardware and Software Range Partitioning",
    "doi": "https://doi.org/10.1145/2638550",
    "publication_date": "2014-08-29",
    "publication_year": 2014,
    "authors": "Lisa Wu; Orestis Polychroniou; Raymond J. Barker; Martha A. Kim; Kenneth A. Ross",
    "corresponding_authors": "",
    "abstract": "Data partitioning is a critical operation for manipulating large datasets because it subdivides tasks into pieces that are more amenable to efficient processing. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries. This article measures the performance and energy of state-of-the-art software partitioners, and describes and evaluates a hardware range partitioner that further improves efficiency. The software implementation is broken into two phases, allowing separate analysis of the partition function computation and data shuffling costs. Although range partitioning is commonly thought to be more expensive than simpler strategies such as hash partitioning, our measurements indicate that careful data movement and optimization of the partition function can allow it to approach the throughput and energy consumption of hash or radix partitioning. For further acceleration, we describe a hardware range partitioner, or HARP, a streaming framework that offers a seamless execution environment for this and other streaming accelerators, and a detailed analysis of a 32nm physical design that matches the throughput of four to eight software threads while consuming just 6.9% of the area and 4.3% of the power of a Xeon core in the same technology generation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2018336519",
    "type": "article"
  },
  {
    "title": "Runtime identification of cache conflict misses",
    "doi": "https://doi.org/10.1145/502912.502913",
    "publication_date": "2001-11-01",
    "publication_year": 2001,
    "authors": "Jamison D. Collins; Dean M. Tullsen",
    "corresponding_authors": "",
    "abstract": "This paper describes the miss classification table, a simple mechanism that enables the processor or memory controller to identify each cache miss as either a conflict miss or a capacity (non-conflict) miss. The miss classification table works by storing part of the tag of the most recently evicted line of a cache set. If the next miss to that cache set has a matching tag, it is identified as a conflict miss. This technique correctly identifies 88% of misses.Several applications of this information are demonstrated, including improvements to victim caching, next-line prefetching, cache exclusion, and a pseudo-associative cache. This paper also presents the adaptive miss buffer (AMB), which combines several of these techniques, targeting each miss with the most appropriate optimization, all within a single small miss buffer. The AMB's combination of techniques achieves 16% better performance than any single technique alone.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2032319803",
    "type": "article"
  },
  {
    "title": "The NTree: a two dimension partial order for protection groups",
    "doi": "https://doi.org/10.1145/42186.42187",
    "publication_date": "1988-05-01",
    "publication_year": 1988,
    "authors": "Ravinderpal Singh Sandhu",
    "corresponding_authors": "Ravinderpal Singh Sandhu",
    "abstract": "The benefits of providing access control with groups of users rather than with individuals as the unit of granularity are well known. These benefits are enhanced if the groups are organized in a subgroup partial order. A class of such partial orders, called ntrees, is defined by using a forest of rooted trees or inverted rooted trees as basic partial orders and combining these by refinement. Refinement explodes an existing group into a partially ordered ntree of new groups while maintaining the same relationship between each new group and the nonexploded groups that the exploded group had. Examples are discussed to show the practical significance of ntrees and the refinement operation. It is shown that ntrees can be represented by assigning a pair of integers called lr-values to each group so that g is a subgroup of h if and only if l[g] ≤ l[h] and r[g] ≤ r[h]. Refinement allows a complex ntree to be developed incrementally in a top-down manner and is useful for the initial definition of an ntree as well as for subsequent modifications. To make the latter use of refinement practical, a method is presented for assigning lr-values to the new groups introduced by refinement so lr-values assigned to nonexploded groups need not be changed. It is also shown how to guarantee that the lr-values of the exploded group will get assigned to one of the new groups.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2096499366",
    "type": "article"
  },
  {
    "title": "An evaluation of speculative instruction execution on simultaneous multithreaded processors",
    "doi": "https://doi.org/10.1145/859716.859720",
    "publication_date": "2003-08-01",
    "publication_year": 2003,
    "authors": "Steven Swanson; Luke K. McDowell; Michael M. Swift; Susan J. Eggers; Henry M. Levy",
    "corresponding_authors": "",
    "abstract": "Modern superscalar processors rely heavily on speculative execution for performance. For example, our measurements show that on a 6-issue superscalar, 93% of committed instructions for SPECINT95 are speculative. Without speculation, processor resources on such machines would be largely idle. In contrast to superscalars, simultaneous multithreaded (SMT) processors achieve high resource utilization by issuing instructions from multiple threads every cycle. An SMT processor thus has two means of hiding latency: speculation and multithreaded execution. However, these two techniques may conflict; on an SMT processor, wrong-path speculative instructions from one thread may compete with and displace useful instructions from another thread. For this reason, it is important to understand the trade-offs between these two latency-hiding techniques, and to ask whether multithreaded processors should speculate differently than conventional superscalars.This paper evaluates the behavior of instruction speculation on SMT processors using both multiprogrammed (SPECINT and SPECFP) and multithreaded (the Apache Web server) workloads. We measure and analyze the impact of speculation and demonstrate how speculation on an 8-context SMT differs from superscalar speculation. We also examine the effect of speculation-aware fetch and branch prediction policies in the processor. Our results quantify the extent to which (1) speculation is critical to performance on a multithreaded processor because it ensures an ample supply of parallelism to feed the functional units, and (2) SMT actually enhances the effectiveness of speculative execution, compared to a superscalar processor by reducing the impact of branch misprediction. Finally, we quantify the impact of both hardware configuration and workload characteristics on speculation's usefulness and demonstrate that, in nearly all cases, speculation is beneficial to SMT performance.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2109706066",
    "type": "article"
  },
  {
    "title": "Stateful distributed interposition",
    "doi": "https://doi.org/10.1145/966785.966786",
    "publication_date": "2004-02-01",
    "publication_year": 2004,
    "authors": "John Reumann; Kang G. Shin",
    "corresponding_authors": "",
    "abstract": "Interposition-based system enhancements for multitiered servers are difficult to build because important system context is typically lost at application and machine boundaries. For example, resource quotas and user identities do not propagate easily between cooperating services that execute on different hosts or that communicate with each other via intermediary services. Application-transparent system enhancement is difficult to achieve when such context information is obscured by complex service interaction patterns. We propose a basic mechanism for sharing contextual information across the tiers of multitier computations to support system enhancement for multitier servers and applications.This article introduces generic, cluster-wide context as a new, configurable abstraction for the OS. System administrator- or application-specified context tracking rules determine how context is associated with system processes, sockets, messages, how it is relayed along the interapplication communication channels, and how it is to be interpreted by system interpositions, thus realizing Stateful Distributed Interposition.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2040156347",
    "type": "article"
  },
  {
    "title": "Performance analysis of redundant-path networks for multiprocessor systems",
    "doi": "https://doi.org/10.1145/214438.214443",
    "publication_date": "1985-05-01",
    "publication_year": 1985,
    "authors": "Krishnan Padmanabhan; Duncan H. Lawrie",
    "corresponding_authors": "",
    "abstract": "Performance of a class of multistage interconnection networks employing redundant paths is investigated. Redundant path networks provide significant tolerance to faults at minimal costs; in this paper improvements in performance and very graceful degradation are also shown to result from the availability of redundant paths. A Markov model is introduced for the operation of these networks in the circuit-switched mode and is solved numerically to obtain the performance measures of interest. The structure of the networks that provide maximal performance is also characterized.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2079996780",
    "type": "article"
  },
  {
    "title": "Cluster communication protocols for parallel-programming systems",
    "doi": "https://doi.org/10.1145/1012268.1012269",
    "publication_date": "2004-08-01",
    "publication_year": 2004,
    "authors": "Kees Verstoep; R.A.F. Bhoedjang; Tim Rühl; Henri E. Bal; Rutger F. H. Hofman",
    "corresponding_authors": "",
    "abstract": "Clusters of workstations are a popular platform for high-performance computing. For many parallel applications, efficient use of a fast interconnection network is essential for good performance. Several modern System Area Networks include programmable network interfaces that can be tailored to perform protocol tasks that otherwise would need to be done by the host processors. Finding the right trade-off between protocol processing at the host and the network interface is difficult in general. In this work, we systematically evaluate the performance of different implementations of a single, user-level communication interface. The implementations make different architectural assumptions about the reliability of the network and the capabilities of the network interface. The implementations differ accordingly in their division of protocol tasks between host software, network-interface firmware, and network hardware. Also, we investigate the effects of alternative data-transfer methods and multicast implementations, and we evaluate the influence of packet size. Using microbenchmarks, parallel-programming systems, and parallel applications, we assess the performance of the different implementations at multiple levels. We use two hardware platforms with different performance characteristics to validate our conclusions. We show how moving protocol tasks to a relatively slow network interface can yield both performance advantages and disadvantages, depending on specific characteristics of the application and the underlying parallel-programming system.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1977525878",
    "type": "article"
  },
  {
    "title": "Deca",
    "doi": "https://doi.org/10.1145/3310361",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Xuanhua Shi; Zhixiang Ke; Yongluan Zhou; Hai Jin; Lu Lu; Xiong Zhang; Ligang He; Zhenyu Hu; Fei Wang",
    "corresponding_authors": "",
    "abstract": "In-memory caching of intermediate data and active combining of data in shuffle buffers have been shown to be very effective in minimizing the re-computation and I/O cost in big data processing systems such as Spark and Flink. However, it has also been widely reported that these techniques would create a large amount of long-living data objects in the heap. These generated objects may quickly saturate the garbage collector, especially when handling a large dataset, and hence, limit the scalability of the system. To eliminate this problem, we propose a lifetime-based memory management framework, which, by automatically analyzing the user-defined functions and data types, obtains the expected lifetime of the data objects, and then allo- cates and releases memory space accordingly to minimize the garbage collection overhead. In particular, we present Deca, a concrete implementation of our proposal on top of Spark, which transparently decomposes and groups objects with similar lifetimes into byte arrays and releases their space altogether when their lifetimes come to an end. When systems are processing very large data, Deca also provides field-oriented memory pages to ensure high compression efficiency. Extensive experimental studies using both synthetic and real datasets shows that, in comparing to Spark, Deca is able to 1) reduce the garbage collection time by up to 99.9%, 2) reduce the memory consumption by up to 46.6% and the storage space by 23.4%, 3) achieve 1.2x-22.7x speedup in terms of execution time in cases without data spilling and 16x-41.6x speedup in cases with data spilling, and 4) provide the similar performance comparing to domain specific systems.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2920946928",
    "type": "article"
  },
  {
    "title": "Apache REEF",
    "doi": "https://doi.org/10.1145/3132037",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Byung-Gon Chun; Tyson Condie; Yingda Chen; Brian Cho; Andrew Chung; Carlo Curino; Chris Douglas; Matteo Interlandi; Beomyeol Jeon; Joo Seong Jeong; Gyewon Lee; Yunseong Lee; Tony Majestro; Dahlia Malkhi; Sergiy Matusevych; Brandon Myers; Mariia Mykhailova; Shravan Narayanamurthy; Joseph Noor; Raghu Ramakrishnan; Sriram Rao; Russell Sears; Beysim Sezgin; Taegeon Um; Julia Wang; Markus Weimer; Youngseok Yang",
    "corresponding_authors": "",
    "abstract": "Resource Managers like YARN and Mesos have emerged as a critical layer in the cloud computing system stack, but the developer abstractions for leasing cluster resources and instantiating application logic are very low level. This flexibility comes at a high cost in terms of developer effort, as each application must repeatedly tackle the same challenges (e.g., fault tolerance, task scheduling and coordination) and reimplement common mechanisms (e.g., caching, bulk-data transfers). This article presents REEF, a development framework that provides a control plane for scheduling and coordinating task-level (data-plane) work on cluster resources obtained from a Resource Manager. REEF provides mechanisms that facilitate resource reuse for data caching and state management abstractions that greatly ease the development of elastic data processing pipelines on cloud platforms that support a Resource Manager service. We illustrate the power of REEF by showing applications built atop: a distributed shell application, a machine-learning framework, a distributed in-memory caching system, and a port of the CORFU system. REEF is currently an Apache top-level project that has attracted contributors from several institutions and it is being used to develop several commercial offerings such as the Azure Stream Analytics service.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2762551481",
    "type": "article"
  },
  {
    "title": "EOLE",
    "doi": "https://doi.org/10.1145/2870632",
    "publication_date": "2016-04-21",
    "publication_year": 2016,
    "authors": "Arthur Pérais; André Seznec",
    "corresponding_authors": "",
    "abstract": "Recent work in the field of value prediction (VP) has shown that given an efficient confidence estimation mechanism, prediction validation could be removed from the out-of-order engine and delayed until commit time. As a result, a simple recovery mechanism—pipeline squashing—can be used, whereas the out-of-order engine remains mostly unmodified. Yet, VP and validation at commit time require additional ports on the physical register file, potentially rendering the overall number of ports unbearable. Fortunately, VP also implies that many single-cycle ALU instructions have their operands predicted in the front-end and can be executed in-place, in-order. Similarly, the execution of single-cycle instructions whose result has been predicted can be delayed until commit time since predictions are validated at commit time. Consequently, a significant number of instructions—10% to 70% in our experiments—can bypass the out-of-order engine, allowing for a reduction of the issue width. This reduction paves the way for a truly practical implementation of VP. Furthermore, since VP in itself usually increases performance, our resulting {Early—Out-of-Order—Late} Execution architecture, EOLE, is often more efficient than a baseline VP-augmented 6-issue superscalar while having a significantly narrower 4-issue out-of-order engine.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2342178639",
    "type": "article"
  },
  {
    "title": "An effective synchronization network for hot-spot accesses",
    "doi": "https://doi.org/10.1145/146937.146938",
    "publication_date": "1992-08-01",
    "publication_year": 1992,
    "authors": "William Hsu; Pen-Chung Yew",
    "corresponding_authors": "",
    "abstract": "In large multiprocessor systems, fast synchronization is crucial for high performance. However, synchronization traffic tends to create “hot-spots” in shared memory and cause network congestion. Multistage shuffle-exchange networks have been proposed and built to handle synchronization traffic. Software combining schemes have also been proposed to relieve network congestion caused by hot-spots. However, multistage combining networks could be very expensive and software combining could be very slow. In this paper, we propose a single-stage combining network to handle synchronization traffic, which is separated from the regular memory traffic. A single-stage combining network has several advantages: (1) it is attractive from an implementation perspective because only one stage is needed(instead of log N stages); (2) Only one network is needed to handle both forward and returning requests; (3) combined requests are distributed evenly through the network—the wait buffer size is reduced; and (4) fast-finishing algorithms [30] can be used to shorten the network delay. Because of all these advantages, we show that a single-stage combining network gives good performance at a lower cost than a multistage combining network.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1977658637",
    "type": "article"
  },
  {
    "title": "Run-time support for distributed sharing in safe languages",
    "doi": "https://doi.org/10.1145/592637.592638",
    "publication_date": "2003-01-10",
    "publication_year": 2003,
    "authors": "Ying Hu; Weimin Yu; Anna L. Cox; Dan S. Wallach; Willy Zwaenepoel",
    "corresponding_authors": "",
    "abstract": "We present a new run-time system that supports object sharing in a distributed system. The key insight in this system is that a handle-based implementation of such a system enables efficient and transparent sharing of data with both fine- and coarse-grained access patterns. In addition, it supports efficient execution of garbage-collected programs. In contrast, conventional distributed shared memory (DSM) systems are limited to providing only one granularity with good performance, and have experienced difficulty in efficiently supporting garbage collection. A safe language, in which no pointer arithmetic is allowed, can transparently be compiled into a handle-based system and constitutes its preferred mode of use. A programmer can also directly use a handle-based programming model that avoids pointer arithmetic on the handles, and achieve the same performance but without the programming benefits of a safe programming language. This new run-time system, DOSA (Distributed Object Sharing Architecture), provides a shared object space abstraction rather than a shared address space abstraction. The key to its efficiency is the observation that a handle-based distributed implementation permits VM-based access and modification detection without suffering false sharing for fine-grained access patterns. We compare DOSA to TreadMarks, a conventional DSM system that is efficient at handling coarse-grained sharing. The performance of fine-grained applications and garbage-collected applications is considerably better than in TreadMarks, and the performance of coarse-grained applications is nearly as good as in TreadMarks. Inasmuch as the performance of such applications is already good in TreadMarks, we consider this an acceptable performance penalty.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1972815008",
    "type": "article"
  },
  {
    "title": "Determining Application-Specific Peak Power and Energy Requirements for Ultra-Low-Power Processors",
    "doi": "https://doi.org/10.1145/3148052",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Hari Cherupalli; Henry Duwe; Weidong Ye; Rakesh Kumar; John Sartori",
    "corresponding_authors": "",
    "abstract": "Many emerging applications such as the Internet of Things, wearables, implantables, and sensor networks are constrained by power and energy. These applications rely on ultra-low-power processors that have rapidly become the most abundant type of processor manufactured today. In the ultra-low-power embedded systems used by these applications, peak power and energy requirements are the primary factors that determine critical system characteristics, such as size, weight, cost, and lifetime. While the power and energy requirements of these systems tend to be application specific, conventional techniques for rating peak power and energy cannot accurately bound the power and energy requirements of an application running on a processor, leading to overprovisioning that increases system size and weight. In this article, we present an automated technique that performs hardware–software coanalysis of the application and ultra-low-power processor in an embedded system to determine application-specific peak power and energy requirements. Our technique provides more accurate, tighter bounds than conventional techniques for determining peak power and energy requirements. Also, unlike conventional approaches, our technique reports guaranteed bounds on peak power and energy independent of an application’s input set. Tighter bounds on peak power and energy can be exploited to reduce system size, weight, and cost.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2780799857",
    "type": "article"
  },
  {
    "title": "Highly Concurrent Latency-tolerant Register Files for GPUs",
    "doi": "https://doi.org/10.1145/3419973",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Mohammad Sadrosadati; Amirhossein Mirhosseini; Ali Hajiabadi; Seyed Borna Ehsani; Hajar Falahati; Hamid Sarbazi‐Azad; Mario Drumond; Babak Falsafi; Rachata Ausavarungnirun; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Graphics Processing Units (GPUs) employ large register files to accommodate all active threads and accelerate context switching. Unfortunately, register files are a scalability bottleneck for future GPUs due to long access latency, high power consumption, and large silicon area provisioning. Prior work proposes hierarchical register file to reduce the register file power consumption by caching registers in a smaller register file cache. Unfortunately, this approach does not improve register access latency due to the low hit rate in the register file cache. In this article, we propose the Latency-Tolerant Register File (LTRF) architecture to achieve low latency in a two-level hierarchical structure while keeping power consumption low. We observe that compile-time interval analysis enables us to divide GPU program execution into intervals with an accurate estimate of a warp’s aggregate register working-set within each interval. The key idea of LTRF is to prefetch the estimated register working-set from the main register file to the register file cache under software control, at the beginning of each interval, and overlap the prefetch latency with the execution of other warps. We observe that register bank conflicts while prefetching the registers could greatly reduce the effectiveness of LTRF. Therefore, we devise a compile-time register renumbering technique to reduce the likelihood of register bank conflicts. Our experimental results show that LTRF enables high-capacity yet long-latency main GPU register files, paving the way for various optimizations. As an example optimization, we implement the main register file with emerging high-density high-latency memory technologies, enabling 8× larger capacity and improving overall GPU performance by 34%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3120481279",
    "type": "article"
  },
  {
    "title": "Metron",
    "doi": "https://doi.org/10.1145/3465628",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Georgios P. Katsikas; Tom Barbette; Dejan Kostić; Gerald Q. Maguire; Rebecca Steinert",
    "corresponding_authors": "",
    "abstract": "Deployment of 100Gigabit Ethernet (GbE) links challenges the packet processing limits of commodity hardware used for Network Functions Virtualization (NFV). Moreover, realizing chained network functions (i.e., service chains) necessitates the use of multiple CPU cores, or even multiple servers, to process packets from such high speed links. Our system Metron jointly exploits the underlying network and commodity servers’ resources: ( i ) to offload part of the packet processing logic to the network, ( ii ) by using smart tagging to setup and exploit the affinity of traffic classes, and ( iii ) by using tag-based hardware dispatching to carry out the remaining packet processing at the speed of the servers’ cores, with zero inter-core communication. Moreover, Metron transparently integrates, manages, and load balances proprietary “blackboxes” together with Metron service chains. Metron realizes stateful network functions at the speed of 100GbE network cards on a single server, while elastically and rapidly adapting to changing workload volumes. Our experiments demonstrate that Metron service chains can coexist with heterogeneous blackboxes, while still leveraging Metron’s accurate dispatching and load balancing. In summary, Metron has ( i ) 2.75–8× better efficiency, up to ( ii ) 4.7× lower latency, and ( iii ) 7.8× higher throughput than OpenBox, a state-of-the-art NFV system.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3181288501",
    "type": "article"
  },
  {
    "title": "Adaptable concurrency control for atomic data types",
    "doi": "https://doi.org/10.1145/146937.146939",
    "publication_date": "1992-08-01",
    "publication_year": 1992,
    "authors": "M. Stella Atkins; M. Y. Coady",
    "corresponding_authors": "",
    "abstract": "In many distributed systems concurrent access is required to a shared object, where abstract object servers may incorporate type-specific properties to define consistency requirements. Each operation and its outcome is treated as an event, and conflicts may occur between different event types. Hence concurrency control and synchronization are required at the granularity of conflicting event types. With such a fine granularity of locking, the occurrence of conflicts is likely to be lower than with whole-object locking, so optimistic techniques become more attractive. This work describes the design, implementation, and performance of servers for a shared atomic object, a semiqueue, where each server employs either pessimistic or optimistic locking techniques on each conflicting event type. We compare the performance of a purely optimistic server, a purely pessimistic server, and a hybrid server which treats certain event types optimistically and others pessimistically, to demonstrate the most appropriate environment for using pessimistic, optimistic, or hybrid control. We show that the advantages of low overhead on optimistic locking at low conflict levels is offset at higher conflict levels by the wasted work done by aborted transactions. To achieve optimum performance over the whole range of conflict levels, an adaptable server is required, whereby the treatment of conflicting event types can be changed dynamically between optimistic and pessimistic, according to various criteria depending on the expected frequency of conflict. We describe our implementations of adaptable servers which may allocate concurrency control strategy on the basis of state information, the history of conflicts encountered, or by using preset transaction priorities. We show that the adaptable servers perform almost as well as the best of the purely optimistic, pessimistic, or hybrid servers under the whole range of conflict levels, showing the versatility and efficiency of the dynamic servers. Finally we outline a general design methodology for implementing adaptable concurrency control in servers for atomic objects, illustrated using an atomic shared B-tree.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2048853312",
    "type": "article"
  },
  {
    "title": "The development and proof of a formal specification for a multilevel secure system",
    "doi": "https://doi.org/10.1145/13677.22724",
    "publication_date": "1987-03-01",
    "publication_year": 1987,
    "authors": "Janice Glasgow; Glenn H. MacEwen",
    "corresponding_authors": "",
    "abstract": "This paper describes current work on the design and specification of a multilevel secure distributed system called SNet. It discusses security models in general, the various problems of information flows in SNet, and the abstract and concrete security model components for SNet. It also introduces Lucid as a language for specifying distributed systems. The model components are expressed in Lucid; these Lucid partial specifications are shown to be correct with respect to the formal model, and the two model components are shown to be consistent. The complete functional specification of SNet in Lucid, its implementation in Concurrent Euclid, and the verification of the implementation with respect to the Lucid specification are not discussed.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2038132387",
    "type": "article"
  },
  {
    "title": "Cryptography as an operating system service",
    "doi": "https://doi.org/10.1145/1124153.1124154",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Angelos D. Keromytis; Jason L. Wright; Theo de Raadt; Matthew Burnside",
    "corresponding_authors": "",
    "abstract": "Cryptographic transformations are a fundamental building block in many security applications and protocols. To improve performance, several vendors market hardware accelerator cards. However, until now no operating system provided a mechanism that allowed both uniform and efficient use of this new type of resource.We present the OpenBSD Cryptographic Framework (OCF), a service virtualization layer implemented inside the operating system kernel, that provides uniform access to accelerator functionality by hiding card-specific details behind a carefully designed API. We evaluate the impact of the OCF in a variety of benchmarks, measuring overall system performance, application throughput and latency, and aggregate throughput when multiple applications make use of it.We conclude that the OCF is extremely efficient in utilizing cryptographic accelerator functionality, attaining 95% of the theoretical peak device performance and over 800 Mbps aggregate throughput using 3DES. We believe that this validates our decision to opt for ease of use by applications and kernel components through a uniform API and for seamless support for new accelerators. Furthermore, our evaluation points to several bottlenecks in system and operating system design: data copying between user and kernel modes, PCI bus signaling inefficiency, protocols that use small data units, and single-threaded applications. We identify some of these limitations through a set of measurements focusing on application-layer cryptographic protocols such as SSL. We offer several suggestions for improvements and directions for future work. We provide experimental evidence of the effectiveness of a new approach which we call operating system shortcutting. Shortcutting can improve the performance of application-layer cryptographic protocols by 27% with very small changes to the kernel.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2159862054",
    "type": "article"
  },
  {
    "title": "Incrementally parallelizing database transactions with thread-level speculation",
    "doi": "https://doi.org/10.1145/1328671.1328673",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "Christopher B. Colohan; Anastassia Ailamaki; J. Gregory Steffan; Todd C. Mowry",
    "corresponding_authors": "",
    "abstract": "With the advent of chip multiprocessors, exploiting intratransaction parallelism in database systems is an attractive way of improving transaction performance. However, exploiting intratransaction parallelism is difficult for two reasons: first, significant changes are required to avoid races or conflicts within the DBMS; and second, adding threads to transactions requires a high level of sophistication from transaction programmers. In this article we show how dividing a transaction into speculative threads solves both problems—it minimizes the changes required to the DBMS, and the details of parallelization are hidden from the transaction programmer. Our technique requires a limited number of small, localized changes to a subset of the low-level data structures in the DBMS. Through this method of incrementally parallelizing transactions, we can dramatically improve performance: on a simulated four-processor chip-multiprocessor, we improve the response time by 44--66% for three of the five TPC-C transactions, assuming the availability of idle processors.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2010221388",
    "type": "article"
  },
  {
    "title": "A programmable memory controller for the DDRx interfacing standards",
    "doi": "https://doi.org/10.1145/2534845",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Mahdi Nazm Bojnordi; Engin İpek",
    "corresponding_authors": "",
    "abstract": "Modern memory controllers employ sophisticated address mapping, command scheduling, and power management optimizations to alleviate the adverse effects of DRAM timing and resource constraints on system performance. A promising way of improving the versatility and efficiency of these controllers is to make them programmable—a proven technique that has seen wide use in other control tasks, ranging from DMA scheduling to NAND Flash and directory control. Unfortunately, the stringent latency and throughput requirements of modern DDRx devices have rendered such programmability largely impractical, confining DDRx controllers to fixed-function hardware. This article presents the instruction set architecture (ISA) and hardware implementation of PARDIS, a programmable memory controller that can meet the performance requirements of a high-speed DDRx interface. The proposed controller is evaluated by mapping previously proposed DRAM scheduling, address mapping, refresh scheduling, and power management algorithms onto PARDIS. Simulation results show that the average performance of PARDIS comes within 8% of fixed-function hardware for each of these techniques; moreover, by enabling application-specific optimizations, PARDIS improves system performance by 6 to 17% and reduces DRAM energy by 9 to 22% over four existing memory controllers.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2090537601",
    "type": "article"
  },
  {
    "title": "The Hipster Approach for Improving Cloud System Efficiency",
    "doi": "https://doi.org/10.1145/3144168",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Rajiv Nishtala; Paul Carpenter; Vinícius Petrucci; Xavier Martorell",
    "corresponding_authors": "",
    "abstract": "In 2013, U.S. data centers accounted for 2.2% of the country’s total electricity consumption, a figure that is projected to increase rapidly over the next decade. Many important data center workloads in cloud computing are interactive, and they demand strict levels of quality-of-service (QoS) to meet user expectations, making it challenging to optimize power consumption along with increasing performance demands. This article introduces Hipster, a technique that combines heuristics and reinforcement learning to improve resource efficiency in cloud systems. Hipster explores heterogeneous multi-cores and dynamic voltage and frequency scaling for reducing energy consumption while managing the QoS of the latency-critical workloads. To improve data center utilization and make best usage of the available resources, Hipster can dynamically assign remaining cores to batch workloads without violating the QoS constraints for the latency-critical workloads. We perform experiments using a 64-bit ARM big.LITTLE platform and show that, compared to prior work, Hipster improves the QoS guarantee for Web-Search from 80% to 96%, and for Memcached from 92% to 99%, while reducing the energy consumption by up to 18%. Hipster is also effective in learning and adapting automatically to specific requirements of new incoming workloads just enough to meet the QoS and optimize resource consumption.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2775600496",
    "type": "article"
  },
  {
    "title": "Transactuations",
    "doi": "https://doi.org/10.1145/3380907",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Tanakorn Leesatapornwongsa; Aritra Sengupta; Masoud Saeida Ardekani; Gustavo Petri; Cesar A. Stuardo",
    "corresponding_authors": "",
    "abstract": "A large class of IoT applications read sensors, execute application logic, and actuate actuators. However, the lack of high-level programming abstractions compromises correctness, especially in the presence of failures and unwanted interleaving between applications. A key problem arises when operations on IoT devices or the application itself fails, which leads to inconsistencies between the physical state and application state, breaking application semantics and causing undesired consequences. Transactions are a well-established abstraction for correctness, but assume properties that are absent in an IoT context. In this article, we study one such environment, smart home, and establish inconsistencies manifesting out of failures. We propose an abstraction called transactuation that empowers developers to build reliable applications. Our runtime, Relacs , implements the abstraction atop a real smart-home platform. We evaluate programmability, performance, and effectiveness of transactuations to demonstrate its potential as a powerful abstraction and execution model.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3033721587",
    "type": "article"
  },
  {
    "title": "A Simulation Software for the Evaluation of Vulnerabilities in Reputation Management Systems",
    "doi": "https://doi.org/10.1145/3458510",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Vincenzo Agate; Alessandra De Paola; Giuseppe Lo Re; Marco Morana",
    "corresponding_authors": "",
    "abstract": "Multi-agent distributed systems are characterized by autonomous entities that interact with each other to provide, and/or request, different kinds of services. In several contexts, especially when a reward is offered according to the quality of service, individual agents (or coordinated groups) may act in a selfish way. To prevent such behaviours, distributed Reputation Management Systems (RMSs) provide every agent with the capability of computing the reputation of the others according to direct past interactions, as well as indirect opinions reported by their neighbourhood. This last point introduces a weakness on gossiped information that makes RMSs vulnerable to malicious agents’ intent on disseminating false reputation values. Given the variety of application scenarios in which RMSs can be adopted, as well as the multitude of behaviours that agents can implement, designers need RMS evaluation tools that allow them to predict the robustness of the system to security attacks, before its actual deployment. To this aim, we present a simulation software for the vulnerability evaluation of RMSs and illustrate three case studies in which this tool was effectively used to model and assess state-of-the-art RMSs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3170760477",
    "type": "article"
  },
  {
    "title": "Mechanistic Modeling of Architectural Vulnerability Factor",
    "doi": "https://doi.org/10.1145/2669364",
    "publication_date": "2015-01-20",
    "publication_year": 2015,
    "authors": "Arun Arvind Nair; Stijn Eyerman; Jian Chen; Lizy K. John; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Reliability to soft errors is a significant design challenge in modern microprocessors owing to an exponential increase in the number of transistors on chip and the reduction in operating voltages with each process generation. Architectural Vulnerability Factor (AVF) modeling using microarchitectural simulators enables architects to make informed performance, power, and reliability tradeoffs. However, such simulators are time-consuming and do not reveal the microarchitectural mechanisms that influence AVF. In this article, we present an accurate first-order mechanistic analytical model to compute AVF, developed using the first principles of an out-of-order superscalar execution. This model provides insight into the fundamental interactions between the workload and microarchitecture that together influence AVF. We use the model to perform design space exploration, parametric sweeps, and workload characterization for AVF.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2124664738",
    "type": "article"
  },
  {
    "title": "Assisting Static Compiler Vectorization with a Speculative Dynamic Vectorizer in an HW/SW Codesigned Environment",
    "doi": "https://doi.org/10.1145/2807694",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Rakesh Kumar; A. Martínez; Antonio González",
    "corresponding_authors": "",
    "abstract": "Compiler-based static vectorization is used widely to extract data-level parallelism from computation-intensive applications. Static vectorization is very effective in vectorizing traditional array-based applications. However, compilers’ inability to do accurate interprocedural pointer disambiguation and interprocedural array dependence analysis severely limits vectorization opportunities. HW/SW codesigned processors provide an excellent opportunity to optimize the applications at runtime. The availability of dynamic application behavior at runtime helps in capturing vectorization opportunities generally missed by the compilers. This article proposes to complement the static vectorization with a speculative dynamic vectorizer in an HW/SW codesigned processor. We present a speculative dynamic vectorization algorithm that speculatively reorders ambiguous memory references to uncover vectorization opportunities. The speculative reordering of memory instructions avoids the need for accurate interprocedural pointer disambiguation and interprocedural array dependence analysis. The hardware checks for any memory dependence violation due to speculative vectorization and takes corrective action in case of violation. Our experiments show that the combined (static + dynamic) vectorization approach provides a 2× performance benefit compared to the static GCC vectorization alone, for SPECFP2006. Furthermore, the speculative dynamic vectorizer is able to vectorize 48% of the loops that ICC failed to vectorize due to conservative dependence analysis in the TSVC benchmark suite. Moreover, the dynamic vectorization scheme is as effective in vectorization of pointer-based applications as for the array-based ones, whereas compilers lose significant vectorization opportunities in pointer-based applications. Furthermore, we show that speculation is not only a luxury but also a necessity for runtime vectorization.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2229545145",
    "type": "article"
  },
  {
    "title": "Eliminating synchronization overhead in automatically parallelized programs using dynamic feedback",
    "doi": "https://doi.org/10.1145/312203.312210",
    "publication_date": "1999-05-01",
    "publication_year": 1999,
    "authors": "Pedro C. Diniz; Martin Rinard",
    "corresponding_authors": "",
    "abstract": "This article presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment. We have implemented dynamic feedback in the context of a parallelizing compiler for object-based programs. The generated code uses dynamic feedback to automatically choose the best synchronization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies from program to program, that the compiler is unable to statically choose the best policy, and that dynamic feedback enables the generted code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2151855433",
    "type": "article"
  },
  {
    "title": "Authentication in distributed systems",
    "doi": "https://doi.org/10.1145/138873.138874",
    "publication_date": "1992-11-01",
    "publication_year": 1992,
    "authors": "Butler Lampson; Martı́n Abadi; Michael T. Burrows; Edward Wobber",
    "corresponding_authors": "",
    "abstract": "We describe a theory of authentication and a system that implements it. Our theory is based on the notion of principal and a “speaks for” relation between principals. A simple principal either has a name or is a communication channel; a compound principal can express an adopted role or delegated authority. The theory shows how to reason about a principal's authority by deducing the other principals that it can speak for; authenticating a channel is one important application. We use the theory to explain many existing and proposed security mechanisms. In particular, we describe the system we have built. It passes principals efficiently as arguments or results of remote procedure calls, and it handles public and shared key encryption, name lookup in a large name space, groups of principals, program loading, delegation, access control, and revocation.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2413451779",
    "type": "article"
  },
  {
    "title": "A digital multisignature scheme using bijective public-key cryptosystem",
    "doi": null,
    "publication_date": "1988-01-01",
    "publication_year": 1988,
    "authors": "T. Okamoto",
    "corresponding_authors": "T. Okamoto",
    "abstract": "",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2994327826",
    "type": "article"
  },
  {
    "title": "Multigrain shared memory",
    "doi": "https://doi.org/10.1145/350853.350871",
    "publication_date": "2000-05-01",
    "publication_year": 2000,
    "authors": "Donald Yeung; John Kubiatowicz; Anant Agarwal",
    "corresponding_authors": "",
    "abstract": "Parallel workstations, each comprising tens of processors based on shared memory, promise cost-effective scalable multiprocessing. This article explores the coupling of such small- to medium-scale shared-memory multiprocessors through software over a local area network to synthesize larger shared-memory systems. We call these systems Distributed Shared-memory MultiProcessors (DSMPs). This article introduces the design of a shared-memory system that uses multiple granularities of sharing, called MGS, and presents a prototype implementation of MGS on the MIT Alewife multiprocessor. Multigrain shared memory enables the collaboration of hardware and software shared memory, thus synthesizing a single transparent shared-memory address space across a cluster of multiprocessors. The system leverages the efficient support for fine-grain cache-line sharing within multiprocessor nodes as often as possible, and resorts to coarse-grain page-level sharing across nodes only when absolutely necessary. Using our prototype implementation of MGS, an in-depth study of several shared-memory application is conducted to understand the behavior of DSMPs. Our study is the first to comprehensively explore the DSMP design space, and teh compare the performance of DSMPs against all-software and all-hardware DSMs on a signle experimental platform. Keeping the total number of processors fixed, we show that applications execute up to 85% faster on a DSMP as compared to an all-software DSM. We also show that all-hardware DSMs hold a significant performance advantage over DSMPs on challenging applications, between 159% and 1014%. However, program transformations to improve data locality for these applications allow DSMPs to almost match the performance of an all-hardware multiprocessor of the same size.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2079617220",
    "type": "article"
  },
  {
    "title": "Balancing performance and flexibility with hardware support for network architectures",
    "doi": "https://doi.org/10.1145/945506.945508",
    "publication_date": "2003-10-10",
    "publication_year": 2003,
    "authors": "Ilija Hadžić; Jonathan M. Smith",
    "corresponding_authors": "",
    "abstract": "The goals of performance and flexibility are often at odds in the design of network systems. The tension is common enough to justify an architectural solution, rather than a set of context-specific solutions. The Programmable Protocol Processing Pipeline (P4) design uses programmable hardware to selectively accelerate protocol processing functions. A set of field-programmable gate arrays (FPGAs) and an associated library of network processing modules implemented in hardware are augmented with software support for function selection and composition, and applied to processing-intensive portions of a user-programmable protocol stack. The system is sufficiently flexible to support protocol stacks that are dynamically altered in reaction to changing network conditions or user needs.The P4 can be transparently inserted into a conventional protocol architecture, such as that of TCP/IP. This experimental demonstration shows that the P4's programmability can be used to significantly improve the performance of TCP/IP under operating conditions where the protocol would perform poorly without augmentation. Generalizing from these experiments, the P4 is shown to have many applications as an open platform for implementing adaptive and programmable networks, and has illustrated new security issues that arise in FPGA-based architectures.The P4 and closely-related systems, such as network processors, are attractive architectural solutions to balancing performance and flexibility.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2084976408",
    "type": "article"
  },
  {
    "title": "Unified Holistic Memory Management Supporting Multiple Big Data Processing Frameworks over Hybrid Memories",
    "doi": "https://doi.org/10.1145/3511211",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Lei Chen; Jiacheng Zhao; Chenxi Wang; Ting Cao; John Zigman; Haris Volos; Onur Mutlu; Fang Lv; Xiaobing Feng; Guoqing Xu; Huimin Cui",
    "corresponding_authors": "",
    "abstract": "To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy inefficient. Emerging non-volatile memory (NVM) technologies offer high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages and executed on top of a managed runtime that already performs various dimensions of memory management. Supporting hybrid physical memories adds a new dimension, creating unique challenges in data replacement. This article proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division information is accurate enough to guide the GC for data layout, which hardly incurs overhead in data monitoring and moving. We implemented Panthera in OpenJDK and Apache Spark. Based on Big Data applications’ memory access pattern, we also implemented a new profiling-guided optimization strategy, which is transparent to applications. With this optimization, our extensive evaluation demonstrates that Panthera reduces energy by 32–53% at less than 1% time overhead on average. To show Panthera’s applicability, we extend it to QuickCached, a pure Java implementation of Memcached. Our evaluation results show that Panthera reduces energy by 28.7% at 5.2% time overhead on average.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4210382784",
    "type": "article"
  },
  {
    "title": "The Role of Compute in Autonomous Micro Aerial Vehicles: Optimizing for Mission Time and Energy Efficiency",
    "doi": "https://doi.org/10.1145/3511210",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Behzad Boroujerdian; Hasan Genc; Srivatsan Krishnan; Bardienus P. Duisterhof; Brian Plancher; Kayvan Mansoorshahi; Marcelino M. de Almeida; Wenzhi Cui; Aleksandra Faust; Vijay Janapa Reddi",
    "corresponding_authors": "",
    "abstract": "Autonomous and mobile cyber-physical machines are becoming an inevitable part of our future. In particular, Micro Aerial Vehicles (MAVs) have seen a resurgence in activity. With multiple use cases, such as surveillance, search and rescue, package delivery, and more, these unmanned aerial systems are on the cusp of demonstrating their full potential. Despite such promises, these systems face many challenges, one of the most prominent of which is their low endurance caused by their limited onboard energy. Since the success of a mission depends on whether the drone can finish it within such duration and before it runs out of battery, improving both the time and energy associated with the mission are of high importance. Such improvements have traditionally been arrived at through the use of better algorithms. But our premise is that more powerful and efficient onboard compute can also address the problem. In this article, we investigate how the compute subsystem, in a cyber-physical mobile machine such as a Micro Aerial Vehicle, can impact mission time (time to complete a mission) and energy. Specifically, we pose the question as what is the role of computing for cyber-physical mobile robots? We show that compute and motion are tightly intertwined, and as such a close examination of cyber and physical processes and their impact on one another is necessary. We show different “impact paths” through which compute impacts mission metrics and examine them using a combination of analytical models, simulation, and micro and end-to-end benchmarking. To enable similar studies, we open sourced MAVBench , our tool-set, which consists of (1) a closed-loop real-time feedback simulator and (2) an end-to-end benchmark suite composed of state-of-the-art kernels. By combining MAVBench, analytical modeling, and an understanding of various compute impacts, we show up to 2X and 1.8X improvements for mission time and mission energy for two optimization case studies, respectively. Our investigations, as well as our optimizations, show that cyber-physical co-design, a methodology with which both the cyber and physical processes/quantities of the robot are developed with consideration of one another, similar to hardware-software co-design, is necessary for arriving at the design of the optimal robot.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4213334549",
    "type": "article"
  },
  {
    "title": "Filesystem Fragmentation on Modern Storage Systems",
    "doi": "https://doi.org/10.1145/3611386",
    "publication_date": "2023-08-02",
    "publication_year": 2023,
    "authors": "Jonggyu Park; Young Ik Eom",
    "corresponding_authors": "",
    "abstract": "Filesystem fragmentation has been one of the primary reasons for computer systems to get slower over time. However, there have been rapid changes in modern storage systems over the past decades, and modern storage devices such as solid state drives have different mechanisms to access data, compared with traditional rotational ones. In this article, we revisit filesystem fragmentation on modern computer systems from both performance and fairness perspectives. According to our extensive experiments, filesystem fragmentation not only degrades I/O performance of modern storage devices, but also incurs various problems related to I/O fairness, such as performance interference. Unfortunately, conventional defragmentation tools are designed primarily for hard disk drives and thus generate an unnecessarily large amount of I/Os for data migration. To mitigate such problems, this article present FragPicker, a new defragmentation tool for modern storage devices. FragPicker analyzes the I/O behaviors of each target application and defragments only necessary pieces of data whose migration can contribute to performance improvement, thereby effectively minimizing the I/O amount for defragmentation. Our evaluation with YCSB workload-C shows FragPicker reduces the total amount of I/O for defragmentation by around 66% and the elapsed time by around 84%, while showing a similar level of defragmentation effect.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385497422",
    "type": "article"
  },
  {
    "title": "A formal protection model of security in centralized, parallel, and distributed systems",
    "doi": "https://doi.org/10.1145/99926.99928",
    "publication_date": "1990-08-01",
    "publication_year": 1990,
    "authors": "Glenn S. Benson; Ian F. Akyildiz; William F. Appelbe",
    "corresponding_authors": "",
    "abstract": "One way to show that a system is not secure is to demonstrate that a malicious or mistake-prone user or program can break security by causing the system to reach a nonsecure state. A fundamental aspect of a security model is a proof that validates that every state reachable from a secure initial state is secure. A sequential security model assumes that every command that acts as a state transition executes sequentially, while a concurrent security model assumes that multiple commands execute concurrently. This paper presents a security model called the Centralized-Parallel-Distributed model (CPD model) that defines security for logically, or physically centralized, parallel, and distributed systems. The purpose of the CPD model is to define concurrency conditions that guarentee that a concurrent system cannot reach a state in which privileges are configured in a nonsecure manner. As an example, the conditions are used to construct a representation of a distributed system.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2014874601",
    "type": "article"
  },
  {
    "title": "Preface: Special issue on operating systems principles",
    "doi": "https://doi.org/10.1145/2080.357389",
    "publication_date": "1984-02-01",
    "publication_year": 1984,
    "authors": "Anita K. Jones",
    "corresponding_authors": "Anita K. Jones",
    "abstract": "No abstract available.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2061152959",
    "type": "article"
  },
  {
    "title": "Improving peer-to-peer performance through server-side scheduling",
    "doi": "https://doi.org/10.1145/1455258.1455260",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "義行 高橋; Fabián E. Bustamante; Peter A. Dinda; S. Birrer; Lu Dong",
    "corresponding_authors": "",
    "abstract": "We show how to significantly improve the mean response time seen by both uploaders and downloaders in peer-to-peer data-sharing systems. Our work is motivated by the observation that response times are largely determined by the performance of the peers serving the requested objects, that is, by the peers in their capacity as servers. With this in mind, we take a close look at this server side of peers, characterizing its workload by collecting and examining an extensive set of traces. Using trace-driven simulation, we demonstrate the promise and potential problems with scheduling policies based on shortest-remaining-processing-time (SRPT), the algorithm known to be optimal for minimizing mean response time. The key challenge to using SRPT in this context is determining request service times. In addressing this challenge, we introduce two new estimators that enable predictive SRPT scheduling policies that closely approach the performance of ideal SRPT. We evaluate our approach through extensive single-server and system-level simulation coupled with real Internet deployment and experimentation.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2032596249",
    "type": "article"
  },
  {
    "title": "Mitigating Load Imbalance in Distributed Data Serving with Rack-Scale Memory Pooling",
    "doi": "https://doi.org/10.1145/3309986",
    "publication_date": "2018-05-31",
    "publication_year": 2018,
    "authors": "Stanko Novaković; Alexandros Daglis; Dmitrii Ustiugov; Edouard Bugnion; Babak Falsafi; Boris Grot",
    "corresponding_authors": "",
    "abstract": "To provide low-latency and high-throughput guarantees, most large key-value stores keep the data in the memory of many servers. Despite the natural parallelism across lookups, the load imbalance, introduced by heavy skew in the popularity distribution of keys, limits performance. To avoid violating tail latency service-level objectives, systems tend to keep server utilization low and organize the data in micro-shards, which provides units of migration and replication for the purpose of load balancing. These techniques reduce the skew but incur additional monitoring, data replication, and consistency maintenance overheads. In this work, we introduce RackOut, a memory pooling technique that leverages the one-sided remote read primitive of emerging rack-scale systems to mitigate load imbalance while respecting service-level objectives. In RackOut, the data are aggregated at rack-scale granularity, with all of the participating servers in the rack jointly servicing all of the rack’s micro-shards. We develop a queuing model to evaluate the impact of RackOut at the datacenter scale. In addition, we implement a RackOut proof-of-concept key-value store, evaluate it on two experimental platforms based on RDMA and Scale-Out NUMA, and use these results to validate the model. We devise two distinct approaches to load balancing within a RackOut unit, one based on random selection of nodes—RackOut_static—and another one based on an adaptive load balancing mechanism—RackOut_adaptive. Our results show that RackOut_static increases throughput by up to 6× for RDMA and 8.6&amp;times for Scale-Out NUMA compared to a scale-out deployment, while respecting tight tail latency service-level objectives. RackOut_adaptive improves the throughput by 30% for workloads with 20% of writes over RackOut_static.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2939976185",
    "type": "article"
  },
  {
    "title": "SmartIO",
    "doi": "https://doi.org/10.1145/3462545",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Jonas Markussen; Lars Kristiansen; Pål Halvorsen; Halvor Kielland-Gyrud; Håkon Kvale Stensland; Carsten Griwodz",
    "corresponding_authors": "",
    "abstract": "The large variety of compute-heavy and data-driven applications accelerate the need for a distributed I/O solution that enables cost-effective scaling of resources between networked hosts. For example, in a cluster system, different machines may have various devices available at different times, but moving workloads to remote units over the network is often costly and introduces large overheads compared to accessing local resources. To facilitate I/O disaggregation and device sharing among hosts connected using Peripheral Component Interconnect Express (PCIe) non-transparent bridges, we present SmartIO. NVMes, GPUs, network adapters, or any other standard PCIe device may be borrowed and accessed directly, as if they were local to the remote machines. We provide capabilities beyond existing disaggregation solutions by combining traditional I/O with distributed shared-memory functionality, allowing devices to become part of the same global address space as cluster applications. Software is entirely removed from the data path, and simultaneous sharing of a device among application processes running on remote hosts is enabled. Our experimental results show that I/O devices can be shared with remote hosts, achieving native PCIe performance. Thus, compared to existing device distribution mechanisms, SmartIO provides more efficient, low-cost resource sharing, increasing the overall system performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3178293493",
    "type": "article"
  },
  {
    "title": "A Declarative Language Approach to Device Configuration",
    "doi": "https://doi.org/10.1145/2110356.2110361",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Adrian Schüpbach; Andrew Baumann; Timothy Roscoe; Simon Peter",
    "corresponding_authors": "",
    "abstract": "C remains the language of choice for hardware programming (device drivers, bus configuration, etc.): it is fast, allows low-level access, and is trusted by OS developers. However, the algorithms required to configure and reconfigure hardware devices and interconnects are becoming more complex and diverse, with the added burden of legacy support, “quirks,” and hardware bugs to work around. Even programming PCI bridges in a modern PC is a surprisingly complex problem, and is getting worse as new functionality such as hotplug appears. Existing approaches use relatively simple algorithms, hard-coded in C and closely coupled with low-level register access code, generally leading to suboptimal configurations. We investigate the merits and drawbacks of a new approach: separating hardware configuration logic (algorithms to determine configuration parameter values) from mechanism (programming device registers). The latter we keep in C, and the former we encode in a declarative programming language with constraint-satisfaction extensions. As a test case, we have implemented full PCI configuration, resource allocation, and interrupt assignment in the Barrelfish research operating system, using a concise expression of efficient algorithms in constraint logic programming. We show that the approach is tractable, and can successfully configure a wide range of PCs with competitive runtime cost. Moreover, it requires about half the code of the C-based approach in Linux while offering considerably more functionality. Additionally it easily accommodates adaptations such as hotplug, fixed regions, and “quirks.”",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2133337233",
    "type": "article"
  },
  {
    "title": "Seer",
    "doi": "https://doi.org/10.1145/3132036",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Nuno Diegues; Paolo Romano; Stoyan Garbatov",
    "corresponding_authors": "",
    "abstract": "The ubiquity of multicore processors has led programmers to write parallel and concurrent applications to take advantage of the underlying hardware and speed up their executions. In this context, Transactional Memory (TM) has emerged as a simple and effective synchronization paradigm, via the familiar abstraction of atomic transactions. After many years of intense research, major processor manufacturers (including Intel) have recently released mainstream processors with hardware support for TM (HTM). In this work, we study a relevant issue with great impact on the performance of HTM. Due to the optimistic and inherently limited nature of HTM, transactions may have to be aborted and restarted numerous times, without any progress guarantee. As a result, it is up to the software library that regulates the HTM usage to ensure progress and optimize performance. Transaction scheduling is probably one of the most well-studied and effective techniques to achieve these goals. However, these recent mainstream HTMs have some technical limitations that prevent the adoption of known scheduling techniques: unlike software implementations of TM used in the past, existing HTMs provide limited or no information on which memory regions or contending transactions caused the abort. To address this crucial issue for HTMs, we propose S eer , a software scheduler that addresses precisely this restriction of HTM by leveraging on an online probabilistic inference technique that identifies the most likely conflict relations and establishes a dynamic locking scheme to serialize transactions in a fine-grained manner. The key idea of our solution is to constrain the portions of parallelism that are affecting negatively the whole system. As a result, this not only prevents performance reduction but also in fact unveils further scalability and performance for HTM. Via an extensive evaluation study, we show that S eer improves the performance of the Intel’s HTM by up to 3.6×, and by 65% on average across all concurrency degrees and benchmarks on a large processor with 28 cores.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2769939424",
    "type": "article"
  },
  {
    "title": "The effect of seance communication on multiprocessing systems",
    "doi": "https://doi.org/10.1145/377769.377780",
    "publication_date": "2001-05-01",
    "publication_year": 2001,
    "authors": "Avi Mendelson; Freddy Gabbay",
    "corresponding_authors": "",
    "abstract": "This paper introduces the seance communication phenomenon and analyzes its effect on a multiprocessing environment. Seance communication is an unnecessary coherency-related activity that is associated with dead cache information. Dead information may reside in the cache for various reasons: task migration, context switches, or working-set changes. Dead information does not have a significant performance impact on a single-processor system; however, it can dominate the performance of multicache environment. In order to evaluate the overhead of seance communication, we develop an analytical model that is based on the fractal behavior of the memory references. So far, all previous works that used the same modeling approach extracted the fractal parameters of a program manually. This paper provides an additional important contribution by demonstrating how these parameters can be automatically extracted from the program trace. Our analysis indicates that Seance communication may severely reduce the overall system performance when using write-update or write-invalidate cache coherency protocols. In addition, we find that the performance of write-update protocols is affected more severely than write-invalidate protocols. The results that are provided by our model are important for better understanding of the coherency-related overhead in multicache systems and for better development of parallel applications and operating systems.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1968205174",
    "type": "article"
  },
  {
    "title": "Accelerating shared virtual memory via general-purpose network interface support",
    "doi": "https://doi.org/10.1145/367742.367747",
    "publication_date": "2001-02-01",
    "publication_year": 2001,
    "authors": "Angelos Bilas; Dongming Jiang; Jaswinder Singh",
    "corresponding_authors": "",
    "abstract": "Clusters of symmetric multiprocessors (SMPs) are important platforms for high-performance computing. With the success of hardware cache-coherent distributed shared memory (DSM), a lot of effort has also been made to support the coherent shared-address-space programming model in software on clusters. Much research has been done in fast communication on clusters and in protocols for supporting software shared memory across them. However, the performance of software virtual memory (SVM) is still far from that achieved on hardware DSM systems. The goal of this paper is to improve the performance of SVM on system area network clusters by considering communication and protocol layer interactions. We first examine what are the important communication system bottlenecks that stand in the way of improving parallel performance of SVM clusters; in particular, which parameters of the communication architecture are most important to improve further relative to processor speed, which ones are already adequate on modern systems for most applications, and how will this change with technology in the future. We find that the most important communication subsystem cost to improve is the overhead of generating and delivery interrupts for asynchronous protocol processing. Then we proceed to show, that by providing simple and general support for asynchronous message handling in a commodity network interface (NI) and by altering SVM protocols appropriately, protocol activity can be decoupled from asynchronous message handling, and the need for interrupts or polling can be eliminated. The NI mechanisms needed are generic, not SVM-dependent. We prototype the mechanisms and such a synchronous home-based LRC protocol, called GeNIMA (GEneral-purpose Network Interface support for shared Memory Abstractions), on a cluster of SMPs with a programmable NI. We find that the performance improvements are substantial, bringing performance on a small-scale SMP cluster much closer to that of hardware-coherent shared memory for many applications, and we show the value of each of the mechanisms in different applications.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1993356916",
    "type": "article"
  },
  {
    "title": "Effects of job loading policies for multiprogramming systems in processing a job stream",
    "doi": "https://doi.org/10.1145/6306.6310",
    "publication_date": "1986-02-10",
    "publication_year": 1986,
    "authors": "Hisao Kameda",
    "corresponding_authors": "Hisao Kameda",
    "abstract": "The scheduling of jobs for multiprogramming systems includes the selection of jobs to be loaded into memory (job loading policy or memory schedule) and the scheduling for CPU processing (CPU schedule). There has been a successful empirical claim for the optimal CPU schedule; its optimality has been proved in a Markovian model of job-stream processing that uses the first-come-first-loaded (FCFL) job loading policy. We extend this model to gain insight into the effects of job loading policies. The model studied consists of an input stream of jobs of two classes and a multiple-resource system (the model of a multiprogramming system) with a stack for waiting jobs. The system consists of a finite amount of memory and a cyclic queue of a single (CPU) server station and a multiple (I/O) server station. The values of parameters describing each class of jobs are distinct except the mean I/O service time and the amount of memory required. The estimate of the maximum processing capacity (throughput bound) of the system is obtained and is shown to be achieved by the combination of the empirically claimed optimal CPU schedule and a job loading policy whereby the set of jobs in memory is kept to be (nearly) balanced with respect to the job stream. Furthermore, we show that the job loading policies independent of the system status have no improvement over the FCFL policy. Our investigation, supported by numerical calculations, suggests that much more care may be needed in implementing the job loading policy that aims at the optimal processing capacity than in implementing the optimal CPU schedule. This agrees with what has been conjectured on the basis of empirical studies.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2081308127",
    "type": "article"
  },
  {
    "title": "Trace cache sampling filter",
    "doi": "https://doi.org/10.1145/1189736.1189739",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Michael Behar; Avi Mendelson; Avinoam Kolodny",
    "corresponding_authors": "",
    "abstract": "A simple mechanism to increase the utilization of a small trace cache, and simultaneously reduce its power consumption, is presented in this article. The mechanism uses selective storage of traces (filtering) that is based on a new concept in computer architecture: random sampling. The sampling filter exploits the “hot/cold trace” principle, which divides the population of traces into two groups. The first group contains “hot traces” that are executed many times from the trace cache and contribute the majority of committed instructions. The second group contains “cold traces” that are rarely executed, but are responsible for the majority of writes to an unfiltered cache. The sampling filter selects traces without any prior knowledge of their quality. However, as most writes to the cache are of “cold traces” it statistically filters out those traces, reducing cache turnover and eventually leading to higher quality traces residing in the cache. In contrast with previously proposed filters, which perform bookkeeping for all traces in the program, the sampling filter can be implemented with minimal hardware. Results show that the sampling filter can increase the number of hits per build (utilization) by a factor of 38, reduce the miss rate by 20% and improve the performance-power efficiency by 15%. Further improvements can be obtained by extensions to the basic sampling filter: allowing “hot traces” to bypass the sampling filter, combining of sampling together with previously proposed filters, and changing the replacement policy in the trace cache. Those techniques combined with the sampling filter can reduce the miss rate of the trace cache by up to 40%. Although the effectiveness of the sampling filter is demonstrated for a trace cache, the sampling principle is applicable to other micro-architectural structures with similar access patterns.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2004130873",
    "type": "article"
  },
  {
    "title": "Venice",
    "doi": "https://doi.org/10.1145/3310360",
    "publication_date": "2018-02-28",
    "publication_year": 2018,
    "authors": "Boyan Zhao; Rui Hou; Jianbo Dong; Michael Huang; Sally A. McKee; Qianlong Zhang; Yueji Liu; Ye Li; Lixin Zhang; Dan Meng",
    "corresponding_authors": "",
    "abstract": "Consolidated server racks are quickly becoming the standard infrastructure for engineering, business, medicine, and science. Such servers are still designed much in the way when they were organized as individual, distributed systems. Given that many fields rely on big-data analytics substantially, its cost-effectiveness and performance should be improved, which can be achieved by flexibly allowing resources to be shared across nodes. Here we describe Venice, a family of data-center server architectures that includes a strong communication substrate as a first-class resource. Venice supports a diverse set of resource-joining mechanisms that enables applications to leverage non-local resources efficiently. We have constructed a hardware prototype to better understand the implications of design decisions about system support for resource sharing. We use it to measure the performance of at-scale applications and to explore performance, power, and resource-sharing transparency tradeoffs (i.e., how many programming changes are needed). We analyze these tradeoffs for sharing memory, accelerators, and NICs. We find that reducing/hiding latency is particularly important, the chosen communication channels should match the sharing access patterns of the applications, and of which we can improve performance by exploiting inter-channel collaboration.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2922514770",
    "type": "article"
  },
  {
    "title": "Effective Detection of Sleep-in-atomic-context Bugs in the Linux Kernel",
    "doi": "https://doi.org/10.1145/3381990",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Jia-Ju Bai; Julia Lawall; Shi‐Min Hu",
    "corresponding_authors": "",
    "abstract": "Atomic context is an execution state of the Linux kernel in which kernel code monopolizes a CPU core. In this state, the Linux kernel may only perform operations that cannot sleep, as otherwise a system hang or crash may occur. We refer to this kind of concurrency bug as a sleep-in-atomic-context (SAC) bug. In practice, SAC bugs are hard to find, as they do not cause problems in all executions. In this article, we propose a practical static approach named DSAC to effectively detect SAC bugs in the Linux kernel. DSAC uses three key techniques: (1) a summary-based analysis to identify the code that may be executed in atomic context, (2) a connection-based alias analysis to identify the set of functions referenced by a function pointer, and (3) a path-check method to filter out repeated reports and false bugs. We evaluate DSAC on Linux 4.17 and find 1,159 SAC bugs. We manually check all the bugs and find that 1,068 bugs are real. We have randomly selected 300 of the real bugs and sent them to kernel developers. 220 of these bugs have been confirmed, and 51 of our patches fixing 115 bugs have been applied.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3022678461",
    "type": "article"
  },
  {
    "title": "KylinX",
    "doi": "https://doi.org/10.1145/3436512",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Yiming Zhang; Chengfei Zhang; Yaozheng Wang; Kai Yu; Guangtao Xue; Jon Crowcroft",
    "corresponding_authors": "",
    "abstract": "Unikernel specializes a minimalistic LibOS and a target application into a standalone single-purpose virtual machine (VM) running on a hypervisor, which is referred to as (virtual) appliance . Compared to traditional VMs, Unikernel appliances have smaller memory footprint and lower overhead while guaranteeing the same level of isolation. On the downside, Unikernel strips off the process abstraction from its monolithic appliance and thus sacrifices flexibility, efficiency, and applicability. In this article, we examine whether there is a balance embracing the best of both Unikernel appliances (strong isolation) and processes (high flexibility/efficiency). We present KylinX, a dynamic library operating system for simplified and efficient cloud virtualization by providing the pVM (process-like VM) abstraction. A pVM takes the hypervisor as an OS and the Unikernel appliance as a process allowing both page-level and library-level dynamic mapping. At the page level, KylinX supports pVM fork plus a set of API for inter-pVM communication (IpC, which is compatible with conventional UNIX IPC). At the library level, KylinX supports shared libraries to be linked to a Unikernel appliance at runtime. KylinX enforces mapping restrictions against potential threats. We implement a prototype of KylinX by modifying MiniOS and Xen tools. Extensive experimental results show that KylinX achieves similar performance both in micro benchmarks (fork, IpC, library update, etc.) and in applications (Redis, web server, and DNS server) compared to conventional processes, while retaining the strong isolation benefit of VMs/Unikernels.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3129383293",
    "type": "article"
  },
  {
    "title": "AI Tax",
    "doi": "https://doi.org/10.1145/3440689",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Daniel Richins; Dharmisha Doshi; Matthew Blackmore; Aswathy Thulaseedharan Nair; Neha Pathapati; Ankit Patel; Brainard Daguman; Daniel Dobrijalowski; Ramesh Illikkal; Kevin Long; David Zimmerman; Vijay Janapa Reddi",
    "corresponding_authors": "",
    "abstract": "Artificial intelligence and machine learning are experiencing widespread adoption in industry and academia. This has been driven by rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into specialized hardware AI accelerators. Given the rapid pace of advances, it is easy to forget that they are often developed and evaluated in a vacuum without considering the full application environment. This article emphasizes the need for a holistic, end-to-end analysis of artificial intelligence (AI) workloads and reveals the “AI tax.” We deploy and characterize Face Recognition in an edge data center. The application is an AI-centric edge video analytics application built using popular open source infrastructure and machine learning (ML) tools. Despite using state-of-the-art AI and ML algorithms, the application relies heavily on pre- and post-processing code. As AI-centric applications benefit from the acceleration promised by accelerators, we find they impose stresses on the hardware and software infrastructure: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By specializing for AI applications, we show that a purpose-built edge data center can be designed for the stresses of accelerated AI at 15% lower TCO than one derived from homogeneous servers and infrastructure.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3150298648",
    "type": "article"
  },
  {
    "title": "Systemizing Interprocedural Static Analysis of Large-scale Systems Code with Graspan",
    "doi": "https://doi.org/10.1145/3466820",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Zhiqiang Zuo; Kai Wang; Aftab M. Hussain; Ardalan Amiri Sani; Yiyu Zhang; Shenming Lu; Wensheng Dou; Linzhang Wang; Xuandong Li; Chenxi Wang; Guoqing Xu",
    "corresponding_authors": "",
    "abstract": "There is more than a decade-long history of using static analysis to find bugs in systems such as Linux. Most of the existing static analyses developed for these systems are simple checkers that find bugs based on pattern matching. Despite the presence of many sophisticated interprocedural analyses, few of them have been employed to improve checkers for systems code due to their complex implementations and poor scalability. In this article, we revisit the scalability problem of interprocedural static analysis from a “Big Data” perspective. That is, we turn sophisticated code analysis into Big Data analytics and leverage novel data processing techniques to solve this traditional programming language problem. We propose Graspan , a disk-based parallel graph system that uses an edge-pair centric computation model to compute dynamic transitive closures on very large program graphs. We develop two backends for Graspan, namely, Graspan-C running on CPUs and Graspan-G on GPUs, and present their designs in the article. Graspan-C can analyze large-scale systems code on any commodity PC, while, if GPUs are available, Graspan-G can be readily used to achieve orders of magnitude speedup by harnessing a GPU’s massive parallelism. We have implemented fully context-sensitive pointer/alias and dataflow analyses on Graspan. An evaluation of these analyses on large codebases written in multiple languages such as Linux and Apache Hadoop demonstrates that their Graspan implementations are language-independent, scale to millions of lines of code, and are much simpler than their original implementations. Moreover, we show that these analyses can be used to uncover many real-world bugs in large-scale systems code.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3189632290",
    "type": "article"
  },
  {
    "title": "Scaling Performance via Self-Tuning Approximation for Graphics Engines",
    "doi": "https://doi.org/10.1145/2631913",
    "publication_date": "2014-08-29",
    "publication_year": 2014,
    "authors": "Mehrzad Samadi; Janghaeng Lee; D. Anoushe Jamshidi; Scott Mahlke; Amir Hormati",
    "corresponding_authors": "",
    "abstract": "Approximate computing, where computation accuracy is traded off for better performance or higher data throughput, is one solution that can help data processing keep pace with the current and growing abundance of information. For particular domains, such as multimedia and learning algorithms, approximation is commonly used today. We consider automation to be essential to provide transparent approximation, and we show that larger benefits can be achieved by constructing the approximation techniques to fit the underlying hardware. Our target platform is the GPU because of its high performance capabilities and difficult programming challenges that can be alleviated with proper automation. Our approach—SAGE—combines a static compiler that automatically generates a set of CUDA kernels with varying levels of approximation with a runtime system that iteratively selects among the available kernels to achieve speedup while adhering to a target output quality set by the user. The SAGE compiler employs three optimization techniques to generate approximate kernels that exploit the GPU microarchitecture: selective discarding of atomic operations, data packing, and thread fusion. Across a set of machine learning and image processing kernels, SAGE's approximation yields an average of 2.5× speedup with less than 10% quality loss compared to the accurate execution on a NVIDIA GTX 560 GPU.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2083986745",
    "type": "article"
  },
  {
    "title": "Trinity: High-Performance and Reliable Mobile Emulation through Graphics Projection",
    "doi": "https://doi.org/10.1145/3643029",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Hao Lin; Zhenhua Li; Di Gao; Yunhao Liu; Feng Qian; Tianyin Xu",
    "corresponding_authors": "",
    "abstract": "Mobile emulation, which creates full-fledged software mobile devices on a physical PC/server, is pivotal to the mobile ecosystem. Unfortunately, existing mobile emulators perform poorly on graphics-intensive apps in terms of efficiency and compatibility. To address this, we introduce graphics projection , a novel graphics virtualization mechanism that adds a small-size projection space inside the guest memory, which processes graphics operations involving control contexts and resource handles without host interactions. While enhancing performance, the decoupled and asynchronous guest/host control flows introduced by graphics projection can significantly complicate emulators’ reliability issue diagnosis when faced with a variety of uncommon or non-standard app behaviors in the wild, hindering practical deployment in production. To overcome this drawback, we develop an automatic reliability issue analysis pipeline that distills the critical code paths across the guest and host control flows by runtime quarantine and state introspection. The resulting new Android emulator, dubbed Trinity, exhibits an average of 97% native hardware performance and 99.3% reliable app support, in some cases outperforming other emulators by more than an order of magnitude. It has been deployed in Huawei DevEco Studio, a major Android IDE with millions of developers.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391169437",
    "type": "article"
  },
  {
    "title": "PMAlloc: A Holistic Approach to Improving Persistent Memory Allocation",
    "doi": "https://doi.org/10.1145/3643886",
    "publication_date": "2024-02-03",
    "publication_year": 2024,
    "authors": "Zheng Dang; Shuibing He; Xuechen Zhang; Peiyi Hong; Zhenxin Li; Xinyu Chen; Haozhe Song; Xian–He Sun; Gang Chen",
    "corresponding_authors": "",
    "abstract": "Persistent memory allocation is a fundamental building block for developing high-performance and in-memory applications. Existing persistent memory allocators suffer from many performance issues. First, they may introduce repeated cache line flushes and small random accesses in persistent memory for their poor heap metadata management. Second, they use static slab segregation resulting in a dramatic increase in memory consumption when allocation request size is changed. Third, they are not aware of NUMA effect, leading to remote persistent memory accesses in memory allocation and deallocation processes. In this article, we design a novel allocator, named PMAlloc, to solve the above issues simultaneously. (1) PMAlloc eliminates cache line reflushes by mapping contiguous data blocks in slabs to interleaved metadata entries stored in different cache lines. (2) It writes small metadata units to a persistent bookkeeping log in a sequential pattern to remove random heap metadata accesses in persistent memory. (3) Instead of using static slab segregation, it supports slab morphing, which allows slabs to be transformed between size classes to significantly improve slab usage. (4) It uses a local-first allocation policy to avoid allocating remote memory blocks. And it supports a two-phase deallocation mechanism including recording and synchronization to minimize the number of remote memory access in the deallocation. PMAlloc is complementary to the existing consistency models. Results on six benchmarks demonstrate that PMAlloc improves the performance of state-of-the-art persistent memory allocators by up to 6.4× and 57× for small and large allocations, respectively. PMAlloc with NUMA optimizations brings a 2.9× speedup in multi-socket evaluation and is up to 36× faster than other persistent memory allocators. Using PMAlloc reduces memory usage by up to 57.8%. Besides, we integrate PMAlloc in a persistent FPTree. Compared to the state-of-the-art allocators, PMAlloc improves the performance of this application by up to 3.1×.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391510905",
    "type": "article"
  },
  {
    "title": "Architectural support for reduced register saving/restoring in single-window register files",
    "doi": "https://doi.org/10.1145/103727.103730",
    "publication_date": "1991-02-01",
    "publication_year": 1991,
    "authors": "Miquel Huguet; Tomás Lang",
    "corresponding_authors": "",
    "abstract": "The use of registers in a processor reduces the data and instruction memory traffic. Since this reduction is a significant factor in the improvement of the program execution time, recent VLSI processors have a large number of registers which can be used efficiently because of the advances in compiler technology. However, since registers have to be saved/restored across function calls, the corresponding register saving and restoring (RSR) memory traffic can almost eliminate the overall reduction. This traffic has been reduced by compiler optimizations and by providing multiple-window register files. Although these multiple-window architectures produce a large reduction in the RSR traffic, they have several drawbacks which make the single-window file preferable. We consider a combination of hardware support and compiler optimizations to reduce the RSR traffic for a single-window register file, beyond the reductions achieved by compiler optimizations alone. Basically, this hardware keeps track of the registers that are written during execution, so that the number of registers saved is minimized. Moreover, hardware is added so that a register is saved in the activation record of the function that uses it (instead of in the record of the current function); in this way a register is restored only when it is needed, rather than wholesale on procedure return. We present a register saving and restoring policy that makes use of this hardware, discuss its implementation, and evaluate the traffic reduction when the policy is combined with intraprocedural and interprocedural compiler optimizations. We show that, on the average for the four general-purpose programs measured, the RSR traffic is reduced by about 90 percent for a small register file (i.e., 32 registers), which results in an overall data memory traffic reduction of about 15 percent.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2093287305",
    "type": "article"
  },
  {
    "title": "Comprehensive multivariate extrapolation modeling of multiprocessor cache miss rates",
    "doi": "https://doi.org/10.1145/1189736.1189738",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Ilya Gluhovsky; David Vengerov; B. O'Krafka",
    "corresponding_authors": "",
    "abstract": "Cache miss rates are an important subset of system model inputs. Cache miss rate models are used for broad design space exploration in which many cache configurations cannot be simulated directly due to limitations of trace collection setups or available resources. Often it is not practical to simulate large caches. Large processor counts and consequent potentially high degree of cache sharing are frequently not reproducible on small existing systems. In this article, we present an approach to building multivariate regression models for predicting cache miss rates beyond the range of collectible data. The extrapolation model attempts to accurately estimate the high-level trend of the existing data, which can be extended in a natural way. We extend previous work by its applicability to multiple miss rate components and its ability to model a wide range of cache parameters, including size, line size, associativity and sharing. The stability of extrapolation is recognized to be a crucial requirement. The proposed extrapolation model is shown to be stable to small data perturbations that may be introduced during data collection.We show the effectiveness of the technique by applying it to two commercial workloads. The wide design space contains configurations that are much larger than those for which miss rate data were available. The fitted data match the simulation data very well. The various curves show how a miss rate model is useful for not only estimating the performance of specific configurations, but also for providing insight into miss rate trends.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2106063054",
    "type": "article"
  },
  {
    "title": "Corrigendum to “The IX Operating System: Combining Low Latency, High Throughput and Efficiency in a Protected Dataplane”",
    "doi": "https://doi.org/10.1145/3154292",
    "publication_date": "2017-08-31",
    "publication_year": 2017,
    "authors": "Adam Belay; George Prekas; Mia Primorac; Ana Klimovic; Samuel Grossman; Christos Kozyrakis; Edouard Bugnion",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2778763463",
    "type": "erratum"
  },
  {
    "title": "Symphony: Orchestrating Sparse and Dense Tensors with Hierarchical Heterogeneous Processing",
    "doi": "https://doi.org/10.1145/3630007",
    "publication_date": "2023-10-27",
    "publication_year": 2023,
    "authors": "Michael Pellauer; Jason Clemons; Vignesh Balaji; Neal Crago; Aamer Jaleel; Donghyuk Lee; Mike O’Connor; Anghsuman Parashar; Sean Treichler; Po-An Tsai; Stephen W. Keckler; Joel Emer",
    "corresponding_authors": "",
    "abstract": "Sparse tensor algorithms are becoming widespread, particularly in the domains of deep learning, graph and data analytics, and scientific computing. Current high-performance broad-domain architectures, such as GPUs, often suffer memory system inefficiencies by moving too much data or moving it too far through the memory hierarchy. To increase performance and efficiency, proposed domain-specific accelerators tailor their architectures to the data needs of a narrow application domain, but as a result cannot be applied to a wide range of algorithms or applications that contain a mix of sparse and dense algorithms. This article proposes Symphony, a hybrid programmable/specialized architecture that focuses on the orchestration of data throughout the memory hierarchy to simultaneously reduce the movement of unnecessary data and data movement distances. Key elements of the Symphony architecture include (1) specialized reconfigurable units aimed not only at roofline floating-point computations but also at supporting data orchestration features, such as address generation, data filtering, and sparse metadata processing; and (2) distribution of computation resources (both programmable and specialized) throughout the on-chip memory hierarchy. We demonstrate that Symphony can match non-programmable ASIC performance on sparse tensor algebra and provide 31× improved runtime and 44× improved energy over a comparably provisioned GPU for these applications.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387968429",
    "type": "article"
  },
  {
    "title": "Modeling the Interplay between Loop Tiling and Fusion in Optimizing Compilers Using Affine Relations",
    "doi": "https://doi.org/10.1145/3635305",
    "publication_date": "2023-11-30",
    "publication_year": 2023,
    "authors": "Jie Zhao; Jinchen Xu; Peng Di; Wang Nie; Jiahui Hu; Yanzhi Yi; Sijia Yang; Zhen Geng; Renwei Zhang; Bojie Li; Zhiliang Gan; Xuefeng Jin",
    "corresponding_authors": "",
    "abstract": "Loop tiling and fusion are two essential transformations in optimizing compilers to enhance the data locality of programs. Existing heuristics either perform loop tiling and fusion in a particular order, missing some of their profitable compositions, or execute ad-hoc implementations for domain-specific applications, calling for a generalized and systematic solution in optimizing compilers. In this paper, we present a so-called basteln (an abbreviation for backward slicing of tiled loop nests) strategy in polyhedral compilation to better model the interplay between loop tiling and fusion. The basteln strategy first groups loop nests by preserving their parallelism/tilability and next performs rectangular/parallelogram tiling to the output groups that produce data consumed outside the considered program fragment. The memory footprints required by each tile are then computed, from which the upwards exposed data are extracted to determine the tile shapes of the remaining fusion groups. Such a tiling mechanism can construct complex tile shapes imposed by the dependences between these groups, which are further merged by a post-tiling fusion algorithm for enhancing data locality without losing the parallelism/tilability of the output groups. The basteln strategy also takes into account the amount of redundant computations and the fusion of independent groups, exhibiting a general applicability. We integrate the basteln strategy into two optimizing compilers, with one a general-purpose optimizer and the other a domain-specific compiler for deploying deep learning models. The experiments are conducted on CPU, GPU, and a deep learning accelerator to demonstrate the effectiveness of the approach for a wide class of application domains, including deep learning, image processing, sparse matrix computation, and linear algebra. In particular, the basteln strategy achieves a mean speedup of 1.8 × over cuBLAS/cuDNN and 1.1 × over TVM on GPU when used to optimize deep learning models; it also outperforms PPCG and TVM by 11% and 20%, respectively, when generating code for the deep learning accelerator.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389245786",
    "type": "article"
  },
  {
    "title": "Design and verification of the Rollback Chip using HOP",
    "doi": "https://doi.org/10.1145/151244.151245",
    "publication_date": "1993-05-01",
    "publication_year": 1993,
    "authors": "Ganesh Gopalakrishnan; Richard Fujimoto",
    "corresponding_authors": "",
    "abstract": "The use of formal methods in hardware design improves the quality of designs in many ways: it promotes better understanding of the design; it permits systematic design refinement through the discovery of invariants; and it allows design verification (informal or formal). In this paper we illustrate the use of formal methods in the design of a custom hardware system called the “Rollback Chip” (RBC), conducted using a simple hardware design description language called “HOP”. An informal specification of the requirements of the RBC is first given, followed by a behavioral description of the RBC stating its desired behavior . The behavioral description is refined into progressively more efficient designs, terminating in a structural description . Key refinement steps are based on system invariants that are discovered during the design, and proved correct during design verification. The first step in design verification is to apply a program called PARCOMP to derive a behavioral description from the structural description of the RBC. The derived behavior is then compared against the desired behavior using equational verification techniques. This work demonstrates that formal methods can be fruitfully applied to a nontrivial hardware design. It also illustrates the particular advantages of our approach based on HOP and PARCOMP. Last, but not the least, it formally verifies the RBC mechanism itself.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1968183737",
    "type": "article"
  },
  {
    "title": "The effects of processor architecture on instruction memory traffic",
    "doi": "https://doi.org/10.1145/99926.99933",
    "publication_date": "1990-08-01",
    "publication_year": 1990,
    "authors": "Chad Leland Mitchell; Michael Flynn",
    "corresponding_authors": "",
    "abstract": "The relative amount of instruction traffic for two architectures is about the same in the presence of a large cache as with no cache. Furthermore, the presence of an intermediate-sized cache probably substantially favors the denser architecture. Encoding techniques have a much greater impact on instruction traffic than do the differences between instruction set families such as stack and register set. However, register set architectures have somewhat lower instruction traffic than directly comparable stack architectures of some local variables are allocated in registers. This study has clearly indicated that cache factors should be taken into consideration when making architectural tradeoffs. The differences in memory traffic between two architectures may be greatly amplified in the presence of a cache.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2023559031",
    "type": "article"
  },
  {
    "title": "Stating security requirements with tolerable sets",
    "doi": "https://doi.org/10.1145/45059.214409",
    "publication_date": "1988-08-01",
    "publication_year": 1988,
    "authors": "Dale Johnson; F. Javier Thayer",
    "corresponding_authors": "",
    "abstract": "This paper introduces and develops the concept of tolerable sets for analyzing general security requirements. Tolerable sets, and corresponding purging functions and invisibility based on the sets, are used to state and test such requirements. The approach used in this paper resulted from our attempt to apply the noninterference ideas of Goguen and Meseguer to the problem of stating special security requirements in the case of so-called trusted subjects. It turns out that the conditional purging function defined by Goguen and Meseguer is only one example, though an important one, of a conditional purging function. This paper provides a definition and characterization of a general class of purging functions similar to the purging function of Goguen and Meseguer. Furthermore, it relates purging and invisibility to security requirements. Some particular applications are described toward the end of the paper. At the end there are some critical remarks about purging functions.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2044385702",
    "type": "article"
  },
  {
    "title": "Optimality of scheduling policy for processing a job stream",
    "doi": "https://doi.org/10.1145/2080.357395",
    "publication_date": "1984-02-01",
    "publication_year": 1984,
    "authors": "Hisao Kameda",
    "corresponding_authors": "Hisao Kameda",
    "abstract": "article Free Access Share on Optimality of scheduling policy for processing a job stream Author: Hisao Kameda Department of Computer Science, The University of Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo 182, Japan Department of Computer Science, The University of Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo 182, JapanView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 101 February 1984pp 78–90https://doi.org/10.1145/2080.357395Published:01 February 1984Publication History 6citation551DownloadsMetricsTotal Citations6Total Downloads551Last 12 Months10Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2063773033",
    "type": "article"
  },
  {
    "title": "A Retargetable System-level DBT Hypervisor",
    "doi": "https://doi.org/10.1145/3386161",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Tom Spink; Harry Wagstaff; Björn Franke",
    "corresponding_authors": "",
    "abstract": "System-level Dynamic Binary Translation (DBT) provides the capability to boot an Operating System (OS) and execute programs compiled for an Instruction Set Architecture (ISA) different from that of the host machine. Due to their performance-critical nature, system-level DBT frameworks are typically hand-coded and heavily optimized, both for their guest and host architectures. While this results in good performance of the DBT system, engineering costs for supporting a new architecture or extending an existing architecture are high. In this article, we develop a novel, retargetable DBT hypervisor, which includes guest-specific modules generated from high-level guest machine specifications. Our system simplifies retargeting of the DBT, but it also delivers performance levels in excess of existing manually created DBT solutions. We achieve this by combining offline and online optimizations and exploiting the freedom of a Just-in-time (JIT) compiler operating in a bare-metal environment provided by a Virtual Machine (VM) hypervisor. We evaluate our DBT using both targeted micro-benchmarks as well as standard application benchmarks, and we demonstrate its ability to outperform the de facto standard Q EMU DBT system. Our system delivers an average speedup of 2.21× over Q EMU across SPEC CPU2006 integer benchmarks running in a full-system Linux OS environment, compiled for the 64-bit ARMv8-A ISA and hosted on an x86-64 platform. For floating-point applications the speedup is even higher, reaching 6.49× on average. We demonstrate that our system-level DBT system significantly reduces the effort required to support a new ISA while delivering outstanding performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3033378404",
    "type": "article"
  },
  {
    "title": "Distributed Graph Processing System and Processing-in-memory Architecture with Precise Loop-carried Dependency Guarantee",
    "doi": "https://doi.org/10.1145/3453681",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Youwei Zhuo; Jingji Chen; Gengyu Rao; Qinyi Luo; Yanzhi Wang; Hailong Yang; Depei Qian; Xuehai Qian",
    "corresponding_authors": "",
    "abstract": "To hide the complexity of the underlying system, graph processing frameworks ask programmers to specify graph computations in user-defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. It exemplifies a gap between programming model and runtime execution. This article proposes novel graph processing frameworks for distributed system and Processing-in-memory (PIM) architecture that precisely enforces loop-carried dependency; i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. Our approach instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. We propose to circulant scheduling in the framework to allow different nodes to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. The technique achieves an excellent trade-off between precise semantics and parallelism—the benefits of eliminating unnecessary computation and communication offset the reduced parallelism. We implement a new distributed graph processing framework SympleGraph, and two variants of runtime systems— GraphS and GraphSR —for PIM-based graph processing architecture, which significantly outperform the state-of-the-art.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3179626783",
    "type": "article"
  },
  {
    "title": "Shooting Down the Server Front-End Bottleneck",
    "doi": "https://doi.org/10.1145/3484492",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Rakesh Kumar; Boris Grot",
    "corresponding_authors": "",
    "abstract": "The front-end bottleneck is a well-established problem in server workloads owing to their deep software stacks and large instruction footprints. Despite years of research into effective L1-I and BTB prefetching, state-of-the-art techniques force a trade-off between metadata storage cost and performance. Temporal Stream prefetchers deliver high performance but require a prohibitive amount of metadata to accommodate the temporal history. Meanwhile, BTB-directed prefetchers incur low cost by using the existing in-core branch prediction structures but fall short on performance due to BTB’s inability to capture the massive control flow working set of server applications. This work overcomes the fundamental limitation of BTB-directed prefetchers, which is capturing a large control flow working set within an affordable BTB storage budget. We re-envision the BTB organization to maximize its control flow coverage by observing that an application’s instruction footprint can be mapped as a combination of its unconditional branch working set and, for each unconditional branch, a spatial encoding of the cache blocks around the branch target. Effectively capturing a map of the application’s instruction footprint in the BTB enables highly effective BTB-directed prefetching that outperforms the state-of-the-art prefetchers by up to 10% for equivalent storage budget.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4206345300",
    "type": "article"
  },
  {
    "title": "Reconfiguration for fault tolerance using graph grammars",
    "doi": "https://doi.org/10.1145/273011.273019",
    "publication_date": "1998-02-01",
    "publication_year": 1998,
    "authors": "Molisa Derk; Linda S. DeBrunner",
    "corresponding_authors": "",
    "abstract": "Reconfiguration for fault tolerance is a widely studied field, but this work applies graph grammars to this discipline for the first time. Reconfiguration Graph Grammars (RGG) are defined and applied to the definition of processor array reconfiguration algorithms. The nodes of a graph are associated with the processors of a processor array, and the edges are associated with those interprocessor communication lines that are active. The resulting algorithms for dynamic (run-time) reconfiguration are efficient and can be implemented distributively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1974112080",
    "type": "article"
  },
  {
    "title": "A high-speed network interface for distributed-memory systems",
    "doi": "https://doi.org/10.1145/244764.244767",
    "publication_date": "1997-02-01",
    "publication_year": 1997,
    "authors": "Peter Steenkiste",
    "corresponding_authors": "Peter Steenkiste",
    "abstract": "Distributed-memory systems have traditionally had great difficulty performing network I/O at rates proportional to their computational power. The problem is that the network interface has to support network I/O for a supercomputer, using computational and memory bandwidth resources similar to those of a workstation. As a result, the network interface becomes a bottleneck. In this article we present an I/O architecture that addresses these problems and supports high-speed network I/O on distributed-memory systems. The key to good performance is to partition the work appropriately between the system and the network interface. Some communication tasks are performed on the distributed-memory parallel system, since it is more powerful and less likely to become a bottleneck than the network interface. Tasks that do not parallelize well are performed on the network interface, and hardware support is provided for the most time-critical operations. This architecture has been implemented for the iWarp distributed-memory system and has been used by a number of applications. We describe this implementaiton, present performance results, and use application examples to validated the main features of the I/O architecture.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1996431245",
    "type": "article"
  },
  {
    "title": "A computer communication technique using content-induced transaction overlap",
    "doi": "https://doi.org/10.1145/2080.357393",
    "publication_date": "1984-02-01",
    "publication_year": 1984,
    "authors": "Simon Berkovich; Colleen Roe Wilson",
    "corresponding_authors": "",
    "abstract": "article Free AccessA computer communication technique using content-induced transaction overlap Authors: Simon Y. Berkovich Allied Aerospace Technology Center, 9140 Md. Route 108, Columbia, MD and Washington University Allied Aerospace Technology Center, 9140 Md. Route 108, Columbia, MD and Washington UniversityView Profile , Colleen Roe Wilson Allied Aerospace Technology Center, 9140 Md. Route 108, Columbia, MD Allied Aerospace Technology Center, 9140 Md. Route 108, Columbia, MDView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 1February 1984 pp 60–77https://doi.org/10.1145/2080.357393Published:01 February 1984Publication History 6citation377DownloadsMetricsTotal Citations6Total Downloads377Last 12 Months11Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2018029204",
    "type": "article"
  },
  {
    "title": "Boosting Inter-process Communication with Architectural Support",
    "doi": "https://doi.org/10.1145/3532861",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Yubin Xia; Dong Du; Zhichao Hua; Binyu Zang; Haibo Chen; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "IPC (inter-process communication) is a critical mechanism for modern OSes, including not only microkernels such as seL4, QNX, and Fuchsia where system functionalities are deployed in user-level processes, but also monolithic kernels like Android where apps frequently communicate with plenty of user-level services. However, existing IPC mechanisms still suffer from long latency. Previous software optimizations of IPC usually cannot bypass the kernel that is responsible for domain switching and message copying/remapping across different address spaces; hardware solutions such as tagged memory or capability replace page tables for isolation, but usually require non-trivial modification to existing software stack to adapt to the new hardware primitives. In this article, we propose a hardware-assisted OS primitive, XPC (Cross Process Call), for efficient and secure synchronous IPC. XPC enables direct switch between IPC caller and callee without trapping into the kernel and supports secure message passing across multiple processes without copying. We have implemented a prototype of XPC based on the ARM AArch64 with Gem5 simulator and RISC-V architecture with FPGA boards. The evaluation shows that XPC can reduce IPC call latency from 664 to 21 cycles, 14×–123× improvement on Android Binder (ARM), and improve the performance of real-world applications on microkernels by 1.6× on Sqlite3.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4229371391",
    "type": "article"
  },
  {
    "title": "Efficient Instruction Scheduling Using Real-time Load Delay Tracking",
    "doi": "https://doi.org/10.1145/3548681",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Andreas Diavastos; Trevor E. Carlson",
    "corresponding_authors": "",
    "abstract": "Issue time prediction processors use dataflow dependencies and predefined instruction latencies to predict issue times of repeated instructions. In this work, we make two key observations: (1) memory accesses often take additional time to arrive than the static, predefined access latency that is used to describe these systems. This is due to contention in the memory hierarchy and variability in DRAM access times, and (2) we find that these memory access delays often repeat across iterations of the same code. We propose a new processor microarchitecture that replaces a complex reservation-station-based scheduler with an efficient, scalable alternative. Our scheduling technique tracks real-time delays of loads to accurately predict instruction issue times and uses a reordering mechanism to prioritize instructions based on that prediction. To accomplish this in an energy-efficient manner we introduce (1) an instruction delay learning mechanism that monitors repeated load instructions and learns their latest delay, (2) an issue time predictor that uses learned delays and dataflow dependencies to predict instruction issue times, and (3) priority queues that reorder instructions based on their issue time prediction. Our processor achieves 86.2% of the performance of a traditional out-of-order processor, higher than previous efficient scheduler proposals, while consuming 30% less power.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3198277378",
    "type": "article"
  },
  {
    "title": "Using Pattern of On-Off Routers and Links and Router Delays to Protect Network-on-Chip Intellectual Property",
    "doi": "https://doi.org/10.1145/3548680",
    "publication_date": "2022-07-13",
    "publication_year": 2022,
    "authors": "Arnab Kumar Biswas",
    "corresponding_authors": "Arnab Kumar Biswas",
    "abstract": "Intellectual Property (IP) reuse is a well known practice in chip design processes. Nowadays, network-on-chips (NoCs) are increasingly used as IP and sold by various vendors to be integrated in a multiprocessor system-on-chip (MPSoC). However, IP reuse exposes the design to IP theft, and an attacker can launch IP stealing attacks against NoC IPs. With the growing adoption of MPSoC, such attacks can result in huge financial losses. In this article, we propose four NoC IP protection techniques using fingerprint embedding: ON-OFF router-based fingerprinting (ORF), ON-OFF link-based fingerprinting (OLF), Router delay-based fingerprinting (RTDF), and Row delay-based fingerprinting (RWDF). ORF and OLF techniques use patterns of ON-OFF routers and links, respectively, while RTDF and RWDF techniques use router delays to embed fingerprints. We show that all of our proposed techniques require much less hardware overhead compared to an existing NoC IP security solution (square spiral routing) and also provide better security from removal and masking attacks. In particular, our proposed techniques require between 40.75% and 48.43% less router area compared to the existing solution. We also show that our solutions do not affect the normal packet latency and hence do not degrade the NoC performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4285087222",
    "type": "article"
  },
  {
    "title": "Partial Network Partitioning",
    "doi": "https://doi.org/10.1145/3576192",
    "publication_date": "2022-12-19",
    "publication_year": 2022,
    "authors": "Basil Alkhatib; Sreeharsha Udayashankar; Sara Qunaibi; Ahmed Alquraan; Mohammed Alfatafta; Wael Al-Manasrah; Alex Depoutovitch; Samer Al-Kiswany",
    "corresponding_authors": "",
    "abstract": "We present an extensive study focused on partial network partitioning. Partial network partitions disrupt the communication between some but not all nodes in a cluster. First, we conduct a comprehensive study of system failures caused by this fault in 13 popular systems. Our study reveals that the studied failures are catastrophic (e.g., lead to data loss), easily manifest, and are mainly due to design flaws. Our analysis identifies vulnerabilities in core systems mechanisms including scheduling, membership management, and ZooKeeper-based configuration management. Second, we dissect the design of nine popular systems and identify four principled approaches for tolerating partial partitions. Unfortunately, our analysis shows that implemented fault tolerance techniques are inadequate for modern systems; they either patch a particular mechanism or lead to a complete cluster shutdown, even when alternative network paths exist. Finally, our findings motivate us to build Nifty, a transparent communication layer that masks partial network partitions. Nifty builds an overlay between nodes to detour packets around partial partitions. Nifty provides an approach for applications to optimize their operation during a partial partition. We demonstrate the benefit of this approach through integrating Nifty with VoltDB, HDFS, and Kafka.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4313408232",
    "type": "article"
  },
  {
    "title": "Garbage collection for a client-server persistent object store",
    "doi": "https://doi.org/10.1145/320656.322741",
    "publication_date": "1999-08-01",
    "publication_year": 1999,
    "authors": "Laurent Amsaleg; Michael J. Franklin; Olivier Gruber",
    "corresponding_authors": "",
    "abstract": "We describe an efficient server-based algorithm for garbage collecting persistent object stores in a client-server environmnet. The algorithm is incremental and runs concurrently with client transactions. Unlike previous algorithms, it does not hold any transactional locks on data and does non require callbacks to clients. It is fault-tolerant, but performs very little logging. The algorithm has been designed to be integrated into existing systems, and therefore it works with standard implementation techniques such as Two-Phase Locking and Write-Ahead-Logging. In addition, it supports client-server performance optimizations such as client caching and flexible management of client buffers. We describe an implementation of the algorithm in the EXODUS storage manager and present the results of a performance study of the implementation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1985330774",
    "type": "article"
  },
  {
    "title": "Device reservation in audio/video editing systems",
    "doi": "https://doi.org/10.1145/253145.253149",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "David P. Anderson",
    "corresponding_authors": "David P. Anderson",
    "abstract": "What fraction of disks and other shared devices must be reserved to play an audio/video document without dropouts? In general, this question cannot be answered precisely. For documents with complex and irregular structure, such as those arising in audio/video editing, it is difficult even to give a good estimate. We describe three approaches to this problem. The first, based on long-term average properties of segments, is fast but imprecise: it underreserves in some cases and overreserves in others. The second approach models individual disk and network operations. It is precise but slow. The third approach, a hybrid, is both precise and fast.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2135638437",
    "type": "article"
  },
  {
    "title": "Lightweight Recoverable Virtual Memory - Corrigendum.",
    "doi": null,
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Mahadev Satyanarayanan; Henry H. Mashburn; Puneet Kumar; David C. Steere; James J. Kistler",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W110814608",
    "type": "article"
  },
  {
    "title": "Ordering subscribers on cable networks",
    "doi": "https://doi.org/10.1145/357401.357405",
    "publication_date": "1984-11-01",
    "publication_year": 1984,
    "authors": "Raphaël Rom",
    "corresponding_authors": "Raphaël Rom",
    "abstract": "article Free AccessOrdering subscribers on cable networks Author: Raphael Rom SRI International, 333 Ravenswood Ave., Menlo Park, CA SRI International, 333 Ravenswood Ave., Menlo Park, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 4pp 322–334https://doi.org/10.1145/357401.357405Published:01 November 1984Publication History 4citation294DownloadsMetricsTotal Citations4Total Downloads294Last 12 Months6Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2041686861",
    "type": "article"
  },
  {
    "title": "Using Multicore Reuse Distance to Study Coherence Directories",
    "doi": "https://doi.org/10.1145/3092702",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Minshu Zhao; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "Researchers have proposed numerous techniques to improve the scalability of coherence directories. The effectiveness of these techniques not only depends on application behavior, but also on the CPU's configuration, for example, its core count and cache size. As CPUs continue to scale, it is essential to explore the directory's application and architecture dependencies. However, this is challenging given the slow speed of simulators. While it is common practice to simulate different applications, previous research on directory designs have explored only a few—and in most cases, only one—CPU configuration, which can lead to an incomplete and inaccurate view of the directory's behavior. This article proposes to use multicore reuse distance analysis to study coherence directories. We develop a framework to extract the directory access stream from parallel least recently used (LRU) stacks, enabling rapid analysis of the directory's accesses and contents across both core count and cache size scaling. A key part of our framework is the notion of relative reuse distance between sharers , which defines sharing in a capacity-dependent fashion and facilitates our analyses along the data cache size dimension. We implement our framework in a profiler and then apply it to gain insights into the impact of multicore CPU scaling on directory behavior. Our profiling results show that directory accesses reduce by 3.3× when scaling the data cache size from 16KB to 1MB, despite an increase in sharing-based directory accesses. We also show that increased sharing caused by data cache scaling allows the portion of on-chip memory occupied by the directory to be reduced by 43.3%, compared to a reduction of only 2.6% when scaling the number of cores. And, we show certain directory entries exhibit high temporal reuse. In addition to gaining insights, we also validate our profile-based results, and find they are within 2--10% of cache simulations on average, across different validation experiments. Finally, we conduct four case studies that illustrate our insights on existing directory techniques. In particular, we demonstrate our directory occupancy insights on a Cuckoo directory; we apply our sharing insights to provide bounds on the size of Scalable Coherence Directories (SCD) and Dual-Grain Directories (DGD); and, we demonstrate our directory entry reuse insights on a multilevel directory design.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2740569903",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1365815",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4247729128",
    "type": "paratext"
  },
  {
    "title": "Ace",
    "doi": "https://doi.org/10.1145/320656.320657",
    "publication_date": "1999-08-01",
    "publication_year": 1999,
    "authors": "Mukund Raghavachari; Anne Rogers",
    "corresponding_authors": "",
    "abstract": "Customizing the protocols that manage accesses to different data structures within an application can improve the performance of software shared-memory programs substantially. Existing systems for using customizable protocols are hard to use directly because the mechanisms they provide for manipulating protocols are low-level ones. This article is an in-depth study of the issues involved in providing language support for application-specific protocols. We describe the design and implementation of a new language for parallel programming, Ace, that integrates support for customizable protocols with minimal extensions to C. Ace applications are developed using a shared-memory model with a default sequentially consistent protocol. Performance can then be optimized, with minor modifications to the application, by experimenting with different protocol libraries. The design of Ace was driven by a detailed study of the use of customizable protocols. We delineate the issues that arise when programming with customizable protocols and present novel abstractions that allow for their easy use. We describe the design and implementation of a runtime system and compiler for Ace nd discuss compiler optimizations that improve the performance of such software shared-memory systems. We study the communication patterns of a set of benchmark applications and consider the use of customizable protocols to optimize their performance. We evaluate the performance of our system through experiments on a Thinking Machine CM-5 and a Cray T3E. We also present measurements that demonstrate that Ace has good performance compared to that of a modern distributed shared-memory system.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1986399996",
    "type": "article"
  },
  {
    "title": "Modular and Distributed Management of Many-Core SoCs",
    "doi": "https://doi.org/10.1145/3458511",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Marcelo Ruaro; Anderson Camargo Sant'Ana; Axel Jantsch; Fernando Moraes",
    "corresponding_authors": "",
    "abstract": "Many-Core Systems-on-Chip increasingly require Dynamic Multi-objective Management (DMOM) of resources. DMOM uses different management components for objectives and resources to implement comprehensive and self-adaptive system resource management. DMOMs are challenging because they require a scalable and well-organized framework to make each component modular, allowing it to be instantiated or redesigned with a limited impact on other components. This work evaluates two state-of-the-art distributed management paradigms and, motivated by their drawbacks, proposes a new one called Management Application (MA) , along with a DMOM framework based on MA. MA is a distributed application, specific for management, where each task implements a management role. This paradigm favors scalability and modularity because the management design assumes different and parallel modules, decoupled from the OS. An experiment with a task mapping case study shows that MA reduces the overhead of management resources (-61.5%), latency (-66%), and communication volume (-96%) compared to state-of-the-art per-application management. Compared to cluster-based management (CBM) implemented directly as part of the OS, MA is similar in resources and communication volume, increasing only the mapping latency (+16%). Results targeting a complete DMOM control loop addressing up to three different objectives show the scalability regarding system size and adaptation frequency compared to CBM, presenting an overall management latency reduction of 17.2% and an overall monitoring messages’ latency reduction of 90.2%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3183107927",
    "type": "article"
  },
  {
    "title": "Apache Nemo: A Framework for Optimizing Distributed Data Processing",
    "doi": "https://doi.org/10.1145/3468144",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Won Wook Song; Youngseok Yang; Jeongyoon Eo; Jangho Seo; Joo Yeon Kim; Sanha Lee; Gyewon Lee; Taegeon Um; Haeyoon Cho; Byung-Gon Chun",
    "corresponding_authors": "",
    "abstract": "Optimizing scheduling and communication of distributed data processing for resource and data characteristics is crucial for achieving high performance. Existing approaches to such optimizations largely fall into two categories. First, distributed runtimes provide low-level policy interfaces to apply the optimizations, but do not ensure the maintenance of correct application semantics and thus often require significant effort to use. Second, policy interfaces that extend a high-level application programming model ensure correctness, but do not provide sufficient fine control. We describe Apache Nemo, an optimization framework for distributed dataflow processing that provides fine control for high performance and also ensures correctness for ease of use. We combine several techniques to achieve this, including an intermediate representation of dataflow, compiler optimization passes, and runtime extensions. Our evaluation results show that Nemo enables composable and reusable optimizations that bring performance improvements on par with existing specialized runtimes tailored for a specific deployment scenario. Apache Nemo is open-sourced at https://nemo.apache.org as an Apache incubator project.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3205831022",
    "type": "article"
  },
  {
    "title": "Charlotte: Reformulating Blockchains into a Web of Composable Attested Data Structures for Cross-Domain Applications",
    "doi": "https://doi.org/10.1145/3607534",
    "publication_date": "2023-07-22",
    "publication_year": 2023,
    "authors": "Isaac Sheff; Xin-Wen Wang; Kushal Babel; Haobin Ni; Robbert van Renesse; Andrew C. Myers",
    "corresponding_authors": "",
    "abstract": "Cross-domain applications are rapidly adopting blockchain techniques for immutability, availability, integrity, and interoperability. However, for most applications, global consensus is unnecessary and may not even provide sufficient guarantees. We propose a new distributed data structure: Attested Data Structures (ADS), which generalize not only blockchains but also many other structures used by distributed applications. As in blockchains, data in ADSs is immutable and self-authenticating. ADSs go further by supporting application-defined proofs ( attestations ). Attestations enable applications to plug in their own mechanisms to ensure availability and integrity. We present Charlotte , a framework for composable ADSs. Charlotte deconstructs conventional blockchains into more primitive mechanisms. Charlotte can be used to construct blockchains but does not impose the usual global-ordering overhead. Charlotte offers a flexible foundation for interacting applications that define their own policies for availability and integrity. Unlike traditional distributed systems, Charlotte supports heterogeneous trust: different observers have their own beliefs about who might fail, and how. Nevertheless, each observer has a consistent, available view of data. Charlotte’s data structures are interoperable and composable : applications and data structures can operate fully independently or can share data when desired. Charlotte defines a language-independent format for data blocks and a network API for servers. To demonstrate Charlotte’s flexibility, we implement several integrity mechanisms, including consensus and proof of work. We explore the power of disentangling availability and integrity mechanisms in prototype applications. The results suggest that Charlotte can be used to build flexible, fast, composable applications with strong guarantees.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385075275",
    "type": "article"
  },
  {
    "title": "A recursive algorithm for binary multiplication and its implementation",
    "doi": "https://doi.org/10.1145/6110.214399",
    "publication_date": "1985-11-01",
    "publication_year": 1985,
    "authors": "Renato De Mori; R. Cardin",
    "corresponding_authors": "",
    "abstract": "A new recursive algorithm for deriving the layout of parallel multipliers is presented. Based on this algorithm, a network for performing multiplications of two's complement numbers is proposed. The network can be implemented in a synchronous or an asynchronous way. If the factors to be multiplied have N bits, the area complexity of the network is O ( N 2 ) for practical values of N as in the case of cellular multipliers. Due to the design approach based on a recursive algorithm, a time complexity O (log N ) is achieved. It is shown how the structure can he pipelined with period complexity O (1) and used for single and double precision multiplication.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2049896844",
    "type": "article"
  },
  {
    "title": "A high-level abstraction of shared accesses",
    "doi": "https://doi.org/10.1145/332799.332800",
    "publication_date": "2000-02-01",
    "publication_year": 2000,
    "authors": "Peter J. Keleher",
    "corresponding_authors": "Peter J. Keleher",
    "abstract": "We describe the design and use of the tape mechanism, a new high-level abstraction of accesses to shared data for software DSMs. Tapes consolidate and generalize a number of recent protocol optimizations, including update-based locks and recorded-replay barriers. Tapes are usually created by “recording” shared accesses. The resulting recordings can be used to anticipate future accesses by tailoring data movement to application semantics. Tapes-based mechanisms are layered on top of existing shared-memory protocols, and are largely independent of the underlying memory model. Tapes can also be used to emulate the data-movement semantics of several update-based protocol implementations, without altering the underlying protocol implementation. We have used tapes to create the Tapeworm synchronization library. Tapeworm implements sophisticated record-replay mechanisms across barriers, augments locks with data-movement semantics, and allows the use of producer-consumer segments, which move entire modified segments when any portion of the segment is accessed. We show that Tapeworm eliminates 85% of remote misses, reduces message traffic by 63%, and improves performance by an average of 29% for our application suite.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2005903624",
    "type": "article"
  },
  {
    "title": "Optimizing Storage Performance for VM-Based Mobile Computing",
    "doi": "https://doi.org/10.1145/2465346.2465348",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Stephen Smaldone; Ben Gilbert; Jan Harkes; Liviu Iftode; Mahadev Satyanarayanan",
    "corresponding_authors": "",
    "abstract": "This article investigates the transient use of free local storage for improving performance in VM-based mobile computing systems implemented as thick clients on host PCs. We use the term TransientPC systems to refer to these types of systems. The solution we propose, called TransPart , uses the higher-performing local storage of host hardware to speed up performance-critical operations. Our solution constructs a virtual storage device on demand (which we call transient storage ) by borrowing free disk blocks from the host’s storage. In this article, we present the design, implementation, and evaluation of a TransPart prototype, which requires no modifications to the software or hardware of a host computer. Experimental results confirm that TransPart offers low overhead and startup cost, while improving user experience.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2033746939",
    "type": "article"
  },
  {
    "title": "Remark on \"Disk Cashe—miss ratio analysis and design consideration\"",
    "doi": "https://doi.org/10.1145/7351.8930",
    "publication_date": "1987-01-05",
    "publication_year": 1987,
    "authors": "Alan Jay Smith",
    "corresponding_authors": "Alan Jay Smith",
    "abstract": "No abstract available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2002976690",
    "type": "article"
  },
  {
    "title": "Coupling compiler-enabled and conventional memory accessing for energy efficiency",
    "doi": "https://doi.org/10.1145/986533.986535",
    "publication_date": "2004-05-01",
    "publication_year": 2004,
    "authors": "Raksit Ashok; Saurabh Chheda; Csaba Andras Moritz",
    "corresponding_authors": "",
    "abstract": "This article presents Cool-Mem, a family of memory system architectures that integrate conventional memory system mechanisms, energy-aware address translation, and compiler-enabled cache disambiguation techniques, to reduce energy consumption in general-purpose architectures. The solutions provided in this article leverage on interlayer tradeoffs between architecture, compiler, and operating system layers. Cool-Mem achieves power reduction by statically matching memory operations with energy-efficient cache and virtual memory access mechanisms. It combines statically speculative cache access modes, a dynamic content addressable memory-based (CAM-based) Tag-Cache used as backup for statically mispredicted accesses, different conventional multilevel associative cache organizations, embedded protection checking along all cache access mechanisms, as well as architectural organizations to reduce the power consumed by address translation in virtual memory. Because it is based on speculative static information, a superset of the predictable program information available at compile-time, our approach removes the burden of provable correctness in compiler analysis passes that extract static information. This makes Cool-Mem highly practical, applicable for large and complex applications, without having any limitations due to complexity issues in our compiler passes or the presence of precompiled static libraries. Based on extensive evaluation, for both SPEC2000 and Mediabench applications, we obtain from 6% to 19% total energy savings in the processor, with performance ranging from 1.5% degradation to 6% improvement, for the applications studied. We have also compared Cool-Mem to several prior arts and have found Cool-Mem to perform better in almost all cases.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2019644923",
    "type": "article"
  },
  {
    "title": "Corrigendum: “Computational algorithms for state dependent queuing networks”",
    "doi": "https://doi.org/10.1145/357377.357385",
    "publication_date": "1983-11-01",
    "publication_year": 1983,
    "authors": "Charles H. Sauer",
    "corresponding_authors": "Charles H. Sauer",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2039763523",
    "type": "erratum"
  },
  {
    "title": "An OpenMP Runtime for Transparent Work Sharing across Cache-Incoherent Heterogeneous Nodes",
    "doi": "https://doi.org/10.1145/3505224",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Robert Lyerly; Carlos Bilbao; Changwoo Min; Christopher J. Rossbach; Binoy Ravindran",
    "corresponding_authors": "",
    "abstract": "In this work, we present libHetMP , an OpenMP runtime for automatically and transparently distributing parallel computation across heterogeneous nodes. libHetMP targets platforms comprising CPUs with different instruction set architectures (ISA) coupled by a high-speed memory interconnect, where cross-ISA binary incompatibility and non-coherent caches require application data be marshaled to be shared across CPUs. Because of this, work distribution decisions must take into account both relative compute performance of asymmetric CPUs and communication overheads. libHetMP drives workload distribution decisions without programmer intervention by measuring performance characteristics during cross-node execution. A novel HetProbe loop iteration scheduler decides if cross-node execution is beneficial and either distributes work according to the relative performance of CPUs when it is or places all work on the set of homogeneous CPUs providing the best performance when it is not. We evaluate libHetMP using compute kernels from several OpenMP benchmark suites and show a geometric mean 41% speedup in execution time across asymmetric CPUs. Because some workloads may showcase irregular behavior among iterations, we extend libHetMP with a second scheduler called HetProbe-I. The evaluation of HetProbe-I shows it can further improve speedup for irregular computation, in some cases up to a 24%, by triggering periodic distribution decisions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4226469047",
    "type": "article"
  },
  {
    "title": "Optimal semicomputable approximations to reachable and invariant sets",
    "doi": null,
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Pieter Collins",
    "corresponding_authors": "Pieter Collins",
    "abstract": "In this paper we consider the computation of reachable, viable and invariant sets for discrete-time systems. We use the framework of type-two effectivity,\r\nin which computations are performed by Turing machines with inﬁnite input and output tapes, with the representations of computable topology. We see that the reachable set is lower-semicomputable, and the viability and invariance kernels are upper-semicomputable. We then deﬁne an upper-semicomputable over-approximation to the reachable set, and lower-semicomputable under-approximations to the viability and invariance kernels, and show that these approximations are optimal.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2568858028",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1233307",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Silicon technology will continue to provide an exponential increase in the availability of raw transistors. Effectively translating this resource into application performance, however, is an open challenge that conventional superscalar designs will not ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4235606052",
    "type": "paratext"
  },
  {
    "title": "Preface to the Special Issue on Operating System Principles.",
    "doi": null,
    "publication_date": "1986-01-01",
    "publication_year": 1986,
    "authors": "Anita K. Jones",
    "corresponding_authors": "Anita K. Jones",
    "abstract": "",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1967732503",
    "type": "article"
  },
  {
    "title": "Corrigendum to “Derecho",
    "doi": "https://doi.org/10.1145/3395604",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Jha",
    "corresponding_authors": "Jha",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3042017768",
    "type": "erratum"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3323874",
    "publication_date": "2019-04-16",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Cloud computing services often replicate data and may require ways to coordinate distributed actions. Here we present Derecho, a library for such tasks. The API provides interfaces for structuring applications into patterns of subgroups and shards, ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4237532833",
    "type": "paratext"
  },
  {
    "title": "Boki: Towards Data Consistency and Fault Tolerance with Shared Logs in Stateful Serverless Computing",
    "doi": "https://doi.org/10.1145/3653072",
    "publication_date": "2024-09-12",
    "publication_year": 2024,
    "authors": "Zhipeng Jia; Emmett Witchel",
    "corresponding_authors": "",
    "abstract": "Boki is a new serverless runtime that exports a shared log API to serverless functions. Boki shared logs enable stateful serverless applications to manage their state with durability, consistency, and fault tolerance. Boki shared logs achieve high throughput and low latency. The key enabler is the metalog , a novel mechanism that allows Boki to address ordering, consistency and fault tolerance independently. The metalog orders shared log records with high throughput and it provides read consistency while allowing service providers to optimize the write and read path of the shared log in different ways. To demonstrate the value of shared logs for stateful serverless applications, we build Boki support libraries that implement fault-tolerant workflows, durable object storage, and message queues. Our evaluation shows that shared logs can speed up important serverless workloads by up to 4.2 ×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402530340",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3696656",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Dear readers, in almost all editorials I talked about volunteers dropping out of our Editorial Board and new ones coming in. It may sound like a lot of fluctuation among the members of the Editorial Board, but it is not. Associate Editors start with a two-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404621717",
    "type": "editorial"
  },
  {
    "title": "Optimally adaptive, minimum-distance, circuit-switched routing in hypercubes",
    "doi": "https://doi.org/10.1145/253145.253261",
    "publication_date": "1997-05-01",
    "publication_year": 1997,
    "authors": "Ausif Mahmood; D. J. Lynch; Roger B. Shaffer",
    "corresponding_authors": "",
    "abstract": "In circuit-switched routing, the path between a source and its destination is established by incrementally reserving all required links before the data transmission can begin. If the routing algorithm is not carefully designed, deadlocks can occur in reserving these links. Deadlock-free algorithms based on dimension-ordered routing, such as the E-cube , exist. However, E-cube does not provide any flexibility in choosing a path from a source to its destination and can thus result in long latencies under heavy or uneven traffic. Adaptive, minimum-distance routing algorithms, such as the Turn Model and the UP Preference algorithms, have previously been reported. In this article, we present a new class of adaptive, provably deadlock-free, minimum-distance routing algorithms. We prove that the algorithms developed here are optimally adaptive in the sense that any further flexibility in communication will result in deadlock. We show that the Turn Model is actually a member of our new class of algorithms that does not perform as well as other algorithms within the new class. It creates artificial hotspots in routing the traffic and allows fewer total paths. We present an analytical comparison of the flexibility and balance in routing provided by various algorithms and a comparison based on uniform and nonuniform traffic simulations. The Extended UP Preference algorithm developed in this article is shown to have improved performance with respect to existing algorithms. The methodology and the algorithms developed here can be used to develop routing for other schemes such as wormhole routing, and for other recursively defined networks such as k -ary n -cubes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2070807444",
    "type": "article"
  },
  {
    "title": "An optimized contention protocol for broadband networks",
    "doi": "https://doi.org/10.1145/24068.27644",
    "publication_date": "1987-08-01",
    "publication_year": 1987,
    "authors": "Worth Kirkman",
    "corresponding_authors": "Worth Kirkman",
    "abstract": "This paper describes the concepts underlying an alternative link-level protocol for broadband local networks. The protocol uses implicit slotting of the contention channel to support larger networks, improve performance, and provide reliable distributed collision recognition without reinforcement. It is designed such that compatible interfaces to existing CSMA/CD-based systems can be provided.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2088183430",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2912575",
    "publication_date": "2016-05-05",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recent work in the field of value prediction (VP) has shown that given an efficient confidence estimation mechanism, prediction validation could be removed from the out-of-order engine and delayed until commit time. As a result, a simple recovery ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231509189",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2818727",
    "publication_date": "2015-09-11",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "RAMCloud is a storage system that provides low-latency access to large-scale datasets. To achieve low latency, RAMCloud stores all data in DRAM at all times. To support large capacities (1PB or more), it aggregates the memories of thousands of servers ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232261090",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2723895",
    "publication_date": "2015-01-20",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "What opportunities for multicore scalability are latent in software interfaces, such as system call APIs? Can scalability challenges and opportunities be identified even before any implementation exists, simply by considering interface specifications? ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236477232",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2841315",
    "publication_date": "2015-11-02",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recent device hardware trends enable a new approach to the design of network server operating systems. In a traditional operating system, the kernel mediates access to device hardware by server applications to enforce process isolation as well as ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237069181",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2785582",
    "publication_date": "2015-06-08",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Mobile System-on-Chips (SoC) that incorporate heterogeneous coherence domains promise high energy efficiency to a wide range of mobile applications, yet are difficult to program. To exploit the architecture, a desirable, yet missing capability is to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238108443",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2666140",
    "publication_date": "2014-09-23",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Approximate computing, where computation accuracy is traded off for better performance or higher data throughput, is one solution that can help data processing keep pace with the current and growing abundance of information. For particular domains, such ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243068556",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2966277",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Complex data queries, because of their need for random accesses, have proven to be slow unless all the data can be accommodated in DRAM. There are many domains, such as genomics, geological data, and daily Twitter feeds, where the datasets of interest ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245529851",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2745713",
    "publication_date": "2015-03-11",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article studies undefined behavior arising in systems programming languages such as C/C++. Undefined behavior bugs lead to unpredictable and subtle systems behavior, and their effects can be further amplified by compiler optimizations. Undefined ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247066285",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2584468",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As GPU hardware becomes increasingly general-purpose, it is quickly outgrowing the traditional, constrained GPU-as-coprocessor programming model. This article advocates for extending standard operating system services and abstractions to GPUs in order ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251339293",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2642648",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In August 2011, Linux entered its third decade. Ten years before, Chou et al. published a study of faults found by applying a static analyzer to Linux versions 1.0 through 2.4.1. A major result of their work was that the drivers directory contained up ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255287857",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2912578",
    "publication_date": "2016-04-06",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The L4 microkernel has undergone 20 years of use and evolution. It has an active user and developer community, and there are commercial versions that are deployed on a large scale and in safety-critical systems. In this article we examine the lessons ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256282039",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2003690",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Distributed mobile transactions utilize commit protocols to achieve atomicity and consistent decisions. This is challenging, as mobile environments are typically characterized by frequent perturbations such as network disconnections and node failures. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230553845",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2542150",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "CORFU is a global log which clients can append-to and read-from over a network. Internally, CORFU is distributed over a cluster of machines in such a way that there is no single I/O bottleneck to either appends or reads. Data is fully replicated for ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230766142",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2063509",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231178475",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1925109",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235962023",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2427631",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Reuse Distance (RD) analysis is a powerful memory analysis tool that can potentially help architects study multicore processor scaling. One key obstacle, however, is that multicore RD analysis requires measuring Concurrent Reuse Distance (CRD) and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238271965",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2465346",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Content-based publish/subscribe (CPS) is an appealing abstraction for building scalable distributed systems, e.g., message boards, intrusion detectors, or algorithmic stock trading platforms. Recently, CPS extensions have been proposed for location-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239734656",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1963559",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Large-scale network services can consist of tens of thousands of machines running thousands of unique software configurations spread across hundreds of physical networks. Testing such services for complex performance problems and configuration errors ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247611971",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2518037",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present a taxonomy and modular implementation approach for data-parallel accelerators, including the MIMD, vector-SIMD, subword-SIMD, SIMT, and vector-thread (VT) architectural design patterns. We introduce Maven, a new VT microarchitecture based on ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249676484",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2542150.2542151",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251885794",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1813654",
    "publication_date": "2010-07-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229769589",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1534909",
    "publication_date": "2009-05-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A mechanistic model for out-of-order superscalar processors is developed and then applied to the study of microarchitecture resource scaling. The model divides execution time into intervals separated by disruptive miss events such as branch ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231271804",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1841313",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236762752",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1880018",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237165891",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1629087",
    "publication_date": "2009-11-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a new paradigm for building scalable distributed systems. Our approach does not require dealing with message-passing protocols, a major complication in existing distributed systems. Instead, developers just design and manipulate data ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239321624",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2110356",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents S2E, a platform for analyzing the properties and behavior of software systems, along with its use in developing tools for comprehensive performance profiling, reverse engineering of proprietary software, and automated testing of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247686547",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1482619",
    "publication_date": "2009-02-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248141129",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2382553",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article describes the historical context, technical challenges, and main implementation techniques used by VMware Workstation to bring virtualization to the x86 architecture in 1999. Although virtual machine monitors (VMMs) had been around for ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251812001",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1731060",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252863528",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2166879",
    "publication_date": "2012-04-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Asymmetric multicore processors (AMPs) consist of cores with the same ISA (instruction-set architecture), but different microarchitectural features, speed, and power consumption. Because cores with more complex features and higher speed typically use ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254159322",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1658357",
    "publication_date": "2009-12-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A longstanding vision in distributed systems is to build reliable systems from unreliable components. An enticing formulation of this vision is Byzantine Fault-Tolerant (BFT) state machine replication, in which a group of servers collectively act as a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255041286",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2324876",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Smartphones are increasingly ubiquitous, and many users carry multiple phones to accommodate work, personal, and geographic mobility needs. We present Cells, a virtualization architecture for enabling multiple virtual smartphones to run simultaneously ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255458897",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3014162",
    "publication_date": "2017-01-16",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modern storage technology (solid-state disks (SSDs), NoSQL databases, commoditized RAID hardware, etc.) brings new reliability challenges to the already-complicated storage stack. Among other things, the behavior of these new components during power ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229736093",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1275517",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Designing and implementing system software so that it scales well on shared-memory multiprocessors (SMMPs) has proven to be surprisingly challenging. To improve scalability, most designers to date have focused on concurrency by iteratively eliminating ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231021562",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1455258",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Worm containment must be automatic because worms can spread too fast for humans to respond. Recent work proposed network-level techniques to automate worm containment; these techniques have limitations because there is no information about the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234322834",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1394441",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We introduce external synchrony, a new model for local file I/O that provides the reliability and simplicity of synchronous I/O, yet also closely approximates the performance of asynchronous I/O. An external observer cannot distinguish the output of a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235649105",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1189736",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Write buffering is one of many successful mechanisms that improves the performance and scalability of multiprocessors. However, it leads to more complex memory system behavior, which cannot be described using intuitive consistency models, such as ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237350607",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3067095",
    "publication_date": "2017-07-21",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Mobile applications are increasingly being built using web technologies as a common substrate to achieve portability and to improve developer productivity. Unfortunately, web applications often incur large performance overhead, directly affecting the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239376559",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3160907",
    "publication_date": "2017-12-29",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The ubiquity of multicore processors has led programmers to write parallel and concurrent applications to take advantage of the underlying hardware and speed up their executions. In this context, Transactional Memory (TM) has emerged as a simple and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241397387",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3129286",
    "publication_date": "2017-10-10",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247154888",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1328671",
    "publication_date": "2008-02-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Component-based software structuring principles are now commonplace at the application level; but componentization is far less established when it comes to building low-level systems software. Although there have been pioneering efforts in applying ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250217681",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1314299",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many real-time systems, such as battery-operated embedded devices, are energy constrained. A common problem for these systems is how to reduce energy consumption in the system as much as possible while still meeting the deadlines; a commonly used power ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253044565",
    "type": "paratext"
  },
  {
    "title": "Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing",
    "doi": "https://doi.org/10.1145/3630006",
    "publication_date": "2023-11-18",
    "publication_year": 2023,
    "authors": "Laiping Zhao; Yangyang Cui; Yanan Yang; Xiaobo Zhou; Tie Qiu; Keqiu Li; Yungang Bao",
    "corresponding_authors": "",
    "abstract": "Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput. We present Rhythm , a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3%, CPU utilization by 38.6%, and memory bandwidth utilization by 45.4% while guaranteeing the tail latency requirement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388796489",
    "type": "article"
  },
  {
    "title": "Diciclo: Flexible User-level Services for Efficient Multitenant Isolation",
    "doi": "https://doi.org/10.1145/3639404",
    "publication_date": "2023-12-30",
    "publication_year": 2023,
    "authors": "Giorgos Kappes; Stergios V. Anastasiadis",
    "corresponding_authors": "",
    "abstract": "Containers are a mainstream virtualization technique for running stateful workloads over persistent storage. In highly utilized multitenant hosts, resource contention at the system kernel leads to inefficient container input/output (I/O) handling. Although there are interesting techniques to address this issue, they incur high implementation complexity and execution overhead. As a cost-effective alternative, we introduce the Diciclo architecture with our assumptions, goals, and principles. For each tenant, Diciclo isolates the control and data I/O path at user level and runs dedicated storage systems. Diciclo includes the libservice unified user-level abstraction of system services and the node structure design pattern for the application and server side. We prototyped a toolkit of user-level components that comprise the library to invoke the standard I/O calls, the I/O communication mechanism, and the I/O services. Based on Diciclo, we built Danaus, a filesystem client that integrates a union filesystem with a Ceph distributed filesystem client and configurable shared cache. Across different host configurations, workloads, and systems, Danaus achieves improved performance stability, because it handles I/O with reserved per-tenant resources and avoids intensive kernel locking. Based on having built and evaluated Danaus, we share valuable lessons about resource contention, file management, service separation, and performance stability in multitenant systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390444060",
    "type": "article"
  },
  {
    "title": "Editor's introduction",
    "doi": "https://doi.org/10.1145/357353.357354",
    "publication_date": "1983-02-01",
    "publication_year": 1983,
    "authors": "Anita K. Jones",
    "corresponding_authors": "Anita K. Jones",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4234047810",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on the Award Papers of USENIX ATC 2019",
    "doi": "https://doi.org/10.1145/3395034",
    "publication_date": "2018-11-30",
    "publication_year": 2018,
    "authors": "Dahlia Malkhi; Dan Tsafrir",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3034009154",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1124153",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Cryptographic transformations are a fundamental building block in many security applications and protocols. To improve performance, several vendors market hardware accelerator cards. However, until now no operating system provided a mechanism that ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237187996",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3341160",
    "publication_date": "2019-08-16",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Arm Triple Core Lock-Step (TCLS) architecture is the natural evolution of Arm Cortex-R Dual Core Lock-Step (DCLS) processors to increase dependability, predictability, and availability in safety-critical and ultra-reliable applications. TCLS is ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239915444",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1189256",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents a new mechanism that enables applications to run correctly when device drivers fail. Because device drivers are the principal failing component in most systems, reducing driver-induced failures greatly improves overall reliability. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240141591",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1132026",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246856248",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3319851",
    "publication_date": "2019-03-28",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A plethora of optimized mutex lock algorithms have been designed over the past 25 years to mitigate performance bottlenecks related to critical sections and locks. Unfortunately, there is currently no broad study of the behavior of these optimized lock ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247787902",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3297862",
    "publication_date": "2018-12-16",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Monitoring and troubleshooting distributed systems is notoriously difficult; potential problems are complex, varied, and unpredictable. The monitoring and diagnosis tools commonly used today—logs, counters, and metrics—have two important limitations: ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248465709",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1151690",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In digital hardware system design, the quality of the product is directly related to the number of meaningful design alternatives properly considered. Unfortunately, existing modeling methodologies and tools have properties which make them less than ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252848383",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1047915.1047916",
    "publication_date": "2005-02-02",
    "publication_year": 2005,
    "authors": "Carla Schlatter Ellis",
    "corresponding_authors": "Carla Schlatter Ellis",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254293339",
    "type": "editorial"
  },
  {
    "title": "Scaling Membership of Byzantine Consensus",
    "doi": "https://doi.org/10.1145/3473138",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Burcu Canakci; Robbert van Renesse",
    "corresponding_authors": "",
    "abstract": "Scaling Byzantine Fault Tolerant (BFT) systems in terms of membership is important for secure applications with large participation such as blockchains. While traditional protocols have low latency, they cannot handle many processors. Conversely, blockchains often have hundreds to thousands of processors to increase robustness, but they typically have high latency or energy costs. We describe various sources of unscalability in BFT consensus protocols. To improve performance, many BFT protocols optimize the “normal case,” where there are no failures. This can be done in a modular fashion by wrapping existing BFT protocols with a building block that we call alliance . In normal case executions, alliance can scalably determine if the initial conditions of a BFT consensus protocol predetermine the outcome, obviating running the consensus protocol. We give examples of existing protocols that solve alliance. We show that a solution based on hypercubes and MAC s has desirable scalability and performance in normal case executions, with only a modest overhead otherwise. We provide important optimizations. Finally, we evaluate our solution using the ns3 simulator and show that it scales up to thousands of processors and compare with prior work in various network topologies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3206058309",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/859716.859717",
    "publication_date": "2003-08-01",
    "publication_year": 2003,
    "authors": "Carla Schlatter Ellis",
    "corresponding_authors": "Carla Schlatter Ellis",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4214569170",
    "type": "editorial"
  },
  {
    "title": "ROME: All Overlays Lead to Aggregation, but Some Are Faster than Others",
    "doi": "https://doi.org/10.1145/3516430",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Marcel Blöcher; Emilio Coppa; Pascal Kleber; Patrick Eugster; William Culhane; Masoud Saeida Ardekani",
    "corresponding_authors": "",
    "abstract": "Aggregation is common in data analytics and crucial to distilling information from large datasets, but current data analytics frameworks do not fully exploit the potential for optimization in such phases. The lack of optimization is particularly notable in current “online” approaches that store data in main memory across nodes, shifting the bottleneck away from disk I/O toward network and compute resources, thus increasing the relative performance impact of distributed aggregation phases. We present ROME, an aggregation system for use within data analytics frameworks or in isolation. ROME uses a set of novel heuristics based primarily on basic knowledge of aggregation functions combined with deployment constraints to efficiently aggregate results from computations performed on individual data subsets across nodes (e.g., merging sorted lists resulting from top- k ). The user can either provide minimal information that allows our heuristics to be applied directly, or ROME can autodetect the relevant information at little cost. We integrated ROME as a subsystem into the Spark and Flink data analytics frameworks. We use real-world data to experimentally demonstrate speedups up to 3× over single-level aggregation overlays, up to 21% over other multi-level overlays, and 50% for iterative algorithms like gradient descent at 100 iterations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220916461",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/244764.258064",
    "publication_date": "1997-02-01",
    "publication_year": 1997,
    "authors": "Kenneth P. Birman",
    "corresponding_authors": "Kenneth P. Birman",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247327773",
    "type": "editorial"
  },
  {
    "title": "Guest editorial",
    "doi": "https://doi.org/10.1145/265924.267934",
    "publication_date": "1997-11-01",
    "publication_year": 1997,
    "authors": "Henry M. Levy",
    "corresponding_authors": "Henry M. Levy",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253916643",
    "type": "editorial"
  },
  {
    "title": "Preface: Special issue on measurement and modeling of computer systems",
    "doi": "https://doi.org/10.1145/357360.357361",
    "publication_date": "1983-05-01",
    "publication_year": 1983,
    "authors": "Herbert D. Schwetman",
    "corresponding_authors": "Herbert D. Schwetman",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2002309669",
    "type": "article"
  },
  {
    "title": "Preface: Special issue on communication architectures and protocols",
    "doi": "https://doi.org/10.1145/357377.357378",
    "publication_date": "1983-11-01",
    "publication_year": 1983,
    "authors": "Anita K. Jones",
    "corresponding_authors": "Anita K. Jones",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2020991889",
    "type": "article"
  },
  {
    "title": "Author Index",
    "doi": "https://doi.org/10.1145/235543.237494",
    "publication_date": "1996-11-01",
    "publication_year": 1996,
    "authors": "Author Index",
    "corresponding_authors": "Author Index",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229671433",
    "type": "paratext"
  },
  {
    "title": "Preface to the Special Issue on Architectural Support for Programming Languages and Systems.",
    "doi": null,
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "Kenneth P. Birman",
    "corresponding_authors": "Kenneth P. Birman",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W118243126",
    "type": "article"
  },
  {
    "title": "Preface: Special issue on measurement and modeling of computer systems",
    "doi": "https://doi.org/10.1145/190.357396",
    "publication_date": "1984-05-01",
    "publication_year": 1984,
    "authors": "Alan Jay Smith",
    "corresponding_authors": "Alan Jay Smith",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2014397647",
    "type": "article"
  },
  {
    "title": "Guest Editorial - Special Section on Communication Architectures and Protocols.",
    "doi": null,
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "Larry Peterson",
    "corresponding_authors": "Larry Peterson",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W35584357",
    "type": "editorial"
  },
  {
    "title": "Performance analysis of checkpointing strategies",
    "doi": "https://doi.org/10.1145/190.357398",
    "publication_date": "1984-05-01",
    "publication_year": 1984,
    "authors": "Asser Tantawi; Manfred Ruschitzka",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Performance analysis of checkpointing strategies Authors: Asser N. Tantawi IBM Thomas J. Watson Research Center, Yorktown Heights, NY IBM Thomas J. Watson Research Center, Yorktown Heights, NYView Profile , Manfred Ruschitzka Department of Electrical and Computer Engineering, University of California at Davis, Davis, CA Department of Electrical and Computer Engineering, University of California at Davis, Davis, CAView Profile Authors Info & Claims ACM Transactions on Computer SystemsVolume 2Issue 2pp 123–144https://doi.org/10.1145/190.357398Published:01 May 1984Publication History 94citation944DownloadsMetricsTotal Citations94Total Downloads944Last 12 Months29Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4362174728",
    "type": "article"
  },
  {
    "title": "Preface to the Special Section on Measurement and Modeling of Computer Systems.",
    "doi": null,
    "publication_date": "1991-01-01",
    "publication_year": 1991,
    "authors": "Alan Jay Smith",
    "corresponding_authors": "Alan Jay Smith",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W6697879",
    "type": "article"
  },
  {
    "title": "Preface to the Special Issues on Computer Architecture.",
    "doi": null,
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Kenneth P. Birman",
    "corresponding_authors": "Kenneth P. Birman",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W94382054",
    "type": "article"
  }
]